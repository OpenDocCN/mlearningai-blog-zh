<html>
<head>
<title>How does PyTorch 2.0 perform in inference? A benchmark with TensorRT and ONNX Runtime</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 2.0在推理方面表现如何？带有TensorRT和ONNX运行时的基准测试</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/how-does-pytorch-2-0-perform-in-inference-a-benchmark-with-tensorrt-and-onnx-runtime-fa1e59237f93?source=collection_archive---------1-----------------------#2022-12-27">https://medium.com/mlearning-ai/how-does-pytorch-2-0-perform-in-inference-a-benchmark-with-tensorrt-and-onnx-runtime-fa1e59237f93?source=collection_archive---------1-----------------------#2022-12-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/1e160d57ece809e06f338cc999da0238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9oy8pgXZbztifhBK.png"/></div></div></figure><p id="07ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PyTorch 2.0 于2022年12月初在<a class="ae jn" href="https://nips.cc/" rel="noopener ugc nofollow" target="_blank"> NeurIPS 2022 </a>上发布，并因其主要的torch.compile组件而引起了很多关注，预计该组件将比PyTorch的先前版本带来更大的计算速度。</p><p id="5e56" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这对人工智能世界来说是一个惊人的消息，训练时间改进的早期结果令人印象深刻。PyTorch团队在PyTorch GitHub 上的<a class="ae jn" href="https://pytorch.org/get-started/pytorch-2.0/" rel="noopener ugc nofollow" target="_blank">发布新闻稿</a>和<a class="ae jn" href="https://github.com/pytorch/torchdynamo/issues/681" rel="noopener ugc nofollow" target="_blank">中没有提到的是PyTorch 2.0推理性能。</a></p><p id="d525" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们对这个主题进行更多的研究，并发现PyTorch 2.0与其他推理加速器如<a class="ae jn" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"> Nvidia TensorRT </a>和<a class="ae jn" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX Runtime </a>相比表现如何。</p><p id="2af1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们使用<a class="ae jn" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster" rel="noopener ugc nofollow" target="_blank"> Speedster </a>、<a class="ae jn" href="https://www.nebuly.com/" rel="noopener ugc nofollow" target="_blank"> Nebuly </a>的开源库运行了一些推理测试，以应用SOTA优化技术，并在您的硬件上实现最大的推理加速。对于这个用例，Speedster允许我们运行TensorRT、ONNX运行时，并在短短两行代码内将它们与16位和8位动态和静态量化结合起来。在测试期间，我们还使用Speedster收集顶层策略的性能信息，以减少推理延迟。<br/>我们在配有ResNet的Nvidia 3090Ti GPU上运行了测试，该型号与PyTorch 2.0新闻稿中的示例中使用的型号相同。</p><h1 id="1383" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">用<a class="ae jn" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster" rel="noopener ugc nofollow" target="_blank"> Speedster </a>测试PyTorch 2.0的推理性能</h1><figure class="kn ko kp kq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es km"><img src="../Images/909a3c6bbe4fb745c02a6c58851a6427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FGeCEkdnqchMGbpk.jpeg"/></div></div></figure><p id="509c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是测试得出的4个主要结论:</p><ol class=""><li id="288d" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated"><strong class="ir hi"> PyTorch 2.0与之前批量更大的版本相比变得越来越有效</strong>。在更高的批处理速度下，fp16精度变得比fp32编译版本更有效。这很容易解释，因为Pytorch 2.0编译主要是为训练设计的，通常批量比推理大。关注fp16是有意义的，因为训练程序最近已经从全精度转向半精度，特别是对于大型模型。</li><li id="8aea" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated"><strong class="ir hi"> ONNX运行时在小批量时比PyTorch 2.0执行得好得多，而在大批量时结果却相反</strong>。同样，这是因为ONNX运行时主要是为推理而设计的(通常使用较小的批量)，而如前所述，PyTorch 2.0的主要目标是训练。</li><li id="2e83" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated"><strong class="ir hi">py torch紧急模式和PyTorch 2.0(已编译)显示批量1和8 </strong>的运行时间相同。这表明两个运行时在批处理大小等于1时没有使用全部计算能力，而其他推理驱动的优化器(如ONNX运行时)能够更好地管理计算能力。同样，这可能与PyTorch编译器主要是为训练而设计的这一事实有关，忽略了批处理大小不足以使用内核的所有计算能力的情况。</li><li id="373e" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm kw kx ky kz bi translated"><strong class="ir hi">在测试的Nvidia GPU上，TensorRT在小批量和大批量方面都远远优于竞争对手</strong>。事实上，随着批量的增加，相对速度会变得更快。这显示了Nvidia的工程师如何能够在推理时更好地利用硬件缓存，因为激活占用的内存随着批量大小线性增长，适当的内存使用可以大大提高性能。</li></ol><p id="8b24" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，基准测试高度依赖于所使用的数据、模型、硬件和优化技术。为了在推理中获得最佳性能，在将模型部署到产品中之前，总是建议<a class="ae jn" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster" rel="noopener ugc nofollow" target="_blank">测试所有优化器</a>。</p><h1 id="c9a6" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><ul class=""><li id="3d2f" class="kr ks hh ir b is lf iw lg ja lh je li ji lj jm lk kx ky kz bi translated"><a class="ae jn" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster" rel="noopener ugc nofollow" target="_blank"> Speedster </a>，这款开源工具可以自动应用SOTA优化技术，在您的硬件上实现最大的推理加速。</li><li id="17d0" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated">GitHub <a class="ae jn" href="https://github.com/diegofiori/benchmark-pytorch2.0-with-nebullvm" rel="noopener ugc nofollow" target="_blank">存储库</a>用代码复制基准测试实验</li></ul><p id="6be6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi">___</p><h1 id="1de3" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">推荐读物</h1><ul class=""><li id="4739" class="kr ks hh ir b is lf iw lg ja lh je li ji lj jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/chatllama-0-0-2-release-notes" rel="noopener ugc nofollow" target="_blank">遇见ChatLLaMA:用有限的计算资源构建一个类似ChatGPT的助手</a></li><li id="fd64" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps" rel="noopener ugc nofollow" target="_blank">如何利用NVIDIA多进程服务(MPS)提高Kubernetes中的GPU利用率</a></li><li id="96cd" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/reinforcement-learning-from-human-feedback-rlhf-a-simplified-explanation" rel="noopener ugc nofollow" target="_blank">人类反馈强化学习(RLHF)——一个简化的解释</a></li><li id="9a02" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/metas-llama-a-small-language-model-beating-giants" rel="noopener ugc nofollow" target="_blank"> META的美洲驼:打败巨人的小语种模型</a></li><li id="4fdc" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/geoffrey-hinton-forward-forward" rel="noopener ugc nofollow" target="_blank"> PyTorch实现Geoffrey Hinton的前向-前向算法</a></li><li id="b0c5" class="kr ks hh ir b is la iw lb ja lc je ld ji le jm lk kx ky kz bi translated"><a class="ae jn" href="https://www.nebuly.com/blog/top-10-newsletters-on-machine-learning-and-ai-in-2023" rel="noopener ugc nofollow" target="_blank">2023年关于机器学习和人工智能的10大最佳时事通讯</a></li></ul><div class="ll lm ez fb ln lo"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">medium.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc in lo"/></div></div></a></div></div></div>    
</body>
</html>