<html>
<head>
<title>Feedforward Neural Networks (Multi layers Preceptors MLPs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">前馈神经网络(多层前馈神经网络)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/feedforward-neural-networks-multi-layers-preceptors-mlps-1bea7ff11e07?source=collection_archive---------5-----------------------#2022-03-25">https://medium.com/mlearning-ai/feedforward-neural-networks-multi-layers-preceptors-mlps-1bea7ff11e07?source=collection_archive---------5-----------------------#2022-03-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="a1e8" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">深度学习技术的一些关键概念的基本概述。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/ea89c3f5ab56c51d92a81029b5336d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LgGpZW_XWv7L3l0N.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Feed-Forward Neural Network</figcaption></figure><p id="a190" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated"><em class="ij">先决条件:微分分析、线性代数、最优化问题</em></p><p id="70d5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">让我们在高层次上定义什么是学习算法:</p><blockquote class="ie if ig"><p id="1abf" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated"><strong class="ik hi">如果由P测量的算法在任务T中的性能随着经验E而提高，则该算法被认为从关于某类任务T和性能P的经验E中学习</strong></p></blockquote><p id="06ea" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">根据上述定义，三个实体被加了下划线:</p><blockquote class="ie if ig"><p id="a417" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">T = {分类，回归，聚类，对象检测，图像识别，问答，语法纠正，…}</p><p id="918e" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">P = {准确率，F-beta评分，交叉熵，准确率-召回率曲线，…}</p><p id="f81f" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">E = {有监督的，无监督的，强化学习，…}</p></blockquote><p id="186a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">上面的定义非常笼统，而且即使允许得到一个大概的想法，也没有说如何建立一个学习算法。</p><blockquote class="jz"><p id="7c97" class="ka kb hh bd kc kd ke kf kg kh ki jf dx translated">从基线模型开始，这可能是有用的，以便强调深度学习模型相对于一般机器学习模型的主要差异。</p></blockquote><h1 id="7455" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">基线示例:线性回归</h1><p id="d84c" class="pw-post-body-paragraph ih ii hh ik b il lh in io ip li ir is jw lj iv iw jx lk iz ja jy ll jd je jf ha bi translated">在线性回归算法中，最终输出给出预测函数，其形式为:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lm"><img src="../Images/0e0b8ec0ec40c88fcd6c5202b4dc7c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/1*BlAxm7-1hGSNx998aVfa8w.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">The functional relationship between the regressor and the regressed is suppose to be linear. Please note that, the linear relationship is related to the weights coefficient, while the value of the input x can be even not linear.</figcaption></figure><p id="8d2e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">该问题的最佳参数通过以解析形式求解以下方程来获得:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ln"><img src="../Images/418286e59b75326256f7a6cf4d372dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/1*hziBKmyeLNO9hwTtagv_vA.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">The gradient, computed as a function of the weights, of the loss function, is imposed to be zero. The weights which solve that equation gives the best (optimal) parameters for the problem.</figcaption></figure><p id="6a76" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">其中损失函数，可以如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lo"><img src="../Images/77c19ea1560266923e7bdfa1db8bb275.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/1*Rr9ZmenDXtlaOg8VoFHBNA.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Root mean square error. It measures the distance between the true target and the predicted ones.</figcaption></figure><p id="6400" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">因为预测函数的线性形式，梯度方程具有权重的解析解，如下所示:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lp"><img src="../Images/f7b27a9d9f66a50395004d13f992ce48.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/1*Nj8eixfULbx928w76QLtwA.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">The optimal weights</figcaption></figure><p id="4088" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">众所周知，所获得的最佳权重是可能的最佳权重，因为如上所述，损失函数是凸函数，因此它的hessian(它的二阶导数)不依赖于权重，它们是常数。这意味着，函数有一个唯一的全局最小值。</p><p id="2e17" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">以上算法称为<strong class="ik hi">线性最小二乘法。</strong></p><p id="d8f9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">当然，并不是所有的问题都可以通过使用线性函数来建模，而是经常观察到非线性，因此上述算法，即使它返回模型参数的最佳解析形式，也不总是可行的。</p><h1 id="7b33" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku lq kw kx ky lr la lb lc ls le lf lg bi translated">深度学习方法:</h1><p id="55aa" class="pw-post-body-paragraph ih ii hh ik b il lh in io ip li ir is jw lj iv iw jx lk iz ja jy ll jd je jf ha bi translated">在DL方法中，最终目标保持不变:获得实函数f的近似函数f*，前馈神经网络(FFN)定义映射</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lt"><img src="../Images/5a7f86c665ab98c227aecfaad8673741.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/1*WRWpYefHleDi7hoBKf_u6A.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Goal: find the best parameters that gives the smallest error between the prediction and the target.</figcaption></figure><p id="2d82" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">然而，映射的最终形式不是强加的，而是由网络本身以自动方式发现的。</p><p id="8376" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">术语<strong class="ik hi">前馈</strong>来源于这样一个概念，即信息流过从x开始计算的函数，通过用于定义f的中间计算。术语<strong class="ik hi">网络</strong>是因为，FFN通常通过组合许多不同的函数来表示。事实上，该模型与描述功能如何组合在一起的<strong class="ik hi">有向无环图(DAG) </strong>相关联。术语<strong class="ik hi">神经</strong>来源于这样一个事实，即最小的模型单元是大脑神经元功能行为的数学表示(前感受器模型)，如神经科学所报道的。</p><blockquote class="ie if ig"><p id="f69b" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">FFN的最终输出具有输入的递归嵌套组合函数的形式:</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lu"><img src="../Images/4d93de9bc84e0538f177e6b638210345.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/1*7_UBfZ6EQ7uhaAypBmjRzw.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx">the final output, is the result of n composed functions, where n is the number of layers of the model.</figcaption></figure><p id="6476" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">最终函数的非线性不是由几个函数的递归嵌套组合给出的，因为，当然，将多个线性函数堆叠在一起只会得到一个线性函数。相反，每个函数本身都是非线性的。非线性由<em class="ij">激活功能</em>给出，如下所述。</p><p id="70ea" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">FNN模型可以描述为一系列函数变换，由输入变量的M个线性组合组成</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lv"><img src="../Images/13c4b7918ed045497cde16946272567a.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*rVQRcoc2GnJgMErlj9t7jQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Inputs data</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lw"><img src="../Images/d10e3d5677095d6526f5c3e7ab5591ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*VatrAL2T6wRnw1ULOCl5dA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">j-ith linear combination of the input variables (hidden unit), for the k-ith layer.</figcaption></figure><p id="33f3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">其中，k是所选层的数量，I从1到M(输入向量的维数)，w_ji是权重，而w_j0是偏差。术语a^k_j是第k层的第j个输入激活。然后，使用可微分的非线性函数来转换每个输入激活，以给出(激活函数):</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lx"><img src="../Images/ebfedf22ee5a987ed68d1936a98091a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:164/format:webp/1*-L3wYYPAol_NMMaa3mLjYA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">non-linear transformed activation of the hidden units j of the k-ith layer.</figcaption></figure><p id="4b76" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">下面，报告了应用中使用的典型非线性激活函数<em class="ij"> h </em>，它是可微分的和非线性的。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ly"><img src="../Images/d38165f8d8b5633fcaec026c413d565d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nnmi7S4Ak5j09ZILCiw6Bg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">A typical activation function used in the applications.</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lz"><img src="../Images/656a00cf34c0f2983c974f0dabe0a747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qPbIrguc0wEmwlAwcAbPw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Linear Algebra and Optimization for Machine Learning: Charu C. Aggarwal</figcaption></figure><p id="06e0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">如上所述，在DL框架中，通过遵循<em class="ij">最小平方</em>的相同数学概念来获得最佳参数。定义了一个损失函数，计算了它相对于模型参数的梯度，然而，与<em class="ij">线性最小二乘法</em>不同的是，损失函数不是凸函数，因此，不能更好地保证唯一全局最小值的存在。</p><h2 id="53e5" class="ma kk hh bd kl mb mc md kp me mf mg kt jw mh mi kx jx mj mk lb jy ml mm lf mn bi translated">随机梯度下降</h2><p id="2e41" class="pw-post-body-paragraph ih ii hh ik b il lh in io ip li ir is jw lj iv iw jx lk iz ja jy ll jd je jf ha bi translated">DL训练过程需要解决一个优化问题。通过将模型权重向损失函数的最小值方向移动来学习模型权重:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mo"><img src="../Images/9605f5dccb5f71a90e44859e8303a56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/0*CMJOeZnDYkPV3FvO.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Loss function, seen as a function of the only weights.</figcaption></figure><p id="e8a7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">现在，程序如下所示。首先，计算损失函数相对于权重变量的梯度。然后，更新权重，以便将损失函数向前移动到更低的值。阿尔法参数称为<em class="ij">学习率</em>，它控制学习过程的速度(这是一个重要参数，需要在训练阶段进行调整)。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mp"><img src="../Images/d5103f9b789989a53e05a78e77c499f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*jrRCi10Yqt6NH7D4.png"/></div></figure><p id="ad54" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">术语<em class="ij">随机</em>来源于这样一个事实，即算法是为一批M个输入值计算的，使得M &lt; N，其中N是输入的总数。对于数量为E的时期，以随机方式从N个元素的总子集提取最小批的M个元素。</p><p id="9394" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jw iu iv iw jx iy iz ja jy jc jd je jf ha bi translated">参考资料:</p><ol class=""><li id="3f63" class="mq mr hh ik b il im ip iq jw ms jx mt jy mu jf mv mw mx my bi translated">古德菲勒、伊恩、约舒阿·本吉奥和亚伦·库维尔。<em class="ij">深度学习</em>。麻省理工学院出版社，2016年。</li><li id="6d4c" class="mq mr hh ik b il mz ip na jw nb jx nc jy nd jf mv mw mx my bi translated">机器学习的线性代数与优化</li></ol><div class="ne nf ez fb ng nh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hi fi z dy nm ea eb nn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv jq nh"/></div></div></a></div></div></div>    
</body>
</html>