<html>
<head>
<title>Understanding Adaline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解Adaline</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/understanding-adaline-da79ab8bbc5a?source=collection_archive---------10-----------------------#2022-01-22">https://medium.com/mlearning-ai/understanding-adaline-da79ab8bbc5a?source=collection_archive---------10-----------------------#2022-01-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="cadd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">供稿人:<a class="ae jc" rel="noopener" href="/@pshunmugapriya">shumgapriya，</a>Sathya Krishnan Suresh<br/>Github链接:<a class="ae jc" href="https://github.com/SathyaKrishnan1211/Medium-Article-Code" rel="noopener ugc nofollow" target="_blank">https://github.com/SathyaKrishnan1211/Medium-Article-Code</a></p><p id="7112" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adaline-是由Widrow教授和他的学生Ted Hoff在1960年开发的单层神经网络。Adaline可以被认为是感知器模型的改进。如果你还没有读过我之前写的感知器模型，在阅读本文之前，请在这里查看一下。</p><p id="da4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">Adaline背后的理论:</strong></p><p id="98f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adaline改进了感知器模型，包括两个非常重要的优化概念，称为<em class="jd">激活函数</em>和<em class="jd">成本函数。</em></p><p id="ccff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活函数是给我们一个值来与阈值函数一起工作的函数。激活函数使用要素的权重来工作。激活函数的一些著名例子是逻辑回归的sigmoid函数、支持向量机的径向基函数等。但是对于一个单层Adaline，我们将使用一个标识函数，它简单地将发送给它的内容返回给我们。我在这里提到了激活函数，因为在大多数监督分类模型中，都会存在激活函数。</p><p id="6bbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">成本函数是每个机器学习算法将努力最小化的函数，以便它可以给出准确的结果。这是Adaline真正偏离感知器的地方，因为感知器中没有成本函数，没有成本函数感知器什么也不会最小化。这就是为什么Adaline被认为是感知器的改进，因为你可以通过简单地查看它的成本函数来了解它是如何改进其预测的。我说过机器学习算法会试图降低成本函数。但是具体怎么做呢？这就是梯度下降的由来。当你谈论成本函数时，必须提到梯度下降，它本身值得一篇单独的文章，但我将尝试在这里给出一个简短的定义。</p><p id="c3c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降:<br/>它是一种寻找微分函数局部极小值的一阶迭代优化算法。看看下面这张图。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/d29b2e98be31b9216c236dc8d9515985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XUP-8UyjW7TmhR6gt10sQ.png"/></div></div></figure><p id="6f18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">权重沿x轴测量，成本函数沿y轴测量。下面给出了成本函数J。好好看看这个成本函数，因为我稍后会指出感知机和Adaline之间的另一个重要区别。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jq"><img src="../Images/5aa174a6a1c37c35de99f8ccf459d32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*pDkyLDGeZgw5F-ywWi0ewA.png"/></div></figure><p id="0654" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降首先找到具有以下形式的成本函数的微分方程</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jr"><img src="../Images/33b9616bbc93de16f5f6eb3441932f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*sgZuc9qbs-QWO9YIIouadQ.png"/></div></figure><p id="9cba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是上图的斜率，将用于更新要素的权重。所以梯度下降的作用类似于下山走向山谷。在这种情况下，谷是成本函数的全局最小值。它试图通过使用斜率、作为超参数的学习率和特征的当前权重来更新特征的权重，从而达到全局最小值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/eaa547d9102a42ba2559015fa3bf6b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*_izGduEd9x9gznFplGi4Xw.png"/></div></figure><p id="3b3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上述两个等式构成Adaline的核心，其中权重使用梯度下降优化技术进行自我更新。</p><p id="94d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以现在我们已经讨论了Adaline使用的三个新概念。现在来说说Adaline的工作流程。</p><p id="0dd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adaline的工作流程:<br/> 1。它不是一次接受一个特征值，而是接受整个特征集。<br/> 2。将其乘以相应的权重，这是由网络输入函数完成的。<br/> 3。将其传递给激活函数。<br/> 4。激活函数返回的值用于更新权重。<br/> 5。当专门调用预测函数时，单位步长函数用于预测标注。</p><p id="c73e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了Adaline算法的工作流程。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jt"><img src="../Images/cd29acb1be73c7492681b25288a1c7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUEeP8thm3aE6mpJ8Qh67g.jpeg"/></div></div></figure><p id="c373" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">Adaline的Python实现:</strong></p><p id="eef7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据上面讨论的概念，让我们尝试为Adaline实现一个python代码。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ju"><img src="../Images/d8cb995b2b12df3caae06fb0556fa2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NougXOlf0wmhzBLoPNbJyg.png"/></div></div></figure><p id="8c35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如前所述，我们有一个激活函数和一个成本函数，这里是成本变量的形式。让我们一个功能一个功能地来。</p><p id="3828" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在“init”函数中，我们得到超参数，如学习率、随机状态和迭代次数。随机状态在模型的预测准确性中不发挥主要作用，但是其他两个起着非常重要的作用，尤其是学习率。学习率起着重要的作用，因为它是我们的模型在迭代过程中努力最小化的成本函数的一部分。如果你没有为学习率设置一个合适的值，就会有超调的危险，或者模型永远达不到全局最小值。</p><p id="ccee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在‘net _ input’函数中，我们采用输入特征。特征向量和相应的权重相乘。请记住，权重向量的第一个值(索引0)始终是一个偏差单位。</p><p id="005c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在激活函数中，我们简单地返回“net_input”函数的返回值。正如我前面说过的，激活函数在这里没有任何作用，但是它为理解分类模型的工作流程奠定了基础，我们将在以后的文章中讨论。</p><p id="141a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“合身”功能是神奇之处。首先，我们设置权重，为了实现成本函数，我们将成本变量分配给一个空列表。这是另一点，你应该注意到感知器和Adaline之间的区别。感知器更新每行数据的权重，而Adaline接受整个数据并更新整个数据的权重。它不会更新每一行的权重。还记得我让你们仔细看看成本函数的那一点吗，这是我解释原因的地方。在感知器模型中，预测值是<strong class="ig hi">离散类别标签</strong>，但是在adaline中，由于使用了激活函数，与真实标签相比的<strong class="ig hi">值是连续值。</strong>是的，这是所有奇迹发生的地方。因为将类标签与连续值进行比较，所以误差到处都是，并且它们在迭代开始时受到严重惩罚，并且随着它们向迭代结束前进，误差将开始减少。</p><p id="b6b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了强调为学习率设置适当值的重要性，我将在没有“viriginica”的“Iris”数据集上实现两个Adaline模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/d7fa3d620fafd504343d1da901338e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O4ZW22Qv2vHIGlKhoNGJHg.png"/></div></div></figure><p id="6522" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个模型的学习率为0.01，而第二个模型的学习率为0.0001。让我们看看他们的成本函数值的日志。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jw"><img src="../Images/7e1228669e9b3c4cd2eb0397ad20155f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4ScBRltfDfjw9HQPp6PBA.png"/></div></div></figure><p id="9c27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从第一个模型中，我们可以看到，对于学习率的大值，该模型简单地超调，并且将永远无法达到全局最小值。对于较大的学习率，减少迭代次数将产生更好的结果。从第二个模型，我们可以推断，对于小的学习率，梯度下降将永远达到全局最小值。对于小值，建议增加迭代次数，但这在计算上将是昂贵的。因此，总有一个人必须记住的权衡。</p><p id="feff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong></p><p id="5126" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Adaline是另一个单层神经网络，其高级版本在现实生活中有很多应用。下一篇文章将讨论逻辑回归。希望你和我写这篇文章时一样开心。</p><div class="jx jy ez fb jz ka"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kb ab dw"><div class="kc ab kd cl cj ke"><h2 class="bd hi fi z dy kf ea eb kg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kh l"><h3 class="bd b fi z dy kf ea eb kg ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ki l"><p class="bd b fp z dy kf ea eb kg ed ef dx translated">medium.com</p></div></div><div class="kj l"><div class="kk l kl km kn kj ko jo ka"/></div></div></a></div></div></div>    
</body>
</html>