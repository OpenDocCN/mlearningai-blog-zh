<html>
<head>
<title>How to build a Web Scraper for LinkedIn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为LinkedIn建立一个网页抓取器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/how-to-build-a-web-scraper-for-linkedin-6b49b6b6adfc?source=collection_archive---------0-----------------------#2021-08-09">https://medium.com/mlearning-ai/how-to-build-a-web-scraper-for-linkedin-6b49b6b6adfc?source=collection_archive---------0-----------------------#2021-08-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5133bae5994d3c8e1d9f0bd580212f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04LoNGM0rOUCfb82LZOlUg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@gregbulla" rel="noopener ugc nofollow" target="_blank">Greg Bulla</a> on <a class="ae it" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Upslash</a></figcaption></figure><p id="0356" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">构建机器学习算法来获取和分析Linkedin数据是ML爱好者中的一个流行想法。但每个人都遇到的一个障碍是缺乏数据，考虑到从Linkedin收集数据是多么乏味以及这一切背后的合法性。</p><p id="9759" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">美国第九巡回上诉法院裁定，CFAA不禁止公司收集互联网上公开的数据，尽管LinkedIn声称这侵犯了用户隐私。</p><p id="139d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">总部位于旧金山的初创企业hiQ Labs从LinkedIn收集用户资料，并利用这些资料分析劳动力数据，例如预测员工何时可能离职，或者哪里可能出现技能短缺。</p><p id="8181" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在LinkedIn采取措施阻止hiQ这么做后，hiQ在两年前赢得了强制令，迫使微软所有的公司取消了封锁。该禁令现已被美国第九巡回上诉法院以3比0的裁决维持。</p><p id="047c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将讨论如何使用Selenium和Beautiful Soup构建并自动化您自己的Web抓取工具，从任何Linkedin个人资料中提取数据。这是一个分步指南，每个步骤都有完整的代码片段。对于那些想要完整代码的人，我在文章末尾添加了GitHub资源库链接。</p><h1 id="b001" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">要求</h1><ul class=""><li id="6d2f" class="kq kr hh iw b ix ks jb kt jf ku jj kv jn kw jr kx ky kz la bi translated">python(显然。3+推荐)</li><li id="e8dc" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">美汤(美汤是一个库，可以很容易的从网页中抓取信息。)</li><li id="d903" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">selenium(selenium包用于自动化Python中的web浏览器交互。)</li><li id="bf11" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">一个网络驱动，我用过Chrome的网络驱动。</li><li id="f9db" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">此外，您还需要pandas、time和regex库。</li><li id="3e1b" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">一个代码编辑器，我用的是Jupyter Notebook，你可以用Vscode/Atom/Sublime或者任何你选择的。</li></ul><blockquote class="lg lh li"><p id="1907" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated"><em class="hh">使用</em> <code class="du ln lo lp lq b"><em class="hh">pip install selenium</em></code> <em class="hh">安装硒库。</em></p><p id="82ad" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated"><em class="hh">使用</em> <code class="du ln lo lp lq b"><em class="hh">pip install beautifulsoup4</em></code> <em class="hh">安装美汤库。</em></p><p id="6e5e" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated"><em class="hh">你可以从这里下载Chrome网络驱动:</em></p><p id="bcc2" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated"><a class="ae it" href="https://chromedriver.chromium.org/" rel="noopener ugc nofollow" target="_blank"><em class="hh">https://chromedriver.chromium.org/</em></a></p></blockquote><h1 id="e18b" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">概观</h1><p id="78e9" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">这里是本文涵盖的所有主题的完整列表。</p><ul class=""><li id="96a6" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">如何使用Selenium实现Linkedin自动化？</li><li id="7246" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">如何使用BeautifulSoup从给定的Linkedin个人资料中提取帖子及其作者。</li><li id="5325" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">将数据写入. csv或。xlsx文件可供以后使用。</li><li id="5e11" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">如何自动处理一次获得多个作者的帖子。</li></ul><h1 id="47ed" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">说完这些，让我们开始吧</h1><ul class=""><li id="4b05" class="kq kr hh iw b ix ks jb kt jf ku jj kv jn kw jr kx ky kz la bi translated">我们将从进口我们需要的一切开始。</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="f536" class="mf jt hh lq b fi mg mh l mi mj">from selenium import webdriver<br/>from selenium.webdriver.common.keys import Keys<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/>from selenium import webdriver<br/>from bs4 import BeautifulSoup as bs<br/>import re as re<br/>import time<br/>import pandas as pd</span></pre><ul class=""><li id="85b3" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">除了selenium和beautiful soup之外，我们还将使用regex库来获取帖子的作者，使用time库来使用sleep和pandas等时间功能来处理大规模数据，以及写入电子表格。马上会有更多的报道！</li><li id="01af" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">Selenium需要一些东西来启动自动化过程。系统中web驱动程序的位置、用户名和登录密码。因此，让我们从获取它们开始，并分别将它们存储在变量PATH、USERNAME和PASSWORD中。</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="6cc5" class="mf jt hh lq b fi mg mh l mi mj">PATH = input("Enter the Webdriver path: ")<br/>USERNAME = input("Enter the username: ")<br/>PASSWORD = input("Enter the password: ")<br/>print(PATH)<br/>print(USERNAME)<br/>print(PASSWORD)</span></pre><ul class=""><li id="68c7" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">现在我们将在一个变量中初始化我们的web驱动程序，Selenium将用它来执行所有的操作。姑且称之为司机吧。我们告诉它网络驱动的位置，即路径</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="d07a" class="mf jt hh lq b fi mg mh l mi mj">driver = webdriver.Chrome(PATH)</span></pre><ul class=""><li id="20e8" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">接下来，我们将告诉我们的驱动程序它应该获取的链接。在我们的例子中，它是Linkedin的主页。</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="19a4" class="mf jt hh lq b fi mg mh l mi mj">driver.get("<a class="ae it" href="https://www.linkedin.com/uas/login" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/uas/login</a>")<br/>time.sleep(3)</span></pre><blockquote class="lg lh li"><p id="ce37" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated">你会注意到，我在上面的代码片段中使用了睡眠功能。你会发现它在本文中用得很多。sleep函数基本上将任何进程(在我们的例子中是自动化进程)暂停指定的秒数。如果你需要绕过验证码验证，你可以随意使用它在任何你喜欢的地方暂停这个过程。</p></blockquote><ul class=""><li id="ed4a" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">我们现在将告诉驱动程序使用我们提供的凭据登录。</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="39f3" class="mf jt hh lq b fi mg mh l mi mj">email=driver.find_element_by_id("username")<br/>email.send_keys(USERNAME)<br/>password=driver.find_element_by_id("password")<br/>password.send_keys(PASSWORD)<br/>time.sleep(3)<br/>password.send_keys(Keys.RETURN)</span></pre><ul class=""><li id="e8be" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">现在让我们创建几个列表来存储数据，如个人资料链接、文章内容和每篇文章的作者。我们将分别称它们为post_links、post_texts和post_names。</li><li id="13e1" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">一旦完成，我们将开始实际的网页抓取过程。让我们声明一个函数，这样我们就可以使用我们的web抓取代码，以递归方式从多个帐户获取帖子。我们就叫它Scrape_func吧。</li></ul><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="d67c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">好吧，这个函数很长。不要担心！我会一步步解释。让我们先来看看我们的函数是做什么的。</p><ul class=""><li id="8775" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">它有三个参数，即post_links、post_texts和post_names，分别为a、b和c。</li><li id="5222" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">现在我们将进入函数的内部工作。它首先获取概要文件链接，并截取概要文件名称。</li><li id="c9df" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">现在，我们使用驱动程序来获取用户配置文件的“文章”部分。驱动程序滚动浏览帖子，使用beautiful soup收集帖子的数据，并将其存储在“容器”中。</li><li id="c66f" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">上述代码的第17行规定了驱动程序收集帖子的时间。在我们的例子中，它是20秒，但是您可以更改它以适应您的数据需求。</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="dce7" class="mf jt hh lq b fi mg mh l mi mj">if round(end-start)&gt;20:        <br/>    breakexcept:<br/>    pass</span></pre><ul class=""><li id="b4d4" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">我们还从每个帐户获取用户想要的帖子数量，并将其存储在变量“nos”中。</li><li id="f96f" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">最后，我们遍历每个“容器”，获取存储在其中的文章数据，并将其与post_names一起添加到post_texts列表中。当达到期望的帖子数量时，我们会中断循环。</li></ul><blockquote class="lg lh li"><p id="1e10" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated"><em class="hh">你会注意到，我们在一个try-catch块中包含了容器迭代循环。这是针对可能出现的异常的安全措施。</em></p></blockquote><ul class=""><li id="d19a" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">这就是我们的功能！现在，是时候使用我们的函数了！我们从用户那里获得一个配置文件列表，并以递归方式将其发送给函数，以重复所有帐户的数据收集过程。</li><li id="e571" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">该函数返回两个列表:包含所有帖子数据的post_texts和包含所有相应帖子作者的post_names。</li><li id="bab9" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">现在我们已经到了自动化最重要的部分:保存数据！</li></ul><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="ce3d" class="mf jt hh lq b fi mg mh l mi mj">data = {<br/>    "Name": post_names,<br/>    "Content": post_texts,<br/>}df = pd.DataFrame(data)<br/>df.to_csv("gtesting2.csv", encoding='utf-8', index=False)writer = pd.ExcelWriter("gtesting2.xlsx", engine='xlsxwriter')<br/>df.to_excel(writer, index =False)<br/>writer.save()</span></pre><ul class=""><li id="78f9" class="kq kr hh iw b ix iy jb jc jf lu jj lv jn lw jr kx ky kz la bi translated">我们用该函数返回的列表创建一个字典，并使用pandas将它保存到一个变量' df '中。</li><li id="a3d0" class="kq kr hh iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">您可以选择将收集的数据保存为. csv文件或. xlsx文件。</li></ul><h1 id="b48c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">对于csv:</h1><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="c9e2" class="mf jt hh lq b fi mg mh l mi mj">df.to_csv("test1.csv", encoding='utf-8', index=False)</span></pre><h1 id="8852" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">对于xlsx:</h1><pre class="lx ly lz ma fd mb lq mc md aw me bi"><span id="8bec" class="mf jt hh lq b fi mg mh l mi mj">writer = pd.ExcelWriter("test1.xlsx", engine='xlsxwriter')<br/>df.to_excel(writer, index =False)<br/>writer.save()</span></pre><blockquote class="lg lh li"><p id="4b12" class="iu iv lj iw b ix iy iz ja jb jc jd je lk jg jh ji ll jk jl jm lm jo jp jq jr ha bi translated">在上面的代码片段中，我给出了一个示例文件名‘text 1’。你可以给任何你选择的文件名！</p></blockquote><h1 id="82ca" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论:</h1><p id="5b75" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf lr jh ji jj ls jl jm jn lt jp jq jr ha bi translated">唷！那是很长的时间，但是我们已经成功地创建了一个全自动的网络抓取器，它可以让你在一瞬间获得Linkedin帖子的数据！希望这对你有帮助！以后一定要关注更多这样的文章！下面是GitHub上完整代码的链接:【https://github.com/AhmedKNegm/Scrape_func<a class="ae it" href="https://github.com/AhmedKNegm/Scrape_func" rel="noopener ugc nofollow" target="_blank"/></p><p id="1873" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">谢谢你的来访！快乐学习！</p></div></div>    
</body>
</html>