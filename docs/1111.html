<html>
<head>
<title>My First NLP Problem on Kaggle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我在Kaggle上的第一个NLP问题</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/my-first-nlp-problem-on-kaggle-d12904239e04?source=collection_archive---------2-----------------------#2021-10-04">https://medium.com/mlearning-ai/my-first-nlp-problem-on-kaggle-d12904239e04?source=collection_archive---------2-----------------------#2021-10-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0d2d6b12c94633e495b40ac1ec446e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tI-TWV--K05xbXUgA4Qm1w.png"/></div></div></figure><p id="e478" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">NLP专注于分析语音数据，以生成单词预测或从语音数据中生成见解，如情感。我在Kaggle上处理的第一个NLP问题是猫途鹰网站评论的评级预测系统，我们从用户给出的评论中预测可能的评级。尽管这是一个简单的问题，但这个数据集让我开始学习NLP。</p><p id="5845" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">数据加载</strong></p><p id="1547" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据集是一个CSV文件，以<strong class="ir hi">评论</strong>和<strong class="ir hi">评级</strong>为列。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="9a19" class="jw jx hh js b fi jy jz l ka kb">data = pd.read_csv(“../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv”)</span><span id="2fe8" class="jw jx hh js b fi kc jz l ka kb">data.head(5)<br/>data.isna().sum()</span></pre><figure class="jn jo jp jq fd ii er es paragraph-image"><div class="er es kd"><img src="../Images/57489bc49a6d8b1f7bedd5867dc96eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*nVSq13hnk2DBMGJkCwz01w.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">Display of the Dataset</figcaption></figure><p id="f26a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据集是干净的，没有缺失值。</p><p id="4cd6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> EDA </strong></p><p id="4b50" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里要执行的EDA是每个评论的评分分布和情绪分布。为了产生情感，我使用了TextBlob库。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="45b5" class="jw jx hh js b fi jy jz l ka kb">sentiments = []</span><span id="b27a" class="jw jx hh js b fi kc jz l ka kb">for review <strong class="js hi">in</strong> data['Review']:</span><span id="9bbb" class="jw jx hh js b fi kc jz l ka kb">   if TextBlob(review).sentiment.polarity &lt; 0:</span><span id="ed84" class="jw jx hh js b fi kc jz l ka kb">      sentiments.append("Negative")</span><span id="ed56" class="jw jx hh js b fi kc jz l ka kb">   if TextBlob(review).sentiment.polarity == 0:</span><span id="3291" class="jw jx hh js b fi kc jz l ka kb">      sentiments.append("Neutral")</span><span id="a0c0" class="jw jx hh js b fi kc jz l ka kb">   if TextBlob(review).sentiment.polarity &gt; 0:</span><span id="dd1b" class="jw jx hh js b fi kc jz l ka kb">      sentiments.append("Positive")</span><span id="6a8a" class="jw jx hh js b fi kc jz l ka kb">data["Sentiment"] = np.array(sentiments)</span></pre><p id="eac3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们现在生成可视化。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="648c" class="jw jx hh js b fi jy jz l ka kb">plt.figure(figsize=(15,5))</span><span id="9265" class="jw jx hh js b fi kc jz l ka kb">plt.subplot(1,2,1)</span><span id="ccb8" class="jw jx hh js b fi kc jz l ka kb">x_axis,counts = np.unique(data['Rating'],return_counts=True)</span><span id="950b" class="jw jx hh js b fi kc jz l ka kb">plt.bar([str(i) for i <strong class="js hi">in</strong> x_axis],counts)</span><span id="58b1" class="jw jx hh js b fi kc jz l ka kb">plt.title("Rating vs Counts")</span><span id="d06d" class="jw jx hh js b fi kc jz l ka kb">plt.xlabel("Rating")</span><span id="bfb0" class="jw jx hh js b fi kc jz l ka kb">plt.ylabel("Count")</span><span id="5e17" class="jw jx hh js b fi kc jz l ka kb">plt.subplot(1,2,2)</span><span id="2bb1" class="jw jx hh js b fi kc jz l ka kb">x_axis,counts = np.unique(data['Sentiment'],return_counts=True)</span><span id="8c1a" class="jw jx hh js b fi kc jz l ka kb">plt.bar(x_axis,counts)</span><span id="50ae" class="jw jx hh js b fi kc jz l ka kb">plt.title("Sentiment vs Counts")</span><span id="12fd" class="jw jx hh js b fi kc jz l ka kb">plt.xlabel("Sentiment")</span><span id="9584" class="jw jx hh js b fi kc jz l ka kb">plt.ylabel("Count")</span><span id="4424" class="jw jx hh js b fi kc jz l ka kb">plt.tight_layout()</span></pre><figure class="jn jo jp jq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/123b541d5066cb46bff4ee012ef34766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqL7ILPGGAUk8Umr2mDQiw.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx">EDA of the Dataset</figcaption></figure><p id="7e6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从可视化中可以明显看出，等级1、2和3的数量相似，等级4、5的数量相似，但高于1、2和3。此外，大多数评论都是正面的，很少有负面评论，中性评论更少。</p><p id="3bfb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">自然语言处理和建模</strong></p><p id="e7ad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在有趣的部分来了，NLP。这里是文本数据，需要转换成向量，以便算法处理它。为此，我们使用tfid vectorizer——NLP最常用的矢量化算法。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="97f4" class="jw jx hh js b fi jy jz l ka kb">vectorizer = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS)</span></pre><p id="0c08" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们使用ENGLISH_STOP_WORDS参数来移除停用词，因为这些停用词在上下文中是不相关的，并且在任何语音/文本数据中占据大量。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="0cb5" class="jw jx hh js b fi jy jz l ka kb">X_train,X_test,Y_train,Y_test = train_test_split(vectorizer.fit_transform(data['Review']).toarray(),</span><span id="066d" class="jw jx hh js b fi kc jz l ka kb">data['Rating'].values,</span><span id="157b" class="jw jx hh js b fi kc jz l ka kb">test_size = 0.2,</span><span id="3981" class="jw jx hh js b fi kc jz l ka kb">random_state=42)</span></pre><p id="b29c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们现在将把数据分成训练和测试用于建模，但是作为特征，我们将被传递转换的矢量数据而不是文本数据，并且标签将是评级。</p><p id="5bb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于建模，我们将使用三种模型——逻辑回归、随机森林和决策树。</p><p id="607c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述三种模型的训练和测试精度如下:</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="6748" class="jw jx hh js b fi jy jz l ka kb"><strong class="js hi">Logistic Regression :</strong></span><span id="b647" class="jw jx hh js b fi kc jz l ka kb">Test Accuracy  : 62.11 %</span><span id="112a" class="jw jx hh js b fi kc jz l ka kb">Train Accuracy : 76.15 %</span><span id="0c01" class="jw jx hh js b fi kc jz l ka kb"><strong class="js hi">Decision Tree :</strong></span><span id="0219" class="jw jx hh js b fi kc jz l ka kb">Train Accuracy : 100.00 %</span><span id="57f5" class="jw jx hh js b fi kc jz l ka kb">Test Accuracy  : 45.72 %</span><span id="8f3d" class="jw jx hh js b fi kc jz l ka kb"><strong class="js hi">Random Forest Classifier :</strong></span><span id="95e9" class="jw jx hh js b fi kc jz l ka kb">Train Accuracy : 100.00 %</span><span id="1cb6" class="jw jx hh js b fi kc jz l ka kb">Test Accuracy  : 51.55 %</span></pre><p id="0ad8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上所述，树模型在预测评级方面表现不佳。一个看似合理的原因是对每种情绪给出的评级不一致——一个负面评级可能在一个评论中得到3分，而在另一个评论中可能会更少。类似地，每次评论中使用的不同单词及其相应的评级也会影响预测能力。一种解决方案是基于评级的分布对评级进行分组，即评级1、2和3在单个组中(组0)，而4、5具有更接近的分布，但高于1、2、3，因此在组1中。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="388b" class="jw jx hh js b fi jy jz l ka kb">groups = []</span><span id="5339" class="jw jx hh js b fi kc jz l ka kb">for rating <strong class="js hi">in</strong> data['Rating']:</span><span id="911e" class="jw jx hh js b fi kc jz l ka kb">    if rating <strong class="js hi">in</strong> [1,2,3]:</span><span id="9bfa" class="jw jx hh js b fi kc jz l ka kb">       groups.append(0)</span><span id="b3a3" class="jw jx hh js b fi kc jz l ka kb">    else:</span><span id="750f" class="jw jx hh js b fi kc jz l ka kb">       groups.append(1)</span><span id="c88b" class="jw jx hh js b fi kc jz l ka kb">data['Group'] = groups</span></pre><p id="a260" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，问题从评级预测转变为最大评级预测——我们不是预测准确的评级，而是预测该评论的最大可能评级。</p><p id="44cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，执行矢量化和分割，但我们使用分组作为特征，而不是评级作为特征。</p><pre class="jn jo jp jq fd jr js jt ju aw jv bi"><span id="6c0d" class="jw jx hh js b fi jy jz l ka kb">vectorizer = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS)</span><span id="44f3" class="jw jx hh js b fi kc jz l ka kb">X_train,X_test,Y_train,Y_test = train_test_split(vectorizer.fit_transform(data['Review']).toarray(),                 data['Group'].values,<br/>test_size = 0.2,<br/>random_state=42)</span></pre><p id="a63d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们尝试对数据建模。通过简单的逻辑回归，我们实现了90%的测试准确率和93%的训练准确率。</p><figure class="jn jo jp jq fd ii er es paragraph-image"><div class="er es kj"><img src="../Images/2b0365c26fc81fc21a66532d3baa663e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*_w26yO92ykBVr12j4LKQog.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">Final Metrics from a Logistic Regression Model</figcaption></figure><p id="a082" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果模型预测为第0组，则评论的最高评级为3，如果预测为第1组，则评论的最高评级为4 / 5。随着对不同模型的更多优化和试验，精确度可以提高。</p></div><div class="ab cl kk kl go km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="ha hb hc hd he"><p id="1032" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的阅读！欢迎大家来叉，改进我的工作。</p><h2 id="c1b4" class="jw jx hh bd kr ks kt ku kv kw kx ky kz ja la lb lc je ld le lf ji lg lh li lj bi translated"><strong class="ak">有用链接:</strong></h2><ul class=""><li id="83e1" class="lk ll hh ir b is lm iw ln ja lo je lp ji lq jm lr ls lt lu bi translated">在LinkedIn上找我:【https://linkedin.com/in/vishnuu0399 T2】</li><li id="4bae" class="lk ll hh ir b is lw iw lx ja ly je lz ji ma jm lr ls lt lu bi translated">更了解我:【https://bit.ly/vishnu-u】T4</li><li id="9f60" class="lk ll hh ir b is lw iw lx ja ly je lz ji ma jm lr ls lt lu bi translated">在Kaggle上找到我的笔记本:<a class="ae lv" href="https://www.kaggle.com/vishnu0399/trip-advisor-reviews-eda-and-nlp-90" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Vishnu 0399/trip-advisor-reviews-EDA-and-NLP-90</a></li></ul></div></div>    
</body>
</html>