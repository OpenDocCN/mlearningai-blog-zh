<html>
<head>
<title>Paper Summary — Searching for TrioNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要——寻找TrioNet</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/paper-summary-searching-for-trionet-6ec2d3b56593?source=collection_archive---------4-----------------------#2022-06-29">https://medium.com/mlearning-ai/paper-summary-searching-for-trionet-6ec2d3b56593?source=collection_archive---------4-----------------------#2022-06-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="666b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我将对论文<strong class="ig hi">寻找TrioNet:结合卷积与局部和全局自我关注</strong>进行简单总结<em class="jc">。</em>在本文中，作者提出了一种新的结构，它是卷积、局部和全局自我注意的结合。除此之外，他们还引入了用于神经结构搜索的分层采样技术和用于自我关注的多头共享技术。</p><blockquote class="jd je jf"><p id="d203" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">搜索TrioNet论文— </strong> <a class="ae jj" href="https://arxiv.org/abs/2111.07547" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">链接</strong> </a></p><p id="ac20" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><strong class="ig hi">注意</strong>——“任何从纸上复制的内容都将是斜体并加引号”</p></blockquote><h1 id="56d0" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">摘要</h1><p id="afac" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">如今，CNN架构正在使用神经架构搜索或NAS进行采样，其中自关注视觉模型是通过堆叠多层手工制作的。卷积和自我关注相结合的建筑空间很少被探索。作者使用权重共享NAS算法探索了这一领域。<em class="jc">结果架构被命名为</em><strong class="ig hi"><em class="jc">TrioNet——卷积、局部自关注和全局(轴向)自关注的组合</em> </strong> <em class="jc">。</em>除此之外，作者还提出了一种新颖的<strong class="ig hi"> <em class="jc">权重分担策略</em></strong><strong class="ig hi"><em class="jc">多头自我关注算子</em> </strong>。该模型优于基于CNN的模型，在ImageNet分类数据上具有更少的FLOPs。TrioNet能够在较小的数据集上匹配CNN模型的性能，而其他自我关注模型无法达到CNN模型。</p><h1 id="d1ab" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">介绍</h1><p id="c514" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">大多数模型都是建立在卷积上的。但是最近我们已经看到自我关注模型优于或者至少与基于CNN的分类/分割模型相当。但是，这些自我关注模型是人为设计的，因此在新的数据集或任务上实现最佳结果具有挑战性。</p><p id="3fc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">神经架构搜索</strong>或NAS是一种有效的技术，可以用最少的人力自动找到想要的架构。简单来说，NAS将目标数据集、任务和计算预算作为输入。NAS已经成功地应用于诸如对象检测、视频理解和语义分割等任务。高效网络体系结构是NAS的成果！要了解有关NAS的更多信息，请阅读此博客— <a class="ae jj" rel="noopener" href="/digital-catapult/neural-architecture-search-the-foundations-a6cc85f7562">媒体链接</a>。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/ceddc10fe3e6ee8713d15d1efc8dfd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wb2sU4UE59Tcmt1wfJhIQA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Figure 1 (Source — Paper)</figcaption></figure><blockquote class="jd je jf"><p id="dbb7" class="ie if jc ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated"><em class="hh">图1 : A和D是搜索到的TrioNet。x1/2，x1/4，x1/8是膨胀率。这种架构在较低层使用convs，在较高层使用self-attention。</em></p></blockquote><p id="8181" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在NAS中，通常，自我注意算子从未被认为是诸如卷积之类的实际运算。作者发现，在权重共享n as方法中使用自我关注作为操作是困难的，因为自我关注具有更宽的搜索空间，包括查询、键、值、空间范围和多头数，而CNN在搜索空间中具有核大小和宽度。</p><p id="7897" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者提出<strong class="ig hi">分层抽样</strong>，保证每个操作者(Conv，注意)在超网中得到平等的训练机会。</p><p id="d327" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一个问题是当前NAS算法的权重共享策略。当前算法“<em class="jc">”共享全权重矩阵的前几个通道来构建小模型的权重。然而，在自我关注中，通道被分成多头组以捕捉不同的依赖性。当前的权重共享策略忽略了权重中的多头结构，并将相同的通道分配给不同的头，迫使相同的通道同时捕获不同类型的依赖性”。</em>为了克服这一点，作者提出了<em class="jc"> </em> <strong class="ig hi"> <em class="jc">多头共享</em> </strong> <em class="jc">的策略。</em></p><p id="e7cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是作者的四个贡献，包括<strong class="ig hi"> TrioNet架构</strong>。</p><h1 id="0b2e" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">方法</h1><h2 id="eb0b" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">操作员级搜索空间</h2><p id="10d7" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">在本文中，作者将自关注算子引入到NAS的算子级空间中。作者使用轴向注意，而不是完全连接的2D自我注意。局部自我关注在搜索空间中被用作“<em class="jc">不清楚每个阶段应该使用多少个局部自我关注层或者如何为每个层选择窗口大小</em>”。</p><h2 id="c3f1" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">架构级搜索空间</h2><p id="af9c" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">作者使用类似ResNet的模型来构建他们的体系结构，使用NAS中的搜索空间。它们用包含conv、局部和全局/轴向自关注的算子级搜索空间替换所有3x3 convs。</p><h2 id="3c98" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">搜索管道</h2><p id="2fb9" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">为了搜索模型，作者使用一次性NAS管道。</p><h2 id="349d" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">分层抽样</h2><p id="6a46" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">对于每个块，首先均匀地采样空间算子，然后采样来自算子空间的候选算子，因为发现搜索空间高度偏向局部注意算子。<strong class="ig hi">由于搜索空间充满了中型模型，在为每个块选择操作符后，使用三明治规则</strong>。</p><h2 id="aa39" class="ld jl hh bd jm le lf lg jq lh li lj ju ip lk ll jy it lm ln kc ix lo lp kg lq bi translated">多头共享</h2><p id="3376" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">我发现论文中的描述很容易理解，因此我引用了论文中的内容。"</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lr"><img src="../Images/5d2070ddb18ee4524c3b32a0abd61f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2A8hLzR95z9eYVSAgN8Ppg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Source — Paper</figcaption></figure><p id="964a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">参考上图，作者考虑到多头结构，首先将所有输出通道分成若干头组。然后，只有在多头自关注中属于同一个头的情况下，它们才共享信道权重。</em></p><h1 id="88d5" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ls"><img src="../Images/f2ba07cbf157ea3dd990f2006e3fab0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owi2BO2sZH2JsIDBXYTDUQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Source — Paper</figcaption></figure><p id="f3c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从这些结果中，我们可以看到，与以前的SOTA模型相比，TrioNet的表现更好。与其他SOTA模型相比，TrioNets需要较少的失败次数。</p><h1 id="c3c5" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">收场白</h1><p id="f28c" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">因此，我们已经看到了Convs和自我关注层的组合如何比仅Convs或仅自我关注模型执行得更好，同时需要更少的FLOPs。</p><p id="cdd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">据信，类似变压器的架构迟早也会投入生产，并将取代CNN模型。类似Transformer的模型仍然存在一些问题，例如它们需要大量的数据，并且它们不是等变翻译。</p><p id="81f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我强烈建议读一遍这篇论文，以便对所做的实验有一个清晰的了解。</p><p id="2f0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢阅读:)。一定要跟着我在媒体上获得这样的论文摘要。</p><p id="f0ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也在<a class="ae jj" href="https://www.linkedin.com/in/sahil-chachra/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我:d。</p><div class="lt lu ez fb lv lw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lx ab dw"><div class="ly ab lz cl cj ma"><h2 class="bd hi fi z dy mb ea eb mc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="md l"><h3 class="bd b fi z dy mb ea eb mc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="me l"><p class="bd b fp z dy mb ea eb mc ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kx lw"/></div></div></a></div></div></div>    
</body>
</html>