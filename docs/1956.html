<html>
<head>
<title>Gradient Boosting Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进机器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/gradient-boosting-machines-f13e06e60efb?source=collection_archive---------5-----------------------#2022-02-15">https://medium.com/mlearning-ai/gradient-boosting-machines-f13e06e60efb?source=collection_archive---------5-----------------------#2022-02-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7b0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在机器学习应用中，我们会遇到许多不同的算法。这些算法中的每一种都在某些特定领域实现了一定程度的准确性。但是可能存在这些精度值不够的情况。在这一点上，我们可能会寻找一些“<strong class="ig hi">算法独立的方法</strong>，以提高我们单一模型的性能。</p><p id="0865" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中一种方法叫做<strong class="ig hi">助推</strong>。Boosting是将弱学习者组合成性能更好的集成模型的方法。boosting的主要思想是使用“<strong class="ig hi">最具信息量的数据</strong>”来训练每一个弱学习者。尽管有很多方法与“增强”相关，我们将在这篇文章中研究“梯度增强”的概念。</p><p id="00f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度提升机器大多使用决策树作为弱学习器。因此，它们也被称为梯度推进决策树。让我们检查下图，以更好地展示梯度推进的概念。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/f075619c7bb6b0443cc54987f73122f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*0KqYN--E0iWg50EAoQdc1A.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Gradient Boosting Decision Trees [1]</figcaption></figure><p id="b096" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在图中，我们看到了<strong class="ig hi"> N </strong>个决策树。在这种情况下，每棵树都可以被认为是“弱学习者”。如果我们仔细观察每棵树的输入/输出对，我们会看到输入X对于每一个弱学习者都是常数。然而，训练的期望输出在每次迭代时都是变化的，从“y”开始，到r(N-1)结束。我们称这些“r”值为先前弱学习者的“残差”或“梯度”。在这里，它被表示为从期望输出中减去实际输出。但是，事情可能比这更复杂。下面，我们可以看到残差计算的一个稍微复杂一点的表达式。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jo"><img src="../Images/36513d11c706f3e7f7ed827ecc8984e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*lenyR39l_rmZ8KBXgYkJCA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Residual Calculation</figcaption></figure><p id="b660" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，<strong class="ig hi"> x(i) </strong>是具有索引I的输入样本，并且<strong class="ig hi"> y(i) </strong>是相同索引值的相应期望输出。<strong class="ig hi">“L”</strong>是可微的(这很重要)损失函数，而<strong class="ig hi"> F(xi) </strong>是迭代“<strong class="ig hi"> m </strong>”时的当前“集合”模型输出。所以，如果我们总结这个表达式；</p><ul class=""><li id="491f" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">在迭代“<strong class="ig hi"> m </strong>”中训练一个新的弱学习者。</li><li id="7e72" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">计算输入<strong class="ig hi"> x(i) </strong>，<strong class="ig hi"> </strong>的集合输出，记为F(xi)。</li><li id="73b9" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">计算在迭代<strong class="ig hi"> m </strong>时<strong class="ig hi"> F(xi) </strong>和<strong class="ig hi"> </strong>期望输出之间的损失。</li><li id="7ab5" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">并且简单地取这个损失相对于当前集合模型输出的梯度，<strong class="ig hi"> F(xi) </strong>。</li><li id="1aad" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">结果值的负数就是<strong class="ig hi">的余数</strong>！现在，我们准备在迭代<strong class="ig hi"> (m+1) </strong>中将这些新的残差馈送给新的弱学习者。</li><li id="e36d" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">在每次迭代结束时，集合模型输出更新如下:</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kd"><img src="../Images/85e7777422aab4e50e5032416c46faf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*PPNfCnZ3Qy6FyFhQWmlKTg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Update the Ensemble Model Output</figcaption></figure><ul class=""><li id="84da" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated"><strong class="ig hi"> h(x) </strong>是迭代<strong class="ig hi"> (m+1) </strong>时新的弱学习器。<strong class="ig hi"> γ </strong>是这个新的弱学习者的学习率。如果我们将该乘法与当前集合输出<strong class="ig hi"> F(x) </strong>相加，我们获得迭代<strong class="ig hi"> (m+1) </strong>的新集合输出！</li><li id="c4a7" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">重复这个过程，直到每一个弱学习者都被训练出来！</li></ul><p id="0bc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面，我插入一段伪代码，以便更好地解释梯度增强概念:</p><pre class="jd je jf jg fd ke kf kg kh aw ki bi"><span id="ebc3" class="kj kk hh kf b fi kl km l kn ko">% X: Feature set (nxd)<br/>% Y: Label set   (nx1)</span><span id="db3d" class="kj kk hh kf b fi kp km l kn ko">residual = Y; % Initialize the residual with the desired outputs Y<br/>% Use the gradient of the MSE function for residual calculation<br/>% 2(Y-residual) is the gradient of MSE</span><span id="b467" class="kj kk hh kf b fi kp km l kn ko">for m = 1:Number_of_Weak_Learners</span><span id="604e" class="kj kk hh kf b fi kp km l kn ko">     Weak_Learners{m} = Train_Decision_Tree(X,residual);        <br/>     gamma(m) = 0.1; % Use this as a learning rate. Can be    <br/>     %adaptively chosen for each iteration as well!</span><span id="77bf" class="kj kk hh kf b fi kp km l kn ko">     % Find new residual for the next iteration</span><span id="169f" class="kj kk hh kf b fi kp km l kn ko">     residual = 2*(residual — gamma(m) * ...   <br/>     predict(Weak_Learners{m},X));<br/>end</span></pre><p id="4cc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望你喜欢这篇文章。下次再见！:)</p><h2 id="91dd" class="kj kk hh bd kq kr ks kt ku kv kw kx ky ip kz la lb it lc ld le ix lf lg lh li bi translated">参考</h2><ul class=""><li id="5b9c" class="jp jq hh ig b ih lj il lk ip ll it lm ix ln jb ju jv jw jx bi translated">[1]<a class="ae lo" href="https://www.geeksforgeeks.org/ml-gradient-boosting/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/ml-gradient-boosting/</a></li><li id="1f2e" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">Freund，Schapire，1997，“<em class="lp">在线学习的决策理论概括和促进的应用</em>”</li><li id="d680" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">Mason，Baxter，Bartlett，Frean，1999，“函数空间中梯度下降的<em class="lp"> Boosting算法</em></li></ul><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh ji lt"/></div></div></a></div><p id="f225" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">🔵<a class="ae lo" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ig hi">成为作家</strong> </a></p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div></div>    
</body>
</html>