# 知识蒸馏介绍——欧比万·天行者算法

> 原文：<https://medium.com/mlearning-ai/an-introduction-to-knowledge-distillation-the-obi-wan-skywalker-algorithm-bc117d80d69?source=collection_archive---------4----------------------->

闭上眼睛，想象一下你在塔图因。你坐在年轻的卢克·天行者和 C3PO 的旁边，寻找 R2D2。是的，车内的空间有点狭窄，但是，嘿，你是在星球大战传奇中，所以你会得到你想要的。当干旱的山峰和山谷呼啸而过时，沙漠的空气吹过你的脸庞。卢克问你是否看到了什么，你眯着眼睛看着广阔的天空。最后，你发现了 R2D2。但在你庆祝之前，塔斯肯突袭机发动了攻击，你的船员无可救药地被制服了。记住，这是路加在知道什么是“原力”之前。但是等等！不知从哪里冒出来一个戴着兜帽的人，赶走了讨厌的袭击者，拯救了你们所有人。

我将在这里停止对《星球大战》的详细叙述，但那个戴兜帽的人成了卢克的导师，欧比万·克诺比。然后，他教卢克“原力”的方式，我们都知道后来发生了什么。

你有没有想过，如果不是欧比万，卢克会不会发现"原力"的方法？想象一下银河系的困境，如果是这样的话。绝地从师傅到徒弟的教学方式是强有力的。首先，它帮助[学徒](https://starwars.fandom.com/wiki/Padawan)跳过多年的错误和错误开始，更快地成为原力的一员。这个学生也不断收到反馈，以提高自己，成为一名更好的绝地。

你在自己的生活中没见过这种情况吗？当你从走在你前面的人那里学习一项新技能时，你会比自学学得更快。如果能把这个原理用在机器学习上岂不是很牛逼？事实证明，我们可以。

这个技巧叫做[知识升华](https://arxiv.org/abs/1503.02531)。

![](img/b7afe9cbd12239d0c93103217891f2a9.png)

Distillation of the “force”

# 什么是知识升华？

知识提炼是一种将知识从计算量大的教师模型转移到更小的轻量级学生模型的聪明方法。大老师模式真的很擅长你想解决的任务。但它真的很好是有原因的。因此，您不能在应用程序中部署它，也不能在移动(边缘)设备上使用它，或者在资源受限的情况下使用它。不要放弃，你可以使用知识提炼将一些辛苦获得的知识从这个大模型转移到一个更小的模型，你知道*可以*在上面所有的场景中工作。

你知道什么是疯狂的吗？较小的型号可能不会成功。但是当你执行知识提炼时，你得到了两个世界的最好的东西，因为你得到了一个非常轻量级但是聪明的模型。

另一种思考知识提炼的方式是将其视为压缩的一种变体，只不过我们通过转移来压缩，而不是从现有模型中删除神经元。

# 知识提炼的组成部分

知识的升华有三个组成部分——老师，这是你精心训练的大模型。或者，这也可能是你在某处找到的一个非常好的预训练模型。第二部分是学生。这是您计划最终使用的轻量级模型，但还不够好，需要学习。这里轻量级是什么意思？嗯，它可能是教师模型的一个更浅的版本，具有更少的层和每层的神经元，教师的一个量化版本，一个不同的更小的网络，等等。第三个也是最后一个组件是用于将知识转移到学生模型的方法。

# 理解知识提炼是如何工作的

让我们回到《星球大战》的类比——如果卢克必须自己学习如何使用原力，他可能会在大量的试验和错误之后学会(毕竟他是《星球大战》传奇中的主要英雄之一)。但是想想看，他要花多长时间*才能真正*理解原力，并变得如此优秀，以至于能够多次拯救世界。另一方面，老师欧比万非常有经验，对原力了如指掌。通过他的指导，卢克的学习速度大大加快。

卢克得到了两个反馈来源。首先，他感受到了与原力的联系。在机器学习术语中，这将是模型看不到的基本事实，但我们用它来衡量其性能。卢克的第二个反馈来自欧比万，他告诉卢克他是否很好地运用了原力。这是对第一个人的称赞性反馈，他可以用它来提高自己的绝地技能。用知识升华的术语来说，这是教师模型给学生的信号。

很自然，问题是欧比旺如何给卢克反馈？为此，我们需要首先了解什么样的知识值得从导师向学员传授。

# 知识的类型

大致有两种知识可以用作反馈信号。首先是结果本身。在这种情况下，老师和学生都被要求做同样的事情，我们会比较他们的结果。然后学生们将会知道他们的结果和他们的老师有多么不同。现在，这种情况的一个特例是当我们需要执行复杂的任务时。除了从最终结果中学习，中间结果也可以用作反馈信号。例如，奥比万可以在卢克的注视下表演一个绝地心灵魔术([“你不需要看我们的证件”](https://www.youtube.com/watch?v=gALR3SnDEWw))，或者向他解释这个过程的每个部分。

用知识提炼的术语来说，这些被称为**基于响应的知识**(仅仅是结果)和**基于特征的知识**(也是中间结果)。

第二大类捕获中间步骤之间的关系——这些步骤如何联系在一起。这被称为**基于关系的知识**，用知识提炼的说法就是网络的[特征图之间的关系。](https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c#:~:text=Feature%20maps%20are%20generated%20by,Convolutional%20layers%20in%20the%20model.)

# 知识升华的模式

既然我们知道了值得传授的知识类型，我们怎样才能把知识传授出去呢？

第一种方式是**离线蒸馏**，你有一个已经预先训练好的教师模型，并且只让学生学习。第二个叫做**在线蒸馏**。这是一种更罕见的形式，在这种形式中，你无法接触到预先训练好的模型。所以，你得自己培养老师。在这样做的同时，您还可以并排设置学生模型，并对两者进行训练。第三种是**自提**，师生用同一个模型。在这里，模型的更深层次可以教导更早的层次，或者，来自更早时代的模型可以用于在其学习旅程中训练更深层次的模型。很 meta，我同意。

# 知识在野外的升华

这里有两个应用知识提炼的著名故事。第一个是[distill BERT](https://arxiv.org/abs/1910.01108v4)，来自一个巨大的语言模型 BERT 的知识被成功地提炼到一个较小的模型中。这个结果模型比它的老师小 40%(66M 对 110M 参数)，快 60%。更令人印象深刻的是，这个较小的模型在特定任务中的表现有 97%与老师一样好。第二个实际上是在[亚马逊 Alexa](https://arxiv.org/abs/1904.01624) 中使用的知识提炼，在那里，一个师生模型组合帮助从仅仅 7000 小时的已标记示例中标记超过 100 万小时的未标记语音。

因此，下次当你在计算资源方面遇到困难，但真的想要一个好的模型时，考虑一下知识提炼。

# 🤖💪想要更多的想法成为一个多产的 ML 从业者？

每周，我都会发送一份时事通讯，提供实用的技巧和资源，以提升自己作为机器学习从业者的水平。 [**免费加入→**](https://newsletter.artofsaience.com/)

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)