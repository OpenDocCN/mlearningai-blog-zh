# 人工智能地缘战略探讨

> 原文：<https://medium.com/mlearning-ai/discussion-on-ai-geopolitical-strategy-51ad302182b9?source=collection_archive---------6----------------------->

![](img/cf43474ea2ff938f39bf086a489bdb94.png)

Image from WP Engine

我最喜欢的是国际关系、经济和政治。我在抽着雪茄的族长身边长大，他们详细谈论美国、俄罗斯、中国以及其间的所有创新、战争、冲突和进步。我对我们的全球世界感到惊叹，我对影响力的来源和它的作用着迷。我被世界超级大国的规模和他们影响全世界无辜人民生活的无数方式吓坏了。自早期以来，没有什么太大的变化。

当我开始上大学的时候到了该选专业的时候，我已经知道自己属于哪里了。我会说四种语言，我在旅行中长大，经常独自旅行，去见世界各地的亲戚，我长大后有了强烈的个人财务责任感。不管是好是坏，我已经为全球化的世界做好了准备。

自从我加入数据科学和人工智能世界以来，我一直敏锐地意识到人工智能不仅对我们作为一个国家的政治地位，而且对我们自己的个人成长和扩张具有不可思议的潜力和力量。技术的好坏取决于你用它做什么，我每天都被别人经常提出的创新所鼓舞。自从我的旅程开始以来，我已经加入了许多人工智能组织，包括[推进对人工智能的信任](https://www.linkedin.com/company/advancingtrustinai/?viewAsMember=true)，但我最近最喜欢的一次会谈是与戴夫·巴恩斯上校的会谈，他是美国陆军人工智能任务组的首席人工智能道德官，也是 USMA 英语和哲学教授兼副主任，他加入了我们，从军事角度谈论我们在人工智能中的定位。[看看这里](https://www.linkedin.com/video/live/urn:li:ugcPost:6776547678043037696/)！下面是我们讨论中出现的一些关键概念的总结。

# 我们如何在军队中部署 AI？

自主系统并不局限于陆军，因为人工智能已经无处不在，整个军队可能会在许多方面被人工智能所改变，但它比战场应用要广泛得多。从人力资源、人才管理和预测性维护，到人道主义援助、救灾或接种疫苗，任何东西都是在更广泛的背景下思考人工智能的有用框架。同样，我们看到数据科学和人工智能产品的兴起，但不仅仅是这样——人工智能正在影响我们如何被雇用，如何被解雇，我们有资格获得什么抵押贷款，我们看到什么广告，等等。

# 让我们来谈谈人工智能中的伦理和信任

早期提出的一个观点是“当我们说信任人工智能时，我们指的是什么。”当创建和使用人工智能系统时，每个人都有自己的世界观，我们的士兵需要信任这项新技术。但是我们所说的信任是什么意思呢？这是指系统本身算法的健壮性，还是指系统的有效性？

当我们谈论人际关系时，例如，在我们是否信任一个特定的人或一群人的情况下，我们会抛出信任这个词。但是当我们谈论功能的时候，我们也会用到它。比起谷歌地图，我们更信任 Waze 吗？尽管同为一家公司所有，Waze 懒惰的方向让我经常错过转弯的路，所以我信任谷歌地图。我知道，已经有一批心怀不满的 Waze 粉丝准备向我证明我是多么的错误。但他们不会改变我的想法。

当我们谈论机器和工具时，信任通常更适用于可靠性，但当谈到人工智能时，我们确实意味着信任的两种观点都非常重要。安全、信任、可靠、可持续——这些都是拥有道德和负责任的人工智能所必需的，但光有这些还不够。

大多数有原则的人不希望自己的工作不道德。然而，在我们的许多组织中，我们觉得必须有人关心道德。我们将合规性和合法性委托给一些外部道德或合规性团队，但实际上每个人都有自己的角色，这些框架需要出现在产品生命周期的每个阶段，而不仅仅是在最后。而不仅仅是在一个公司或组织被起诉之后，这是常有的事。我们必须记住，如果一个系统正在以某种方式影响人类或环境，我们必须*不断*评估它构建的价值和道德以及如何使用它。

人工智能不是价值中立的。当然，数学是客观的，技术是中立的，但人类正在制造和使用这些系统。这种中立在人类介入的那一刻就结束了。多元化很重要，因为我们把自己的世界观带到了我们创造的产品中。优化具有下游效应，我们必须敏锐地意识到我们在优化什么。脸书为屏幕时间进行了优化，但我们看到这对我们的青年以及政治和科学信息的吸收产生了不利影响。Clickbait 对《clickbait》和《脸书》的制作人来说是很棒的，但对我们的精神状态来说是不好的。

我们可能会从隐喻的屋顶到我们的内心大声疾呼伦理，但这并不像只是将伦理“编码”到一个系统中那么简单。这说起来容易做起来难。首先，我们没有一个假定的普遍规范理论来解释什么是“道德人工智能”假设我们可以把那个理论编码成一个系统，即使它是可能的或真实的，可能在操作上是“道德的”，但这并不意味着它实际上是一个大计划中的道德系统。这些定义比什么都更有哲理性。道德规范本身在不同的地区或国家有很大的不同。即使在一个国家或地区内，道德规范也会随着时间的推移而演变。100 年、50 年、10 年前我们认为合乎道德的东西，现在可能已经不符合道德了。我们该如何从地缘政治上调和这一点呢？这些都是难题。

# 中国、俄罗斯和其他对手

本月[我们为](https://www.linkedin.com/events/bookclubdiscussion-aisuper-powe6770943532015386624/)[书友](https://www.linkedin.com/groups/12494999/)阅读的书是 [*人工智能超级大国*](https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X/ref=sr_1_1?crid=3AYRZVBKH4A1D&keywords=ai+superpowers&qid=1616193947&s=books&sprefix=ai+s%2Cstripbooks%2C157&sr=1-1) *s* 在所有国家中，中国目前拥有最雄心勃勃的全球人工智能战略。到 2030 年，中国有义务“[成为世界领先的人工智能创新中心](https://futureoflife.org/ai-policy-china/)”，并将自己定位为人工智能的全球领导者。美国新兴的数字自由民主、俄罗斯的数字混合政权和中国的数字专制政权都是创造全球人工智能文化的关键角色，这种文化仍在诞生。

这就引出了一个问题:如果美国在道德上建设人工智能，如果我们的对手不分享我们对人工智能的道德和伦理，人工智能伦理会让我们处于劣势吗？

我们一定要玩阴的才能赢吗？

根据巴恩斯上校的说法，不。人工智能系统将根据我们国家的价值观和法治建设，因为我们自己的完整性。武装冲突法的四项基本原则、基于正义战争原则的我们各国的条约和法律为我们提供了信息。

> “我们的国家期望我们做正确的事情”
> 
> -戴夫·巴恩斯上校，教授，USMA 大学英语和哲学副主任，美国陆军人工智能(AI)任务组首席人工智能道德官

人工智能将从本质上允许我们防止更多的附带损害，并使决策更加及时，因此有很多希望。如果我们可以更好地了解信息，拥有帮助我们更快做出反应的技术，那么我们就有道德义务善用这项技术，特别是在军事上。

我们的对手会利用任何限制吗？当然可以。会有一些窗口，他们可以利用技术将我们置于不利地位，我很感激我们正在认真对待这些风险。目前情况并非如此，但我们需要确保我们在引领人工智能研究和开发的同时保持我们的价值观不变。精神伤害也威胁着我们士兵的精神健康。我喜欢上校提出的观点。包容性的自由民主和多样性是我们的优势，这将使我们能够创造领先的技术，我们作为美国必须保持在人工智能开发和研究方面的竞争优势，因为地缘政治的原因。

# 致命自主武器系统

致命自主有它的顾虑。无论人工智能是通过武器部署，还是用于人才管理或预测性维护计划，欧盟关于人工智能是合法、强大和道德的概念都同样适用。信息处理系统，从狭隘的、基于规则的早期人工智能专家系统，到我们今天看到的深度学习和生成系统，都依赖于可靠性的概念。我们依赖这些系统来更快地处理信息，以便我们可以利用它的承诺，我们需要了解人工智能如何得出自己的结论。一个学习系统将会是一个与你第一次部署它时不同的系统。这就是 AI 的本质。

全球广泛关注致命自主。问题之一是缺乏一个共同的定义。问题的另一部分是过度依赖科幻小说中人工智能变坏的例子。自二战以来，美国和其他国家已经部署了自主系统，因此这种使用不一定是一件新鲜事。黑仔机器人很可怕，但我们还没有接近一般的人工智能。有些人认为我们可能永远也到不了那里。然而，当技术按照广告和计划运行时，人工智能支持的自主系统带来了另一种程度的恐惧以及道德和法律问题。当技术崩溃时，有哪些道德和法律问题？只要看看最近几天出来的 Teslas 自动驾驶[的问题就知道了。](https://www.vox.com/recode/2020/2/26/21154502/tesla-autopilot-fatal-crashes)

对美国来说，我们的政策源自国防部第 3000.09 号指令:“自主和半自动武器系统的设计应允许指挥官和操作员对武力的使用进行适当程度的人类判断。”一直没有立法的部分原因实际上是因为对自治系统可能需要什么有各种各样的看法。另一个最近的区别是致命自主系统和完全致命自主系统之间的区别。

困难不仅仅是定义系统。如果有禁止致命自主系统的普遍愿望，我们必须定义限制。这些系统通常是大型系统的一部分，可能包括从目标识别、处理情报、选择武器系统、推荐目标或以上所有内容。与其他大规模杀伤性武器不同，人工智能是民主化的，更广泛的。这增加了如何对其使用进行立法的复杂性。

算法、数据和计算能力的三位一体可以为我们指明正确的方向，确定我们如何识别和监管这些武器的使用。此外，AI winter 还提供了大量我们之前没有数据的潜在产品和用例。进入大数据时代。

不管我们代表什么行业，如果还没有准备好，我们就不应该在世界上部署人工智能。国防部长签署了[国防部人工智能原则](https://www.defense.gov/Explore/News/Article/Article/2094085/dod-adopts-5-principles-of-artificial-intelligence-ethics/):负责、公平、可追踪、可靠、可管理。不同的研究机构发现，全球 36 个主要国家或更大的机构有自己的一套原则。公共区域是可见的，但是如何操作它们呢？原则是路标，但是我们如何用它们来影响过程呢？在为时已晚之前，我们能否通过敏捷冲刺来审视一下法律伦理政策的考量？专家团队将需要与研究生命周期一起工作，这将影响到在生命周期的每个方面工作的每个人。风险管理已经是军队中关于潜在风险的一种方法。这也适用于伦理。将原则翻译下来，并确保国防部从事这些工作的人按照原则工作。

好消息是:每个人都在处理这个问题。每个国家仍在努力解决人工智能是什么，如何使用它，以及它如何造成危害。像我们组织的这样的草根活动可以更好地将不同背景的人聚集在一起。我们可以通过使用人工智能实现更光明的未来，如果我们继续进行这些艰难的对话，我们可以将人工智能可能造成的风险和伤害降至最低。比我更敬业、更聪明的人已经承诺成为解决方案的一部分，这让我松了一口气。

如果这很有趣，请随意查看另一个关于美国军方人工智能原则的链接

如果你想继续读下去，也可以看看下面的书:

[无限制战争](https://www.amazon.com/Unrestricted-Warfare-Chinas-Destroy-America/dp/1626543054)

[AI 超能力](https://www.amazon.com/AI-Superpowers-Kai-Fu-Lee-audiobook/dp/B07G8KTTX4/ref=sr_1_1?crid=2HLK7WEY7S4KJ&dchild=1&keywords=ai+superpowers&qid=1616190087&s=books&sprefix=ai+superpowers%2Cstripbooks%2C155&sr=1-1)

[无一人之军](https://www.amazon.com/Army-None-Autonomous-Weapons-Future/dp/0393608980)

[生活 3.0](https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598)