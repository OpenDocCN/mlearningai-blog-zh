<html>
<head>
<title>Learning-based models for Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于学习的分类模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/learning-based-models-for-classification-c21513e14621?source=collection_archive---------7-----------------------#2022-11-20">https://medium.com/mlearning-ai/learning-based-models-for-classification-c21513e14621?source=collection_archive---------7-----------------------#2022-11-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="bed9" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="ee2a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在机器学习领域已经开发了成千上万种学习算法。科学家通常会从这些算法中进行选择，以解决特定的问题。他们的选择常常受限于他们对这些算法的熟悉程度。在这个经典/传统的机器学习框架中，科学家被迫做出一些假设，以便使用现有的算法。虽然这在某些情况下可能是限制性的，但它可以提供速度、低成本计算和易用性的好处，作为过拟合和降低精度的折衷。</p><p id="5226" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在本文中，我们建立了多个基于学习的模型，用于对序列数据(ECG)进行分类，以检测心脏病的概率。集成学习、支持向量机、伯努利朴素贝叶斯、K近邻和随机森林分类器在我们的分类任务中得到了深入研究。您可以在这个<a class="ae kf" href="https://github.com/nandangrover/learning-models" rel="noopener ugc nofollow" target="_blank">存储库</a>中签出代码和数据集。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/606cd08a1e795ce8b53747675c3e01ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLpqxqWd-Leu_jg5-3MBiA.png"/></div></div></figure><h1 id="2ae0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">朴素贝叶斯</h1><p id="5235" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">朴素贝叶斯分类器使用贝叶斯定理和所有预测因子相互独立的假设将数据分类。假设一个类中一个特征的存在与其他特征的存在无关。</p><p id="f1ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例如，如果一个水果是绿色的，圆形的，直径为10英寸，它就是西瓜。这些特征可能是相互排斥的，但是每一个都独立地导致了所考虑的水果是西瓜的可能性。这就是为什么这个分类器的名称中会出现“Naive”这个词。</p><p id="fa96" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">以下是贝叶斯定理，它是该算法的基础:</p><pre class="kh ki kj kk fd ks kt ku bn kv kw bi"><span id="29c7" class="kx if hh kt b be ky kz l la lb">P(c|x) = P(x|c) * P(c)/P(x)</span></pre><p id="f2cd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在这个等式中，“c”代表类，“x”代表属性。P(c/x)代表预测值的类别后验概率。P(x)是预测值的先验概率，P(c)是类的先验概率。P(x/c)表示基于类的预测值的概率。</p><h2 id="65b5" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">它如何用于我们的数据集？</h2><p id="89df" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们将使用朴素贝叶斯的一个特殊版本，称为伯努利朴素贝叶斯。这种情况下的预测器是布尔变量。所以我们的选项是‘真’和‘假’(对心脏病是真，对心脏病不是真)。</p><p id="5993" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">它是最常见的朴素贝叶斯模型类型之一:其操作与多项式分类器的操作相同。另一方面，预测变量是独立的布尔变量。例如，它可以确定文档中是否存在特定的单词。BernoulliNB是为二进制/布尔特性设计的。由于我们的特征集是一组离散的二进制值，伯努利朴素贝叶斯特别适合从我们的数据集中预测心脏病。</p><h1 id="bda8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">k-最近邻</h1><p id="f603" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">k-最近邻(KNN)算法是一种数据分类方法，它根据最接近数据点的数据点来估计数据点属于两个组之一的可能性。</p><p id="fecb" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">监督机器学习算法k-最近邻用于解决分类和回归问题。但是，它主要用于解决分类问题。</p><p id="cc47" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">考虑以下两组:A组和b组。</p><p id="d1e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该算法检查附近数据点的状态，以确定一个数据点属于A组还是b组。如果大多数数据点属于A组，则所讨论的数据点几乎肯定属于A组，反之亦然。</p><p id="7fec" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">简而言之，KNN需要通过检查最近的带注释的数据点(也称为最近邻)来对数据点进行分类。</p><h2 id="08a2" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">它如何用于我们的数据集？</h2><p id="dc40" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当KNN用于分类时，输出是K个最相似实例中频率最高的类。本质上，每个实例为它们的类投票，投票最多的类被选为预测。</p><p id="e7aa" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类别概率可以被计算为属于每个类别的新数据实例的K个最相似实例的集合中的样本的归一化频率。例如，在二进制分类问题中(类为0或1)，</p><pre class="kh ki kj kk fd ks kt ku bn kv kw bi"><span id="f99b" class="kx if hh kt b be ky kz l lq lb">p(class=0) = count(class=0) / count(class=0) + count(class=1)</span></pre><p id="2059" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们使用了K (np.arange(1，25，2))的一系列值，并让GridSearchCV为n _ neighbours找到最佳参数。这有助于我们以高精度执行分类。</p><h1 id="f6a6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">支持向量机</h1><p id="2833" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">弗拉迪米尔·N·瓦普尼克和阿列克谢·亚创造了第一个SVM算法。1963年的切尔沃嫩基斯。当时该算法还处于早期阶段。唯一的选择是为线性分类器绘制超平面。Bernhard E. Boser、Isabelle M Guyon和Vladimir N Vapnik在1992年提出了一种通过将核技巧应用于最大间隔超平面来创建非线性分类器的方法。Corinna Cortes和Vapnik于1993年提出了当前标准，并于1995年发布。</p><p id="385c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">支持向量机可用于执行线性分类。除了使用核技巧的线性分类之外，SVM还可以有效地执行非线性分类。它允许我们将输入隐式映射到高维特征空间。</p><h2 id="8b4e" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">它如何用于我们的数据集？</h2><p id="f1c1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">支持向量机的主要目标是在给定的数据集中找到一个支持向量之间具有最大可能间隔的超平面。在接下来的两步中，SVM寻找具有最高边缘的超平面</p><ul class=""><li id="142b" class="lr ls hh je b jf ka jj kb jn lt jr lu jv lv jz lw lx ly lz bi translated">创建尽可能好的分类超平面。有许多超平面可以用来对数据进行分类。我们应该寻找这两个类之间有最大间隔的超平面。</li><li id="d648" class="lr ls hh je b jf ma jj mb jn mc jr md jv me jz lw lx ly lz bi translated">所以我们选择超平面，使得它和每一边的支持向量之间的距离尽可能短。如果这样的超平面存在，则称为最大间隔超平面，并且它定义的线性分类器称为最大间隔分类器。</li></ul><p id="dc6f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">支持向量机(SVM)是用于分类和回归的机器学习算法。支持向量机是用于分类、回归和异常值检测的最强大的机器学习算法之一。SVM分类器创建一个模型，将新的数据点分配到给定的类别之一。因此，它是非概率二元线性分类器。因为我们的特征集是一组离散的二进制值，所以支持向量机是从我们的数据集中预测心脏病的理想选择。</p><h1 id="4abe" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">集成学习模型</h1><p id="b52c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">对于给定的数据集，单一算法可能无法做出最佳预测。机器学习算法有局限性，创建高精度的模型很难。我们可以通过构建和组合多个模型来提高整体准确性。然后通过聚集每个模型的输出来实现模型的组合，并牢记两个目标:</p><ul class=""><li id="1b99" class="lr ls hh je b jf ka jj kb jn lt jr lu jv lv jz lw lx ly lz bi translated">模型误差减少</li><li id="ba2d" class="lr ls hh je b jf ma jj mb jn mc jr md jv me jz lw lx ly lz bi translated">保持模型的概括性</li></ul><h2 id="1366" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">随机森林</h2><p id="4502" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">随机森林是一种用于解决回归和分类问题的机器学习技术。它利用集成学习，一种结合许多分类器来解决复杂问题的技术。</p><p id="743a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">随机森林算法由许多决策树组成。使用bagging或bootstrap聚合来训练随机森林算法的“森林”。Bagging是一种元算法，通过集成方法提高机器学习算法的准确性。</p><p id="0cc9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">结果由基于决策树预测的(随机森林)算法决定。它通过平均或平均各种树的输出来预测。结果的精确度随着树的数量的增加而提高。</p><h2 id="5b25" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">投票分类器</h2><p id="5378" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">VotingClassifier在概念上组合不同的机器学习分类器，并使用多数投票或平均预测概率(软投票)来预测类别标签。这种类型的分类器可用于平衡一组表现相似的模型的各自弱点。</p><h2 id="9319" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">它如何用于我们的数据集？</h2><p id="b249" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们决定使用加权平均概率(软投票)作为我们的投票分类器。软投票将类标签作为预测概率之和的argmax返回。权重参数可用于为每个分类器分配特定的权重。当提供权重时，每个分类器的预测类概率被累加，乘以分类器权重，然后进行平均。然后，具有最高平均概率的类别标签被用于生成最终的类别标签。</p><h1 id="1040" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">描述数据集</h1><p id="57f3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们的数据集已经标准化，如<strong class="je hi">图1.1 </strong>所示。它总共有140列和1621行。我们仍然需要规范化我们的数据库，因为伯努利朴素贝叶斯不能接受负值。当我们遇到伯努利朴素贝叶斯分类器时，我们通过创建一个管道并在管道中添加MinMaxScaler来实现这一点。</p><pre class="kh ki kj kk fd ks kt ku bn kv kw bi"><span id="18b6" class="kx if hh kt b be ky kz l lq lb">classifier = Pipeline([(‘Normalizing’,MinMaxScaler()), (‘BernoulliNB’,BernoulliNB())])</span></pre><p id="ea76" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">缩放将一组变量转换成具有相同数量级的另一组变量。因为它通常是线性变换，所以不影响特征的相关性或预测能力。</p><p id="07a3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为什么有必要对我们的数据进行标准化或规范化？这是因为特性的数量级会影响某些模型的性能。例如，如果一个要素的数量级等于1000，而另一个要素的数量级等于10，则一些模型可能“认为”一个特征比另一个特征更重要。数量级没有告诉我们任何关于预测能力的信息，因此它是有偏差的。我们可以通过改变变量使它们具有相同的数量级来消除这种偏差。</p><p id="be74" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">标准化和规范化是这些转换中的两种，它们将每个变量转换为0-1区间(将每个变量转换为0均值和单位方差变量)。理论上，标准化优于规范化，因为它不会导致变量的概率分布在异常值出现时缩小</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mf"><img src="../Images/30a73b87d6d8c49bc077c5282d1f35ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vneUKobFtvYgWGa6"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 1</figcaption></figure><h1 id="1169" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">构建模型</h1><p id="bf90" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们先后使用了支持向量机、伯努利朴素贝叶斯、K近邻和随机森林分类器来有效地衡量所有模型之间的差异。对于KNN，我们使用了n _ neighbours的不同参数(步骤2大小为1到25)来为我们的分类器找到最佳参数。GridSearchCV使用交叉验证方法为字典中传递的每个值组合评估模型。使用该函数的结果是，我们可以计算每个超参数组合的精确度/损失，并选择具有最佳性能的一个。</p><pre class="kh ki kj kk fd ks kt ku bn kv kw bi"><span id="f5ba" class="kx if hh kt b be ky kz l lq lb">from sklearn.model_selection import GridSearchCV<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.naive_bayes import BernoulliNB<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn import svm<br/>from sklearn.svm import SVC<br/>import warnings<br/>warnings.filterwarnings(‘ignore’)<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>processed_data = {}<br/>def construct_model(dataframe):<br/> # the list of classifiers to use<br/> # use random_state for reproducibility<br/> classifiers = [<br/> SVC(probability=True, random_state=42),<br/> BernoulliNB(),<br/> KNeighborsClassifier(),<br/> RandomForestClassifier(random_state=42),<br/> ]<br/> <br/> svm_parameters = {<br/> ‘gamma’: [‘auto’]<br/> }<br/><br/>gaussian_parameters = {<br/> }<br/><br/>knn_parameters = {<br/> ‘n_neighbors’: np.arange(1, 25, 2)<br/> }<br/><br/>rf_parameters = {<br/> <br/> }<br/> # stores all the paramete rs in a list<br/> parameters = [<br/> svm_parameters,<br/> gaussian_parameters,<br/> knn_parameters,<br/> rf_parameters<br/> ]<br/> processed_data[‘estimators’] = []<br/> # iterate through each classifier and use GridSearchCV<br/> for i, classifier in enumerate(classifiers):<br/> if i == 1:<br/> classifier = Pipeline([(‘Normalizing’,MinMaxScaler()), (‘BernoulliNB’,BernoulliNB())])<br/> else:<br/> clf = GridSearchCV(classifier, # model<br/> param_grid = parameters[i], # hyperparameters<br/> scoring=’accuracy’, # metric for scoring<br/> cv=10,<br/> n_jobs=-1, error_score=’raise’)<br/> clf.fit(dataframe[“X_train”], dataframe[“Y_train”])<br/> # add the clf to the estimators list<br/> processed_data[‘estimators’].append((classifier.__class__.__name__, clf))</span></pre><p id="7997" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们还建立了一个集成模型，特别是加权平均概率(软投票)分类器。软投票将类标签作为预测概率之和的argmax返回。权重参数可用于为每个分类器分配特定的权重。当提供权重时，每个分类器的预测类概率被累积并由分类器相乘。</p><pre class="kh ki kj kk fd ks kt ku bn kv kw bi"><span id="e168" class="kx if hh kt b be ky kz l lq lb">from sklearn.ensemble import VotingClassifier<br/><br/>ensemble = VotingClassifier(processed_data[‘estimators’],<br/> voting=’soft’,<br/> weights=[1,1,2,1], n_jobs=-1) # n-estimators<br/><br/>ensemble.fit(transformed_df_train[“X_train”], transformed_df_train[“Y_train”])</span></pre><h1 id="5b78" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模型评估</h1><p id="bbf4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">图1.2 </strong>显示了我们的最后一个模型，即集合投票分类器的表现，以及它通过使用混淆矩阵来逼近患病与未患病之间关系的程度。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mk"><img src="../Images/8e428c59e756577520850b3427d8ca60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*6NtkChzgCTllL-qs"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 2</figcaption></figure><p id="9923" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">图1.3 </strong>显示了ROC曲线，<strong class="je hi">图1.4 </strong>报告了模型在一系列指标上的表现，如精确度、召回率、准确度和F1分数。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ml"><img src="../Images/4ea4e18b9d7f5738a83f0d6a796db928.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*Q47hyPRzjKji8QK3"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 3</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mf"><img src="../Images/7b39f59544038b015c9ffffd3100c72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0an-nDfMT6l8591G"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 4</figcaption></figure><h2 id="7fde" class="lc if hh bd ig ld le lf ik lg lh li io jn lj lk is jr ll lm iw jv ln lo ja lp bi translated">模型评估的正当性</h2><p id="eef6" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">混乱矩阵</strong></p><p id="48d8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这里有四种可能的结果:真阳性(TP)表示模型预测了真实的结果，并且实际观察是真实的。假阳性(FP)表示模型预测了真实的结果，但是观察到的结果是假的。假阴性(FN)表示模型预测了错误的结果，而观察到的结果是正确的。最后，真阴性(TN)表示模型预测了错误的结果，而实际结果也是错误的。我们的模型预测疾病的真阳性为100%(是)，假阳性为1.28%，这是衡量准确性的良好指标。随后，真阴性为99.72%，而真阳性为0%。</p><p id="c48d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">投票分类器中不同的权重如何影响准确率？</strong></p><p id="8629" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们决定使用加权平均概率(软投票)作为我们的投票分类器。软投票将类标签作为预测概率之和的argmax返回。权重参数可用于为每个分类器分配特定的权重。当提供权重时，每个分类器的预测类概率被累加，乘以分类器权重，然后进行平均。然后，具有最高平均概率的类别标签被用于生成最终的类别标签。简而言之，在对用于软投票的所述权重取平均值之前，为每个分类器分配一个权重序列，以对预测类别概率的出现进行加权。</p><p id="9e5d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">实验一</strong></p><p id="5b4a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">分类器</strong>:投票分类器</p><p id="0606" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">权重</strong> : [1，1，1，1]</p><p id="00f2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">观察</strong> : <strong class="je hi">图1.5 </strong>显示了我们用上述权重得到的混淆矩阵</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mk"><img src="../Images/06fcd73ef61c5992cb437bb637d14813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*-L0ABdx0-Syzb6oH"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 5</figcaption></figure><p id="bb53" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">推断</strong>:在所有分类器的权重均匀分布的情况下，所达到的准确度是所有集成分类器的平均准确度，即支持向量机、伯努利朴素贝叶斯、K最近邻和随机森林分类器的平均值。达到的平均准确度约为98.8%。</p><p id="5ad4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">实验二</strong></p><p id="a54d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">分类器</strong>:投票分类器</p><p id="d4b1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">权重</strong> : [1，1，2，1]</p><p id="c3f6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">观察</strong> : <strong class="je hi">图1.6 </strong>显示了我们用上述权重得到的混淆矩阵</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mm"><img src="../Images/b4528f22540e3762b56e58134170d870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*GSkWF46SVTERyKbI"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Figure 6</figcaption></figure><p id="5f58" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">推论</strong>:由于K近邻在所有其他分类器中具有最好的准确性，我们给KNN分类器更多的权重。结果，我们的准确度跃升至约99.4%，比实验1中达到的准确度高。</p><h1 id="2f93" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="7aaf" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">集成学习、支持向量机、伯努利朴素贝叶斯、K近邻和随机森林分类器都在我们的分类模型的开发中大量使用。使用GridSearchCV进行超参数调整，并使用投票分类器对所有现有模型进行最终集成，我们实现了99.4%的最大准确度。</p><h1 id="fa03" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><ol class=""><li id="031a" class="lr ls hh je b jf jg jj jk jn mn jr mo jv mp jz mq lx ly lz bi translated"><a class="ae kf" href="https://github.com/nandangrover/learning-models" rel="noopener ugc nofollow" target="_blank">https://github.com/nandangrover/learning-models</a></li><li id="0615" class="lr ls hh je b jf ma jj mb jn mc jr md jv me jz mq lx ly lz bi translated">https://en.wikipedia.org/wiki/Support-vector_machine<a class="ae kf" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank"/></li><li id="30dc" class="lr ls hh je b jf ma jj mb jn mc jr md jv me jz mq lx ly lz bi translated">aure lien ge Ron使用Scikit-Learn和Tensorflow进行机器实践学习</li></ol><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hi fi z dy mz ea eb na ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kq mu"/></div></div></a></div></div></div>    
</body>
</html>