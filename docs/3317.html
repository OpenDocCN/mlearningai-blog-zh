<html>
<head>
<title>Generating music using images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用图像生成音乐</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/generating-music-using-images-13bdf1c78437?source=collection_archive---------1-----------------------#2022-08-18">https://medium.com/mlearning-ai/generating-music-using-images-13bdf1c78437?source=collection_archive---------1-----------------------#2022-08-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="dfd6" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">拥抱脸的新<code class="du iw ix iy iz b">diffusers</code>包</h2></div><figure class="jb jc jd je fd jf er es paragraph-image"><div role="button" tabindex="0" class="jg jh di ji bf jj"><div class="er es ja"><img src="../Images/fd3654842078e594122b28bb92229e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmks2hoar6jivmqhJRczsw.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Aphex Twin embedded a self-portrait in the spectrogram of Equation (image credit <a class="ae jq" href="http://www.bastwood.com/?page_id=10" rel="noopener ugc nofollow" target="_blank">Jarmo Niinisalo</a>)</figcaption></figure><p id="a5fa" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">[ <strong class="jt hi">更新</strong>:我还在音乐中使用的3万个样本上训练了模型，这些样本来自<a class="ae jq" href="https://whosampled.com" rel="noopener ugc nofollow" target="_blank"> WhoSampled </a>和<a class="ae jq" href="https://youtube.com" rel="noopener ugc nofollow" target="_blank"> YouTube </a>。这个想法是，这个模型可以用来生成循环或“中断”,这些循环或“中断”可以被采样来制作新的轨迹。人们(“挖箱子的人”)会不遗余力或者愿意花大价钱来打破旧记录。]</p><p id="d96f" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">最近深度学习模型在图像生成(<a class="ae jq" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank"> DALL-E 2 </a>、<a class="ae jq" href="https://www.midjourney.com/home/" rel="noopener ugc nofollow" target="_blank"> MidJourney </a>、<a class="ae jq" href="https://imagen.research.google/" rel="noopener ugc nofollow" target="_blank"> Imagen </a>、<a class="ae jq" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/" rel="noopener ugc nofollow" target="_blank"> Make-A-Scene </a>等领域的改进让我感到惊讶。)和文字生成(<a class="ae jq" href="https://openai.com/api/" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>、<a class="ae jq" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank">布鲁姆</a>、<a class="ae jq" href="https://huggingface.co/facebook/bart-large" rel="noopener ugc nofollow" target="_blank">巴特</a>、<a class="ae jq" href="https://huggingface.co/google/t5-v1_1-xl" rel="noopener ugc nofollow" target="_blank"> T5 </a>等。)但与此同时，对音频生成相对缺乏进展感到惊讶。我想到了两个值得注意的例外:<a class="ae jq" href="https://openai.com/blog/musenet/" rel="noopener ugc nofollow" target="_blank"> MuseNet </a>将乐谱视为连续令牌(类似于文本)并利用GPT-2，而<a class="ae jq" href="https://openai.com/blog/jukebox/" rel="noopener ugc nofollow" target="_blank">点唱机</a>和<a class="ae jq" href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio" rel="noopener ugc nofollow" target="_blank"> WaveNet </a>从原始波形中生成音乐。即便如此，音频代落后是因为人们对它的兴趣减少，还是因为它本质上更具挑战性？</p><p id="9dbb" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">不管是哪种情况，通过<a class="ae jq" href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" rel="noopener ugc nofollow" target="_blank"> mel光谱图</a>，音频可以很容易地转换成图像，反之亦然。</p><figure class="jb jc jd je fd jf er es paragraph-image"><div class="er es kn"><img src="../Images/c77bab9fa86febfb1beb1a5d771d2dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*rIMacCDYkQ1ZGBB1yEJNLg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Left as an exercise for the reader to determine which song this was taken from.</figcaption></figure><p id="7e48" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">频谱图的水平轴是时间，垂直轴是频率(在对数标度上)，阴影表示振幅(也在对数标度上)。梅尔声谱图应该与人耳感知声音的方式非常接近。</p><p id="82ae" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">如果我们现在可以使用人工智能轻松生成令人信服的名人照片，为什么不尝试生成可信的光谱图并将它们转换成音频呢？这正是我使用新的拥抱脸<code class="du iw ix iy iz b">diffusers</code>包所做的。</p><h2 id="c2b3" class="ko kp hh bd kq kr ks kt ku kv kw kx ky ka kz la lb ke lc ld le ki lf lg lh li bi translated">TL；速度三角形定位法(dead reckoning)</h2><p id="a98e" class="pw-post-body-paragraph jr js hh jt b ju lj ii jw jx lk il jz ka ll kc kd ke lm kg kh ki ln kk kl km ha bi translated">那么，它的效果如何呢？查看一些自动生成的循环:</p><figure class="jb jc jd je fd jf"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="710d" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">你也可以在Google Colab上为自己创造更多</p><div class="lq lr ez fb ls lt"><a href="https://colab.research.google.com/github/teticio/audio-diffusion/blob/master/notebooks/test_model.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">谷歌联合实验室</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">音频扩散</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">colab.research.google.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh jk lt"/></div></div></a></div><p id="8824" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">你可以从我的Spotify“喜欢”播放列表的近500首歌曲(约20，000个频谱图)中选择一个模型，或者从音乐中使用的30，000个样本中选择一个模型。</p><h2 id="9ec2" class="ko kp hh bd kq kr ks kt ku kv kw kx ky ka kz la lb ke lc ld le ki lf lg lh li bi translated">告诉我更多…</h2><div class="lq lr ez fb ls lt"><a href="https://github.com/teticio/audio-diffusion" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">GitHub - teticio/audio-diffusion:使用新的…应用去噪扩散概率模型</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">应用去噪扩散概率模型使用新的拥抱脸扩散包来合成音乐，而不是…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">github.com</p></div></div><div class="mc l"><div class="mi l me mf mg mc mh jk lt"/></div></div></a></div><p id="8473" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">在上面的报告中，您将发现一些实用工具，可以从音频文件目录中创建一个声谱图图像数据集，训练一个模型来生成类似的声谱图，并将生成的声谱图转换为音频。您还可以找到允许您摆弄预先训练好的模型的笔记本。</p><p id="81f6" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">如果你对模型的细节感兴趣，那么我推荐你阅读<a class="ae jq" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank">的去噪扩散概率模型</a>论文。根据开放人工智能的说法，<a class="ae jq" href="https://arxiv.org/pdf/2105.05233.pdf" rel="noopener ugc nofollow" target="_blank">扩散模型在他们自己的游戏</a>中击败了甘斯。基本思想是训练一个模型来从被高斯噪声破坏的版本中恢复图像。例如，如果用名人的照片来训练模型，它将学习什么是典型的(或者也许不那么典型！)五官长得像。为了生成一个名人的随机脸，该模型被给予一个完全随机的图像，并且每次运行时，输出图像比以前稍微少一些噪声，看起来更像一张脸(或者在我们的情况下是一个声谱图)。</p><p id="88d7" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">为了简单起见，我选择创建256 x 256像素的方形声谱图图像，这相当于五秒钟的合理质量音频。我使用Hugging Face的<code class="du iw ix iy iz b">accelerate</code>包将这些批次分割成碎片，以便安装在我的单个RTX 2080 Ti GPU上。培训持续了大约40个小时。</p><p id="fac8" class="pw-post-body-paragraph jr js hh jt b ju jv ii jw jx jy il jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">请记住，我的Spotify播放列表有点混合了不同风格的音乐。我觉得使用我非常熟悉的训练数据集很重要，这样我就能够判断模型产生了多少数据，以及有多少数据是回流的(而且我很有可能真的喜欢这些结果)。同样，名人数据集相对同质，例如，用纯钢琴音乐或纯电子音乐来训练模型，以查看它是否能够学习关于特定流派的任何东西，这将是有趣的。</p><h2 id="502b" class="ko kp hh bd kq kr ks kt ku kv kw kx ky ka kz la lb ke lc ld le ki lf lg lh li bi translated">下一步是什么？</h2><p id="03a9" class="pw-post-body-paragraph jr js hh jt b ju lj ii jw jx lk il jz ka ll kc kd ke lm kg kh ki ln kk kl km ha bi translated">随着采样器的出现，音乐永远地改变了。起初它们是有争议的，一个叫做Quest的部落必须支付“我能踢它吗？”的所有收益去卢·里德看《荒野大镖客》的样片(T1)，但之后他们被几乎所有类型的电影所接受(T3)。找到一个新的钩子来取样可能是一笔大生意，那么为什么不使用人工智能来创造新的钩子呢？我想看到的是相当于DALL-E 2和公司，但对于即时驱动的音频生成…</p><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="mj l me mf mg mc mh jk lt"/></div></div></a></div></div></div>    
</body>
</html>