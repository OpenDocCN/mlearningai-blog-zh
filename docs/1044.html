<html>
<head>
<title>Question-Answering in association with roBERTa</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与罗伯塔一起回答问题</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/question-answering-in-association-with-roberta-a11518e70507?source=collection_archive---------0-----------------------#2021-09-21">https://medium.com/mlearning-ai/question-answering-in-association-with-roberta-a11518e70507?source=collection_archive---------0-----------------------#2021-09-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="eb5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">喝一口chaii，用谷歌的<a class="ae jc" href="https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering" rel="noopener ugc nofollow" target="_blank">chaii——北印度语和泰米尔语问答</a>,刷新你的心情。</p><h2 id="3454" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">【amitnikhade.com T2】号</h2><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jy"><img src="../Images/7e76deb22aa5a49c32f83e49d839961a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-R8CLLWULT88fdlp6blSQ.jpeg"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">A beautiful click by <a class="ae jc" href="https://www.flickr.com/photos/nukelarburrito/" rel="noopener ugc nofollow" target="_blank">nukelarburrito</a></figcaption></figure><h2 id="839c" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">介绍</h2><p id="4a05" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">愿上帝保佑你掌握所有的数据结构技能。变形金刚一直是革命性的模型，产生了像伯特、GPT、mt5、t5、塔帕斯、阿尔伯特、罗伯特以及更多来自他们家族的艺术变体。<a class="ae jc" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>库为各种真实场景的实现提供了优秀的文档。</p><p id="a3ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们将尝试实现问答系统的<a class="ae jc" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> Roberta </a>模型。我们不会直接深入我们的代码，我们将修改一些理解代码所必须的理论概念。相信我，对于一个自然语言处理的新手来说，理解代码背后的逻辑需要稍微长一点的时间。我尽力从概念上解释它。读者也有责任通过同时浏览代码来理解代码，直到你不理解为止，浏览你在本文中不知道的东西肯定会对你有用。</p><h2 id="38a3" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">你应该熟悉的术语</h2><ul class=""><li id="8599" class="kt ku hh ig b ih ko il kp ip kv it kw ix kx jb ky kz la lb bi translated"><strong class="ig hi">记号</strong>:记号是自然语言处理的构建块。标记是在对文本进行标记化之后创建的。标记化主要可以分为词、字符和子词，即n元字符标记化。</li><li id="2f87" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">输入标识</strong>:输入标识为令牌索引；它们是记号的数字表示；这些表示的列表被称为序列，用作模型的输入。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lh"><img src="../Images/878730647b5c979451d88da897f79411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LADvcVbMOhuwRgcyoRXwyw.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">input_ids</figcaption></figure><ul class=""><li id="4d34" class="kt ku hh ig b ih ii il im ip li it lj ix lk jb ky kz la lb bi translated"><strong class="ig hi">Sequence _ id:</strong>Sequence _ id告诉我们序列中的哪一部分是问题，哪一部分是答案。唯一标记编码为None，其中0表示问题，1表示上下文。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es ll"><img src="../Images/6af1643b88e911f05d923b3113e40dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lm6dsvM2LJsKwZMjrubsWA.png"/></div></div></figure><ul class=""><li id="054d" class="kt ku hh ig b ih ii il im ip li it lj ix lk jb ky kz la lb bi translated"><strong class="ig hi">offset _ mapping:</strong>offset _ mapping参数为我们提供了记号的位置，这是记号的开始和结束索引的元组，便于找到它们的原始位置。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lm"><img src="../Images/a05a0f3524aa0b66ab866bfb77e28102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQJhDmmQzHSLpgZc75OEPw.png"/></div></div></figure><ul class=""><li id="3832" class="kt ku hh ig b ih ii il im ip li it lj ix lk jb ky kz la lb bi translated"><strong class="ig hi"> overflow_sampling </strong>:就像示例(即上下文)是如何由于max_length限制而被截断的，因此被修剪的部分以可靠的步幅率在下面的序列中继续，溢出样本是上下文被拟合了多少分裂的映射。如下图所示，0是数据集中的第一个上下文，数据足够大，因此包含在许多拆分中。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es ln"><img src="../Images/406381a8258a15e19cbd1dd69fe18d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoxRtjDulEWWynIxs55UlQ.png"/></div></div></figure><ul class=""><li id="93f1" class="kt ku hh ig b ih ii il im ip li it lj ix lk jb ky kz la lb bi translated"><strong class="ig hi"> max_length: </strong>为要处理的序列定义的恒定长度，并馈入模型。</li><li id="33e2" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">填充:</strong>填充满足具有给定max_length的序列，比如如果max_length是20，而我们的文本只有15个单词，那么在对其进行标记化之后，文本将被填充1，从而得到长度为20的序列。填充可以从序列的前端开始，也可以从序列的末端开始。</li><li id="0ef5" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi"> Stride: </strong> Stride是截断序列与下一个序列进行修补的令牌长度速率。它基本上用于处理溢出。</li><li id="deac" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">截断:</strong>达到max_length后切割序列。</li><li id="2455" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">start _ token/end _ _ token:</strong>需要取一个特定观察的起始标记，比如上下文中答案的起始标记，类似的就是结束标记，也就是那个答案的结束标记。</li><li id="70f3" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">Attention _ mask:</strong>Attention mask告诉模型中的注意机制排除不是实际标记的填充。这使得我们很容易区分组合序列中的实际标记和填充。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lo"><img src="../Images/f992efd85ccf7d1577d8510954ae2a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5CFJmnYej1dzPxwLD3Ugg.png"/></div></div></figure><h2 id="3c28" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">罗伯塔标记化风格</h2><p id="c5da" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">罗伯塔使用从<a class="ae jc" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>派生的字节级<a class="ae jc" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank">字节对编码</a>方法。词汇表由50000个单词组成。\U0120作为字节对编码中使用的唯一字符，还没有通过拥抱脸让我们看到。BPE就像一种数据压缩算法，其中最常见的一对连续数据字节被替换为该数据中不存在的字节。</p><p id="1127" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在对数据进行编码时，考虑数据<strong class="ig hi">ggghghghgh</strong>。字节对<strong class="ig hi"> gg </strong>出现的频率最高，所以我们会用<strong class="ig hi"> K来代替它</strong>生僻字被分解成更多的子字令牌。这是BPE背后的一个基本理念。</p><h2 id="2537" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">模型</h2><p id="dd77" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">Roberta代表<a class="ae jc" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">R</strong>obustable<strong class="ig hi">O</strong>optimized<strong class="ig hi">BERT</strong>Pre-training<strong class="ig hi">A</strong>pproach</a>在160 GB的数据上进行训练，如图书语料库(朱等，2015)、维基百科以及一些附加数据。罗伯特只是一个熟悉动态掩蔽的伯特，而没有下一句预测，就像伯特通常如何预先训练掩蔽语言建模和下一句预测一样，在罗伯特模型的情况下，NSP被消除。我不会详述内部模型的工作原理，我在以前的文章中已经解释过了。</p><h2 id="1c17" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">履行</h2><blockquote class="lp lq lr"><p id="7d81" class="ie if ls ig b ih ii ij ik il im in io lt iq ir is lu iu iv iw lv iy iz ja jb ha bi translated"><em class="hh">在这次比赛中，你将预测印地语和泰米尔语问题的答案。答案直接从有限的上下文中得出(详见评估页)。我们提供了少量样本来检查您的代码。还有一个隐藏的测试集。</em></p><p id="30a7" class="ie if ls ig b ih ii ij ik il im in io lt iq ir is lu iu iv iw lv iy iz ja jb ha bi translated"><em class="hh">所有文件应编码为UTF-8。</em></p></blockquote><p id="f11b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">项目包含的步骤:</strong></p><ul class=""><li id="527c" class="kt ku hh ig b ih ii il im ip li it lj ix lk jb ky kz la lb bi translated"><strong class="ig hi">加载我的案例中已经给出的训练和测试数据。</strong></li><li id="9764" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">预处理数据似乎是整个问题中最具挑战性的部分之一。这包括准备培训和验证功能。</li><li id="5aab" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">根据我们预处理的数据训练模型</strong></li><li id="6dab" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">后处理</strong></li><li id="489e" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">预测</strong></li></ul><p id="2728" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据取自谷歌主办的Kaggle竞赛<strong class="ig hi">“chaii——印地语和泰米尔语问答”</strong>。这是<a class="ae jc" href="https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/data" rel="noopener ugc nofollow" target="_blank">数据</a>的链接。我们的任务是识别在数据集中的印度语言段落中发现的问题的答案。数据中的列包括id上下文问题答案_文本答案_开始语言。</p><p id="2c78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练集有747个印地语和368个泰米尔语示例。在测试数据中，我们提供了5个例子；我们没有<em class="ls">回答_正文</em>和<em class="ls">回答_开始</em>。我们已经获得了上下文、问题、答案文本和答案开始。我们只需要在使用开始和结束跨度的上下文中找到答案。我们只需要预测答案文本。</p><p id="b4e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">依赖关系</strong></p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="ae48" class="jd je hh lx b fi mb mc l md me">pip install transformers<br/>pip install tensorflow<br/>pip install Dataset<br/>pip uninstall fsspec -qq -y<br/>pip install --no-index --find-links ../input/hf-datasets/wheels datasets -qq</span></pre><p id="a755" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">导入库</strong></p><p id="0b4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您需要在您的系统上安装transformers库。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="d230" class="jd je hh lx b fi mb mc l md me">import pandas as pd<br/>import numpy as np<br/><br/>from transformers import *<br/>import tensorflow as tf<br/>import collection<br/>from datasets import Dataset</span></pre><p id="ebcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">加载数据</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="1d62" class="jd je hh lx b fi mb mc l md me">train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')<br/>train.head()</span><span id="6152" class="jd je hh lx b fi mf mc l md me">test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')<br/>test.head()</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lo"><img src="../Images/9868245c690791b9ed9f5b1ab69b4118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdHqlunDkObvgs6NZk9Z9g.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">train data</figcaption></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es mg"><img src="../Images/9ff282316e834d43adea5491e231fe32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zcqhnrjzj0tLt_V4_hggnA.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">test data</figcaption></figure><p id="e8ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">记号赋予器类</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="fd99" class="jd je hh lx b fi mb mc l md me">tokenizer = AutoTokenizer.from_pretrained("../path-to-pretrained-tokenizer/xlm-roberta-large-squad2")</span></pre><p id="2b2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设置最大长度、批量大小和</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="a4f4" class="jd je hh lx b fi mb mc l md me">batch_size = 4<br/>max_length = 384 <br/>doc_stride = 128</span></pre><p id="32a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定义一个为我们准备训练数据的函数。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="2e67" class="jd je hh lx b fi mb mc l md me">def prepare_train_features(examples):<br/>    <br/>    examples["question"] = [q.lstrip() for q in examples["question"]]</span><span id="9a15" class="jd je hh lx b fi mf mc l md me"><br/>    tokenized_examples = tokenizer(<br/>        examples["question" if pad_on_right else "context"],<br/>        examples["context" if pad_on_right else "question"],<br/>        truncation="only_second" if pad_on_right else "only_first",<br/>        max_length=max_length,<br/>        stride=doc_stride,<br/>        return_overflowing_tokens=True,<br/>        return_offsets_mapping=True,<br/>        padding="max_length",<br/>    )</span><span id="f32d" class="jd je hh lx b fi mf mc l md me"><br/>    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")<br/>    <br/>    offset_mapping = tokenized_examples.pop("offset_mapping")</span><span id="5928" class="jd je hh lx b fi mf mc l md me"><br/>    tokenized_examples["start_positions"] = []<br/>    tokenized_examples["end_positions"] = []</span><span id="3c32" class="jd je hh lx b fi mf mc l md me">for i, offsets in enumerate(offset_mapping):<br/>       <br/>        input_ids = tokenized_examples["input_ids"][i]<br/>        cls_index = input_ids.index(tokenizer.cls_token_id)</span><span id="4065" class="jd je hh lx b fi mf mc l md me"><br/>        sequence_ids = tokenized_examples.sequence_ids(i)</span><span id="5522" class="jd je hh lx b fi mf mc l md me"><br/>        sample_index = sample_mapping[i]<br/>        answers = examples["answers"][sample_index]<br/>        <br/>        if len(answers["answer_start"]) == 0:<br/>            tokenized_examples["start_positions"].append(cls_index)<br/>            tokenized_examples["end_positions"].append(cls_index)<br/>        else:<br/>         <br/>            start_char = answers["answer_start"][0]<br/>            end_char = start_char + len(answers["text"][0])</span><span id="3435" class="jd je hh lx b fi mf mc l md me"><br/>            token_start_index = 0<br/>            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):<br/>                token_start_index += 1</span><span id="617b" class="jd je hh lx b fi mf mc l md me"><br/>            token_end_index = len(input_ids) - 1<br/>            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):<br/>                token_end_index -= 1</span><span id="b38d" class="jd je hh lx b fi mf mc l md me"><br/>            if not (offsets[token_start_index][0] &lt;= start_char and offsets[token_end_index][1] &gt;= end_char):<br/>                tokenized_examples["start_positions"].append(cls_index)<br/>                tokenized_examples["end_positions"].append(cls_index)<br/>            else:<br/>               <br/>                while token_start_index &lt; len(offsets) and offsets[token_start_index][0] &lt;= start_char:<br/>                    token_start_index += 1<br/>                tokenized_examples["start_positions"].append(token_start_index - 1)<br/>                while offsets[token_end_index][1] &gt;= end_char:<br/>                    token_end_index -= 1<br/>                tokenized_examples["end_positions"].append(token_end_index + 1)<br/>                break</span><span id="648b" class="jd je hh lx b fi mf mc l md me">return tokenized_examples</span></pre><p id="568a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看这个函数到底是如何工作的。</p><p id="9428" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将把训练数据传递给这个函数。首先，空白将被删除，它出现在训练数据中一些问题的左边。接下来是在每个转换器模型中使用的广泛步骤之一，即记号赋予器。分词器将句子分词成组块，如下所示。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="d8da" class="jd je hh lx b fi mb mc l md me"><strong class="lx hi">&lt;s&gt; sequence_1 &lt;/s&gt;&lt;/s&gt; sequence_2 &lt;/s&gt; </strong></span></pre><p id="e132" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中<s>是分类符标记，</s>是分隔符标记。我们将问题和上下文传递给记号赋予器，检查记号赋予器填充位置是在右边还是左边，但默认情况下是在左边。因此，记号赋予器将对序列进行记号化，并用max_length填充序列，以满足给定的序列限制。我们在记号赋予器中添加了一个截断参数，它在超过max_length限制后截断序列。跨距指定了该序列应该与先前溢出的序列重叠多少长度的标记。截断序列时，我们可能会丢失大量数据，从数据科学的角度来看，这不是一个好的做法。为了克服这个问题，我们需要返回溢出的序列，以将它们转发给进一步的序列，从而与它们重叠。我们还返回偏移量映射，这为我们提供了标记位置的映射。</p><p id="574b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一步中，我们提取样本映射，该样本映射对它所做的一对分割进行计数。众所周知，offset_mapping给出了序列的位置信息。最后，我们开始收集开始和结束标记，并在循环中应用一些逻辑。我们提取sequence _ ids，这有助于我们区分问题和上下文。cls_index是分类器标记索引。上面的函数一次处理一个样本。在上下文中不存在答案的情况下，我们放置开始和结束位置。sample_index是包含该文本范围的示例的索引，即样本映射索引。最后，我们将收集循环中特定实例的开始和结束索引；如果答案不在示例中，我们将用CLS令牌标记它；否则，我们将把开始和结束索引移动到答案的开始和结束点。该函数返回预处理示例的列表。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="05f7" class="jd je hh lx b fi mb mc l md me">def convert_answers(r):<br/>    start = r[0]<br/>    text = r[1]<br/>    return {<br/>        'answer_start': [start],<br/>        'text': [text]<br/>    }</span><span id="08d1" class="jd je hh lx b fi mf mc l md me">train = train.sample(frac=1, random_state=42)<br/>train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)</span><span id="6b71" class="jd je hh lx b fi mf mc l md me">df_train = train[:-64].reset_index(drop=True)<br/>df_valid = train[-64:].reset_index(drop=True)</span><span id="b2b4" class="jd je hh lx b fi mf mc l md me">train_dataset = Dataset.from_pandas(df_train)<br/>valid_dataset = Dataset.from_pandas(df_valid)</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lo"><img src="../Images/94aaae9bfef75f240165121b1b3a82ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0ad0RsZ2dJo3ZKAgzmpSA.png"/></div></div></figure><p id="8f88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在将训练数据传递给特征准备函数之前，我们需要创建一个名为answers的维度，由answer_start和answer_text组成；此外，我们生成数据的随机样本，并从pandas数据帧中表示Dataset对象。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="3dd1" class="jd je hh lx b fi mb mc l md me">train_tokenized_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)</span><span id="8a6c" class="jd je hh lx b fi mf mc l md me">test_tokenized_dataset = test_dataset.map(prepare_train_features, batched=True, remove_columns=test_dataset.column_names)</span></pre><p id="52d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过对其应用prepare_train_feature函数，上述代码将用于预处理训练和测试数据。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="24e0" class="jd je hh lx b fi mb mc l md me">args = TrainingArguments(<br/>    f"chaii-qa",<br/>    evaluation_strategy = "epoch",<br/>    save_strategy = "epoch",<br/>    learning_rate=3e-5,<br/>    warmup_ratio=0.1,<br/>    gradient_accumulation_steps=8,<br/>    per_device_train_batch_size=batch_size,<br/>    per_device_eval_batch_size=batch_size,<br/>    num_train_epochs=1,<br/>    weight_decay=0.01,<br/>)</span></pre><p id="bf63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">配置训练参数。data_collator自动对一批模型输入执行填充，填充到数据集中最大扩展示例的长度，这样就不需要设置通常是固定的最大序列长度，从而提高了性能。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="1dde" class="jd je hh lx b fi mb mc l md me">from transformers import default_data_collator</span><span id="a67f" class="jd je hh lx b fi mf mc l md me">data_collator = default_data_collator</span><span id="48de" class="jd je hh lx b fi mf mc l md me">trainer = Trainer(<br/>    model,<br/>    args,<br/>    train_dataset=tokenized_train_ds,<br/>    eval_dataset=tokenized_valid_ds,<br/>    data_collator=data_collator,<br/>    tokenizer=tokenizer,<br/>)</span><span id="3650" class="jd je hh lx b fi mf mc l md me">trainer.train()<br/>trainer.save_model("chaii-bert-trained")</span></pre><p id="2754" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> trainer.train() </strong>开始训练，之后保存模型。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="1970" class="jd je hh lx b fi mb mc l md me">def prepare_validation_features(examples):<br/> <br/>    examples["question"] = [q.lstrip() for q in examples["question"]]</span><span id="8527" class="jd je hh lx b fi mf mc l md me"><br/>    tokenized_examples = tokenizer(<br/>        examples["question" if pad_on_right else "context"],<br/>        examples["context" if pad_on_right else "question"],<br/>        truncation="only_second" if pad_on_right else "only_first",<br/>        max_length=max_length,<br/>        stride=doc_stride,<br/>        return_overflowing_tokens=True,<br/>        return_offsets_mapping=True,<br/>        padding="max_length",<br/>    )<br/>    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")<br/>    tokenized_examples["example_id"] = []</span><span id="c642" class="jd je hh lx b fi mf mc l md me">for i in range(len(tokenized_examples["input_ids"])):<br/>        <br/>        sequence_ids = tokenized_examples.sequence_ids(i)<br/>        context_index = 1 if pad_on_right else 0</span><span id="340d" class="jd je hh lx b fi mf mc l md me"><br/>        sample_index = sample_mapping[i]<br/>        tokenized_examples["example_id"].append(examples["id"][sample_index])</span><span id="20de" class="jd je hh lx b fi mf mc l md me"><br/>        tokenized_examples["offset_mapping"][i] = [<br/>            (o if sequence_ids[k] == context_index else None)<br/>            for k, o in enumerate(tokenized_examples["offset_mapping"][i])<br/>        ]</span><span id="852c" class="jd je hh lx b fi mf mc l md me">return tokenized_examples</span></pre><p id="e283" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了验证，我们不需要计算开始和结束位置；相反，我们将收集组合成一个特征的例子。sample_mapping关键字为我们提供了一个映射，该映射提供了关于上下文和由于max_length限制而从中分割的特征之间的对应关系的信息。我们将使用sequence _ ids来了解问题和上下文在序列中的确切位置。pad_to_right为真，因此上下文索引将为1。最后，我们将把不包含在上下文中的offset_mapping设置为None，这样可以更简单地检测上下文。</p><p id="65df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与训练集一样，我们将对数据应用prepare_validation_features函数。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="3186" class="jd je hh lx b fi mb mc l md me">validation_features = valid_dataset.map(<br/>    prepare_validation_features,<br/>    batched=True,<br/>    remove_columns=valid_dataset.column_names<br/>)</span><span id="32ad" class="jd je hh lx b fi mf mc l md me">valid_feats_small = validation_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])</span></pre><p id="a458" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">预言</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="aabf" class="jd je hh lx b fi mb mc l md me">raw_predictions = trainer.predict(valid_feats_small)</span></pre><p id="08c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的代码块告诉我们一个例子被分成了多少个特性，并给出了例子及其特性的列表。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="4657" class="jd je hh lx b fi mb mc l md me">max_answer_length = 30</span><span id="7cbe" class="jd je hh lx b fi mf mc l md me">examples = valid_dataset<br/>features = validation_features<br/><br/>example_id_to_index = {k: i for i, k <strong class="lx hi">in</strong> enumerate(examples["id"])}<br/>features_per_example = collections.defaultdict(list)<br/>for i, feature <strong class="lx hi">in</strong> enumerate(features):<br/>    features_per_example[example_id_to_index[feature["example_id"]]].append(i)</span></pre><p id="15da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">后处理:</strong>后处理将问答模型的预测转换成作为原始上下文子串的答案。后处理代码由示例上的嵌套循环组成。它收集循环中正在进行的例子的特征的索引以及上下文。此外，它循环遍历连续示例中的每个要素，并从由分别包含开始逻辑和结束逻辑的两个数组组成的模型中收集对相同要素的预测。min_null_score是None，它必须在训练第二组数据时使用。空答案的得分是与作为我们最小空分数的[CLS]令牌相关联的start_logit和end_logit之和。开始和结束逻辑的任何合理组合，即start_logit + end_logit，都可以被认为是一个可能的答案。组合得分越高，获得最佳答案的信心越高。如果结束标记在开始标记之前，在这种情况下，应该将其排除。开始或结束标记与问题标记相关联的答案也被排除，因为我们知道问题的答案在问题中不是显而易见的。可以使用-n _ best _ size参数调整每个示例的最佳预测数；代码遍历所有的可能性以得到最佳答案。长度小于0或大于max_answer_length的答案不包括在内；不考虑超出范围的答案。</p><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="4c9b" class="jd je hh lx b fi mb mc l md me">from tqdm.auto import tqdm<br/><br/>def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):<br/>    all_start_logits, all_end_logits = raw_predictions<br/>    <br/>    example_id_to_index = {k: i for i, k <strong class="lx hi">in</strong> enumerate(examples["id"])}<br/>    features_per_example = collections.defaultdict(list)<br/>    for i, feature <strong class="lx hi">in</strong> enumerate(features):<br/>        features_per_example[example_id_to_index[feature["example_id"]]].append(i)<br/><br/>    predictions = collections.OrderedDict()<br/><br/>   <br/>    for example_index, example <strong class="lx hi">in</strong> enumerate(tqdm(examples)):<br/>       <br/>        feature_indices = features_per_example[example_index]<br/><br/>        min_null_score = None <br/>        valid_answers = []<br/>        <br/>        context = example["context"]<br/>        <br/>        for feature_index <strong class="lx hi">in</strong> feature_indices:<br/>           <br/>            start_logits = all_start_logits[feature_index]<br/>            end_logits = all_end_logits[feature_index]<br/>          <br/>            offset_mapping = features[feature_index]["offset_mapping"]<br/><br/>         <br/>            cls_index = features[feature_index]["input_ids"].index(tokenizer.cls_token_id)<br/>            feature_null_score = start_logits[cls_index] + end_logits[cls_index]<br/>            if min_null_score <strong class="lx hi">is</strong> None <strong class="lx hi">or</strong> min_null_score &lt; feature_null_score:<br/>                min_null_score = feature_null_score<br/><br/>            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()<br/>            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()<br/>            for start_index <strong class="lx hi">in</strong> start_indexes:<br/>                for end_index <strong class="lx hi">in</strong> end_indexes:<br/>  <br/>                    if (<br/>                        start_index &gt;= len(offset_mapping)<br/>                        <strong class="lx hi">or</strong> end_index &gt;= len(offset_mapping)<br/>                        <strong class="lx hi">or</strong> offset_mapping[start_index] <strong class="lx hi">is</strong> None<br/>                        <strong class="lx hi">or</strong> offset_mapping[end_index] <strong class="lx hi">is</strong> None<br/>                    ):<br/>                        continue<br/>                    <br/>                    if end_index &lt; start_index <strong class="lx hi">or</strong> end_index - start_index + 1 &gt; max_answer_length:<br/>                        continue<br/><br/>                    start_char = offset_mapping[start_index][0]<br/>                    end_char = offset_mapping[end_index][1]<br/>                    valid_answers.append(<br/>                        {<br/>                            "score": start_logits[start_index] + end_logits[end_index],<br/>                            "text": context[start_char: end_char]<br/>                        }<br/>                    )<br/>        <br/>        if len(valid_answers) &gt; 0:<br/>            best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]<br/>        else:<br/>          <br/>            best_answer = {"text": "", "score": 0.0}<br/>        <br/>        predictions[example["id"]] = best_answer["text"]<br/><br/>    return predictions</span></pre><p id="1fc4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将把valid_dataset、validation_features、raw_predictions传递给postprocess_qa_predictions函数，以获得最终预测。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es mh"><img src="../Images/a6e5cfaf2b33da109a0fe075d2e49fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ryi-KElU8R3lhDTek5U5xg.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">validation features</figcaption></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es mi"><img src="../Images/8144a819558d59891949d82dcf7583bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cn4x47oFFEDlfLDhNuwhRw.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx">valid dataset</figcaption></figure><pre class="jz ka kb kc fd lw lx ly lz aw ma bi"><span id="9186" class="jd je hh lx b fi mb mc l md me">final_predictions = postprocess_qa_predictions(valid_dataset, validation_features, raw_predictions.predictions)</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mj"><img src="../Images/1b3f59797feddc55e1a2338bbf9b93dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*8dRkBoyBH_T9cUHbtfjs9A.png"/></div></figure><p id="961e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是了，如果你还不明白这一点，我建议你至少看三遍整篇文章。并且试着在网上浏览你不理解的话题，我相信你会的。</p><p id="401c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ls">链接到</em> <a class="ae jc" href="https://github.com/AmitNikhade/Kaggle/blob/main/chaii%20-%20Hindi%20and%20Tamil%20Question%20Answering/question-answering-roberta-starter-explained.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ls"> GitHub </em> </a> <em class="ls">。</em></p><h2 id="c2ce" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">参考</h2><div class="mk ml ez fb mm mn"><a href="https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">chaii -印地语和泰米尔语问答</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">找出印度语文章中问题的答案</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">www.kaggle.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ki mn"/></div></div></a></div><p id="112e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1907.11692.pdf</a>/</p><div class="mk ml ez fb mm mn"><a href="https://huggingface.co/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">拥抱脸-人工智能社区建设未来。</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">我们正在通过开源和开放科学来推进和民主化人工智能的旅程。</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">huggingface.co</p></div></div><div class="mw l"><div class="nc l my mz na mw nb ki mn"/></div></div></a></div><p id="fd05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">_____________________________感谢__________________________感谢</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es nd"><img src="../Images/d20875a482faf0336a8922989826b3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*JvzfN85FCQegYLj6.gif"/></div></figure></div></div>    
</body>
</html>