<html>
<head>
<title>“Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications” Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“Salus:面向深度学习应用的细粒度GPU共享原语”摘要</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/salus-fine-grained-gpu-sharing-primitives-for-deep-learning-applications-49c0fbb5c5f7?source=collection_archive---------5-----------------------#2022-02-17">https://medium.com/mlearning-ai/salus-fine-grained-gpu-sharing-primitives-for-deep-learning-applications-49c0fbb5c5f7?source=collection_archive---------5-----------------------#2022-02-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="73ca" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="cf81" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">自从GPU证明其在深度学习(DL)工作负载方面比传统CPU有显著的性能提升以来，GPU计算一直很受欢迎。然而，对于这些工作负载，GPU未得到充分利用。这是因为DL工作负载的性质和GPU缺乏对细粒度共享原语的支持。Yu、裴峰等人[1]提出的机制实现了<strong class="je hi">快速作业切换</strong> ( <strong class="je hi">实现分时抢占</strong>)和<strong class="je hi">内存共享</strong>实现多个DL应用间细粒度的GPU共享。关键思想是以迭代粒度进行调度，并利用DL模型的持久内存。结果显示，完成时间和GPU利用率都有显著提高。</p><h1 id="d33d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">观察和研究激励</h1><ol class=""><li id="96f9" class="ka kb hh je b jf jg jj jk jn kc jr kd jv ke jz kf kg kh ki bi translated">GPU调度粒度是每个应用一个GPU。</li><li id="745e" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">第一个事实导致<strong class="je hi">H</strong>EAD-<strong class="je hi">O</strong>f-<strong class="je hi">L</strong>ine(<strong class="je hi">HOL</strong>)阻塞问题。DL应用通常是长时间的任务，尤其是培训。</li><li id="091e" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">DL作业通常不会完全利用GPU。作者的观察显示，GPU内存利用率往往低于<strong class="je hi"> 50% </strong>。</li><li id="d81b" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">DL模型的自动超参数调整并行生成许多训练作业。很多都是因为质量差被杀的。</li><li id="1d2e" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz kf kg kh ki bi translated">他们观察到模型和框架内部内存分配明显少于用作暂存区的空间。</li></ol><h1 id="edf4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">拟议机制</h1><p id="9814" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">下图显示了Salus的架构概述。当用户创建DL作业时，Salus采纳者创建相应的会话(<strong class="je hi"> 1a </strong>)。DL作业的计算图也传输到Salus。然后，会话继续向存储器管理器请求通道(<strong class="je hi"> 1b </strong>)。根据系统中的当前作业，此进程可能会阻塞，会话将排队。在作业运行期间，无论是训练还是干扰，迭代都由用户脚本生成，并转发给Salus中的相应会话(<strong class="je hi"> 2a </strong>)。迭代调度器(<strong class="je hi"> 2b </strong>)根据它们相关的GPU通道对它们进行调度，并发送到GPU执行。</p><p id="506b" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi"> <em class="kt">注意Salus执行服务通过DL作业的迭代粒度调度实现GPU共享。</em> </strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/71b93add853d299aac08fea4208318aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGAbTDYqjhgTxsGY0jREAQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[1]</figcaption></figure><p id="e7ac" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">根据观察结果(5)，可以在GPU中保留多个作业的持久内存，同时仍然有足够的空间来容纳其中任何一个超热内存。</p><p id="040f" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated"><strong class="je hi"> <em class="kt">请注意，Salus旨在通过保留持久内存来实现显著更快的挂起/恢复操作，然后迭代粒度作业调度程序(例如，分时或基于抢占)决定接下来应运行哪个作业的迭代。</em>T3】</strong></p><p id="9571" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">记住<strong class="je hi">更细粒度的调度也会给执行服务</strong>增加更多的开销。更精细的粒度是内核级调度，它可能会在每个内核不断增长的短暂内存上导致死锁。此外，这个级别打破了DL框架中常见的效率优化，如内核批处理和流水线。相反，迭代粒度允许避开渐进记忆的问题。这是因为所有短暂的分配都是在每次迭代后由框架释放的，而模型和框架内部的分配在迭代中保持不变。</p><h2 id="1019" class="lk if hh bd ig ll lm ln ik lo lp lq io jn lr ls is jr lt lu iw jv lv lw ja lx bi translated">GPU通道</h2><p id="1b42" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者使用lane来指代每个GPU的短期内存块。永久存储器被设置并放在一边。短暂区域被分成通道，这些通道是可以包含用于迭代的短暂内存分配的连续内存空间。</p><p id="17fc" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">车道不仅仅关乎记忆。迭代执行在通道内序列化，并行性在通道间实现，这是使用GPU流(并发)实现的。碎片整理发生在每次迭代结束时，这是选择迭代粒度的结果。下图显示了当小型作业停止时，它的通道在迭代边界处被分配到它下面的作业快速回收。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ly"><img src="../Images/d4635d316564d1b752d67286ceb2e36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvCsnDhQyjhK1kXO9V141A.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[1]</figcaption></figure><p id="67ab" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">Salus使用启发式算法来确定GPU中通道的大小和数量，以及如何将通道分配给作业。从最高级别开始，该算法尝试打开新的通道，使用现有的通道，重新组织现有作业的通道分配，以减小短暂区域的大小。你可以查一下文[1]里的算法。</p><h2 id="ffe9" class="lk if hh bd ig ll lm ln ik lo lp lq io jn lr ls is jr lt lu iw jv lv lw ja lx bi translated">Salus中的调度策略</h2><p id="6c76" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Salus使用PACK来提高资源利用率，使用SRTF来防止HOL阻塞问题，使用FAIR来均衡并发作业的资源份额。在包策略中考虑安全条件以防止崩溃，确保持久和短暂区域永远不会碰撞。作者提到他们认为执行时间是已知的，因此实现STRF是可能的。其他方法试图估计执行时间[ <strong class="je hi"> 2 </strong> ]。</p><h1 id="2a46" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">估价</h1><p id="72db" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Salus与TensorFlow集成，并使用一组训练、超参数调整和推理工作负载进行评估。</p><p id="5b78" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">基线:集群和MPS中使用的FIFO调度。</p><ul class=""><li id="9164" class="ka kb hh je b jf ko jj kp jn lz jr ma jv mb jz mc kg kh ki bi translated">使用SRTF策略的性能优于FIFO 3.18倍</li><li id="f64f" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz mc kg kh ki bi translated">通过在超参数调优期间运行多个DL作业，GPU利用率提高了2.38倍。</li><li id="4203" class="ka kb hh je b jf kj jj kk jn kl jr km jv kn jz mc kg kh ki bi translated">GPU利用率比不共享和MPS分别提高了42倍和7倍。</li></ul><p id="9d45" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">他们在英特尔CPU和两个英伟达P100 GPU上运行。</p><h1 id="e666" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Salus的开销</h1><p id="7794" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者提到，所提出的机制的开销是，一些DL模型，如自动编码器和超分辨率，除了繁重的GPU计算之外，还执行大部分CPU处理。由于Salus实现了它的执行引擎，所以CPU计算也被重定向并发送给Salus执行，这一点没有得到很好的优化。</p><h1 id="8859" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="7e8c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">虽然GPU在DL应用中无处不在，但它们的利用率很低。这是因为DL应用程序的性质和GPU缺乏细粒度的共享原语。Salus提出了一种新的机制来缓解这些问题，它通过在迭代粒度上进行调度并保存每个模型的持久数据来实现快速作业切换。对于要求高CPU计算和GPU计算的应用程序来说，它以一些开销实现了相当大的改进。</p><h1 id="81cf" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="3497" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">[1]余、裴峰和莫沙拉夫·乔杜里。"<strong class="je hi"> Salus:面向深度学习应用的细粒度GPU共享原语</strong>"<em class="kt"> arXiv预印本arXiv:1902.04610</em>(<strong class="je hi">2019</strong>)。</p><p id="a5d5" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">彭，等。<strong class="je hi"> Optimus:一种高效的深度学习集群动态资源调度器。</strong><em class="kt">第十三届EuroSys会议记录</em>。<strong class="je hi"> 2018 </strong>。</p><div class="md me ez fb mf mg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hi fi z dy ml ea eb mm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu le mg"/></div></div></a></div><p id="8758" class="pw-post-body-paragraph jc jd hh je b jf ko jh ji jj kp jl jm jn kq jp jq jr kr jt ju jv ks jx jy jz ha bi translated">🔵<a class="ae mv" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="je hi">成为作家</strong> </a></p></div></div>    
</body>
</html>