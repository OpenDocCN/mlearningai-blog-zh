<html>
<head>
<title>Tuning Neural Networks Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调整神经网络第二部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69?source=collection_archive---------2-----------------------#2021-10-30">https://medium.com/mlearning-ai/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69?source=collection_archive---------2-----------------------#2021-10-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2c24" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">初始化的注意事项</h2></div><p id="5035" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本系列旨在通过研究调整参数如何影响神经网络的学习内容和学习方式，提供对神经网络的深入理解。内容假设了一些神经网络的先验知识，可以通过阅读 <a class="ae jt" rel="noopener" href="/@gallettilance/neural-networks-a-very-simple-derivation-from-logistic-regression-b2b972f29138"> <em class="js">本系列</em> </a> <em class="js">获得。</em></p><p id="fcc6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8">第一部分:规范化数据的重要性</a></p><p id="0f60" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-iii-43dfd0c8600f">第三部分:哪些激活功能让你学会了</a></p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="7f42" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们从下面的网络开始，其中一个隐藏层包含六个神经元，所有神经元都用<strong class="iy hi"> ReLU </strong>激活。让我们称这个<strong class="iy hi">网络为</strong>:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/1aa4908b063cc8056bb8e9305ed33494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-oHzc_iakAUx6EjaRCkdmw.png"/></div></div></figure><p id="968d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了简单起见，在整篇文章中，我们将使用<strong class="iy hi"> ReLU </strong>激活函数激活隐藏的神经元(上面绿色部分),因为如果激活的输入大于0，神经元将被激活，否则将被禁用。</p><h1 id="b2e6" class="kn ko hh bd kp kq kr ks kt ku kv kw kx in ky io kz iq la ir lb it lc iu ld le bi translated">非中心数据与非中心权重</h1><p id="2579" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">当所有数据为正，权重以0为中心时，一个神经元是否激活(即WX是否&gt; 0)完全取决于权重。由于所有的数据都是正的，我们可以预计一个神经元大约有50%的时间被激活。第一部分中的模拟显示了神经元是如何被全部数据或没有数据激活的，很少被介于两者之间的任何数据激活。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lk"><img src="../Images/a3c24aba0bb3f431e30248496caa03c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qHXiVa9-ihcxS3KprZ36Dg.png"/></div></figure><p id="47af" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果神经元激活，正权重的梯度将总是正的(因为数据也总是正的)，负权重的梯度将总是负的。这严重限制了网络的学习能力，因为权重在训练期间的移动方向是由其初始值决定的。</p><p id="4c94" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当数据以0为中心但权重都为正时，神经元是否激活(即WX是否&gt; 0)完全取决于数据。由于所有权重都是正的，我们可以预期神经元会基于固定的50%数据块激活。这意味着要么所有神经元都被激活，要么都不被激活。很少能找到只让少数神经元激活的权重组合。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lk"><img src="../Images/116f967cd9e306768843644349cd4f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*YZBnCtKmhMryetWixHki7Q.png"/></div></figure><p id="ab6e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这也限制了网络的学习能力，因为梯度的方向仅由数据决定。</p><p id="4e6f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">只要数据和权重都包含0，神经元是否激活就取决于权重和数据。这意味着梯度能够在不同方向上更自由地更新权重，因为它们可以是正的或负的，而与初始化无关。</p><h1 id="481e" class="kn ko hh bd kp kq kr ks kt ku kv kw kx in ky io kz iq la ir lb it lc iu ld le bi translated">为什么要包含偏见术语</h1><p id="d288" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">即使当权重和数据都集中在一起时，让神经元在随机的50%的数据上激活仍然显得僵硬。一个网络包含的神经元越多，我们可能会在学习的特征中看到越多的重叠和重复。</p><p id="8dab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">进一步分而治之，我们或许能更有效地学习。如果每个神经元获得随机比例的数据会怎么样？</p><p id="a766" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是怎么做呢？</p><p id="ac1b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们包括一个偏见条款！</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ll"><img src="../Images/236ac25751124d7dc20c9bd22b5c45eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*eMVEmSnvPBYzFf8SQRW8iA.png"/></div></figure><p id="16ff" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">突然之间，使给定神经元激活的数据比例变得更加灵活，介于0到100%的数据之间。</p><p id="92ea" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">将偏置从0移开导致网络中神经元的系统激活或去激活:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ll"><img src="../Images/38693d862fa3211411aa7e635447bfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/1*kdhJekaKBb7TFrA21h5OwQ.gif"/></div></figure><p id="7237" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是需要注意的事情，因为如果平均偏差过大或过小，神经元将被冻结。</p><h1 id="663b" class="kn ko hh bd kp kq kr ks kt ku kv kw kx in ky io kz iq la ir lb it lc iu ld le bi translated">方差呢？</h1><p id="43e5" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">到目前为止，我们已经讨论了在训练前集中数据、权重和偏差。让我们看看当它们的分布变化时会发生什么。</p><p id="0934" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">改变偏差分布的方差(从而改变其可能取值的范围)允许我们调整分布的尾端:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ll"><img src="../Images/fa6c7d7626ecca5a31f2f5bddee79e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/1*ucoRgm-MNnIUZoWuFFvqGQ.gif"/></div></figure><p id="a41c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">即使以0为中心，我们也需要注意数据、权重和偏差的可能值范围。</p><p id="ff84" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果初始化时数据和权重的方差(范围)太大，则梯度可能太大，并且网络可能超过最小成本。这类似于在梯度下降期间选择过大的步长。</p><p id="df62" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">相反，如果范围太小，梯度可能会非常小，从而阻碍网络的学习能力，或者权重初始化将非常接近常数(或在这种情况下为零初始化)，这将冻结模型，因为所有梯度都是相同的。</p><p id="9d3a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果偏差的方差(范围)太高，我们将看到神经元完全打开或关闭。基于初始化时数据的50%来激活一个神经元将是罕见的。随着方差的减小，偏置的初始化变得更接近常数，这对于学习来说不像常数初始化对于权重那样有害。</p><h1 id="01d7" class="kn ko hh bd kp kq kr ks kt ku kv kw kx in ky io kz iq la ir lb it lc iu ld le bi translated">许多隐藏层</h1><p id="4143" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">现在，考虑以下网络:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/3f37ebed38f9b56c0867d219757b6046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWh-sGsepYxXrV3pFLL6ug.png"/></div></div></figure><p id="826d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个网络就是上一个例子中的网络A，但是前面增加了一层。我们在上面获得的关于网络A的直觉仍然适用，但是现在<strong class="iy hi">网络A部分的“数据”是第一层</strong>的激活输出。</p><p id="d2f9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第二隐藏层(具有六个神经元)的“数据”是第一隐藏层(具有两个神经元)的输出。如果那个“数据”不是以零为中心的，我们会遇到我们在<a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8">第一部分</a>中学到的同样的问题。</p><p id="ee74" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">例如，sigmoid和ReLU输出不是以零为中心的</strong>，并且总是正的。因此，不推荐在隐藏层中使用<strong class="iy hi"> sigmoid </strong>或<strong class="iy hi"> ReLU </strong>激活，因为它可能在初始化时冻结网络更深层中的神经元，因为所学习的特征实际上是恒定的。<strong class="iy hi"> Sigmoid </strong>在更深的层中也有消失梯度的问题。</p><p id="ad49" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是训练后的网络激活。数据、权重和偏差都在初始化时居中，所有隐藏层都被<strong class="iy hi"> ReLU </strong>激活。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lm"><img src="../Images/755ebddbb932eb4fe8de0c75133660a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/1*YDTb9u0OJD6b46kQ10mTWw.gif"/></div></figure><p id="b200" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第一层似乎是随机激活的。然后，随着我们在网络中前进，神经元变得越来越冻结。在这种情况下，使用较少的层会对<strong class="iy hi"> ReLU </strong>更好。</p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><h1 id="f9b8" class="kn ko hh bd kp kq ln ks kt ku lo kw kx in lp io kz iq lq ir lb it lr iu ld le bi translated">结论</h1><p id="69cd" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">即使使用以零为中心的数据，网络在初始化时仍可能冻结，如果:</p><ol class=""><li id="2026" class="ls lt hh iy b iz ja jc jd jf lu jj lv jn lw jr lx ly lz ma bi translated">权重和偏差不是以零为中心的。</li><li id="d5a1" class="ls lt hh iy b iz mb jc mc jf md jj me jn mf jr lx ly lz ma bi translated">数据、权重或偏差的方差(范围)太大。</li><li id="4dfb" class="ls lt hh iy b iz mb jc mc jf md jj me jn mf jr lx ly lz ma bi translated">非零中心激活函数用于网络的深层隐藏层。</li></ol><p id="bc5b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，请考虑:</p><ol class=""><li id="5ba9" class="ls lt hh iy b iz ja jc jd jf lu jj lv jn lw jr lx ly lz ma bi translated">包括使网络中的重复工作随机化并尽可能最小化的偏置项。</li><li id="0f51" class="ls lt hh iy b iz mb jc mc jf md jj me jn mf jr lx ly lz ma bi translated">在隐藏层中使用以零为中心的激活来帮助抵消先前层的非正常化或保持整个层的正常化。</li><li id="ef0f" class="ls lt hh iy b iz mb jc mc jf md jj me jn mf jr lx ly lz ma bi translated">数据、权重和偏差都是相互关联的，必须在初始化时一起仔细检查。</li><li id="80d9" class="ls lt hh iy b iz mb jc mc jf md jj me jn mf jr lx ly lz ma bi translated">使用多个隐藏层时使用批量规范化。</li></ol></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="9bbe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-iii-43dfd0c8600f">第三部分:哪些激活功能让你学会了</a></p><h1 id="14aa" class="kn ko hh bd kp kq kr ks kt ku kv kw kx in ky io kz iq la ir lb it lc iu ld le bi translated">感谢</h1><p id="3a96" class="pw-post-body-paragraph iw ix hh iy b iz lf ii jb jc lg il je jf lh jh ji jj li jl jm jn lj jp jq jr ha bi translated">感谢杨易进、卡梅隆·加里森、玛利亚·舍甫琴科、詹姆斯·昆斯特勒、林、克里斯蒂娜·徐、的贡献。</p></div></div>    
</body>
</html>