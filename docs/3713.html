<html>
<head>
<title>Adversarial Machine Learning — Fight Against Attackers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性机器学习——对抗攻击者</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/adversarial-machine-learning-fight-against-attackers-e38d5f0917f6?source=collection_archive---------8-----------------------#2022-10-11">https://medium.com/mlearning-ai/adversarial-machine-learning-fight-against-attackers-e38d5f0917f6?source=collection_archive---------8-----------------------#2022-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2680" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，你将概述什么是<strong class="ig hi">对抗性机器学习</strong>以及如何在具体案例中实现它。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/ca8a5f9a889a780b5baa11fd39e1502e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3TEzcG_A2jW1rWUV"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@ngeshlew?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lewis Kang'ethe Ngugi</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5289" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">应用机器学习模型的现实世界案例必须应对它们用于学习的数据的变化。事实上，模型从训练数据集<strong class="ig hi">中学习的规则随着时间的推移</strong>会有所不同。此训练数据集必须更新以保持相关性，并允许模型在其预测工作中有效。</p><p id="93b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对抗性机器学习就是在这种背景下出现的。随着每天都有新的创新出现，迫使公司专注于它们与市场的相关性，机器学习模型面临着类似的事情，因为它们最有价值的支持，<strong class="ig hi">使它们产生的数据</strong>，是<strong class="ig hi">不稳定</strong>和<strong class="ig hi">变化</strong>。</p><p id="1a43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，在机器学习中，这种使模型过时的数据转换可能不是自然或偶然的，而是来自我们所谓的<strong class="ig hi">攻击者</strong>。这个想法是，人类知道面对一个有缺陷的模型，将通过转换数据集或简单地通过理解它正在做什么来最终摆脱它来找到缺陷。</p><p id="5628" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对抗性机器学习</strong>的概念可以看做:</p><blockquote class="jt"><p id="713d" class="ju jv hh bd jw jx jy jz ka kb kc jb dx translated">一个最小最大博弈，其中攻击者的目标是最大化其欺骗分类器的可能性，而分类器的目标是最小化其在这种最坏情况下的风险。</p></blockquote><p id="95f3" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip kf ir is it kg iv iw ix kh iz ja jb ha bi translated">在任何矛盾的情况下，<strong class="ig hi">防守方</strong>面对试图欺骗模型的<strong class="ig hi">进攻方</strong>。</p><p id="a99d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们可以通过其目标来描述对抗性ML，其目标是:</p><blockquote class="jt"><p id="b89d" class="ju jv hh bd jw jx jy jz ka kb kc jb dx translated"><em class="ki">正式调查在敌对环境中使用机器学习技术引入的问题，在这种环境中，聪明的对手试图利用这些技术中的弱点。</em></p></blockquote><p id="b497" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip kf ir is it kg iv iw ix kh iz ja jb ha bi translated">现在，让我们举例说明在不同类型的问题中…</p><h1 id="d2d7" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">对抗性机器学习的例子</h1><p id="357d" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">在进入攻击者和防御者的现有方法之前，让我们看看机器学习的主要子领域中的敌对上下文是什么样的:监督、非监督和强化。</p><h2 id="4f66" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">在监督学习问题中</h2><p id="8795" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated"><strong class="ig hi">监督学习问题</strong>是那些利用标签的帮助，在这些标签上进行模型化训练的问题。这些模型由与每个数据样本相关联的标签来支持，这就是为什么我们称它们为受监督的。主要有两种，其中最常见的一种:<strong class="ig hi">回归</strong>问题和<strong class="ig hi">分类</strong>。如果你对这些主题感兴趣，我为每一个主题写了一篇文章<a class="ae js" rel="noopener" href="/mlearning-ai/the-well-known-regression-what-is-it-6dbe5e0b2cd1">这里</a>和<a class="ae js" rel="noopener" href="/mlearning-ai/understand-classification-problems-in-less-than-5-minutes-712244d259c8">这里</a>。</p><p id="fa90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi">回归</strong>中，当预测的目标可以被控制或操纵时，会出现敌对的环境。例如，如果在短时间内大量买入或卖出，股票的价格就会发生变化。这个过程可以导致一个模型，基于股票价格演变的某些特征，来决定购买或出售所讨论的股票。然而，模型和它实现的动作都被欺骗了，因为攻击者可以预测这次购买或销售。这显然是一个<strong class="ig hi">对抗性的机器学习问题</strong>，因为<strong class="ig hi">攻击者会识别出一个模型是如何工作的，并通过操纵股票市场</strong>来触发它。</p><p id="853a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi">分类</strong>问题中，这样的上下文是众所周知的，例如，<strong class="ig hi">垃圾邮件</strong>或<strong class="ig hi">恶意软件</strong> <strong class="ig hi">检测</strong>，其中的想法是通过查看使用的单词(垃圾邮件)或程序的行为(恶意软件)来识别新的攻击。在这两种情况下，我们面对的是一个面对攻击者的模型。<strong class="ig hi">信用卡欺诈检测</strong>是另一种类型的分类对抗示例，其中一些个人制造漏洞，试图不被训练有素的模型检测到。</p><h2 id="e9fc" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">在无监督学习问题中</h2><p id="4605" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">与有监督的问题相比，<strong class="ig hi">无监督的</strong>问题不使用标签来学习模型，而是旨在识别模式或对数据样本进行分组，以便回答问题或获得分析元素。</p><p id="9ee5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们发现像<strong class="ig hi">群集</strong> <strong class="ig hi">恶意软件</strong>这样的问题，例如能够发现一个程序是否接近任何类别的恶意软件。<strong class="ig hi">异常</strong> <strong class="ig hi">检测</strong>是另一个无监督对抗环境的例子，其想法是有一个系统来检查行为是正常的还是有分歧的。这可以应用于各种领域。</p><h2 id="6c8f" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">在强化学习问题中</h2><p id="4525" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated"><strong class="ig hi">强化</strong> <strong class="ig hi">学习</strong>是一个机器学习子领域，其思想是拥有一个通过犯错来学习的模型。事实上，这是一个有奖励和约束的博弈情况，允许模型(也称为代理)知道什么是好的或坏的，以便解决问题。它从行动中获得的回报越多，它就越能推进解决方案并学到更多东西。因此，它将学习人类无法识别的模式，这就是强化学习的全部力量。</p><p id="82ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">强化学习问题是对抗性机器学习环境的明显例子，因为该模型面临惩罚。更准确地说，<strong class="ig hi"> Self </strong> <strong class="ig hi"> Driving </strong>问题是模型针对攻击者的情况的很好的例子，因为汽车中的模型看到的任何未被识别的元素都可能与攻击相关联。在这些问题中，试图预测这种不可预测的情况是防御方必须实施的防御策略。</p><p id="7e1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们深入具体问题，看看如何从攻击者和防御者两方面来描述对抗性问题…</p><h1 id="4873" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">进攻与防守策略</h1><p id="b173" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">攻击策略是何时以及如何开发的？防御者如何反击这些攻击？</p><h2 id="54a0" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">攻击策略</h2><p id="c062" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">在文献中，不同的策略通过它们的维度来区分。其中一些攻击的特征是攻击模型的时间，另一些攻击的特征是攻击目标系统的信息，最后，还有一些攻击的目的是不同的。</p><ol class=""><li id="a409" class="ma mb hh ig b ih ii il im ip mc it md ix me jb mf mg mh mi bi translated"><strong class="ig hi">计时</strong> <strong class="ig hi">区别</strong>:攻击计数的时刻。我们从本质上根据对算法的攻击的时间来区分对模型的<strong class="ig hi">攻击和对算法</strong>的<strong class="ig hi">攻击。事实上，对模型的攻击也被称为对决策时间的<strong class="ig hi">攻击</strong>，因为这是模型实际进行预测的时刻:例如，预测电子邮件是否是垃圾邮件的垃圾邮件检测器在接收电子邮件时起作用。大多数情况下，这些攻击也被称为<strong class="ig hi">规避攻击</strong>。<br/>换句话说，<strong class="ig hi">对算法的攻击</strong>是对训练数据的攻击，因此是在模型被训练之前进行的。因此，学习被破坏，并且最终模型结合了坏的模式(<em class="mj">训练数据的对抗性破坏</em>)。<strong class="ig hi">投毒攻击</strong>或<strong class="ig hi">标签翻转</strong> <strong class="ig hi">攻击</strong>都是攻击算法的例子。</strong></li><li id="9b17" class="ma mb hh ig b ih mk il ml ip mm it mn ix mo jb mf mg mh mi bi translated"><strong class="ig hi">信息</strong> <strong class="ig hi">区别</strong>:攻击者所拥有的关于其目标模型/算法的信息。<strong class="ig hi">白盒攻击</strong>与<strong class="ig hi">黑盒</strong> <strong class="ig hi">黑盒</strong> <strong class="ig hi">攻击</strong>并行。第一组代表完全了解模型(特征、参数……)的攻击，而第二组代表几乎没有信息的攻击。</li><li id="b464" class="ma mb hh ig b ih mk il ml ip mm it mn ix mo jb mf mg mh mi bi translated"><strong class="ig hi">目标区分</strong>:攻击的目的可以根据目标的不同而不同。的确，我们谈论的是<strong class="ig hi">有针对性的</strong>和<strong class="ig hi">无针对性的</strong> <strong class="ig hi">攻击</strong>。一方面，攻击者的目标是改变模型本身的预测。他们知道新的输出，例如，如果一个应用程序正在对汽车图像进行分类，攻击将改变图像，使其被分类到特定的汽车。对于无目标攻击，对图像执行的扰动将使得模型是错误的，但是没有有意的标签。</li></ol><h2 id="7925" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">防御策略</h2><p id="048f" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">对所有类型模型的大量攻击的到来迫使防御措施的发展。这是一项非常困难的任务，因为攻击者有各种各样的可能性。以下是我们可以在文献中找到的一些防御策略:</p><ul class=""><li id="991b" class="ma mb hh ig b ih ii il im ip mc it md ix me jb mp mg mh mi bi translated"><strong class="ig hi">鲁棒性</strong>:给模型方程增加约束，使得学习变得不那么复杂，最终由领域内的专家指导。这种防御面临着会改变模型学习内容的中毒攻击。这些约束(<em class="mj">正则化</em>)提供了一种平滑效果，防止攻击者利用复杂的模式。</li><li id="376c" class="ma mb hh ig b ih mk il ml ip mm it mn ix mo jb mp mg mh mi bi translated"><strong class="ig hi">检测攻击</strong>:顾名思义，这个想法是建立测试集来验证模型的分类，并在异常时发出警告。</li><li id="222c" class="ma mb hh ig b ih mk il ml ip mm it mn ix mo jb mp mg mh mi bi translated"><strong class="ig hi">虚假信息</strong>:向攻击者发送嘈杂的信息以制造混乱，阻止他们理解分类器的实际工作原理。我们可以讨论蜜罐，它可以检测攻击者并为防御方提供优势。</li><li id="1caf" class="ma mb hh ig b ih mk il ml ip mm it mn ix mo jb mp mg mh mi bi translated"><strong class="ig hi">随机化</strong>:将目标分类器的决策边界随机化，这样攻击者每次运行流程时都必须理解模型。</li></ul><h1 id="ebb6" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">结论</h1><p id="f9ae" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">机器学习模型的真实世界实现是<strong class="ig hi">许多攻击的目标</strong>只要模型有所了解。这创造了对抗性机器学习的环境，其中攻击者试图操纵和破坏ML系统的功能。</p><p id="b87a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这在许多领域都有发现，例如异常检测、自动驾驶系统、欺诈检测等等。由于攻击策略的想象力丰富，防守方不得不努力保持其模式不变。</p><p id="5328" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢它，并发现什么是对抗性机器学习！如果你对数据科学和机器学习感兴趣，请点击查看我的其他文章<a class="ae js" href="https://www.npogeant.com/#articles" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="658e" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">资源</h2><div class="mq mr ez fb ms mt"><a href="https://www.semanticscholar.org/paper/Security-Matters%3A-A-Survey-on-Adversarial-Machine-Li-Zhu/6ede8b02b817a1354b8bde1ab7af07b0ddb02acf" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">[PDF]安全问题:对抗性机器学习的调查|语义学者</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">本文旨在全面介绍对抗性深度学习主题的一系列方面</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.semanticscholar.org</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh jm mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://dl.acm.org/doi/10.1145/1128817.1128824" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">机器学习能安全吗？2006年美国计算机学会信息、计算机…</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">机器学习系统在处理各种应用程序中不断变化的输入方面提供了无与伦比的灵活性，例如…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">dl.acm.org</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh jm mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://www.morganclaypool.com/doi/abs/10.2200/S00861ED1V01Y201806AIM039" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">对抗性机器学习</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">大型高质量数据集的日益丰富，加上过去几年的重大技术进步…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.morganclaypool.com</p></div></div><div class="nc l"><div class="nj l ne nf ng nc nh jm mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">medium.com</p></div></div><div class="nc l"><div class="nk l ne nf ng nc nh jm mt"/></div></div></a></div></div></div>    
</body>
</html>