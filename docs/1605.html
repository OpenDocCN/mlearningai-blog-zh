<html>
<head>
<title>paper review: “BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文综述:“BART:自然语言生成、翻译和理解的去噪序列间预训练”</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe?source=collection_archive---------0-----------------------#2022-01-11">https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe?source=collection_archive---------0-----------------------#2022-01-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="ea0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">https://arxiv.org/abs/1910.13461</p><h1 id="e956" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">要点</h1><ul class=""><li id="1ea6" class="kb kc hh ig b ih kd il ke ip kf it kg ix kh jb ki kj kk kl bi translated">提出名为BART的自回归模型，它在结构上类似于标准的变压器编码器+解码器</li><li id="bda3" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">检查5个预训练任务，并试验哪个预训练任务最有帮助</li><li id="79e4" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">通过对下游任务进行大规模预处理来测试BART性能</li></ul></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><h1 id="d387" class="jd je hh bd jf jg ky ji jj jk kz jm jn jo la jq jr js lb ju jv jw lc jy jz ka bi translated">模型架构</h1><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ld"><img src="../Images/222a77215d3b88cd41688f7d12ac323f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*CaRevRvgejf4TEJVBDBK_A.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">encoder and decoder of standard transformer architecture</figcaption></figure><p id="08a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这项工作引入了BART，它基本上与标准序列间变压器架构几乎相同，但有一些修改</p><ul class=""><li id="f328" class="kb kc hh ig b ih ii il im ip lp it lq ix lr jb ki kj kk kl bi translated">用格鲁代替RELU</li><li id="ae69" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">不同层大小<br/> -基础版:编码器6层，解码器6层<br/> -大版:编码器12层，解码器12层</li><li id="617f" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">每个解码器层另外执行来自编码器的最终隐藏层的输出的交叉注意。</li><li id="ac50" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">在末端没有前馈网络</li></ul></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><p id="4318" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然模型架构非常简单，但这项工作的主要贡献是对各种预处理任务进行了精心的实验。而许多其他论文都是关于“哦，我们和其他人一起使用了这个预训练任务，获得了更好的性能！哇”，这篇文章更多的是关于“在所有这些准备工作中，哪些<strong class="ig hi">真的</strong>有用和有效？”</p><h1 id="de00" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">训练前任务</h1><p id="b506" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">任务都是关于从文档损坏中恢复。使用了五种类型的“噪声”方法。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/8807f9b0b1606c16d0cc64b312e08543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXmmQ2jmD2rcAwpXFdK9EQ.png"/></div></div></figure><h1 id="9ca4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">令牌屏蔽</h1><p id="874b" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">追随伯特。</p><h1 id="82cf" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">令牌删除</h1><p id="22fd" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">删除令牌并使模型在正确的位置恢复被删除的令牌。</p><h1 id="254d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">文本填充</h1><p id="03f2" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">在一个跨度中选择多个单词，并用单个掩码标记替换。这将教会模型预测有多少令牌丢失。</p><h1 id="162e" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">句子排列</h1><p id="0182" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">打乱句子，让模型恢复它们。</p><h1 id="f009" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">文档旋转</h1><p id="f4d0" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">选择随机令牌。将文档排序更改为从所选令牌开始。让模型预测原始文档的开始。</p></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><h1 id="b060" class="jd je hh bd jf jg ky ji jj jk kz jm jn jo la jq jr js lb ju jv jw lc jy jz ka bi translated">微调</h1><p id="d26a" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">为了评估模型性能(并最终发现哪些预训练任务对性能的贡献最大)，可以使用以下下游任务进行微调。</p><h1 id="c6f1" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">序列分类</h1><p id="e40a" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">相同的输入序列被馈送到编码器和解码器。</p><p id="566d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最终解码器令牌的最终隐藏状态用于分类。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ma"><img src="../Images/b41e64e7b9f2a2bd9b0ef8945e798da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*piIJW0crs-5g7uhk2R3TEQ.png"/></div></figure><p id="76a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与BERT中的[CLS]令牌输出方法相似，但不同之处在于，在BART中，我们使用最终令牌的输出，因此输出令牌是关注所有先前输入令牌的结果。</p><h1 id="64b4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">令牌分类</h1><p id="54c8" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">向编码器和解码器提供输入序列。</p><p id="6e25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解码器的顶部隐藏状态被用作对每个相应的输入标记进行分类的表示。</p><h1 id="e580" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">序列生成</h1><p id="28af" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">输入序列馈入编码器，而解码器自回归产生输出。</p><h1 id="e30c" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">机器翻译</h1><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mb"><img src="../Images/7374fd5c37d55e1befbfa3b01cc603e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*bvI9aUmbCIXfNT97RTI-vw.png"/></div></figure><p id="aaa6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">乍一看，有人可能会问:这不就是“序列生成”任务的一部分吗？是的，但是这个特殊的任务采用了稍微不同的方法。</p><p id="5d99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它不是根据当前的下游任务(机器翻译)来微调BART模型本身，而是使用预训练的BART模型作为子模型，其中另一个小编码器连接到BART编码器。</p><p id="d3d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此配置旨在表明，通过为新语言的机器翻译任务添加小型前端编码器，可以将预先训练的BART模型本身作为一个整体加以利用。</p><p id="f49e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现有的BART的第一个编码器的嵌入层被替换为随机初始化的编码器，然后整个模型被端到端地训练。这个新的编码器可以使用独立于预训练的词汇表。</p><p id="f22f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在此配置中进行微调时，训练分为两个阶段。第一阶段将只训练新编码器的参数、BART位置嵌入和BART的第一编码器层的自我注意输入投影矩阵。在第二阶段，更新所有模型参数。</p><h1 id="8b9a" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">预训练任务比较</h1><p id="1d85" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">如前所述，在本节中，作者分析了哪些预处理任务是有效的。</p><p id="0a67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者做了这项调查，其中5个其他模型与之前提到的类似的预训练任务进行了比较，6个下游任务。</p><p id="8c08" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结果总结在下表中。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mc"><img src="../Images/cffdd74e8540f2b09042312a7f123ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9l9jX7Ked0OgQxfRkiRX1w.png"/></div></div></figure><p id="2971" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者从这个关于预训练任务的实验中得出的结论可以总结如下:</p><ul class=""><li id="d6c4" class="kb kc hh ig b ih ii il im ip lp it lq ix lr jb ki kj kk kl bi translated">不同任务的预训练方法的表现差异很大</li><li id="7007" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">令牌屏蔽至关重要</li><li id="d3ea" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">从左到右的预训练改善了生成</li><li id="0e03" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">双向编码器对小队至关重要</li><li id="12e2" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">训练前的目标不是唯一重要的因素。架构选择很重要，比如相对位置嵌入、段级递归等。</li><li id="b2e0" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">纯语言模型在ELI5下游任务上表现最好。</li><li id="4bdf" class="kb kc hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">BART实现了最稳定的强劲表现</li></ul><h1 id="6438" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">大规模预训练实验</h1><p id="8565" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">最近的工作表明，当大批量进行预训练时，下游任务性能显著提高。因此作者也测试了这种方法，但是仅使用文本填充和句子置换的组合，因为这种设置在预训练客观比较实验中表现出最佳性能。</p><p id="c644" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在大批量预训练之后，作者测试了下游任务的性能。</p><h1 id="59d4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">辨别任务</h1><p id="b5e3" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">总的来说，BART的表现与其他模型相似</p><h1 id="07e4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">生成任务</h1><p id="be11" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">总的来说，BART比以前的作品表现更好。特别是在高度抽象的XSum数据集上，它的性能明显优于。</p><p id="7e67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于对话，巴特胜过其他人。</p><p id="25ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于抽象的QA，BART比其他人做得更好，但即使对BART来说，这项任务仍然非常具有挑战性。</p><h1 id="cf6d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">机器翻译</h1><p id="4326" class="pw-post-body-paragraph ie if hh ig b ih kd ij ik il ke in io ip ls ir is it lt iv iw ix lu iz ja jb ha bi translated">它试验了一个预训练任务中提到的“机器翻译”设置，其中通过在前端添加一个全新的新编码器来重用预训练的BART。</p><p id="9ddf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该实验使用WMT16罗马尼亚语-英语数据集，与基线变压器架构进行比较。</p><p id="22e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结果表明，以这种方式使用BART是无效的。</p></div><div class="ab cl kr ks go kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="ha hb hc hd he"><p id="83c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="md">有关用于下游任务的数据集和测试结果的详细信息可在论文</em>中找到</p><div class="me mf ez fb mg mh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hi fi z dy mm ea eb mn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">medium.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv lj mh"/></div></div></a></div></div></div>    
</body>
</html>