<html>
<head>
<title>Handling Noisy Label Data with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用深度学习处理有噪声的标签数据</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/handling-noisy-label-data-with-deep-learning-ff986deedc76?source=collection_archive---------1-----------------------#2022-07-17">https://medium.com/mlearning-ai/handling-noisy-label-data-with-deep-learning-ff986deedc76?source=collection_archive---------1-----------------------#2022-07-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="665f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当你的数据集被粗略标记时，在这些数据上实际执行任何深度学习技术都不容易。结果不可信。然而，如果您非常确定您的大多数数据都被正确标记，但有些数据没有，那么您可以将数据输入到深度学习模型中，并使用噪声校正技术。因此，以下是处理高噪声标注数据集时可能需要参考的步骤:</p><h2 id="f136" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><span class="l jx jy jz bm ka kb kc kd ke di"> 1。</span> <strong class="ak">使用深度学习模型而不是传统的ML模型</strong></h2><p id="5ae3" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">从本质上讲，深度学习模型可以处理训练数据集中10–20%的噪声。然而，传统的机器学习模型，如XGBoost或任何基于树的模型，在用嘈杂的训练数据集进行训练时，会很快崩溃。XGBoost和LGBM等基于树的模型以其处理表格数据的强大性能而闻名。历史上，这些模型赢得了许多卡格尔比赛。然而，它们可能不太适合处理真实世界的噪声数据集。</p><p id="4fa9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下表显示了XGBoost在高噪声数据集上训练时的性能下降情况。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es kk"><img src="../Images/0266f2afc48c896618d6e424a287bc35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JjkefzYFk32LTjE8cI_1w.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Table 1</figcaption></figure><p id="24a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过将20%的噪声注入到干净的数据集中来构成有噪声的数据组。来自阳性类别(1)的样本被故意更改为阴性类别(0)。因此，数据集现在是用单侧噪声合成生成的。双侧噪声数据集是通过在两个类上插入噪声而创建的。大约15%的0类样品转化为1类样品，反之亦然。结果是数据集中30%的噪声。非常有趣的是，XGBoost在双侧噪声数据集上表现稍好，与单侧噪声数据集相比，双侧噪声数据集包含更多噪声。解释这种现象的假设是，单侧噪声仅将基于树的模型的决策边界移动到一侧，而双侧噪声数据集将在两侧移动边界，导致决策边界没有变化。从上表中可以看出，噪音对DNN的表现没有太大影响。然而，随着噪声被引入到训练数据集中，XGBoost的性能迅速恶化。</p><h2 id="96a0" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><span class="l jx jy jz bm ka kb kc kd ke di"> 2。</span>为验证和测试集创建干净的数据集</h2><p id="2f62" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">只要有足够的数据集，并且噪声在数据集中的比例不是压倒性的，深度学习模型就可以很好地处理训练数据集中的噪声。然而，我们需要一个100%干净的数据集来进行验证和测试，这样模型的性能就可以用一种值得信赖的方式来衡量。验证和测试集的大小不需要很大。它可能很小，但需要非常精确的标签。实际上，创建这样干净的数据集可能既困难又昂贵。如果我们对数据集无能为力，我们能做什么？谢天谢地，有办法处理这种情况。</p><h2 id="f631" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><span class="l jx jy jz bm ka kb kc kd ke di"> 3。</span>发挥损失功能</h2><p id="b831" class="pw-post-body-paragraph ie if hh ig b ih kf ij ik il kg in io ip kh ir is it ki iv iw ix kj iz ja jb ha bi translated">当您能够以某种方式构建准确的验证和测试集时，您有多种选择。例如，您可以最小化一组与评估程序一致的无偏倚的干净验证示例的损失。“<a class="ae la" href="https://arxiv.org/pdf/1803.09050.pdf" rel="noopener ugc nofollow" target="_blank">学习重新加权稳健深度学习的示例</a>”如果您能够构建一个没有噪声的验证数据集，那么这是一篇实现重新加权技术的出色论文。</p><p id="e03d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">处理复杂情况的一种方法是，利用一种称为<a class="ae la" href="http://proceedings.mlr.press/v97/shen19e/shen19e.pdf" rel="noopener ugc nofollow" target="_blank">迭代调整损失最小化</a> ( <strong class="ig hi"> ITLM </strong>)的技术，我们所拥有的只是一个粗略标记的数据集。ITLM的美妙之处在于它不需要任何确定的干净数据。本文的思想是在训练的早期阶段，干净样本的学习曲线应该比坏样本的学习曲线好。微调损失意味着共同选择<em class="lb"> αn </em>样本θ的子集，使得在子集和参数的所有选择中，该子集的损失最小。为了将损失降至最低，您可以在<strong class="ig hi">之间交替进行:( a)选择电流损失最小的样本</strong>和<strong class="ig hi">;( b)在这些电流损失最小的样本上重新训练模型</strong>。通过迭代执行微调损失，您可以在早期阶段过滤掉不良样本。您可以使用提前停止来更好地选择坏的样本，因为在以后的回合中，一切都会开始过度。</p><p id="0342" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是ITLM的张量流实现:</p><pre class="kl km kn ko fd lc ld le lf aw lg bi"><span id="1672" class="jc jd hh ld b fi lh li l lj lk">alpha = 0.95</span><span id="d297" class="jc jd hh ld b fi ll li l lj lk">class CustomModel(keras.Model):<br/>    def train_step(self, data):<br/>        # Unpack the data. Its structure depends on your model and<br/>        # on what you pass to `fit()`.<br/>        x, y = data<br/>        _len = y.shape[0]<br/>        _alpha = alpha<br/>        _idx = math.floor(_len*_alpha)</span><span id="ed75" class="jc jd hh ld b fi ll li l lj lk"># forward for get loss from every samples<br/>        y_pred = self(x, training=False)  # Forward pass<br/>        loss_ITLM = keras.losses.binary_crossentropy(y, y_pred)<br/>        loss_ITLM_np = loss_ITLM.numpy()<br/>        argsort = np.argsort(loss_ITLM_np)</span><span id="5119" class="jc jd hh ld b fi ll li l lj lk">        # get new training data<br/>        x, y = tf.gather(x, argsort[:_idx]), tf.gather(y, argsort[:_idx])</span><span id="c162" class="jc jd hh ld b fi ll li l lj lk">        with tf.GradientTape() as tape:<br/>           y_pred = self(x, training=True)  # Forward pass<br/>           # Compute the loss value<br/>           # (the loss function is configured in `compile()`)<br/>           loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)</span><span id="3985" class="jc jd hh ld b fi ll li l lj lk">        # Compute gradients<br/>        trainable_vars = self.trainable_variables<br/>        gradients = tape.gradient(loss, trainable_vars)<br/>        # Update weights<br/>        self.optimizer.apply_gradients(zip(gradients, trainable_vars))<br/>        # Update metrics (includes the metric that tracks the loss)<br/>        self.compiled_metrics.update_state(y, y_pred)<br/>        # Return a dict mapping metric names to current value<br/>        return {m.name: m.result() for m in self.metrics}</span></pre><p id="772c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，α是超参数，所用纸张是0.95。</p><p id="c537" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以构建自己的深度学习模型。我建立了四层DNN模型，在每个完全连接的层之间使用批处理规范化。</p><pre class="kl km kn ko fd lc ld le lf aw lg bi"><span id="1c34" class="jc jd hh ld b fi lh li l lj lk">input_layer = Input(shape=(9,), name='input_layer')</span><span id="efdd" class="jc jd hh ld b fi ll li l lj lk">Dense_1 = Dense(8, activation = 'relu', kernel_initializer = HeNormal(seed=1))(input_layer)</span><span id="d869" class="jc jd hh ld b fi ll li l lj lk">batch_norm = BatchNormalization(momentum = 0.9)(Dense_1)</span><span id="547c" class="jc jd hh ld b fi ll li l lj lk">Dense_2 = Dense(5, activation = 'relu', kernel_initializer = HeNormal(seed=1))(batch_norm)</span><span id="ffa8" class="jc jd hh ld b fi ll li l lj lk">batch_norm_2 = BatchNormalization(momentum = 0.9)(Dense_2)</span><span id="9885" class="jc jd hh ld b fi ll li l lj lk">Dense_3 = Dense(4, activation = 'relu', kernel_initializer = GlorotUniform(seed=1))(batch_norm_2)</span><span id="763e" class="jc jd hh ld b fi ll li l lj lk">batch_norm_3 = BatchNormalization(momentum = 0.99)(Dense_3)</span><span id="18bc" class="jc jd hh ld b fi ll li l lj lk">Dense_4 = Dense(2, activation = 'relu', kernel_initializer = HeNormal(seed=1))(batch_norm_3)</span><span id="a54d" class="jc jd hh ld b fi ll li l lj lk">batch_norm_4 = BatchNormalization(momentum = 0.9)(Dense_4)</span><span id="6557" class="jc jd hh ld b fi ll li l lj lk">output_layer = Dense(1, activation='sigmoid', kernel_initializer = GlorotUniform(seed=1))(batch_norm_4)</span></pre><p id="d487" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，编译您配备ITLM的定制模型。在TensorFlow中，您可以通过将“<em class="lb">run _热切</em>”设置为true来调试您的模型。</p><pre class="kl km kn ko fd lc ld le lf aw lg bi"><span id="3ab9" class="jc jd hh ld b fi lh li l lj lk">model = CustomModel(input_layer, output_layer)</span><span id="1351" class="jc jd hh ld b fi ll li l lj lk">model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001),  metrics=['acc', 'Recall', 'AUC'], run_eagerly=True)</span></pre><p id="c8ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下表总结了在单侧噪声数据集上训练和测试的结果(请注意，该数据集与表1不同)。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es lm"><img src="../Images/29f8e293409b547cc968d483736e7252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*ZJ7Zl74o7rRMRj65suUqRw.png"/></div></figure><p id="d956" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结果表明，利用ITLM提高了DNN模型的整体准确性和F1值！当面临粗标注数据集问题时，ITLM等技术可以极大地帮助您的模型从嘈杂的数据集设置中学习。</p><p id="e31e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lb">参考文献<br/> </em>任，m，曾，w，杨，b，&amp;乌尔塔松，R. (2018)。学习重新加权健壮深度学习的例子。<em class="lb"> ICML </em></p><p id="f192" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">沈和桑哈维(2019)。通过迭代修整损失最小化用坏训练数据学习。<em class="lb"> ICML </em>。</p><div class="ln lo ez fb lp lq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hi fi z dy lv ea eb lw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">medium.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ku lq"/></div></div></a></div></div></div>    
</body>
</html>