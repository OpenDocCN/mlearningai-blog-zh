<html>
<head>
<title>Text Messages Classification using LSTM, Bi-LSTM, and GRU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTM、双LSTM和GRU的文本消息分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad?source=collection_archive---------0-----------------------#2022-08-21">https://medium.com/mlearning-ai/the-classification-of-text-messages-using-lstm-bi-lstm-and-gru-f79b207f90ad?source=collection_archive---------0-----------------------#2022-08-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8ecd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文本分类也称为文本标记或文本分类，是将文本分类到有组织的组中的过程。通过使用自然语言处理(NLP)，文本分类器可以自动分析文本，然后根据其内容分配一组预定义的标签或类别。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/735ccf812c0cf8a75799d947e9d2841c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LUWni7VEjvS4o-bwPtRStQ.png"/></div></div></figure><p id="1a23" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文旨在建立一个二元分类模型来检测哪些短信是垃圾短信，哪些不是垃圾短信。此外，我们还增加了一个部分来预测或检测使用短信的垃圾邮件，这是我们从来没有见过的。</p><p id="9f71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用来自<a class="ae jo" href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" rel="noopener ugc nofollow" target="_blank">UCL</a>的公共数据集。它包含5.574条手机短信。这些数据是为了手机垃圾邮件研究的目的而收集的。这些数据被标记为垃圾邮件或非垃圾邮件(ham)。</p><p id="3f39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用密集分类器、长短期记忆(LSTM)、双向长短期记忆(双LSTM)和门控循环单元(GRU)作为我们的方法，并根据模型性能比较所有这些方法。</p><p id="03e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是做实验的步骤:</p><ul class=""><li id="f239" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">导入库</li><li id="601f" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">加载数据集</li><li id="f592" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">使用wordcloud可视化垃圾邮件</li><li id="ff9b" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">处理不平衡数据</li><li id="b71e" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">文本预处理</li><li id="ab0c" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">定义模型架构并训练四个模型</li><li id="b247" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">比较四种不同模型的结果</li><li id="7597" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">使用最终训练的模型对新消息进行分类</li></ul><h1 id="287e" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第一步。导入库</h1><p id="5785" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">让我们导入我们需要的库:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="1349" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Load, explore and plot data<br/>import </strong>numpy as np<br/><strong class="lh hi">import </strong>pandas as pd<br/><strong class="lh hi">import </strong>seaborn as sns<br/><strong class="lh hi">import </strong>matplotlib.pyplot as plt<br/><strong class="lh hi">from</strong> wordcloud <strong class="lh hi">import</strong> WordCloud, STOPWORDS, ImageColorGenerator<br/>%matplotlib inline</span><span id="04bb" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># Train test split<br/>from</strong> sklearn.model_selection <strong class="lh hi">import</strong> train_test_split</span><span id="3c55" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># Text pre-processing<br/>import </strong>tensorflow as tf<br/><strong class="lh hi">from</strong> tensorflow.keras.preprocessing.text <strong class="lh hi">import</strong> Tokenizer<br/><strong class="lh hi">from</strong> tensorflow.keras.preprocessing.sequence<strong class="lh hi"> import</strong> pad_sequences<br/><strong class="lh hi">from</strong> tensorflow.keras.callbacks <strong class="lh hi">import</strong> EarlyStopping</span><span id="8355" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># Modeling<br/>from </strong>tensorflow.keras.models <strong class="lh hi">import</strong> Sequential<br/><strong class="lh hi">from</strong> tensorflow.keras.layers <strong class="lh hi">import</strong> LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional</span></pre><h1 id="6fb1" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第二步。加载数据集</h1><p id="857b" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">我们在本文中使用的数据集来自UCI机器学习知识库:<a class="ae jo" href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" rel="noopener ugc nofollow" target="_blank">垃圾短信收集数据集</a>。然而，为了使本文更简单，我们尝试从这个github访问数据集:<a class="ae jo" href="https://raw.githubusercontent.com/kenneth-lee-ch/SMS-Spam-Classification/master/spam.csv" rel="noopener ugc nofollow" target="_blank">https://raw . githubusercontent . com/Kenneth-lee-ch/SMS-Spam-Classification/master/Spam . CSV</a>。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="3fb1" class="ll ke hh lh b fi lm ln l lo lp">df = pd.read_csv('https://raw.githubusercontent.com/kenneth-lee-ch/SMS-Spam-Classification/master/spam.csv', encoding='ISO-8859-1')</span><span id="7a23" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># rename the columns<br/></strong>df = df[['v1','v2']]<br/>df.rename(columns={'v1':'label', 'v2':'message'}, inplace=True)<br/>df.head()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/d17576f17982897f5d82399d9d6fbd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*cWz9F4sqGVbZ7cMa59mtLQ.png"/></div></figure><p id="fa82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">汇总统计有助于更好地理解数据:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="46e7" class="ll ke hh lh b fi lm ln l lo lp">df.describe()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/1f1f6fd2c2e202812cb0a73bed36fc0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*6dAmR1rhfvx6bVAEMBjX2Q.png"/></div></figure><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="ce6e" class="ll ke hh lh b fi lm ln l lo lp">df.groupby('label').describe().T</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lt"><img src="../Images/454f9d4b67029260f5598dbe78fee783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6GGoGldLMp8wPQ1tJxqhQ.png"/></div></div></figure><h1 id="5208" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第三步。使用wordcloud可视化垃圾邮件</h1><p id="222c" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">下一步是使用WordCloud可视化给定文本中出现频率最高的单词。</p><p id="50c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">“火腿”消息的文字云:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="376c" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi">ham_msg_cloud</strong> = WordCloud(width =520, height =260, stopwords = STOPWORDS, max_font_size = 50, background_color = "black", colormap = 'Pastel1').generate(ham_msg_text)</span><span id="f7e5" class="ll ke hh lh b fi lq ln l lo lp">plt.figure(figsize=(16,10))<br/>plt.imshow(<strong class="lh hi">ham_msg_cloud</strong>, interpolation = 'bilinear')<br/>plt.axis('off') # turn off axis<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lu"><img src="../Images/72218d043845fc54f9a00b5d1d0821ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZF31HIPMRX4C0B-XgCDug.png"/></div></div></figure><p id="c0a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于上述WordCloud的ham消息中出现频率最高的词有:现在、将要、好的、今天、抱歉等。</p><p id="1c98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">“垃圾”消息的词云:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="8a70" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi">spam_msg_cloud</strong> = WordCloud(width =520,<br/>                          height =260,<br/>                          stopwords=STOPWORDS,<br/>                          max_font_size=50, <br/>                          background_color ="black",<br/>                         colormap='Pastel1').generate(<strong class="lh hi">spam_msg_text</strong>)<br/>plt.figure(figsize=(16,10))<br/>plt.imshow(<strong class="lh hi">spam_msg_cloud</strong>, interpolation='bilinear')<br/>plt.axis('off') # turn off axis<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lu"><img src="../Images/1052a98c4a83022deeaab9f4c58f7857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xycK3FHhN_JnumOanDd6Yg.png"/></div></div></figure><p id="9cdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面来自WordCloud的垃圾短信中出现频率最高的词有:免费、通话、加急、移动等。</p><h1 id="fd38" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第四步。处理不平衡数据</h1><p id="f56a" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">下一步:让我们来看看火腿和垃圾短信是如何分布的。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="4e59" class="ll ke hh lh b fi lm ln l lo lp">plt.figure(figsize=(8,6))<br/>sns.countplot(df.label)<br/>plt.title(<strong class="lh hi">'The distribution of ham and spam messages'</strong>)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/3464d4b63f49022a987b435560d55d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*F4HNDoZxkzWMqR9IXe6i2A.png"/></div></figure><p id="c66e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于上述数据分布的可视化，可以得出数据不平衡的结论。最常见的是垃圾邮件。</p><p id="e0f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">处理不平衡数据集问题的方法:</p><ul class=""><li id="f224" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">选择合适的评估指标</li><li id="9585" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">重采样(过采样和欠采样)</li><li id="5059" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">合成少数过采样技术(SMOTE)</li><li id="ddcf" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">平衡分级器</li><li id="2ba3" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">阈值移动</li></ul><p id="e968" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们使用<strong class="ig hi">欠采样</strong>方法来处理不平衡数据。欠采样是指随机且均匀地对多数类进行欠采样。这可能会导致信息丢失。但是如果多数类的例子离其他的很近，这种方法可能会产生好的结果。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="609e" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># downsample the ham msg</strong><br/>ham_msg_df = ham_msg.sample(n = len(spam_msg), random_state = 44)<br/>spam_msg_df = spam_msg</span></pre><p id="0392" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的ham和sample消息分布(缩减采样后)显示了考虑不平衡数据后消息类型的类似分布:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/23c0d79f0f321a34d6a8739a742fbe32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*AEN_1V2MXiTc9YbIkd6uew.png"/></div></figure><h1 id="42e2" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第五步。文本预处理</h1><h2 id="c9eb" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 5.1。获取每个文本的长度列，并将文本标签转换为数值:</strong></h2><p id="43f9" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">在我们得到最终的数据帧之后，接下来我们添加<code class="du mk ml mm lh b">text_length</code>列(每条文本消息的长度)和<code class="du mk ml mm lh b">msg_type</code>列(数据的转换后的数字标签)。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="4cce" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Get length column for each text</strong><br/>msg_df['text_length'] = msg_df['message'].apply(len)</span><span id="7988" class="ll ke hh lh b fi lq ln l lo lp"><br/>msg_df['msg_type'] = msg_df['label'].map({'ham':0, 'spam':1})<br/>msg_label = msg_df['msg_type'].values</span><span id="06f3" class="ll ke hh lh b fi lq ln l lo lp">msg_df.head()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mn"><img src="../Images/2bd95cfed64cf919fe09f6fd01b40ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ciW2I_-EAs1qtNXfKbYoBw.jpeg"/></div></div></figure><h2 id="109d" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 5.2。列车测试分割</strong></h2><p id="e0af" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">之后，我们进行训练测试分割，将数据分成80%的训练数据和20%的测试数据。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="7c18" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi">x_train, x_test, y_train, y_test</strong> = train_test_split(msg_df['message'], msg_label, test_size=0.2, random_state=434)</span></pre><h2 id="cb57" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 5.3。标记化</strong></h2><p id="45ae" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">我们需要将文本消息数据转换成数字表示，这样模型才能理解它。</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="8b4b" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Defining pre-processing parameters</strong><br/>max_len = <strong class="lh hi">50</strong> <br/>trunc_type = <strong class="lh hi">'post'</strong><br/>padding_type = <strong class="lh hi">'post'</strong><br/>oov_tok = <strong class="lh hi">'&lt;OOV&gt;'</strong> # out of vocabulary token<br/>vocab_size = <strong class="lh hi">500</strong></span></pre><p id="bbc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TensorFlow Keras的Tokenizer API可以将句子拆分成单词，并编码成整数。</p><p id="357a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">记号赋予器将执行所有必要的预处理步骤:</p><ul class=""><li id="8752" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">标记成单词字符(单词级)</li><li id="d2db" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">num_words表示唯一令牌的最大数量</li><li id="2dc6" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">过滤掉标点符号</li><li id="4eac" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">将所有单词转换成小写</li><li id="64ce" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">将所有单词转换为整数索引</li></ul><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="6b33" class="ll ke hh lh b fi lm ln l lo lp">tokenizer = <strong class="lh hi">Tokenizer</strong>(num_words = vocab_size, <br/>                      char_level = False,<br/>                      oov_token = oov_tok)</span><span id="5d5d" class="ll ke hh lh b fi lq ln l lo lp">tokenizer.fit_on_texts(x_train)</span></pre><ul class=""><li id="fe6c" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated"><code class="du mk ml mm lh b">num_words</code>:我们希望在训练和测试数据中加载多少个独特的词</li><li id="7f93" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated"><code class="du mk ml mm lh b">oov_token</code>:词汇外标记将被添加到用于建立模型的语料库中的单词索引中。这用于在text_to_sequence调用期间替换词汇表之外的单词(不在我们的语料库中的单词)。</li></ul><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="9b1d" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Get the word_index</strong><br/>word_index = tokenizer.word_index<br/>total_words = len(word_index)</span><span id="4df4" class="ll ke hh lh b fi lq ln l lo lp">total_words</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mo"><img src="../Images/a883f5b2685fab90446a315c8a03ae94.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*6YHKHCd0atKMJbekHnu6pQ.jpeg"/></div></figure><h2 id="9ed8" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 5.4。序列和填充</strong></h2><p id="7c04" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">下一步:让我们使用来自Tokenizer对象的<code class="du mk ml mm lh b">texts_to_sequences</code>用数字序列表示每个句子。之后，我们填充序列，这样我们就可以有相同长度的每个序列。</p><p id="d8ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于数据训练:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="9e52" class="ll ke hh lh b fi lm ln l lo lp">training_sequences = tokenizer.texts_to_sequences(x_train)<br/>training_padded = pad_sequences(training_sequences,<br/>                                maxlen = max_len,<br/>                                padding = padding_type,<br/>                                truncating = trunc_type)</span></pre><p id="d318" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于数据测试:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="53c1" class="ll ke hh lh b fi lm ln l lo lp">testing_sequences = tokenizer.texts_to_sequences(x_test)<br/>testing_padded = pad_sequences(testing_sequences,<br/>                               maxlen = max_len,<br/>                               padding = padding_type,<br/>                               truncating = trunc_type)</span></pre><ul class=""><li id="be29" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated"><code class="du mk ml mm lh b">padding</code>:“前”或“后”(默认前)。通过使用pre，我们将在每个序列之前填充，post将在每个序列之后填充。</li><li id="66ef" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated"><code class="du mk ml mm lh b">maxlen</code>:所有序列的最大长度。如果没有提供，默认情况下它将使用最长句子的最大长度。</li><li id="8cff" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated"><code class="du mk ml mm lh b">truncating</code>:‘前’或‘后’(默认为‘前’)。如果序列长度大于提供的<code class="du mk ml mm lh b">maxlen</code>值，那么这些值将被截断为<code class="du mk ml mm lh b">maxlen</code>。“pre”选项将在序列的开头截断，而“post”将在序列的结尾截断。</li></ul><p id="a4e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练和测试填充的形状(张量):</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="8f9c" class="ll ke hh lh b fi lm ln l lo lp">print('Shape of training tensor: ', training_padded.shape)<br/>print('Shape of testing tensor: ', testing_padded.shape)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mp"><img src="../Images/6947cc29b3c9acfcede0e76a21dfc064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*_0imEzwJsBZtjE8CanFshA.jpeg"/></div></figure><h1 id="0828" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">第六步。定义模型架构并训练模型</h1><h2 id="efd7" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 6.1密集模型</strong></h2><p id="2f6f" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">定义密集分类器模型架构:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="ebfd" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Define parameter</strong><br/>vocab_size = <strong class="lh hi">500</strong> <br/>embedding_dim = <strong class="lh hi">16</strong><br/>drop_value = <strong class="lh hi">0.2</strong><br/>n_dense = <strong class="lh hi">24</strong></span><span id="03c9" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># Define Dense Model Architecture</strong><br/>model = Sequential()<br/>model.add(Embedding(vocab_size,<br/>                    embedding_dim,<br/>                    input_length = max_len))<br/>model.add(GlobalAveragePooling1D())<br/>model.add(Dense(24, activation='relu'))<br/>model.add(Dropout(drop_value))<br/>model.add(Dense(1, activation='sigmoid'))</span></pre><p id="ce96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du mk ml mm lh b">Sequential</code>调用Keras顺序模型，其中层按顺序添加。嵌入层将每个单词映射到实数的N维向量。<code class="du mk ml mm lh b">embedding_dim</code>是<code class="du mk ml mm lh b">word_vector</code>的大小，这里我们用16。因为嵌入层是我们模型中的第一个隐藏层，我们需要按照<code class="du mk ml mm lh b">input_length</code> = <code class="du mk ml mm lh b">max_len</code>的定义来设置我们的输入层</p><p id="de11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们使用<code class="du mk ml mm lh b">GlobalAveragePooling1D</code>作为池层，这有助于减少模型中的参数数量，并避免过度拟合。</p><p id="b7c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们使用一个带有激活函数<code class="du mk ml mm lh b">relu</code>的密集层，然后是一个用于避免过拟合的丢弃层和一个带有<code class="du mk ml mm lh b">sigmoid</code>激活函数的最终输出层。<code class="du mk ml mm lh b">sigmoid</code>激活函数输出概率将在0和1之间。</p><p id="2df6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">密集模型的概要:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="10ec" class="ll ke hh lh b fi lm ln l lo lp">model.summary()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mq"><img src="../Images/18eaf54bc3ff3d0a75d81b21af177a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5ig5l_x5C7lgJY9tSXWmQ.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mr"><img src="../Images/2104965d660891e9b38dd64e4ff5a2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*yJtjA6aH7H2wAvvCR4fvAQ.png"/></div></figure><p id="b97a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编译模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="8ed9" class="ll ke hh lh b fi lm ln l lo lp">model.compile(loss = 'binary_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])</span></pre><p id="aedc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用<code class="du mk ml mm lh b">binary_crossentropy</code>作为损失函数，因为模型的输出是二进制的，对于优化器，我们使用<code class="du mk ml mm lh b">adam</code>，它利用动量来避免局部最小值。</p><p id="f5c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">训练模型:</strong></p><p id="acfe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，让我们使用<code class="du mk ml mm lh b">model.fit</code>来训练模型。它使用填充的训练数据</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="95ed" class="ll ke hh lh b fi lm ln l lo lp">num_epochs = 30<br/>early_stop = EarlyStopping(monitor='val_loss', patience=3)<br/>history = model.fit(training_padded,<br/>                    y_train,<br/>                    epochs=num_epochs, <br/>                    validation_data=(testing_padded, y_test),<br/>                    callbacks =[early_stop],<br/>                    verbose=2)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ms"><img src="../Images/56c0d0083c056bfd61d5d0be29eb3122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*km7KZaEOTBvY3pi1r_FeMw.jpeg"/></div></div></figure><ul class=""><li id="9fc7" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">epoch:学习算法在整个训练数据中工作的次数。</li><li id="021a" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">回调:传递早期停止参数。early stopping(monitor = ' val _ loss '，patience=2)用于定义我们希望监控验证损失，如果验证损失在2个时期后没有改善，则模型训练将停止。这种技术有助于避免过拟合问题。</li><li id="403f" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">verbose : 2，它将向我们显示每个时期的损失和准确性。</li></ul><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="f997" class="ll ke hh lh b fi lm ln l lo lp">model.evaluate(testing_padded, y_test)</span><span id="e7b4" class="ll ke hh lh b fi lq ln l lo lp">0s 10ms/step - loss: 0.0873 - accuracy: 0.9599</span></pre><p id="c980" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型得出，训练损失为0.07，训练准确率为97.49%，验证损失为0.0873，验证准确率为95.99%。</p><p id="9a48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制精度曲线图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mt"><img src="../Images/09c4a9296406823802688a9771b1fbb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*_11QmkUCBUMY2u9AybxZXg.png"/></div></figure><p id="4c8e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的精度图来看，精度随着时代的推移而增加。该模型在训练数据中的表现优于在有效数据中的表现。</p><p id="2800" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制损失图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mu"><img src="../Images/f41feb4095862ee270396196ff351fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*rr5LMWtxYj4ZyfH9GdzEgg.png"/></div></figure><p id="6191" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从下面的损失图中，我们可以得出结论，损失是否随着历元数的增加而减少。</p><p id="1fc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">打印训练和有效数据的准确性:</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="aff1" class="ll ke hh lh b fi lm ln l lo lp">train_dense_results = model.evaluate(training_padded, np.asarray(y_train), verbose=2, batch_size=256)<br/>valid_dense_results = model.evaluate(testing_padded, np.asarray(y_test), verbose=2, batch_size=256)<br/>print(f'Train accuracy: {train_dense_results[1]*100:0.2f}')<br/>print(f'Valid accuracy: {valid_dense_results[1]*100:0.2f}')</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mv"><img src="../Images/3bc7bc4e3fd7e46b448336646cefbf68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DH2Kpd4LlnBYXyTotcgg0A.jpeg"/></div></div></figure><h2 id="9ec3" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 6.2长短期记忆(LSTM) </strong></h2><p id="ba37" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">长短期记忆(LSTM)旨在通过允许网络将数据存储在一种可以在以后访问的存储器中来克服简单递归神经网络(RNN)的问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mw"><img src="../Images/32cad68032a117722561f2fc76e1deb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4LeK_zO5728QFgOB.png"/></div></div><figcaption class="mx my et er es mz na bd b be z dx">Source: <a class="ae jo" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><p id="feca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LSTM模型的关键是<strong class="ig hi">细胞状态。</strong>用很少的计算更新单元状态两次，得到稳定的梯度。它还有一个隐藏的状态，就像短期记忆一样。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mw"><img src="../Images/10979d13ef5a6474225f9112e8cee204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yFa8vs_3yuVRcXAj.png"/></div></div></figure><p id="0fbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在LSTM有遗忘门、输入门和输出门。</p><ul class=""><li id="dac3" class="jp jq hh ig b ih ii il im ip jr it js ix jt jb ju jv jw jx bi translated">第一步是决定我们要从细胞状态中丢弃什么信息。这个决定是由被称为“忘记门”层的s形层做出的。</li><li id="19b9" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">第二步是决定我们要在单元格状态中存储什么样的<em class="nb">新</em>信息。这有两个部分。首先，称为“输入门”层的sigmoid层决定我们将更新哪些值。接下来，tanh层创建一个新的候选值向量，可以添加到状态中。</li><li id="7bfd" class="jp jq hh ig b ih jy il jz ip ka it kb ix kc jb ju jv jw jx bi translated">最后，我们需要决定我们要输出什么。该输出将基于我们的单元状态，但将是过滤后的版本。首先，我们运行一个sigmoid层，它决定我们要输出细胞状态的哪些部分。然后，我们将单元状态通过tanh(将值推到-1和1之间)并乘以sigmoid门的输出，这样我们只输出我们决定的部分</li></ul><p id="04cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">定义LSTM模型架构:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="5c29" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi"># Define parameter<br/></strong>n_lstm = 128<br/>drop_lstm = 0.2</span><span id="f877" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi"># Define LSTM Model </strong><br/>model1 = Sequential()<br/>model1.add(Embedding(vocab_size, embedding_dim, input_length=max_len))<br/>model1.add(SpatialDropout1D(drop_lstm))<br/>model1.add(LSTM(n_lstm, return_sequences=False))<br/>model1.add(Dropout(drop_lstm))<br/>model1.add(Dense(1, activation='sigmoid'))</span></pre><p id="e54f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">模型总结:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="4938" class="ll ke hh lh b fi lm ln l lo lp">model1.summary()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nc"><img src="../Images/cd84ae0db8bab8e86fb28961242fd51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tk_1Ct599y5Eyjgq5Zgd9g.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nd"><img src="../Images/0859b0579520bc7e68e347cf248fb379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*tm4hXks_pJwN8SPBhTx5VQ.png"/></div></figure><p id="647b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编译模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="e3b8" class="ll ke hh lh b fi lm ln l lo lp">model1.compile(loss = 'binary_crossentropy',<br/>               optimizer = 'adam',<br/>               metrics = ['accuracy'])</span></pre><p id="5784" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">训练模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="7600" class="ll ke hh lh b fi lm ln l lo lp">num_epochs = 30<br/>early_stop = EarlyStopping(monitor='val_loss', patience=2)<br/>history = model1.fit(training_padded,<br/>                     y_train,<br/>                     epochs=num_epochs, <br/>                     validation_data=(testing_padded, y_test),<br/>                     callbacks =[early_stop],<br/>                     verbose=2)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ne"><img src="../Images/a93df502e5c5be914eae34b6078aa79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIddQH77S4aFFroZN-l6vA.jpeg"/></div></div></figure><p id="ce54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制精度图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nf"><img src="../Images/284a3637cb7750f26e03f5cc5ffdf5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*6GDpfGMyjVPoQBm6Qpz9sw.png"/></div></figure><p id="d293" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制损失图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ng"><img src="../Images/370099d784fd343d59ec6593ef490e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*mDc5g4Yh5MrN51Fpvx5ykQ.png"/></div></figure><p id="3170" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">打印列车精度和有效数据:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nh"><img src="../Images/e5f99978de1aa2de5a2a75605ac68c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDZPMnW8B_64KbYDGUUGCA.jpeg"/></div></div></figure><h2 id="8f3a" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 6.3双向长短期记忆(双LSTM) </strong></h2><p id="dd00" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">双向LSTM(bil STM)是一种序列处理模型，由两个LSTM组成:一个接收正向输入，另一个接收反向输入。BiLSTMs有效地增加了网络可用的信息量，改善了算法可用的上下文(例如，知道句子中哪个单词紧跟在哪个单词之后和之前)。与标准LSTM不同，双LSTM的输入是双向的，它能够利用双方的信息。它也是一个强大的工具，用于在序列的两个方向上对单词和短语之间的顺序依赖性进行建模。</p><p id="49f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BiLSTM增加了一个LSTM层，它逆转了信息流的方向。简而言之，这意味着输入序列在附加的LSTM层中向后流动。然后，我们以几种方式组合来自两个LSTM层的输出，例如平均、求和、乘法或级联。</p><p id="9da9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为便于说明，展开的BiLSTM如下图所示:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mw"><img src="../Images/00306416ad42ec452e9ffde896fcd0aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*abus573283IWAPxX.png"/></div></div><figcaption class="mx my et er es mz na bd b be z dx">Source: <a class="ae jo" href="https://www.baeldung.com/cs/bidirectional-vs-unidirectional-lstm" rel="noopener ugc nofollow" target="_blank">https://www.baeldung.com/cs/bidirectional-vs-unidirectional-lstm</a></figcaption></figure><p id="4c52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">定义双LSTM模型架构:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="4254" class="ll ke hh lh b fi lm ln l lo lp">model2 = Sequential()<br/>model2.add(Embedding(vocab_size,<br/>                     embedding_dim,<br/>                     input_length = max_len))<br/>model2.add(Bidirectional(LSTM(n_lstm,<br/>                              return_sequences = False)))<br/>model2.add(Dropout(drop_lstm))<br/>model2.add(Dense(1, activation='sigmoid'))</span></pre><p id="d4e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">模型概要:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="20a7" class="ll ke hh lh b fi lm ln l lo lp">model2.summary()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ni"><img src="../Images/0d1efcf4cac9f85ac4138984f2ccfd0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L4OS4uUtwtevx8veganSFg.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nj"><img src="../Images/ed7c76c8cd7f30ba692deb4824358dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*VKzEKpiUAUNpU1uHeY4L9Q.png"/></div></figure><p id="f571" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编译模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="ddc4" class="ll ke hh lh b fi lm ln l lo lp">model2.compile(loss = 'binary_crossentropy',<br/>               optimizer = 'adam',<br/>               metrics=['accuracy'])</span></pre><p id="02a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">训练模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="2450" class="ll ke hh lh b fi lm ln l lo lp">num_epochs = 30<br/>early_stop = EarlyStopping(monitor = 'val_loss',<br/>                           patience = 2)<br/>history = model2.fit(training_padded,<br/>                     y_train,<br/>                     epochs = num_epochs,<br/>                     validation_data = (testing_padded, y_test),<br/>                     callbacks = [early_stop],<br/>                     verbose = 2)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nk"><img src="../Images/0cb2feb7bd44dec3c64747034365b278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYaQkO10uhGWkXOOycE7IA.jpeg"/></div></div></figure><p id="3131" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制精度图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nf"><img src="../Images/58e3c3a5bba8922e5d9dc30d6753fc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*loK8s0eexVOe6yU2A7bYdQ.png"/></div></figure><p id="06b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制损失图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ng"><img src="../Images/29f1bf34acc28bb9dd1302c0ca0ac145.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*hnXEM8MAu9wUE-aitvnlpg.png"/></div></figure><p id="58b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">打印列车精度和有效数据:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nl"><img src="../Images/d07f5a667c00beb1313f155d801f3c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mZNc5p4QhzXGX0b82ajGfA.jpeg"/></div></div></figure><h2 id="1bef" class="ll ke hh bd kf lx ly lz kj ma mb mc kn ip md me kr it mf mg kv ix mh mi kz mj bi translated"><strong class="ak"> 6.4门控循环单元(GRU) </strong></h2><p id="ddec" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">门控递归单元或GRU是一种递归神经网络。它类似于LSTM，但只有两个门——一个复位门和一个更新门，并且明显缺少输出门。更少的参数意味着gru通常比LSTM同行更容易/更快地训练。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nm"><img src="../Images/034ad07938996dc2d22714086dde590a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iRhDIDdPWF23SUEm.png"/></div></div><figcaption class="mx my et er es mz na bd b be z dx">Source: <a class="ae jo" href="https://paperswithcode.com/method/gru" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/method/gru</a></figcaption></figure><p id="b910" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">定义GRU模型架构:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="6f64" class="ll ke hh lh b fi lm ln l lo lp">model3 = Sequential()<br/>model3.add(Embedding(vocab_size,<br/>                     embedding_dim,<br/>                     input_length = max_len))<br/>model3.add(SpatialDropout1D(0.2))<br/>model3.add(GRU(128, return_sequences = False))<br/>model3.add(Dropout(0.2))<br/>model3.add(Dense(1, activation = 'sigmoid'))</span></pre><p id="903d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">模型概要:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="bf91" class="ll ke hh lh b fi lm ln l lo lp">model3.summary()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nn"><img src="../Images/71b5246dbd81a4213947ecd64643a497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hjs7qaMnn_3cF9akEF0Zlg.jpeg"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nd"><img src="../Images/8ae0bedeef350d0f161514e5e5f7f4a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*Llyxe8_P0OWZbXNGl80wKg.png"/></div></figure><p id="ee5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编译模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="e86c" class="ll ke hh lh b fi lm ln l lo lp">model3.compile(loss = 'binary_crossentropy',<br/>                       optimizer = 'adam',<br/>                       metrics=['accuracy'])</span></pre><p id="ad38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">训练模型:</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="0a1f" class="ll ke hh lh b fi lm ln l lo lp">num_epochs = 30<br/>early_stop = EarlyStopping(monitor='val_loss', patience=2)<br/>history = model3.fit(training_padded,<br/>                     y_train,<br/>                     epochs=num_epochs, <br/>                     validation_data=(testing_padded, y_test),<br/>                     callbacks =[early_stop],<br/>                     verbose=2)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es no"><img src="../Images/5a4348e5c98e9bbb940a252d22f82d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J72F1K45IBib1Vf1N4GHQA.jpeg"/></div></div></figure><p id="467e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制精度图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nf"><img src="../Images/03f067409d55f45b0a3f7cc6e7a266a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*l1bJ0hPfk98lSpXqFeedJw.png"/></div></figure><p id="c41f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">绘制损失图:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es np"><img src="../Images/da029820f55a7c88b5d598a70ca45cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*pnIeRC5auZPp_vczLcoyYA.png"/></div></figure><p id="2549" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">打印列车精度和有效数据:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nq"><img src="../Images/bf5db1bba8082031a7926e2a1baf5368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BVzJ2L-7Mp2aU6GIWjiqg.jpeg"/></div></div></figure><h1 id="ac4f" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">比较四种不同型号</strong></h1><p id="053d" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">下一步是比较我们使用的四个模型，</p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="abb2" class="ll ke hh lh b fi lm ln l lo lp"># Comparing the four different models<br/>print(f"Dense model loss and accuracy: {model.evaluate(testing_padded, y_test)} " )<br/>print(f"LSTM model loss and accuracy: {model1.evaluate(testing_padded, y_test)} " )<br/>print(f"Bi-LSTM model loss and accuracy: {model2.evaluate(testing_padded, y_test)} " )<br/>print(f"GRU model loss and accuracy: {model3.evaluate(testing_padded, y_test)}")</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nr"><img src="../Images/94f832513772c4ae94f4b51a0dfa3326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iuRxfvvNAsr5MkkpLGhNig.jpeg"/></div></div></figure><p id="a8b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这四个模型的验证损失分别为0.087、0.095、0.084和0.69。验证准确率分别为95.98%、96.98%、97.32%和48.16%。</p><p id="0fdd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于损失、准确度和图，我们可以断定双LSTM模型是否是该分类情况的最佳模型，验证准确度= 97.32 %，损失= 0.084。</p><h1 id="2ea8" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">预测新邮件的垃圾邮件</h1><p id="ffb3" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">根据原始数据中的文本，评估密集模型如何预测/分类它是垃圾邮件还是火腿。下面的第一条消息是<strong class="ig hi">垃圾消息，</strong>而第二条<strong class="ig hi">是<strong class="ig hi">火腿</strong>消息。</strong></p><pre class="jd je jf jg fd lg lh li lj aw lk bi"><span id="1c9b" class="ll ke hh lh b fi lm ln l lo lp"><strong class="lh hi">predict_msg</strong> = ["Have friends and colleagues who could benefit from<br/>                these weekly updates? Send them to this link to<br/>                subscribe",</span><span id="b9ac" class="ll ke hh lh b fi lq ln l lo lp">               "Call me"]</span><span id="c8eb" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi">def predict_spam(predict_msg)</strong>:<br/>  new_seq = tokenizer.texts_to_sequences(predict_msg)<br/>  padded = pad_sequences(new_seq,<br/>                         maxlen = max_len,<br/>                         padding = padding_type,<br/>                         truncating = trunc_type)<br/>  return(model.predict(padded))</span><span id="b3ea" class="ll ke hh lh b fi lq ln l lo lp"><strong class="lh hi">predict_spam(predict_msg)</strong></span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ns"><img src="../Images/d0912a9cdf33e5474f7733c99104a4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12KgVgbtXPa9UyPdHvnd6Q.png"/></div></div></figure><p id="b07f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如下图所示，模型正确预测第一句为<strong class="ig hi">垃圾邮件</strong>，第二句为非垃圾邮件或<strong class="ig hi">火腿。</strong>第一句话有95%的几率是<strong class="ig hi">垃圾信息</strong>。</p><h1 id="77dd" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">总结:</h1><p id="89fc" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">在本文中，我们使用几个深度学习模型对<a class="ae jo" href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" rel="noopener ugc nofollow" target="_blank"> UCL数据集</a>进行了二元分类，包括:Dense、LSTM、Bi-LSTM和GRU。基于已经进行的实验，得出的结论是Bi-LSTM模型是具有最佳性能的模型(在这种情况下)，准确率值为97.32%，损失值= 0.084。</p><h1 id="dee2" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参考资料:</h1><div class="nt nu ez fb nv nw"><a href="https://towardsdatascience.com/nlp-spam-detection-in-sms-text-data-using-deep-learning-b8632db85cc8" rel="noopener follow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">NLP:使用深度学习检测短信(文本)数据中的垃圾邮件</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">TensorFlow2中使用密集网络、LSTM和双LSTM架构的文本分类</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok jm nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://github.com/ShresthaSudip/SMS_Spam_Detection_DNN_LSTM_BiLSTM/blob/master/NLP_SMS_Spam_Detection_FinalVersion_Downsampling_Dense_LSTM_BiLSTM.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">SMS _ Spam _ Detection _ DNN _ LSTM _ BiLSTM/NLP _ SMS _ Spam _ Detection _ final version _ down sampling _ Dense _ LSTM _ BiLST…</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">通过在GitHub上创建一个帐户，为ShresthaSudip/SMS _ Spam _ Detection _ DNN _ LSTM _ BiLSTM开发做出贡献。</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">github.com</p></div></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">什么是不平衡数据|处理不平衡数据的技术</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">本文是作为数据科学博客的一部分发表的，分类问题在</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok jm nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592" rel="noopener follow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">基于Keras和LSTM的多类文本分类</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">在本教程中，我们将与克拉斯和LSTM建立一个文本分类，以预测BBC新闻的类别…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">djajafer.medium.com</p></div></div><div class="of l"><div class="om l oh oi oj of ok jm nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://paperswithcode.com/method/bilstm" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">论文与代码-比尔斯特姆解释</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">双向LSTM，简称biLSTM，是一个序列处理模型，由两个LSTM组成:一个在一个</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">paperswithcode.com</p></div></div><div class="of l"><div class="on l oh oi oj of ok jm nw"/></div></div></a></div><div class="nt nu ez fb nv nw"><a href="https://paperswithcode.com/method/gru" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">有代码的文件——GRU解释道</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">门控递归单元或GRU是一种递归神经网络。它类似于LSTM，但只有两个门…</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">paperswithcode.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok jm nw"/></div></div></a></div><p id="366a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jo" href="https://www.baeldung.com/cs/bidirectional-vs-unidirectional-lstm" rel="noopener ugc nofollow" target="_blank">https://www . bael dung . com/cs/bidirectional-vs-单向-lstm </a></p><div class="nt nu ez fb nv nw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">medium.com</p></div></div><div class="of l"><div class="op l oh oi oj of ok jm nw"/></div></div></a></div></div></div>    
</body>
</html>