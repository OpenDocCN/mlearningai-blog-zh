<html>
<head>
<title>A Non-technical Introduction to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的非技术性介绍</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-non-technical-introduction-to-reinforcement-learning-6afd28364da?source=collection_archive---------2-----------------------#2021-12-20">https://medium.com/mlearning-ai/a-non-technical-introduction-to-reinforcement-learning-6afd28364da?source=collection_archive---------2-----------------------#2021-12-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="3924" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="539f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">上周，我有机会向一位不是IT专业人士的同事解释一种机器学习概念，即强化学习。他很好地理解了机器学习是如何工作的，但误解了监督学习是整个领域。</p><p id="c033" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我通过对比强化学习和监督学习来解释强化学习，他理解的相当好。受该事件的启发，本文将以非技术人员可以理解的方式解释强化学习的基本概念以及它与监督学习的不同之处。</p><h1 id="2ac7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">RL上下文</h1><p id="065a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">强化学习问题由两部分组成，<em class="kf">主体</em>和<em class="kf">环境</em>，它们相互作用。代理观察情况并执行一个动作来改变它所包围的环境的状态。环境然后提供奖励或惩罚作为对代理的反馈。重复这个循环，直到满足退出条件。代理人的目标是获得最大的长期回报。“长期”这个词至关重要，因为它意味着代理人必须在整体上做得很好，而不仅仅是在任何特定的情况下。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/05900cb0840ae1ab90b2ed0f6857a141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0M0mf53AewET8JF6aMedw@2x.jpeg"/></div></div></figure><p id="0da6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">说明RL最好的例子是一个游戏。想象你正在学习玩一个游戏，比如说嘟嘟跳。你不知道这个游戏是如何运作的，也不知道一开始该做什么。你唯一知道的是你在控制涂鸦者左右移动。你从随机移动涂鸦者开始。几次尝试后，你注意到当涂鸦者落在一个平台上时，它会飞得更高，飞得越高，分数越高。另一方面，如果涂鸦者击中了怪物，分数就会减少，如果失败，游戏就结束了。</p><p id="9497" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在上面的例子中，涂鸦者是一个<em class="kf">代理</em>，游戏规则是<em class="kf">环境</em>。涂鸦者在采取行动后获得和扣除的分数分别是<em class="kf">奖励</em>和<em class="kf">惩罚</em>。</p><h1 id="2abb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">强化与监督</h1><p id="506d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">给定的设置使得强化学习变得完全不同于监督学习。在所有的差异中，有两个是最重要的。</p><h2 id="4778" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated">决策结果</h2><p id="af06" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在监督学习中，所建立的预测模型不会对另一个预测产生影响。输出是否正确，与新样本及其输出无关(仅供参考，这方面的专业术语是<em class="kf">独立同分布</em>–<em class="kf">IID</em>)。每个决定都是自成一体的。</p><p id="5d5d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">强化学习完全不同。回想一下开头那个嘟嘟跳的例子。如果你在一个回合走了一步好棋，它会积极地影响下一个回合，反之亦然。总的来说，最终的结果是你一路做的所有决定的积累，每个决定都相互影响。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lg"><img src="../Images/aabf3334d79a5940994a60567012428c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yK3_lgwQDM4pS4sxHU5Yg@2x.jpeg"/></div></div></figure><h1 id="4fee" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">没有正确答案</h1><p id="9958" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们通过将模型的预测与监督学习中的真实答案进行比较来训练和优化模型。强化学习的问题没有像那样的标签数据，因为没有正确或错误的答案。想象一下玩一个游戏，有不止一个策略来解决它，你不能判断哪一个更好，直到最后，或者有时你不能做到这一点。</p><p id="273d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">你可能会想，如果环境在代理人每次行动时都提供一个反馈，那不是可以作为一个标签吗？这是一个有效的观点，然而，从环境中获得的奖励/惩罚只表明在特定时间的<em class="kf">的<em class="kf">质量</em>。这和监督学习中<em class="kf">正确</em>的定义大相径庭。</em></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lh"><img src="../Images/c9d8a8afa94db9c77e32ae937801a527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7O6AaAOHWxZEfqRQeMg4qA@2x.jpeg"/></div></div></figure><h1 id="589e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">习惯</h1><p id="c880" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">上面提到的所有差异不仅使解决强化学习问题需要不同的方法，而且使它的使用完全不同于监督学习。以下是现实世界中强化学习应用的例子。</p><h2 id="c7cd" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated">推荐系统</h2><p id="927c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">传统的推荐系统要么使用基于内容的模型，要么使用数学方法向用户推荐项目。总的来说，这些方法识别与特定用户已经带来/观看的项目相似的项目，并向他推荐这些项目。</p><p id="1c39" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通过引入强化学习，我们可以建立一个考虑用户反馈的推荐系统，并让模型根据其经验调整策略。</p><h2 id="6ec7" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated">机器人控制</h2><p id="dfba" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">机器人控制问题通常通过求解代表机器人运动学的复杂数学方程来解决。这些方程的解然后被编程到控制机器人的软件中。这样做的一个局限是传感器的精度，因为它影响作为数学模型输入的数据。</p><p id="7356" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">强化学习为这个问题提供了一个较少依赖硬件而更多依赖数据的解决方案。我们可以像人类学习玩游戏一样，通过反复试验为不同类型的传感器找到最佳控制策略，而不是求解运动学方程。</p><h1 id="7d4d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="5c1e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在本文中，我们讨论了强化学习的基本概念，以及它与大多数人熟悉的监督学习的不同之处。此外，这篇文章给出了现实世界中强化学习应用的几个例子，以及它与该领域中传统解决方案的不同之处。</p><p id="caa4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">感谢您的阅读。如果你喜欢这篇文章，请<a class="ae li" rel="noopener" href="/subscribe/@thanakornpanyapiang">在Medium </a>上跟随我查看更多。</p><p id="7f1e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下期帖子再见！</p><div class="lj lk ez fb ll lm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hi fi z dy lr ea eb ls ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">medium.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kq lm"/></div></div></a></div></div></div>    
</body>
</html>