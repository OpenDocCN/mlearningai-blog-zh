<html>
<head>
<title>HyperParameter Tuning: Fixing Overfitting in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整:修复神经网络中的过拟合</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/hyperparameter-tuning-fixing-overfitting-in-neural-networks-b983b21d60bd?source=collection_archive---------0-----------------------#2021-08-11">https://medium.com/mlearning-ai/hyperparameter-tuning-fixing-overfitting-in-neural-networks-b983b21d60bd?source=collection_archive---------0-----------------------#2021-08-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="c65c" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated"><em class="hh">减少神经网络中高方差(过拟合)问题的快速方法。</em></p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/69fde83511c4a0138978bb89c5ad4299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*MJ12oaTGUlGXFTxsSVtLoA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Hyper-Parameter Tuning</figcaption></figure><h1 id="f76c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">介绍</h1><blockquote class="ie if ig"><p id="7f3d" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">在我上一篇<a class="ae kq" rel="noopener" href="/mlearning-ai/hyperparameter-tuning-fixing-high-bias-underfitting-in-neural-networks-5184ead3cbed">博客</a>中，我讨论了各种参数对偏差的影响，以及我们如何解决高偏差(欠拟合问题)。</p></blockquote><p id="4d70" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">在这篇博客中，我们将通过一些方法和技术来解决神经网络中的高方差(过拟合)问题。高方差是神经网络训练过程中面临的常见问题。当模型过拟合训练数据但在验证数据上表现不佳时，会出现高方差的问题。该问题通常象征着被训练的模型已经很好地学习了输入-输出映射，但是不能在交叉验证或测试集上正确地概括。</p><p id="a561" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们将在这篇博客中一步一步地检查各种因素对验证准确性和验证损失的影响。</p><h1 id="a6ce" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">导入和预处理</h1><p id="f78a" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">我们将从导入<strong class="ik hi"> TensorFlow、NumPy和Matplotlib </strong>库开始，并初始化一些超级参数，如历元数、学习率和优化器</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="5cde" class="le jt hh la b fi lf lg l lh li">import tensorflow as tf <br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="7356" class="le jt hh la b fi lj lg l lh li">tf.random.set_seed(1)<br/>EPOCHS = 40<br/>LR = 0.0001<br/>OPT = tf.keras.optimizers.SGD(LR , 0.99)<br/>plt.style.use('fivethirtyeight')<br/>plt.rcParams["figure.figsize"] = (8,5)</span></pre><p id="25dc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们将使用著名的<strong class="ik hi"> Mnist </strong>数据集进行演示。Mnist数据集包含<strong class="ik hi"> 60，000张</strong>图像，具有<strong class="ik hi"> 80:20 </strong>训练测试分割。所有的图像都是灰度的，并且具有形状<strong class="ik hi"> (28，28) </strong>。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="c1d8" class="le jt hh la b fi lf lg l lh li">(x_train , y_train) , (x_test , y_test ) = tf.keras.datasets.mnist.load_data()</span><span id="3619" class="le jt hh la b fi lj lg l lh li">x_train = x_train /255 <br/>x_test = x_test/255</span></pre><p id="07e7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们可以通过TensorFlow库直接访问这个数据集。数据已经被分成训练和测试子集。下一步，我们将正常化我们的图像。</p><h1 id="dd85" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">模型设计</h1><p id="5675" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">我们将首先建立一个简单的神经网络，没有隐藏层，只有一个输入层和一个输出层。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="49fc" class="le jt hh la b fi lf lg l lh li">model = tf.keras.Sequential(<br/>[tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="7307" class="le jt hh la b fi lj lg l lh li">model.compile(optimizer=OPT,<br/>              loss = "sparse_categorical_crossentropy",<br/>              metrics = ["accuracy"])</span></pre><p id="11de" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们将使用稀疏分类交叉熵作为损失来编译这个模型，并将度量设置为准确性。</p><h1 id="51c9" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">数据增长的影响</h1><p id="e1b3" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">我们将训练上面定义的模型两次，但是使用不同的数据分布。为了证明数据对高方差的影响，我们将定义一个新的训练数据子集，其中仅包含总训练数据的60%<strong class="ik hi">。</strong></p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="c63a" class="le jt hh la b fi lf lg l lh li">(x_train_partial , y_train_partial) =   (x_train[:30000], y_train[:30000])</span></pre><p id="e843" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">与原始数据集中的<strong class="ik hi"> 50，000张</strong>图像相比，新的(x_train_partial，y_train_partial)数据集具有<strong class="ik hi"> 30，000张</strong>图像。训练完这些数据集后，我们现在将绘制两个图来检查增加数据的影响。验证准确度与时期数的关系图和验证准确度与时期数的关系图。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/8a5f9d299bfaafd749173a4dd1189c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjAFXUed3xQJxcNV82_sKQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of Data</figcaption></figure><p id="ce01" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">从上图中可以清楚地看到，增加数据有助于解决高方差问题。</p><h1 id="5673" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">增加隐藏层的效果</h1><p id="cb25" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">现在，我们将增加网络中隐藏层的数量，并验证其对模型训练准确性的影响。我们将训练四个不同的模型，其中几个隐藏层分别设置为<strong class="ik hi"> 1、2、3和5 </strong> <strong class="ik hi">层</strong>。所有4种模型的架构如下:</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="3463" class="le jt hh la b fi lf lg l lh li">one_layer_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="4800" class="le jt hh la b fi lj lg l lh li">two_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10 , activation = "relu"),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="7f26" class="le jt hh la b fi lj lg l lh li">three_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(40 , activation = "relu"),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="e658" class="le jt hh la b fi lj lg l lh li">five_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),                              tf.keras.layers.Dense(10 , activation = "relu"),                              tf.keras.layers.Dense(20 , activation = "relu"),                              tf.keras.layers.Dense(40 , activation = "relu"),                              tf.keras.layers.Dense(20 , activation = "relu"),                             tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="7b13" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">在对上述所有模型的20个时期的完整数据集进行训练后，我们得到了下面的验证准确性和验证损失比较图:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lp"><img src="../Images/631615b073a115dcadd625847aa34ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cHE6n_28uwvrbVQffVLgyw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of hidden layers</figcaption></figure><p id="0d58" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">显而易见，增加隐藏层的数量可以提高验证的准确性，减少验证的损失。随着我们在培训过程中的深入。对于mnist数据集，选择3个隐藏层似乎会产生最佳结果。</p><p id="e479" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们现在将使用这个3个隐藏层的神经网络作为我们的参考，并检查在这个体系结构的不同层中增加节点的效果。</p><h1 id="f765" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">隐藏层中单元(节点)数量的影响</h1><p id="43d7" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">我们现在将增加先前训练的3层网络的不同层中的节点数量。通常的做法是按降序设置不同层中的单元数量。在这次演示中，我们将训练两种不同的模型。第一种型号的单位数量较少，而第二种型号的单位数量较多。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="e7e1" class="le jt hh la b fi lf lg l lh li">small_units_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(80,activation = "relu"),<br/>tf.keras.layers.Dense(40,activation = "relu"),<br/>tf.keras.layers.Dense(20,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="4b52" class="le jt hh la b fi lj lg l lh li">large_units_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),                              tf.keras.layers.Dense(512,activation = "relu"),                              tf.keras.layers.Dense(128,activation = "relu"),                              tf.keras.layers.Dense(64,activation = "relu"),                             tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="e9d7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们将第二个模型中的单位设置为2的次方<strong class="ik hi">。这被认为是在我们的神经网络中设置单元数量的最佳默认选择。</strong></p><p id="7464" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">在对上述两个模型的20个时期的完整数据集进行训练后，我们得到了下面的验证准确性和验证损失比较图:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lq"><img src="../Images/841931be07be0aec218413d6e3b282c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EROZBDf7ducD9tJsFeiTyg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of units</figcaption></figure><p id="7aec" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">很明显，单位数量对验证准确性和验证损失都有很大的影响。在上面的例子中，随着层数和每层单元数的增加，验证精度从<strong class="ik hi"> 92% </strong>增加到超过<strong class="ik hi"> 98% </strong>。</p><h1 id="7e09" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">批量标准化的效果</h1><p id="64af" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">接下来，我们将检查添加批量规范化图层对修复高方差的影响。我们将用之前的最佳模型作为验证批量归一化效果的参考。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="ff22" class="le jt hh la b fi lf lg l lh li">bn_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(512,activation = "relu"),<br/>tf.keras.layers.BatchNormalization(),<br/>tf.keras.layers.Dense(128,activation = "relu"),<br/>tf.keras.layers.BatchNormalization(),<br/>tf.keras.layers.Dense(64,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="fd60" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们在隐藏层之间添加了一对<strong class="ik hi"> BatchNormalization </strong>层。我们现在将训练该模型，并将其验证准确性和验证损失与我们以前的最佳模型进行比较。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lr"><img src="../Images/37dd3b620fc1a4b6b75a8eb1939c9726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mzPQN6UIlqo81fng9ZMcA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of Batch Normalization</figcaption></figure><p id="c40c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">显而易见，与没有批次归一化的模型相比，添加批次归一化无疑有助于提高验证准确度，并保持验证损失不变。</p><h1 id="9685" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">辍学的影响</h1><p id="3c3c" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">最后，我们将检查辍学层在解决高方差问题的影响。我们将在之前的最佳模型中添加<strong class="ik hi">两个下降层</strong>。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="8d78" class="le jt hh la b fi lf lg l lh li">dropout_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(512,activation = "relu"),<br/>tf.keras.layers.Dropout(0.3),<br/>tf.keras.layers.Dense(128,activation = "relu"),<br/>tf.keras.layers.Dropout(0.2),<br/>tf.keras.layers.Dense(64,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="3a3a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我们在隐藏层之间添加了两个丢弃概率分别为<strong class="ik hi"> 0.3 </strong>和<strong class="ik hi"> 0.2 </strong>的丢弃层。我们现在将训练该模型，并将其验证准确性和验证损失与我们以前的最佳模型进行比较。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ls"><img src="../Images/f650c9d39149087b446eb7a918a356f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PzCc5HK8DBYR_Rsv7W5ywQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of Dropout</figcaption></figure><p id="e366" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">可以清楚地看到，在我们的隐藏层之间添加丢弃层确实有助于提高验证准确性，并且与具有丢弃的普通网络相比，更平滑和更快速地减少验证损失。</p><h1 id="441a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="6d2a" class="pw-post-body-paragraph ih ii hh ik b il ku in io ip kv ir is kr kw iv iw ks kx iz ja kt ky jd je jf ha bi translated">在使用不同的超参数对多个模型训练相同的数据后，我们可以得出结论，以下更改可以帮助我们解决高方差问题:</p><ul class=""><li id="96f0" class="lt lu hh ik b il im ip iq kr lv ks lw kt lx jf ly lz ma mb bi translated"><strong class="ik hi">增加训练数据量。</strong></li><li id="1760" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">增加隐藏层数。</strong></li><li id="eb41" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">增加隐藏单元的数量。</strong></li><li id="ee80" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">添加批量归一化。</strong></li><li id="dcbd" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">添加辍学者。</strong></li><li id="438a" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">为更高数量的纪元进行训练。</strong></li><li id="b362" class="lt lu hh ik b il mc ip md kr me ks mf kt mg jf ly lz ma mb bi translated"><strong class="ik hi">尝试更多的神经网络。</strong></li></ul><p id="6873" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">我希望你们都喜欢这个快速的小博客！！！</p><p id="93d6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated">这个博客中所有模型和图表的代码都可以在这里获得—</p><p id="cdf6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kr iu iv iw ks iy iz ja kt jc jd je jf ha bi translated"><a class="ae kq" href="https://github.com/sanskar-hasija/Hyperparameter-Tuning" rel="noopener ugc nofollow" target="_blank">https://github.com/sanskar-hasija/Hyperparameter-Tuning</a></p></div></div>    
</body>
</html>