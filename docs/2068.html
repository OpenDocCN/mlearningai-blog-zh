<html>
<head>
<title>ElegantRL: Much More Stable Deep Reinforcement Learning Algorithms than Stable-Baseline3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ElegantRL:比稳定基线3更稳定的深度强化学习算法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/elegantrl-much-much-more-stable-than-stable-baseline3-f096533c26db?source=collection_archive---------0-----------------------#2022-03-03">https://medium.com/mlearning-ai/elegantrl-much-much-more-stable-than-stable-baseline3-f096533c26db?source=collection_archive---------0-----------------------#2022-03-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8092" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener ugc nofollow" target="_blank"> ElegantRL </a>专为从业者开发，具有以下优势:</p><ul class=""><li id="308f" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">云原生:遵循云原生范式，例如，<a class="ae jc" href="https://arxiv.org/abs/2112.05923" rel="noopener ugc nofollow" target="_blank"> ElegantRL-Podracer </a>和<a class="ae jc" href="https://arxiv.org/abs/2111.05188" rel="noopener ugc nofollow" target="_blank"> FinRL-Podracer </a>。</li><li id="78e1" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">可扩展:在多个层面上充分利用DRL算法的并行性，使其可以轻松扩展到云平台上的数百或数千个计算节点，例如拥有数千个GPU的<a class="ae jc" href="https://www.nvidia.com/en-us/data-center/dgx-superpod/" rel="noopener ugc nofollow" target="_blank"> DGX SuperPOD平台</a>。</li><li id="fbfa" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">轻量级:核心代码&lt;1,000 lines (check <a class="ae jc" href="https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld" rel="noopener ugc nofollow" target="_blank"> Elegantrl_Helloworld </a>。</li><li id="be17" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">高效:在很多测试案例中(单GPU/多GPU/GPU云)，我们发现它比<a class="ae jc" href="https://github.com/ray-project/ray" rel="noopener ugc nofollow" target="_blank"> Ray RLlib </a>更高效。</li><li id="666c" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated"><strong class="ig hi">稳定:比</strong> <a class="ae jc" href="https://github.com/DLR-RM/stable-baselines3" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">稳定得多——基线3</strong></a><strong class="ig hi">【2】利用各种系综方法。</strong></li></ul><p id="bc71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章由Steven Li，和撰写，描述了H项，这是ElegantRL的一个关键设计，它极大地提高了稳定性。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="ff4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jy translated">稳定性一直是深度强化学习(DRL)研究中的一个主要挑战。例如，DQN论文[1]中的学习曲线是如此的不稳定，以至于导致了一种错觉:那些DRL的代理人真的在学习什么吗？</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/518474db59aee30efc99f89e6652afb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q23ARYuoQVXZTab_SPs1jw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Image from [1].</figcaption></figure><blockquote class="kx ky kz"><p id="473a" class="ie if la ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated">“图2中最左边的两幅图显示了在Seaquest和Breakout游戏的训练过程中，平均总奖励是如何变化的。这两个平均奖励图确实非常嘈杂，<strong class="ig hi">给人的印象是学习算法没有取得稳定的进展</strong>。——作者Mnih等人。全部。</p></blockquote><p id="7b15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">稳定性在将DRL应用程序产品化以解决现实世界问题的过程中起着关键作用，这使得它成为DRL研究人员和实践者关注的焦点。最近，许多算法和开源软件被开发出来应对这一挑战。一个流行的开源库<a class="ae jc" href="https://github.com/DLR-RM/stable-baselines3" rel="noopener ugc nofollow" target="_blank">稳定基线3</a>[2]提供了一组可靠的DRL算法实现，可以匹配先前的结果。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="f815" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们引入了一个<strong class="ig hi">哈密顿项(H-term) </strong> [3]，这是ElegantRL中的一个通用附件，可以应用于现有的无模型DRL算法。H项本质上是用计算能力换取稳定性。</p><h1 id="bf5c" class="le lf hh bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">我们的基本想法</h1><p id="ad72" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">在标准RL问题中，决策过程可以建模为马尔可夫决策过程(MDP)。贝尔曼方程给出了MDP问题的最优性条件:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mh"><img src="../Images/cef5f1b1f4057375591869ff66c6f002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFtzZbWgXIrAPcglTGbXMg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">The Bellman equation.</figcaption></figure><p id="eef9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上述等式本身是递归的，因此我们将其展开如下:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mi"><img src="../Images/3579465ede28c79b6f0e26b59f4be3c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__EuodLU7W6C-GUvTdIqpQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">The recursive form. Copyright by AI4Finance-Foundation.</figcaption></figure><p id="da55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在实践中，我们的目标是找到一个最大化Q值的策略。通过采用变分法，我们可以将贝尔曼方程改写成哈密顿方程。然后，我们的目标转变为找到一个最小化系统能量的策略。(详见我们的<a class="ae jc" href="https://www.semanticscholar.org/paper/Quantum-Tensor-Networks-for-Variational-Learning-Liu-Fang/caa14bff1573192b94fe37b8803b6f788d30f472" rel="noopener ugc nofollow" target="_blank">论文</a>【3】)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mj"><img src="../Images/e7ede80403c9378c3968c1326a987d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRrFczmF3I8IMH_XyOMm8A.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">The Hamiltonian Equation. Copyright by AI4Finance-Foundation.</figcaption></figure><h1 id="7d91" class="le lf hh bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">一个简单的附件</h1><p id="7182" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">(在<a class="ae jc" href="https://www.semanticscholar.org/paper/Quantum-Tensor-Networks-for-Variational-Learning-Liu-Fang/caa14bff1573192b94fe37b8803b6f788d30f472" rel="noopener ugc nofollow" target="_blank">论文</a>中)的推导和物理解释可能有点吓人，然而，H项的实际实现超级简单。这里，我们给出了伪代码，并与演员-评论家算法进行了比较(用红色标记):</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mk"><img src="../Images/eedfdfc2e4ff3d772f8fec46d879285f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*hRXHTfpVO6pKcEBGvst-Kw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">The pseudocode of Actor-Critic + H. Copyright by AI4Finance-Foundation.</figcaption></figure><p id="d361" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如第19–20行所示，我们包括了策略网络的额外更新，以最小化H项。与大多数优化单个步骤(批量转换)的算法不同，我们强调来自轨迹(批量轨迹)的顺序信息的重要性。</p><p id="4e45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，优化H项是计算密集型的，由超参数<em class="la"> L </em>(选定轨迹的数量)和<em class="la"> K </em>(每个轨迹的长度)控制。幸运的是，ElegantRL完全支持从单个GPU到数百个GPU的并行计算，这提供了用计算能力换取稳定性的机会。</p><h1 id="e118" class="le lf hh bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">性能赋值</h1><p id="08b4" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">目前，我们已经在几个广泛使用的DRL算法中实现了H项，包括PPO、SAC、TD3和DDPG。这里，我们展示了在基准问题<a class="ae jc" href="https://gym.openai.com/envs/Hopper-v2/" rel="noopener ugc nofollow" target="_blank"> Hopper-v2 </a>上的性能。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ml"><img src="../Images/4c10c7a7109fce90bce7f5f0b9e11051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zVh-i83-mKB2ao5J"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Columative rewards vs. #samples. Copyright by AI4Finance-Foundation.</figcaption></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mm"><img src="../Images/bdbefd1ec9ef9896047b5f82f66fc188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xONvTWGBd0cPu0O3"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Columative rewards vs. training time. Copyright by AI4Finance-Foundation.</figcaption></figure><p id="a7b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就方差而言，很明显ElegantRL大大优于稳定基线3 [2]。8次运行的方差要小得多。此外，ElegantRL中的PPO+H完成5M样本的训练过程比稳定基线快约6倍3 [2]。</p><p id="dd46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们正在将H-term作为一个通用的附加组件来实现，并将很快发布一系列的实验和演示！如果你等不及正式发布的版本，请先查看我们在<a class="ae jc" href="https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl/agents/AgentPPO_H.py" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中的PPO+H实现。</p><p id="61aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献</strong></p><p id="e4f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1] Mnih、Volodymyr、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis Antonoglou、金奎大·威斯特拉和马丁·里德米勒。“用深度强化学习玩雅达利。”ICLR 2013。</p><p id="7511" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]拉芬、安东宁、阿什利·希尔、亚当·格里夫、安西·卡内维斯托、马克西米利安·埃内斯托斯和诺亚·多尔曼。"稳定的基线3:可靠的强化学习实现."<em class="la">机器学习研究杂志</em> (2021)。</p><p id="c174" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]和方，变分强化学习的量子张量网络.机器学习中的量子张量网络研讨会。</p><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hi fi z dy mv ea eb mw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne kr mq"/></div></div></a></div></div></div>    
</body>
</html>