<html>
<head>
<title>Double Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双重下降</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/double-descent-8f92dfdc442f?source=collection_archive---------1-----------------------#2021-04-11">https://medium.com/mlearning-ai/double-descent-8f92dfdc442f?source=collection_archive---------1-----------------------#2021-04-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2a40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习的突破正在迅速改变社会，但偏差-方差权衡，机器学习中最基本的概念之一，似乎与更现代的模型中观察到的现象不一致，如神经网络和梯度推进。最近的两篇论文<a class="ae jc" href="https://arxiv.org/pdf/1912.02292.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>和<a class="ae jc" href="https://arxiv.org/pdf/1812.11118.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>试图通过在统一的性能曲线内调和经典理解和现代实践来弥合这一鸿沟。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="4cad" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">传统观点——“越大的模型越差”</h1><p id="e25d" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">统计学中的传统观点是“模型越大越差”，这由以下偏差方差权衡曲线显示:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/670e9dded9142495a4a9f5ec307b400c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ER-lc08B1DGM_BjhYmLKSQ.png"/></div></div></figure><p id="c3cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们增加假设类(H)的容量时，即当我们使我们的模型更复杂时，模型过度拟合训练数据，因此训练风险/误差达到0。测试风险/错误也遵循相同的趋势，直到我们到达“甜蜜点”，它描述了我们用例的最佳模型，在这一点之后，测试风险增加。</p><p id="c9e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习中的传统智慧建议通过平衡欠拟合和过拟合，基于偏差-方差权衡来控制函数类H的容量。</p><ul class=""><li id="074a" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">如果H太小，H中的所有预测器可能不符合训练数据(即，具有大的经验风险)，因此对新数据的预测很差。</li><li id="bb87" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">如果H太大，经验风险最小化器可能过度拟合训练数据中的虚假模式，导致新示例的准确性差(经验风险小，真实风险大)。</li></ul><p id="5981" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这条曲线的教科书推论是:</p><blockquote class="ln lo lp"><p id="bb92" class="ie if lq ig b ih ii ij ik il im in io lr iq ir is ls iu iv iw lt iy iz ja jb ha bi translated">训练误差为零的模型会过度拟合训练数据，并且通常会概括得很差</p></blockquote><p id="df3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，实践者通常使用越来越大的神经网络来提高测试集的准确性，而不改变任何其他参数，为什么它会提高准确性呢？现代机器学习方法不遵循偏差方差曲线吗？事实上，许多实践者试图达到零训练误差，这与现代的过度适应有着经典的关联。那么到底是什么改变了呢？</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="17ad" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">双重下降</h1><p id="d1f0" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">随着我们增加假设类的复杂性，现代机器学习方法表现出稍微不同的现象。最初的U形曲线符合经典的理解，但是超过某个点(称为插值阈值)，测试风险再次开始降低。米哈伊尔·贝尔金等人<a class="ae jc" href="https://arxiv.org/pdf/1812.11118.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>的论文将这种现象描述为“双重下降”。</p><p id="147e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文将曲线分为两个区域，经典区域(基于经典理解)也称为欠参数区域，现代插值区域也称为“过参数”区域。现代偏差-方差权衡曲线见下图:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lu"><img src="../Images/dd106ec8f73498efaf10a7dd2a51e96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TnyYQwbN0g6ghkFqjd7-A.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx">Double Descent Curve</figcaption></figure><p id="d22b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在现代机器学习方法(例如深度神经网络)中，模型的复杂性也与模型中可学习参数的数量有关。超过某个阈值，如果我们增加可学习参数的数量，我们将达到这种现代插值机制，其中曲线显示测试风险/误差的下降趋势。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="da3b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">达到插值阈值需要多少个参数？</h1><p id="0798" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">虽然它取决于输入特征和许多其他因素，但本文观察到，当参数的数量大致等于样本的数量时，经典的偏差方差权衡U曲线的峰值出现，此后我们可以观察到现代的双重下降机制。</p><p id="d879" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常使用具有大量参数的神经网络。但是为了实现单个输出(回归或两类分类)的插值，人们期望至少需要与数据点一样多的参数。此外，如果预测问题有一个以上的输出(如在多类分类中)，则所需参数的数量应乘以输出的数量。对于神经网络来说，这确实是经验上的情况。因此，例如，像ImageNet这样大的数据集(有1，000，000个示例和103个类)可能需要具有1，000，000，000个参数的网络来实现插值；这比ImageNet的许多神经网络模型都要大。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="136e" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">真实例子的结果</h1><p id="563d" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">本文进行了几个实验来证明这种双下降曲线的存在，其中一个实验是在著名的MNIST数据集上进行的，作者观察了下面的误差与参数数量的关系图:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lz"><img src="../Images/578c8f1d539fab198f7303c2e31821a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8EcwbQYfziVqICjqXEtSQ.png"/></div></div></figure><p id="66a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于MNIST，n(样本数)= 4000，d(维数)= 784，K(类别数)= 10。作者根据预测的参数数量(n.K)观察到插值阈值(在上图中用虚线表示)。(前面提到过。)</p><p id="369e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样如预期的，测试误差在插值阈值之后开始减小。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="f0e7" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">双下降只对神经网络观察到吗？</h1><p id="be7a" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">该论文还指出，类似的现象也见于其他一些机器学习方法，如AdaBoost和Random Forrest。更一般地，作者指出，有证据表明，通过增强决策树和随机福里斯特探索的函数族也有类似的行为。</p><p id="d16a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Abraham J. Wyner等人<a class="ae jc" href="https://www.jmlr.org/papers/volume18/15-240/15-240.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>的一篇期刊给出了经验证据，即当AdaBoost和随机森林与最大(插值)决策树一起使用时，拟合方法的灵活性产生的插值预测值比刚性非插值方法(例如，AdaBoost或具有浅树的随机森林)产生的预测值对训练数据中的噪声更鲁棒。据说这反过来会产生更好的概括。(近)插值树的平均确保了所得到的函数比任何单独的树都要平滑得多，这与许多现实世界问题兼容的归纳偏差相一致。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="4a94" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">随机福里斯特的结果</h1><p id="c82a" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">作者再次使用MNIST数据集的子集进行分类，并观察到以下误差与参数数量的关系图:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ma"><img src="../Images/926674da4a26ca2721e89f2eeff769c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPNNV-1RrAtxHKDn8Oad-w.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><p id="e7b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机森林的复杂度由树的数量(Ntree)和每棵树允许的最大叶子数量(Nmaxleaf)控制。从上图可以看出，在达到一定的模型复杂度(插值阈值)后，测试误差开始下降。</p><h1 id="de03" class="jk jl hh bd jm jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh bi translated">更正式的方法</h1><p id="0088" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">Preetum Nakkiran等人<a class="ae jc" href="https://arxiv.org/pdf/1912.02292.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>的论文表明，双重下降是一种健壮的现象，出现在各种任务、架构和优化方法中。除此之外，本文还展示了一个更一般的双下降概念，它不仅仅依赖于参数的数量。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="08e1" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">有效模型复杂性(EMC)</h1><p id="aa87" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">本文将训练过程的有效模型复杂度定义为模型达到接近0训练误差的最大样本数。有效模型的复杂性不仅取决于数据分布和分类器的架构，还取决于训练过程，特别是增加训练时间会增加EMC。</p><p id="d38f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">形式上有效的模型复杂性定义为:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mg"><img src="../Images/ba5cc6c04b76bb4cf8277f2014bd5138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrRxIvhR6rbyZyK9J18b5g.png"/></div></div></figure><p id="17fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，训练过程/程序T被定义为接受样本列表并输出将数据映射到标签的分类器T(S)的任何程序。</p><p id="8fa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">非正式地说，EMC可以分为三个部分:</p><ul class=""><li id="dcbd" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">在参数化状态下:如果EMC充分小于n，增加其有效复杂性的训练过程T的任何扰动将减少测试误差。</li><li id="479f" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">临界状态:如果EMC大约等于训练样本的数量n，那么增加其有效复杂性的训练过程T的扰动可能减少或增加测试误差。</li><li id="5b36" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">过参数化状态:如果EMC充分大于n，增加其有效复杂性的训练过程T的任何扰动将减少测试误差。</li></ul><p id="09cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是Belkin等人在第一篇论文中观察到的结果的延伸。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="571c" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">更多标签噪声的影响</h1><p id="1d7f" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">当训练完成时(对于固定的大量优化步骤)，该论文执行了几个实验来研究增加规模的模型的测试误差。临界区域在插值点周围表现出明显不同的测试行为，并且通常在测试误差中存在峰值，该峰值在具有标签噪声的设置中变得更加突出。这可以在下图中看到(设置—在Resnet18上训练的Cifar10):</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mh"><img src="../Images/81e908b7a96baa8989ed2fe5d09e74c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rd7uTxfFWmO6q-tV03IcZw.png"/></div></div></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="5f03" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">有时候数据多了会疼？</h1><p id="2f09" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">该论文描述了在没有添加标签噪声的情况下对语言翻译任务执行的实验，其中添加更多数据实际上会在特定机制中损害性能。如下图所示:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mi"><img src="../Images/5e77b7cca9bf574d1c3b8ead1a516f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHdF002cMiQQC0INrsWEKA.png"/></div></div></figure><p id="1a2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">增加样本数量会使曲线向更低的测试误差方向下移。然而，由于更多的样本需要更大的模型来拟合，因此增加样本数量也会将插值阈值(和测试误差峰值)向右移动。</p><p id="c4dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于中等大小的模型(红色箭头)，这两种影响结合在一起，我们看到对4.5倍以上的样本进行训练实际上会损害测试性能。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="4aca" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">训练时间更长可以逆转过度拟合？</h1><p id="3c3b" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">该论文指出，更长时间的训练实际上可以扭转过度拟合！</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/801524ef954d528e02cf106f9d68bcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3tRFaeIdapXkGrtaLgJ3Q.png"/></div></div></figure><p id="85d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的图表显示了作为模型大小和优化步骤数量的函数的测试和训练误差。对于给定数量的优化步骤(固定的y坐标)，测试和训练误差呈现出模型大小的双重下降。对于给定的模型尺寸(固定的x坐标)，随着训练的进行，测试和训练误差减小、增大、再减小；论文称这种现象为<strong class="ig hi">划时代双下降</strong>。</p><blockquote class="ln lo lp"><p id="5610" class="ie if lq ig b ih ii ij ik il im in io lr iq ir is ls iu iv iw lt iy iz ja jb ha bi translated"><em class="hh">一般来说，当模型勉强能够适应训练集时，测试误差的峰值会系统性地出现。</em></p></blockquote><h1 id="81ae" class="jk jl hh bd jm jn mb jp jq jr mc jt ju jv md jx jy jz me kb kc kd mf kf kg kh bi translated">结论</h1><p id="d8da" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">这两篇论文都提出了一个非常有趣的想法，并试图弥合机器学习方法的理论和实践之间的差距。Deep Double Descent的论文进一步指出了某些传统方法可能不总是给出最佳结果的情况，作为一个例子，作者提到，与一般的看法(“数据越多越好”)相反，更多的数据可能实际上导致更差的测试准确性。研究人员应该明白，这种情况只发生在特定的范围内，进一步增加有效模型的复杂性实际上可能会导致更好的结果。</p><p id="923c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管完全理解双重下降背后的机制是一个开放的问题，我希望从业者和研究人员在建立他们的模型时记住这一现象。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="b1db" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">参考</h1><p id="db96" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">有关更多详细信息，请参考以下参考资料:</p><p id="3e2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/pdf/1812.11118.pdf" rel="noopener ugc nofollow" target="_blank"> [1]调和现代机器学习实践和偏差-方差权衡，Mikhail Belkin等人</a></p><p id="4bc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/pdf/1912.02292.pdf" rel="noopener ugc nofollow" target="_blank"> [2]深度双重下降:更大的模型和更多的数据带来的伤害——Preetum nak kiran等人</a></p><p id="a201" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://www.jmlr.org/papers/volume18/15-240/15-240.pdf" rel="noopener ugc nofollow" target="_blank"> [3]解释adaboost和随机森林作为插值分类器的成功。—大卫·米纳斯等人</a></p><p id="11e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]https://openai.com/blog/deep-double-descent/<a class="ae jc" href="https://openai.com/blog/deep-double-descent/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>