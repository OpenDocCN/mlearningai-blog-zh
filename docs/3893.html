<html>
<head>
<title>Convolution Neural Nets and Multi-Class Image Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络与多类图像分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/convolution-neural-nets-and-multi-class-image-classification-fastai-51cd5e59ef19?source=collection_archive---------4-----------------------#2022-11-04">https://medium.com/mlearning-ai/convolution-neural-nets-and-multi-class-image-classification-fastai-51cd5e59ef19?source=collection_archive---------4-----------------------#2022-11-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1622" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="aad5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">2015年，创造一个可以识别鸟类的计算机系统的想法被认为是如此具有挑战性，以至于它成为了这个XKCD笑话的基础。使用fastai，只需写几行代码就可以建立一个卷积神经网络，能够以超过95%的准确率识别鸟类。</p><p id="1887" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">虽然现在我们可以以相当高的精度进行二元分类，但多类分类仍然有点棘手。对于固定数量的训练示例，二元分类通常比多类分类更容易执行。随着您试图学习的类数量的增长，您必须学习的数据量也在增长。</p><p id="b707" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在本文中，我们将建立一个卷积神经网络，它可以从四个不同的类中正确地检测血细胞组。为了建立这个分类器，我们将使用卷积神经网络。由于我们可用的数据集有限，我们将采用归一化、数据扩充、提前停止和剔除等综合技术来提高模型的准确性。你可以在这里找到代码:<a class="ae ka" href="https://github.com/nandangrover/blood-cell-detection-cnn" rel="noopener ugc nofollow" target="_blank">nandangrover/blood-cell-detection-CNN</a></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/acce7c1b91b7eba0ad621ccc56d54bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*OJr5VZI8goSz3l3-.jpg"/></div></figure><h1 id="dca3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">为什么用CNN做图像分类？</h1><p id="d105" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">传统的生理学试验似乎几乎不可能被发现，因为人们对大脑中模式识别的过程知之甚少。如果我们能够创建一个像人类一样能够识别模式的神经网络模型，这将是我们对大脑神经机制的理解向前迈出的一大步。1980年，Kunihiko Fukushima发表了第一个图像分类器Neocognitron [2]，它在CNN中建立了两种类型的层:卷积层和下采样层。</p><p id="76b0" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">CNN已经被证明在解决图像分类问题上非常有效。由于CNN的研究，许多图像数据库，包括MNIST数据库、NORB数据库和CIFAR10数据集，已经显著提高了它们的最高性能。它特别擅长识别视觉数据中的局部和全局结构。边缘和曲线等简单的局部特征可以合并产生角点和形状等更复杂的特征，最后，手写数字或人脸等图片对象具有明显的局部和全局结构。CNN最近被用于医学成像分析，例如膝关节软骨分割。[3].</p><p id="9324" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在此任务中，我们将使用卷积层、BatchNormalization层和ReLU层的多个块构建一个模型。过滤器和内核大小是经过测试和调整的两个参数，以达到预期的结果。在下一节中，我们将更多地讨论为什么过滤器和内核大小如此重要。</p><h2 id="0c60" class="ko if hh bd ig kp kq kr ik ks kt ku io jn kv kw is jr kx ky iw jv kz la ja lb bi translated">过滤</h2><p id="7cd3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">过滤器的数量越多，我们的网络可以从图像数据中提取的抽象就越多。滤波器数量普遍增加的原因是网络在输入层接收原始像素数据。原始数据总是有噪声的，图像数据尤其如此。因此，我们让CNN先从嘈杂的、“肮脏的”原始像素数据中提取一些相关信息。一旦有用的特征被提取出来，CNN就被用来创建更复杂的抽象。这就是为什么过滤器的数量通常会随着网络的深入而增加，尽管情况并非总是如此。出于这个原因，我们使用了多层，过滤器从16个增加到64个。</p><h2 id="4728" class="ko if hh bd ig kp kq kr ik ks kt ku io jn kv kw is jr kx ky iw jv kz la ja lb bi translated">内核大小</h2><p id="8db3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">卷积神经网络基于两个前提:</p><ul class=""><li id="86d4" class="lc ld hh je b jf kb jj kc jn le jr lf jv lg jz lh li lj lk bi translated">本地化的低级功能</li><li id="1178" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk bi translated">在一个地方有用的东西在另一个地方也会有用。</li></ul><p id="22bb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">核的大小应该由我们对手头情况的假设的置信水平来决定。当我们使用1x1内核时，我们有效地声明了低级特征是逐像素的，对相邻像素没有影响，并且应该在所有像素上执行。另一个极端是整个图像大小的核。在这种情况下，CNN变成完全连接的，并且不再是CNN，并且不进行低级特征局部性假设。在这点上，我们选择了内核大小为<strong class="je hi"> 3x3 </strong>，这是在fastai ConvLayer方法的情况下选择的默认参数。</p><h1 id="f3df" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">数据预处理</h1><p id="1bd8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">医学中的深度学习应用有许多限制[4]。其中之一是不确定性的概化发展的模型，即他们的能力，预测的数据来源，而不是那些用于模型训练[5]。在这些条件下，模型的评估可能会导致对其整体性能过于乐观的假设。我们使用标准化、剔除、数据扩充和提前停止等技术来限制这种泛化。</p><h2 id="1b0e" class="ko if hh bd ig kp kq kr ik ks kt ku io jn kv kw is jr kx ky iw jv kz la ja lb bi translated">检查不平衡的类</h2><p id="fb43" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当一个类别(少数群体)包含的样本明显少于另一个类别(多数群体)时，就会出现类别不平衡。在许多问题中，少数群体是利益阶层，即积极阶层。大多数机器学习算法在每个类别的样本数量大致相等时效果最佳。这是因为大多数算法都是为了最大限度地提高精度和减少误差而设计的。<strong class="je hi">图2.1 </strong>显示我们的数据集没有面临这个问题，我们不必担心类别不平衡。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/4642028bb2b795a6dbb01b346524c997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BQEUCe5jd6IE4dxt"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.1</figcaption></figure><h2 id="0872" class="ko if hh bd ig kp kq kr ik ks kt ku io jn kv kw is jr kx ky iw jv kz la ja lb bi translated">数据扩充和标准化</h2><p id="ec21" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">众所周知，机器学习算法的数据越多，可能就越有效。即使数据质量很差，如果模型可以从中提取相关信息，算法也可以胜过原始数据集。对于我们的图像分类器，我们使用了fastai <strong class="je hi"> aug_transforms </strong>方法来扩充图像。我们也已经标准化了我们的图像。数据标准化是确保每个输入参数(本例中为像素)的数据分布的重要步骤。这在训练网络的同时加速了收敛。</p><pre class="kh ki kj kk fd lz ma mb mc aw md bi"><span id="3257" class="ko if hh ma b fi me mf l mg mh">batch_tfms = [ToTensor(), *aug_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.), Normalize()]<br/>cells = DataBlock(blocks=(ImageBlock, CategoryBlock),<br/> get_items=get_image_files,<br/> splitter=RandomSplitter(valid_pct=0.2, seed=42),<br/> get_y=parent_label,<br/> batch_tfms = batch_tfms)</span><span id="bf95" class="ko if hh ma b fi mi mf l mg mh">dls = cells.dataloaders(path/”TRAIN”,bs=128)</span></pre><p id="4227" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">图2.2 </strong>显示了一批扩充和归一化的数据。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/81c9738b45b5a3db28fe21a69508a1df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/0*fYsWXvZwHt-TAGC-"/></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.2</figcaption></figure><h1 id="597c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模型架构</h1><p id="2800" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该模型的架构如图<strong class="je hi">图2.3所示。</strong>二维卷积层是模型的初始层。网络在输入层接收原始像素数据，这解释了为什么过滤器的数量通常在增加。原始数据本来就有噪声，图像数据也不例外。</p><p id="f614" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">该层将包含16个输出过滤器，每个过滤器具有3x3的内核大小，并且将使用relu激活函数。每组层(总共3组)有一组3个Conv2d和3个BatchNorm2d以及3个ReLU激活。批量标准化(也称为batchnorm)的工作原理是对图层激活的平均值和标准差进行平均，并使用这些值对激活进行标准化。但是，这可能会导致问题，因为网络可能需要大量激活才能做出准确的预测。因此，他们添加了两个可学习的参数，gamma和beta，它们将在SGD步骤中更新。</p><p id="011a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在标准化激活以获得新的激活向量y之后，batchnorm层返回gamma*y + beta。因此，我们的激活可以具有任何均值或方差，与前一层结果的均值和标准差无关。这些统计数据是单独学习的，这使得训练我们的模型更加容易。训练和验证之间的行为是不同的:在训练期间，我们使用批平均值和标准偏差来标准化数据，而在验证期间，我们使用训练期间计算的统计数据的移动平均值。</p><p id="397e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这些层之后是AdaptiveAvgPool2D层和AdaptiveMaxPool2D层。通过在最后一层添加这两者，我们让神经网络选择什么是最好的，而不必自己进行实验。卷积层的输出然后被展平并传递到线性层。因为这个线性层是网络的输出层，所以它有4个节点，每个类一个。我们将使用CrossEntropyLossFlat作为损失函数，它最适合多类分类。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mk"><img src="../Images/f93c208fdf21c3a258c7d84243bf9d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2ICBMXShBgR4om5m"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.3</figcaption></figure><h2 id="287d" class="ko if hh bd ig kp kq kr ik ks kt ku io jn kv kw is jr kx ky iw jv kz la ja lb bi translated">拒绝传统社会的人</h2><p id="b6e8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">dropboard层是一个屏蔽，它可以消除某些神经元对下一层的贡献，而对其他所有神经元则不闻不问。丢弃层在训练CNNs时非常有用，因为它们可以防止训练数据过度拟合。如果它们不存在，则第一批训练样本对学习具有不成比例的大的影响。结果，仅出现在随后的样本或批次中的特征的学习将被阻止。我们使用的drop层的速率为0.25，这将导致25%的输入单元掉线。加入脱扣后，准确度由<strong class="je hi"> 91.43% </strong>提高到<strong class="je hi"> 93.6% </strong></p><h1 id="27ad" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">提前停止</h1><p id="a4c9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">提前停止是一种用于规范机器学习模型的创新方法。它通过在验证错误最小时停止训练来实现这一点。<strong class="je hi">图2.3 </strong>显示了我们正在训练的模型。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es ml"><img src="../Images/169cf8334f8244f10998d8eed91e66e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9wGGCjR9bsP8SkG-"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.3</figcaption></figure><p id="bfd7" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">随着时代的推移，算法学习，其在训练集上的误差以及其在验证集上的误差自然减小。然而，过了一段时间后，验证错误停止减少，并开始再次上升。这表明模型开始过度填充训练数据。当验证错误达到某个阈值时，我们只需使用“提前停止”停止训练。</p><h1 id="4460" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">模型评估</h1><p id="d0de" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">根据测试数据预测模型输出后，创建热图<strong class="je hi">图2.4 </strong>。从热图显示的结果可以看出，我们的预测偏向中性粒细胞(95.83%)，而预测单核细胞(75%)的结果最差。虽然结果并不坏，但肯定可以通过增加正在训练的参数数量或使用最先进的模型(如用于我们训练目的的Resnet)来改善这些结果。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mm"><img src="../Images/54ecd33797423bf1806c679257e2987d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*GyBD4V5xByN2t-_A"/></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.4</figcaption></figure><p id="819b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">图2.5 </strong>显示了我们的准确度在每个时期如何提高。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es ml"><img src="../Images/39234ed8c525d70f65b17a4136b28a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yqoijVrVW4DJioXj"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx">Figure 2.5</figcaption></figure><h1 id="1260" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="6737" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">卷积神经网络在神经网络的建立中发挥了巨大的作用。通过建立一个自定义分类器，我们已经达到了CNN模型93.62%的平均多类准确率。虽然可用的数据量有限，但我们已经尝试通过使用诸如归一化、数据增大、丢失和早期停止等方法来解决泛化问题。通过利用相对较小的数据集进行有效的训练，我们的模型显示出对正确的血细胞图像进行分类的高概率。</p><h1 id="a504" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><ol class=""><li id="d3f8" class="lc ld hh je b jf jg jj jk jn mn jr mo jv mp jz mq li lj lk bi translated">k .福岛(1980年)。生物控制论新神经元:不受位置变化影响的模式识别机制的自组织神经网络模型。在<em class="mr"> Biol中。控制论</em>(第36卷)。</li><li id="7c3d" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz mq li lj lk bi translated">Prasoon，a .、Petersen，k .、Igel，c .、Lauze，f .、Dam，e .、Nielsen，m .:使用三平面回旋神经网络之膝盖软骨分割之深层特徵学习。LNCS MICCAI 8150，246–253(2013)</li><li id="a4b3" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz mq li lj lk bi translated">高性能医学:人类和人工智能的融合。纳特。医学。25, 44–56.<a class="ae ka" href="https://doi." rel="noopener ugc nofollow" target="_blank"> https://doi。</a>org/10.1038/s 41591–018–0300–7(2019)。</li><li id="6398" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz mq li lj lk bi translated">Krois，j .，Garcia Cantu，a .，Chaurasia，a .，Patil，r .，Chaudhari，P. K .，Gaudin，r .，Gehrung，s .，和Schwendicke，F. (2021)。用于牙齿图像分析的深度学习模型的可推广性。<em class="mr">科学报道</em>，<em class="mr">11</em>①。<a class="ae ka" href="https://doi.org/10.1038/s41598-021-85454-5" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1038/s41598-021-85454-5</a></li><li id="1b18" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz mq li lj lk bi translated">A.Halevy，P. Norvig和F. Pereira。数据的不合理有效性。IEEE智能系统，24(2):8–12，2009年3月。一</li><li id="28e2" class="lc ld hh je b jf ll jj lm jn ln jr lo jv lp jz mq li lj lk bi translated">代码库:<a class="ae ka" href="https://github.com/nandangrover/blood-cell-detection-cnn" rel="noopener ugc nofollow" target="_blank">nandangrover/血细胞检测-cnn </a></li></ol><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj km mv"/></div></div></a></div></div></div>    
</body>
</html>