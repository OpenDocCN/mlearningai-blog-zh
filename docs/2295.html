<html>
<head>
<title>12 Activation Functions That You May Want To Consider — Part-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你可能要考虑的12个激活功能—第2部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/12-activation-functions-that-you-may-want-to-consider-part-2-93037405c443?source=collection_archive---------3-----------------------#2022-04-09">https://medium.com/mlearning-ai/12-activation-functions-that-you-may-want-to-consider-part-2-93037405c443?source=collection_archive---------3-----------------------#2022-04-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e8351167fe24a28514f3bb193f4a4592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c6w_N_BiGVynKv0j.jpeg"/></div></div></figure><h2 id="e7c2" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">如果你还没有读过这个系列的第一部分，那么这就是第一部分。</h2><p id="635b" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">那么，让我们从下一组激活函数开始吧！</p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es kj"><img src="../Images/99312ededbcea12c3ce9deb87664ef42.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*3gxJmEfDCWx3VVqt.gif"/></div></figure></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="e479" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated"><strong class="ak"> <em class="lm">参数化热路:</em> </strong></h1><p id="3b11" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">参数ReLU是leaky ReLU的变种，用来解决ReLU(垂死ReLU问题)和Leaky ReLU(对负输入值的不一致预测)问题。leaky ReLU和parametric ReLU之间的唯一区别是，parametric ReLU没有预定的或恒定的斜率0.01，而是有一个<strong class="jq hi">参数</strong>(因此得名parametric)供神经网络自行计算，即<strong class="jq hi"><em class="ln">a</em></strong><em class="ln"/>(alpha)。</p><p id="7d54" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:(-∞，∞) </strong></p><p id="d1aa" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/6249250f242e131127dedb7e1064bff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*3ZxV7MKYn0A58T-3Mbypgg.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="631e" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图形:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/c04016888290bc98e2de5493b897bac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TN5rUZu2gVkAi3UbsXqMCw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b132" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优势:</strong></p><ul class=""><li id="d736" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">由于参数<em class="ln">a</em>(α)的存在，它具有基于其学习率微调激活函数的灵活性，不同于ReLU情况下的0和泄漏ReLU情况下的0.01。</li><li id="a0e8" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">ReLU和漏ReLU的所有优点。</li></ul><p id="a5d2" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="1683" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">遭遇<strong class="jq hi">爆炸梯度</strong>问题。</li><li id="96ba" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">微分后，函数变为<strong class="jq hi">线性</strong>。</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="f454" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated"><strong class="ak">指数线性单位(ELU): </strong></h1><p id="2808" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">它是ReLU的变体。该激活函数的重要之处在于，它使用对数曲线修改函数负部分的斜率，以定义负值，这与使用直线的ReLU和leaky ReLU不同。</p><p id="7fe8" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:(-∞，∞) </strong></p><p id="ee7c" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/a59dd2f2b81263c1165aac39ee2d5e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*60wSwRyP24bOQXFHyWyr3Q.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="6e9a" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图形:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/63395d43613c27e729633360d3f47f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O1FVaxJGRLtXQbQy.jpg"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="5af7" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优点:</strong></p><ul class=""><li id="e545" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">它处理垂死的ReLU问题。</li><li id="862a" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">计算渐变时，产生激活而不是让它们为零。</li><li id="2539" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">产生负输出，这有助于网络向正确的方向推动权重和偏差。</li></ul><p id="5614" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="adcc" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">没有解决爆炸梯度的问题。</li><li id="6144" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">由于所涉及的指数运算，计算成本很高。</li><li id="ffaf" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">不学习α值(通常在0.1和0.3之间)。</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="91fe" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated"><strong class="ak"> Softmax功能:</strong></h1><p id="3612" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">简单地说，softmax函数只不过是多个sigmoid函数的组合，因为它也返回每个类的概率。在大多数情况下，该激活函数用于神经网络的最后一层，并用于多类分类。</p><p id="fe5c" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">但是…</p><p id="5d09" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">你们可能都在想，当我们可以使用sigmoid或logistic函数时，为什么还需要使用softmax函数！</p><p id="65a3" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">好吧，这就是答案！</p><p id="8f2d" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">假设我们有5个输出概率值-0.89、0.97、0.78、0.63和0.91。</p><p id="2da9" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">当它们的和不等于1(最大概率值总是1)时，我们如何选择它们中的任何一个来推进我们的分类问题呢？</p><p id="ad05" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">所以，为了解决这个问题，softmax函数来拯救我们。它计算类似于sigmoid/logistic函数的相对概率，并返回每个类的概率。</p><p id="f6ea" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:[0，1] </strong></p><p id="b144" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es mp"><img src="../Images/d3413755961d2a74528e503267c09aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*SgPJz8GshL8kkK2DXdvDzw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="3f0e" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/a4de9c77fec1123e782f17ab724c25ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xY37jUGeSUm70pg-.jpg"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="5098" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优点:</strong></p><ul class=""><li id="51b9" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">这个函数的范围是从<strong class="jq hi"> 0到1 </strong>，所有概率之和将是<strong class="jq hi">等于1</strong>。如果softmax函数用于多分类问题，则它将返回每个类的概率，其中目标类的概率最高。</li></ul><p id="6b47" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="1e9d" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">计算开销很大。</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="eebc" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated"><strong class="ak">唰功能:</strong></h1><p id="ea60" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">这是谷歌研究人员开发的一种自我门控激活功能。该激活函数在图像分类和机器翻译领域表现非常好，优于ReLU激活函数。</p><p id="6786" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:[0，∞) </strong></p><p id="f3de" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/64e28d624a01771e347202e3d7ea6b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*avBGu4A01sFvKsf5PUyNJA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="00f7" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/e95548490202be8e3bb9a5aa95593910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b48-Oybj7FDoaJ2J.jpg"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="1b8a" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优点:</strong></p><ul class=""><li id="7d55" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">本质上是非单调的。</li><li id="e447" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">这是一个<strong class="jq hi">无界</strong>函数，这意味着对于任何大的值，梯度都不会变为零，从而进行有效的学习。</li><li id="b7d5" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">下面是<strong class="jq hi">无界</strong>，这意味着随着输入趋于负无穷大，输出趋于某个常数，因此在模型中引入正则化。</li></ul><p id="73ae" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="c47c" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">它的计算量很大(是的，你猜对了！).</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="4df1" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated">高斯误差线性单位(GELU):</h1><p id="5b40" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">这个激活功能比较新，也比较好。高斯误差线性单位或GeLU是一个简单地将其输入乘以输入处正态分布的累积密度函数的函数。</p><p id="eeda" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">它对于NLP模型非常有效，并且与BERT、ROBERTa和ALBERT高度兼容。这个激活函数是由dropout、zoneout和ReLUs的属性组合而成的。</p><p id="6c5c" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:[0，∞) </strong></p><p id="2f64" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/213378d1010fb56d288358c8b0eb3e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_R_H5UN7e8xLQBCWpxavfg.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="49fc" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图形:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/c9471ed6f01a2770c4706c73484cd489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AUOg25PRomIjPfLc.jpg"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="269d" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优点:</strong></p><ul class=""><li id="05b3" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">它最适合NLP模型(尤其是变压器模型)，并且在NLP的情况下也优于所有其他激活函数。</li><li id="1c54" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">解决渐变消失的问题。</li></ul><p id="5306" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="e6f1" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">2016年上线，所以实际应用还是新手。</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><h1 id="43fb" class="kv iq hh bd ir kw kx ky iv kz la lb iz lc ld le jd lf lg lh jh li lj lk jl ll bi translated"><strong class="ak">标度指数线性单位(SELU): </strong></h1><p id="a2be" class="pw-post-body-paragraph jo jp hh jq b jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki ha bi translated">SELU(标度指数线性单位)是诱导<strong class="jq hi">自归一化</strong>的激活函数。这个激活函数自动收敛于零均值和单位方差。为了移动平均值，它有正值和负值，这在ReLU的情况下是不可能的，因为它不输出负值。</p><p id="e9b1" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">范围:[-2，∞) </strong></p><p id="d739" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">数学表达式:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/28bab0fd407c3aa940e0a854299c59de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*HtHd3_YDGgbZhdmSndxqeg.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Image by author</figcaption></figure><p id="9c99" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">图表:</strong></p><figure class="kk kl km kn fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/e360a537c7df440d800408cb1c17bc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C3ay1AXjEKz-JOmC.jpg"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx"><a class="ae jn" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="4bb0" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">优点:</strong></p><ul class=""><li id="ea42" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">和瑞鲁相比，SELU不会死。</li><li id="3d75" class="lz ma hh jq b jr mi jv mj ja mk je ml ji mm ki me mf mg mh bi translated">SELUs <strong class="jq hi">比其他激活函数</strong>学习得更快更好，无需进一步处理。</li></ul><p id="75b2" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated"><strong class="jq hi">缺点:</strong></p><ul class=""><li id="7bac" class="lz ma hh jq b jr lo jv lp ja mb je mc ji md ki me mf mg mh bi translated">这是一个新的激活函数，仍然需要大量的探索，因此对于实际应用来说是新的。</li></ul></div><div class="ab cl ko kp go kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ha hb hc hd he"><p id="2ebc" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">这就是我今天的全部内容！</p><p id="1566" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">我希望你喜欢这个博客！如果是的话，不要忘记给一些掌声(你想给多少就给多少)😉).</p><p id="0055" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">非常感谢你坚持到最后！</p><p id="805f" class="pw-post-body-paragraph jo jp hh jq b jr lo jt ju jv lp jx jy ja lq ka kb je lr kd ke ji ls kg kh ki ha bi translated">再见！</p><figure class="kk kl km kn fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/00ef864a333b011d70ea4a676b02e1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/0*-SChNFB_SAFUj3dZ.gif"/></div></figure><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl in mx"/></div></div></a></div></div></div>    
</body>
</html>