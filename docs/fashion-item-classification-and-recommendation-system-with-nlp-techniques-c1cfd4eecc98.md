# åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ—¶å°šå•†å“åˆ†ç±»å’Œæ¨èç³»ç»Ÿ

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/fashion-item-classification-and-recommendation-system-with-nlp-techniques-c1cfd4eecc98?source=collection_archive---------0----------------------->

![](img/5b96b60516bd09fda25eb9ef34472e7b.png)

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»å¤šä¸ªåœ¨çº¿é›¶å”®ç½‘ç«™æå–çš„æ•°æ®ï¼Œé€šè¿‡åˆ©ç”¨**è‡ªç„¶è¯­è¨€å¤„ç†(NLP)** æŠ€æœ¯æ¥æ„å»ºç‰©å“å±æ€§å·¥å…·å’Œè£…å¤‡æ¨èç³»ç»Ÿã€‚

æ•´ä¸ªé¡¹ç›®å°†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†:

1.  äº§å“å±æ€§é¢„æµ‹
2.  æœè£…æ¨è

å®Œæ•´ä»£ç è¯·å‚è€ƒ[https://github . com/Rachel sung/thread together-Fashion-Item-class ification-and-Recommendation-System](https://github.com/rachelsung/ThreadTogether-Fashion-Item-Classification-and-Recommendation-System)

# ç¬¬ 1 éƒ¨åˆ†:äº§å“å±æ€§é¢„æµ‹

## **A .æ•°æ®é¢„å¤„ç†**

åŸå§‹æ•°æ®é›†åŒ…å«äº§å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ç‰Œã€äº§å“åç§°ã€æè¿°ã€å“ç‰Œç±»åˆ«å’Œäº§å“è¯¦ç»†ä¿¡æ¯ã€‚

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¤„ç†åŸå§‹æ•°æ®é›†çš„æ­¥éª¤:

a.å°†æ‰€æœ‰åˆ—ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å¤„ç†ã€‚

b.åˆ é™¤åœç”¨è¯:

*   æ ‡è®°æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„å­—ç¬¦ä¸²å¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨
*   åˆ é™¤åœç”¨è¯
*   åˆ é™¤â€œæœªçŸ¥â€ï¼Œå› ä¸ºåœ¨åŸå§‹æ•°æ®é›†ä¸­ï¼Œæœ‰äº›äº§å“ä¿¡æ¯æ˜¯æœªçŸ¥çš„ã€‚

```
**def** remove_stopwords(input_data):
    *'''Tokenize string.*
 *Returns a list.'''*

    **import** **pandas** **as** **pd**
    **import** **numpy** **as** **np**
    **import** **os**
    **from** **collections** **import** Counter
    **import** **nltk**
    **import** **warnings**
    warnings.filterwarnings("ignore")

    *# combine all columns - call combin_col() function*
    data2 = combine_col(input_data)

    *# remove_stopwords*
    **from** **nltk.corpus** **import** stopwords
    **import** **nltk**
    **import** **re**

    regex_word_tokenize = nltk.RegexpTokenizer(r"(\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)")
    nltk_stopwords = list((set(stopwords.words('english'))))

    nltk_stopwords.append('unknown')

    result2 = []
    **for** line **in** data2['combined_data']:
        filtered_words = []
        **if** isinstance(line, str):
            line = re.sub(r'\d+\+*[\- ]*[\-]*',' ',line)
            **for** word **in** regex_word_tokenize.tokenize(line):
                **if** word.isdigit() == **False**:
                    **if** word.lower() **not** **in** nltk_stopwords:
                        filtered_words.append(word.lower())
            result2.append(" ".join(filtered_words))
        **else**:
            result2.append(np.nan)
    data2['rm_sw'] = result2
    **return** data2
```

*   è¯æ±‡è¯æ¡åŒ–

```
**def** lemmatize_word(input_data):
    **import** **pandas** **as** **pd**
    **import** **numpy** **as** **np**
    **import** **os**
    **from** **collections** **import** Counter
    **import** **nltk**
    **import** **warnings**
    warnings.filterwarnings("ignore")

    d1 = remove_stopwords(input_data)

    **from** **nltk.stem** **import** WordNetLemmatizer
    **from** **nltk.corpus** **import** stopwords
    nltk_stopwords = list((set(stopwords.words('english'))))
    regex_word_tokenize = nltk.RegexpTokenizer(r"(\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)")
    lemmatizer = WordNetLemmatizer()

    result3 = []
    **for** i **in** range(len(d1['rm_sw'])):
        lemmatized = []
        **if** isinstance(d1['rm_sw'].iloc[i],str):
            **for** word **in** regex_word_tokenize.tokenize(d1['rm_sw'].iloc[i]):
                lemmatized.append(lemmatizer.lemmatize(word))
            result3.append(" ".join(lemmatized))
        **else**:
            result3.append(d1['rm_sw'].iloc[i])
    d1['lemmatized'] = result3
    d1['final'] = d1['lemmatized'] + " " + d1['brand']
    d1['final_list'] = d1['final'].str.split()
    **return** d1
```

*   è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç å»é‡å¤:å¯¹äºæ‰€æœ‰å±æ€§åŒ–æ•°æ®(è®­ç»ƒæ•°æ®)ï¼Œæ¯ä¸ªå±æ€§éƒ½è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç å’Œå»é‡å¤ï¼Œè¿™æ˜¯é’ˆå¯¹è®­ç»ƒæ¨¡å‹çš„ã€‚

```
LOL = [] *#LOL List of Lists, to store each of the 5 dataframes***for** cat **in** chosen_cats:
    sub = fulldata[fulldata['attribute_name'] == cat] *#gather "sub" set of rows that contain a category (i.e. selecting all rows that have a 'style' in their attribute name)* x = pd.get_dummies(sub['attribute_value']) *#create one-hot encoding for a given subset of data*    merged = sub.merge(x,how='left',on=sub.index) *#merge the one-hot encoding with the category* LOL.append(merged)*#gather the unique tags for each of the 5 attributes*styles = fulldata[fulldata['attribute_name'] == 'style']['attribute_value'].unique()embels = fulldata[fulldata['attribute_name'] == 'embellishment']['attribute_value'].unique()occasi = fulldata[fulldata['attribute_name'] == 'occasion']['attribute_value'].unique()catego = fulldata[fulldata['attribute_name'] == 'category']['attribute_value'].unique()drycle = fulldata[fulldata['attribute_name'] == 'dry_clean_only']['attribute_value'].unique()
```

## B.å•è¯åµŒå…¥

```
**from** **sklearn.feature_extraction.text** **import** TfidfVectorizer

unique_prod_doc = output_data['final'].drop_duplicates(keep = 'first')
corpus = list(unique_prod_doc.values)
vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
training_tfidf_vectors = vectorizer.transform(list(output_data['final']))
```

## C.æ¨¡å‹ç»“æ„

æˆ‘ä»¬æœ€åˆæ„å»ºäº† 4 ç§ä¸åŒçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬:

*   é€’å½’ç¥ç»ç½‘ç»œ(RNN):ä½¿ç”¨æ‰‹å¥—åµŒå…¥çš„é€’å½’ç¥ç»ç½‘ç»œæ¨¡å‹å…·æœ‰ 85.3%çš„é¢„æµ‹å‡†ç¡®åº¦ã€‚
*   ç‰©æµå›å½’:é¢„æµ‹å‡†ç¡®ç‡ 83.1%ã€‚
*   æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ:é¢„æµ‹å‡†ç¡®ç‡ 84.2%ã€‚
*   ç‰©æµå’Œå†³ç­–æ ‘ç»„åˆ:é¢„æµ‹å‡†ç¡®ç‡ä¸º 86.8%ã€‚

ä»¥ä¸‹æ˜¯ç‰©æµå’Œå†³ç­–æ ‘ç»„åˆçš„ä»£ç ï¼Œè¿™æ˜¯æœ€ç»ˆé€‰æ‹©çš„æ¨¡å‹ã€‚

```
*# TFIDF* **from** **sklearn.feature_extraction.text** **import** TfidfVectorizer

unique_prod_doc = output_data['final'].drop_duplicates(keep = 'first')
corpus = list(unique_prod_doc.values)
vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
training_tfidf_vectors = vectorizer.transform(list(output_data['final']))*# Build model* 
**from** **sklearn** **import** preprocessing, linear_model 
**from** **sklearn.model_selection** **import** train_test_split 
**from** **sklearn.linear_model** **import** LogisticRegression 
**from** **sklearn.tree** **import** DecisionTreeClassifier  
**from** **sklearn.metrics** **import** classification_report, confusion_matrix, roc_curve, roc_auc_score, auc, classification_report to_predict = lemmatize_word(data) to_predict['vectorized_doc'] = list(vectorizer.transform(to_predict['final']).toarray()) to_predict_df = to_predict
```

## D.é¢„è¨€ï¼›é¢„æµ‹ï¼›é¢„å‘Š

ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹é¢„æµ‹æœªé¢„æµ‹æ•°æ®é›†çš„ 5 ä¸ªå±æ€§(â€œ*å¹²æ´—*â€ã€â€œ*ç±»åˆ«*â€ã€â€œ*ä¿®é¥°*â€ã€â€œ*é£æ ¼*â€ã€â€œ*åœºåˆ*â€)ï¼Œå¹¶æä¾› excel æ–‡ä»¶ä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºã€‚

# ç¬¬ 2 éƒ¨åˆ†:æœè£…æ¨è

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ¨èç³»ç»Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®æä¾›çš„è¾“å…¥(äº§å“ id æˆ–äº§å“æè¿°)æä¾›æœè£…ç»„åˆã€‚è¾“å‡ºå°†æ˜¯ä¸è¾“å…¥ç›¸å…³çš„æ‰€æœ‰æ¨èæœè£…ã€‚

## A.æ•°æ®é¢„å¤„ç†

è¿™äº›æ–¹æ³•ä¸ç¬¬ 1 éƒ¨åˆ†åŸºæœ¬ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†ä¸å†è®¨è®ºã€‚

## B.æ¨èæ¨¡å‹æ„å»º

æˆ‘ä»¬å‡è®¾æœ‰ 2 ç§è¾“å…¥æ•°æ®ï¼Œä¸€ç§æ˜¯äº§å“ idï¼Œå¦ä¸€ç§æ˜¯äº§å“æè¿°ã€‚å› æ­¤ï¼Œå¯¹äºæ¯ç§è¾“å…¥æ•°æ®ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥è·å¾—å»ºè®®ã€‚

a.è¾“å…¥:äº§å“ Id

å¯¹äºè¿™ç§è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† **Fuzzy Wuzzy:**

```
def recommend_id(test):
    '''Searches user's inputted product id and returns the recommended outfit. '''

    # List of all product id
    strOptions =list(set(df['product_id'].to_list()))

    # Str2match = user input
    str2Match = test

    # Use fuzzywuzzy's process.extract() to get similarity ratio of the most similar product id to user input
    Ratios = process.extract(str2Match,strOptions)

    # Most similar product id to the user input
    highest = process.extractOne(str2Match,strOptions)

    # Product id of the most smilar
    final_prod=highest[0]

    # Top few of outfit code of the most similar products to user input
    outfit_code=df.loc[df['product_id']==final_prod]['outfit_id'].to_list()

    # Get the top one of the outfit code
    outfit_code=outfit_code[0]

    # Return outfit
    final_result=df[df["outfit_id"] == outfit_code]

    return final_result
```

b.è¾“å…¥:äº§å“æè¿°

å¯¹äºè¿™ç§è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ **TFIDF**

```
def recommend_description(test):
    '''Searches user's inputted product description and returns the recommended outfit. '''

    test = test.lower()
    data = list(df.new_column)
    d = []
    for words in data:
        words = str(words)
        d.append(words)

    # creating test_model and dictionary
    test_model = [[word for word in clean(words)] for words in d]
    dictionary = corpora.Dictionary(test_model,prune_at=2000000)

    # constructing corpus
    corpus_model= [dictionary.doc2bow(test) for test in test_model]
    tfidf_model = models.TfidfModel(corpus_model)

    # constructing tfidf based on processed corpus
    corpus_tfidf = tfidf_model[corpus_model]

    # creating the bag of words and calculating tfidf
    test_bow = dictionary.doc2bow([word for word in word_tokenize(test)])
    test_tfidf = tfidf_model[test_bow]

    # calculating similarities between test and original data
    index = similarities.MatrixSimilarity(corpus_tfidf)
    sims = pd.DataFrame(index[test_tfidf])
    sims.columns = ["similaritie"]
    sims["information"] = data
    sims = sims[sims["similaritie"] <= 0.98]
    sims = sims.sort_values(by="similaritie", ascending=False).head(1)

    # get the product's id with the highest similarity
    target_product = list(sims["information"])[0]

    # get the outfit id of the target product
    outfitid = list(df[[v == target_product for v in df['new_column'].tolist()]].outfit_id)[0]

    # return all products with the same outfit id
    target = df[df["outfit_id"] == outfitid]

    return target
```

C.ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ€åä¸€æ­¥æ˜¯å°†æ¨¡å‹ç»„ç»‡æˆä¸€ä¸ªå‡½æ•°ã€‚

```
def get_recommendation(input_str):
    if test[0].isdigit(): 
        result = recommend_id(test)
        result_short = result[['outfit_item_type','product_full_name','product_id']]
    else:
        result = recommend_description(test)
        result_short = result[['outfit_item_type','product_full_name','product_id']]
    for i in result_short.index:
        print(f'\t{result_short.loc[i][0]} : {result_short.loc[i][1]} ({result_short.loc[i][2]})')

def more_rec_details(input_str):
    if input_str.lower().startswith('y'):
        if test[0].isdigit(): 
            result = recommend_id(test)
        else:
            result = recommend_description(test)
        return result[['product_full_name','brand','outfit_item_type','product_id','details','description']]
```

***è¯•è¯•å§ï¼Œé©¬ä¸Šè·å¾—ä½ çš„è£…å¤‡æ¨èï¼*ğŸ˜„**

```
test = input("Enter your content (product_id or product descriptions/details): \n")
get_recommendation(test)

more_details = input("\n\nDo you want more details on the outfit (y/n): \n")
more_rec_details(more_details)
```