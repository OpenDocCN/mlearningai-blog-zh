<html>
<head>
<title>ML-Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/ml-principal-component-analysis-200e992b837?source=collection_archive---------8-----------------------#2022-03-04">https://medium.com/mlearning-ai/ml-principal-component-analysis-200e992b837?source=collection_archive---------8-----------------------#2022-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="106d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">降维</p><p id="4fe1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天，我们将学习PCA，主成分分析。</p><h2 id="b9a8" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">PCA概念和原因</h2><p id="d12c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">PCA的基本概念是降维。当我们有很多数据特征时，数据的维度自然会很高。如果我们已经处理了这些数据，降低维度将是有益的。</p><p id="7cf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它将消除相当不重要的特征，从而降低时间/空间复杂度，并减少数据中的噪声。此外，如果我们用复杂数据训练模型，参数可能会过度拟合。但是如果我们进行降维，这个模型会更一般化，变得更健壮。</p><p id="9b51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有两种方法可以降维。特征选择和特征提取就是这两个。PCA属于特征提取，由于从来没有被告知使用/提取什么特征，所以属于无监督的特征提取。</p><h2 id="727f" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">什么是PCA？</h2><p id="2001" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">那么我们如何做到这一点呢？</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/645e393e6047897bb9a8a583144624b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AuDU4zVPXpTtdSZ1ItogtA.png"/></div></div></figure><p id="5c7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本思路是这样的。我们希望将我们的数据点投射到一个更低维度的跨度中。这样维度就变低了。</p><p id="4bb9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，问题总是如何。我们不知道如何选择投射的向量。</p><p id="5661" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA提出了一种方法，我们找到一个彼此正交的新基，同时保持(最大化)数据的方差，将该数据投影到一个新基上。</p><p id="4423" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样，在新基上的单位向量，在n维中有n个单位向量，它们也是相互正交的。我们称它们为“主要成分”。</p><p id="9ef8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么要最大化方差？</p><p id="6777" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直观地看，投影数据时最大化方差最能保留该数据的“特征”。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/b5876d352e0b925b03e747ddaf89a56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UHPs5ejg1qCwx0vSHoJWA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">max variance vs min variance</figcaption></figure><p id="af5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上图，第一次投影的输出似乎比第二次投影更能代表原始数据。</p><p id="ecae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么我们怎样才能找到一个最能代表数据的基础，怎样才能最大化方差呢？</p><h2 id="621d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">协方差矩阵和特征向量，特征值</h2><p id="fdcf" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">为了找出数据是如何分布的，我们可以使用协方差矩阵。</p><p id="a6be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵如下所示:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ks"><img src="../Images/ea578e8edec7b91df4499a9292e0115d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*mSn083AQBlGnod1MADd8xQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">contains the variance and the covariance of the pairs of the examples</figcaption></figure><p id="4428" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵定义为:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kt"><img src="../Images/7032660fe53b20358335d0e43115aaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Q0Z_hlU23DYws-bdc5DEyw.png"/></div></figure><p id="8c64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，为了使用主成分分析，我们需要集中数据样本。所以E(X)应该是0。</p><p id="0a74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，协方差矩阵将是:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ku"><img src="../Images/ca8165042934cf5f3ea02563687df1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*BV7DqC-TisTGBTed_Ld6Aw.png"/></div></figure><p id="d0fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵有什么意义？</p><p id="f7f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">协方差矩阵包含关于多变量如何相互关联的信息。因此，我们希望找到最大化方差(最大限度地保留特征)的基础来投影我们的数据，协方差矩阵的特征向量就是该基础。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kv"><img src="../Images/f6adbec067e0a61f0b0d59969072cef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*BQ17Wtsh0BHomTfHfs31rw.png"/></div></figure><p id="6d8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，我们现在有了协方差矩阵的特征向量，那么:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kw"><img src="../Images/3bc219ecab8e510ac890366932fedd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Vs4TAotj0W168AjsoSynVQ.png"/></div></figure><p id="1262" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我们的新功能。</p><h2 id="2c0a" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">PCA步骤</h2><ol class=""><li id="91ba" class="kx ky hh ig b ih jx il jy ip kz it la ix lb jb lc ld le lf bi translated">数据正则化(均值-居中-使均值为0)</li><li id="3d3d" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">计算现有要素的协方差矩阵</li><li id="dd3e" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">计算协方差矩阵的特征值和特征向量</li><li id="7e83" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">根据特征值按顺序排列特征向量</li><li id="3400" class="kx ky hh ig b ih lg il lh ip li it lj ix lk jb lc ld le lf bi translated">将数据投影到特征向量中(使用特征向量作为基础)</li></ol><h2 id="b8de" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么将数据投影到协方差矩阵的特征向量上会使方差最大化？</h2><p id="65ac" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">因为我们将数据投影到最能代表数据的线上。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/b5876d352e0b925b03e747ddaf89a56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UHPs5ejg1qCwx0vSHoJWA.png"/></div></div></figure><p id="7812" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，也可以使用一些技术从逻辑上证明这一点。</p><p id="f695" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们要将数据投影到的矢量是某个矢量e，那么投影就是:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ll"><img src="../Images/c5e8f5312a43f558a49eeb6e1778b724.png" data-original-src="https://miro.medium.com/v2/resize:fit:80/format:webp/1*8eiqV5x7q0TxdTVY9Mq-Zg.png"/></div></figure><p id="f2f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么方差就是:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lm"><img src="../Images/56f10329272fb8b571de7870ee30dd27.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*vxWJayZ44mB1dFaJa2vfdQ.png"/></div></figure><p id="7579" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们将数据以平均值为中心，并使平均值为0，则:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ln"><img src="../Images/821fc418a99edd7793d296b258757657.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*oLU4vKhv5-YVcMcLdRM3Iw.png"/></div></figure><p id="693d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们需要最大化方差，那么:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lo"><img src="../Images/5b98f0f4229d93b143661093b09963c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*R23SQn41lnZePamEeaHeXg.png"/></div></figure><p id="0e0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">向量e是一个单位向量，我们想求出向量e，所以用拉格朗日乘数法，</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lp"><img src="../Images/dbaaf81a26dad2942e4740a5325e4db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*8n5Kvd4FPCqyPPfGZ6gYlg.png"/></div></figure><p id="5ff8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据特征向量的定义，e成为协方差矩阵的特征向量，λ成为协方差矩阵的特征值。因此，它证明了协方差矩阵的特征向量最大化投影数据的方差。</p><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh km lt"/></div></div></a></div></div></div>    
</body>
</html>