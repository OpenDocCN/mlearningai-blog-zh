<html>
<head>
<title>Is my AI model biased? — A brief overview of AI Fairness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的AI模型有偏差吗？—人工智能公平性概述</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/is-my-ai-model-biased-a-brief-overview-of-ai-fairness-252a912e5f14?source=collection_archive---------10-----------------------#2022-06-13">https://medium.com/mlearning-ai/is-my-ai-model-biased-a-brief-overview-of-ai-fairness-252a912e5f14?source=collection_archive---------10-----------------------#2022-06-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6d511bad1588d9094ae5b043fe334bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lljBOZwEXmN2Z3Cqwu6GfQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@elisa_ventur?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Elisa Ventur</a> on <a class="ae it" href="https://unsplash.com/s/photos/frustrated?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="1cbb" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="68b3" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">一个晴朗的夜晚，我正在训练一个贷款金额预测模型。这是一个回归问题，我们根据可能授予特定申请人的贷款金额给出预测。在训练过程中进行了成功的实验后，我最终得到了一个性能可以接受的模型，均方差在30左右。</p><p id="3909" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">但是，当我在新的看不见的数据上测试模型时，我突然观察到一件事，那就是肯定存在模型偏差，由于这种偏差，每个性别的预测范围是不同的。</p><p id="42e1" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">想象一下把这个模型投入生产。</p><p id="5bd1" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在训练ML模型时，我们通常会遇到模型偏差，有一些公开可用的技术可以帮助减轻它。</p><h1 id="4681" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">偏见与公平</h1><p id="23ba" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">模型偏差是由于ML模型在决策过程中所做的一些错误假设而产生的。一般来说，偏见意味着对某个特定的个人或群体做出典型的预测。</p><p id="6bdd" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">偏差可以在建模过程中的任何阶段引入，无论是数据收集、数据清理、特征工程、模型训练还是模型测试。在每个阶段进行检查、监控、评估、调查和评价是非常重要的。</p><p id="812b" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">公平指的是不基于个人或群体的属性对他们有偏见。偏见是对特定个人或群体的歧视。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/dc77410b1a970fff3af3bd32f8689989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNqqpNK3ZF-lzsNEqOmYDw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Example of minor vs major entity in a group</figcaption></figure><h1 id="2ee4" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">如何减轻模型偏差？</h1><p id="bcb1" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">让我们讨论一下可以减少任何ML模型偏差的不同方法。</p><p id="2ea3" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">一、截然不同的影响:</strong>假设有两组，我们的模型偏向一组。所以，与另一组相比，有偏见的一组肯定得到了一些积极的结果。不同的影响计算为每组内积极结果之间的比率。</p><p id="35b0" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">为了减少视差的影响，我们可以使用一种称为<strong class="ju hi"> <em class="la">视差影响消除器</em> </strong>的预处理技术。</p><p id="3743" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">二。对抗性去偏置:</strong>这是一种在模型训练中执行的技术。基本上，我们训练两个模型。一个模型试图从特征和实际目标中学习。第二模型或对抗模型试图从原始特征中学习，但是目标取自第一模型的预测。对抗模型将无法在敏感属性上表现良好，这就是我们如何能够在第一个模型中检测到偏差。</p><p id="7fba" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">三世。均衡赔率:</strong>这是一种后处理方法，其中分类器以相同的方式处理每组，如果它们具有相同的错误率。</p><h1 id="4e6a" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">使用Python处理偏差</h1><p id="bf70" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">python中有一些库可以帮助减少偏差。</p><p id="aa86" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi"> i. FairLearn: </strong>我们可以使用FairLearn，这是一个python库，用于计算准确性、精确度等指标。基于性别等敏感特征。它也有一个减少模块，通过它我们可以减少人口均等。更多关于FairLearn的细节可以在<a class="ae it" href="https://fairlearn.org/v0.7.0/quickstart.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="61e9" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">二世。人工智能公平360: </strong> AIF 360最初是由IBM开发的，我们可以使用它来减轻任何ML模型的偏差。因此，我们可以在不同的阶段引入模型处理，以使模型更加公平。不同的阶段是预处理、加工中和后处理。在这里通过AIF 360 <a class="ae it" href="https://aif360.readthedocs.io/en/stable/modules/algorithms.html#id12" rel="noopener ugc nofollow" target="_blank">探索不同的模型处理方法。</a></p><h1 id="de7d" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">参考</h1><p id="c2a6" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我想感谢以下博文的作者写了关于人工智能中的公平，我请求你浏览一下这些链接。</p><ol class=""><li id="a90a" class="lb lc hh ju b jv kq jz kr kd ld kh le kl lf kp lg lh li lj bi translated"><a class="ae it" href="https://towardsdatascience.com/algorithm-fairness-sources-of-bias-7082e5b78a2c" rel="noopener" target="_blank">你的模型做出不公平预测的5个原因</a></li></ol><p id="aaff" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">2.<a class="ae it" href="https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1" rel="noopener" target="_blank"> AI公平性——不同冲击消除器的说明</a></p><p id="a4d1" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">3.<a class="ae it" href="https://towardsdatascience.com/reducing-bias-from-models-built-on-the-adult-dataset-using-adversarial-debiasing-330f2ef3a3b4" rel="noopener" target="_blank">使用对抗性去偏置减少模型偏置</a></p><p id="e15f" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">4.<a class="ae it" rel="noopener" href="/data-from-the-trenches/measuring-fairness-in-machine-learning-models-2be070fab712">衡量机器学习模型中的公平性</a></p><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb in ln"/></div></div></a></div></div></div>    
</body>
</html>