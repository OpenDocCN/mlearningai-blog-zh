<html>
<head>
<title>What is Kernel PCA? using R &amp; Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是核主成分分析？使用研发Python</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/what-is-kernel-pca-using-r-python-4864c2471e62?source=collection_archive---------1-----------------------#2021-06-24">https://medium.com/mlearning-ai/what-is-kernel-pca-using-r-python-4864c2471e62?source=collection_archive---------1-----------------------#2021-06-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="b3e8" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">在艾里面</h2><div class=""/><p id="27d6" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">4行简单代码，对非线性可分离数据应用最先进的PCA。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/3e0f92661c192bab75c3707732ac0d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJGKBf3MhYn9kU9AA4O3WA.jpeg"/></div></div></figure><p id="a9fd" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">什么是PCA首先？</p><p id="9bb7" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">主成分分析是一种统计过程，允许我们<strong class="ip hr">汇总/提取</strong>解释整个数据集的唯一重要数据。</p><p id="4080" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">主成分分析是当今最流行的多元统计技术之一。PCA是MVDA多元数据分析的基本方法</p><p id="ef88" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">它已被广泛应用于模式识别和信号处理领域以及统计分析中，以降低维数，换句话说，只理解和提取解释整个数据的重要因素。从而有助于避免处理不必要的数据。</p><p id="098d" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">既然我们对pca有了基本的了解。</p><p id="aa9b" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">让我们了解什么是<strong class="ip hr">核主成分分析。</strong></p><p id="ecf5" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">核PCA利用rbf径向基函数将非线性可分数据转换到更高维度，使其可分。因此，它在非线性数据中表现更好。</p><p id="2749" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">让我们加载我们的数据，定义X和Y，将数据集分解成序列&amp;测试并缩放它，以减少数据点的数量。您可以保存我们在应用任何模型之前经常需要使用的数据预处理模板。</p><p id="61f4" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">在本例中，我们将使用一个著名的可用数据集“wine_quality ”,假设我们有葡萄酒及其客户的关键组成因素。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/3e6500111f7fa2cfc17fe0e6242f055f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3AfkcabMkz_1tZkt8bLblw.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx">Wine Data set</figcaption></figure><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="f04d" class="kh ki hh kd b fi kj kk l kl km">#Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="f220" class="kh ki hh kd b fi kn kk l kl km">#Importing the dataset<br/>dataset = pd.read_csv('Wine.csv')<br/>X = dataset.iloc[:, 0:13].values<br/>y = dataset.iloc[:, 13].values</span><span id="b69b" class="kh ki hh kd b fi kn kk l kl km">#Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><span id="fec2" class="kh ki hh kd b fi kn kk l kl km">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><blockquote class="ko"><p id="d7db" class="kp kq hh bd kr ks kt ku kv kw kx jk dx translated">应用核主成分分析的时间</p></blockquote><pre class="ky kz la lb lc kc kd ke kf aw kg bi"><span id="012e" class="kh ki hh kd b fi kj kk l kl km">#Applying Kernel PCA<br/>from sklearn.decomposition import KernelPCA<br/>kpca = KernelPCA(n_components = 2, kernel = 'rbf')<br/>X_train = kpca.fit_transform(X_train)<br/>X_test = kpca.transform(X_test)</span></pre><p id="1a05" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">我们所需要的只是n_components = 2即主成分的个数和核<strong class="ip hr">‘RBF’</strong>，其余的都一样。是的，前面提到的“kpca.fit_transform”我们不需要写<strong class="ip hr">“kpca . fit”</strong>就可以了，正如python自己知道kpca用于转换一样。</p><p id="cc0e" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">然后，我们将在我们的模型中拟合2个主n分量。</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="0152" class="kh ki hh kd b fi kj kk l kl km">#Fitting Logistic Regression to the Training set<br/>from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span></pre><p id="ce72" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">让我们用测试(未知)数据来预测我们的模型，以获得n_components = 2的模型的精度</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="7c00" class="kh ki hh kd b fi kj kk l kl km">#Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="887a" class="kh ki hh kd b fi kn kk l kl km">#Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="9794" class="kh ki hh kd b fi kn kk l kl km">#Another evaluation Metrics<br/>from sklearn import metrics<br/>print('Accuracy Score:', metrics.accuracy_score(y_test, y_pred))</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ld"><img src="../Images/853c9de2485c5c19f4141990ac0feb59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xoye4NU5BLmpv9J1teiWJA.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx">Kernel PCA model accuracy score</figcaption></figure><p id="8921" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">不错。我们的模型对我们的测试数据集(看不见的)数据有100%的准确度，在混淆矩阵(cm)中有完全分离/识别的类</p><p id="2f25" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">让我们用简单的PCA重做一遍，然后比较性能。</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="66e0" class="kh ki hh kd b fi kj kk l kl km">#Simple PCA-------------------------------------<br/>#Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><span id="8433" class="kh ki hh kd b fi kn kk l kl km">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span><span id="c1b0" class="kh ki hh kd b fi kn kk l kl km">#Applying PCA<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components = 2)<br/>X_train = pca.fit_transform(X_train)<br/>X_test = pca.transform(X_test)</span><span id="57cb" class="kh ki hh kd b fi kn kk l kl km">#Fitting Logistic Regression to the Training set<br/>from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="2a90" class="kh ki hh kd b fi kn kk l kl km">#Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="fba4" class="kh ki hh kd b fi kn kk l kl km">#Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="3757" class="kh ki hh kd b fi kn kk l kl km">#Another evaluation Metrics<br/>from sklearn import metrics<br/>print('Accuracy Score:', metrics.accuracy_score(y_test, y_pred))</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es le"><img src="../Images/71999fe0c273968bb3feea7879a07e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzObgYiQIAdnmdXA5m5Nrw.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx">Simple PCA model accuracy score</figcaption></figure><p id="58f9" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">哦，是的！现在，我们的模型有2个n _分量，准确率为97%。</p><p id="14cc" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">那么，到目前为止，我们学到了什么？基于径向的函数' rbf '在从非线性可分离数据中正确识别类别方面表现更好</p><blockquote class="ko"><p id="c1e7" class="kp kq hh bd kr ks lf lg lh li lj jk dx translated">经验法则:当我们的数据是线性可分的时，使用简单的主成分分析；当我们的数据是复杂的、非线性可分的时，使用核“rbf”主成分分析。</p></blockquote><p id="b4ca" class="pw-post-body-paragraph in io hh ip b iq lk is it iu ll iw ix iy lm ja jb jc ln je jf jg lo ji jj jk ha bi translated">让我们把所有的碎片放在一起</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="c569" class="kh ki hh kd b fi kj kk l kl km">#Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="be2e" class="kh ki hh kd b fi kn kk l kl km">#Importing the dataset<br/>dataset = pd.read_csv('Wine.csv')<br/>X = dataset.iloc[:, 0:13].values<br/>y = dataset.iloc[:, 13].values</span><span id="9609" class="kh ki hh kd b fi kn kk l kl km">#Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><span id="76a8" class="kh ki hh kd b fi kn kk l kl km">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span><span id="cc02" class="kh ki hh kd b fi kn kk l kl km">#Applying Kernel PCA<br/>from sklearn.decomposition import KernelPCA<br/>kpca = KernelPCA(n_components = 2, kernel = 'rbf')<br/>X_train = kpca.fit_transform(X_train)<br/>X_test = kpca.transform(X_test)</span><span id="ec8e" class="kh ki hh kd b fi kn kk l kl km">#Fitting Logistic Regression to the Training set<br/>from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="e440" class="kh ki hh kd b fi kn kk l kl km">#Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="d8db" class="kh ki hh kd b fi kn kk l kl km">#Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="3b4c" class="kh ki hh kd b fi kn kk l kl km">#Another evaluation Metrics <br/>from sklearn import metrics<br/>print('Accuracy Score:', metrics.accuracy_score(y_test, y_pred))</span><span id="9d45" class="kh ki hh kd b fi kn kk l kl km">#----------------------------------------<br/>#simple PCA</span><span id="e7b2" class="kh ki hh kd b fi kn kk l kl km">#Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><span id="731f" class="kh ki hh kd b fi kn kk l kl km">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span><span id="f900" class="kh ki hh kd b fi kn kk l kl km">#Applying PCA<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components = 2)<br/>X_train = pca.fit_transform(X_train)<br/>X_test = pca.transform(X_test)<br/>explained_variance = pca.explained_variance_ratio_</span><span id="bb66" class="kh ki hh kd b fi kn kk l kl km">#Fitting Logistic Regression to the Training set<br/>from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="25d7" class="kh ki hh kd b fi kn kk l kl km">#Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="f9f5" class="kh ki hh kd b fi kn kk l kl km">#Making the Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)</span><span id="d9ec" class="kh ki hh kd b fi kn kk l kl km">#Another evaluation Metrics <br/>from sklearn import metrics<br/>print('Accuracy Score:', metrics.accuracy_score(y_test, y_pred))</span></pre><p id="1cfe" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">我建议使用这个模板来应用两者，并观察差异，如果差异很小，我们可以使用简单的主成分分析或者内核主成分分析。在我们的例子中，我们与1–0.97有0.3%的差异，所以我们可以使用3或2个n_components来构建我们的模型。</p><h1 id="b498" class="lp ki hh bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">现在让我们对R执行相同的内核PCA</h1><p id="02ef" class="pw-post-body-paragraph in io hh ip b iq mm is it iu mn iw ix iy mo ja jb jc mp je jf jg mq ji jj jk ha bi translated">最初，我们将从数据预处理步骤开始，从导入数据到将数据分成训练集和测试集，然后进行特征缩放，然后我们必须应用这4行代码来应用内核pca。</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="b08f" class="kh ki hh kd b fi kj kk l kl km">#Applying Kernel PCA<br/>#install.packages('kernlab')<br/>library(kernlab)<br/>kpca = kpca(~., data = training_set[-14], kernel = 'rbfdot', features = 2)<br/>training_set_pca = as.data.frame(predict(kpca, training_set))<br/>head(training_set_pca)</span><span id="2579" class="kh ki hh kd b fi kn kk l kl km">training_set_pca$Customer_Segment = training_set$Customer_Segment</span><span id="6a65" class="kh ki hh kd b fi kn kk l kl km">test_set_pca = as.data.frame(predict(kpca, test_set))<br/>test_set_pca$Customer_Segment = test_set$Customer_Segment</span></pre><p id="d724" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">首先要应用内核pca，我们需要kernellab包，其后的波浪号' ~ '是DV ~ IV的分隔符，后跟点'.'指包括所有的列(iv)。</p><p id="16cf" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">然后，我们必须调用主成分数为2的核= 'rbfdot ',即特征= 2</p><p id="61d2" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">一旦完成，我们将使用从训练集中预测两个最重要的主成分</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es mr"><img src="../Images/e7ccf9f9a7c99b194e3b24536ee49ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*mjOVBFf5OB-pa72XPJaC0A.jpeg"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">head(training_set_pca)</figcaption></figure><p id="0808" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">但是，如果您发现我们新保存的结果training_set_pca没有因变量列，我们将添加它，然后我们将对测试集重复同样的2个步骤…..搞定了。</p><p id="c12a" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">在我们的训练和测试数据集中，有两个最重要的主成分。现在是时候建立一个模型并预测它的准确性了。</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="f562" class="kh ki hh kd b fi kj kk l kl km">#Fitting our data to a svm model<br/>library(e1071)<br/>classifier = svm(formula = Customer_Segment ~ .,data = training_set,type = 'C-classification',kernel = 'linear')</span><span id="fa24" class="kh ki hh kd b fi kn kk l kl km">#Predicting the Test set results<br/>y_pred = predict(classifier, newdata = test_set[-14])</span><span id="ad15" class="kh ki hh kd b fi kn kk l kl km">#the Confusion Matrix<br/>cm = table(test_set[, 14], y_pred)</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ms"><img src="../Images/11fbd18d53c85140d04a54cc0fc62315.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZwwujljCuNESJZj3966k1A.jpeg"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">Confusion Matrix Results in R</figcaption></figure><blockquote class="ko"><p id="08f8" class="kp kq hh bd kr ks kt ku kv kw kx jk dx translated">很好，我们有完美的混淆矩阵结果。</p></blockquote><p id="ac77" class="pw-post-body-paragraph in io hh ip b iq lk is it iu ll iw ix iy lm ja jb jc ln je jf jg lo ji jj jk ha bi translated">让我们把所有的R代码放在一起</p><pre class="jm jn jo jp fd kc kd ke kf aw kg bi"><span id="2f2e" class="kh ki hh kd b fi kj kk l kl km">#Kernel PCA</span><span id="4381" class="kh ki hh kd b fi kn kk l kl km">#Importing the dataset<br/>dataset = read.csv(file.choose())</span><span id="47b8" class="kh ki hh kd b fi kn kk l kl km">#Splitting the dataset into the Training set and Test set<br/>#install.packages('caTools')<br/>library(caTools)<br/>set.seed(123)<br/>split = sample.split(dataset$Customer_Segment, SplitRatio = 0.7)<br/>training_set = subset(dataset, split == TRUE)<br/>test_set = subset(dataset, split == FALSE)<br/>nrow(training_set)/nrow(dataset)<br/>nrow(test_set)/nrow(dataset)</span><span id="38fc" class="kh ki hh kd b fi kn kk l kl km">#Feature Scaling [-14] refers exclude the 14th column i.e.DV<br/>training_set[-14] = scale(training_set[-14])<br/>test_set[-14] = scale(test_set[-14])</span><span id="5e4e" class="kh ki hh kd b fi kn kk l kl km">#Applying Kernel PCA<br/>#install.packages('kernlab')<br/>library(kernlab)<br/>kpca = kpca(~., data = training_set[-14], kernel = 'rbfdot', features = 2)</span><span id="4da4" class="kh ki hh kd b fi kn kk l kl km">training_set_pca = as.data.frame(predict(kpca, training_set))<br/>head(training_set_pca)<br/>training_set_pca$Customer_Segment = training_set$Customer_Segment</span><span id="3aaf" class="kh ki hh kd b fi kn kk l kl km">test_set_pca = as.data.frame(predict(kpca, test_set))<br/>test_set_pca$Customer_Segment = test_set$Customer_Segment</span><span id="8e32" class="kh ki hh kd b fi kn kk l kl km">#fitting data to a svm model<br/>library(e1071)<br/>classifier = svm(formula = Customer_Segment ~ .,<br/>                 data = training_set,<br/>                 type = 'C-classification',<br/>                 kernel = 'linear')</span><span id="1a74" class="kh ki hh kd b fi kn kk l kl km">#Predicting the Test set results<br/>y_pred = predict(classifier, newdata = test_set[-14])</span><span id="fdd6" class="kh ki hh kd b fi kn kk l kl km">#The Confusion Matrix<br/>cm = table(test_set[, 14], y_pred)</span></pre><blockquote class="mt mu mv"><p id="3baa" class="in io mw ip b iq ir is it iu iv iw ix mx iz ja jb my jd je jf mz jh ji jj jk ha bi translated">现在，对于那些希望用R编程实现更多PCA的人，我有一个全新的课程来实现各种类型的PCA。大数据PCA，随机森林PCA进一步分为分类和回归，广义Boosted模型PCA(GBM)，广义线性模型PCA(GLMNET)，集成PCA，fs caret PCA等等。</p></blockquote><p id="497d" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">感谢您抽出时间阅读完。我尽我所能保持它的简短，记住在我们的日常生活中使用这个代码。</p><p id="6d53" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">我希望你喜欢它。</p><p id="945a" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">请随意提问，因为“好奇心导致完美”</p><p id="726e" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">我的另类网络存在，脸书，博客，<a class="ae na" href="https://www.linkedin.com/in/bobrupak/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，Instagram，ISSUU</p><p id="95f6" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">也可以在https://www.quora.com/profile/Rupak-Bob-Roy的Quora @ <a class="ae na" href="https://www.quora.com/profile/Rupak-Bob-Roy" rel="noopener ugc nofollow" target="_blank">上找到</a></p><p id="adca" class="pw-post-body-paragraph in io hh ip b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk ha bi translated">敬请关注更多更新。！祝你有愉快的一天…</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es nb"><img src="../Images/83e41b92e06a7fc4330ed4b12fa1bb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfPIy2ojh9kAO9_6-I_EBA.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx"><a class="ae na" href="https://www.quora.com/profile/Rupak-Bob-Roy" rel="noopener ugc nofollow" target="_blank">https://www.quora.com/profile/Rupak-Bob-Roy</a></figcaption></figure><h1 id="a635" class="lp ki hh bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">~开心享受！</h1><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/49a4da4db203562273f06f11ffbdce94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VTDC3xz_J8p9g7S9.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx">Pexel</figcaption></figure><div class="nc nd ez fb ne nf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hr fi z dy nk ea eb nl ed ef hq bi translated">Mlearning.ai提交建议</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt jv nf"/></div></div></a></div></div></div>    
</body>
</html>