<html>
<head>
<title>Fine-Tuning Bert for Tweets Classification ft. Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调推特分类的伯特。拥抱脸</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf?source=collection_archive---------0-----------------------#2021-12-12">https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf?source=collection_archive---------0-----------------------#2021-12-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cf1b1a22aab153b6f96a887379cd91c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSSld7QWwSggc29EbzX7mQ.png"/></div></div></figure><p id="465f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">来自变压器的双向编码器表示(BERT)是基于google开发的变压器的最新模型。它可以预先训练，然后针对特定任务进行微调。我们将在这篇文章中看到微调的作用。</p><p id="1a1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将在分类任务上对bert进行微调。任务是对covid相关推文的情绪进行分类。</p><p id="a305" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里我们使用拥抱人脸库来微调模型。拥抱脸使得从文本预处理到训练的整个过程变得简单。</p><h1 id="76e2" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">伯特(男子名ˌ等于Burt)</h1><p id="35b8" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">Bert在BooksCorpus数据集和英文维基百科上进行了预训练。它在11个自然语言处理任务上获得了最先进的结果。</p><p id="4e37" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">伯特同时接受了两项任务的训练</p><ul class=""><li id="cf81" class="kq kr hh ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">掩蔽语言建模(MLM)——15%的标记被掩蔽，并被训练来预测掩蔽的单词</li><li id="cde1" class="kq kr hh ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">下一句预测(NSP) —给定两个句子A和B，预测B是否跟在A后面</li></ul><p id="0032" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">BERT旨在通过在所有层中联合调节左右上下文来预训练来自未标记文本的深度双向表示。</p><p id="0945" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，预训练的BERT模型可以通过一个额外的输出层进行微调，从而为广泛的任务(如问答和语言推理)创建最先进的模型，而无需对特定任务的架构进行实质性修改。</p><h1 id="78f2" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">资料组</h1><p id="4ba9" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们正在使用冠状病毒推文NLP——在<a class="ae le" href="https://www.kaggle.com/datatattle/covid-19-nlp-text-classification" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上可用的文本分类数据集。</p><p id="6b39" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据集有两个文件Corona _ NLP _ test . CSV<strong class="ir hi"/>(40k个条目)和Corona_NLP_test.csv (4k个条目)。</p><p id="efdc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是训练数据的前五个条目:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lf"><img src="../Images/1fd9f2ed9865548ea21a10877ebfc347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BAQHuxVUdwfbEeYElGXRkA.png"/></div></div></figure><p id="bad9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如你所见，我们的数据中有5个<strong class="ir hi">特征</strong>:用户名、昵称位置、TweetAt、OriginalTweet、情感，但我们只对2个感兴趣，即<strong class="ir hi"> OriginalTweet </strong>包含实际的Tweet和<strong class="ir hi">情感</strong>，它们是我们Tweet的标签。</p><p id="81ee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些推文分为5类——“中立”、“积极”、“极度消极”、“消极”、“极度积极”。因此标签<strong class="ir hi">的数量</strong>为5。</p><h1 id="887e" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">加载数据和预处理</h1><p id="12c7" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们将在这个项目中使用拥抱脸库。我们需要安装两个模块:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="e847" class="lp jo hh ll b fi lq lr l ls lt">pip install transformers</span><span id="0ebe" class="lp jo hh ll b fi lu lr l ls lt">pip install datasets</span></pre><ul class=""><li id="f969" class="kq kr hh ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">变形金刚:拥抱脸变形金刚的实现。我们可以下载大量预先训练好的模型</li><li id="efc6" class="kq kr hh ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">数据集:加载数据集，也可以下载可用于拥抱面部中枢的不同数据集</li></ul><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="c7f3" class="lp jo hh ll b fi lq lr l ls lt">from datasets import load_dataset</span></pre><p id="08db" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里我们使用数据集库中的<code class="du lv lw lx ll b">load_dataset</code>。<code class="du lv lw lx ll b">load_dataset</code>可用于从hugging face hub下载数据集，或者我们可以加载我们的自定义数据集。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="c9f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将数据类型指定为CSV，将文件名作为字典传递给<code class="du lv lw lx ll b">data_files</code>。我们正在将训练和测试文件加载到数据集变量中。</p><p id="7eb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是我们打印变量<code class="du lv lw lx ll b">dataset</code>的输出:</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><h1 id="6bda" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">预处理数据</h1><p id="30de" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们将保持简单，只做2个预处理步骤，即标记化和转换标签为整数。</p><p id="2f67" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">拥抱脸<code class="du lv lw lx ll b">AutoTokenizer</code>负责标记化部分。我们可以下载与我们的模型相对应的标记器，在本例中是bert。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="fbc9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">bert tokenizer自动将句子转换成bert模型所期望的形式的记号、数字和注意力屏蔽。</p><p id="56ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是一个通过分词器的例句</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="4b9d" class="lp jo hh ll b fi lq lr l ls lt">&gt;&gt; tokenizer("Attention is all you need")</span><span id="d052" class="lp jo hh ll b fi lu lr l ls lt">output:<br/>{<br/>'input_ids': [101, 1335, 5208, 2116, 1110, 1155, 1128, 1444, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], <br/>'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]<br/>}</span></pre><p id="563e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为预处理步骤的一部分，我们将执行两个步骤:</p><ul class=""><li id="ec1e" class="kq kr hh ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated">将情绪转换成整数</li><li id="1da7" class="kq kr hh ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated">将推文符号化</li></ul><p id="eb8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将使用数据集的<code class="du lv lw lx ll b">map</code>函数，它类似于熊猫数据框的应用函数。它将函数作为参数，并应用于整个数据集。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="0bcf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的代码中，我们定义了一个将标签转换成整数的方法，并对tweets进行了标记，同时删除了不需要的列。</p><p id="c840" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们已经为训练部分准备好了。</p><h1 id="9ccb" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">培养</h1><p id="43aa" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">有两种方法来训练数据，要么我们编写自己的训练循环，要么我们可以使用拥抱面部库中的训练器。</p><p id="78a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，我们将使用库中的培训师。要使用trainer，首先我们需要定义训练参数，如name、num_epochs、batch_size等。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="8e45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们下载伯特模型，使用<code class="du lv lw lx ll b">AutoModelForSequenceClassificatio</code>类非常简单。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="9c19" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下载的分类模型还需要一个参数<code class="du lv lw lx ll b">num_labels</code>，它是我们数据中的类的数量。在bert模型的末尾附加了一个线性层，以给出与类的数量相等的输出。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a46e" class="lp jo hh ll b fi lq lr l ls lt">(classifier): Linear(in_features=768, out_features=5, bias=True)</span></pre><p id="5dbf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的线性层被自动添加为最后一层。由于bert输出大小为768，并且我们的数据有5个类，因此添加了一个线性图层，其中in_features=768，out_features为5。</p><p id="8d2a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在开始训练之前，我们将把训练数据分成训练集和评估集。我们有40k的培训和1k的评估集。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="d554" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们使用一个拥抱脸训练器，我们需要导入模块<code class="du lv lw lx ll b">Trainer </code>并传递模型、数据集和训练参数给它。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="96fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就这样，现在我们准备好开始训练了。我们需要在训练器上调用<code class="du lv lw lx ll b">train </code>方法，训练将开始</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="46ea" class="lp jo hh ll b fi lq lr l ls lt">trainer.train()</span></pre><p id="c129" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">训练将运行3个时期，可通过训练参数进行调整。</p><p id="a654" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦训练完成，我们可以运行<code class="du lv lw lx ll b">trainer.evalute()</code>来检查准确性，但在此之前，我们需要导入指标。</p><figure class="lg lh li lj fd ii"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="7505" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据集库提供了广泛的指标。我们在这里使用的是准确性。根据我们的数据，仅通过3个时期的训练，我们就获得了83%的准确率。</p><p id="2887" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过更多时间的训练或对数据进行更多预处理，如从推文中删除提及和不必要的混乱，可以进一步提高准确性，但这是以后的事情了。</p><p id="c23f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢阅读。</p><ul class=""><li id="a9ad" class="kq kr hh ir b is it iw ix ja ks je kt ji ku jm kv kw kx ky bi translated"><a class="ae le" href="https://www.linkedin.com/in/codistro/" rel="noopener ugc nofollow" target="_blank">我的Linkedin </a></li><li id="eaab" class="kq kr hh ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated"><a class="ae le" href="https://github.com/codistro/Articles/blob/main/covid_tweet_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li><li id="edfc" class="kq kr hh ir b is kz iw la ja lb je lc ji ld jm kv kw kx ky bi translated"><a class="ae le" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱紧脸</a></li></ul><div class="ma mb ez fb mc md"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">medium.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr in md"/></div></div></a></div></div></div>    
</body>
</html>