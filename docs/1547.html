<html>
<head>
<title>Explainable AI: A Comprehensive Review of the Main Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释人工智能:主要方法综述</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7?source=collection_archive---------1-----------------------#2022-01-04">https://medium.com/mlearning-ai/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7?source=collection_archive---------1-----------------------#2022-01-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a097" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">近年来，随着各种类型的AI模型(树、神经网络等)的传播。)，可解释的人工智能的角色变得至关重要。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/1104f00d4ceeb858bd059112002da3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJ3zljWvTyD1w3NcCdODjw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">[Image by Author]</figcaption></figure><p id="46df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们来定义一下什么是可解释的AI:</p><blockquote class="js jt ju"><p id="2fb5" class="ie if jv ig b ih ii ij ik il im in io jw iq ir is jx iu iv iw jy iy iz ja jb ha bi translated"><strong class="ig hi">可解释的人工智能(XAI) </strong>是一组过程和方法，允许人类用户理解和信任机器学习算法创建的结果和输出。可解释的人工智能用于描述人工智能模型、其预期影响和潜在偏差。它有助于在人工智能决策中描述模型的准确性、公平性、透明度和结果。[IBM]</p></blockquote><p id="1fd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将一起去看用于可解释人工智能的主要方法(SHAP，石灰，树代理等。)，以及他们的特点。当然，它们并不是迄今存在的所有方法，但无论如何，我认为这份清单已经足够详尽了。</p></div><div class="ab cl jz ka go kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="ha hb hc hd he"><p id="399d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另外，专门针对它们的文章将很快到达，其中包含Python的见解和实现，所以请保持最新状态！😜</p><h1 id="e673" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">内容</h1><p id="a714" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">下面是我们将一起分析的可解释性技术的列表:</p><ul class=""><li id="bc8c" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated">SHAP</li><li id="c35a" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">石灰</li><li id="4997" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">排列重要性</li><li id="4d3c" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">部分相关图</li><li id="17c3" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">莫里斯敏感性分析</li><li id="30dd" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">累积局部效应</li><li id="c72f" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">锚</li><li id="a6a4" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">对比解释法(CEM)</li><li id="c18c" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">反事实的例子</li><li id="6ffc" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">集成渐变</li><li id="bf6b" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">通过递归分割的全局解释(GIRP)</li><li id="56a2" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">原破折号</li><li id="5d0b" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">可扩展贝叶斯规则列表</li><li id="e683" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">树替代物</li><li id="5895" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">可解释增压机(EBM)</li></ul><p id="cab1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">很长的名单，对吧？别害怕，我会很轻松的😄</p><p id="6fbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，在开始之前，我们必须介绍一下<strong class="ig hi">可解释性水平</strong>的概念。可解释技术主要分为两类:全局和局部。</p><ul class=""><li id="9ddb" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated"><strong class="ig hi">全球</strong>:他们从总体上解释这种模式，指出其通用的操作规则。</li><li id="4eee" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">本地:他们解释每一个数据，模型是如何推理的，以及导致特定输出的规则。</li></ul><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">GIF by The Office on <a class="ae lz" href="https://giphy.com/gifs/theoffice-nbc-the-office-tv-WsNbxuFkLi3IuGI9NU" rel="noopener ugc nofollow" target="_blank">gyphy.com</a></figcaption></figure><h1 id="3548" class="kg kh hh bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">解释技巧</h1><h2 id="9dff" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP </a></h2><p id="5aba" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi">SHAP(Shapley Additive explaints)</strong>是一个使用SHapley值解释任何模型输出的框架，SHapley值是一种博弈论方法，常用于最优信贷分配。虽然这可以用于任何黑盒模型，但SHAP可以在特定的模型类(如树集合)上更有效地计算。</p><p id="127d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此方法是附加功能属性方法类的成员；特征属性指的是这样的事实，即要解释的结果(例如，分类问题中的类别概率)相对于基线(例如，训练集中该类别的平均预测概率)的变化可以以不同的比例归因于模型输入特征。</p><p id="b1c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SHAP既可以在全球使用<strong class="ig hi">，也可以在本地使用</strong>。</p><h2 id="22e2" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">石灰</a></h2><p id="ae16" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi">局部可解释模型不可知解释(LIME) </strong>是一种在任何黑盒模型预测的决策空间周围拟合<strong class="ig hi">代理玻璃盒模型的方法</strong>。LIME明确尝试对任何预测的局部邻域进行建模。LIME的工作原理是干扰任何单个数据点，并生成合成数据，这些数据由黑盒系统进行评估，最终用作玻璃盒模型的训练集。</p><p id="6858" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">石灰被设计成局部施用<strong class="ig hi"/>。</p><h2 id="864f" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://christophm.github.io/interpretable-ml-book/feature-importance.html" rel="noopener ugc nofollow" target="_blank">排列重要性</a></h2><p id="86eb" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">思路如下:<strong class="ig hi">特征重要性</strong>可以通过看<strong class="ig hi">得分多少</strong>(准确率，F1，R等)来衡量。—我们感兴趣的任何分数)<strong class="ig hi">当功能不可用时</strong>减少。</p><p id="e7aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为此，可以从数据集中移除一个特征，重新训练估计器并检查分数。</p><p id="fc78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，排列重要性只能全局应用<strong class="ig hi"/>。</p><h2 id="f9f4" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">部分依赖图</a></h2><p id="9140" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi">部分依赖图(短PDP或PD图)</strong>显示了一个或两个特征对机器学习模型的预测结果的边际效应。部分相关性图可以显示目标和特征之间的<strong class="ig hi">关系是线性的、单调的还是更复杂的。</strong></p><p id="87f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于基于<strong class="ig hi">扰动的</strong>可解释性方法来说，它是相对快速的。PDP假设功能之间的独立性，当不满足这一点时，可能会误导可解释性</p><p id="69b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">至于前一个，只能全局<strong class="ig hi">应用</strong>。</p><h2 id="be36" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Morris_method" rel="noopener ugc nofollow" target="_blank">莫里斯敏感度分析</a></h2><p id="a043" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">这是一个<strong class="ig hi">一次一步(OAT)全局灵敏度分析</strong>，其中每次运行仅调整一个输入的级别(离散值)。相对于其他敏感性分析算法，莫里斯方法速度更快(模型执行次数更少)，但代价是无法区分相互作用的非线性。<strong class="ig hi">这通常用于筛选对进一步分析足够重要的输入。</strong></p><p id="5a32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样，它只能全局应用<strong class="ig hi"/>。</p><h2 id="d3d2" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://christophm.github.io/interpretable-ml-book/ale.html" rel="noopener ugc nofollow" target="_blank">累积局部效应(ALE) </a></h2><p id="2ced" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi">累积局部效应(ALE) </strong>是一种计算特征效应的方法。该算法为表格数据的分类和回归模型提供了模型不可知(黑盒)的全局解释。ALE解决了部分相关图(PDP)的一些关键缺点。</p><p id="d755" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然违反直觉，但它只能适用于全球的<strong class="ig hi">。</strong></p><h2 id="e8da" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf" rel="noopener ugc nofollow" target="_blank">主播</a></h2><p id="ee98" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">锚背后的想法是用称为锚的高精度规则来解释复杂模型的行为。这些锚是确保具有高度置信度的某个预测的局部充分条件。</p><p id="7df2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由此可见，它只能在本地<strong class="ig hi">应用</strong>。</p><h2 id="31f7" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html" rel="noopener ugc nofollow" target="_blank">对比解释法(CEM) </a></h2><p id="3692" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi"> CEM </strong>根据相关肯定(PP)和相关否定(PN)为分类模型生成基于实例的局部黑盒解释。不仅强调什么应该<strong class="ig hi">最低限度地和充分地</strong>存在以证明神经网络对输入示例的分类(相关的肯定)，而且强调什么应该<strong class="ig hi">最低限度地和必然地不存在</strong>(相关的否定)，以便形成更完整和全面的解释。</p><p id="ada5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CEM设计用于局部应用<strong class="ig hi">。</strong></p><h2 id="3347" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html" rel="noopener ugc nofollow" target="_blank">反事实事例</a></h2><p id="beb4" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">反事实解释“询问”一个模型，以显示<strong class="ig hi">为了翻转整体预测，许多单个特征值必须如何改变。对结果或情况的反事实解释采取“如果没有发生，就不会发生”的形式。在机器的上下文中，学习分类器将是感兴趣的实例，并且将是由模型预测的标签。</strong></p><p id="c7d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">反事实实例被设计成在本地应用<strong class="ig hi">。</strong></p><h2 id="67bf" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://www.tensorflow.org/tutorials/interpretability/integrated_gradients" rel="noopener ugc nofollow" target="_blank">综合渐变</a></h2><p id="9489" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">集成梯度旨在基于模型输出相对于输入的梯度，将<strong class="ig hi">重要性值赋予机器学习模型的每个输入特征</strong>。它有许多用例，包括理解特性重要性、识别数据偏差和调试模型性能。</p><p id="52b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">集成梯度设计用于局部应用<strong class="ig hi"/>。</p><h2 id="9f5a" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://arxiv.org/pdf/1802.04253.pdf" rel="noopener ugc nofollow" target="_blank">通过递归分割(GIRP)进行全局解释</a></h2><p id="01a7" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">一个紧凑的二叉树，通过使用输入变量的贡献矩阵表示模型中隐含的<strong class="ig hi">最重要的决策规则</strong>，对ML模型进行全局解释。为了生成解释树，<br/>统一过程通过最大化分割空间之间分割变量的平均贡献差来递归分割输入变量空间。</p><p id="5b5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所猜测的，GIRP只能在全球范围内应用<strong class="ig hi"/>。</p><h2 id="7dcd" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://arxiv.org/abs/1707.01212" rel="noopener ugc nofollow" target="_blank">原破折号</a></h2><p id="ebb8" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">一种在现有机器学习程序中寻找“原型”的新方法。原型可视为对模型预测能力有较大影响的数据子集。原型的要点是这样说的，如果你删除了这些数据点，模型就不能正常工作，这样人们就可以理解是什么在驱动预测。</p><p id="bd0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">原破折号只能局部应用<strong class="ig hi"/>。</p><h2 id="3d10" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://www.seltzer.com/assets/publications/Scalable-Bayesian-Rule-Lists.pdf" rel="noopener ugc nofollow" target="_blank">可扩展贝叶斯规则列表</a></h2><p id="848d" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">从数据中学习并创建决策规则列表。它们的逻辑结构是一系列IF-THEN规则，与决策列表或单边决策树相同。</p><p id="5c2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可扩展的贝叶斯规则列表既可以在全局<strong class="ig hi">使用，也可以在局部</strong>使用。</p><h2 id="8035" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://christophm.github.io/iml/reference/TreeSurrogate.html" rel="noopener ugc nofollow" target="_blank">树代理</a></h2><p id="a30c" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated"><strong class="ig hi">树代理是一种可解释的模型，它被训练成近似黑盒模型的预测。通过解释代理模型，我们可以得出关于黑箱模型的结论。策略树很容易被人类理解，并提供对未来行为的定量预测。</strong></p><p id="50aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">树代理既可以在全球使用<strong class="ig hi">，也可以在本地使用</strong>。</p><h2 id="c2dc" class="ma kh hh bd ki mb mc md km me mf mg kq ip mh mi ku it mj mk ky ix ml mm lc mn bi translated"><a class="ae lz" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">可解释增压机(EBM) </a></h2><p id="c930" class="pw-post-body-paragraph ie if hh ig b ih le ij ik il lf in io ip lg ir is it lh iv iw ix li iz ja jb ha bi translated">循证医学是微软研究院开发的一个可解释的模型。它使用现代机器学习技术，如装袋、梯度推进和自动交互检测，为传统gam(广义加性模型)注入新的活力。</p><p id="d1bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可解释提升机器(EBM)是一个基于树的、循环梯度提升的广义加法模型，具有自动交互检测功能。EBM通常与最先进的黑盒模型一样精确，同时保持完全可解释性。虽然EBM的训练速度通常比其他现代算法慢，但EBM在预测时非常紧凑和快速。</p><p id="d7ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ECM既可以在全球<strong class="ig hi">使用，也可以在本地</strong>使用。</p></div><div class="ab cl jz ka go kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="ha hb hc hd he"><p id="ca84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文到此为止。非常感谢你能走到这一步。</p><p id="100a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">再见，<br/>弗朗西斯科</p><div class="mo mp ez fb mq mr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">medium.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf jm mr"/></div></div></a></div></div></div>    
</body>
</html>