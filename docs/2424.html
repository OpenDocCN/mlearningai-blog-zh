<html>
<head>
<title>Identifying Nepali Politician from audio</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从音频中识别尼泊尔政治家</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/identifying-nepali-politician-from-audio-c0365f74f8ef?source=collection_archive---------6-----------------------#2022-04-30">https://medium.com/mlearning-ai/identifying-nepali-politician-from-audio-c0365f74f8ef?source=collection_archive---------6-----------------------#2022-04-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="2918" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">简介</strong></h1><p id="d65f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">根据音频识别人是一个有趣的话题。每个人的声音都是独特的；随着人们年龄的增长，他们的声音会发生变化，在不同的环境中听起来也不同，从而相应地改变了问题的面貌。说话人识别涉及基于一个人的口头音频来识别该人。传统上，语音处理模型被广泛认可，但卷积神经网络最近证明，它们也可能产生令人震惊的结果。</p><p id="5250" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">数据描述</strong></p><p id="8a73" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我一直有兴趣用我的语言训练一个模特。因此，我从youtube上收集了正好34位政治家在不同场合发表讲话的尼泊尔音频，适当注意不要在音频中包含噪音，平均音频长度约为5分钟。数据集在<a class="ae kf" href="https://drive.google.com/file/d/11s4vXYbSpFh-G5RdQzIS4oYl_MZC78XA/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>可用</p><h1 id="bf6a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">转化为深度学习问题</strong></h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/482835f9611ae0b60c56b752e47dc52d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiuTG4UgSKhFt9wN5XhFeA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Showing the formulation of deep learning problem</figcaption></figure><p id="3b4e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">必要的预处理取决于我们所拥有的音频，它可能包括去除音频的某些部分、去除静音等。由于CNN只能在图像上工作(1d CNN可以在文本上工作，但这里不考虑)，所以从预处理的音频中创建频谱图图像，并将其作为输入提供给模型，该模型将执行训练以产生结果。基本上有四个步骤:</p><p id="347e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">1.阅读音频</p><p id="f0c4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">2.执行必要的预处理</p><p id="275f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">3.选择模型</p><p id="43a8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">4.为模型准备数据</p><p id="b74e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">5.模型架构</p><h2 id="e7f9" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated">1.阅读音频</h2><p id="23b0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这个多步骤过程的第一步是读取音频，音频可以是多种格式。wav、. mp3等。这可以通过使用信号处理库Librosa来完成。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="09a0" class="kw if hh ll b fi lp lq l lr ls"># Reading the audio<br/>import librosa<br/>path= 'path to audio'</span><span id="d0b4" class="kw if hh ll b fi lt lq l lr ls">sample_rate= 22050<br/>arr_audio,_=librosa.load(path,sr= sample_rate) </span><span id="6980" class="kw if hh ll b fi lt lq l lr ls">#arr_audio is an array of amplitudes if the audio is 2sec in length<br/>#it will be (2,2*22050)= (2,44100), where 2 in the array is due to #dual channel of audio</span></pre><p id="a308" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">音频只不过是一种信号，x轴表示时间，y轴表示幅度，因为它是一种连续信号，我们希望将其转换为离散形式，以便从中提取特征。通过以特定的速率对信号进行采样，我们可以做到这一点，这意味着我们可以从某些时刻获得幅度值。这里的采样率是22050，这意味着在一秒钟的音频中，我们将提取幅度22050。</p><h2 id="af49" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated"><strong class="ak"> 2。预处理</strong></h2><p id="f2f7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> 2.1转换为单声道</strong></p><p id="74c7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">由于音频数据来自youtube，对于特定的时间实例，这是一个双通道，有两个振幅值，一个用于左耳机，一个用于右耳机。但是我们在任何情况下都需要单一的振幅值。因此，只需取双通道幅度阵列的平均值，我们就可以将其转换为单声道。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="2059" class="kw if hh ll b fi lp lq l lr ls">arr_audio= arr_audio.sum(0)/2</span></pre><p id="9b7f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 2.2去除噪声</strong></p><p id="1543" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在每个音频中，前1分钟是广告或其他一些不想要的音频片段。因此，我们将删除每个音频的第一分钟和最后一分钟。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="07ba" class="kw if hh ll b fi lp lq l lr ls">arr_audio= arr_audio[sample_rate*60:arr_audio.shape[0]-sample_rate*60]</span></pre><p id="5795" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 2.3消除沉默</strong></p><p id="061a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在说话时，他们可能会在我们之间短暂停顿，我们希望我们的模型能够学习个人音频的特征，它将从沉默中一无所获。所以沉默需要被消除。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lu"><img src="../Images/147c783a7e86b9ceaf162c7c8f5d34a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGd2ErQigpjxDeMyrjbqpA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Steps to remove silence</figcaption></figure><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="366b" class="kw if hh ll b fi lp lq l lr ls">from pydub import AudioSegment<br/>from pydub.silence import split_on_silence</span><span id="8a9f" class="kw if hh ll b fi lt lq l lr ls">def float_to_int(array, type=np.int16):<br/>  if array.dtype == type:<br/>        return array</span><span id="561a" class="kw if hh ll b fi lt lq l lr ls">  if array.dtype not in [np.int16, np.int32, np.int64]:<br/>        if np.max(np.abs(array)) == 0:<br/>            array[:] = 0<br/>            array = type(array * np.iinfo(type).max)<br/>        else:<br/>            array = type(array / np.max(np.abs(array)) *     np.iinfo(type).max)<br/>   return array</span><span id="de71" class="kw if hh ll b fi lt lq l lr ls">def int_to_float(array, type=np.float32):<br/>  if array.dtype == type:<br/>            return array</span><span id="2954" class="kw if hh ll b fi lt lq l lr ls">  if array.dtype not in [np.float16, np.float32, np.float64]:<br/>            if np.max(np.abs(array)) == 0:<br/>                array = array.astype(np.float32)<br/>                array[:] = 0<br/>            else:<br/>                array = array.astype(np.float32) / np.max(np.abs(array))<br/>   return array</span><span id="9b2c" class="kw if hh ll b fi lt lq l lr ls">#Step1:Create an convert to int and create audio_segment</span><span id="c83a" class="kw if hh ll b fi lt lq l lr ls">#Convert to float<br/>feature= float_to_int(arr_audio)</span><span id="0d04" class="kw if hh ll b fi lt lq l lr ls">#Create audio segment<br/>audio = AudioSegment(feature.tobytes(),frame_rate = self.sample_rate,sample_width = feature.dtype.itemsize\<br/>                             ,channels = 1)<br/>        </span><span id="5805" class="kw if hh ll b fi lt lq l lr ls">#Step2:removing the silence from the audio segment<br/>audio_chunks= split_on_silence(audio,min_silence_len=min_silence,silence_thresh= -30,keep_silence= 100)<br/></span><span id="7cda" class="kw if hh ll b fi lt lq l lr ls">#Step3:converting it back to the array<br/>#Convert audio segment to array<br/>arr_audio= sum(audio_chunks)<br/>arr_audio= np.array(arr_audio.get_array_of_samples())</span><span id="3b93" class="kw if hh ll b fi lt lq l lr ls">#Convert back to int<br/>arr_audio= self.int_to_float(arr_audio)<br/></span></pre><p id="ecfa" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">完成所有预处理后，数据如下所示:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lv"><img src="../Images/edbf18213f967321838b2efd24f5a555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8MlE7HkDzwRmRK7vmV_z2A.png"/></div></div></figure><p id="aae0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">单个数据点包含说话者的姓名、性别、持续时间(以秒为单位表示的音频长度),最后是包含以特定采样率采样的音频幅度的特征。</p><h1 id="865c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 3。选择型号</strong></h1><p id="75d2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">有34个演讲者，如果我们把每个演讲者看作一个单独的类，那么将有34个类。随着发言者的增加，班级也将增加，这不是一个理想的情况。为了识别单个类别假设k1，模型需要学习将它与其他(K-k1)类别明确区分的特征，这将需要大量数据，即增加单个说话者的音频长度。我不认为一个用户想花时间记录它的音频，他/她愿意记录5-30秒，然后完成它。所以，使用这种策略不是一种选择。</p><p id="5be2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">暹罗网络是包含两个相同子网的网络'<em class="lw">'相同'</em>这里的意思是，它们具有相同的配置，具有相同的参数和权重。参数更新在两个子网络上都是镜像的。它通过比较输入的特征向量来发现输入的相似性，因此这些网络被用于许多应用中。</p><p id="a69f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果我们想在这里添加一个新的扬声器，我们可以更新神经网络，并在整个数据集上重新训练它，这在传统架构中是不可能的。另一方面，SNNs学习相似性函数。因此，我们可以训练它来看看这两个图像是否相同(我们将在这里这样做)。这使得我们能够对新说话者进行分类，而无需再次训练网络。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lx"><img src="../Images/a7887dcb96300ba6bfbd49b7d1cf5c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ObEvWMVJA0AAdW__g-4kA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Siamese Network.(<a class="ae kf" rel="noopener" href="/swlh/one-shot-learning-with-siamese-network-1c7404c35fda">https://medium.com/swlh/one-shot-learning-with-siamese-network-1c7404c35fda</a>)</figcaption></figure><p id="eeae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">暹罗网络接受两个输入x1和x2。对于我们的情况，它或者是来自同一用户或者来自不同用户的两个频谱图图像。来自同一用户的图像被标记为1，来自不同用户的图像被标记为0。这些图像通过相同的网络发送，并生成特征向量f(x1)和f(x2)。使用一个函数来计算这两个特征向量之间的差。同一用户的特征向量之间的差异分数将很小，因为它们将位于该维度空间中的相同邻域中，而对于不同用户，特征向量差异将很大。</p><p id="8450" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">让我们以两个特征向量(0.5，0.7)和(1，0.7)为例</p><p id="6b4d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">不同用户:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ly"><img src="../Images/fc22e544ecf56afea0d62a57ced64c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*PTAYjnjYVYPg42u3O0LYZw.png"/></div></div></figure><p id="2711" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">正如我们可以看到的，如果特征向量来自不同的用户，该模型试图增加特征向量之间的距离。</p><p id="4353" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">来自同一用户:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lz"><img src="../Images/61c36d60759b40197ffc7fbb3d6b1058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*gsKzbC3RMMmUjy8hCbGeHg.png"/></div></div></figure><p id="e4c2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似地，如果特征向量在每次迭代后来自同一用户，则模型减小两者之间的距离，使它们在向量空间中更接近。</p><h1 id="c553" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> 4。为暹罗网络准备数据</strong></h1><p id="4c2b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">假设有一个来自扬声器Hari的0.3毫秒的音频，通过使用采样率= 22050，我们可以计算8个实例的幅度，即(0.3*1000秒)/22050~10。因此，这个音频片段的特征向量看起来像这样:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ma"><img src="../Images/b88a774ceca387ddd15dc56051d42783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ae7TmYPKXcvKYqc4AOReMQ.png"/></div></div></figure><h2 id="0b98" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated"><strong class="ak"> 1。列车测试拆分</strong></h2><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mb"><img src="../Images/8909cb02fd8848a594c2d9c97b64a7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BjBafPxBRRb8veW6HAwdA.png"/></div></div></figure><p id="f8ed" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">音频段的前80%用于生成训练集，而剩余的20%用于创建测试集。</p><h2 id="8198" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated"><strong class="ak"> 2从扬声器音频中获得相似和不相似的频谱对</strong></h2><p id="ba72" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> 2.1相似对:</strong></p><p id="01be" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">从特定用户音频创建的一对声谱图图像将创建一个数据点，并且该数据点的标签将是1，指示这对声谱图图像属于单独的音频。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mc"><img src="../Images/24678068d8ac628f2c9ac47f4669039b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8miEz-MVNMlpILGGYTPmtw.png"/></div></div></figure><p id="8eb5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">创建声谱图时最重要的参数是音频窗口长度，它定义了用于创建单个声谱图图像的音频段的长度。类似地，这两个窗口的起始位置将被随机选择。对于我的实现，我为单个用户选择了100对数据点，因此标签为1的数据点总数为34*100= 3400。</p><p id="43b2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 2.2相异线对:</strong></p><p id="8d5b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">单个数据点包含来自一个用户的一个频谱图和来自另一个用户的另一个频谱图，标签0表示频谱图属于单独的用户。因此，我们将每个用户与其他每个用户配对，并从不同的用户构建所有可能的配对组合。</p><p id="58ff" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们选择了25对声谱图照片，分别来自两个不同的人。因此，我们将为每个用户生成25对光谱图(在我们的情况下是33对)，总共33*25= 825对图像。类似地，34个人的不相似谱图对的总数是34*825=28050，并且这些对被标记为0，指示该对中的谱图图像来自不同的人。</p><p id="eb62" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">执行完所有这些任务后，数据集如下所示:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es md"><img src="../Images/ffa55f3837330a28ce45927431106b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Nk4kOCYZKZOpKmAcMLkZA.png"/></div></div></figure><h1 id="c773" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">4模型架构</h1><p id="1d97" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">因为图像编号刚好是31000，所以迁移学习是唯一的选择。因此，这里采用了已经在“imagenet”上训练过的DenseNet121。我们必须利用我们在初始化该网络时创建的谱图高度和宽度。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="6c88" class="kw if hh ll b fi lp lq l lr ls">subnetwork=    tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet', ,input_tensor=None,input_shape=(height,width,3), pooling=None)</span></pre><p id="e2de" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似地，我们需要一个距离函数来计算DenseNet产生的两个特征向量之间的距离。为此，我们将使用欧几里德距离:</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="017a" class="kw if hh ll b fi lp lq l lr ls">def euclidean_distance(self,inp):</span><span id="dc09" class="kw if hh ll b fi lt lq l lr ls">  x,y= inp<br/>  sum_square = tf.math.reduce_sum(tf.math.square(x -     y),axis=1,keepdims=True)<br/>  return tf.math.sqrt(tf.math.maximum(sum_square,1e-07))</span></pre><p id="1ba6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">最后，让我们把所有的部分放在一起，创建一个完整的模型。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="4731" class="kw if hh ll b fi lp lq l lr ls">def create_siamese_network(height,width):<br/>        #Inputs for two spectrogram images<br/>        image1= Input(shape=(height,width,3))<br/>        image2= Input(shape=(height,width,3))</span><span id="ce88" class="kw if hh ll b fi lt lq l lr ls">        #sending the images through the DenseNet<br/>        output1= Flatten()(subnetwork(image1))<br/>        output2= Flatten()(subnetwork(image2))</span><span id="f1fe" class="kw if hh ll b fi lt lq l lr ls">        #Calculating the distance between the feature vector<br/>        output = layers.Lambda(euclidean_distance)([output1, output2])<br/>        <br/>        model= Model(inputs=[image1,image2],outputs=output)<br/>        return model</span></pre><p id="41d1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">DenseNet将生成具有多个通道的特征矩阵；为了从中获得一个向量，我们展平输出矩阵并计算通道之间的距离。</p><h1 id="b0ed" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">5.训练模型</h1><h2 id="503f" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated">5.1收缩损失</h2><p id="08bb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们需要一个损失函数来最小化同一个人的光谱图之间的距离，并最大化不同个人的光谱图之间的距离。收缩损失的工作方式与此完全相同，我们将在这里使用它来训练模型。</p><pre class="kh ki kj kk fd lk ll lm ln aw lo bi"><span id="64ac" class="kw if hh ll b fi lp lq l lr ls">margin= 3.0<br/>def contractive_loss(label,distance):<br/>    <br/>    difference= margin-distance<br/>    temp= tf.where(tf.less(difference,[0.0]),[0.0],difference)   <br/>    loss= tf.math.reduce_mean(tf.cast(label,dtype=tf.float32)*tf.math.square(distance)+(1-tf.cast(label,dtype=tf.float32))*tf.math.square(temp))<br/>    <br/>    return loss</span></pre><p id="f622" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">边际价值是收缩损失中最关键的参数。这个数字告诉模型不同人的特征向量之间的最小距离应该是多少，在我们的实现中，我们将margin值设置为3。类似地，该模型试图将同一图像的光谱图之间的距离减少到0。</p><h2 id="e8dd" class="kw if hh bd ig kx ky kz ik la lb lc io jn ld le is jr lf lg iw jv lh li ja lj bi translated">5.2评估</h2><p id="25d4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">相似距离=从同一个人生成的两个声谱图图像的特征向量之间的距离。</p><p id="ee55" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">相异距离=从不同的人生成的两个频谱图图像的特征向量之间的距离。</p><p id="f8e1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">起初，相似和不相似的距离几乎相同，但是随着模型的训练，它们开始发散，并且在30个时期之后，训练和测试之间的距离看起来像这样:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es me"><img src="../Images/b3bc4b36d2a2b49128869a10caee6113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1KQ-JBHGGa0GzVbWl-e8sw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Epoch vs Distance</figcaption></figure><p id="7e80" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">对于训练集，同一个人的光谱图之间的平均距离是1.4，而不同人的光谱图之间的距离是4.7。类似地，对于测试集，平均相似距离是1.8，平均不相似距离是4.4。</p><p id="c296" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因为模型只是输出配对之间的距离值，所以我们可以得出结论，如果配对之间的距离小于1.8，我们就将其标记为1，否则将标记为0。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mf"><img src="../Images/ea0b8e09ec6cd36340725726fc801780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*HRiVAUdY2Isq67TFSIiCsg.png"/></div></figure><p id="8765" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">训练集的精度和召回值分别被确定为0.87和0.84，而测试集的精度和召回值接近0.8。</p><h1 id="e4a1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">6.训练参数</h1><p id="850f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该模型以1e-4的学习率被训练30个时期。采用Adam优化器，多项式学习率衰减，模型的前100步作为warn步骤。</p><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx kq mj"/></div></div></a></div></div></div>    
</body>
</html>