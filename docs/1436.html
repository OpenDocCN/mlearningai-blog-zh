<html>
<head>
<title>“Fathom: Reference Workloads for Modern Deep Learning Methods” Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“Fathom:现代深度学习方法的参考工作负载”摘要</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fathom-reference-workloads-for-modern-deep-learning-methods-2a36eee41760?source=collection_archive---------3-----------------------#2021-12-16">https://medium.com/mlearning-ai/fathom-reference-workloads-for-modern-deep-learning-methods-2a36eee41760?source=collection_archive---------3-----------------------#2021-12-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="172f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="ea59" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">深度学习(DL)已经变得很受欢迎，因为它在为图像分类、自然语言处理、对象检测等应用提出可接受的解决方案方面表现出了有效性。DL任务需要巨大的计算能力。它们是浮点计算密集型任务。他们在大规模并行GPU或其他加速器(如谷歌的TPUs)上接受训练和服务。计算机架构师已经提出了大量的加速器来促进他们的计算。所提出的机制通常集中在狭窄的应用领域，这并没有降低它们的价值。然而，对于更灵活的体系结构，需要检查来自整个DL的最新模型的特征。<a class="ae ka" href="https://github.com/rdadolf/fathom" rel="noopener ugc nofollow" target="_blank"> Fathom </a> [1]收集了一组8个DL工作负载，用于研究它们的特征和行为。</p><h1 id="6b51" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">所选工作负载的特征</h1><p id="5a66" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Robert Adolf等人在[1]中指出了工作负载的三个特性，这些特性可以融入到捆绑包中。这些特性是(1) <strong class="je hi">代表性</strong>、(2) <strong class="je hi">多样性</strong>、(3) <strong class="je hi">影响力</strong>。(1)基准测试套件应该反映DL社区已经提出的最佳方案。(2)每个模型必须带来一些独特的东西，以保持多样性。(3)影响是特定技术改变数字图书馆研究前景的程度。</p><h1 id="a172" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Fathom Suite的工作负载</h1><h2 id="9169" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">1.序列间翻译</h2><p id="8f1f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> seq2seq </strong>是一个<a class="ae ka" href="https://www.freecodecamp.org/news/the-ultimate-guide-to-recurrent-neural-networks-in-python/" rel="noopener ugc nofollow" target="_blank">递归神经网络(<strong class="je hi"> RNN </strong> ) </a>用于解决机器翻译。它是2014年在谷歌开发的，使用多层管道的<a class="ae ka" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM) </a>神经元提取句子的意思，然后将其重新发射到另一种语言中[ <strong class="je hi"> 2，3 </strong> ]。</p><h2 id="e124" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">2.端到端存储网络</h2><p id="7593" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">记忆网络将状态从神经网络的结构中分离出来。记忆网络的发展源于有状态神经元在捕捉长程相关性方面的困难。脸书的人工智能研究小组通过将间接可寻址存储器与神经网络结合起来解决了这个问题，从而产生了一种可以显式存储和回忆信息的模型。端到端的内存网络[ <strong class="je hi"> 5 </strong> ]是一种扩展，它消除了对输入的类型注释的需要，并极大地简化了训练(自然语言处理)。</p><h2 id="6879" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">3.深沉的演讲</h2><p id="49d4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是百度研究院对可扩展语音识别模型的尝试[ <strong class="je hi"> 6 </strong> ]。它使用连接主义时间分类(CTC)损失函数，该函数可以从不分段的数据中学习，从而显著降低产生训练数据的成本。</p><h2 id="2e47" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">4.变分自动编码器</h2><p id="5e98" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">编码器是灵活的、无监督的模型，通常用于维度、特征提取或生成数据。2013年发明的变分编码器对真实数据的属性进行统计假设，以学习有效地重建其输入[ <strong class="je hi"> 8 </strong> ]。</p><h2 id="d43a" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">5.剩余网络</h2><p id="bdaa" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">残差网络是实现非常深的神经网络(超过150层)的里程碑。</p><h2 id="c47a" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">6.VGG-19</h2><p id="8ffa" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">它是牛津大学视觉几何小组开发的19层卷积网络的实现。它的灵感来自于AlexNet的成功。关键的见解是，更多层的较小卷积滤波器更容易训练，这提高了精度，并大大减少了可学习参数的数量。</p><h2 id="e575" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">7.AlexNet</h2><p id="3b59" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这个模型被认为是DL历史上的重要事件之一[ <strong class="je hi"> 11 </strong> ]。</p><h2 id="7372" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">8.深度强化学习</h2><p id="e667" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><a class="ae ka" href="https://en.wikipedia.org/wiki/DeepMind" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<strong class="je hi"> 2013 </strong>用一个深度强化学习系统震惊了AI社区，该系统学会了仅从像素和分数赢得几十个雅达利游戏。与监督学习不同，该算法在接收游戏中的反馈时改进其选择的动作，而不是通过观察整个游戏。该方法的架构是一个卷积网络，它使用2–3个卷积层和2–3个密集层[ <strong class="je hi"> 12 </strong> ]来选择动作。</p><h1 id="7259" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">了解DL工作负载的性能特征</h1><p id="ef64" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者从不同的角度研究了组装的工作负载。这些旨在让计算机架构师对DL工作负载的行为有一个直观的了解，例如，时间花在哪里，以及给定模型和它在其上运行的硬件之间的关系。</p><h2 id="b9fa" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">DL框架中的测量和分析</h2><p id="5173" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者使用<strong class="je hi">操作</strong>作为理解Fathom模型性能的主要抽象。操作是粗粒度数据流图中定义张量流模型的节点。它被实现为一个Python函数，指示框架构建该节点，还被实现为一个C++函数，执行计算或调用低级库来执行计算。底层库可以是CPU上的<a class="ae ka" href="https://eigen.tuxfamily.org/index.php?title=Main_Page" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> Eigen </strong> </a> <strong class="je hi"> </strong>线性代数包，也可以是GPU上的<a class="ae ka" href="https://docs.nvidia.com/cuda/cublas/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="je hi">cuBLAS</strong></a><strong class="je hi"/>或<a class="ae ka" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank"><strong class="je hi">cud nn</strong></a><strong class="je hi"/>等CUDA库。操作是TensorFlow运行时中最小的可调度单元。示例包括像2D矩阵乘法(<strong class="je hi">马特穆尔</strong>)、逐元素张量取幂(<strong class="je hi">幂</strong>)这样的函数，或者像从正态分布采样(<strong class="je hi">标准随机正态</strong>)或者计算特定优化算法的损失函数(<strong class="je hi">交叉熵</strong>)这样的专用函数。</p><h2 id="fc62" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">操作类型分析</h2><p id="4b81" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">关于工作负载，最基本的性能问题是时间花在了哪里。这取决于所考虑的模型、环境和用例。下图显示了几个繁重的操作类型(通常在5到15个之间)共同承担了程序持续时间的90%以上。每条曲线上的每个点都代表单一操作类型对执行时间的累积贡献。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kp"><img src="../Images/c659070a47c513ce39d4fbf03ba4c11b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjyThZ3nQIWkec3lVfDHzA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="acf7" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">下图显示，这些重要的独特操作对于每个型号都不相同，也不呈现相同的比率。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lk"><img src="../Images/67690e8469fe1b99ca9d03750aadbb3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lrwaE_q71rBGEQ_e6WmttQ.png"/></div></div></figure><p id="3cf7" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">来自作者实验的一些见解:</p><ol class=""><li id="af9a" class="ll lm hh je b jf lf jj lg jn ln jr lo jv lp jz lq lr ls lt bi translated">卷积神经网络确实是由卷积支配的。</li><li id="5c3f" class="ll lm hh je b jf lu jj lv jn lw jr lx jv ly jz lq lr ls lt bi translated">全连接网络严重依赖矩阵乘法。</li><li id="02b2" class="ll lm hh je b jf lu jj lv jn lw jr lx jv ly jz lq lr ls lt bi translated"><strong class="je hi"> alexnet </strong>、<strong class="je hi"> vgg </strong>和<strong class="je hi"> residual </strong>的运算分解表明，卷积网络已经变得更深，并且大多数运算都是卷积。此外，完全联网的大学在久而久之的份额较少。</li><li id="da80" class="ll lm hh je b jf lu jj lv jn lw jr lx jv ly jz lq lr ls lt bi translated"><strong class="je hi">语音</strong>模型正如其作者所提出的那样，有意避免使用更复杂的组件，而倾向于结构简单、易于优化的网络。如上图所示，这种模型几乎只受矩阵-矩阵乘法运算的影响。</li></ol><h2 id="cf86" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">性能相似性</h2><p id="d84b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者使用下面的公式来计算操作向量的向量之间的距离，这些向量在上图中显示为行。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es lz"><img src="../Images/19c0dc6d41906513da8647c5190005b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*6Q2k9HInhrQgIEe6XGTDaQ.png"/></div></figure><p id="76f7" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">结果如下图所示，对于一个DL专家来说可能并不奇怪。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ma"><img src="../Images/20b253a6670bae5cc689fb5821e0efdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBSM6JhtuiOEjR4Ppba_8w.png"/></div></div></figure><p id="8436" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">speech和seq2seq这两个递归网络之间的巨大差异会有点意思。虽然这两个网络都是递归的，但与序列到序列翻译模型使用的有状态LSTM神经元和标准交叉熵损失相比，深度语音使用CTC损失和一堆迟钝连接的神经元。seq2seq中的元素乘法是LSTM神经元的结果，数据移动操作是它使用的基于注意力的编码器/解码器的一部分。</p><h2 id="13ce" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">训练和推理</h2><p id="2ac5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在下图中，作者显示了训练和推理阶段的标准化执行时间之间的差异。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es mb"><img src="../Images/823b3d3eac06baae7e3b4a90cf1e44a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_xiQ3NQFDWXANnlj49Gzg.png"/></div></div></figure><h2 id="c60b" class="kb if hh bd ig kc kd ke ik kf kg kh io jn ki kj is jr kk kl iw jv km kn ja ko bi translated">并行性和操作平衡</h2><p id="21b4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者在下图中强调了三种模型的主要操作以及并行性对它们的影响。他们将学到的经验总结为:</p><blockquote class="mc md me"><p id="f547" class="jc jd mf je b jf lf jh ji jj lg jl jm mg lh jp jq mh li jt ju mi lj jx jy jz ha bi translated">DL模型的性能行为依赖于它们的应用程序级结构。虽然卷积和矩阵乘法对硬件支持来说是有吸引力的目标，但从中获得的好处是有限的。对于具有非卷积层、复杂损失函数或优化算法或稀疏存储的DL模型来说尤其如此。</p></blockquote><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es mj"><img src="../Images/9d99a2a00b2272a801ea5b299e90c758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGAcx13VMdSDtMGg-r7GAQ.png"/></div></div></figure><h1 id="9dd6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="d351" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">[1] Adolf，Robert等.“<strong class="je hi"> Fathom:现代深度学习方法的参考工作负载。</strong><em class="mf">2016 IEEE国际工作负载特性研讨会(IISWC) </em>。IEEE，<strong class="je hi"> 2016 </strong>。</p><p id="de4d" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">[ <strong class="je hi"> 2 </strong> ]苏茨基弗、伊利亚、奥里奥尔·维尼亚尔斯、阔克诉勒。<strong class="je hi">用神经网络进行序列对序列学习。</strong><em class="mf">神经信息处理系统的进展</em>。<strong class="je hi"> 2014 </strong>。</p><p id="9d79" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">[ <strong class="je hi"> 3 </strong> ] Bahdanau，Dzmitry，Kyunghyun Cho，Yoshua Bengio。<strong class="je hi">神经机器翻译通过联合学习来对齐和翻译。</strong><em class="mf">arXiv预印本arXiv:1409.0473</em>(<strong class="je hi">2014</strong>)。</p><p id="0536" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">韦斯顿、杰森、苏米特·乔普拉和安托万·博德斯。"<strong class="je hi">记忆网络</strong>"<em class="mf"> arXiv预印本arXiv:1410.3916</em>(<strong class="je hi">2014</strong>)。</p><p id="38c2" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">【<strong class="je hi"> 5 </strong>】苏赫巴托尔、塞恩巴雅尔等】<strong class="je hi">端到端存储网络。</strong><em class="mf">arXiv预印本arXiv:1503.08895</em>(<strong class="je hi">2015</strong>)。</p><p id="8501" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">【<strong class="je hi">6</strong>】Hannun，Awni等.<strong class="je hi">深度语音:放大端到端语音识别。</strong><em class="mf">arXiv预印本arXiv:1412.5567</em>(<strong class="je hi">2014</strong>)。</p><p id="1338" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">辛顿、杰弗里·e和鲁斯兰·r·萨拉胡季诺夫。<strong class="je hi">用神经网络降低数据的维度。</strong>“<em class="mf">理科</em>313.5786(<strong class="je hi">2006</strong>):504–507。</p><p id="95d2" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">金玛、迪德里克·p和马克斯·韦林。<strong class="je hi">随机梯度VB和变分自动编码器。</strong><em class="mf">第二届国际学术代表大会，ICLR </em>。第19卷。<strong class="je hi"> 2014 </strong>。</p><p id="6f4a" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated"><strong class="je hi"> 9 </strong>何，，等.<strong class="je hi">深度残差学习用于图像识别。</strong><em class="mf">IEEE计算机视觉与模式识别会议论文集</em>。<strong class="je hi"> 2016 </strong>。</p><p id="13a3" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">西蒙扬、凯伦和安德鲁·齐泽曼。<strong class="je hi">用于大规模图像识别的极深度卷积网络。</strong><em class="mf">arXiv预印本arXiv:1409.1556</em>(<strong class="je hi">2014</strong>)。</p><p id="a5aa" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">克里日夫斯基、亚历克斯、伊利亚·苏茨基弗和杰弗里·e·辛顿。<strong class="je hi">用深度卷积神经网络进行Imagenet分类。</strong><em class="mf">神经信息处理系统进展</em>25(<strong class="je hi">2012</strong>):1097–1105。</p><p id="d3e2" class="pw-post-body-paragraph jc jd hh je b jf lf jh ji jj lg jl jm jn lh jp jq jr li jt ju jv lj jx jy jz ha bi translated">【<strong class="je hi">12</strong>】belle mare，Marc G .等.<strong class="je hi">街机学习环境:总代理评估平台。</strong><em class="mf">人工智能研究杂志</em>47(<strong class="je hi">2013</strong>):253–279。</p><div class="mk ml ez fb mm mn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb kz mn"/></div></div></a></div></div></div>    
</body>
</html>