<html>
<head>
<title>Train like labels can’t harm the learning: Learning with Noisy Labels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">像标签一样训练不会伤害学习:用嘈杂的标签学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/train-like-labels-cant-harm-the-learning-learning-with-noisy-labels-1479da6f49af?source=collection_archive---------3-----------------------#2022-01-11">https://medium.com/mlearning-ai/train-like-labels-cant-harm-the-learning-learning-with-noisy-labels-1479da6f49af?source=collection_archive---------3-----------------------#2022-01-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/215bdeef2ff9d3b199c6174342bd59c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pQ5S63tEDxar-Q79"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@kasiape?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Katarzyna Pe</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="iu"><p id="150c" class="iv iw hh bd ix iy iz ja jb jc jd je dx translated">我还不够年轻，不能什么都知道。- <a class="ae it" href="http://www.1000advices.com/authors/wilde_oscar_hq.html" rel="noopener ugc nofollow" target="_blank">奥斯卡·王尔德</a></p></blockquote><h1 id="4c7f" class="jf jg hh bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">Tl；博士</strong></h1><p id="1609" class="pw-post-body-paragraph kd ke hh kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je ha bi translated">我们将了解一种新的图像分类机器学习方法，称为<em class="la"> DIVIDEMIX </em>，它将帮助机器理解和分类带有错误标签(有噪声的标签)的图像。</p><blockquote class="lb lc ld"><p id="2508" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">引用:李，2020年。Dividemix:带噪声标签的学习作为半监督学习。arXiv预印本arXiv:2002.07394 。</p></blockquote><h1 id="f55a" class="jf jg hh bd jh ji jj jk jl jm jn jo jp jq lm js jt ju ln jw jx jy lo ka kb kc bi translated"><strong class="ak">先决条件术语</strong></h1><blockquote class="lb lc ld"><p id="601a" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><a class="ae it" href="https://www.ibm.com/au-en/cloud/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="hh"> IBM </em> </a> <em class="hh">:机器学习是</em> <a class="ae it" href="https://www.ibm.com/au-en/cloud/learn/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <em class="hh">人工智能(AI) </em> </a> <em class="hh">和计算机科学的一个分支，专注于数据和算法来模仿人类如何学习，逐渐提高准确性。</em></p></blockquote><p id="12d3" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi"> <em class="la">机器学习/深度学习:</em> </strong>用最直白的话来说，机器学习就是我们把已有的数据提供给机器的过程。机器在当前数据中找到模式，称为学习或训练。它可以用于各种任务，如分类、预测等。(深度文章:<a class="ae it" rel="noopener" href="/free-code-camp/want-to-know-how-deep-learning-works-heres-a-quick-guide-for-everyone-1aedeca88076">想知道深度学习是如何工作的？下面给大家一个快速指南。</a>)</p><p id="fdee" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi"> <em class="la">图像分类:</em> </strong>对图像进行分类对人类来说是一件痛苦的事情，对机器来说却不是。我们可以在给定的带有各自标签的图像上训练我们的机器，以使用各种算法来执行分类任务。(深度文章:<a class="ae it" href="https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d" rel="noopener" target="_blank">10分钟内使用MNIST数据集进行影像分类</a>)</p><p id="111a" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi"> <em class="la">机器/深度学习算法类型:</em> </strong> <em class="la">各种算法取决于</em>我们拥有的数据类型。几个主要类别是:</p><ol class=""><li id="770c" class="lp lq hh kf b kg le kk lf ko lr ks ls kw lt je lu lv lw lx bi translated"><strong class="kf hi">监督学习:</strong>我们有可用于训练的输入和输出特征(标签)，算法试图找到输入和输出特征之间的模式。学习和模式可以对新的未知输入数据的输出进行预测或分类。一些标准的监督技术是线性回归、SVM、神经网络等。</li><li id="9d04" class="lp lq hh kf b kg ly kk lz ko ma ks mb kw mc je lu lv lw lx bi translated"><strong class="kf hi">无监督学习:</strong>在某些情况下，我们只有输入特征，没有关于标签的信息，算法只是试图找到输入特征之间的模式。算法试图在可用数据中找到方向、模式和组。一些标准的无监督技术是聚类等。</li><li id="9351" class="lp lq hh kf b kg ly kk lz ko ma ks mb kw mc je lu lv lw lx bi translated"><strong class="kf hi">半监督学习:</strong>这是上面两种的混合，其中我们对一些数据有标签，对其余的没有标签。这些对于建模非常有用。一些标准的半监督技术是生成模型、启发式方法、混合匹配等。</li><li id="c719" class="lp lq hh kf b kg ly kk lz ko ma ks mb kw mc je lu lv lw lx bi translated"><strong class="kf hi">强化学习</strong>通过基于奖励的反馈，帮助机器学习并确定适当的行为，以最大限度地提高性能。奖励就像赋予模型的一个特征，表示模型做得很好，该算法的主要目标是降低风险和增加奖励。一些标准的强化技术是Q学习、对抗网络等。</li></ol><p id="96a7" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">(深度文章:<a class="ae it" href="https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861" rel="noopener" target="_blank">你应该知道的机器学习算法类型</a>、<a class="ae it" rel="noopener" href="/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12">人类的机器学习</a>)</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mk"><img src="../Images/c5f12de637cd2eae640a0eb7a9717663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1jsHNW6FcW15xAZN"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@goian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ian Schneider</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="667e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi"> <em class="la">在本文中，我们将重点讨论监督和半监督学习。</em>T13】</strong></p><p id="d375" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">如上所述，在监督学习中，我们有带标签的数据。机器找到关系和模式，但找到正确的标签是很辛苦的。</p><p id="74ae" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">大多数带有标签的数据收集都是人工标注的，这是荒谬且耗时的，并且包含许多错误。如果我们的数据中有错误，我们的机器要么无法学习，要么学习了错误的东西，并在未知数据上表现不佳。</p><p id="cbdd" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">许多提出的算法帮助机器在存在错误标签的情况下正确学习。本文将讨论一种非常著名的方法，称为<strong class="kf hi"> <em class="la"> DivideMix </em> </strong>。</p><p id="bd27" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">DivideMix的形式化算法如下:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mp"><img src="../Images/cb9fa06865214d67da3f0c6e4f9686dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwwsbjZhlCuxPf1kpRyAXQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Li, J., Socher, R. and Hoi, S.C., 2020. Dividemix: Learning with noisy labels as semi-supervised learning. <em class="mq">arXiv preprint arXiv:2002.07394</em>.</figcaption></figure><p id="a85e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">这看起来并不容易，但如果我们把算法分成几个部分，一行一行地进行，就很容易了。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/caac265471d48cb0f734bcc66b529e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZG_zqX6lZ06Wg5IY2ufBZg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Li, J., Socher, R. and Hoi, S.C., 2020. Dividemix: Learning with noisy labels as semi-supervised learning. <em class="mq">arXiv preprint arXiv:2002.07394</em>.</figcaption></figure><p id="4307" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">我们将尝试结合使用作者提供的图形和算法来逐步理解这种方法，以使我们的生活变得更容易<em class="la">(因为生活是简单还是像你认为的那样困难——未知)</em></p><p id="153f" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">当我们使用算法训练(提供已知信息)我们的模型时，我们在整个数据集上运行这个次数，称为epochs。样本被定义为一个单一的数据单位，我们在不改变模型(参数)的任何值的情况下运行模型的样本数量称为批量。(深度文章:<a class="ae it" href="https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/" rel="noopener ugc nofollow" target="_blank">神经网络</a>中批次和时期之间的差异)</p><p id="78a3" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">DivideMix中使用的方法是我们有各种带有噪声标签的图像。从上图中我们可以看到，两个网络同时被训练，以避免<strong class="kf hi">确认偏差</strong>。(在每个时期过滤彼此的误差)</p><blockquote class="lb lc ld"><p id="e9ec" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">确认偏差:机器学习算法的目的是学习模式以给出更好的结果；为了实现这一点，可能会出现这样一种情况:只重视少数几个结果，而忽略其他结果，因为这样做会得到更好的结果。它每次都向网络提供相似类型的信息。(深度文章:<a class="ae it" href="https://www.explorium.ai/blog/data-bias-and-what-it-means-for-your-machine-learning-models/" rel="noopener ugc nofollow" target="_blank">数据偏差及其对你的机器学习模型的意义</a>)</p></blockquote><p id="0470" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">对于每个历元，DivideMix模型执行共除，共除背后的主要思想是在干净的标注数据集和嘈杂的标注数据集之间划分网络。<em class="la">我们忽略了有噪声的带标签数据集的标签，因此它变成了有噪声的无标签集，我们保留了干净的带标签集的标题</em>。在这一步，我们的问题从监督学习转换为半监督学习，在这之后，我们可以使用半监督MixMatch学习的改进版本。</p><p id="56db" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">但主要问题是如何执行共除？</strong></p><p id="3968" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">研究证明，在干净的样本上学习比在嘈杂的样本上学习更快更容易。</p><p id="32d7" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">共分的主要目的是找到样本干净的概率。</p><blockquote class="lb lc ld"><p id="c076" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">这可以通过将混合模型拟合到每个样本的损失分布来实现。</p></blockquote><p id="74a3" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">简单来说，损失定义为对错误结果的罚款，参数是机器学习的首要关键。参数是模型在训练期间学习的一组值，正式表示为θ。(深度文章:<a class="ae it" href="https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac" rel="noopener" target="_blank">机器学习和深度学习中的参数和超参数</a>)</p><p id="2433" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">步骤1 </strong>:如第2行提到的，算法名为预热阶段，我们需要用噪声数据集训练我们的模型几个时期。我们可以使用作者使用的任何图像分类监督技术ResNet(参考:<a class="ae it" rel="noopener" href="/@nina95dan/simple-image-classification-with-resnet-50-334366e7311a">图像分类</a>)。我们并行运行两个模型，以避免确认偏差。会得到一个双参数集θ_1和θ_2。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/3735210b9688b76cf1d07db52b1d89a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGGNfCWAFIbIYz6krT5xOg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 2</figcaption></figure><p id="6802" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">第二步</strong>:我们的目标是使用交叉熵计算每个样本的损失。</p><blockquote class="lb lc ld"><p id="b4dd" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><em class="hh">交叉熵损失</em>是机器学习中最常用的损失类型，表示模型适合或符合训练数据的程度。它只是建立在给出两个概率分布之间的差异上。数学上，它被定义为:</p></blockquote><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/425d737394b60b64128fe8502f4cca3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORRstWsM2I2FQeHETIL5Aw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e" rel="noopener" target="_blank">https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e</a></figcaption></figure><blockquote class="lb lc ld"><p id="7633" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">在上面的数学方程中，p_i是从我们的模型预测的那一类的概率，t_i是正确的概率。在上面的公式中，计算了交叉熵损失。</p></blockquote><p id="46bd" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">Softmax概率是模型预测的输出。(深度文章:<a class="ae it" href="https://towardsdatascience.com/softmax-and-uncertainty-c8450ea7e064" rel="noopener" target="_blank"> Softmax和不确定性</a>)。</p><p id="6ccf" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在DivideMix方法中，我们考虑了具有给定模型参数θ的所有样本的损失，并且我们的数据总共有<em class="la"> C </em>个不同的类别或标签。我们可以将数据表示为D = (X，Y)，其中X表示图像，Y表示它们各自的标签。因此，所有样本的损耗<em class="la"> l(θ) </em>(假设<em class="la"> N </em>)为:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/dfb1cd4aa3d0a650be0b4cae21231ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*kl5BieMmmL50ld11Po2Hgw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Li, J., Socher, R. and Hoi, S.C., 2020. Dividemix: Learning with noisy labels as semi-supervised learning. <em class="mq">arXiv preprint arXiv:2002.07394</em>.</figcaption></figure><blockquote class="lb lc ld"><p id="d89b" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">在上面的数学等式中，我们使用上面的公式计算每个样本(总共N个样本)的交叉熵损失。我们用y作为实际样本，p^c项给出了我们模型的预测类别。</p></blockquote><p id="6186" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">[第3–5行]第3步:</strong>我们有所有样品的损失；我们将在两分量高斯混合模型GMM(聚类机器学习模型)中拟合我们发现的损失<em class="la"> l </em>以获得干净概率。我们将选择具有较小平均值的高斯，并且如果预测的概率大于阈值，则样本被认为是干净的，并且标签被保留；否则，噪声和标签将被移除。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mv"><img src="../Images/cb42a87c708ed7677e28b76b328c46ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5v1JBXmUA16SmgQ1Ml1lxA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 3–5</figcaption></figure><p id="e189" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在两分量高斯混合模型GMM中，我们尝试用两个钟形曲线(称为高斯曲线)拟合我们的数据，这两个钟形曲线具有参数均值μ、协方差σ、混合概率π。我们会选择平均值较小、误差较小的钟形曲线。(为了更好地解释GMM，阅读下一节，或者如果我们有GMM背后的基本想法，我们可以跳过下一节)</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="ec2e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">要做到这一点，我们需要理解高斯混合模型GMM的概念:</strong></p><p id="f38b" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">高斯一般表示<a class="ae it" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>，这种聚类技术就是基于此。无监督的目标是将数据聚类成固定数量的组件。每个分量也称为高斯分量，由均值μ(中心)、协方差σ(宽度)、混合概率π(高斯小/大的大小，或者我们可以说每个高斯的权重)作为参数组成。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mw"><img src="../Images/36caff7ec448f98b28661639382cce4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H7o7l6ly9umRcZky.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Gaussian Mixture Models in PyTorch: <a class="ae it" href="https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html" rel="noopener ugc nofollow" target="_blank">https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html</a></figcaption></figure><p id="81dd" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">我们将数据作为X，不需要标签Y，因为这是一种无监督的学习，所以在数学上，我们需要最大化由下式给出的概率函数:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mx"><img src="../Images/26a572ba06d504cd6bf239b320432931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XjntjnxYqm6npU1sxo1p1w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Gaussian density function</figcaption></figure><blockquote class="lb lc ld"><p id="0fa0" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">上面的数学方程给出了其中K个总样本的正态分布我们有N的概率密度函数，N是分布总数；这里有N个高斯，所以它被称为N分量高斯混合模型</p></blockquote><p id="6a5e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">(<a class="ae it" href="https://stackoverflow.com/questions/62603935/expectation-maximization-algorithm-em-for-gaussian-mixture-models-gmms" rel="noopener ugc nofollow" target="_blank"> <em class="la"> R </em> </a> ef <em class="la"> : </em> <a class="ae it" href="https://stackoverflow.com/questions/62603935/expectation-maximization-algorithm-em-for-gaussian-mixture-models-gmms" rel="noopener ugc nofollow" target="_blank">高斯混合模型的期望最大化算法(EM)</a>)</p><p id="593e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">期望最大化(E.M .)方法最大化了上面提到的概率，因为它不可微。根据电磁法:</p><p id="319f" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">步骤1 </strong>:正如我们所讨论的，参数是μ、σ和π，所以用一些随机值初始化这些参数。</p><p id="7266" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">第二步期望</strong>:责任是每个高斯与输出之间的关系，这意味着从该聚类中产生任何数据点的后验概率。在这一步中，我们需要使用参数μ、σ和π找出每个数据点的责任。数学上:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es my"><img src="../Images/f4e08d3bdd4d27ba2c293a510511d4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*MVLq7GT6ozSapjoMqMp7aw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Responsibility</figcaption></figure><blockquote class="lb lc ld"><p id="30eb" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">上面的数学方程求出了在K个数据存在的情况下，以给定的均值(μ)、协方差(σ)和混合概率(π)从高斯(N高斯)产生数据的概率。</p></blockquote><p id="495d" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">步骤3最大化:</strong>使用期望步骤的职责更新参数μ、σ和π。数学上:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/44bccc71ef066cf793634bf9c4dd1226.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*yswHVcs_Dru65MHqQYD9cA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Parameters Update</figcaption></figure><blockquote class="lb lc ld"><p id="c376" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">通过训练，我们需要改变参数μ、σ和π，而上述数学方程基于新值对参数进行平均。</p></blockquote><p id="13e3" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">步骤4 </strong>:重复步骤2和步骤3，直到收敛(局部最优)</p><p id="af01" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">(深度文章:<a class="ae it" href="https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95" rel="noopener" target="_blank">高斯混合模型讲解</a>)</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="49e3" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在使用期望值最大化算法对2-成分GMM模型的损失进行拟合之后。我们有高斯函数所有参数的最佳值。我们将选择具有较小平均值的高斯分量<em class="la"> (g) </em>，因为它将具有最低误差。</p><p id="6e1a" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">我们的目标是使用后验概率<strong class="kf hi"> <em class="la"> p(g|l_i) </em> </strong> <em class="la">，</em>找到干净样本的，其中g是具有较小均值的高斯分量，l_i是每个样本的损失。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es na"><img src="../Images/4c68645e4e41c2128f7a0020466e0b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYKQWSL7SsfdAae-xxUBVg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 6–8</figcaption></figure><p id="d06b" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">【第6–8行】第4步:</strong>现在我们有了每个样本的干净概率损失，如果大于指定的阈值，那么模型就是干净的，所以标签被保留；否则，选择是嘈杂的，所以标题被删除，它是嘈杂的未标记。(根据研究论文，大多数情况下阈值为0.5)。</p><p id="68ce" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">共同分割的结论:</strong></p><blockquote class="lb lc ld"><p id="2ce5" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 1。在有噪声的数据集上运行图像分类模型几个时期，以找到参数θ。(步骤1) </strong></p><p id="e39e" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 2。使用交叉熵损失，计算每个样本的损失，并使用期望最大化，使用计算的损失来拟合双组分GMM。(步骤2) </strong></p><p id="61cc" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 3。我们有整体损失的高斯分量；选择具有较小平均值的高斯分量。(步骤3) </strong></p><p id="77da" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 4。给定每个样本的损失l <em class="hh"> </em>，我们找到所选高斯分量g <em class="hh">(较小标准)</em>的概率p(g|l );如果它大于阈值0.5，则它是一个干净的数据集，否则就是有噪声的数据集，因此移除有噪声数据集的标注。(步骤4) </strong></p></blockquote></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="74ff" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在共除步骤之后，我们得到了带有标签的清洁数据集和没有标签的噪声数据集；现在，它是半监督学习。我们将关注MixMatch半监督学习方法，因为它的改进版本用于半监督学习的DivideMix方法。</p><p id="c29a" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">为此，我们需要理解MixMatch半监督学习(无旁路)的概念:</strong></p><blockquote class="lb lc ld"><p id="4c41" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">引文:d .贝特洛、n .卡利尼、I .古德费勒、n .纸诺、a .奥立佛和c .拉弗尔，2019年。Mixmatch:半监督学习的整体方法。<em class="hh"> arXiv预印本arXiv:1905.02249 </em>。</p></blockquote><p id="f467" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">半监督学习包含标记和未标记的数据，但它专注于利用未标记数据的模型。在这种方法中，我们需要关注三个术语:</p><ol class=""><li id="04ef" class="lp lq hh kf b kg le kk lf ko lr ks ls kw lt je lu lv lw lx bi translated">一致性正则化:当我们对输入做小的改变时，它迫使模型产生相似的输出分布。</li><li id="2a21" class="lp lq hh kf b kg ly kk lz ko ma ks mb kw mc je lu lv lw lx bi translated">熵最小化:它鼓励模型对未标记的数据给出有把握的预测。</li><li id="71ed" class="lp lq hh kf b kg ly kk lz ko ma ks mb kw mc je lu lv lw lx bi translated">通用正则化:很好地概括模型，避免过度拟合数据。</li></ol><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/ccc1d7528294f35266c777f0fc107e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ck5gER5JBH3IUCtPPZZntQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A. and Raffel, C., 2019. Mixmatch: A holistic approach to semi-supervised learning. <em class="mq">arXiv preprint arXiv:1905.02249</em></figcaption></figure><p id="ca33" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">MixMatch在训练时结合了这三个特性:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/eb846c3897207b1b8e97a0dfab3f2c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mU3QLSU-xQp0tpOJyd3dQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A. and Raffel, C., 2019. Mixmatch: A holistic approach to semi-supervised learning. <em class="mq">arXiv preprint arXiv:1905.02249</em></figcaption></figure><p id="a51c" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在不深入这个模型的数学知识的情况下，我们将讨论所遵循的方法。如上所述，我们考虑所有未标记的数据进行一致性正则化，并生成K个数据增强(增强是特定图像排列的改变)。我们将预测每幅图像的标签并计算平均值。数学上，</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/d6e58ecdad76fdb88ba036af90b6299a.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*5tVJhOtEzMerYR6fAaQkjg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Average (Consistency Regularisation)</figcaption></figure><blockquote class="lb lc ld"><p id="79b6" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">上面的数学等式猜测从单个图像产生的每K个增强图像的标签y。上述公式对增强图像上的预测进行了平均。</p></blockquote><p id="9dd0" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">之后，我们执行锐化作为熵最小化步骤。我们这样做是为了减少标签分布的熵(结果的随机性)。我们可以通过调整给定公式中的温度值来实现这一点:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/3a2530032b34b52240a518e6a6ffad6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*cqwSAdx6bCJJH-ADKP4iQQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Sharpen (Entropy Minimisation)</figcaption></figure><blockquote class="lb lc ld"><p id="0527" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">上面的数学方程取的是概率分布p，如果我们让T更接近0 (T — &gt; 0)，它将趋近于one-hot分布，对预测类取1，对其他所有类取0。所以降低温度会减少预测的随机性。</p></blockquote><p id="4b12" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">最后一步是概括；我们可以使用L-2损耗进行归纳。数学上，</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/884b4d092b38bcb11eb37b7d364f87ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6jy-xT_-TYcUxmIe.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">A generalisation (L2 Loss — MSE)</figcaption></figure><blockquote class="lb lc ld"><p id="1778" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">Y是上面数学方程中的正确值，Y-hat是预测值。上面的公式计算了两者之间的距离的平方。它处理图像之间的重大错误或较大差异。它有助于应用于两者的图像重建。</p></blockquote><p id="6ecf" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">上面解释的所有术语都是从数学公式中自明的。我们可以跳过数学部分，坚持从上面提到的定义来看它们的用处。</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><h1 id="697d" class="jf jg hh bd jh ji ng jk jl jm nh jo jp jq ni js jt ju nj jw jx jy nk ka kb kc bi translated"><strong class="ak"> <em class="mq">最主要的诀窍是如何在DivideMix方法中使用</em> </strong></h1><p id="22a1" class="pw-post-body-paragraph kd ke hh kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz je ha bi translated">我们知道，两个网络是并行运行的，所以我们需要一个网络可以帮助教导其他网络的东西。</p><p id="cd0a" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">[第12–19行]第5步:</strong>现在，通过共分，我们已经有了标记和未标记的数据，我们需要找到现有图像的增强。对于带标签的样本，我们将原始标签与预测标签相结合，对多个参数进行平均，并锐化结果。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nl"><img src="../Images/ac755d5d90c025a0f79bd80ca901720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yVx4Hj6zDgRFPTQxENnxzQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 12 -19</figcaption></figure><p id="3de9" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在上面的行中，在第18行，使用的公式是:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es nm"><img src="../Images/a44bc1724c048ee1c417f55048f7b007.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*2ok6uZk-rWZWbvRsaiDY-Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Label co-refinement</figcaption></figure><blockquote class="lb lc ld"><p id="8df3" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">在上面的公式中，y_b是实际的标签(记住在这一步中我们只考虑标记的数据)，p_b是预测的标签，它是多个增强的平均值；我们使用了w_b，这是来自另一个网络的干净概率(这是两个网络相互作用和相互帮助的地方)。</p></blockquote><p id="2d0e" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">锐化的整个过程(第19行)被称为标签协同细化。</p><p id="a5eb" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">[Line 20–21]步骤6: </strong>现在我们剩下未标记的数据，我们使用两个网络进行预测(这就是为什么我们在第20行求平均时有2M)，这有助于执行更可靠的标签猜测，然后应用锐化，整个过程称为共同猜测。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nn"><img src="../Images/0dbdf05b46a14694ca2f3ea8ab67d4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3l0sOTMfW1Uw8OPUR3xMdg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 20–21</figcaption></figure><p id="e410" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">【第23–27行】:第7步MixMatch</strong>现在，我们有了各种增加的标记样本和未标记数据的猜测标签。然后从这些集合中随机选择数据点，交叉熵，并计算MSE，精确地说是MixMatch方法。因此，我们丢失了标记和未标记的数据，如L_x和L_u(第26行)。</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es no"><img src="../Images/a60a1bc21d81dcca5f117d5c22f63f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_3tFEwuIJLwpvob39QwMw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Line 23–27</figcaption></figure><p id="bc3b" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">但是等等，当噪音太大的时候还有一个问题。该模型将只预测相同的类别以最小化损失，因此我们可以使用正则化公式L_reg来避免这种情况:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es np"><img src="../Images/9ed4842fc0ce1fcd39819ec02691f9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YEG4bcAxyvVvkl4qWdy4Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Regularisation Loss</figcaption></figure><blockquote class="lb lc ld"><p id="8b06" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">L_reg有助于控制特定批次中所有样本的所有输出的模型平均值。上式中，π为1/C(类)，X/U为以p为预测的标记和未标记数据。</p></blockquote><p id="db00" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">我们将所有损失相加，并使用<strong class="kf hi"> λ，</strong>控制它们的权重，因此我们的总损失是所有这些损失的总和(第26行):</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es nq"><img src="../Images/92390401d3676ab5b8958d36da3a3598.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*48Ebp1T7TSJwbcHXrM-Dpg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Total Loss</figcaption></figure><p id="54cc" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">在第27行中，我们应用随机梯度下降，其主要目的是通过改变参数值来减少损失。(深度文章:<a class="ae it" rel="noopener" href="/@hakobavjyan/stochastic-gradient-descent-sgd-10ce70fea389"> <em class="la">【随机梯度下降(SGD) </em> </a>)</p><p id="4ff5" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated"><strong class="kf hi">改进MixMatch的结论:</strong></p><blockquote class="lb lc ld"><p id="4c2f" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 1。我们考虑了所有的标记数据，找到了扩充版本，并使用地面真相，我们平均预测值。(步骤5) </strong></p><p id="2d11" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 2。我们考虑了未标记的数据，并使用两个网络来猜测标记，并对平均值进行集合预测。(步骤6) </strong></p><p id="a283" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated"><strong class="kf hi"> 3。在最后一步中，我们应用MixMatch并控制高噪声水平的预测，我们使用正则化损失并使用SGD和总损失更新参数。(步骤7) </strong></p></blockquote><figure class="ml mm mn mo fd ii er es paragraph-image"><div class="er es nr"><img src="../Images/899648c11c4ffb7f82d354e14f7d2389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*G2yDrkfHeeeJ9m1Z.jpg"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://memegenerator.net/Uber-Shatner" rel="noopener ugc nofollow" target="_blank"><strong class="bd jh">UBER SHATNER</strong></a></figcaption></figure><p id="6aae" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">总之，DivideMix是co-divide步骤和改进的MixMatch方法的混合，并且在噪声数据集上给出了极好的分类结果。DivideMix对CIFAR-10和CIFAR-100的影响:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ns"><img src="../Images/c1a6191dbb5d82f83184d95e23683c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yen4LEP3ccRg4CHXCgVwQQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Li, J., Socher, R. and Hoi, S.C., 2020. Dividemix: Learning with noisy labels as semi-supervised learning. <em class="mq">arXiv preprint arXiv:2002.07394</em>.</figcaption></figure><p id="bd48" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">本文附录中的图像结果:</p><figure class="ml mm mn mo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nt"><img src="../Images/f590ea28ba2a2b3b73d764d94bffaed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9Drbmh5P5sNzAwwAYPqHg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Li, J., Socher, R. and Hoi, S.C., 2020. Dividemix: Learning with noisy labels as semi-supervised learning. <em class="mq">arXiv preprint arXiv:2002.07394</em>.</figcaption></figure><blockquote class="lb lc ld"><p id="3efe" class="kd ke la kf b kg le ki kj kk lf km kn lg lh kq kr li lj ku kv lk ll ky kz je ha bi translated">作者在<a class="ae it" href="https://github.com/LiJunnan1992/DivideMix" rel="noopener ugc nofollow" target="_blank">https://github.com/LiJunnan1992/DivideMix</a>提供工作代码</p></blockquote><p id="49bc" class="pw-post-body-paragraph kd ke hh kf b kg le ki kj kk lf km kn ko lh kq kr ks lj ku kv kw ll ky kz je ha bi translated">要深入了解DivideMix，请跟随论文:</p><figure class="ml mm mn mo fd ii"><div class="bz dy l di"><div class="nu nv l"/></div></figure><div class="nw nx ez fb ny nz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="oa ab dw"><div class="ob ab oc cl cj od"><h2 class="bd hi fi z dy oe ea eb of ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="og l"><h3 class="bd b fi z dy oe ea eb of ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oh l"><p class="bd b fp z dy oe ea eb of ed ef dx translated">medium.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on in nz"/></div></div></a></div></div></div>    
</body>
</html>