<html>
<head>
<title>Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/neural-networks-ba6fa76eb719?source=collection_archive---------9-----------------------#2022-02-23">https://medium.com/mlearning-ai/neural-networks-ba6fa76eb719?source=collection_archive---------9-----------------------#2022-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d833" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DeepMind系列讲座笔记。</p><p id="7dea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里展示的大部分图片来自DeepMind讲座2的 <a class="ae jd" href="https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L2%20-%20UCLxDeepMind%20DL2020.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jc">幻灯片</em> </strong> <em class="jc">。</em>T9】</a></p><h2 id="970a" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">什么是人工神经网络？</h2><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es jz"><img src="../Images/0f7a25180275229184701d1706f54b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZG8m5DzgHa-DeKuWmJr0Aw.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 1</strong>: A simplified version of a real neuron in human brain vs an artificial neuron in a neural network.</figcaption></figure><p id="e27f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当谈到深度学习和神经网络时，我们经常看到或听到真实神经元与人工神经元的类比。但是这两者到底有什么相似之处呢？(<em class="jc">图1 </em></p><p id="c9c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">真正的</em> </strong>神经元是人脑中的一种特殊细胞，它代表一种简单的计算。真正的神经元包含三个基本部分:</p><ul class=""><li id="0d65" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">多个<em class="jc">树突</em>接收来自其他神经元的输入。</li><li id="6bdf" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated">一个<em class="jc"> soma </em>来执行简单的计算</li><li id="a416" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated">一个<em class="jc">轴突</em>产生一个输出</li></ul><p id="e3ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">据估计，人类大脑包含大约860亿个这样的神经元。每个神经元都与成千上万的其他神经元相连，形成网络。</p><p id="f46e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个<strong class="ig hi"> <em class="jc">人工</em> </strong>神经元是<strong class="ig hi"> <em class="jc">人工</em> </strong> <strong class="ig hi"> <em class="jc">神经网络(模型)</em> </strong>的一个积木。每个人工神经元代表一个简单的计算，并反映了来自真实神经元的一些神经生理学观察。然而，这并不意味着重现它们的动态。人工神经元还包含三个重要的元素，用下面的公式表示:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ld"><img src="../Images/f02a3b31a32951e3f1273780beed2c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*S6sb-sQz_lwTn7ytSLTeWg.png"/></div></figure><ul class=""><li id="b258" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated"><strong class="ig hi"> x </strong>:输入</li><li id="fe15" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi"> w </strong>:与每个输入相关的权重</li><li id="4903" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi"> b </strong>:偏置</li></ul><p id="b734" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个人工神经元中，“树突”接收那些(蓝色)输入，并传递给“体细胞”。“soma”然后执行一些简单的计算，将每个输入与其相关联的(红色)权重相乘，并将其与偏差相加。最后，“轴突”产生最终输出。</p><blockquote class="le lf lg"><p id="97d8" class="ie if jc ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated"><strong class="ig hi">注<em class="hh"> : </em> </strong>在机器学习中，我们经常将这些术语互换使用。<strong class="ig hi">线性</strong>表示<strong class="ig hi">仿射。层中的神经元</strong>通常被称为<strong class="ig hi">单元</strong>。<strong class="ig hi">参数</strong>通常被称为<strong class="ig hi">权重。</strong></p></blockquote><h2 id="88e3" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">我们如何构建一个神经网络？</h2><p id="2f63" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">现在，我们知道什么是神经网络。我们可能想知道如何构建这样的网络。让我们从一个单层神经网络开始(<em class="jc">图2 </em>)，并从那里建立复杂性。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es lp"><img src="../Images/f102b7dd5090cb8e4cdfd8f9bd3d3017.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*hpWCDrrTssHbiTicjhlJ0Q.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 2</strong>: A single-layer neural network</figcaption></figure><p id="97ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个<strong class="ig hi"> <em class="jc">单层</em> </strong>神经网络由(<em class="jc">图3 </em>)组成:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lq"><img src="../Images/b479bbd26465e6e5026796fca5f6a777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djh_OFOxYrmcHIvzm5MPcQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 3</strong>: A single layer perceptron with n inputs with their corresponding synaptic weights. All weighted inputs are added and an activation function controls the generation of the output signal</figcaption></figure><ul class=""><li id="d1ed" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">一个<strong class="ig hi">输入</strong>层:包含矢量化输入</li><li id="abaf" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated">一个<strong class="ig hi">线性</strong>层:一个人工神经元的集合，可以有效地向量化，并且容易组成。</li><li id="5b1c" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated">一个<strong class="ig hi">激活函数</strong>:一个通常用于诱导更复杂模型的对象。该对象引入了非线性行为。它产生概率估计，并有简单的导数。<em class="jc">表1 </em>描述了三种标准激活功能及其用法和注意事项:</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lr"><img src="../Images/e700ffd181d7f81f843f441688096c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GbgL7rgHe3Z3Uy34GlF2g.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Table 1</strong>: Most commonly used activation functions</figcaption></figure><ul class=""><li id="06fd" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">A <strong class="ig hi">损失</strong>:损失函数计算出的值，用于评估模型的性能。更小的损失意味着更好的模式。对于不同的任务，有许多损失函数。然而，我们用于二元分类的最常见的损失是<strong class="ig hi"> <em class="jc">交叉熵损失</em> </strong> <em class="jc">(图4) </em> <strong class="ig hi"> <em class="jc">。</em> </strong></li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ls"><img src="../Images/e1723e14619942a1a996239eed3fd43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*u3uVAr2J-5QNzoTbjGkbsg.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 4</strong>: Cross entropy loss</figcaption></figure><p id="05de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中<strong class="ig hi"> t </strong>是我们的目标，<strong class="ig hi"> p </strong>是我们的预测。交叉熵损失也称为<strong class="ig hi"> <em class="jc">负对数似然</em> </strong>或<strong class="ig hi"> <em class="jc">逻辑损失。</em>T29】</strong></p><ul class=""><li id="ee9c" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">一个<strong class="ig hi">目标</strong>:数据的地面真相</li></ul><p id="42e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们有了一些构建单层神经网络的基本要素。我们用它来构造一个两层的神经网络。</p><p id="a97a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个<strong class="ig hi"> <em class="jc">两层</em> </strong>神经网络(<em class="jc">图5 </em>):</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es lt"><img src="../Images/2c84098e1895f65edb097f3ddecec667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*ugXvPS_iG_4TVv80yxxubg.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 5</strong>: A two-layer neural network</figcaption></figure><p id="f19c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到这些部分是高度可组合的功能，可以以任何方式排列。然而，我们需要巧妙地将它们组合起来，以获得新的品质。</p><blockquote class="le lf lg"><p id="d0b1" class="ie if jc ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated"><strong class="ig hi">注意:</strong>添加层会增加模型的深度，而添加神经元会增加模型的宽度。深度模型比广度模型更容易表达对称性和规律性。</p></blockquote><p id="7d93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们可以通过添加更多层来扩展我们的神经网络(<em class="jc">图6 </em>)。每一层都越来越抽象，以检测不同的特征。</p><p id="bb5b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个<strong class="ig hi"> <em class="jc">多层</em> </strong>(深度)神经网络(<em class="jc">图6 </em>):</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lu"><img src="../Images/ab4ad0c5a8c53045b8f7d9f0ca0201ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TRc6KvkeGsdYVSgiDHZvWw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 6</strong>: A multi-layer neural network</figcaption></figure><p id="ebed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">太酷了！现在，我们有了构建和修改神经网络结构的基本知识。到目前为止，我们所遇到的被认为是一个<strong class="ig hi"> <em class="jc">正向</em> </strong>传递，我们从左(输入)到右(输出)正向传递变量的值。然而，对于一个模型来说，这还不足以学习和最小化损失。所以让我们来看看:</p><h2 id="bc4d" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">神经网络是如何学习的？</h2><p id="2abc" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">首先，我们需要知道如何将一个神经网络表示为一个<a class="ae jd" href="https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm" rel="noopener ugc nofollow" target="_blank"> <em class="jc">计算图</em> </a> ( <em class="jc">图7 </em>)。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lv"><img src="../Images/9642f1389a8725bd9b253600fcc67fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-ZhLXODLt0LngwZptTvUw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 7</strong>: Neural networks as computational graphs</figcaption></figure><p id="55e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，计算图是一个有向图，其中每个节点代表一个数学运算。由于神经网络由多个可组合的函数组成，我们希望将其描述为计算图，以便更好地表达和评估那些数学表达式。</p><p id="7fbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，我们需要复习一些<a class="ae jd" href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21" rel="noopener" target="_blank"> <strong class="ig hi"> <em class="jc">线性代数</em> </strong> </a> <strong class="ig hi"> <em class="jc">。</em>T9】</strong></p><p id="46b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">梯度</em> </strong> <em class="jc">(图8) </em>:从d维空间到标量空间的函数。计算梯度无非是计算偏导数的向量。高阶抽象中的偏导数只是函数增长最多的一个方向，而负梯度是下降最多的方向。对于jᵗʰ维函数，我们有这个函数关于jʰ输入的偏导数。</p><p id="b207" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">雅可比矩阵</em> </strong> <em class="jc">(图8)</em>:kᵗʰ维数泛化如果你有k个输出，那么它是一个矩阵，其中你有iᵗʰ输出相对于jᵗʰ输入的偏导数。这个雅可比矩阵集合了<strong class="ig hi"> <em class="jc">反向传播</em> </strong>所需的偏导数，称为一个<strong class="ig hi"> <em class="jc">向后</em> </strong>传递。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lw"><img src="../Images/eddce1f37c1c7255e8da13c8f7b295fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R2CeFNGKnUouwI3p7OsLYA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 8</strong>: Gradient and Jacobian recap</figcaption></figure><p id="e251" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">【梯度下降】(GD) </em> </strong> <em class="jc">(图9) </em> <strong class="ig hi"> <em class="jc"> : </em> </strong>求给定函数的局部极小值/极大值的一种迭代一阶优化算法。GD通过取从当前点获得的值来迭代计算下一个点，然后减去/加上学习率乘以该特定点的梯度。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es lx"><img src="../Images/a3f70a2d4071ba5175692f58ffce05e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*DJ2cmlfiEyJTBcsV_0bBBw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 9</strong>: Gradient descent recap to minimize the loss function</figcaption></figure><p id="c278" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中<strong class="ig hi"> θ </strong>是我们要调整以最小化损失(代价)函数的参数(权重)，而<strong class="ig hi"> α </strong>是学习率<strong class="ig hi"> </strong>以缩放梯度。</p><blockquote class="le lf lg"><p id="a77b" class="ie if jc ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated"><strong class="ig hi">注:</strong>学习率的选择至关重要。此外，还有其他基于GD开发的优化算法，如Adam或rms prop…然而，Adam是主要用于训练神经网络的黄金标准优化算法。</p></blockquote><p id="d42c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，让我们把所有的碎片放在一起！</p><p id="1a3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让神经网络学习:</p><ul class=""><li id="a946" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">首先，我们将x作为输入进行正向传递，并计算成本c作为输出<em class="jc">(图10) </em>。</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ly"><img src="../Images/f501b6331098c013064631dfbd56666e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXujQ5Y15PM302M3beEnFQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 10:</strong> A forward pass</figcaption></figure><ul class=""><li id="8505" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">然后，我们从c点开始进行反向遍历，计算计算图<em class="jc">(图11) </em>中所有节点(包括代表权重和偏差的节点)的梯度。</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lz"><img src="../Images/9b9fa8ab04ffcae33f32217861285169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7Po8P7S8J9ng61s4yZyow.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 11:</strong> A backward pass</figcaption></figure><ul class=""><li id="2794" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">然后，我们通过应用梯度下降算法来更新权重(参数)。</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ma"><img src="../Images/4663133962d7fa7f18cbb3340e79fe35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*CyNnnICwYj_qJyQ0IP5RgA.png"/></div></figure><ul class=""><li id="bee9" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated">我们重复这个过程，直到满足停止标准。</li></ul><h2 id="e48f" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">一个模型可以有哪些实际问题？</h2><p id="a404" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">既然我们知道模型是如何学习的，我们就需要考虑它在训练过程中可能遇到的一些实际问题。</p><p id="7391" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从一个训练集开始——一个有限的数据集，我们在其上建立我们的模型。我们的目标是最小化损失函数，从而最小化我们数据的<strong class="ig hi"> <em class="jc">训练误差</em> </strong>(一种训练风险)。然而，我们实际上并不关心这个训练误差，因为GD算法保证它会减少。相反，我们关心一个<strong class="ig hi"> <em class="jc">测试错误</em> </strong>(一个测试风险)，它测量我们的模型在一个测试集中的表现——一个我们的模型以前从未遇到过的有限数据集。</p><p id="aae1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型的复杂性和这两个错误的行为<em class="jc">(图12) </em>之间有关系。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es mb"><img src="../Images/908d5635f12d53d01a3d830560fcac06.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*0zAgB7a_coCjZ_Tr0aKqwA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 12:</strong> Classical U-shaped risk curve and training curve</figcaption></figure><p id="15e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在统计学和统计学习理论的经典结果中，随着模型变得越来越复杂，它可以创建高度不必要的复杂假设。换句话说，测试风险先下降，但最终会上升并导致<strong class="ig hi"> <em class="jc">过拟合</em> </strong>。相比之下，如果模型过于简单，就不能足够代表我们的数据。因此，测试风险仍然很高，并导致<strong class="ig hi"> <em class="jc">不适合</em> </strong>。因此，我们希望通过应用一些正则化技术来确保我们的模型既不欠拟合也不过拟合，例如:</p><ul class=""><li id="a8ab" class="kp kq hh ig b ih ii il im ip kr it ks ix kt jb ku kv kw kx bi translated"><strong class="ig hi"> Lp正则化</strong>:将额外损失之一直接附加到权重上，这样你的权重就小了。如果你的重量很小，这个函数就不会太复杂。</li><li id="0f11" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi">脱落</strong>:一些神经元被随机去激活。因此，表示更复杂的事物要困难得多。</li><li id="e154" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi">噪声数据</strong>:添加噪声</li><li id="36ee" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi">提前停止:</strong>如果训练误差没有改善，则提前停止训练过程</li><li id="8769" class="kp kq hh ig b ih ky il kz ip la it lb ix lc jb ku kv kw kx bi translated"><strong class="ig hi">批量/层定额:</strong>为我们的数据增加一些规范化</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es mc"><img src="../Images/8e6a561f17cf1731ae1a61b1b61b28ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*B0E8khD__uXdv6vWh89o3Q.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx"><strong class="bd jg">Figure 13</strong>: Modern curves for training risk (dashed lines) and test risk (solid lines)</figcaption></figure><p id="0eac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一方面，现代结果表明，随着模型的增长，它们的学习动力会发生变化，使它们不太容易过度拟合。然而，这些大模型仍然可以从正则化技术中受益。</p><h2 id="d0f9" class="je jf hh bd jg jh ji jj jk jl jm jn jo ip jp jq jr it js jt ju ix jv jw jx jy bi translated">下一步是什么？</h2><blockquote class="md"><p id="3c53" class="me mf hh bd mg mh mi mj mk ml mm jb dx translated">接下来是DeepMind深度学习系列第三讲的笔记:<a class="ae jd" rel="noopener" href="/@nghihuynh_37300/convolutional-neural-networks-for-image-recognition-7148a19f981f">用于图像识别的卷积神经网络。</a></p></blockquote><div class="mn mo mp mq mr ms"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">medium.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng kj ms"/></div></div></a></div></div></div>    
</body>
</html>