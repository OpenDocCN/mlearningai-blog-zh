# 可解释人工智能:主要方法综述

> 原文：<https://medium.com/mlearning-ai/explainable-ai-a-complete-summary-of-the-main-methods-a28f9ab132f7?source=collection_archive---------1----------------------->

近年来，随着各种类型的 AI 模型(树、神经网络等)的传播。)，可解释的人工智能的角色变得至关重要。

![](img/1104f00d4ceeb858bd059112002da3f1.png)

[Image by Author]

首先，我们来定义一下什么是可解释的 AI:

> **可解释的人工智能(XAI)** 是一组过程和方法，允许人类用户理解和信任机器学习算法创建的结果和输出。可解释的人工智能用于描述人工智能模型、其预期影响和潜在偏差。它有助于在人工智能决策中描述模型的准确性、公平性、透明度和结果。[IBM]

在这篇文章中，我们将一起去看用于可解释人工智能的主要方法(SHAP，石灰，树代理等。)，以及他们的特点。当然，它们并不是迄今存在的所有方法，但无论如何，我认为这份清单已经足够详尽了。

另外，专门针对它们的文章将很快到达，其中包含 Python 的见解和实现，所以请保持最新状态！😜

# 内容

下面是我们将一起分析的可解释性技术的列表:

*   SHAP
*   石灰
*   排列重要性
*   部分相关图
*   莫里斯敏感性分析
*   累积局部效应
*   锚
*   对比解释法(CEM)
*   反事实的例子
*   集成渐变
*   通过递归分割的全局解释(GIRP)
*   原破折号
*   可扩展贝叶斯规则列表
*   树替代物
*   可解释增压机(EBM)

很长的名单，对吧？别害怕，我会很轻松的😄

然而，在开始之前，我们必须介绍一下**可解释性水平**的概念。可解释技术主要分为两类:全局和局部。

*   **全球**:他们从总体上解释这种模式，指出其通用的操作规则。
*   本地:他们解释每一个数据，模型是如何推理的，以及导致特定输出的规则。

GIF by The Office on [gyphy.com](https://giphy.com/gifs/theoffice-nbc-the-office-tv-WsNbxuFkLi3IuGI9NU)

# 解释技巧

## [SHAP](https://github.com/slundberg/shap)

**SHAP(Shapley Additive explaints)**是一个使用 SHapley 值解释任何模型输出的框架，SHapley 值是一种博弈论方法，常用于最优信贷分配。虽然这可以用于任何黑盒模型，但 SHAP 可以在特定的模型类(如树集合)上更有效地计算。

此方法是附加功能属性方法类的成员；特征属性指的是这样的事实，即要解释的结果(例如，分类问题中的类别概率)相对于基线(例如，训练集中该类别的平均预测概率)的变化可以以不同的比例归因于模型输入特征。

SHAP 既可以在全球使用**，也可以在本地使用**。

## [石灰](https://github.com/marcotcr/lime)

**局部可解释模型不可知解释(LIME)** 是一种在任何黑盒模型预测的决策空间周围拟合**代理玻璃盒模型的方法**。LIME 明确尝试对任何预测的局部邻域进行建模。LIME 的工作原理是干扰任何单个数据点，并生成合成数据，这些数据由黑盒系统进行评估，最终用作玻璃盒模型的训练集。

石灰被设计成局部施用**。**

## **[排列重要性](https://christophm.github.io/interpretable-ml-book/feature-importance.html)**

**思路如下:**特征重要性**可以通过看**得分多少**(准确率，F1，R 等)来衡量。—我们感兴趣的任何分数)**当功能不可用时**减少。**

**为此，可以从数据集中移除一个特征，重新训练估计器并检查分数。**

**当然，排列重要性只能全局应用**。****

## ****[部分依赖图](https://christophm.github.io/interpretable-ml-book/pdp.html)****

******部分依赖图(短 PDP 或 PD 图)**显示了一个或两个特征对机器学习模型的预测结果的边际效应。部分相关性图可以显示目标和特征之间的**关系是线性的、单调的还是更复杂的。******

****对于基于**扰动的**可解释性方法来说，它是相对快速的。PDP 假设功能之间的独立性，当不满足这一点时，可能会误导可解释性****

****至于前一个，只能全局**应用**。****

## ****[莫里斯敏感度分析](https://en.wikipedia.org/wiki/Morris_method)****

****这是一个**一次一步(OAT)全局灵敏度分析**，其中每次运行仅调整一个输入的级别(离散值)。相对于其他敏感性分析算法，莫里斯方法速度更快(模型执行次数更少)，但代价是无法区分相互作用的非线性。**这通常用于筛选对进一步分析足够重要的输入。******

****同样，它只能全局应用**。******

## ******[累积局部效应(ALE)](https://christophm.github.io/interpretable-ml-book/ale.html)******

********累积局部效应(ALE)** 是一种计算特征效应的方法。该算法为表格数据的分类和回归模型提供了模型不可知(黑盒)的全局解释。ALE 解决了部分相关图(PDP)的一些关键缺点。******

****虽然违反直觉，但它只能适用于全球的**。******

## ****[主播](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)****

****锚背后的想法是用称为锚的高精度规则来解释复杂模型的行为。这些锚是确保具有高度置信度的某个预测的局部充分条件。****

****由此可见，它只能在本地**应用**。****

## ****[对比解释法(CEM)](https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html)****

******CEM** 根据相关肯定(PP)和相关否定(PN)为分类模型生成基于实例的局部黑盒解释。不仅强调什么应该**最低限度地和充分地**存在以证明神经网络对输入示例的分类(相关的肯定)，而且强调什么应该**最低限度地和必然地不存在**(相关的否定)，以便形成更完整和全面的解释。****

****CEM 设计用于局部应用**。******

## ****[反事实事例](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html)****

****反事实解释“询问”一个模型，以显示**为了翻转整体预测，许多单个特征值必须如何改变。对结果或情况的反事实解释采取“如果没有发生，就不会发生”的形式。在机器的上下文中，学习分类器将是感兴趣的实例，并且将是由模型预测的标签。******

****反事实实例被设计成在本地应用**。******

## ****[综合渐变](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)****

****集成梯度旨在基于模型输出相对于输入的梯度，将**重要性值赋予机器学习模型的每个输入特征**。它有许多用例，包括理解特性重要性、识别数据偏差和调试模型性能。****

****集成梯度设计用于局部应用**。******

## ******[通过递归分割(GIRP)进行全局解释](https://arxiv.org/pdf/1802.04253.pdf)******

******一个紧凑的二叉树，通过使用输入变量的贡献矩阵表示模型中隐含的**最重要的决策规则**，对 ML 模型进行全局解释。为了生成解释树，
统一过程通过最大化分割空间之间分割变量的平均贡献差来递归分割输入变量空间。******

****正如我们所猜测的，GIRP 只能在全球范围内应用**。******

## ******[原破折号](https://arxiv.org/abs/1707.01212)******

******一种在现有机器学习程序中寻找“原型”的新方法。原型可视为对模型预测能力有较大影响的数据子集。原型的要点是这样说的，如果你删除了这些数据点，模型就不能正常工作，这样人们就可以理解是什么在驱动预测。******

******原破折号只能局部应用**。********

## ******[可扩展贝叶斯规则列表](https://www.seltzer.com/assets/publications/Scalable-Bayesian-Rule-Lists.pdf)******

******从数据中学习并创建决策规则列表。它们的逻辑结构是一系列 IF-THEN 规则，与决策列表或单边决策树相同。******

******可扩展的贝叶斯规则列表既可以在全局**使用，也可以在局部**使用。******

## ****[树代理](https://christophm.github.io/iml/reference/TreeSurrogate.html)****

******树代理是一种可解释的模型，它被训练成近似黑盒模型的预测。通过解释代理模型，我们可以得出关于黑箱模型的结论。策略树很容易被人类理解，并提供对未来行为的定量预测。******

****树代理既可以在全球使用**，也可以在本地使用**。****

## ****[可解释增压机(EBM)](https://interpret.ml/docs/ebm.html)****

****循证医学是微软研究院开发的一个可解释的模型。它使用现代机器学习技术，如装袋、梯度推进和自动交互检测，为传统 gam(广义加性模型)注入新的活力。****

****可解释提升机器(EBM)是一个基于树的、循环梯度提升的广义加法模型，具有自动交互检测功能。EBM 通常与最先进的黑盒模型一样精确，同时保持完全可解释性。虽然 EBM 的训练速度通常比其他现代算法慢，但 EBM 在预测时非常紧凑和快速。****

****ECM 既可以在全球**使用，也可以在本地**使用。****

****本文到此为止。非常感谢你能走到这一步。****

****再见，
弗朗西斯科****

****[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)****