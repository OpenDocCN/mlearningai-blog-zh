<html>
<head>
<title>Fundamentals of BIG DATA with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark的大数据基础</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/big-data-fundamentals-with-pyspark-939112ef2281?source=collection_archive---------1-----------------------#2021-06-04">https://medium.com/mlearning-ai/big-data-fundamentals-with-pyspark-939112ef2281?source=collection_archive---------1-----------------------#2021-06-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/37e5f0ae731e6bfd88aca8674809d9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Tpw9vFpxzGb7eanXvE2TQ.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" href="https://www.edureka.co/blog/big-data-analytics/" rel="noopener ugc nofollow" target="_blank">https://www.edureka.co/blog/big-data-analytics/</a></figcaption></figure><div class=""/><blockquote class="iu iv iw"><p id="c0af" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">本文介绍了激动人心的大数据世界，以及处理大数据的各种概念和不同框架。您将理解为什么Apache Spark被认为是大数据的最佳框架</p></blockquote><h1 id="6206" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">大数据概念和术语</strong></h1><p id="c80e" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">大数据到底是什么？这个术语指的是对传统数据处理软件来说过于复杂的数据集的研究和应用。</p><p id="6a82" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">大数据有三个v用来描述其特征:<strong class="ja hy">量</strong>指数据的大小，<strong class="ja hy">种类</strong>指数据的不同来源和格式，<strong class="ja hy">速度</strong>指数据生成和可供处理的速度</p><p id="d6bc" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz">现在，我们来看看大数据的一些概念和术语</em></p><blockquote class="iu iv iw"><p id="b951" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">集群计算</strong>是将多台机器的资源汇集起来完成任务</p><p id="4ff8" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">并行计算</strong>是一种同时进行许多计算的计算类型</p><p id="995a" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">分布式计算</strong>涉及并行运行作业的节点或联网计算机</p><p id="7080" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">批处理</strong>是指将数据分成更小的块，并在单独的机器上运行每一块</p><p id="765c" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">实时处理</strong>要求信息被立即处理并准备就绪</p></blockquote><h1 id="2b2d" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">大数据处理系统</h1><p id="325c" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated"><strong class="ja hy"> Hadoop/MapReduce : </strong>一个用于批量数据的开源可扩展框架。</p><p id="0ff3" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">Apache Spark: 它也是开源的，适合批处理和实时数据处理。这是一个用于大数据处理的快速通用框架。Apache Spark提供Scala、Java、Python和r的高级API。它在内存中运行大多数计算，从而为交互式数据挖掘等应用程序提供更好的性能。</p><p id="b9b7" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">它是Hadoop MapReduce的强大替代品，具有丰富的功能，如机器学习、实时流处理和图形计算。生态系统的中心是Spark核心，它包含Spark的基本功能。Spark的其余库都是基于它构建的。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lc"><img src="../Images/afaa66357b5df249a0927585b0663816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsAaANasQRWb0PEW_jHCqQ.png"/></div></div></figure><blockquote class="iu iv iw"><p id="7d52" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">停车以两种模式运行。第一种是本地模式，您可以在一台机器上运行Spark，比如您的笔记本电脑。这对于测试、调试和演示非常方便。第二种是集群模式，Spark在集群上运行。集群模式主要用于生产。</p></blockquote><h1 id="1127" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak"> Spark的Python版本:PySpark </strong></h1><p id="3742" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">Apache Spark最初是用Scala编程语言编写的。为了用Spark支持Python，PySpark的计算能力与Scala相似。PySpark中的API类似于Pandas &amp; Scikit-learn python包。</p><p id="c9a8" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">Spark附带了一个交互式python shell，其中已经安装了PySpark。在集群上运行作业之前，这对于快速交互式原型设计特别有帮助。与大多数其他shell不同，Spark shell允许您与分布在许多机器上的磁盘或内存中的数据进行交互，Spark负责自动分配这种处理。</p><p id="5dba" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">Spark提供了三种编程语言的外壳:Scala的spark-shell、Python的PySpark和R. PySpark的spark。与Scala Shell类似，Pyspark shell已经得到了扩展，可以支持连接到集群。</p><p id="f0fe" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">在PySpark Shell中，SparkContext表示Spark功能的入口点。PySpark在PySpark shell中自动为您创建一个<code class="du lh li lj lk b">SparkContext</code>(这样您就不必自己创建了)，并通过变量<code class="du lh li lj lk b">sc</code>公开。您可以在PySpark shell中将SparkContext作为一个名为sc的变量来访问。就像你的车钥匙。没有钥匙你不能进入房子，同样，没有入口点，你不能运行任何PySpark作业。</p><p id="94dd" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">现在，让我们来看看SparkContext的一些重要属性。</p><blockquote class="iu iv iw"><p id="974f" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy"> sc.version </strong>显示你当前运行的spark版本</p><p id="b6db" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy"> sc.pythonVer </strong>显示Spark目前使用的Python版本。</p><p id="287f" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy"> sc.master </strong>显示集群的URL或以本地模式运行的“本地”字符串。</p></blockquote><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ll"><img src="../Images/8f7c0edcfe8c95441a03c7d2c4bed83e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fRlionhyxK6yrMjsCbEUw.png"/></div></div></figure><p id="26ae" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">要展示PySpark的shell的强大功能，最简单的方法就是开始使用它。让我们以PySpark shell中包含从1到100的数字的简单列表为例。</p><p id="a43a" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">这里要理解的最重要的事情是，我们没有创建任何SparkContext对象，因为PySpark自动创建名为<code class="du lh li lj lk b">sc</code>的SparkContext对象，默认情况下在PySpark shell中。</p><p id="2355" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">您可以使用SparkContext通过两种不同的方法将原始数据加载到PySpark中，我们将在后面讨论:</strong></p><ol class=""><li id="c9da" class="lm ln hx ja b jb jc jf jg kw lo ky lp la lq jv lr ls lt lu bi translated">SparkContext的并行化方法</li><li id="8a80" class="lm ln hx ja b jb lv jf lw kw lx ky ly la lz jv lr ls lt lu bi translated">SparkContext的textFile方法</li></ol><h1 id="1fba" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">在</strong> Python中使用Lambda函数</h1><p id="bffd" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">Python支持匿名函数的创建。也就是说，函数在运行时不绑定到名称，而是使用一个名为lambda的结构。它与典型的函数概念(如映射和过滤函数)结合使用。和def一样，lambda创建了一个函数，这个函数将在程序的后面被调用。让我们看看它的一些用途:</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ma"><img src="../Images/92224cea9dc250d97d0e9d63472b62fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XdCt-ff7IgqE7qhINXfbBA.png"/></div></div></figure><p id="d198" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"> <em class="iz">在python中使用Lambda函数— map(): </em> </strong> <em class="iz">使用列表中的所有项目调用map函数，并返回一个新列表，其中包含该函数为每个项目返回的项目。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mb"><img src="../Images/2d3991cb9eec1b630506949241dad683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntWipwXPSG2UhLLALaCyJg.png"/></div></div></figure><p id="48a6" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"> <em class="iz">使用python中的Lambda函数-filter():</em></strong><em class="iz">对列表中的所有项目调用该函数，并返回一个新列表，其中包含该函数计算结果为真的项目。</em></p><h1 id="7331" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">PySpark RDD简介</h1><p id="df58" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">它只是一个分布在集群中的数据集合。RDD是PySpark的基础和中枢数据类型。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mc"><img src="../Images/237b2dc6da1945c47e717869f5ade604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m3gvxdeqDGQ3ZM6JTlbGVw.png"/></div></div></figure><p id="5cf9" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">现在，让我们看看RDD的不同特征。RDD这个名字抓住了3个重要特征:</p><blockquote class="iu iv iw"><p id="51a5" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">弹性</strong>，指承受故障并重新计算缺失或损坏的分区的能力。</p><p id="5237" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">分布式</strong>，即为了高效计算，将作业跨越集群中的多个节点。</p><p id="57bd" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hy">数据集</strong>，是已分区数据的集合，例如数组、表、元组或其他对象。</p></blockquote><p id="9181" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">有三种不同的方法来创建rdd，其中您已经看到了两种方法，这两种方法在前面已经提到过。</p><p id="c5ce" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"> SparkContext的并行化方法:</strong></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es md"><img src="../Images/282bd333d01cd5caf687d2b9eb029710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-1KJ1GayUH5eZx8ISCwcA.png"/></div></div></figure><p id="70fc" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">来自外部数据集(</strong> SparkContext的textFile方法<strong class="ja hy"> ): </strong></p><p id="16c3" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">存储在HDFS的文件或亚马逊S3桶中的对象，或者本地存储的文本文件中的行，并将其传递给SparkContext的text file方法。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/6ac4cd6a7d42ed998bd30e2a808d5e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_u6e0XZjNWdhgqBOKK2fcw.png"/></div></div></figure><p id="1565" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">来自现有rdd</strong>(突变rdd)<strong class="ja hy">:</strong></p><p id="e37f" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">这种转变是从已经存在的RDD创造一个RDD的方式。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mf"><img src="../Images/8aa02a290cd052e5db9f4dc43a5885f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrGoVBdwe7kClcnl6XFvMQ.png"/></div></div></figure><h1 id="d7ce" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">PySpark中的分区</h1><p id="8898" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">数据分区是Spark中的一个重要概念，理解Spark如何处理分区可以控制并行性。Spark中的分区是大数据集的划分，每个部分都存储在集群中的多个位置。默认情况下，Spark在创建RDD时会根据可用资源、外部数据集等因素对数据进行分区，但是，可以通过传递第二个名为minPartitions的参数来控制这种行为，该参数定义了为RDD创建的最小分区数。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mg"><img src="../Images/cfe0cccf3295a6225fee7b02558c6bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6z7gPz0OzIYcjSSzc3A0UQ.png"/></div></div></figure><h1 id="2e52" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">py spark rdd简介</strong></h1><p id="867b" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">PySpark中有各种支持rdd的操作。PySpark中的rdd支持两种不同类型的操作——转换和操作。</p><blockquote class="iu iv iw"><p id="bb40" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">转换是对返回新RDD的rdd的操作。</p><p id="91bd" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">动作是对RDD进行某种计算的操作。</p></blockquote><p id="6596" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">惰性评估:</strong>帮助rdd容错和优化资源使用的最重要功能。Spark从您在RDD上执行的所有操作创建一个图形，并且只有当在RDD上执行某个操作时，图形才会开始执行。</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/9541a8b4a25be4b31895450385d9d1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARnuCVc4nk-udXAyoZpfNA.png"/></div></div></figure><h2 id="f97f" class="mi jx hx bd jy mj mk ml kc mm mn mo kg kw mp mq kk ky mr ms ko la mt mu ks mv bi translated"><strong class="ak">rdd上的转换:</strong></h2><p id="02a8" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">地图()</em> </strong> <em class="iz">接受一个函数并将其应用于RDD的每个元素。</em></p><p id="3d84" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">filter()</em></strong><em class="iz">接受一个函数并返回一个RDD，该只包含通过条件的元素。</em></p><p id="468a" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">flat map()</em></strong><em class="iz">类似于映射转换，只是它为源RDD中的每个元素返回多个值。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mw"><img src="../Images/54632def2f92b064afd4dc93def81995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dICwgKyUhxiSXsipuxzKbQ.png"/></div></div></figure><p id="7d2f" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">union()</em></strong><em class="iz">返回一个RDD与另一个RDD的联合。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/1dd58eba364b31653435fed5f61b9cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mvy81Fe3j1UDIWNhCf2eng.png"/></div></div></figure><h2 id="a06f" class="mi jx hx bd jy mj mk ml kc mm mn mo kg kw mp mq kk ky mr ms ko la mt mu ks mv bi translated">RDDs上的行动<strong class="ak">:</strong></h2><p id="9734" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">collect()</em></strong><em class="iz">动作从RDD返回完整的元素列表。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es my"><img src="../Images/2f3fa050a35120880e03e45c0b36af64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCigaJPokrNgkL6k_GtLpQ.png"/></div></div></figure><p id="b998" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">take()</em></strong><em class="iz">动作从RDD中打印出N个元素。</em></p><p id="4b3d" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">count()</em></strong><em class="iz">动作返回RDD中的行/元素总数</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mz"><img src="../Images/4e369c4c7822b3a0d10aa6d8e89dea7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkPmZkzm8s2vgAydlLS8_Q.png"/></div></div></figure><p id="9803" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><em class="iz"/><strong class="ja hy"><em class="iz">first()</em></strong><em class="iz">动作返回RDD </em>中的第一个元素</p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es na"><img src="../Images/9d465eee6da0e95ffe6dea72762a1a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVyM2q7Bj55VVzPHSL2qdQ.png"/></div></div></figure><h1 id="0026" class="jw jx hx bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">PySpark中对rdd的介绍</h1><p id="ce5c" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated">真实世界的数据集通常是键/值对。每一行都是映射到一个或多个值的键。为了处理这类数据集，PySpark提供了一种特殊的数据结构，称为pair RDDs。在对rdd中，键指的是标识符，而值指的是数据。</p><p id="923b" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">创建rdd对的两种最常见方式如下:</strong></p><blockquote class="iu iv iw"><p id="c078" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">创建对rdd的第一步是将数据转换成键/值形式。</p><p id="3a2b" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">接下来，我们使用map函数创建一个对RDD，该函数返回带有键/值对的元组，键是名称，年龄是值。</p></blockquote><h2 id="1d86" class="mi jx hx bd jy mj mk ml kc mm mn mo kg kw mp mq kk ky mr ms ko la mt mu ks mv bi translated">对rdd的转换:</h2><p id="5262" class="pw-post-body-paragraph ix iy hx ja b jb ku jd je jf kv jh ji kw kx jl jm ky kz jp jq la lb jt ju jv ha bi translated"><strong class="ja hy">reduceByKey运行多个并行操作，对数据集中的每个键执行一个操作，返回由每个键和该键的缩减值组成的新RDD。</strong></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nb"><img src="../Images/e16e75c881f19260470a24a3b1406853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IktR6g1HT24TEgomgN1r4A.png"/></div></div></figure><p id="a6e0" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"><em class="iz">sortByKey:</em></strong><em class="iz">我们可以对RDD对进行排序，只要在Key中定义了一个排序，并返回一个按key升序或降序排序的RDD。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nc"><img src="../Images/739b586f11ed154c01f0ae6a1e4f7ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ioCpdoVUtvYMgt2_dthG9g.png"/></div></div></figure><p id="e48c" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"><em class="iz">group by key:</em></strong><em class="iz">它将RDD对中具有相同关键字的所有值分组</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nd"><img src="../Images/fc4892d3adcb86793d7b0fea55dd0e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTIoD9diy9gLsL3iv7ol1g.png"/></div></div></figure><p id="97f1" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"> <em class="iz"> join转换:</em> </strong> <em class="iz">应用join转换通过根据相同的键对元素进行分组，将两个对rdd合并在一起。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ne"><img src="../Images/5e475ef7f6414276e3cb9dc5d919b8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNK_uvNqcJ9d_rEwtl5YZw.png"/></div></div></figure><h2 id="d6e4" class="mi jx hx bd jy mj mk ml kc mm mn mo kg kw mp mq kk ky mr ms ko la mt mu ks mv bi translated"><strong class="ak">对rdd对的高级操作:</strong></h2><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nf"><img src="../Images/12f37fb83e4724deeae38ebd62096909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3W4liOvaYihIY4E4GJhog.png"/></div></div></figure><p id="fe3a" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"> <em class="iz"> Reduce() </em> </strong> <em class="iz">对RDD的两个同类型元素进行运算，返回一个同类型的新元素。该函数应该是可交换的和可结合的，这样它就可以被正确地并行计算。</em></p><h2 id="1149" class="mi jx hx bd jy mj mk ml kc mm mn mo kg kw mp mq kk ky mr ms ko la mt mu ks mv bi translated"><em class="ng">在许多情况下，不建议对rdd运行收集操作，因为数据非常庞大。在这些情况下，通常将数据写出到分布式存储系统。</em></h2><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nh"><img src="../Images/91fae509e9650b8170e9ab4c5c39cf02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hr1pFiXvZ8lRqiOqcm76Jg.png"/></div></div></figure><p id="fb17" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"><em class="iz">【saveAsTextFile()</em></strong><em class="iz">默认情况下将RDD与每个分区保存为一个目录下的单独文件。但是，您可以对其进行更改，以返回一个新的RDD，使用coalesce方法将其缩减为一个分区。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mz"><img src="../Images/a4da81d3d1b0d5ed60390026fe61c89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yVs6crCshRpZ8V1tnwcHlg.png"/></div></div></figure><p id="a32b" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"><em class="iz">【countByKey()</em></strong><em class="iz">仅在类型为(Key，Value)的rdd上可用。通过countByKey操作，我们可以计算每个键的元素数量。需要注意的一点是，countByKey应该只用于大小足够小以适合内存的数据集。</em></p><figure class="ld le lf lg fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ni"><img src="../Images/449aaf600cd64856dd95dad1ea661363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1QJ19edom30PdgxjEL4Klw.png"/></div></div></figure><p id="5b6d" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy"><em class="iz">【collectAsMap()</em></strong><em class="iz">将RDD中的键值对作为字典返回给。与SparkContext的并行化方法一样，collectAsMap将RDD中的键值对生成为一个字典，可用于下游分析。与countByKey类似，只有当结果数据预计很小时，才应该使用此操作，因为所有数据都被加载到内存中。</em></p><p id="5181" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated"><strong class="ja hy">因此，我们已经用PySpark概述了大数据基础知识，并在此过程中学习了一些用于RDD变换和操作的有用语法。还有一篇文章介绍了SQL、数据帧和使用MLib的机器学习。要了解它，可以点击这个</strong> <a class="ae hu" href="https://datasciencelogs.medium.com/pyspark-sql-and-dataframes-4c821615eafe" rel="noopener"> <strong class="ja hy">链接</strong> </a> <strong class="ja hy">。</strong></p><p id="5b4e" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji kw jk jl jm ky jo jp jq la js jt ju jv ha bi translated">保持联系，享受阅读！</p></div></div>    
</body>
</html>