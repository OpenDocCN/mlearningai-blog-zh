# 人工智能不需要意识

> 原文：<https://medium.com/mlearning-ai/ais-dont-need-consciousness-890b1a450216?source=collection_archive---------6----------------------->

![](img/88eba39dcd60bde9f1cb662ca28e96c1.png)

我终于有时间评论最后一部人工智能(AI)剧了:谷歌工程师布雷克·莱莫因[警告联邦调查局称 LaMDA(他的公司制造的人工智能)变得有意识](https://www.huffpost.com/entry/blake-lemoine-lamda-sentient-artificial-intelligence-google_n_62a5613ee4b06169ca8c0a2e?d_id=3887326&ref=bffbhuffpost&ncid_tag=fcbklnkushpmg00000063&utm_medium=Social&utm_source=Facebook&utm_campaign=us_main&fbclid=IwAR0o5U4wv2cDP8o3XIAekj2Xh5wVPZVzVhyH696N8tnLv_m-YXtUDt0tFNU)(公平地说，莱莫因与人工智能[对话的文字记录相当惊人](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917))。推特上和咖啡机旁激烈的讨论接踵而至。我发现来自公众和机器学习社区的所有这些热情和困惑令人兴奋和鼓舞。不是因为它告诉我们关于 LaMDA 和它的兄弟们，而是因为它告诉我们关于我们自己:人类。它真实地反映了我们对人类智能代理可能/应该是什么样子的恐惧、希望和偏见。

引起我注意的是我们错综复杂的智力和意识有多强。我们有偏见，在一个唤起人类智慧的实体面前，我们被诱惑去看意识。这种偏见让人想起一个圆和三个弯曲的线段是如何被立即识别为一张脸的，但这种偏见甚至更深。事实上，意识是一种奇怪的属性:每个个体只能确定自己的意识，没有办法证明其他人不只是像有意识的代理人一样“行动”(臭名昭著的[唯我论](https://en.wikipedia.org/wiki/Solipsism))...

在日常生活中，对我们来说很明显，其他人是有意识的，而不仅仅是机械自动机像有意识的代理人一样行动。谢谢，因为如果我们不相信这些，就不会有社会，只有一群精神病患者。几个世纪以来，哲学家们一直在研究，为什么作为人类，我们更喜欢一种解释而不是另一种。我认为，我们围绕最新自然语言处理模型的讨论显然是朝着一个方向发展的。我们接受了下面的推理路线:
1)我以某种特定的方式行事；
2)别人的行为和我一样；所以他们应该像我一样有意识！

我们所有的社会和道德契约都可以以此为基础。这不是随机推理模式而是一种溯因推理，类似于说:
1)苏格拉底必死；
2)人类终有一死；所以苏格拉底是人类。

众所周知，这是你能使用的最糟糕的推理模式！然而我们整个社会都可以靠它生活。这种绑架是如此基础，如此根深蒂固地铭刻在我们的大脑中，以至于它不能很好地适应新事物:比如一台能够对《悲惨世界》的角色心理进行适当分析*的机器，或者解释禅宗公案。在这种情况下，我们的大脑会退回到一种解释，即我们面对一个有意识的代理人:“它看起来像我，所以它应该是有意识的”。*

我认为，在认识到一个人工智能“看起来像我们”之前，我们都有不同的阈值，需要不同的复杂程度来欺骗我们每个人。这个门槛可能取决于许多不同的参数(人工智能如何在引擎盖下工作的知识可能是其中之一)，明确的是，对于黑人莱莫因和其他人来说，这个门槛已经达到了。必然的结果是，随着稳步发展，我们最终都会被骗。

但是这有那么糟糕吗？除了被骗的失望(没人喜欢那样)，真的有什么区别吗？又一个臭名昭著的问题。行为主义者会说，无论如何，另一个人的意识既不能被证明也不能被观察到，[那么，作为科学家，我们为什么要为这样的概念而烦恼呢](https://en.wikipedia.org/wiki/Logical_behaviorism)？艾伦·图灵在他的“测试”中采用了行为主义的观点:重要的是有人被系统欺骗了，而不是其他。最终，这种方法论的转变导致了对意识概念的彻底否定。

但不去这么极端的眼光，我们来举个好玩的例子:邪恶的 AI。想象一下，这样一个人工智能没有意识但非常复杂，它“完美地”表现得像一个试图征服世界的人工智能！作为一个人工智能，它可以使用它所有的计算能力，并利用网络效应来并行化并最终复制自己，然后成功并获得对人类的完全控制。如果我们知道这个人工智能不是有意识的，而只是一堆松散的矩阵乘法，这会是一种安慰吗？我不确定。

你可以问下面这个问题:如果一个人工智能没有意识，它为什么想要征服世界？为此，我们应该考虑当前的模型是从我们提供的数据**中“训练”出来的，这些数据是从我们的日常经验和互动中提取的。然后，我们希望模型以最大化我们对它们的期望的方式运行。但是，我们如何“期望”一个超级智能的人工智能表现出来呢？由于我们从未真正遇到过这样的人工智能，答案更多地植根于我们的集体无意识，而不是其他任何地方。神话(伊卡洛斯、假人、巴别塔……)、文学(弗兰肯斯坦、浮士德)和流行文化(阿西莫夫的机器人、终结者……)都是这种集体无意识的镜子。每次科学家在做超人的发明…**

![](img/cb14170b5c60053a424c7b9338270052.png)

…结局不太好。

回到当前最先进的 NLP 模型，改进它们使它们变得有意识对我来说并不像是优先考虑的事情。通过意识棱镜来判断他们似乎也不完全合适。对于这样的人工智能来说，还有许多其他的“质的”成就，可以在今天产生更大的影响。例如:

*   **复杂性意识**:当前的模型似乎对我们向它们提出的问题的复杂性不敏感。无论是什么问题(加一加一，还是证明 P 等于或不等于 NP)，回答的时间都是一样的。考虑到这样的网络是如何实现的，这是预料之中的。每一个给定的答案都是以同样的方式产生的:通过网络内唯一的前向传递。一个更好的人工智能应该意识到它试图解决的问题的复杂性，并相应地为每个问题分配适当的资源。
*   **后台处理能力**:为了给每个问题分配适当的资源，我们应该跳出今天强加的问答序列。人工智能应该能够并行处理生成答案所需的处理。其中一些可能需要几天时间来制作，但在此期间，人工智能应该仍然能够回答一些更简单的问题。甚至有些人提到人工智能在更难的问题上保持的后台处理(例如:你在困难的问题上取得进展了吗？我的难题的预计到达时间是什么时候？)
*   **短期到长期记忆转换能力**:当我们与这样的人工智能互动时，它会保持类似于短期记忆的上下文，给人一种持续互动讨论的印象。但是在这个交互窗口关闭后，系统不会保留任何发生过的事情的记忆。简单来说就是“重置”。老实说，如果我们每天重置它的上下文，即使是一个非常有意识的人工智能也会显得完全愚蠢。应该有一种方法将记忆从 AI 短期记忆(即，通过将整个讨论历史作为输入重新注入模型而提供的上下文)迁移到其长期记忆(即，通过梯度下降由模型的训练参数编码)。今天，我们更接近这一点的是由网络的创造者手动完成的不频繁的“模型更新”，可能集成了用户和人工智能之间的一些选定的交互。这并不令人满意，最终的机制应该是平滑的、频繁的和自动化的，以使人工智能看起来更智能。
*   **人际交往技能**:人工智能应该能够在相同的背景下与多人讨论，记住谁是谁。并利用从一次讨论中获得的见解来丰富另一次讨论。

这四点实际上是同一个方向:当前的人工智能系统在时间和空间上都被“沙箱化”了。这从根本上限制了我们从这样的系统中感知的智能。

在这样的背景下，意识真的感觉像是表面的东西。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)