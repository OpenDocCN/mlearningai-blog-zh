<html>
<head>
<title>TwHIN-BERT : A large pre-trained language model for Multilingual Tweets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TwHIN-BERT:用于多语言Tweets的大型预训练语言模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/twhin-bert-a-pre-trained-language-model-for-multilingual-tweets-f651628e61e0?source=collection_archive---------1-----------------------#2022-12-13">https://medium.com/mlearning-ai/twhin-bert-a-pre-trained-language-model-for-multilingual-tweets-f651628e61e0?source=collection_archive---------1-----------------------#2022-12-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/125e31e7016dca82f3df73400ef758c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ierDwl38VFKQhK_pnwjxRw.png"/></div></div></figure><p id="300b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Twitter异构信息网络(TwHIN)接受了100多种不同语言的70亿条推文的训练。该模型已经在多种多语言社交推荐和语义理解任务上进行了评估。策划的TwHIN由大约2亿不同的用户、10亿条推文和超过1000亿条边组成。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jn"><img src="../Images/a178272a89c91af3a018d3bebbf041c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXb4F4gDrzoeojJtJHDclg.png"/></div></div></figure><p id="87dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">图表显示，在构建TwHIN-BERT的过程中有三个步骤。</p><p id="6be0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(1)通过嵌入Twitter异构信息网络来挖掘社交相似的tweet对</p><p id="0b75" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(2)以社会和MLM为共同目标的TwHIN-BERT培训，最后</p><p id="0cbd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(3)在下游任务上微调TwHIN-BERT。</p><p id="9e5c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">据Huggingface Hub称，该模型支持89种语言。泰语也在列表中。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/8f89e61fd5a1a9f33f33310c30c862fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vG4wIY849D30c4W434nmpw.png"/></div></div></figure><p id="695a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有4个基准模型已经在下游任务中进行了性能比较。(不包括<strong class="ir hi"> TwHIN-BERT) </strong></p><p id="d81c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">多语言BERT(mBERT) </strong>是一种广泛使用的预训练语言模型，在通用领域web语料库上进行训练。</p><p id="83da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> BERTweet </strong>是之前在推特上训练的单语语言模型，然而这个模型只在英语推特上训练。</p><p id="f783" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">XLM-T是一个基于XLM-R的多语言语言模型，在Tweet语料库上进行持续训练。</p><p id="159a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> TwHIN-BERT-MLM </strong>使用与<strong class="ir hi"> TwHIN-BERT </strong>相同的语料库进行训练，但这个版本仅在掩蔽语言模型上进行训练。</p><p id="80c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本文中使用了两个主要任务。</p><ol class=""><li id="0798" class="jt ju hh ir b is it iw ix ja jv je jw ji jx jm jy jz ka kb bi translated">社会参与预测</li><li id="6d7a" class="jt ju hh ir b is kc iw kd ja ke je kf ji kg jm jy jz ka kb bi translated">推文分类:已应用数据集，如推文标签预测、英语推文情感分析数据集、英语和西班牙语表情预测数据集、阿拉伯语推文情感数据集、日语推文主题分类数据集、印地语+英语和西班牙语+英语的代码混合推文。</li></ol><p id="4387" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，有两种主要类型的预训练TwHIN-BERT模型(基本模型和大模型),它们与HuggingFace BERT模型兼容。</p><p id="921b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特温伯特基地:(<a class="ae kh" href="https://huggingface.co/Twitter/twhin-bert-base" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/Twitter/twhin-bert-base</a>)</p><p id="201f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特温-伯特-拉奇:(<a class="ae kh" href="https://huggingface.co/Twitter/twhin-bert-large" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/Twitter/twhin-bert-large</a>)</p><p id="790a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了应用模型，您可以选择适合特定任务的模型。这些模型存在一些差异，因为TwHIN-BERT-base包含280M参数，而TwHIN-BERT-large包含550M参数。</p><p id="d1b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">除了广受欢迎的M-BERT和XLM-罗伯塔，TwHIN-BERT可能是多语言任务的最佳选择之一。</p><p id="07ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">更多信息:</strong></p><div class="ki kj ez fb kk kl"><a href="https://arxiv.org/abs/2209.07562" rel="noopener  ugc nofollow" target="_blank"><div class="km ab dw"><div class="kn ab ko cl cj kp"><h2 class="bd hi fi z dy kq ea eb kr ed ef hg bi translated">TwHIN-BERT:一个面向多语言Tweet表示的社交丰富的预训练语言模型</h2><div class="ks l"><h3 class="bd b fi z dy kq ea eb kr ed ef dx translated">我们提出TwHIN-BERT，一个基于流行的社交网络Twitter的领域内数据训练的多语言语言模型…</h3></div><div class="kt l"><p class="bd b fp z dy kq ea eb kr ed ef dx translated">arxiv.org</p></div></div><div class="ku l"><div class="kv l kw kx ky ku kz in kl"/></div></div></a></div><div class="ki kj ez fb kk kl"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="km ab dw"><div class="kn ab ko cl cj kp"><h2 class="bd hi fi z dy kq ea eb kr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ks l"><h3 class="bd b fi z dy kq ea eb kr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kt l"><p class="bd b fp z dy kq ea eb kr ed ef dx translated">medium.com</p></div></div><div class="ku l"><div class="la l kw kx ky ku kz in kl"/></div></div></a></div></div></div>    
</body>
</html>