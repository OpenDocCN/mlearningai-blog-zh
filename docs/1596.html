<html>
<head>
<title>A30: Logistic Regression (Part-2)&gt;&gt; Behind the Scene!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">A30:逻辑回归(下)&gt;&gt;幕后！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a?source=collection_archive---------4-----------------------#2022-01-09">https://medium.com/mlearning-ai/a30-logistic-regression-part-2-behind-the-scene-38a98b70192a?source=collection_archive---------4-----------------------#2022-01-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="e64e" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">概率和赔率、e和自然对数、logit链接函数、对数赔率、决策界限、基线准确度、逻辑回归系数、假设检验、准确度悖论、功效分析、混淆矩阵、真阳性/真阴性、准确度、特异性、精确度、错误/未分类率…！</h2></div><blockquote class="iw ix iy"><p id="2c1c" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh">本文是</em><strong class="jc hi"><em class="hh"/></strong><a class="ae jw" href="https://leanpub.com/u/junaidsqazi" rel="noopener ugc nofollow" target="_blank"><strong class="jc hi"><em class="hh">数据科学从无到有—我能我能</em> </strong> </a> <strong class="jc hi"> <em class="hh">”、</em> </strong> <em class="hh">系列的一篇讲义书。(</em> <a class="ae jw" href="https://leanpub.com/u/junaidsqazi" rel="noopener ugc nofollow" target="_blank"> <em class="hh">)今天点击这里领取你的文案</em> </a> <em class="hh">！)</em></p><p id="919b" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><a class="ae jw" href="https://junaidsqazi.medium.com/a29-logistic-regression-part-1-theory-slides-lecture-8c8fcd3dc98d" rel="noopener"> <em class="hh">点击此处查看之前的文章/讲座“A29:逻辑回归(Part-1) &gt; &gt;理论幻灯片/讲座！!"</em> </a></p></blockquote><blockquote class="jx"><p id="e7d9" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated"><a class="ae jw" href="https://junaidsqazi.medium.com" rel="noopener">💐点击这里关注我的新内容💐</a></p></blockquote><p id="a980" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">⚠️这是一个学习讲座，代码是主观的，为学习目的。</p><p id="0d96" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">✅一个建议:<em class="jb">打开一个新的jupyter笔记本，边读这篇文章边输入代码，边做边学，是的，“请阅读评论，它们非常有用…..!"</em></p><h1 id="f506" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">逻辑回归&gt;&gt;幕后</h1><p id="faa6" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">1.概率和赔率</p><p id="ca5d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">2.e和自然对数——快速回顾</p><p id="93f2" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">3.理解逻辑回归</p><ul class=""><li id="1f26" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">3.1:导言</li><li id="9204" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">3.2:Logit链接功能</li><li id="a17c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">3.3:获取概率</li><li id="42f5" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">3.4:衍生—(可选)</li><li id="092b" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">3.5:从对数优势到概率的转换</li></ul><p id="ea94" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">4.逻辑回归实现</p><ul class=""><li id="af0c" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">4.1:数据及其概述</li><li id="abfc" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">4.2:线性回归与逻辑回归——视觉比较</li><li id="9c0f" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">4.3:决策界限</li><li id="ce4d" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">4.4:系数的解释</li></ul><p id="5576" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">5.模型评估</p><ul class=""><li id="e394" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">5.1:基线精度</li><li id="2c18" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">5.2:混淆矩阵</li><li id="46b1" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">5.3:分类报告</li><li id="710e" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">5.4:更改预测阈值</li></ul><p id="f22a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">6.最后的话</p><p id="503c" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">7.额外材料——供你空闲时阅读</p><ul class=""><li id="b56b" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">7.1:假设检验和混淆矩阵</li><li id="931f" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">7.2:建筑分类报告</li><li id="fb97" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.2.1:准确率和误分类率</li><li id="c42c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;准确性悖论</li><li id="0f10" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.2.2:精确度/阳性预测值</li><li id="6e6f" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.2.3:召回率/敏感度/真阳性率(TPR)</li><li id="22a7" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.2.4:假阳性率(FPR)</li><li id="8e92" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.2.5:特异性/真阴性率(TNR)</li><li id="c27b" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7 . 2 . 6:F1-得分</li><li id="d468" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;评论家</li><li id="e91e" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">7.3:求解贝塔系数</li><li id="1627" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">7.4:几个功能的说明</li><li id="cd90" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.4.1:概率与赔率</li><li id="448d" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.4.2:赔率的对数——对数赔率</li><li id="563f" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">&gt; &gt; &gt; 7.4.3:概率的对数</li><li id="17b2" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">7.5:额外资源</li><li id="f30c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">7.6:统计测试、功效分析和样本量</li></ul><p id="2539" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">让我们从所需的导入开始。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="cee0" class="mj kq hh mf b fi mk ml l mm mn"># We are already familiar with these libraries!<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="ef68" class="mj kq hh mf b fi mo ml l mm mn"># scikit-learn imports<br/>from sklearn.linear_model import LinearRegression, LogisticRegression<br/>from sklearn.preprocessing import StandardScaler</span><span id="f257" class="mj kq hh mf b fi mo ml l mm mn">#Retina display to see better quality images.<br/>%config InlineBackend.figure_format = ‘retina’</span><span id="5b2b" class="mj kq hh mf b fi mo ml l mm mn">from scipy import stats</span><span id="17e1" class="mj kq hh mf b fi mo ml l mm mn"># Lines below are just to ignore warnings<br/>import warnings<br/>warnings.filterwarnings(‘ignore’)</span></pre><h1 id="2278" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">1.概率和赔率</h1><p id="f613" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">在我们继续使用逻辑回归之前，我们必须对这些非常重要的统计概念有清楚的了解。</p><p id="8280" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">🧘🏻‍♂️ <strong class="jc hi"> &gt; &gt;概率&lt; &lt; </strong> 🧘🏻‍♂️</p><p id="9db8" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">概率是描述某个事件在<code class="du mp mq mr mf b">0 (impossible) &amp; 1 (certain)</code>之间发生的可能性。概率越高，事件发生的可能性越大。</p><blockquote class="iw ix iy"><p id="04f1" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">投掷一枚公平的硬币或掷骰子，并期待我们多久会在骰子上得到一个头和某个数字，它只是结果除以总的选项或可能性。</p></blockquote><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ms"><img src="../Images/db0c1ee49478867a9920de209770bd73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DNA1Y8SB2vVMWq6QLwnS1Q.png"/></div></div></figure><ul class=""><li id="877c" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">在公平硬币的情况下，获得正面或反面的概率是相同的，<strong class="jc hi"> 1/2 (0.5或50%的机会)</strong>，同样，对于骰子，获得某个数字的机会是<strong class="jc hi"> 1/6 </strong>。</li></ul><p id="31c4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">🧘🏻‍♂️ <strong class="jc hi"> &gt; &gt;赔率&lt; &lt; </strong> 🧘🏻‍♂️</p><p id="d2b1" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">一个事件的概率代表了:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es na"><img src="../Images/5617baa431eed8007125cfea3cfb123d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0iSGeS0q_jUBvPzdS5rr4Q.png"/></div></div></figure><p id="7ba8" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">例如:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es nb"><img src="../Images/993c286dae6aaa0221d0fa9883c77a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQofKWm0YLBQiJpyCyjiDA.png"/></div></div></figure><p id="f170" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">所以我们可以写:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es nc"><img src="../Images/fb89123d34c5899f8502662bf7a56371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbC3x6rtLriEPQ1eL6260g.png"/></div></div></figure><blockquote class="jx"><p id="a726" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated"><strong class="ak"> <em class="nd">把数字赔率想成一个比率是有帮助的，比如:</em> </strong></p><p id="fff7" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b">1/5</code>的意思是&gt; &gt; <code class="du mp mq mr mf b">1 "three-side"</code>对<code class="du mp mq mr mf b">5 "no-three-sides"</code> <em class="nd">(我们在一个骰子中有1到6个数字共六个面，按所需结果考虑三个)</em></p></blockquote><p id="7bfa" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">这个图片描述(<a class="ae jw" href="https://en.wikipedia.org/wiki/Odds#/media/File:Probability_vs_odds.svg" rel="noopener ugc nofollow" target="_blank">来源</a>)可能有助于理解概率和赔率之间的关系。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es nj"><img src="../Images/1406e660d2b16ed3c7d56e24a3a48b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wIMFC73tsb8xVKo64GSjrg.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx"><em class="nd">𝑝</em> is an event under observation and <em class="nd">𝑞</em> is for all other possible events. (image <a class="ae jw" href="https://en.wikipedia.org/wiki/Odds#/media/File:Probability_vs_odds.svg" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="f48f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;例题—概率和赔率:&lt; &lt; </strong></p><p id="8512" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">掷骰子，掷出1或任何数字:</p><ul class=""><li id="6f7e" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Probability = 1/6</code></li><li id="51b8" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Odds = 1/5</code></li></ul><p id="0ca8" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">偶数掷骰子:</p><ul class=""><li id="fec9" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Probability = 3/6</code></li><li id="65c3" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Odds = 3/3</code></li></ul><p id="95be" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">掷骰子少于5:</p><ul class=""><li id="b5bd" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Probability = 4/6</code></li><li id="cfa7" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Odds = 4/2</code></li></ul><blockquote class="iw ix iy"><p id="3d25" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><a class="ae jw" href="https://www.investopedia.com/articles/investing/042115/betting-basics-fractional-decimal-american-moneyline-odds.asp" rel="noopener ugc nofollow" target="_blank"> <strong class="jc hi">赔率常用于赌博</strong> </a> <strong class="jc hi">，例如</strong> <code class="du mp mq mr mf b"><strong class="jc hi">3/2</strong></code> <strong class="jc hi">的意思是，三胜二负，</strong> <code class="du mp mq mr mf b"><strong class="jc hi">4/1</strong></code> <strong class="jc hi">的意思是四胜一负！在这两种情况下，总共有5个行动。</strong></p></blockquote><blockquote class="jx"><p id="f3cc" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated">概率和赔率以不同的方式代表同一件事。</p></blockquote><p id="6fbd" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">因此，概率也可以表示为赔率，这将有助于理解它们之间的关系。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es no"><img src="../Images/d79773ee51463f3a97f1cbdcd156a87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ldd_A6MCob6IFyGuUYGKQ.png"/></div></div></figure><p id="d3eb" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">让我们创建一个特定事件的概率及其赔率的表格。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es np"><img src="../Images/e75b7f10fa2df44176acdd04b3d79549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DHDADSRIdQnhOKJrhwmNg.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Probabilities and their odds are presented in the table (dataframe) above.</figcaption></figure><p id="cc61" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">如上表所示，当我们有分数比时:</p><ul class=""><li id="b0b4" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">对于<code class="du mp mq mr mf b"><strong class="jc hi">p = 0.25</strong></code> : <code class="du mp mq mr mf b"><strong class="jc hi">odds = 0.333..</strong></code> -为0.333..发生的可能性比不发生的可能性大。</li><li id="088b" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">对于<code class="du mp mq mr mf b"><strong class="jc hi">p = 0.5</strong></code>:<code class="du mp mq mr mf b"><strong class="jc hi">odds = 1</strong></code>——发生的可能性和不发生的可能性一样大。</li><li id="afe6" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">对于<code class="du mp mq mr mf b"><strong class="jc hi">p = 0.75</strong></code>:<code class="du mp mq mr mf b"><strong class="jc hi">odds = 3</strong></code>——发生的可能性是不发生的3倍。</li></ul><h1 id="8544" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">2.e和自然对数——快速回顾</h1><p id="8440" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">🧘🏻<strong class="jc hi"><em class="jb">&gt;&gt;‍♂️&lt;&lt;</em></strong>🧘🏻‍♂️</p><ul class=""><li id="611b" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">e ~ 2.71828.....</code>又称<strong class="jc hi"> <em class="jb">欧拉数</em> </strong>，是<strong class="jc hi">中最重要的无理数</strong> <em class="jb">(像𝜋；两者都不能写成分数)</em>数学上。<code class="du mp mq mr mf b">e</code>是自然日志的底数<code class="du mp mq mr mf b">ln</code> <em class="jb">(回忆学校数学- </em> <a class="ae jw" href="https://www.mathsisfun.com/numbers/e-eulers-number.html" rel="noopener ugc nofollow" target="_blank"> <em class="jb">一个好的链接</em> </a> <em class="jb"> ) </em>。</li></ul><p id="e4d9" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">🧘🏻‍♂️ <strong class="jc hi"> &gt; &gt;自然日志— <em class="jb"> 𝑙𝑛 </em>或<em class="jb"> 𝑙𝑜𝑔_𝑒 &lt; &lt; </em> </strong> 🧘🏻‍♂️</p><ul class=""><li id="9fc0" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">基数为<em class="jb"> 𝑒 </em>的对数是自然对数。<em class="jb"> &lt; &lt; </em> <a class="ae jw" href="https://www.mathsisfun.com/definitions/natural-logarithm.html" rel="noopener ugc nofollow" target="_blank"> <em class="jb">链接好</em></a><em class="jb"/><a class="ae jw" href="https://en.wikipedia.org/wiki/Natural_logarithm" rel="noopener ugc nofollow" target="_blank"><em class="jb">维基</em> </a> <em class="jb"> &gt; &gt; </em></li></ul><blockquote class="iw ix iy"><p id="f44d" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh"> 𝑒 </em>是所有持续增长过程<a class="ae jw" href="https://mathbitsnotebook.com/Algebra2/Exponential/EXExpMoreFunctions.html" rel="noopener ugc nofollow" target="_blank">环节</a>共享的基本增长率，而<em class="hh"> 𝑙𝑛 </em>给出了达到某一增长水平<a class="ae jw" href="https://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/" rel="noopener ugc nofollow" target="_blank">环节</a>所需的时间。</p><p id="ce5c" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">自然日志<em class="hh"> 𝑙𝑛 </em>是<em class="hh">𝑒</em>=&gt;t65】𝑙𝑛(<em class="hh">𝑒^𝑥</em>)=<em class="hh">𝑥</em></p></blockquote><p id="73da" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们试试吧！</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es nq"><img src="../Images/714eebbc06f01622845e6bc641a3c100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xemS7l9FBM-Yq2ecJ-s1gQ.png"/></div></div></figure><p id="2bd8" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">现在，是时候回去用<code class="du mp mq mr mf b">ln(odds)</code>在我们的<code class="du mp mq mr mf b">table_po</code>中添加一个新的专栏了。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es nr"><img src="../Images/4f44e7bda209c5efe02bf5eb1aaf31a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0dNkQlHcWHh6ID_PzzuJg.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">With log-odds transformation, we get the range [−∞,∞]</figcaption></figure><blockquote class="iw ix iy"><p id="446c" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi"><em class="hh">对数几率</em>变换有一个非常重要的性质，我们有区间[∞，∞]。这对于比值比来说是不成立的，它永远不可能是负数。</strong></p></blockquote><ul class=""><li id="af4a" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">&lt;&lt;<a class="ae jw" href="https://stats.stackexchange.com/questions/271511/why-do-we-need-natural-log-of-odds-in-logistic-regression" rel="noopener ugc nofollow" target="_blank">为什么天然原木的对数比…点击此stackexchange链接</a> &gt; &gt;</li></ul><h1 id="a8fd" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">3.理解逻辑回归</h1><p id="00c1" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">好了，现在是时候继续理解逻辑回归了！</p><h2 id="00f3" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">3.1:导言</h2><p id="3a53" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">逻辑回归是<strong class="jc hi">最常用的分类算法</strong>(分类器)之一。<strong class="jc hi"> <em class="jb">逻辑回归估计类别成员的概率</em> </strong>，这实际上是通过<strong class="jc hi">从一种回归模型预测对数几率</strong>来完成的。</p><p id="54f4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">逻辑回归可以推广到多类分类，但是，让我们从<strong class="jc hi"> <em class="jb">二元结果</em> </strong>开始，例如:</p><ul class=""><li id="3bf4" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><strong class="jc hi"> <em class="jb">根据症状预测患者患某种疾病的可能性</em> </strong></li><li id="4a09" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><strong class="jc hi"> <em class="jb">根据学生的成绩和医学院的特点预测学生能否获得住院医师资格</em> </strong></li></ul><p id="8214" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">嗯，你可以想出很多例子……!&gt;&gt;&gt;现实中，大多数时候我们都在处理二元结果！</p><h2 id="3415" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">3.2:logit“链接功能”</h2><p id="bc12" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">我们使用逻辑回归来预测类别成员，而不是连续结果，但是我们仍然可以用制定线性回归的方式来制定逻辑回归。我们会有截距和系数！</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es of"><img src="../Images/66e96e3354174c26c1ad9e71e92e2aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VvHnDlZFoColhyujYqXCsQ.png"/></div></div></figure><p id="59b6" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">在预测器的帮助下，我们获得属于类别1的<em class="jb"> 𝑦 </em>的概率的对数优势值，或者，用另一个名字，为logit函数(它是逻辑函数的逆函数)。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es og"><img src="../Images/eb761d4c76d2f39745ea87be6f41f8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWw3io94CQaRsLk4N-noVA.png"/></div></div></figure><blockquote class="jx"><p id="9f7e" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated">对数赔率可以取任何正值或负值，logit链接的<a class="ae jw" href="https://www.sciencedirect.com/topics/mathematics/logit-link-function" rel="noopener ugc nofollow" target="_blank">目的是取∞和∞之间协变量(因变量或响应值)的线性组合，并将其转换为概率范围，即0和1之间。</a></p></blockquote><h2 id="70a7" class="mj kq hh bd kr ns oh nu kv nv oi nx kz kj oj nz lb kl ok ob ld kn ol od lf oe bi translated">3.3:获取概率</h2><blockquote class="iw ix iy"><p id="3991" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们如何得出概率？用“逻辑”功能反转logit链接功能</p></blockquote><p id="dc5d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">logit的反函数称为<strong class="jc hi">逻辑函数</strong>。</p><p id="c0d9" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">通过反转logit，我们可以显式求解<em class="jb"> 𝑃 </em> ( <em class="jb"> 𝑦 </em> =1):</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es om"><img src="../Images/85100030eedf24d3e841f286e2de68bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vD44FoZH4n77cOLvmt9ubg.png"/></div></div></figure><p id="00d4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">并且<a class="ae jw" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">逻辑函数</a>是sigmoid (S形)函数，则最终方程可以写成:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es on"><img src="../Images/6a3da97c2284c87c509ac6b78e5fa0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Zs94R1xkeNzvbBb_2sxDg.png"/></div></div></figure><h2 id="241d" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">3.4:衍生—(可选)</h2><p id="7a67" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">为了推导我们如何从对数优势中获得概率，让我们设定</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es oo"><img src="../Images/d4c3b7aa617f2f0e6f62653e9d7beaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ikJzuZKDzEQ5bjBQE5_0qg.png"/></div></div></figure><h2 id="aa6d" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">3.5:从对数优势到概率的转换</h2><p id="6b23" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">让我们为这个转换创建一个图，如果我们可视化，它会更容易。把下面的代码抄在你自己的jupyter笔记本上，创造一个情节。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es op"><img src="../Images/7afd987fdf49786efb84c7bebb995183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MkSLwmScjCnvXjDKEG5iuw.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es oq"><img src="../Images/1bdc5e753a495467fcd6f8c37b1c0d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3CJ07scHo4wZ67sAFZM7Q.png"/></div></div></figure><h1 id="6b87" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">5.4:逻辑回归实现</h1><h2 id="fb0f" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">4.1:数据及其概述</h2><p id="6f3a" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">是时候实施我们所学的知识了，看看如何在商业中使用逻辑回归。</p><p id="eb39" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们可以考虑这样一种情况，一所大学必须根据申请人的GRE和/或GPA来筛选入学申请人。该学院提供一些专业，代码在字段列中提供。我们尽量保持简单，只考虑几列:</p><ul class=""><li id="4678" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">gre</code>:申请人的GRE成绩</li><li id="9f3e" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">gpa</code>:申请人的平均绩点</li><li id="6aa5" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">field</code>:学生申请的学习领域</li><li id="1e82" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">admit</code>:二进制1-0结果的目标栏显示学生是否成功</li></ul><p id="5616" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><em class="jb">您可以直接从提供的github链接中读取数据，也可以下载并保存在您的机器上，然后开始工作。</em></p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="0920" class="mj kq hh mf b fi mk ml l mm mn">url='<a class="ae jw" href="https://raw.githubusercontent.com/junaidqazi/DataSets_Practice_ScienceAcademy/master/admissions.csv" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/junaidqazi/DataSets_Practice_ScienceAcademy/master/admissions.csv</a>'</span><span id="8e61" class="mj kq hh mf b fi mo ml l mm mn"># Let's read the data directly from the github <br/>adm = pd.read_csv(url)</span><span id="b2dd" class="mj kq hh mf b fi mo ml l mm mn"># Checking the head <br/>adm.head(2)</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es or"><img src="../Images/735dc3d06e93164c1b5f417c93809423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S08Gqcl7zV07wmo-mIIoCQ.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">This how the data head look like. (output from the above code)</figcaption></figure><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="3588" class="mj kq hh mf b fi mk ml l mm mn">adm.info()</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es os"><img src="../Images/545792e19c04fa00ca7fbc16424c4bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6p3ZFKoUCtNoYHRUIR5EPw.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Overview of the data using info() on data frame</figcaption></figure><p id="424d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们可以看到一些<strong class="jc hi">缺失数据</strong>，这是一个小数据集，我们也可以计算出数字。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="56b1" class="mj kq hh mf b fi mk ml l mm mn"># How much (%) data is missing<br/>adm.isnull().sum()/len(adm)*100 # % of missing data</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ot"><img src="../Images/cbc8810e00fda46e09bfac9dd599b322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RypaQaG9-5tm30mSvRM1xA.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">There is missing data but not much. (output from the code above)</figcaption></figure><p id="d07b" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">嗯，只有一小部分数据丢失，我们可以忽略它！</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="4911" class="mj kq hh mf b fi mk ml l mm mn">adm.dropna(inplace=True) # inplace = True for the permanent change</span></pre><p id="da2a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;让我们根据研究领域&lt; &lt; </strong>计算录取的概率和赔率</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="c7bb" class="mj kq hh mf b fi mk ml l mm mn">adm.field.value_counts() #unique()</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ou"><img src="../Images/da056ff3488847d80ae73cfb8065ba5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8GGJMugU3R7g4wwU_vpXg.png"/></div></div></figure><p id="27b7" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">因此，我们总共有4个字段可供申请人使用，大多数学生都是在代码为2的字段中被录取的。让我们计算概率和录取几率！</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ov"><img src="../Images/188936d95aa0b0d735bdc3cf8e079437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4PU8biutjtD7Tbf3xhkaSg.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ow"><img src="../Images/6bd9aa0dfab2edb07039c5a79485e5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRn-Xdy8IJYiNLJkMsyPfA.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Output from the above code &gt;&gt; probabilities and the odds for admission based on the filed of study.</figcaption></figure><h2 id="2f4a" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">4.2:线性回归与逻辑回归——视觉比较</h2><p id="5c02" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">与线性回归类似，我们需要为逻辑回归创建实例，并在数据集上训练模型。一旦模型被训练，我们就可以获取模型系数、预测概率以及它们的标签。</p><blockquote class="iw ix iy"><p id="cb49" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">先把特性标准化吧。这也很重要，因为Scikit-learn在默认情况下实现逻辑回归时应用了<code class="du mp mq mr mf b">l2</code>正则化。</p></blockquote><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ox"><img src="../Images/ee4f0b9b37714a3e88d163ad820862f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s9HVgoXFol4tHyHFQ8QqbQ.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es oy"><img src="../Images/fb44dd4eafad2e86116517ad51b6e8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJ4lpJlXZnj09JCp4rveNg.png"/></div></div></figure><blockquote class="iw ix iy"><p id="08f9" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们在X中的特征(gpa)是标准化的(0均值1方差)，因此<code class="du mp mq mr mf b"><em class="hh">gpa=0</em></code>表示平均值<code class="du mp mq mr mf b"><em class="hh">gpa</em></code>并且<code class="du mp mq mr mf b"><em class="hh">gpa=1</em></code>表示比平均值大一个标准差的值，平均值为0。</p></blockquote><p id="3f16" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">因此，我们从上述数据框架中的线性回归和逻辑回归模型中获得了预测，让我们将它们绘制出来以进行可视化。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es oz"><img src="../Images/c6622a8a1513326f257f0f395a276adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roSLgvwhtk4ry7fGfbyEYw.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pa"><img src="../Images/34f72e7bd4dec088664ab2eedf8545fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0fPVcJRdBPakbigz-7fNg.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">On left, in red, we got predictions from linear regression &lt;&lt;&gt;&gt; on right, the dark red are predictions from logistic regression — — which one makes sense to you?</figcaption></figure><p id="9311" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">因此，我们可以看到，在左边的线性回归图中，预测没有任何意义。</p><p id="a66e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">右边的图，使用逻辑回归解决问题和类预测有意义。</p><h2 id="c931" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">4.3:决策界限</h2><p id="f887" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb">(赔率、概率、系数、截距和特征)</em> </strong></p><p id="ead4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">让我们看看，对于什么样的<strong class="jc hi"> gpa </strong>值，<strong class="jc hi"> log odds为0 </strong>:</p><p id="ea51" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们可以从下面的等式手动计算</p><p id="b21e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><em class="jb"> &gt; &gt; &gt; &gt; log(赔率)= b_0 + b_1 x </em></p><p id="09e0" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们需要的是<em class="jb"> x </em>的值，其中<em class="jb">log(odds)= 0</em><br/><em class="jb">&gt;&gt;&gt;&gt;0 = b _ 0+b _ 1 x<br/>&gt;&gt;&gt;&gt;-b _ 0 = b _ 1 x<br/>&gt;&gt;&gt;&gt;-b _ 0/b _ 1 = x<br/></em></p><p id="7d65" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">所以<em class="jb"> x </em>就是<em class="jb"> log(odds) = 0 </em>的值</p><p id="24af" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">嗯，我们不需要手动这样做，我们可以从我们训练好的逻辑回归模型<strong class="jc hi"> logR </strong>中抓取<em class="jb"> b_0 </em>和<em class="jb"> b_1 </em>，并计算出<em class="jb"> log(odds) = 0 </em>的决策边界</p><blockquote class="iw ix iy"><p id="8c10" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">请阅读下面代码中的注释并查看输出，您需要花费一些时间来获得完整的理解。</p></blockquote><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pb"><img src="../Images/7c0c71313cf9e58a80c695abcf23dbb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2viZZnNQz97Q7UQUmGHphg.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pc"><img src="../Images/2e0207a00344ad4ed86bd2af77cabcc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63YVDiHFUI1HzD3MT4fM5A.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">So, we got the value of gpa where log(odds) = 0.</figcaption></figure><p id="4351" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">让我们继续，想象边界和数据以及预测。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pd"><img src="../Images/1ea62bbf87d6aa6cfafb138569c313ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlbNMGo7nb-OFMO-it9dIg.png"/></div></div></figure><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pe"><img src="../Images/c996edb0a139dd8c3a971f1236a382d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FnPFm4CQbGB-DOBQDOYGFw.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Green dashed line is the value of gpa where log(odds)=0</figcaption></figure><p id="f6c2" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们可以预测数据的概率，以得到逻辑曲线并绘制在上面的图上。让我们完成它。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="26dc" class="mj kq hh mf b fi mk ml l mm mn"># Generating Data for the curve <br/>x_vals = np.linspace(-10.,10.,3000)<br/>y_pp = logR.predict_proba(x_vals[:, np.newaxis])[:,1]</span><span id="7a76" class="mj kq hh mf b fi mo ml l mm mn">#plotting the probabilities (black cure)<br/>ax.plot(x_vals, y_pp, color=’black’, alpha=0.7, lw=4)</span><span id="9cf4" class="mj kq hh mf b fi mo ml l mm mn"># adding blue line for probability cut-off (0.5)<br/>ax.axhline(0.5, lw=3, color=’blue’, ls=’ — ‘, label=’probability = 0.5')<br/>fig</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pf"><img src="../Images/cb2bd53bdbe09500d6bbaf488c636671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWKBfS8fG66NtTrk8DUjlw.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Please see the labels for respective curves.</figcaption></figure><h2 id="d523" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">4.4:系数的解释</h2><p id="ca23" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">万一你想从训练好的logistic回归模型中看到B0和B1系数值！</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="cf9c" class="mj kq hh mf b fi mk ml l mm mn">print(“The logistic regression beta’s are:”)<br/>print(“beta_1 = {} and beta_0 = {}.”.format(logR.coef_[0][0], logR.intercept_[0]))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pg"><img src="../Images/40a674cb137269bc943225da97de1210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bZqQK5-WNlr3hDfO-4B4w.png"/></div></div></figure><p id="c7e0" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;日志中贝塔系数的含义&lt; &lt; </strong></p><p id="7897" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">系数对对数优势有线性影响(回想一下公式)。</p><ul class=""><li id="222d" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">如果<em class="jb"> 𝛽_ </em> 1是0，那么<em class="jb"> 𝛽_ </em> 0代表一个平均绩点学生被录取的对数几率。</li><li id="a7f9" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><em class="jb"> 𝛽_ </em> 1是重标gpa单位增长对录取几率的影响。</li></ul><p id="9055" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">对数赔率很难解读</strong>。幸运的是，我们可以应用逻辑变换来获得不同<em class="jb"> 𝛽 </em>值下的导纳概率。</p><p id="93ae" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">从上图的曲线中，我们可以看到，平均值的2到3个标准偏差内的<code class="du mp mq mr mf b">gpa</code>值导致入院概率几乎呈线性增加。</p><p id="1015" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">当曲线变得非常平坦时，非常向左或向右的值几乎不会进一步增加或减少接纳的概率(s形曲线)。</p><p id="00d5" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb">逻辑回归系数可以进行指数运算，得到</em> </strong>、<strong class="jc hi"> <em class="jb">的比值比，这样更容易解读这些系数。我们将在下节课中尝试使用泰坦尼克号数据集。与此同时，探索这些链接可能是有用的:</em> </strong></p><ul class=""><li id="7d09" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><a class="ae jw" href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/" rel="noopener ugc nofollow" target="_blank">解释系数—逻辑回归中的奇数比率</a></li><li id="0274" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://stats.stackexchange.com/questions/35013/exponentiated-logistic-regression-coefficient-different-than-odds-ratio" rel="noopener ugc nofollow" target="_blank">对数回归系数求幂</a></li></ul><h1 id="0da7" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">5.模型评估</h1><h2 id="f73e" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">5.1:基线精度</h2><p id="619c" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">基线精度是非常重要的计算，在我们评估训练模型的性能时非常重要。</p><blockquote class="iw ix iy"><p id="6ae8" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi"> <em class="hh">基线精度</em> </strong> <em class="hh">:模型通过简单猜测每次观测的多数类所能达到的精度。</em></p></blockquote><blockquote class="jx"><p id="44f0" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">baseline_accuracy = majority_class_N / total_N</em></strong></code></p></blockquote><p id="ad1f" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">典型的人类猜测倾向于认为在二进制分类问题中，两个类的准确率为50%,机会相等。事实上，如果我们有相同比率的两个类，或者在多类分类问题中，如果我们有构成大约50%标签的多数类，这只是偶然猜测。</p><blockquote class="jx"><p id="02df" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><em class="nd"> &gt; &gt;经验之谈；基线精度绝不能低于50% &lt; &lt; </em></p></blockquote><p id="303d" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">在现实生活中的二元类问题中，大多数时候，数据集并不是真正平衡的，我们的基线准确率高于50%。例如，在100个观察值中，如果70个属于第1类，30个属于第0类，则基线精度为70%。创建一个精度低于基线的模型并不是我们真正想要的！</p><blockquote class="jx"><p id="4fb4" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><em class="nd">如果99%的观察值(极度不平衡的数据)属于第1类，那么预测其中99%正确的模型是随机执行的。高质量的数据很重要，99%准确率的模型可能是最差的模型！</em></p></blockquote><pre class="ph pi pj pk pl me mf mg mh aw mi bi"><span id="e53d" class="mj kq hh mf b fi mk ml l mm mn"># Well, we can easily find the baseline accuracy using value_counts()!<br/>y.value_counts(normalize=True)</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pm"><img src="../Images/1bc37f814e9fb87b769bac81e7516030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwwsQnlf0LhS32m38lg4eA.png"/></div></div></figure><p id="111b" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">因此，在我们的数据集中，多数类是1，有54%的观察值，这是我们工作示例中的基线精度！</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="478b" class="mj kq hh mf b fi mk ml l mm mn">baseline_acc = y.value_counts(normalize=True).values[0]*100<br/>print(“Baseline accuracy is: “, baseline_acc)</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pn"><img src="../Images/3ecb0b3916013e33d707bc9423e34cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xtNwiU2gNSMwPgZHHWbOiQ.png"/></div></div></figure><h2 id="ccca" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">5.2:混淆矩阵</h2><p id="d600" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">回想一下上节课的幻灯片<a class="ae jw" href="https://junaidsqazi.medium.com/a29-logistic-regression-part-1-theory-slides-lecture-8c8fcd3dc98d" rel="noopener">中的混淆矩阵或错误矩阵，让我们首先手动计算混淆矩阵，然后我们将使用using </a><a class="ae jw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>来实现这一目的。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="870e" class="mj kq hh mf b fi mk ml l mm mn"># Manually calculate confusion matrix metrics for our model, we need predictions from the model first. </span><span id="16fd" class="mj kq hh mf b fi mo ml l mm mn">predicted = logR.predict(np.array(X['gpa']).reshape(-1,1))</span><span id="be95" class="mj kq hh mf b fi mo ml l mm mn"># Manually calculating confusion matrix<br/>tn = np.sum((y == 0) &amp; (predicted == 0))<br/>tp = np.sum((y == 1) &amp; (predicted == 1))<br/>fp = np.sum((y == 0) &amp; (predicted == 1))<br/>fn = np.sum((y == 1) &amp; (predicted == 0))<br/>print("tn:", tn) <br/>print("tp:", tp)<br/>print("fp:", fp)<br/>print("fn:", fn)<br/>print("Number of classification errors (fp+fn):", fp+fn)</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es po"><img src="../Images/3dca8375123eaf4fbd646abeba2cab74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THHe56dyANLbN1uPyIYDUQ.png"/></div></div></figure><p id="26c4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">让我们使用scikit-learn来获得混淆矩阵…！</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="871a" class="mj kq hh mf b fi mk ml l mm mn">#Verify from sklearn’s metrics.confusion_matrix<br/># We need this import <br/>from sklearn.metrics import confusion_matrix<br/>print(confusion_matrix(y, predicted))#,labels=[1,0])) # Try labels yourself</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pp"><img src="../Images/0dd5a8b07c792299d54388cb220228e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-sDzcVB_j8lb5Hu4vcMDw.png"/></div></div><figcaption class="nk nl et er es nm nn bd b be z dx">Our confusion matrix using scikit-learn, the numbers are same as computed manually above.</figcaption></figure><h2 id="06fb" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">5.3:分类报告</h2><p id="ea19" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">分类报告有助于诊断分类器的有效性。</p><p id="79c2" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">sci kit-learn '<code class="du mp mq mr mf b"><a class="ae jw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" rel="noopener ugc nofollow" target="_blank">metrics.classification_report</a></code>返回三个非常有用的评估指标的报告；<code class="du mp mq mr mf b"><strong class="jc hi"><em class="jb">precision, recall and f1-score</em></strong></code>两个班(如果有多班问题，则为多个班)上的“T2”是指每个班的观察总数。</p><ul class=""><li id="ec7b" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">每个单独类的0行和1行</li><li id="603a" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">加权平均行，顾名思义，给出了两个类的加权平均。</li></ul><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="a7b9" class="mj kq hh mf b fi mk ml l mm mn">from sklearn.metrics import classification_report</span><span id="a3cf" class="mj kq hh mf b fi mo ml l mm mn">print(classification_report(y, predicted))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pq"><img src="../Images/d819397e29217acd6ad0a7f3677d54e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKCvh_C1lfLGsWKwFyjdOw.png"/></div></div></figure><h2 id="bd29" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">5.4:更改预测阈值</h2><p id="5b64" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">分类器的预测默认为猜测具有最高预测概率的类。这必然导致最高可能的准确度(<strong class="jc hi">只是训练数据的保证！</strong>)。</p><p id="e0f0" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">然而，事实上，最大限度地提高精度并不是我们的最终目标。考虑以下场景:</p><blockquote class="iw ix iy"><p id="5564" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">C <strong class="jc hi"> <em class="hh">癌症检测:</em> </strong> <em class="hh">基于一些医学测量，我们开发了一个分类模型(分类器)来检测一个人是否患有癌性肿瘤。与60%的基线准确度相比，该分类器的准确度为96%。</em></p><p id="cbac" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">&gt; &gt; &gt;我们的分类器性能很好，但是在这种情况下，仅仅最大化准确度会有什么问题呢？</p><p id="5053" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt; &gt;认为回到混乱矩阵的目标应该是在还来得及治疗癌症患者之前。</strong></p></blockquote><h1 id="caec" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">6.最后一句话</h1><p id="b8e6" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">逻辑回归是一个非常受欢迎和有吸引力的机器学习分类器，原因有很多:</p><ul class=""><li id="e7f1" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">与线性回归具有相似的性质</li><li id="0c2e" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">非常快速高效</li><li id="2101" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><strong class="jc hi">系数</strong>是可解释的(尽管有些复杂):它们<strong class="jc hi">表示由于输入变量</strong>导致的对数比值的变化</li><li id="2f21" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">也可以在少量观察中表现良好</li></ul><blockquote class="iw ix iy"><p id="4784" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">一般来说，在与其他竞争监督机器学习算法进行比较时，逻辑回归被认为是低端的。</p></blockquote><h1 id="b971" class="kp kq hh bd kr ks kt ku kv kw kx ky kz in la io lb iq lc ir ld it le iu lf lg bi translated">7.额外材料——供您闲暇时阅读！</h1><p id="c6fc" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">在这一部分，我们将修正一些重要的统计概念。我们还会得到一些有用的图来理解概率、几率、对数几率、统计检验、幂分析等等概念。</p><h2 id="429c" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.1:假设检验与混淆矩阵</h2><p id="c5ee" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">在假设检验的上下文中<code class="du mp mq mr mf b">false positives</code>和<code class="du mp mq mr mf b">false negatives</code>分别被称为<code class="du mp mq mr mf b">Type I</code>和<code class="du mp mq mr mf b">Type II</code>错误。</p><ul class=""><li id="98c8" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><strong class="jc hi">第一类错误是错误拒绝原假设</strong>，而事实上原假设为真。这相当于分类中的假阳性率:&gt; &gt;一个模型将一个观测标注为“真”而实际上它是“假”的比率。</li></ul><p id="c3e4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">I型</strong>误差直接对应p值:<strong class="jc hi">p值是错误拒绝零假设的概率。</strong></p><ul class=""><li id="aca1" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">另一方面，II型错误直接对应于假阴性。假设检验中的第二类错误是接受零假设，而事实上另一个假设是真的。</li></ul><blockquote class="iw ix iy"><p id="ce34" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">统计显著性</strong>和<strong class="jc hi">统计功效</strong>是两个基本概念，你可以从任何基础统计学书籍中读到更多。</p></blockquote><h2 id="b2a3" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.2:建筑分类报告</h2><h2 id="d74f" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated"><strong class="ak"> &gt; &gt; 7.2.1:准确率和误判率&lt; &lt; </strong></h2><blockquote class="jx"><p id="1585" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">accuracy = (tp + tn) / total_population</em></strong></code></p></blockquote><p id="2a89" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">只是猜对的比例，不分阶级。</p><blockquote class="jx"><p id="5957" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">misclassification_rate = (fp + fn) / total_population</em></strong></code></p></blockquote><p id="7742" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">只是一个和准确度的区别。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="1ff2" class="mj kq hh mf b fi mk ml l mm mn">from sklearn.metrics import accuracy_score</span><span id="5366" class="mj kq hh mf b fi mo ml l mm mn">total_population = tp + fp + tn + fn</span><span id="ea6b" class="mj kq hh mf b fi mo ml l mm mn">print(“Manually canculating score: “, <br/> float(tp + tn) / total_population) # manual<br/>print(“scikit-learns accuracy_score module: “, <br/> accuracy_score(y, predicted)) # scikit-learns matrix<br/>print(“model’s score function: “, <br/> logR.score(np.array(X[‘gpa’]).reshape(-1,1),y))<br/>print(“Three options returing the same results.”)</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pr"><img src="../Images/164a2a02fda6d60c85329ac8c9c5816e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eTqCIl1e7wI-YdLrB5SNg.png"/></div></div></figure><p id="e47d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">&gt;&gt;&lt;&lt;</strong><a class="ae jw" href="https://en.wikipedia.org/wiki/Accuracy_paradox" rel="noopener ugc nofollow" target="_blank">维基</a></p><p id="5e7a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">准确度是一个非常直观的指标——我们可以想到一个考试分数，在那里我们得到<code class="du mp mq mr mf b">total_correct/total_attempted</code>。然而，在应用中，精确度通常是一个很差的指标。这有许多原因:</p><ul class=""><li id="b556" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">基线中95%阳性的不平衡数据即使没有预测能力也有95%的准确性。</li><li id="1c22" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">这就是悖论；追求准确性往往意味着预测最普通的类，而不是做最有用的工作。</li></ul><p id="c950" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">按照正确的顺序排列预测比让它们正确更重要。T24】</p><p id="1b6b" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">在许多情况下，我们需要知道一个肯定和否定的确切概率，例如:</p><ul class=""><li id="2ee9" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">来计算预期收益。</li><li id="87a7" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">对临界阳性的观察结果进行分类(决定治疗顺序的紧急程度)。</li></ul><p id="483a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">解决这些问题的一些最有用的指标是:</strong></p><p id="ce9d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;分类精度/误差&lt; &lt; </strong></p><ul class=""><li id="e385" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">分类准确度是正确预测的百分比(越高越好)。</li><li id="0be1" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">分类误差是不正确预测的百分比(越低越好)。</li><li id="df24" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">最容易理解的分类标准。</li></ul><p id="15d4" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;混淆矩阵&lt; &lt; </strong></p><ul class=""><li id="8eee" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">让我们更好地了解分类器的性能。</li><li id="4baa" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">允许我们计算<code class="du mp mq mr mf b">sensitivity, specificity</code>和许多其他指标，这些指标可能比准确性更符合我们的业务目标。</li><li id="7bd8" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><code class="du mp mq mr mf b">Precision</code>和<code class="du mp mq mr mf b">recall</code>有利于平衡误分类代价。</li></ul><p id="542d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">&gt;&gt;【ROC曲线与曲线下面积(AUC)】&lt;&lt;</strong></p><ul class=""><li id="5cc9" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">适用于排序和优先级问题。</li><li id="e601" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><strong class="jc hi"> <em class="jb">允许我们在所有可能的分类阈值</em> </strong>上可视化我们的分类器的性能，从而有助于选择一个阈值<code class="du mp mq mr mf b">appropriately balances sensitivity and specificity</code>。</li><li id="a3b9" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">当存在高类别不平衡时仍然有用(不同于分类准确度/误差)。</li><li id="c7c3" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">当有两个以上的响应类时更难使用(多类-尝试一个对所有！).</li></ul><p id="329e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">&gt;&gt;&lt;&lt;</strong></p><ul class=""><li id="71bf" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">当精确校准的预测概率对您的业务目标很重要时最有用。</li><li id="e1bc" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">期望值计算</li><li id="789c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">按性质分类</li></ul><p id="f08f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">所有这些都可以很容易地用Python计算出来，知道我们在寻找什么是很重要的。</p><h2 id="b14c" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated"><strong class="ak"> &gt; &gt; 7.2.2:精度/阳性预测值&lt; &lt; </strong></h2><blockquote class="jx"><p id="1109" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">precision = tp / (tp + fp)</em></strong></code></p></blockquote><p id="b43f" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">分类器精确<em class="jb">的想法</em>与精确<em class="jb">的想法</em>略有不同。</p><p id="7601" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><em class="jb"> Precision仅是对其正类预测正确性的度量，而accuracy是对所有猜测正确性的度量。</em></p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="f252" class="mj kq hh mf b fi mk ml l mm mn">from sklearn.metrics import precision_score</span><span id="2543" class="mj kq hh mf b fi mo ml l mm mn">print(“Precision using scikit-learn:”, <br/> precision_score(y, predicted))<br/>print(“Precision computed manually: “, <br/> float(tp) / (tp + fp))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ps"><img src="../Images/1a6b3ce7dcef1201cabab652f9489ade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-KnqjC1anNhTv1UCDcdnhA.png"/></div></div></figure><h2 id="05d0" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated"><strong class="ak"> &gt; &gt; 7.2.3:召回率/敏感度/真阳性率(TPR) &lt; &lt; </strong></h2><p id="f7fa" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated"><strong class="jc hi">召回</strong>测量出真实标签为阳性的所有时间，预测标签也为阳性。</p><blockquote class="jx"><p id="1b8f" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">recall = tp / (tp + fn)</em></strong></code></p></blockquote><p id="fa3c" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">这也称为<strong class="jc hi">灵敏度</strong>或<strong class="jc hi">真阳性率</strong>。这三个名字指的是同一个量。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="c53f" class="mj kq hh mf b fi mk ml l mm mn">from sklearn.metrics import recall_score</span><span id="16ab" class="mj kq hh mf b fi mo ml l mm mn">print(“Recall using scikit-learn: “, recall_score(y, predicted))<br/>print(“Recall manual calculations: “, float(tp) / (tp + fn))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pt"><img src="../Images/2ec21ceff33da6d7d072d99254438215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aV16phsyW7aPO3SIp9whw.png"/></div></div></figure><p id="88f2" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><code class="du mp mq mr mf b"><strong class="jc hi"><em class="jb">Precision</em></strong></code>可以看作是<code class="du mp mq mr mf b"><strong class="jc hi"><em class="jb">quality</em></strong></code>的度量，<code class="du mp mq mr mf b"><strong class="jc hi"><em class="jb">recall</em></strong></code>可以看作是<code class="du mp mq mr mf b"><strong class="jc hi"><em class="jb">quantity</em></strong></code>的度量。</p><h2 id="0bb0" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated"><strong class="ak"> &gt; &gt; 7.2.4:假阳性率(FPR) &lt; &lt; </strong></h2><p id="7528" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">或者，假阳性率测量真实标记为阴性的所有时间，预测标记为阳性。</p><blockquote class="jx"><p id="34a8" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">fpr = fp / (tn + fp)</em></strong></code></p></blockquote><pre class="ph pi pj pk pl me mf mg mh aw mi bi"><span id="4ef2" class="mj kq hh mf b fi mk ml l mm mn">##Calculate the FPR using the confusion matrix cells.<br/>print(“FPR: “, float(fp) / (tn + fp))<br/># alternative way to calculate the same<br/>print(“FPR:”, 1 — recall_score(y==0, predicted==0))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pu"><img src="../Images/14e2315d92aff8f2b5b552220cb24e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXCczbFkKagqn9KHmcr2OQ.png"/></div></div></figure><p id="c5bd" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt; 7.2.5:特异性/真阴性率(TNR) &lt; &lt; </strong></p><blockquote class="jx"><p id="a39a" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated"><code class="du mp mq mr mf b"><strong class="ak"><em class="nd">specificity = tn / (tn + fp)</em></strong></code></p></blockquote><p id="0972" class="pw-post-body-paragraph iz ja hh jc b jd kh ii jf jg ki il ji kj kk jl jm kl km jp jq kn ko jt ju jv ha bi translated">回想一下，这是一个姐妹指标，测量的是相同的，但都是积极的。</p><p id="e320" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">测量真实标签为阴性的所有时间，预测标签也为阴性。</p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="2457" class="mj kq hh mf b fi mk ml l mm mn">specificity = float(tn) / (tn + fp)<br/>print(“Manually: Specificity / True Negative Rate (TNR)”, specificity)</span><span id="54fb" class="mj kq hh mf b fi mo ml l mm mn"># alternative way to calculate the same<br/>print(“Specificity / True Negative Rate (TNR) Using recall_score module: “, recall_score(y==0, predicted==0))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pe"><img src="../Images/79ed5b06d6524ed0f1ccf522aae1b14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zxiJtx1sQ1-4J8zM8sXAQA.png"/></div></div></figure><h2 id="5567" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated"><strong class="ak">&gt;&gt;7 . 2 . 6:F1-得分&lt; &lt; </strong></h2><p id="f325" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">F1分数是<strong class="jc hi">精度</strong>和<strong class="jc hi">召回</strong>指标的<a class="ae jw" href="https://en.wikipedia.org/wiki/Harmonic_mean" rel="noopener ugc nofollow" target="_blank">调和平均值</a>。</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es ot"><img src="../Images/a9ad7d3f683a9364e1d0a40763e7b55e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7QcpPQ7Jzn3gZf1elhyUMA.png"/></div></div></figure><p id="e9f7" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><em class="jb">F值的最高可能值是1.0，表示完美的精度和召回率，如果精度或召回率为零，则最低可能值是0。</em></p><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="4b44" class="mj kq hh mf b fi mk ml l mm mn">from sklearn.metrics import f1_score</span><span id="9ff8" class="mj kq hh mf b fi mo ml l mm mn"># Manual calculation<br/>precision_1 = float(tp)/(tp+fp)<br/>recall_1 = float(tp)/(tp+fn)<br/>f1_1 = 2/(1/recall_1+1/precision_1)<br/>print(“F1-Score: “, f1_1)<br/># using scikit-learn<br/>print(“F1-Score: “, f1_score(y==1, predicted==1))</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pv"><img src="../Images/6176e58bdea5705b14a97af395590980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJFzz2dFffETDKtPxcvNMw.png"/></div></div></figure><p id="0f20" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb">将两者融合是有用的。</em> </strong> <em class="jb">通过将两者结合起来，我们可以衡量分类器发现阳性标记观察结果的能力，以及对这些标记上的识别错误的容许程度。</em></p><h2 id="c113" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">&gt;&gt;批评者&lt;&lt;</h2><p id="b02d" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">It is also important to consider that f1-score have been criticized by well know statisticians/scientists.</p><ul class=""><li id="fe56" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">In their paper <a class="ae jw" href="https://link.springer.com/article/10.1007/s11222-017-9746-6" rel="noopener ugc nofollow" target="_blank">《关于使用F-measure评估记录链接算法的说明》</a>，2017年出版，<a class="ae jw" href="https://en.wikipedia.org/wiki/David_Hand_(statistician)" rel="noopener ugc nofollow" target="_blank">戴维·汉德</a>和<a class="ae jw" href="https://researchers.anu.edu.au/researchers/christen-pj#publications" rel="noopener ugc nofollow" target="_blank">彼得克里斯滕</a>批评使用f1-score，因为在等式中对精度和召回同等重要。实际上，不同类型的错误分类会导致不同的代价。在他们的研究中，他们表明f-measure也可以表示为精度和召回率的加权和，这种分配给精度和召回率的相对重要性应该是问题的一个方面。</li></ul><p id="8917" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">其他一些阅读材料也很有用:</p><ul class=""><li id="275b" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">由Davide Chicco和Giuseppe Jurman于2020年发表— <a class="ae jw" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/pdf/12864_2019_Article_6413.pdf" rel="noopener ugc nofollow" target="_blank">马修斯相关系数(MCC)相对于f1-score的优势以及二分类评估的准确性</a>。</li><li id="e4cf" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://www.sciencedirect.com/science/article/abs/pii/0005279575901099?via%3Dihub" rel="noopener ugc nofollow" target="_blank"> MCC最初发表于1975年</a>由生物化学家<a class="ae jw" href="https://en.wikipedia.org/wiki/Brian_Matthews_(biochemist)" rel="noopener ugc nofollow" target="_blank">布莱恩·马修斯</a>测量二元分类的质量。</li><li id="3f1c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">由David Power于2011年— <a class="ae jw" href="https://dspace.flinders.edu.au/xmlui/bitstream/handle/2328/27165/Powers%20Evaluation.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">评价:从精度、召回率和F-measure到ROC、信息量、标记性和相关性</a>。他认为<a class="ae jw" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" rel="noopener ugc nofollow" target="_blank"> kappa </a>和相关性是对称的，而f1-score忽略了真正的负值，这对于不平衡的类是误导的。</li></ul><h2 id="d3cc" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.3:求解贝塔系数</h2><p id="5ccc" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated">逻辑回归最大化了预测概率给出正确类别的可能性。</p><p id="72dc" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">为此，</strong>考虑所有数据点的预测概率的乘积<em class="jb">(这被称为</em> <strong class="jc hi"> <em class="jb">似然函数</em> </strong> <em class="jb">，它是每类预测概率的</em> <strong class="jc hi"> <em class="jb">联合概率分布</em></strong><em class="jb">)</em>:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pw"><img src="../Images/2e06171bb7ec0cf0966190b94e50bc82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfJ7OmJHHJpBmq95G9SEQA.png"/></div></div></figure><p id="80a3" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">选择<em class="jb"> 𝛽 </em>系数，使<strong class="jc hi">函数最大化</strong>。</p><p id="a702" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">最佳情况是</p><ul class=""><li id="ca1e" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">所有第一类观察的预测概率实际上是1</li><li id="c520" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">所有零类观测值的预测概率实际上为零</li></ul><p id="1f25" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><em class="jb">贝塔系数没有线性回归那样的封闭解，贝塔系数是通过优化程序找到的。</em></p><p id="b625" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">如果您对数学特别感兴趣，这两个资源很好:</p><p id="a2e6" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><a class="ae jw" href="http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/" rel="noopener ugc nofollow" target="_blank">一篇关于logistic回归beta系数推导的好博文。</a></p><p id="b594" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><a class="ae jw" href="https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文也是很好的参考。</a></p><h2 id="478e" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.4:几个功能的说明</h2><h2 id="b844" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">&gt; &gt; 7.4.1:概率vs赔率&lt;&lt;</h2><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="596b" class="mj kq hh mf b fi mk ml l mm mn"># Function to calculate the odds of success.<br/>def odds(p):return(p/(1-p))</span><span id="3ebd" class="mj kq hh mf b fi mo ml l mm mn"># Generating a range of probabilities.<br/>probabilities=np.linspace(0.001,0.99,200)</span><span id="618b" class="mj kq hh mf b fi mo ml l mm mn"># Generate list of odds.<br/>odds_list = [odds(proba) for proba in probabilities]</span><span id="23f7" class="mj kq hh mf b fi mo ml l mm mn"># Create figure.<br/>plt.figure(figsize=(18,6))</span><span id="5e66" class="mj kq hh mf b fi mo ml l mm mn"># Plot blue line for odds as probability goes from 0.1% (0.001) to 99% (0.99).<br/>plt.plot(probabilities,odds_list,lw=4,color=’DarkBlue’)</span><span id="130e" class="mj kq hh mf b fi mo ml l mm mn"># Plot red dashed line to visualize odds when probability is 50%.<br/>plt.vlines(0.5,0.0,100,ls=”dashed”,lw=3,color=’DarkRed’)<br/>plt.text(0.33,50.0,”odds(P=0.5) = 1",fontsize=18,color=’DarkRed’)</span><span id="e732" class="mj kq hh mf b fi mo ml l mm mn"># Plot orange dotted line to visualize odds when probability is 66.67%.<br/>plt.vlines(0.6667,0.0,100,ls=” — “,lw=3,color=’DarkOrange’)<br/>plt.text(0.68,50,”odds(P=2/3) = 2",fontsize=18,color=’DarkOrange’)</span><span id="989c" class="mj kq hh mf b fi mo ml l mm mn"># Annotate blue line when probability is 100%.<br/>plt.text(0.84,100,”odds(P=1) = $\infty$”,fontsize=18,color=’DarkBlue’)</span><span id="1181" class="mj kq hh mf b fi mo ml l mm mn"># Title, labels…..1<br/>plt.title(“If the probability of success is 50%, then the odds of success are 1.\n\<br/>If the probability of success is 100%, then the odds of success are $\infty$.”, <br/> ha=’left’,position=(0,1), fontsize=18)<br/>plt.xlabel(“Probability (P)”,fontsize=20)<br/>plt.ylabel(“Odds”,fontsize=20);</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es px"><img src="../Images/5a496a01334ce0d7f87378a22ca3ac6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzxOm81ypvh1C6DwIiktKg.png"/></div></div></figure><h2 id="971d" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">&gt; &gt; 7.4.2:赔率的对数—对数赔率&lt;&lt;</h2><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="3cd2" class="mj kq hh mf b fi mk ml l mm mn"># Creating some positive x-values as suitable for odds<br/>odds = np.linspace(start=0.001, stop=5, num=500) <br/># if start=0 ==&gt; RuntimeWarning: divide by zero encountered in log below <br/>log_odds = np.log(odds)</span><span id="b2de" class="mj kq hh mf b fi mo ml l mm mn">plt.figure(figsize=(18,6)); plt.axhline(y=0, linewidth=3, <br/> color=’DarkRed’, ls=’ — ‘, alpha=0.4)<br/>plt.plot(odds, log_odds, lw=4, color=’DarkBlue’, alpha=0.7)<br/>plt.xlabel(‘odds’, fontsize=16); plt.ylabel(‘log(odds)’, fontsize=16)<br/>plt.title(‘Transformation from odds to log(odds)\n’,fontsize=18);</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es py"><img src="../Images/4cfe4ac085e5fceb37ce69f29144359b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt6qorkhMzG2Kx1NnRsiUg.png"/></div></div></figure><p id="3787" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">log-odds can take any value between −∞ and ∞</p><h2 id="25fc" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">&gt;&gt; 7–4–3)概率的对数&lt;&lt;</h2><pre class="ma mb mc md fd me mf mg mh aw mi bi"><span id="f5bb" class="mj kq hh mf b fi mk ml l mm mn"># Creating some x-values between 0 and 1 as suitable for probabilities<br/>pr = np.linspace(start=0.001, stop=0.999, num=500) # p_min=0, p_max=1<br/>log_it = np.log(pr/(1-pr)) # odds=P/1-P</span><span id="ea04" class="mj kq hh mf b fi mo ml l mm mn">plt.figure(figsize=(18,6)); plt.axhline(y=0, linewidth=3, color=’DarkRed’,<br/> ls=’ — ‘, alpha=0.4)<br/>plt.plot(pr, log_it, lw=4, color=’DarkBlue’, alpha=0.7)<br/>plt.xlabel(‘P’, fontsize=16); plt.ylabel(‘log (P/(1-P))’, fontsize=16)<br/>plt.title(“Transformation from probability to — log (P/(1-P)) — \n”,<br/> fontsize=18);</span></pre><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es pz"><img src="../Images/8b660e279722ee08bb1c3bb42574ba82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URcd6k7zrrUXy7Lk6lrPCw.png"/></div></div></figure><h2 id="c979" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.5: Additional Resources</h2><ul class=""><li id="1b90" class="lm ln hh jc b jd lh jg li kj qa kl qb kn qc jv lr ls lt lu bi translated"><a class="ae jw" href="https://www.youtube.com/watch?v=zAULhNrnuL4&amp;noredirect=1" rel="noopener ugc nofollow" target="_blank">逻辑回归视频演练</a></li><li id="caf8" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="http://www.mc.vanderbilt.edu/gcrc/workshop_files/2004-11-12.pdf" rel="noopener ugc nofollow" target="_blank">逻辑回归预排</a></li><li id="132a" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/ARM/ch5/arsenic_wells_switching.ipynb" rel="noopener ugc nofollow" target="_blank">使用Statsmodels的逻辑回归—孟加拉国的油井转换</a></li><li id="907e" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">0和1不是概率</li><li id="56ff" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://www.statisticshowto.com/probability-and-statistics/null-hypothesis/#state" rel="noopener ugc nofollow" target="_blank">零假设</a></li><li id="a8aa" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/" rel="noopener ugc nofollow" target="_blank">假设检验</a></li><li id="a67d" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/" rel="noopener ugc nofollow" target="_blank">Python中的统计功耗和功耗分析</a></li></ul><h2 id="fdd5" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">&gt; &gt; ROC —提醒一下，我们接下来将介绍:&lt;&lt;</h2><ul class=""><li id="bd0e" class="lm ln hh jc b jd lh jg li kj qa kl qb kn qc jv lr ls lt lu bi translated">A deeper <a class="ae jw" href="http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf" rel="noopener ugc nofollow" target="_blank">ROC简介</a></li><li id="675c" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">互动<a class="ae jw" href="http://www.navan.name/roc/" rel="noopener ugc nofollow" target="_blank">玩ROC曲线</a></li><li id="cb61" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">数据学校在<a class="ae jw" href="http://www.dataschool.io/roc-curves-and-auc-explained/" rel="noopener ugc nofollow" target="_blank"> ROC/AUC </a>上的视频和文字记录</li><li id="71e0" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">观看Rahul Patwari在ROC曲线上的<a class="ae jw" href="https://www.youtube.com/watch?v=21Igj5Pr6u4" rel="noopener ugc nofollow" target="_blank">视频</a></li></ul><h2 id="62da" class="mj kq hh bd kr ns nt nu kv nv nw nx kz kj ny nz lb kl oa ob ld kn oc od lf oe bi translated">7.6:统计测试、功效分析和样本量</h2><p id="682e" class="pw-post-body-paragraph iz ja hh jc b jd lh ii jf jg li il ji kj lj jl jm kl lk jp jq kn ll jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;统计测试&lt; &lt; </strong></p><p id="7997" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb">逻辑回归是少数几个我们可以获得全面统计数据的机器学习模型之一。</em> </strong>通过执行假设检验，我们可以了解我们是否有足够的数据来对单个系数和模型整体做出有力的结论。statsmodels是一个非常有用的Python库，只用几行代码就可以提供这些统计数据。</p><p id="5bfd" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;力量分析&lt; &lt; </strong></p><p id="a090" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">正如我们可能会怀疑的，许多因素会影响逻辑回归结果的统计显著性。估计样本大小以检测给定置信度下给定大小的影响的技术称为功效分析。<a class="ae jw" href="https://en.wikipedia.org/wiki/Power_of_a_test#Description" rel="noopener ugc nofollow" target="_blank">阅读本链接描述的最后一段:</a></p><p id="6b9f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">影响我们最终模型准确性的一些因素有:</p><ul class=""><li id="f35a" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">期望的统计显著性(p值)</li><li id="1d15" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">效果的大小</li><li id="9bda" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">从噪音中分辨出一个小的影响是比较困难的。因此，需要更多的数据！</li><li id="dcb5" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">测量精度</li><li id="1310" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">抽样误差</li><li id="4443" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">在较小的样本中更难检测到影响。</li><li id="8aca" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">试验设计</li></ul><p id="a4fb" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">所以，很多因素，除了样本数量，都有助于产生<strong class="jc hi"><em class="jb"/></strong>的统计功效。因此，没有更全面的分析，很难给出一个绝对数字。</p><p id="8bdc" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">&gt;&gt;II型错误&lt; &lt; </strong></p><p id="f869" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">I型误差对应于<a class="ae jw" href="https://en.wikipedia.org/wiki/Statistical_significance" rel="noopener ugc nofollow" target="_blank"> <strong class="jc hi"> <em class="jb">的概念</em> </strong> </a></p><p id="1da1" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">第二类错误对应于<a class="ae jw" href="https://en.wikipedia.org/wiki/Power_of_a_test" rel="noopener ugc nofollow" target="_blank"> <strong class="jc hi"> <em class="jb">的概念</em> </strong> </a> <strong class="jc hi"> <em class="jb">。</em> </strong></p><p id="78cb" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">测试的力量在于:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es qd"><img src="../Images/017875f05191c8421c679a7b26a0adea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LdBE-o_sjxCgjqAoR84-Hw.png"/></div></div></figure><p id="8055" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">更直观地说，<strong class="jc hi">能力衡量的是我们察觉一种存在的效果的能力。表示避免第二类错误的概率。</strong></p><p id="dc41" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb">统计功效的范围从0到1，随着它的增加，犯第二类错误(错误地未能拒绝零假设)的概率降低。</em> </strong></p><p id="7ae5" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们可以将重要性、功效和错误类型的概念形象化为一个矩阵，与上面的混淆矩阵相同:</p><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es qe"><img src="../Images/e2846631f5d62491c633e696e25166b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Al2Uy8AwRr5FXhAZVGgFw.png"/></div></div></figure><ul class=""><li id="fcc3" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">alpha是任何假设检验中第一类错误的概率——错误地拒绝零假设。</li><li id="5465" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><em class="jb"> beta </em>是任何假设检验中第二类错误的概率——错误地拒绝零假设。</li></ul><p id="ca9a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi"> &gt; &gt;需要多少样品？&lt;T59】T33】</strong></p><p id="7142" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">我们经常问，我们的数据集应该有多大才能得到合理的逻辑回归结果。下面，将介绍一些方法来确定最终模型的精确度。</p><p id="1480" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><strong class="jc hi">经验法则</strong></p><ul class=""><li id="327f" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated"><strong class="jc hi">快速:</strong>总共至少100个样本。每个特征至少10个样本。</li></ul><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es qf"><img src="../Images/2e64ab7f1dd67bc7818f7a8f02fd14c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-KY7MgMKUFGA-HNF1LJeg.png"/></div></div></figure><blockquote class="iw ix iy"><p id="7ded" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">以上两种方法均来自于:&gt; &gt; &gt; &gt;<em class="hh">Long，J. S. (1997)的分类和有限因变量的回归模型</em>。千橡，加州:圣人出版社。</p></blockquote><blockquote class="jx"><p id="efb0" class="jy jz hh bd ka kb kc kd ke kf kg jv dx translated"><a class="ae jw" href="https://junaidsqazi.medium.com" rel="noopener">💐点击这里关注我的新内容💐</a></p><p id="7d01" class="jy jz hh bd ka kb ne nf ng nh ni jv dx translated">🌹坚持练习，提高和增加新技能🌹</p></blockquote><blockquote class="iw ix iy"><p id="849c" class="iz ja jb jc b jd kh ii jf jg ki il ji jj kk jl jm jn km jp jq jr ko jt ju jv ha bi translated"><em class="hh"> ✅🌹💐💐💐🌹✅ </em> <strong class="jc hi"> <em class="hh">请鼓掌并分享&gt; &gt; </em> </strong> <em class="hh">你可以帮助我们接触到正在努力学习这些概念的人。✅🌹💐💐💐🌹✅ </em></p></blockquote><p id="a4df" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated">祝你好运！</p><blockquote class="iw ix iy"><p id="cc60" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh">见</em> <strong class="jc hi"> <em class="hh">下期讲座</em> </strong> <em class="hh">上</em><strong class="jc hi"><em class="hh">“A31:逻辑回归&gt; &gt;死或活&gt; &gt;循序渐进完成机器学习项目！”。</em> </strong> </p><p id="5145" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi"> <em class="hh">注:</em> </strong> <em class="hh">此完整课程包括视频讲座和jupyter笔记本，可通过以下链接获得:</em></p></blockquote><ul class=""><li id="6423" class="lm ln hh jc b jd je jg jh kj lo kl lp kn lq jv lr ls lt lu bi translated">leanpub上的书</li><li id="cb10" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated"><a class="ae jw" href="https://www.skillshare.com/r/user/junaidqazi" rel="noopener ugc nofollow" target="_blank"> <em class="jb"> SkillShare link(新用户两个月免费)</em> </a></li><li id="2ab2" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">YouTube上的免费视频</li><li id="6052" class="lm ln hh jc b jd lv jg lw kj lx kl ly kn lz jv lr ls lt lu bi translated">科学院</li></ul><p id="2393" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji kj jk jl jm kl jo jp jq kn js jt ju jv ha bi translated"><a class="ae jw" href="https://www.linkedin.com/in/jqazi/" rel="noopener ugc nofollow" target="_blank"> <strong class="jc hi">关于朱奈德·卡齐博士:</strong> </a></p><blockquote class="iw ix iy"><p id="1c8e" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><a class="ae jw" href="https://www.linkedin.com/in/jqazi/" rel="noopener ugc nofollow" target="_blank"><strong class="jc hi"><em class="hh">Junaid Qazi</em></strong></a><em class="hh">博士是学科专家，数据科学&amp;机器学习顾问，团队建设者。他是专业发展教练、导师、作家、技术作家和特邀演讲者。</em> <a class="ae jw" href="https://www.linkedin.com/in/jqazi/" rel="noopener ugc nofollow" target="_blank"> <strong class="jc hi"> <em class="hh">卡齐博士</em> </strong> </a> <strong class="jc hi"> <em class="hh">可通过</em></strong><a class="ae jw" href="https://www.linkedin.com/in/jqazi/" rel="noopener ugc nofollow" target="_blank"><strong class="jc hi"><em class="hh">LinkedIn</em></strong></a><strong class="jc hi"><em class="hh">获得咨询项目、技术写作和/或职业发展培训。</em> </strong></p></blockquote><figure class="ma mb mc md fd mt er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es qg"><img src="../Images/c05f0b967099e2e47767a319ccb6ea0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wXO98FfhRZrjcvKTD_KJA.png"/></div></div></figure><div class="qh qi ez fb qj qk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ql ab dw"><div class="qm ab qn cl cj qo"><h2 class="bd hi fi z dy qp ea eb qq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="qr l"><h3 class="bd b fi z dy qp ea eb qq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="qs l"><p class="bd b fp z dy qp ea eb qq ed ef dx translated">medium.com</p></div></div><div class="qt l"><div class="qu l qv qw qx qt qy my qk"/></div></div></a></div></div></div>    
</body>
</html>