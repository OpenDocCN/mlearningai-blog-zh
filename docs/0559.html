<html>
<head>
<title>Attention in the natural language processing realm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理领域中的注意力</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/attention-in-the-natural-language-processing-realm-a124632a8ffd?source=collection_archive---------2-----------------------#2021-05-15">https://medium.com/mlearning-ai/attention-in-the-natural-language-processing-realm-a124632a8ffd?source=collection_archive---------2-----------------------#2021-05-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5eb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jc translated">注意力是一种机制，在这种机制中，我们根据数据对我们的重要性来关注部分数据。比如我们在做情感分类的时候，要多关注情感词。注意力机制在两个方向上帮助我们；第一，通常会导致更好的性能，<br/>第二，分类决策可以更加清晰和可视化，因此我们有一个更易于解释的模型，这是令人感兴趣的。在这篇文章中，我们首先回顾注意力背后的数学原理，然后运行两个有和没有注意力的模型，以便有更好的直觉。</p><p id="972e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图1示出了随时间展开的传统LSTM盒。在每一步，一个单词嵌入(具有d维)通过单元进入，并且一个s维状态输出。输出是一个n*d维的矩阵，其中n是句子长度。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jl"><img src="../Images/5e8dee0a6da8ff89929fd5d68473eaaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*wgw8ieVkSMkrLi5h2GAE-A.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Fig 1. An LSTM cell with its input</figcaption></figure><p id="495c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于分类任务，我们如何使用输出？一种不明智的方式是，我们只保留最后一个状态，删除其他状态(回想一下，最后一个状态包含来自最后一个输入和前一个状态的信息)。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jx"><img src="../Images/35598ea6dad6cb22de9a7ef2fa463eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*Fh4HsavKH0PQmE5lEPs4Fw.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Fig 2. Using the last state of the LSTM</figcaption></figure><p id="b56d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我们可以更明智地行动，并利用注意机制。本质上，我们使用一个可训练向量α来为每个状态提供一个重要性值，然后状态的加权平均值提供我们的输出。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jx"><img src="../Images/52503fd0b810626547c04a77692d3efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*DfykZUvVGy31LZGXZIoEOw.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Fig 3. Attention</figcaption></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jy"><img src="../Images/7d925b966fb57c1fa29330f94c80697c.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*SwH_TJz7lvVLuW_5Y3cS9w.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">final attention-base representation of a sentence (h is the states matrix provided by LSTM)</figcaption></figure><p id="27ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是alpha并不是作为一个简单的向量来训练的，因为如果是这样的话，我们将会有一个固定的步长值，这是没有帮助的。我们计算α如下:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es jz"><img src="../Images/9c6ceead354b4adfa904520c25b9e62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*QKlYWNPZpRsaEJxcmwUg3A.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">alpha calculation</figcaption></figure><p id="d3b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">公式相当简单，只是ut和uw上的一个SoftMax。这些是什么？</p><p id="2f0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个是ht的变换，它被计算为神经网络层:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ke"><img src="../Images/c32955186a44a14b428ef41fa85b5ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*6q7myeudTtxESBNEBdMODw.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">non-linear transformation of the states</figcaption></figure><p id="49bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它为网络提供了更多的能力。第二个是上下文向量，由其他参数学习。将会了解到，这样可以夸耀更重要的单词，因为每个utα权重被计算为该和uw的归一化相似度(点积)。因此，我们现在有一个可学习的非刚性阿尔法，并计算最终的表现。</p><p id="d63e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们使用Keras实现一个情感分类。该模型由嵌入层、LSTM(和第二层中的注意层)、致密层组成。任务是将每条推文分类为正面、负面、中性。我们训练模型60个时期。如果你感兴趣，你可以在下面的笔记本中看到代码:</p><div class="kf kg ez fb kh ki"><a href="https://colab.research.google.com/drive/1yXb3ZqjpFLeeTJ7YzIbkh_sIgLUmxuur?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab dw"><div class="kk ab kl cl cj km"><h2 class="bd hi fi z dy kn ea eb ko ed ef hg bi translated">谷歌联合实验室</h2><div class="kp l"><h3 class="bd b fi z dy kn ea eb ko ed ef dx translated">编辑描述</h3></div><div class="kq l"><p class="bd b fp z dy kn ea eb ko ed ef dx translated">colab.research.google.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw jr ki"/></div></div></a></div><p id="06d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意的话结果会稍微好一点；0.7831对0.7875。</p><p id="562b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们可视化一个样本的alpha值，看看模型如何处理不同的单词(笔记本包括代码)。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es kx"><img src="../Images/893db503eda272a037c393c1f32c582f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdDTDP4xT0LZ21GfZhXLng.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">Fig 4. Visualization of attention alpha for a test sample</figcaption></figure><p id="9d20" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，“可怕”这个词的价值是“最低”的。同样，我们可以看到“no”和“hrs”是更多的否定词。最新的是因为这是一个航空公司情绪数据集，这个词指的是航班延误。</p><p id="bdee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我发挥了我们对文本分类的注意的本质。我计划发表下一篇关于更受关注的高级主题的帖子，例如GAT和Transformers。</p><p id="733c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来源:</p><p id="0675" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于上下文感知情感分析的多线程分层深度模型。信息科学杂志。2021年2月。土井:10.1765865866767</p><p id="c06d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于文档分类的分层注意网络。计算语言学协会北美分会2016年会议论文集:人类语言技术2016年6月(第1480–1489页)。</p></div></div>    
</body>
</html>