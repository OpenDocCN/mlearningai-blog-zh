<html>
<head>
<title>Regularization in Linear Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性模型中的正则化</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/regularization-in-linear-models-b39349dbc055?source=collection_archive---------11-----------------------#2021-03-02">https://medium.com/mlearning-ai/regularization-in-linear-models-b39349dbc055?source=collection_archive---------11-----------------------#2021-03-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4889d15f6c18bf2858e6565533b0e8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOzOOW_cH1AZv9myFIXo_g.jpeg"/></div></div></figure><p id="191f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在机器学习中，正则化是对模型进行的修改，以便我们获得更通用的模型，更好地适应新数据。</p><p id="b446" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归是一种使用直线找出变量之间关系的方法。它试图找到一条最符合数据的线。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jn"><img src="../Images/146830ea4381b9bc08e2dea83ac86190.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*NgINXYHyvye2n4aMk32Dbg.jpeg"/></div></figure><p id="2398" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性回归是发现变量之间关系的简单而快速的方法，但是它试图将噪声/异常值也包含在数据中。这可能导致在预测新数据时，模型不太通用，误差较大。因此，为了推广这个模型，我们将其正则化。</p><p id="9327" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们谈到误差时，我们通常会听到偏差和方差这两个术语。所以，让我们先看看什么是偏差和方差，然后再来看看正则化技术。</p><h1 id="a29d" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">偏差和方差</h1><p id="8aa3" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">当我们通过模型预测数据时，我们得到的预测值可能与实际值有一定的距离(距离是预测值与实际值的偏差)。这个距离就是误差。</p><p id="b50a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们得到的误差是通过将<strong class="ir hi">训练</strong>数据传递给模型得到的，则称之为<strong class="ir hi">偏差</strong>。</p><p id="4317" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们得到的误差是通过将<strong class="ir hi">测试</strong>数据传递给模型得到的，则称为<strong class="ir hi">方差</strong>。</p><p id="b4eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">具有<strong class="ir hi">较小偏差和较小方差</strong>的模型被认为是好模型，因为它能够以较小的误差预测训练和测试数据。</p><h1 id="18a6" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">线性回归中残差平方和</h1><p id="4629" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">残差是预测值和实际值之间的差值。</p><p id="75e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在线性回归中，我们预测一条线来显示变量之间的关系，使得数据点的残差平方和较小。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/5d43deecc913a8d9f3fec65cdde9fa3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*wpIZAALt7yW_SS1FkV34bg.jpeg"/></div></figure><p id="3981" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，绿点是数据点，红线代表残差。给定数据的残差平方和较小的线被视为最佳拟合线。</p><p id="782a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们只有2个训练数据点时，最佳拟合线将是通过这两个点的线，在这种情况下残差平方和为0。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/62fa71c8bd356f96f7f352890cb0bd5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*kPnYFBQBBE1-3BC3vqSfzg.jpeg"/></div></figure><p id="d94f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是当我们用新数据测试这条线时，我们可能会得到新数据的残差平方和很大。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/28cd60b674918aba6eb3d9069509b674.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*MEtjdpjZnSDvD6crmFpWwA.jpeg"/></div></figure><p id="ca2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这意味着该预测线具有高方差。换句话说，这条线过度拟合了模型。</p><h1 id="f963" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">里脊回归</h1><p id="6e35" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">岭回归也称为L2正则化，是一种正则化技术。该模型找到一条不太符合训练数据的线，但减少了与测试数据的误差。这意味着我们给拟合数据的线引入了一个小偏差。但是作为回报，我们得到方差的下降。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/be828ed7517c59ce210976d20e8d51c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*YIs-5KXxUf8QH2y8DMzwVw.jpeg"/></div></figure><p id="d895" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这条新直线称为岭回归线，它比以前的直线(线性模型线)提供了更好的预测，尽管这条线并不最适合训练数据。</p><p id="0796" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正则化模型找到一条线，该线减少了残差平方和加上惩罚。当罚值为<strong class="ir hi">平方斜率之和</strong>时，称为<strong class="ir hi">岭回归</strong>。</p><p id="1027" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，岭回归减少了</p><h2 id="f4be" class="kz jt hh bd ju la lb lc jy ld le lf kc ja lg lh kg je li lj kk ji lk ll ko lm bi translated">残差平方和+λ* ^2(斜率)和</h2><p id="db56" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">这里斜率的平方和是惩罚，λ表示惩罚有多严重。λ的值可以是0到∞(无穷大)之间的任何值。</p><p id="512b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当λ为0时，则</p><p id="6800" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">残差平方和+ <strong class="ir hi"> 0* </strong>(斜率)和^2 =残差平方和</p><p id="328e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，当λ为0时，模型仅尝试减少误差平方和，从而得到线性回归线。</p><p id="a229" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着λ值的增加，预测线的斜率减小。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/3f1eae6be761f1a8cd43d82e5bfb6bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*xy8CyOVmqRXwfBDQy1TJJw.jpeg"/></div></figure><p id="3ba4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着λ的增加，直线的斜率逐渐接近0。因此，当λ较大时，输出变量对输入特征的依赖性降低。为了找到方差最小的λ，我们使用交叉验证技术。</p><h1 id="e9e5" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">套索回归</h1><p id="0dac" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">拉索回归也称为L1正则化，类似于岭回归。它还拟合了一条偏差很小但方差比线性回归小的直线。这是通过增加惩罚来缩小线的斜率，直到我们得到最佳拟合。</p><p id="e2a9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，当正则化模型的惩罚是绝对斜率的和时，它被称为套索回归。</p><p id="f89d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，套索回归减少了</p><h2 id="0504" class="kz jt hh bd ju la lb lc jy ld le lf kc ja lg lh kg je li lj kk ji lk ll ko lm bi translated"><strong class="ak">残差平方和+ λ*(斜率之和|) </strong></h2><p id="758c" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">这里，绝对斜率之和是惩罚，λ表示惩罚有多严重。λ的值可以是0到∞(无穷大)之间的任何值。</p><p id="e4e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当λ为0时，则</p><p id="147e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">残差平方和+ <strong class="ir hi"> 0 </strong> *(斜率之和|) =残差平方和</p><p id="982e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，当λ为0时，该模型试图仅减少给出线性回归线的最小二乘误差。</p><p id="c890" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着λ值的增加，预测线的斜率减小。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/8e4fa449a3ae757eff07c405f9f0e4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*hAv7hDHby7noh3nkxkSSUA.jpeg"/></div></figure><p id="ab0e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，随着λ的增加，直线的斜率变为0。这就是套索和岭回归的主要<strong class="ir hi">区别</strong>。当脊回归将斜率减小到非常接近0 时，套索回归<strong class="ir hi">将其完全减小到0 </strong>。</p><p id="f4ab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于Lasso回归可以将斜率缩小到0，因此它消除了输出变量对某些要素的依赖性。因此，当我们的数据包含许多不重要的特征时，Lasso可以进行特征选择，并使对这些特征的依赖性为0。</p><p id="d473" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">相比之下，当输出变量依赖于所有特征时(也就是说，当所有特征都重要时)，岭回归效果更好。</p><h1 id="9040" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">弹性净回归</h1><p id="22ee" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">当有大量的特征，而我们不知道哪些特征是有用的，哪些是无用的，那么我们就不能在Lasso和Ridge回归之间进行选择。在这种情况下，我们可以使用弹性网回归，它结合了套索和脊。</p><p id="890c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">弹性网回归模型试图找到一条线，减少</p><h2 id="8bc4" class="kz jt hh bd ju la lb lc jy ld le lf kc ja lg lh kg je li lj kk ji lk ll ko lm bi translated"><strong class="ak">残差平方和+ λ1*(斜率之和|)+λ2 * ^2(斜率)之和</strong></h2><p id="4dca" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">λ1和λ2的值不同。λ1代表套索，λ2代表山脊。为了获得λ1和λ2的最佳值，我们对不同的值组合使用交叉验证技术。</p><p id="000e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当λ1、λ2都为0时，我们得到线性回归线(仅拟合最小残差平方和的线)。</p><p id="88ff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当只有λ1为0时，我们得到岭回归。当只有λ2为0时，我们得到套索回归。</p><h1 id="b37a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="9b2e" class="pw-post-body-paragraph ip iq hh ir b is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ku jk jl jm ha bi translated">模型的正则化处理过拟合问题。正则化有很多技巧。选择最佳的正则化方法取决于特征的有用性。如果所有的特征都很重要，那么就选择岭回归。如果只有一些要素对输出重要，则使用套索回归。当我们有大量的特征，并且不知道它们的有用性时，我们选择弹性网络回归。</p><p id="ec63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lo">原载于https://www.numpyninja.com</em><a class="ae lp" href="https://www.numpyninja.com/post/regularization-in-linear-models" rel="noopener ugc nofollow" target="_blank">T5</a><em class="lo">2021年3月2日</em></p></div></div>    
</body>
</html>