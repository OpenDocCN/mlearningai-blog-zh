<html>
<head>
<title>Eye gaze estimation using a webcam. 100 lines of code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用网络摄像头进行眼睛注视估计。100行代码</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/eye-gaze-estimation-using-a-webcam-in-100-lines-of-code-570d4683fe23?source=collection_archive---------2-----------------------#2022-01-23">https://medium.com/mlearning-ai/eye-gaze-estimation-using-a-webcam-in-100-lines-of-code-570d4683fe23?source=collection_archive---------2-----------------------#2022-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/64e8e8465850abe5110e9572da29ec0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJZqOeIkFIfMqILRfFlOCQ.jpeg"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx">Taken from <a class="ae hu" href="https://unsplash.com/@introspectivedsgn" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@introspectivedsgn</a></figcaption></figure><div class=""/><p id="d8ec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi js translated">让我们来看看下面的场景，你坐在图书馆里，你刚刚看到最漂亮的女人坐在图书馆的另一边。还有<strong class="iw hy">哎呀，</strong>她发现你在盯着她看。她估计你的目光注视着她，你注意到她通过理解她的目光是针对你而抓住了你。</p><blockquote class="kb"><p id="94b0" class="kc kd hx bd ke kf kg kh ki kj kk jr dx translated"><strong class="ak">眼睛注视点</strong>:一个人的眼睛聚焦的点</p></blockquote><p id="b994" class="pw-post-body-paragraph iu iv hx iw b ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn kp jp jq jr ha bi translated">就像我们神奇的大脑毫不费力地完成的许多任务一样，这是一个“教”计算机的难题，因为我们需要执行几个困难的任务:</p><ul class=""><li id="6b88" class="kq kr hx iw b ix iy jb jc jf ks jj kt jn ku jr kv kw kx ky bi translated">人脸识别</li><li id="f882" class="kq kr hx iw b ix kz jb la jf lb jj lc jn ld jr kv kw kx ky bi translated">眼睛识别和瞳孔定位</li><li id="c761" class="kq kr hx iw b ix kz jb la jf lb jj lc jn ld jr kv kw kx ky bi translated">确定头部和眼睛的3D位置</li></ul><p id="47ed" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">商用凝视跟踪器有各种形状和尺寸。从眼镜到屏幕基础解决方案。尽管这些产品非常精确，但它们使用专有的软件和硬件，而且非常昂贵。</p><h1 id="4d1e" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">让我们开始建造我们的凝视跟踪器</h1><p id="6154" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">为了让这个博客保持一个合理的长度，我们将建立一个基本形式的视线跟踪。做了一些粗略的估计。而且我们不会确定确切的凝视点，而是凝视方向。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mh"><img src="../Images/0d83dabed8763df9b9c76ee3f2cf9483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Fa-EiZDmJSi82uOb3Flrhg.gif"/></div><figcaption class="hq hr et er es hs ht bd b be z dx">Gaze is relative to the camera, and I'm sitting under the camera</figcaption></figure><p id="f01a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">人脸识别和瞳孔定位</strong></p><p id="e17f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于这个任务，我们将使用<a class="ae hu" href="https://google.github.io/mediapipe/solutions/face_mesh.html" rel="noopener ugc nofollow" target="_blank"> MediaPipe </a>，这是一个由谷歌开发的惊人的深度学习框架，它将使用很少的资源，实时为我们提供468个2D人脸地标。</p><p id="715b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看一些代码:</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="c53f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里没有什么特别的，在第27行，我们将当前帧和从mediapipe框架获得的标志点传递给我们的凝视函数，这就是所有的乐趣所在。</p><h1 id="163e" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">3D里的2D？</strong></h1><p id="dd8e" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">视线追踪是一个3D问题，但我们在标题中说我们只使用了一个简单的网络摄像头，这怎么可能呢？</p><p id="c017" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用一些极客魔术(线性代数)来实现它。首先，让我们了解一下我们的相机是如何“看”这个世界的。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mh"><img src="../Images/87dd3d1306ae0284361c80c253fbcef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*YrobljGQ6gjUl6-7M8QNiw.jpeg"/></div><figcaption class="hq hr et er es hs ht bd b be z dx">Image from <a class="ae hu" href="https://docs.opencv.org/4.x/dc/d2c/tutorial_real_time_pose.htmlhttps://docs.opencv.org/4.x/dc/d2c/tutorial_real_time_pose.html" rel="noopener ugc nofollow" target="_blank">OpenCV docs</a></figcaption></figure><p id="9457" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你在屏幕上看到的2D图像用蓝色表示，3D世界用世界坐标系表示。它们之间有什么联系？你问。我们如何从2D图像中绘制3D世界地图，或者至少得到一个粗略的估计？我们来想办法吧！</p><h2 id="acd9" class="mo lf hx bd lg mp mq mr lk ms mt mu lo jf mv mw ls jj mx my lw jn mz na ma nb bi translated">我们都一样</h2><p id="561b" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">我们人类比我们想象的更相似，我们可以获得人脸的通用3D模型，这将是对大多数人口的3D比例的良好估计。</p><p id="105f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们使用这样一个模型来定义一个3D坐标系统，我们将把鼻尖设置为坐标系统的原点，并且相对于此我们将定义另外5个点，如下所示:</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="bb25" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在我们有了从mediapipe获得的6个2D点，以及我们定义的世界坐标系中相应的3D点。我们的目标是通过使用我们的2D图像来了解这些点的3D位置的变化。怎么才能做到呢？</p><h2 id="7987" class="mo lf hx bd lg mp mq mr lk ms mt mu lo jf mv mw ls jj mx my lw jn mz na ma nb bi translated">针孔摄像机模型来拯救</h2><p id="381a" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated"><a class="ae hu" href="https://en.wikipedia.org/wiki/Pinhole_camera_model" rel="noopener ugc nofollow" target="_blank">针孔摄像机模型</a>是一个数学模型，描述了3D世界中的点和它们在2D图像平面上的投影之间的关系。从该模型中，我们将推导出以下等式:</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nc"><img src="../Images/43c424da2c2f63863c8b66a8fd407e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFoJ3Nbi2CYcczUwjE6WeA.jpeg"/></div></div></figure><p id="c009" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用这个等式，我们可以获得将3D点投影到图像2D图像平面的变换。但是我们能解决吗？嗯，至少不是通过简单的代数工具，但是不要担心，这就是OpenCV用<a class="ae hu" href="https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d" rel="noopener ugc nofollow" target="_blank"> solvePnP </a>函数来拯救的地方，查看链接获得更深入的解释。</p><p id="5b68" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将获得6个图像点和相应的3D模型点，并将它们传递给solvepnp函数。我们将得到一个旋转和平移向量，从而得到一个变换，帮助我们将一个点从3D世界点投影到2D平面。</p><blockquote class="nd ne nf"><p id="b5c2" class="iu iv ng iw b ix iy iz ja jb jc jd je nh jg jh ji ni jk jl jm nj jo jp jq jr ha bi translated">点击<a class="ae hu" href="https://learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/" rel="noopener ugc nofollow" target="_blank">这里</a>学习如何估算相机矩阵，或者<a class="ae hu" href="https://docs.opencv.org/3.4/dc/dbb/tutorial_py_calibration.html" rel="noopener ugc nofollow" target="_blank">这里</a>学习如何校准你自己的相机。</p></blockquote><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="d56f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用我们的新变换，我们可以从3D空间中选取一个点，并将其投影到2D图像平面。因此，我们将得到这个3D点在空间中指向哪里的概念。这就是点(0，0，150)的样子。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mh"><img src="../Images/633aaefa0ead33031154f6c4b7433d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*V-IDGa2LPIWgWDKcPXUdqQ.gif"/></div></figure><h1 id="96a9" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">来自3D的2D</strong></h1><p id="c0f2" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">现在我们将瞳孔2D图像坐标投影到我们的三维模型坐标。与我们在头部姿势估计部分所做的正好相反。</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="e3d2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如代码片段所示，我们将使用OpenCV estimateAffline3D函数。这个函数使用了我们讨论过的针孔摄像机模型的相同原理。它采用两组3D点，并返回第一组点和第二组点之间的转换。但是等等，我们的图像点是2D，这怎么可能？我们将获取图像点(x，y)并将其作为(x，y，0)传递，因此将获得图像坐标到模型坐标之间的转换。利用这种方法，我们可以从mediapipe获取的2D图像点得到瞳孔的三维模型点。</p><blockquote class="nd ne nf"><p id="f9b1" class="iu iv ng iw b ix iy iz ja jb jc jd je nh jg jh ji ni jk jl jm nj jo jp jq jr ha bi translated">注意:这不是一个非常准确的估计</p></blockquote><figure class="mi mj mk ml fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nk"><img src="../Images/3b2bc629f670a5295e4e3cd992c23e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xtfS5RHzn8hAIc_-YQgosQ.jpeg"/></div></div></figure><p id="a27d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我没有告诉你，但是如果你看上面的第二个代码片段，你可以看到我们有眼睛中心模型点(3D)，我们刚刚使用estimateAffline3D获取了瞳孔3D模型点。现在要找到凝视的方向，我们需要解决这个线平面相交的问题，如上图所述。我们试图寻找的点用<strong class="iw hy"> <em class="ng"> S </em> </strong>表示。让我们把这个点投影到2D平面上。</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><blockquote class="nd ne nf"><p id="76d9" class="iu iv ng iw b ix iy iz ja jb jc jd je nh jg jh ji ni jk jl jm nj jo jp jq jr ha bi translated">注意:在第5行，我们使用了“神奇”的数字10，这是因为我们不知道主体离相机的距离。因此在图中由t表示的瞳孔到照相机之间的距离是未知的</p></blockquote><h1 id="e35a" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">完事了吗？</h1><p id="e8bf" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">还没有。现在我们需要考虑头部运动，这样，我们的视线跟踪器将对头部运动有弹性。让我们从一开始就使用我们的头部姿态估计。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nl"><img src="../Images/04630d2dc4a5d066df3436081c17e034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M8Fo70UQb9zfYbOwJt4v1w.jpeg"/></div></div></figure><p id="79df" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">瞳孔的2D位置由点<strong class="iw hy"> <em class="ng"> p </em> </strong>表示，点<strong class="iw hy"> <em class="ng"> g </em> </strong>是凝视+头部旋转投影，点<strong class="iw hy"> <em class="ng"> h </em> </strong>是头部姿态投影。现在，为了获得清晰的凝视信息，我们从矢量<strong class="iw hy"> <em class="ng"> A </em> </strong>中子构造矢量<strong class="iw hy"> <em class="ng"> B </em> </strong>。</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mm mn l"/></div></figure><blockquote class="nd ne nf"><p id="0431" class="iu iv ng iw b ix iy iz ja jb jc jd je nh jg jh ji ni jk jl jm nj jo jp jq jr ha bi translated">在第5行中，我们使用了神奇的数字40，原因与我们在上面的代码片段中使用10的原因相同。</p></blockquote><h1 id="bd16" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">剧终</h1><p id="3bbb" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">我们结束了，至少现在。你可以在我的Github页面上看到完整的代码，并在你的机器上运行它。</p><div class="hg hh ez fb hi nm"><a href="https://github.com/amitt1236/Gaze_estimation" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hy fi z dy nr ea eb ns ed ef hw bi translated">GitHub-amitt 1236/Gaze _ estimation</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">github.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ho nm"/></div></div></a></div><h1 id="32ec" class="le lf hx bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">但是我们真的完了吗？</h1><p id="3da8" class="pw-post-body-paragraph iu iv hx iw b ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn mg jp jq jr ha bi translated">嗯，我们抄了一些近路，以保持这个博客在一个合理的长度。因此，我们可以改变一些事情来提高准确性:</p><ol class=""><li id="abcb" class="kq kr hx iw b ix iy jb jc jf ks jj kt jn ku jr ob kw kx ky bi translated">正确校准相机，不要使用估计值。</li><li id="efce" class="kq kr hx iw b ix kz jb la jf lb jj lc jn ld jr ob kw kx ky bi translated">用双眼计算两个位置之间的平均值。(我们只用了左眼)</li><li id="3c6d" class="kq kr hx iw b ix kz jb la jf lb jj lc jn ld jr ob kw kx ky bi translated">我们使用estimateAffine3D方法将2d瞳孔位置投影到3D空间，但这不是一个精确的估计。我们可以使用眼睛结构和眼窝中的瞳孔位置来推断瞳孔的3d位置。</li><li id="677e" class="kq kr hx iw b ix kz jb la jf lb jj lc jn ld jr ob kw kx ky bi translated">我们完全忽略了拍摄对象与摄像机的距离。因此，我们只有一个注视方向，而没有一个注视点。这可能是最重要的部分，但也是最复杂的部分。</li></ol><p id="ca95" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">只需做一点工作，您就可以实现您的解决方案，并将其用于您的特定需求。我会很高兴收到你的来信。你想看第二部分吗？它应该包括什么？或许我们可以更深入地探讨其中一个主题？</p><p id="7f96" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你看到任何错误，不要犹豫，立即联系我们。</p><div class="hg hh ez fb hi nm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hy fi z dy nr ea eb ns ed ef hw bi translated">Mlearning.ai提交建议</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">medium.com</p></div></div><div class="nv l"><div class="oc l nx ny nz nv oa ho nm"/></div></div></a></div></div></div>    
</body>
</html>