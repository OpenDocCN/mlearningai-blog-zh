<html>
<head>
<title>Multi-Layer Neural Networks -In Depth!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多层神经网络-深入！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/all-you-have-to-know-about-multi-layer-neural-networks-9ddcc097576c?source=collection_archive---------7-----------------------#2022-02-06">https://medium.com/mlearning-ai/all-you-have-to-know-about-multi-layer-neural-networks-9ddcc097576c?source=collection_archive---------7-----------------------#2022-02-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="857f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在此我们将了解:</strong></p><ul class=""><li id="5179" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">多层神经网络</strong></li><li id="857e" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi">梯度下降优化器</strong></li><li id="2d81" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi">链式法则</strong></li><li id="ca5f" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi">乙状结肠功能</strong></li><li id="5296" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi">消失渐变问题</strong></li></ul><p id="cc70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在多层神经网络中，可以有n个隐藏层。神经网络中的每个神经元都有各自的权重。在这个多层神经网络中，它有一个输入层、三个隐藏层和一个输出层。X1，X2，X3，X4是输入特征。传递给第一隐藏层的第一个神经元的权重被表示为W11 1。这样就命名了所有神经元的权重。</strong></p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/b327f05dc3605761fe8cd5fa48d06605.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*L2MJuygvBz-b6ton"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">Multi-layer Neural Network</figcaption></figure><h1 id="d3a1" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">多层神经网络</strong></h1><p id="e8ee" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi"> O11是第一个隐层的第一个神经元的输出，这里用的公式是</strong> <a class="ae lf" href="https://snega-s.medium.com/training-of-neural-networks-back-propagation-7047d6ffbdc4" rel="noopener"> <strong class="ig hi">神经网络基础</strong> </a> <strong class="ig hi">。这样，所有的神经元向前传播并预测y-hat..然后计算损失函数，如果损失函数高，则使用优化器反向传播。</strong></p><h1 id="8b2b" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">梯度下降优化器</h1><p id="fde3" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi">优化器用于最小化神经网络的损失函数。其中一个优化器是梯度下降优化器。我们知道的权重更新公式是</strong></p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es lg"><img src="../Images/d91623ca7254c9d84bfd1ad8d215fb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*yKZyPfSE55sxh8Hg"/></div></figure><p id="7335" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数相对于旧权重的导数这一术语意味着我们将梯度下降权重，换句话说，权重将收敛直到它达到全局最小值。微分的原因是什么，但我们要找到权重的斜率。</p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/ab0a0117edbaaf365631dea8ddb0865f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*AnfQq2_f0YPBQBqN"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">Gradient Descent Graph</figcaption></figure><h1 id="3b9b" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">梯度下降图</strong></h1><p id="797d" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi">例如，如果w11’1处于负斜率，则Wnew值变为正，因此权重开始通过x轴增加，并通过y轴减少，并达到全局最小值。如果W11 1处于正斜率，Wnew值变得非常小，并开始减小。以这种方式，权重减小，直到它达到全局最小值。直到它在反向传播中达到全局最小值。</strong></p><p id="e353" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">这就是梯度下降优化器的工作方式</strong>。</p><p id="986d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">正如我们所知，优化器反向传播并更新权重。在多层神经网络中，最终输出受到每个神经元的所有输出的影响。在多层神经网络中，输出O31受到所有先前输出的影响。由于每个输出都影响后面的输出，因此权重更新的导数遵循链式规则。</strong></p><h1 id="fa05" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">重量上升</h1><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/6af81779fe9e6b50489eb8148418f93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*44nu8-mqJRXgaZtf"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">weight updation</figcaption></figure><p id="13e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数相对于旧权重的导数是它将影响的所有输出的微分链。</p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/fce24c3933b43d7e9033f720ea8e7754.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*jRtCEo2zckC8Y7Lx"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">multi-layer neural network</figcaption></figure><h1 id="5cae" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">链式法则</strong></h1><p id="4123" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi">在反向传播中，如果权重w21’3要被更新，它可以被写成:</strong></p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/6a4d9c5da8cb9a1c5368dfd7862a30ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*_YD9_7hNXwQYwuA7"/></div></figure><p id="f1ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">权重w21’3对输出O31有影响，因此找到损失函数相对于输出O31的导数，并与输出相对于将要更新的权重的导数相乘。以这种方式，在权重更新期间遵循链式法则</strong></p><h1 id="5f31" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">消失渐变问题</strong></h1><p id="7659" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi">其中一个激活函数是sigmoid函数，其函数值在0到1之间。如果输出低于0.5，则值为0，高于0.5，则值为1。因此，大输出在0到1之间收敛。这个sigmoid函数的导数范围在0到0.25之间。</strong></p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es jq"><img src="../Images/e59d70f6653cfab918736b2e458c8efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*2MQTskD6733TFG4H"/></div><figcaption class="jy jz et er es ka kb bd b be z dx">sigmoid function</figcaption></figure><p id="a29b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果神经网络具有许多层，例如50层，则应该计算每层中神经元的所有输出的衍生链。在上述神经网络中，如果为w11’2更新权重，则它可以写成</strong></p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es lh"><img src="../Images/c9ef66734b8931c88b469e80f74c7ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/0*mOVOr182MD_-Jf3s"/></div></figure><p id="a53f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">由于sigmoid的导数范围在0到0.5之间，所以O31相对于O21的导数将非常小。当小数值相乘时，结果是一个小数字。因此权重Wnew将近似等于Wold，并且权重需要很长时间来收敛并达到全局最小值。这就是所谓的消失梯度问题。这可以通过使用其他激活函数来解决，例如Leaky relu。</strong></p><h1 id="6297" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">结论</strong></h1><p id="8d50" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated"><strong class="ig hi">因此，我们讨论了多层神经网络、梯度下降优化器、链规则、消失梯度问题以及如何使用激活函数来解决它。</strong></p><p id="3cc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae lf" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">https://medium . com/mlearning-ai/mlearning-ai-submission-suggestions-b 51e 2b 130 bfb</a></p><p id="7278" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">🟠 <a class="ae lf" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ig hi">成为作家</strong> </a></p></div></div>    
</body>
</html>