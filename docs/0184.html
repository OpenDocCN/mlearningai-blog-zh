<html>
<head>
<title>Fast Oriented Text Spotting with a Unified Network (FOTS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用统一网络快速定位文本(FOTS)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fast-oriented-text-spotting-with-a-unified-network-fots-e200917ea0aa?source=collection_archive---------0-----------------------#2021-02-28">https://medium.com/mlearning-ai/fast-oriented-text-spotting-with-a-unified-network-fots-e200917ea0aa?source=collection_archive---------0-----------------------#2021-02-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3d87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jc translated">从图像中检测和识别文本(也称为文本定位)是一个非常有用和具有挑战性的问题，因为它在文档扫描、机器人导航和图像检索等领域有实际应用，所以深度学习研究人员多年来一直在研究这个问题。迄今为止，几乎所有的方法都包括两个独立的阶段:1)文本检测2)文本识别。文本检测只是找出文本在给定图像中的位置，根据这些结果，文本识别实际上从文本中识别字符。由于这两个阶段，需要训练两个独立的模型，因此预测时间有点长。由于测试时间较长，这些模型不适合实时应用。与此相反，FOTS通过同时检测和识别文本，使用统一的端到端可训练模型/网络来解决这个两阶段问题。它在文本检测和识别任务之间使用共享的卷积特征，学习更多的通用特征并改进测试时间，使得它可以在实时应用中有用，例如从较高FPS的视频流进行OCR。FOTS还改进了对具有对齐/旋转文本的场景的文本检测，因为它有一个名为“RoIRotate”(感兴趣区域旋转)的特殊组件，该组件通过保持纵横比不变来旋转对齐的文本，然后应用文本识别。</p><h1 id="6e68" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">概述</h1><ol class=""><li id="dd3b" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated">商业问题</li><li id="5bb5" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">ML问题公式化</li><li id="c0ff" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">数据来源和概述</li><li id="7a28" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">探索性数据分析</em></li><li id="c54c" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">数据预处理/地面实况生成</em></li><li id="ce36" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">造型</em></li><li id="1afb" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">损失计算</em></li><li id="f058" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">结果</li><li id="715f" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">结论</em></li><li id="d432" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated"><em class="kz">未来工作</em></li><li id="1bba" class="kj kk hh ig b ih ku il kv ip kw it kx ix ky jb kq kr ks kt bi translated">参考</li></ol><p id="5f24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我的Github库，包含了全部代码:<a class="ae la" href="https://github.com/Kaushal28/FOTS-PyTorch" rel="noopener ugc nofollow" target="_blank">https://github.com/Kaushal28/FOTS-PyTorch</a></p><h1 id="73a1" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">1.商业问题</h1><p id="8857" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">从自然图像中读取文本在许多领域非常有用，如文档分析、场景理解、机器人导航、图像检索和自动驾驶汽车。这也是最具挑战性的任务之一，因为现实生活中的图像文本有不同的字体、大小/比例和对齐方式。这些应用中的一些还要求不仅准确而且更快地从图像中检测和识别文本(从视频流中识别)，这使得文本识别问题更加具有挑战性。</p><h1 id="c2bb" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">2.ML问题公式化</h1><p id="e368" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">从自然图像中提取文本的问题可以表述为两个阶段的过程:1)文本检测/定位2)文本识别。文本检测可以进一步公式化为边界框回归和文本的每像素分类(无论该像素是否是文本的一部分)。FOTS结合了这两个阶段，并允许训练一个端到端的模型，用于准确的文本检测和识别。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es le"><img src="../Images/b8452873737308493de2f5d8f981bf1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*h8j7ef6nZyTFVR0OfvAcWg.png"/></div></figure><h1 id="2629" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">3.数据来源和概述</h1><p id="619b" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">为了训练端到端FOTS模型，如原始论文所建议的，应使用以下数据集:</p><p id="728e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae la" href="https://www.robots.ox.ac.uk/~vgg/data/scenetext/" rel="noopener ugc nofollow" target="_blank"> SynthText数据集:</a>这是一个合成生成的数据集，其中word实例被放置在自然场景图像中，同时考虑到场景布局。这是一个非常大的数据集，包含80万张不同文本的图片。</p><p id="da5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae la" href="https://iapr.org/archives/icdar2015/index.html?p=254.html" rel="noopener ugc nofollow" target="_blank"> ICDAR-2015 </a>数据集:这是真实世界的数据集，包含来自可穿戴相机的图像。与SynthText数据集相比，该数据集相对较小(只有1000个训练图像)。</p><p id="b159" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于SynthText数据集足够大，该论文建议在其上训练整个模型，然后适应真实世界的图像，该模型可以在ICDAR-2015数据集上进行微调。</p><h1 id="1d5b" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><em class="lm"> 4。探索性数据分析(EDA) </em></h1><p id="6520" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">这里是详细的EDA的关键，这样做是为了理解给定的数据。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ln"><img src="../Images/f8e86d1e11e46adea14b15c715100352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*hyIAypdBwvbXFlLFQ_P3rA.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Distribution of bounding boxes per image</figcaption></figure><p id="650a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图像大小的分布:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/4b4b578514010a587428455253bca6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*LrbpytfgvnguXpdTL-OC8g.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Distribution of image area in px²</figcaption></figure><p id="219e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是来自SynthText数据集的一些示例:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es lx"><img src="../Images/f55a527612f6770865c652c962089cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bHZUymFGkFu1qQU9YvmeAg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">SynthText image samples from SynthText dataset with bboxes</figcaption></figure><p id="e99f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是来自ICDAR-2015数据集的一些样本</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ly"><img src="../Images/1ce1a129fa9247d4a4eb6c9c5d592f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isS-iTUz3xUkN1lR5wpkWQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">ICDAR-2015 image samples with bboxes</figcaption></figure><p id="35d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从图像质量和边界框的大小/方向来看，很明显，与SynthText数据集相比，ICDAR-2015数据集非常具有挑战性。</p><h1 id="c950" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><em class="lm"> 5。数据预处理/地面实况生成</em></h1><p id="7485" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">为了训练FOTS模型的文本检测组件，需要为每个基本事实图像生成以下基本事实遮罩/图像。</p><ol class=""><li id="7b2d" class="kj kk hh ig b ih ii il im ip lz it ma ix mb jb kq kr ks kt bi translated">分数图:这是一个图像通道，表示给定图像中每个像素的像素是文本的一部分还是背景的一部分。以下是地面实况得分图和相应图像的示例:</li></ol><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mc"><img src="../Images/05be1b310faa2769d8f88e6ef3a38c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SF9NGw1TY1gVPuN-S4QHJA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Ground truth image and score-map visualization</figcaption></figure><p id="0923" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.地理图:地理图包含5个遮罩/通道:对于作为文本一部分的每个像素，前4个通道预测其到包含该像素的边界框的顶部、底部、左侧、右侧的距离，最后一个通道预测相应边界框的方向。以下是这5个地理地图通道的可视化，以及相应的地面实况:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es md"><img src="../Images/f148d5474e1086703003208e89b21113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhTwgNtOo6qqm4aZNdW36A.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Geo map visualization</figcaption></figure><p id="9dff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里是用于生成上述图像的地面真实生成的详细信息。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es me"><img src="../Images/d840dc3c580d76d74c7a13ff79c95716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CY66A9KEnWEI9JxDYJuc8Q.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Ground truth generation process. Source - EAST: <a class="ae la" href="https://arxiv.org/pdf/1704.03155.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1704.03155.pdf</a></figcaption></figure><p id="591b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.训练遮罩:这是单通道图像，用于忽略非常小的边界框和没有来自训练和损失计算过程的抄本的边界框。</p><p id="c62a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在训练模型之前，必须生成以下基本事实:1)得分图，2)地理图，3)边界框列表，4)文本抄本，5)训练掩码</p><p id="69fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，地面实况生成过程变得资源昂贵，并占用大量CPU时间。如果模型在GPU上进行训练，并进行动态数据预处理，GPU将继续等待下一批数据，因为批量地面真实生成将在CPU上进行。这将浪费不必要的GPU时间/资源。因此，作为该实现的一部分，数据将被预先预处理和保存，然后直接用于训练。这提高了训练速度以及资源效率。</p><p id="b1bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是作为实现的一部分，为训练生成的预处理数据集的链接:<a class="ae la" href="https://www.kaggle.com/kaushal2896/synth1k-preprocessed" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/kaushal2896/synth1k-preprocessed</a>。它包括来自SynthText数据集的12800幅图像和地面实况。</p><h1 id="3309" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><em class="lm"> 6。建模</em></h1><p id="0398" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">如FOTS文件所述，该模型由以下主要部分组成:</p><h2 id="1c03" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">特征抽出</h2><p id="c285" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">为了从输入图像中提取高级特征，使用共享卷积层，以预先训练的(在ImageNet上)ResNet50作为主干。共享卷积只不过是卷积层之间具有共享权重。以下是共享卷积的体系结构:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mt"><img src="../Images/508913840f7fbd0c122f2586fcace83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdbLofUde7O-iWKXpnGi2w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Shared Convolutions</figcaption></figure><p id="1fe1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">浅橙色块是用于特征提取的预训练ResNet-50层。浅绿色块是相应ResNet50块的输出要素。“去卷积”模块用于对输入图像/特征进行上采样，以增加输出图像/特征的大小。另请注意，低级功能与高级功能地图直接相连，在上图中显示为黑色箭头。</p><p id="f86c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是从ResNet50的不同块中提取特征的代码</p><pre class="lf lg lh li fd mu mv mw mx aw my bi"><span id="fc6a" class="mf jm hh mv b fi mz na l nb nc">def _extract_features(self, x):</span><span id="7e52" class="mf jm hh mv b fi nd na l nb nc">    """Extract features from given input and backbone."""</span><span id="5dc0" class="mf jm hh mv b fi nd na l nb nc">    x = self.back_bone.conv1(x)</span><span id="ed58" class="mf jm hh mv b fi nd na l nb nc">    x = self.back_bone.bn1(x)</span><span id="eeb9" class="mf jm hh mv b fi nd na l nb nc">    x = self.back_bone.relu(x)</span><span id="f927" class="mf jm hh mv b fi nd na l nb nc">    x = self.back_bone.maxpool(x)</span><span id="0a26" class="mf jm hh mv b fi nd na l nb nc">    res2 = self.back_bone.layer1(x)</span><span id="2eda" class="mf jm hh mv b fi nd na l nb nc">    res3 = self.back_bone.layer2(res2)</span><span id="2424" class="mf jm hh mv b fi nd na l nb nc">    res4 = self.back_bone.layer3(res3)</span><span id="ad08" class="mf jm hh mv b fi nd na l nb nc">    res5 = self.back_bone.layer4(res4)</span><span id="10b3" class="mf jm hh mv b fi nd na l nb nc">    return res5, res4, res3, res2</span></pre><p id="e36c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是“deconv”模块的实现:</p><pre class="lf lg lh li fd mu mv mw mx aw my bi"><span id="488b" class="mf jm hh mv b fi mz na l nb nc">def _deconv(self, feature):</span><span id="9542" class="mf jm hh mv b fi nd na l nb nc">    """Apply deconv operation (inverse of pooling) on given feature map."""</span><span id="e16d" class="mf jm hh mv b fi nd na l nb nc">    # Upsample the given feature.</span><span id="a06f" class="mf jm hh mv b fi nd na l nb nc">    # Doc: https://pytorch.org/docs/stable/nn.functional.html#interpolate</span><span id="a3ea" class="mf jm hh mv b fi nd na l nb nc">    return F.interpolate(</span><span id="fbb3" class="mf jm hh mv b fi nd na l nb nc">        feature,</span><span id="fa9d" class="mf jm hh mv b fi nd na l nb nc">        mode='bilinear',</span><span id="9002" class="mf jm hh mv b fi nd na l nb nc">        scale_factor=2,  # As per the paper</span><span id="4670" class="mf jm hh mv b fi nd na l nb nc">        align_corners = True</span><span id="4a38" class="mf jm hh mv b fi nd na l nb nc">    )</span></pre><h2 id="0baa" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">文本检测分支</h2><p id="3cc0" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">文本检测分支使用全卷积网络作为文本检测器。这些卷积层将具有用于分数图和地理图5个通道。一旦边界框由文本检测器分支提出，位置感知NMS(非最大抑制)将被用于获得具有高于地面真实边界框的最高IoU的边界框。</p><p id="3c4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是使用卷积层实现检测器的代码片段。</p><pre class="lf lg lh li fd mu mv mw mx aw my bi"><span id="f072" class="mf jm hh mv b fi mz na l nb nc">class Detector(nn.Module):</span><span id="aa13" class="mf jm hh mv b fi nd na l nb nc">    """Detector branch of FOTS. This is basically fully convolutions."""<br/></span><span id="f1c3" class="mf jm hh mv b fi nd na l nb nc">    def __init__(self):</span><span id="ad77" class="mf jm hh mv b fi nd na l nb nc">        super().__init__()</span><span id="01d7" class="mf jm hh mv b fi nd na l nb nc">        self.conv_score = nn.Conv2d(32, 1, kernel_size = 1)</span><span id="4183" class="mf jm hh mv b fi nd na l nb nc">        self.conv_loc = nn.Conv2d(32, 4, kernel_size = 1)</span><span id="5c23" class="mf jm hh mv b fi nd na l nb nc">        self.conv_angle = nn.Conv2d(32, 1, kernel_size = 1)</span><span id="df9e" class="mf jm hh mv b fi nd na l nb nc">    def forward(self, x):<br/>        ...</span></pre><h2 id="e305" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">RoIRotate(感兴趣区域旋转)</h2><p id="4d5a" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">RoIRotate对定向/对齐的特征区域应用变换，以获得轴对齐的特征图。这是RoIRotate过程的直观视图:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ne"><img src="../Images/705f75cbb4322333de34d898b7385112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGF5gY_DDJx5pWv02nURZQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">RoIRotate</figcaption></figure><p id="149e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，上图只是为了形象化。RoIRotate的实际实现对通过共享卷积而不是原始图像提取的特征图进行操作。</p><h2 id="4a99" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">文本识别处</h2><p id="521f" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">文本识别分支旨在使用由共享卷积提取并由RoIRotate变换的区域特征来预测文本标签。</p><p id="4a7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文本识别分支包括类似VGG的顺序卷积、仅沿高度轴减少的池化、一个双向LSTM、一个全连接和最终的CTC(连接主义时间分类)解码器。这些组件统称为CRNN(卷积递归神经网络)。</p><p id="116f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是典型CRNN的高级架构:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nf"><img src="../Images/27e093f5cba6b8a0c80ead207d5661a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DDYPq52q51BGcXhV6GgseA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">CRNN Architecture: <a class="ae la" href="https://arxiv.org/pdf/1507.05717.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1507.05717.pdf</a></figcaption></figure><p id="7047" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在FOTS，卷积层是类似VGG的连续层。其架构如下表所示:</p><pre class="lf lg lh li fd mu mv mw mx aw my bi"><span id="61a5" class="mf jm hh mv b fi mz na l nb nc">Type                Kernel              Out</span><span id="2ca4" class="mf jm hh mv b fi nd na l nb nc">[size, stride]      Channels</span><span id="82ca" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="278c" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              64</span><span id="f709" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="7603" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              64</span><span id="423b" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="ec67" class="mf jm hh mv b fi nd na l nb nc">height-max-pool     [(2, 1), (2, 1)]    64</span><span id="c5c8" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="cf60" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              128</span><span id="7acf" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="bace" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              128</span><span id="b63f" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="e5c3" class="mf jm hh mv b fi nd na l nb nc">height-max-pool     [(2, 1), (2, 1)]    128</span><span id="80e5" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="9939" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              256</span><span id="fbbb" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="da33" class="mf jm hh mv b fi nd na l nb nc">conv_bn_relu        [3, 1]              256</span><span id="1f6d" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="5c5e" class="mf jm hh mv b fi nd na l nb nc">height-max-pool     [(2, 1), (2, 1)]    256</span><span id="a367" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="3ed8" class="mf jm hh mv b fi nd na l nb nc">bi-directional      lstm                256</span><span id="6974" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span><span id="3885" class="mf jm hh mv b fi nd na l nb nc">fully-connected                     |S| = n_classes</span><span id="ec10" class="mf jm hh mv b fi nd na l nb nc">---------------------------------------------------</span></pre><p id="05ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">FOTS模型是使用上述四个组件构建的。下面是工作流程的简要总结:首先，输入图像将通过共享卷积传递，特征将被提取(特征将具有输入图像的1/4大小)，接下来，边界框将由文本检测分支预测，这些边界框将通过RoIRotate并水平对齐。这些变换后的边界框将通过文本识别分支，该分支将使用CTC解码器从特征中预测抄本。</p><p id="990a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是整个<strong class="ig hi"> FOTS </strong>模型的架构:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ng"><img src="../Images/931efa005573f94c699176fa67abf975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1-DKG0jj4uzGJU988oGfg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">FOTS overall architecture, The network predicts both text regions and text labels in a single forward pass.</figcaption></figure><h1 id="9f5d" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><em class="lm"> 7。损失计算</em></h1><p id="f87e" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">FOTS损失模型包括文本检测和文本识别两个分支。</p><h2 id="a44f" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">检测损失</h2><p id="819a" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">检测损失考虑平衡的二进制交叉熵(根据论文，但是在该实现中，dice系数用于更快的收敛)，这也被称为分类损失。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nh"><img src="../Images/e8d70e98853577e4997187f0f4ef6e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hbJdFdv7UIfr_-xLfh5Urw.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Detection loss (Classification component)</figcaption></figure><p id="fec7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">检测损失的另一个组成部分考虑预测边界框和地面真实边界框之间的IoU以及预测边界框的旋转。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ni"><img src="../Images/0f101740b5b8404153c2576581f0b463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MByCHfeTLzeEvs1TFmXFwQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Detection loss (regression loss)</figcaption></figure><p id="e488" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总检测损耗由上述损耗组成:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nj"><img src="../Images/edf14e3c774ec6eec364c84693c40d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*D4Tr3xesAUomNX8_fzC-8w.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Detection Loss</figcaption></figure><p id="c045" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">lambda_reg是一个超参数，在此实现中设置为20。</p><h2 id="7786" class="mf jm hh bd jn mg mh mi jr mj mk ml jv ip mm mn jz it mo mp kd ix mq mr kh ms bi translated">识别损失</h2><p id="586d" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">CTC损失用作确认损失。</p><p id="fc80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总FOTS损失由检测损失+识别损失组成。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nk"><img src="../Images/9932ec3deebd02b528021770feaef9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*MPWgT3K0zRotVlXun1yQ_Q.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">FOTS Loss</figcaption></figure><p id="0ea3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">lambda_reg也是一个超参数，在此实现中设置为1。</p><h1 id="37b2" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">8.结果</h1><p id="cffd" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">该模型最初仅被训练用于使用约12K SynthText图像和25个时期的文本检测。结果令人满意。</p><p id="e560" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是检测损失图(仅针对文本检测训练模型时):</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nh"><img src="../Images/0f23520184dd44fe32fd98297cd26a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVxgcu6J2h24ZES0DM0JRQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Detection Loss</figcaption></figure><p id="4c70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，为了训练用于文本识别和文本检测的模型，需要相当大的数据集(完整的SynthText)和更长的训练时间。由于硬件限制，该模型没有针对文本识别进行充分的训练和测试。</p><p id="237e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是模型被训练用于检测和识别时的损失图。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nl"><img src="../Images/93183d96ebcccfea0359dfd59f6106b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xrn9BhykER11a-FpDaF_iA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Recognition Loss + Detection Loss</figcaption></figure><p id="ebed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是模型的一些预测(仅检测)</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nm"><img src="../Images/7455bbc2262e3464940415cbedfb4c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpOUipzJjB4_jpsJcXo_ZQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Detection results</figcaption></figure><h1 id="a645" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">9.结论</h1><p id="a597" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">在这项工作中，我实现了FOTS，一个面向场景文本识别的端到端可训练框架。提出了一种新颖的RoIRotate操作，将检测和识别统一到一个端到端的流水线中。通过共享卷积特征，文本识别步骤几乎是免费的，这使得我们的系统能够以实时速度运行。</p><h1 id="3f82" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">10.未来的工作</h1><p id="1a36" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">如前所述，由于硬件限制，该模型没有在SynthText数据的整个800K图像上进行完全训练。我计划一旦有足够的GPU可用于更长时间的训练，就完全训练FOTS模型，以获得良好的识别结果和文本检测结果。</p><h1 id="ba7a" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">11.参考</h1><p id="6963" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><a class="ae la" href="https://arxiv.org/pdf/1801.01671.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.01671.pdf</a></p><div class="nn no ez fb np nq"><a href="https://github.com/jiangxiluning/FOTS.PyTorch" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hi fi z dy nv ea eb nw ed ef hg bi translated">江西路宁/FOTS。PyTorch</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">ICDAR数据集SynthText 800K数据集检测分支(在训练集上验证，有效！)eval多gpu培训…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">github.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe lk nq"/></div></div></a></div><p id="34e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">https://arxiv.org/pdf/1704.03155.pdf<a class="ae la" href="https://arxiv.org/pdf/1704.03155.pdf" rel="noopener ugc nofollow" target="_blank"/></p><div class="nn no ez fb np nq"><a href="https://www.appliedaicourse.com/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hi fi z dy nv ea eb nw ed ef hg bi translated">应用课程</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">我们知道转行是多么具有挑战性。我们的应用人工智能/机器学习课程被设计为整体学习…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">www.appliedaicourse.com</p></div></div><div class="nz l"><div class="of l ob oc od nz oe lk nq"/></div></div></a></div></div><div class="ab cl og oh go oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ha hb hc hd he"><p id="5043" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这篇文章，请为这个故事鼓掌，并在LinkedIn上与我联系:【https://www.linkedin.com/in/kaushal-shah-466587110/<a class="ae la" href="https://www.linkedin.com/in/kaushal-shah-466587110/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>