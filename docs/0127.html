<html>
<head>
<title>Few-shot learning: from a competition to AAAI conference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">少镜头学习:从竞赛到AAAI会议</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/few-shot-learning-from-a-competition-to-aaai-conference-9613a0a17751?source=collection_archive---------1-----------------------#2021-02-14">https://medium.com/mlearning-ai/few-shot-learning-from-a-competition-to-aaai-conference-9613a0a17751?source=collection_archive---------1-----------------------#2021-02-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e9e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人类的大脑有许多惊人的能力，其中之一就是仅根据少量的观察来区分物体。如果你给孩子们看一张狗的照片和一张牛的照片，他们会很快识别出不同之处，一旦他们看到一只真正的狗或一头真正的牛，他们就能把照片上的物体与现实生活中的物体联系起来。他们可能不知道那些是什么，但他们会知道他们看到的动物就是照片上的那只。然而，在训练数据方面，机器学习模型的要求要高得多。在训练阶段只看到一个类的一个实例后，大多数模型根本没有改进，并且任何随后的测试或模型的实际使用都不会产生可用的结果。这个问题目前很多机器学习研究者都在研究，被称为少射学习。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/d88c67ee59b2ca7d0570dcb92977f01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*88ncEuo_6-bIKuKM"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@eric_welch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eric Welch</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="50c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">少数镜头学习[1]专注于创建一个模型，该模型能够仅基于一个小的训练集来正确地区分类别，在极端情况下，该训练集可能仅由每个类别的单个实例组成。这种学习方法在很难获得训练实例或者创建训练实例的成本很高的情况下非常有用。这个问题通常被描述为<em class="jt"> N-way-K-shot分类</em>，其中<em class="jt"> N </em>表示我们想要学习区分的类的数量，<em class="jt"> K </em>表示每个类在训练集中的实例数量。少数镜头设置使得通常的机器学习模型在这项任务中失败，因为它们依赖于大量的参数，并且它们不能很好地概括，因此从头使用模型不是可行的选择。通常，这个问题必须通过使用一个额外的模型来解决，该模型已经在一个不同的、但是更加详尽的数据集上进行了训练。这种方法反映了视觉已经适应识别物体的形状、结构和特征的自然情况。在机器学习设置中，它用预训练的神经网络来表示，也称为主干网络，它能够在嵌入中对对象的重要方面进行编码。</p><h2 id="a7ff" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">AAAI 2021元深度学习挑战赛</h2><p id="3929" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">当谈到在任何领域寻找一种新颖的方法时，没有什么比竞争精神更能激励人了。AAAI 2021元学习研讨会的组织者决定通过组织一场比赛来帮助寻找新方法，比赛的目标是创造一个在少数镜头设置中成功的模型。</p><p id="39a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比赛于2020年底举行，从9月底一直开放到12月底。它分为两个阶段:在第一个<em class="jt">反馈</em>阶段，参与者可以开发自己的方法，并在公共排行榜上检查表现。参与者能够在提供的Omniglot [2]数据集上离线微调他们的模型，但在线评估是在一个看不见的数据集上完成的。这一阶段结束后，<em class="jt">最终</em>阶段开始，在这一阶段中，每个参与者提交的最佳表现再次在未公开披露的数据上进行测试。最后阶段的结果被选为比赛的决定性结果。</p><p id="b109" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每次提交都必须遵循挑战API，并使用提供的数据生成器，该生成器为模型提供训练和测试实例。此外，比赛对访问计算能力设置了时间限制，这意味着最终的学习者必须在提交后的2小时内产生。这一限制极大地减少了训练选择，因为像WideResNet、DenseNet或ResNet这样的骨干需要大量的训练时间，而且他们的向前传球非常耗时。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ku"><img src="../Images/1c429a301e4737744b97bf251a4cd377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhHGYU4S5xnW5s5rl3JcEA.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Example grid from Omniglot data set for one-shot learning. <a class="ae js" href="https://github.com/brendenlake/omniglot" rel="noopener ugc nofollow" target="_blank">https://github.com/brendenlake/omniglot</a></figcaption></figure><h2 id="b34c" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">提交我们的模型</h2><p id="99db" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们的作品在比赛中获得了第二名。它从[3]开始，其中由主干深度网络产生的潜在空间通过幂变换进行预处理，并且最优传输算法将原始类映射到新类，同时迭代地调整新类的中心。主要目标是提出简单但足够好的主干网络，能够产生实例的竞争性潜在表示。所使用的<strong class="ig hi">主干</strong>的灵感来自于由四个主要模块组成的ResNet架构，其中每个模块都由两个基本模块组成。每个基本块包含两个卷积层，由批量归一化层分隔，并由ReLU激活函数结束，强制特征为非负。主要的四个块之后的层执行平均池化，其输出被传递到展平层，为每个图像产生单个矢量。然后，这个向量必须被标准化，并被传递到负责分类的密集层。值得注意的是，这种分类并不是最终的，它的唯一目的是使主干适应识别形状和物体的任务，而不是在测试阶段产生最终结果。为了提高主干能力，数据集通过包括相同的图像但旋转90、180和270度来扩充，此外，饱和度或亮度可以在输入模型之前随机改变。</p><p id="eb3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">测试和评估阶段的实例表示通过最终的隐含层输出获得。用于产生最终分类输出的方法假设表示类似高斯分布，然而这种情况很少发生，因此必须使用<strong class="ig hi">幂变换</strong>方法对其进行变换。接下来，转换后的向量用于最优传输，其中通过受<strong class="ig hi"> Sinkhorn算法</strong>【4】启发的迭代方法来估计类中心，其中来自支持集的实例被设置为初始中心。然后应用具有欧几里德距离的标准<em class="jt"> k </em> NN [5]方法来为查询集中的每个实例分配类别标签。</p><p id="6bab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于组织者对计算时间的限制，训练参数被设置为使模型执行时间正好适合2小时的窗口。比赛的<em class="jt">公开</em>和<em class="jt">最终</em>阶段使用了相同的设置。</p><h2 id="da48" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">后续步骤</h2><p id="d346" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们的方法在比赛中获得了第二名，主办方邀请我们向《机器学习研究学报》(PMLR)关于元学习的特刊提交一篇论文，该论文也将在2月份的AAAI 2021大会上发表。在宣布结果和提交论文的最后期限之间，我们能够通过引入一组用于模型评估阶段的额外参数来改进模型。当在幂变换步骤期间投影到单位球上时，它们允许垂直方向上的一些变化，而不会破坏所得分布与高斯假设的兼容性。此外，它们允许控制归一化的强度，以更好地模拟假设的高斯分布。该方法被称为<strong class="ig hi">潜在空间变换</strong> (LST)，取代了标准的幂变换(PT)。</p><p id="9cfa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在该方法的实验测试期间，我们能够利用具有更显著能力的主干。我们使用预训练的WideResNet主干将我们的方法与[3]进行比较，我们在CIFAR-FS和CUB数据集上观察到5路1次和5路5次设置的改进。此外，我们观察到我们的预处理方法在用不同的类中心估计方法替代Sinkhorn映射的模型中产生更好的性能，例如<em class="jt"> k </em>均值和高斯混合模型。</p><pre class="jd je jf jg fd kv kw kx ky aw kz bi"><span id="5d1a" class="ju jv hh kw b fi la lb l lc ld">                                        5-way 1-shot<br/>+-----------+-------------+-------------------------------------+<br/>|   Method  |   Backbone  |        CIFAR            CUB         |<br/>+-----------+-------------+-------------------------------------+<br/>|  PT+MAP   |     WRN     |   87.69 +- 0.23%   91.55 +- 0.19%   |<br/>|  PT+GMM   |     WRN     |   86.96 +- 0.22%   90.06 +- 0.18%   |<br/>|  PT+KNN   |     WRN     |   86.17 +- 0.19%   89.07 +- 0.17%   |<br/>+-----------+-------------+-------------------------------------+<br/>|  LST+MAP  |     WRN     |   87.79 +- 0.23%   91.68 +- 0.19%   | &lt;&lt;<br/>|  LST+GMM  |     WRN     |   87.01 +- 0.21%   89.9  +- 0.18%   |<br/>|  LST+KNN  |     WRN     |   85.76 +- 0.19%   89.26 +- 0.17%   |<br/>+-----------+-------------+-------------------------------------+</span><span id="978c" class="ju jv hh kw b fi le lb l lc ld">                                        5-way 5-shot<br/>+-----------+-------------+-------------------------------------+<br/>|   Method  |   Backbone  |        CIFAR            CUB         |<br/>+-----------+-------------+-------------------------------------+<br/>|  PT+MAP   |     WRN     |   90.68 +- 0.15%   94.09 +- 0.09%   |<br/>|  PT+GMM   |     WRN     |   87.16 +- 0.21%   90.04 +- 0.2 %   |<br/>|  PT+KNN   |     WRN     |   86.7  +- 0.19%   89.72 +- 0.18%   |<br/>+-----------+-------------+-------------------------------------+<br/>|  LST+MAP  |     WRN     |   90.73 +- 0.15%   94.09 +- 0.09%   | &lt;&lt;<br/>|  LST+GMM  |     WRN     |   87.33 +- 0.20%   90.06 +- 0.18%   |<br/>|  LST+KNN  |     WRN     |   86.56 +- 0.18%   89.64 +- 0.18%   |<br/>+-----------+-------------+-------------------------------------+</span><span id="5cfb" class="ju jv hh kw b fi le lb l lc ld">* PT-MAP authors state accuracy 93.99 +- 0.10% for 5-way 5-shot evaluation on CUB dataset, however we were able to obtain higher accuracy with their configuration. </span></pre><p id="8d85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些增加导致了一个小的，但在统计上有意义的改善，这是目前最常用的少量学习任务的方法之一。研究结果将于2021年2月9日在AAAI 2021元学习研讨会的专题演讲和海报展示中公布。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lf"><img src="../Images/5169a101d96192dc5f9e7017b26d2aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3-DERhh-Ei6xELFQEpKnww.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Overview of our novel approach to few-shot learning.</figcaption></figure><h2 id="4692" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">承认</h2><p id="8970" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们的研究得到了布拉格捷克技术大学和捷克科学基金会的资助。</p><p id="9cff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这项工作是在布拉格捷克技术大学信息技术学院的学生夏季研究项目VyLeT 2020中完成的。</p><h2 id="f961" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">参考</h2><blockquote class="lg lh li"><p id="f7ac" class="ie if jt ig b ih ii ij ik il im in io lj iq ir is lk iu iv iw ll iy iz ja jb ha bi translated">T. Chobola、d . vaata和P. Kordík，2021年。基于迁移学习的少镜头分类，使用来自骨干神经网络的预处理潜在空间的最佳传输映射<em class="hh">。<br/></em><a class="ae js" href="https://arxiv.org/abs/2102.05176" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/2102.05176】</a></p></blockquote><p id="e0e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jt"> AAAI 2021元学习工作坊:</em><a class="ae js" href="https://metalearning.chalearn.org" rel="noopener ugc nofollow" target="_blank">https://metalearning.chalearn.org</a><br/><em class="jt">Meta dl挑战赛:</em><a class="ae js" href="https://competitions.codalab.org/competitions/26638" rel="noopener ugc nofollow" target="_blank">https://competitions.codalab.org/competitions/26638</a></p><p id="7aaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jt">比赛投稿:</em><a class="ae js" href="https://github.com/ctom2/few-shot-comp" rel="noopener ugc nofollow" target="_blank">https://github.com/ctom2/few-shot-comp</a><br/><em class="jt">发布模特:</em><a class="ae js" href="https://github.com/ctom2/latent-space-transform" rel="noopener ugc nofollow" target="_blank">https://github.com/ctom2/latent-space-transform</a></p><p id="fccd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]王，杨，姚，张，郭俊涛，倪，林国明. 2020 .从几个例子中归纳出:一个关于少投学习的调查。美国计算机学会计算调查(CSUR)53(3):1–34。</p><p id="c89e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]莱克，B. M .，萨拉胡特迪诺夫，r .和特南鲍姆，J. B. (2015年)。<em class="jt">通过概率程序归纳的人类级概念学习</em>。科学，350(6266)，1332–1338。</p><p id="4105" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]胡，杨，格里彭，v .和帕多，S. 2020。<em class="jt">利用基于迁移的少镜头学习中的特征分布</em>。ArXiv abs/2006.03806。</p><p id="cccb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]伯曼，R. J. 2020。<em class="jt">sink horn算法、抛物线最优运输和几何蒙日-安培方程</em>。数字数学<em class="jt"/>145:771–836。</p><p id="27e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5]王，杨，晁，温伯格，K. Q .和范德马腾，2019年。<em class="jt"> Simpleshot:重新访问最近邻分类进行少镜头学习</em>。ArXiv <em class="jt"> </em> abs/1911.04623。</p></div></div>    
</body>
</html>