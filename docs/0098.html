<html>
<head>
<title>Image Super-Resolution Using EDSR and WDSR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用EDSR和WDSR实现图像超分辨率</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/image-super-resolution-using-edsr-and-wdsr-f4de0b00e039?source=collection_archive---------0-----------------------#2021-02-02">https://medium.com/mlearning-ai/image-super-resolution-using-edsr-and-wdsr-f4de0b00e039?source=collection_archive---------0-----------------------#2021-02-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/7e53400efa9e3a0c4ef0c2beb1e2caf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6s0Sh78UF8J7PBWRM4CgA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Low-Resolution — Super-Resolution(predicted) — High Resolution(Original )</figcaption></figure></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="41ca" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated">目录</h1><ol class=""><li id="37cb" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">介绍</li><li id="a964" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">商业问题</li><li id="92eb" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">映射到ML/DL问题</li><li id="ea4e" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">探索性数据分析</li><li id="89cb" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">数据扩充</li><li id="bdb5" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">EDSR(用于单幅图像超分辨率的增强型深度残差网络)</li><li id="1596" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">WDSR(高效精确图像超分辨率的宽激活)</li><li id="d474" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">比较结果(根据性能指标)</li><li id="2a70" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">使用不同的验证集预测结果</li><li id="192b" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">AWS EC2上的部署</li><li id="8a59" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">工作模型</li><li id="9e76" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">改进范围(未来工作)</li><li id="31ed" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">参考</li></ol></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="c4f0" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated"><strong class="ak"> 1。简介</strong></h1><p id="8879" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">图像超分辨率问题，尤其是单幅图像超分辨率(SISR)问题，几十年来受到越来越多的关注。SISR(Single Image Super Resolution，单幅图像超分辨率)旨在从单幅低分辨率图像重建高分辨率图像。</p><p id="cb15" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">通常，低分辨率图像和原始高分辨率图像之间的关系可以根据情况而变化。许多研究假设低分辨率图像是高分辨率图像的双三次下采样版本，但在实际应用中也可以考虑其他退化因素，如模糊、抽取或噪声。</p><p id="4c1b" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">这个项目的主要任务是从不同的降级图像中获得超分辨率图像，如双三次或尺度为4的未知降级图像。</p><p id="dbc0" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在这个项目中，我们将使用深度残差网络进行单幅图像的<br/>超分辨率(SR)。该方法直接学习低分辨率图像和高分辨率图像之间的端到端映射。该映射被表示为深度卷积神经网络(CNN ),其将低分辨率图像作为输入，并输出高分辨率图像。</p><p id="5bbc" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在这个项目中，我们将使用一个已知的降级函数(双三次/比例为4的未知下采样),并遵循一种监督学习方法。</p><p id="e846" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在这个项目中，我们将实现<a class="ae lp" href="https://arxiv.org/pdf/1707.02921.pdf" rel="noopener ugc nofollow" target="_blank"> EDSR </a>(用于单幅图像超分辨率的增强型深度残差网络)和<a class="ae lp" href="https://arxiv.org/pdf/1808.08718.pdf" rel="noopener ugc nofollow" target="_blank"> WDSR </a>(用于高效精确图像超分辨率的宽激活)</p></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="cc70" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated"><strong class="ak"> 2。商业问题</strong></h1><p id="45ef" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">数字图像设备已经广泛应用于许多领域，包括个人识别和遥感。捕获的图像是来自潜在观察的退化图像，其中退化处理受诸如照明和噪声破坏的因素影响。具体来说，噪声是在传输和压缩过程中由未知的潜在观测值产生的。使用图像去噪技术从给定的降质图像中去除噪声并恢复潜在观测值是非常必要的。</p><p id="8643" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">超分辨率(SR)成像的关键目标是基于从同一场景获取的一组图像(表示为低分辨率图像)重建更高分辨率的图像，以克服图像获取过程的限制和不适定条件，从而促进更好的内容可视化和场景识别。</p><p id="a921" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">超分辨率图像的方法是从一组给定的较低分辨率图像中重建单个较高分辨率图像。</p><p id="82fc" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">最近，深度神经网络在随机共振问题中在峰值信噪比(PSNR)方面提供了显著改善的性能。然而，这种网络在架构优化方面表现出局限性。</p><p id="1670" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">首先，神经网络模型的重建性能对微小的结构变化敏感。此外，同一模型通过不同的初始化和训练技术实现不同的性能水平。因此，精心设计的模型结构和复杂的优化方法在训练神经网络中是必不可少的。</p><p id="7a73" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">SRResNet成功地解决了时间和内存问题，具有良好的性能，它只是采用了来自he等人的ResNet体系结构，没有做太多的修改。然而，最初的ResNet是为了解决更高级别的计算机视觉问题，如图像分类和检测。因此，将ResNet架构直接应用于像超分辨率这样的低级视觉问题可能是次优的。</p><p id="7c08" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">为了解决这些问题，基于SRResNet架构，我们首先通过分析和删除不必要的模块来优化它，以简化网络架构。</p><p id="a522" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">当模型复杂时，训练网络变得不简单。因此，我们用适当的损失函数(在我们的情况下是平均绝对误差)训练网络，并在训练时仔细修改模型(通过去除批量标准化)。</p><p id="babe" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我们在SET5、SET 14等标准基准数据集和新提供的DIV2K数据集上评估了EDSR和WDSR模型。</p><p id="28a0" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在这个项目中，我们将使用缩放因子为4的双三次降级图像和缩放因子为4的未知降级图像。</p><p id="636e" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在PSNR(峰值信噪比)和SSIM(结构相似性指数)方面，建议的EDSR和WDSR网络在DIV2k、Set5和Set14上显示了最先进的性能。</p></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="8f46" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated">3.映射到ML/DL问题</h1><p id="496d" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">我们将使用基于Resnet的架构作为EDSR和WDSR模型的基本构建模块。</p><p id="4459" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">尽管Resnet块如EDSR论文中所考虑的那样被修改，但我们将从基本的ResBlock体系结构中移除批处理规范化和最终Relu激活。</p><p id="8e1b" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我使用<strong class="ka hi"> <em class="lq">平均绝对误差</em> </strong> <em class="lq"> </em>作为损失函数，因为我们正在处理像素，并且我们正在以监督的方式将低分辨率图像映射到高分辨率，因此在这种情况下使用MAE更好。</p><p id="cb0a" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">为了我们的实际目的，<strong class="ka hi"> <em class="lq">平均绝对误差(MAE) </em> </strong>允许我们比较原始图像和退化图像的“真实”像素值。MAE表示我们的实际图像和噪声图像之间的平均绝对误差。误差是原始图像的值与降级图像的值的差异量。</p><p id="11d6" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">作为性能指标，我用了<em class="lq"/><strong class="ka hi"><em class="lq">【PSNR】(峰值信噪比)</em></strong><strong class="ka hi"><em class="lq">【SSIM】(结构相似指数)</em> </strong>。</p><p id="9c88" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">这两个指标在TensorFlow API中都很容易获得。</p><p id="ce11" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">如果我们可以证明一种算法或一组算法可以增强退化的已知图像，使其更接近原始图像，那么我们可以更准确地得出结论:<strong class="ka hi"> <em class="lq"> PSNR </em> </strong>是一种更好的算法。</p><p id="7eae" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">建议是，<strong class="ka hi"> <em class="lq">、</em> </strong>越高，退化图像被重建得越好以匹配原始图像，并且重建算法越好。这将会发生，因为我们希望相对于图像的最大信号值最小化图像之间的MAE。</p><p id="9fcc" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">制定损失函数和性能指标</strong></p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/f6def85c1123034c27f05a4e609e7686.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*mXr60qFSE8o-t-ALpVr9WA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">MSE</figcaption></figure><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/4df21603f232afc1c0373b10d16a5713.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*gYzAiuh1rSCMxH-1GMsqbQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">PSNR</figcaption></figure><p id="319c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> f </strong> —表示我们的原始图像的矩阵数据<br/> <strong class="ka hi"> g </strong> —表示我们所讨论的退化图像的矩阵数据<br/> <strong class="ka hi"> m </strong> —表示图像的像素行数，I表示该行的索引<br/> <strong class="ka hi"> n </strong> —表示图像的像素列数，j表示该列的索引<br/> <strong class="ka hi"> Max(f) — </strong>是存在的最大信号值</p><h1 id="f9a5" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">4.探索性数据分析</h1><h2 id="5e31" class="mc jb hh bd jc md me mf jg mg mh mi jk kf mj mk jo kh ml mm js kj mn mo jw mp bi translated">4.1数据概述</h2><p id="0a3e" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">在这个项目中，我已经在<a class="ae lp" href="https://www.tensorflow.org/datasets/catalog/div2k" rel="noopener ugc nofollow" target="_blank"> DIV2K数据集</a>上训练了该模型，该数据集包含高质量(2K分辨率)图像和用于图像恢复任务的相应降级图像数据集。</p><p id="0d8c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">DIV2K数据集有许多降级因素，如下所述</p><p id="6511" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">双三次x2，双三次x3，双三次x4，双三次x8，未知x2，未知x3，未知x4，现实温和x4，现实困难x4，现实狂野x4</p><p id="5290" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我用过双三次_x4和未知_x4进行模型训练。</p><p id="c3d5" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我已经使用<a class="ae lp" href="https://www.tensorflow.org/datasets/api_docs/python/tfds" rel="noopener ugc nofollow" target="_blank"> tfds </a> TensorFlow API收集了数据</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">load div2k data using tfds</figcaption></figure><p id="c40f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">4.2 DIV2K数据集分为:</strong></p><p id="7852" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">训练数据:</strong>前800幅高清图像和对应的具有特定降级因子的低分辨率图像。</p><p id="0873" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">验证数据:</strong> 100张高清图像，对应低分辨率图像作为验证数据</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/e63042fbb9093b825c68f207df0ad020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*4UqWEIOK5tCGxshgi_QRqg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">train data overview</figcaption></figure><p id="ae8d" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">通过观察上面的图像，我们不能区分低分辨率和高分辨率图像，但是如果我们打印LR和HR图像的形状，那么我们可以看到差异。</p><p id="3339" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">你可以在下面附的图片中看到上面图片的区别。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/db598fd6343ef38e31cd9b52b9484462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-uSUBkBVq-CxgZJmAuQzA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The shape of LR and HR images</figcaption></figure></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="7397" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated"><strong class="ak"> 5。数据扩充</strong></h1><p id="8c6d" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated"><strong class="ka hi"> 5.1什么是数据增强？？</strong></p><p id="e87e" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">数据增强是一种获得同一幅图像的不同变化的技术，如翻转图像、旋转图像、裁剪图像等。</p><p id="e9a1" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">这是一种获得不同种类数据的方式，尤其是当训练数据的数量非常少时，因为我们有DIV2k数据，它只有800个训练图像。</p><p id="4b86" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">只有800张图像用于训练，这对于训练模型是非常少的，所以我们将对训练数据进行数据扩充，以获得更多种类的不同种类的图像。</p><p id="af4d" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 5.2为什么要增大？？？？</strong></p><p id="6b78" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">扩充对于从训练好的模型中获得更一般化的结果是非常重要的。因为通过对增广数据模型的训练，可以学习各种各样的特征，所以增广给出了更一般化的结果。</p><p id="6aee" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 5.3应用增强(裁剪、旋转、翻转)</strong></p><p id="0be6" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">随机裁剪:</strong>我们将从高分辨率图像中裁剪一个尺寸为96 X 96的图像的随机部分，并从低分辨率图像中裁剪相应的补片。</p><p id="328c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">如果缩放因子是4，并且如果我们从HR图像中裁剪96×96的碎片，那么来自低分辨率图像的相应碎片大小将是24×24(96/4 = 24。</p><p id="f114" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">随机翻转:</strong>在此操作中，我们将翻转LR和HR图像，如果从<strong class="ka hi"><em class="lq">TF . Random . normal</em></strong>生成的随机值小于0.5，则我们进行left_right翻转，否则我们不翻转图像。</p><p id="1e77" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">随机旋转</strong>:在此操作中，我们将多次将LR和HR图像旋转90度。我们将使用<strong class="ka hi"><em class="lq">TF . random . uniform</em></strong>生成一个最大值为4的随机数。例如，如果生成的随机值为2，那么我们将图像旋转90度2次</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">data augmentation</figcaption></figure><p id="525f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 5.4完成数据管道，以获得训练和验证扩充数据:</strong></p><p id="c56f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">数据管道是一种通过使用<strong class="ka hi"> <em class="lq"> tf.data </em> </strong> API以高效的方式获取已处理数据(裁剪、翻转、旋转)的方式。</p><p id="e932" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">从数据管道中，我们可以获得具有特定批量的扩充的训练数据和没有扩充的验证数据。</p><p id="d27d" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我们将执行以下步骤进行预处理</p><p id="e338" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">列车数据</strong>:</p><ol class=""><li id="d058" class="jy jz hh ka b kb lk kd ll kf mu kh mv kj mw kl km kn ko kp bi translated">随机作物</li><li id="d8c7" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">随机旋转</li><li id="5a79" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">随机翻转</li><li id="e454" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">分配批量大小(在我们的例子中，训练数据为16)</li><li id="404d" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">重复(对于训练数据无限)</li><li id="842e" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">预取(为了更快的培训过程)</li></ol><p id="ded5" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">为了获得<strong class="ka hi"><em class="lq"/></strong>验证数据，我们不会做任何类型的转换，我们唯一要做的是分配批量大小并重复计数1。</p><p id="dd1c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">让我们跳到预处理数据的代码。</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">data pipeline</figcaption></figure><p id="3d7f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">train _预处理包含作为训练数据低分辨率小图像补片和作为目标图像的相应高分辨率图像补片，用于端到端映射和计算损失。</p><p id="2ec4" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 5.5可视化来自训练数据的预处理图像</strong></p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mx"><img src="../Images/7fba1d3ac904c31360b41d3eb29bb063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYOVcP7lorKUMajCH9MMSw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">train_preprocessed example</figcaption></figure><p id="8fb8" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">正如我们从上面的图片中看到的，训练图片中有一个随机裁剪的部分，每次我们执行代码块时都会有所不同。</p><p id="1f03" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 5.6可视化来自验证数据的预处理图像</strong></p><p id="1517" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">验证数据没有被扩充，因为我们想要得到整个验证图像的结果。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es my"><img src="../Images/1668084943ca782e7352b6b6c9d48295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bAY2J_4Dlyr6czUKYE_Ew.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">validation_preprocessed</figcaption></figure><p id="2411" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">正如我们所看到的，我们没有对验证数据集进行任何类型的扩充和预处理。我们唯一知道的是分配的批量大小。</p><h1 id="1e60" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">6.EDSR(用于单幅图像超分辨率的增强型深度残差网络)</h1><p id="8441" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">现在是时候实施一项<a class="ae lp" href="https://arxiv.org/pdf/1707.02921.pdf" rel="noopener ugc nofollow" target="_blank"> EDSR研究论文</a></p><p id="c07c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">提议的EDSR网络的总体架构如下。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/f3b26e7623e97d2459ea08812f630518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*fC1goVOCCMfCiMXdeXZQ0w.png"/></div></figure><p id="b361" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">在EDSR，他们提出了不同的ResBlock架构，以更有效地训练模型。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es na"><img src="../Images/6020321c3d48be291eba199dbc551c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*cwk2l6XP9aETEQ_6Jox67A.png"/></div></figure><p id="4fe7" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">提议的网络。正如Nah等人在他们的图像去模糊工作中所提出的，我们从我们的网络中移除批量归一化层。由于批量归一化图层会对要素进行归一化，因此它们会通过对要素进行归一化来消除网络的范围灵活性，因此最好将其移除。</p><p id="899d" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">此外，由于批量标准化层消耗的内存量与前面的卷积层相同，因此GPU内存使用量也大大减少。与SRResNet相比，这种没有批处理规范化层的基线模型在训练期间节省了大约40%的内存使用。因此，我们可以在有限的计算资源下建立一个比传统的ResNet结构具有更好性能的更大的模型。</p><p id="4da2" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">以下是EDSR模型的完整架构</strong></p><ol class=""><li id="56c2" class="jy jz hh ka b kb lk kd ll kf mu kh mv kj mw kl km kn ko kp bi translated">通过减去DIV2K RGB平均值来归一化输入</li><li id="81bc" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">具有64个滤镜和内核大小=3的Conv2d层</li><li id="eb83" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">Resblock(在我们的8型RES block中)</li><li id="4974" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">Conv2d</li><li id="7277" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">添加(ResBlock输出和原始输入)</li><li id="b3e7" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">上采样(Conv2d →像素混合)</li></ol><p id="b48a" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><em class="lq">现在让我们深入到编码部分</em></p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">EDSR model</figcaption></figure><p id="3075" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">为了根据研究论文训练EDSR模型，我们必须训练300000步的模型，并在前10个验证集图像上每1000步评估模型，因此为了满足这一要求，我制作了一个完整的训练管道类来根据要求训练模型，并且我们可以随时恢复最新的检查点，以便我们可以随时恢复训练。</p><p id="5846" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">下面考虑的课程是完整的培训管道，以更可行的方式培训模型。</p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">training pipeline</figcaption></figure><p id="4c18" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">现在，我们已经完成了开始培训程序的所有必要步骤。</p><p id="d87c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><em class="lq">现在让我们开始训练</em></p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div></figure><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/229d2b005dcefd4af60efaecdf464fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlN4vyi0BTfhj_LiAJo-yQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">training started</figcaption></figure><p id="4449" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> <em class="lq">注:</em> </strong> <em class="lq">我没有绘制PSNR vs纪元和损失vs纪元，因为我已经训练了300000个纪元的模型，所以有时我会暂停训练，然后恢复最新的检查点并继续训练。</em></p></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="3542" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated">7.WDSR(高效精确图像超分辨率的宽激活)</h1><p id="f6cc" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">WDSR模型的一般架构如下。</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/490cf271e78c63e9dae5eb61b9555fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WljNZ7LDicgHN6bft02E4A.png"/></div></div></figure><p id="ea81" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">如下面的</strong>所示的残差块架构</p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/8c8b545c46b641394e55c8dba0902c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OC1UsiBzz-cMiGaPignzXw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Resblock architecture</figcaption></figure><p id="180c" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">所提出的工作的直觉是，在ReLU之前扩展特征允许更多的信息通过，同时仍然保持深度神经网络的高度非线性。因此，来自浅层的低级SR特征可能更容易传播到最终层，以获得更好的密集像素值预测。</p><p id="cde2" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我们将执行以下步骤来创建WDSR模型:</p><p id="44cc" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">WDSR架构总共有两种变化:</strong></p><ol class=""><li id="6419" class="jy jz hh ka b kb lk kd ll kf mu kh mv kj mw kl km kn ko kp bi translated">WDSR-A</li><li id="7d5b" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">WDSR-B</li></ol><p id="9fc0" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">WDSR-A和WDSR-B的唯一区别是Resblock架构。</p><p id="1473" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 7.1我们先来了解一下WDSR的总体架构:</strong></p><ol class=""><li id="bf0d" class="jy jz hh ka b kb lk kd ll kf mu kh mv kj mw kl km kn ko kp bi translated">标准化输入</li><li id="0a11" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">Conv2d权重归一化(主分支)</li><li id="7002" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">RES block(WDSR-A和WDSR-B不同)</li><li id="ced9" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">Conv2d重量归一化(具有给定刻度)</li><li id="25bc" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">混洗像素(具有给定的比例)</li><li id="a701" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">Conv2d权重归一化(跳过分支)</li><li id="01d1" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">混洗像素(跳过给定比例的分支)</li><li id="1b24" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">添加主分支输出并跳过分支输出(添加步骤5和步骤7)</li><li id="72f0" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">使用DIV2K RGB平均值反规格化</li><li id="cdfb" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">输出模型</li></ol><p id="dd6f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi">7.2</strong><a class="ae lp" href="https://arxiv.org/pdf/1808.08718.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ka hi">WDSR研究</strong> </a> <strong class="ka hi">概要如下:</strong></p><ol class=""><li id="4a3f" class="jy jz hh ka b kb lk kd ll kf mu kh mv kj mw kl km kn ko kp bi translated">他们证明，在单个图像超分辨率的残差网络中，在相同的参数复杂度下，更宽的激活具有更好的性能。在没有额外计算开销的情况下，他们提出了网络WDSR-A，它具有更宽的(2倍到4倍)激活以获得更好的性能。</li><li id="6bf7" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">为了进一步提高效率，他们还提出将线性低秩卷积作为构建SR网络WDSR-B的基本构建模块。它可以在不增加参数或计算的情况下实现更广泛的激活(6倍至9倍),并进一步提高精度。</li><li id="972e" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">他们建议批量归一化不适于训练深度SR网络，并引入权重归一化以获得更快的收敛和更好的精度。</li><li id="d9a9" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">我们对提出的WDSR-A和WDSR-B进行了训练，并在大规模DIV2K图像超分辨率基准上取得了较好的结果。</li></ol><p id="2880" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><em class="lq">现在让我们做编码部分</em></p><figure class="ls lt lu lv fd ii"><div class="bz dy l di"><div class="mq mr l"/></div></figure><p id="53c2" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我们可以按照EDSR训练中讨论的相同方式训练WDSR模型</p><p id="e8a5" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">我们将使用相同的训练管道类来训练WDSR模型</p><p id="b916" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> <em class="lq">注:</em> </strong> <em class="lq">我没有绘制PSNR vs纪元和损失vs纪元，因为我已经训练了300000个纪元的模型，所以有时我会暂停训练，然后恢复最新的检查点并继续训练。</em></p></div><div class="ab cl it iu go iv" role="separator"><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy iz"/><span class="iw bw bk ix iy"/></div><div class="ha hb hc hd he"><h1 id="2265" class="ja jb hh bd jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx bi translated">8.比较结果</h1><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/55949a291597cd9c83fe0f769ef369b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*b6MmhCkNyTeOEQ1G0z2jig.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">results</figcaption></figure><p id="9fca" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">正如我们在表中看到的，在双三次x4数据集上，WDSR_B往往比EDSR模型表现得更好。</p><p id="3d61" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">SSIM与EDSR和WDSR型号相同</p><h1 id="37b2" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">9.使用不同的验证数据集预测结果</h1><p id="a55d" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">首先，我们将看到EDSR的结果</p><h2 id="c543" class="mc jb hh bd jc md me mf jg mg mh mi jk kf mj mk jo kh ml mm js kj mn mo jw mp bi translated">9.1 EDSR结果</h2><h2 id="d241" class="mc jb hh bd jc md me mf jg mg mh mi jk kf mj mk jo kh ml mm js kj mn mo jw mp bi translated">9.1.1双三次_x4(双三次降级，缩放因子= 4)</h2><ol class=""><li id="308b" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ka hi"> DIV2k验证集结果</strong></li></ol><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/315b8082d0575f06e52c9a15fecd3746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vrJ_Y4MYLyOq1wHIBotqQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">div2k validation set(bicubic_x4)</figcaption></figure><p id="963f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 2。设置5双三次_x4 </strong></p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/7e53400efa9e3a0c4ef0c2beb1e2caf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6s0Sh78UF8J7PBWRM4CgA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">set 5 bicubic_x4</figcaption></figure><p id="2e2e" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> 3。设置14个双三次_x4 </strong></p><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/61ba1b7655a96fca9b4322c819fcf213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2DAjUo_31ULdd2i9WvvgYQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">set 14 bicubic_x4</figcaption></figure><h2 id="3da9" class="mc jb hh bd jc md me mf jg mg mh mi jk kf mj mk jo kh ml mm js kj mn mo jw mp bi translated">9.1.2 Unknown_x4(比例因子= 4的未知降级)</h2><ol class=""><li id="afa7" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ka hi"> DIV2k验证数据集</strong></li></ol><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3bc1394bb2c92d769def94cc6d539b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*az9EvxXs1nhcXu9An_g6yQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">unknown_x4</figcaption></figure><h2 id="a9f8" class="mc jb hh bd jc md me mf jg mg mh mi jk kf mj mk jo kh ml mm js kj mn mo jw mp bi translated">9.2 WDSR结果</h2><ol class=""><li id="6232" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ka hi"> DIV2K验证</strong></li></ol><figure class="ls lt lu lv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cf8dfd358273ab9569b5d8b5ee382aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BujHWtvxyveG3xH9Sp-oLg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">wdsr result</figcaption></figure><h1 id="1e2d" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">10.AWS EC2上的部署</h1><p id="42a1" class="pw-post-body-paragraph kv kw hh ka b kb kc kx ky kd ke kz la kf lb lc ld kh le lf lg kj lh li lj kl ha bi translated">我已经部署了两个EDSR模型。一个在DIV2k Bicubic_x4上训练，另一个在DIV2K Unknon_x4数据集上训练。</p><p id="e32b" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">你可以在  这里找到<a class="ae lp" href="http://ec2-52-87-152-145.compute-1.amazonaws.com:8080/" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi"> <em class="lq">的部署链接</em></strong></a></p><p id="179d" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">您可以从<a class="ae lp" href="https://uofi.box.com/shared/static/kfahv87nfe8ax910l85dksyl2q212voc.zip" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi"> <em class="lq">此处</em> </strong> </a> <em class="lq"> </em>下载SET 5数据集，从<a class="ae lp" href="https://uofi.box.com/shared/static/igsnfieh4lz68l926l8xbklwsnnk8we9.zip" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi"> <em class="lq">此处</em> </strong> </a>下载SET 14数据集进行测试。</p><p id="378f" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated">注意:该模型不会给出实时相机图像的最佳结果，因为该模型已经在自然图像上进行了裁剪训练。</p><h1 id="cc55" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">11.工作模型</h1><figure class="ls lt lu lv fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/3e084fa09e519cd4e408b6381ea51cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/1*Hd2tH_E4mFA1ke85w5ey0g.gif"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Working Model on AWS EC2</figcaption></figure><p id="0e68" class="pw-post-body-paragraph kv kw hh ka b kb lk kx ky kd ll kz la kf lm lc ld kh ln lf lg kj lo li lj kl ha bi translated"><strong class="ka hi"> <em class="lq">注:</em> </strong> <em class="lq">请勿上传图片大小超过200 kb或不上传任何高清图片进行测试。背后的原因是AWS EC2 free tire只提供1 GB的Ram和1个cpu。</em></p><h1 id="089d" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">12.改进范围(未来工作)</h1><ol class=""><li id="fa83" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">到目前为止，我们仅在具有双三次x4和未知x4降级因子的div2k数据集上训练了EDSR和WDSR模型</li><li id="7517" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">为了进一步改进，我们可以使用混合降级因子来训练模型范围内的各种数据，以便我们的模型可以预测任何真实世界的图像。</li><li id="a1e7" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">为了得到更精确的结果，我们可以用感知损失来训练SRGAN模型。由于有限的计算资源和collab使用限制，到目前为止我还没有尝试过SRGAN。</li><li id="773c" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">我们可以肯定地改善结果使用SRGAN与发电机和歧视损失</li></ol><h1 id="295e" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">13.参考</h1><ol class=""><li id="2f86" class="jy jz hh ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">【https://arxiv.org/pdf/1707.02921.pdf T42】</li><li id="38a9" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1808.08718.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1808.08718.pdf</a></li><li id="482c" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">https://github.com/krasserm/super-resolution<a class="ae lp" href="https://github.com/krasserm/super-resolution" rel="noopener ugc nofollow" target="_blank"/></li><li id="3d28" class="jy jz hh ka b kb kq kd kr kf ks kh kt kj ku kl km kn ko kp bi translated">【https://www.appliedaicourse.com/ T4】</li></ol><h1 id="7874" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">Github简介</h1><div class="ng nh ez fb ni nj"><a href="https://github.com/sumittagadiya/Image-super-resolution" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">超高分辨率图像</h2><div class="nq l"><p class="bd b fp z dy no ea eb np ed ef dx translated">利用github.com EDSR和WDSR研究所实现图像超分辨率</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw in nj"/></div></div></a></div><h1 id="20e3" class="ja jb hh bd jc jd lx jf jg jh ly jj jk jl lz jn jo jp ma jr js jt mb jv jw jx bi translated">LinkedIn个人资料</h1><div class="ng nh ez fb ni nj"><a href="https://www.linkedin.com/in/sumit-tagadiya-450397194/" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">Sumit Tagadiya -印度古吉拉特邦艾哈迈达巴德|职业简介| LinkedIn</h2><div class="nx l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">查看世界上最大的职业社区LinkedIn上Sumit Tagadiya的个人资料。Sumit的教育上市于…</h3></div><div class="nq l"><p class="bd b fp z dy no ea eb np ed ef dx translated">www.linkedin.com</p></div></div><div class="nr l"><div class="ny l nt nu nv nr nw in nj"/></div></div></a></div></div></div>    
</body>
</html>