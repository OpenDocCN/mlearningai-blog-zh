<html>
<head>
<title>Paper Summary: Masked Autoencoders Are Scalable Vision Learners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要:屏蔽自动编码器是可扩展的视觉学习者</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/paper-summary-masked-autoencoders-are-scalable-vision-learners-2dea8cdb1884?source=collection_archive---------4-----------------------#2022-01-23">https://medium.com/mlearning-ai/paper-summary-masked-autoencoders-are-scalable-vision-learners-2dea8cdb1884?source=collection_archive---------4-----------------------#2022-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="10a5" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">由于有限的GPU，对于我们来说，用论文中提到的大型模型或长时间计划进行训练确实是一个挑战。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b4ed4a2b002f5bdca4e8871aa7e1f3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0eHITOGns9aO12cQSQGBQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="88b8" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这篇论文是可以在现实世界中实际应用的令人兴奋的研究之一；换句话说，这篇论文提出<strong class="jq hi">屏蔽自动编码器</strong> (MAE)是<strong class="jq hi">可扩展的</strong>自我监督学习器。通过掩蔽输入(这里是图像)的随机补丁并重建丢失的像素来证明这一点是非常有趣的。</p><p id="8c16" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">研究人员获得了两个巧妙的设计:</p><ol class=""><li id="43c0" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">开发<strong class="jq hi">非对称编码器-解码器架构</strong></li><li id="b173" class="kk kl hh jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">屏蔽大部分输入产生了一个不平凡且有意义的自我监督任务<strong class="jq hi">。</strong></li></ol><p id="9fc9" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">此外，获得上述两种设计表明，我们可以用<strong class="jq hi">大型训练数据集</strong>高效且有效地训练我们的模型。表示<strong class="jq hi">加速训练</strong>加速了<strong class="jq hi">3倍或更多</strong>，并且<strong class="jq hi">精度</strong>提高了<strong class="jq hi"/>。</p><p id="c60c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这种可扩展的方法使得学习泛化能力好的高容量模型变得可行(这可以在实验结果中看到)。</p><h1 id="ce56" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">介绍</h1><p id="a8c1" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">有许多架构是不可停止的，扩展了它们的能力和容量；此外，随着硬件模型的进步，可以很容易地适应数百万张图像。</p><p id="2b70" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这就承认了大量的数据可以在<em class="lv"> NLP </em> s中轻松看到。基于<strong class="jq hi"> GPT </strong>中的<em class="lv">自回归语言建模</em>和<strong class="jq hi"> BERT </strong>中的<em class="lv">屏蔽自动编码</em>的答案并不复杂:它们删除一定比例的数据，并学习预测被删除的内容。这些方法使得<em class="lv"> NLPs模型</em>的训练，包括数十亿亿的参数都是可行的。</p><p id="5a59" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这篇论文提到，屏蔽自动编码器作为通用去噪自动编码器的想法可以毫无问题地在计算机视觉中实现。</p><p id="343f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi"> <em class="lv">是什么让蒙版自动编码在视觉和语言上有所不同？</em> </strong>为了回答这个问题，作者带来了以下观点:</p><ul class=""><li id="6dea" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj lw kq kr ks bi translated">在过去的十年中，CNN一直被认为是一个强有力的模型。一般来说，它们在规则的网格上执行，并且不能直接集成“指示器”(掩码标记、位置嵌入等)。这个差距不再被考虑，因为2021年的研究(<a class="ae jn" href="https://arxiv.org/pdf/2010.11929" rel="noopener ugc nofollow" target="_blank">一张图像抵得上16x16个字:大规模图像识别的变形金刚</a>)。</li><li id="10a9" class="kk kl hh jq b jr kt ju ku jx kv kb kw kf kx kj lw kq kr ks bi translated">信息密度:语言是人类制造的信号(高度语义和信息密集)。图像是自然生成的信号(大量的空间冗余)。为了应对这种不同，作者将展示一种优于计算机视觉的简单策略:<em class="lv">屏蔽大多数路径。</em></li><li id="4f56" class="kk kl hh jq b jr kt ju ku jx kv kb kw kf kx kj lw kq kr ks bi translated">自动编码器的解码器(将潜在表示映射回输入)对图像和文本的作用不同。在vision中:<em class="lv">像素</em>被重建(因此，输出具有较低的语义级别)。相比之下，在语言中，该模型预测包含丰富语义信息的缺失单词。</li></ul><p id="4504" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">本研究中提出的<strong class="jq hi"> MAE </strong>(屏蔽自动编码器):</p><ol class=""><li id="220b" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">屏蔽输入图像中的随机面片</li><li id="650e" class="kk kl hh jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">在像素空间中重建缺失的面片</li></ol><p id="f58a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">有一种<em class="lv">非对称编码器-解码器</em>设计，其中编码器仅在补丁的可见子集上运行(没有掩码令牌)。解码器从潜在表示连同掩码标记一起重建输入。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/9e530b426c5c7ac9ed83aa8878db1fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*ahjgI5yuJBxzd2_B1cN-3Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Fig 1. The architecture of the proposed <strong class="bd la">MAE </strong>in this research. Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="d9fa" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">通过将掩码标记转移到小型解码器，可以减少计算量。因此，我们可以实现高屏蔽率(例如75%)，因此，我们可以优化精度，同时，在少数小块(例如25%)上训练模型。此外，预训练时间可以减少3倍或更多(同样，可以减少存储器消耗)。</p><h1 id="53dc" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">相关著作</h1><h2 id="644f" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">蒙面语言建模(+同行如<a class="ae jn" href="https://arxiv.org/pdf/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae jn" href="https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd" rel="noopener" target="_blank"> GPT </a>等。)</h2><ul class=""><li id="afd4" class="kk kl hh jq b jr lq ju lr jx mm kb mn kf mo kj lw kq kr ks bi translated">这些是成功的模型，已用于<strong class="jq hi"> NLP </strong>中的<strong class="jq hi">预训练</strong>。这些为<strong class="jq hi">顺序输入</strong>提供信息，以预测缺失的<strong class="jq hi">内容。</strong>此外，这些也是可扩展的。</li></ul><h2 id="1640" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">自动编码</h2><ul class=""><li id="bc19" class="kk kl hh jq b jr lq ju lr jx mm kb mn kf mo kj lw kq kr ks bi translated">这个经典方法包括两个主要部分:<strong class="jq hi">编码器</strong>(将输入映射到潜在表示)和<strong class="jq hi">解码器</strong>(重建输入)。比如<strong class="jq hi"> <em class="lv"> PCA </em> </strong>，<strong class="jq hi"> <em class="lv"> K-means </em> </strong>，<strong class="jq hi"><em class="lv">DAE</em></strong><em class="lv"/>(去噪自动编码器)等。</li></ul><h2 id="78b9" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">掩蔽图像编码方法</h2><ul class=""><li id="9df5" class="kk kl hh jq b jr lq ju lr jx mm kb mn kf mo kj lw kq kr ks bi translated">这些从图像中学习表征。</li></ul><h2 id="b8ae" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">自我监督学习</h2><ul class=""><li id="5184" class="kk kl hh jq b jr lq ju lr jx mm kb mn kf mo kj lw kq kr ks bi translated">在这里，工程师们往往会集中精力做各种文本任务进行前期训练，比如<a class="ae jn" href="https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607#:~:text=Contrastive%20learning%20is%20a%20machine,points%20are%20similar%20or%20different%20.&amp;text=In%20essence%2C%20contrastive%20learning%20allows,to%20do%20the%20same%20thing." rel="noopener" target="_blank">对比学习</a>；这模拟了两个或多个视图之间的图像相似性和不相似性。这些都与<a class="ae jn" href="https://towardsdatascience.com/tagged/data-augmentation" rel="noopener" target="_blank">数据增强</a>紧密相连。</li></ul><h1 id="34a6" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">方法</h1><p id="e214" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">本研究中提出的<strong class="jq hi"> MAE </strong>并不复杂；一个<strong class="jq hi">简单的自动编码器</strong>，它使用<strong class="jq hi">局部观察</strong>(输入图像不完整)，然后结果图像是<strong class="jq hi">完整的</strong> ( <em class="lv">见图1 </em>)。这种自动编码器几乎和以前的(经典)自动编码器一样，除了它的<em class="lv">非对称架构</em>不同于其他。这种设计让模型不在图像的所有像素上训练。</p><h2 id="8e46" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">掩饰</h2><p id="30ba" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">研究人员将一幅图像分割成<strong class="jq hi">规则的非重叠小块，</strong>然后对小块的子集进行采样，并掩盖其余部分(例如移除)。本研究中使用的策略(<strong class="jq hi"><em class="lv"/></strong>)是直接的:<strong class="jq hi">随机抽样</strong>小块(无替换)，遵循<a class="ae jn" href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">均匀分布</strong> </a> <strong class="jq hi"> </strong>(避免了潜在的中心偏差)。</p><p id="fea3" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">高遮蔽率(补片移除率)的结果会显著消除大量补片，因此产生了一个无法通过从可见的相邻补片外推来简单解决的任务(<em class="lv">见图2–4</em>)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/a3c6f49b28b56b91016ea9d06239a3d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3R85pbYOcKdyYGLbHHnykA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Fig. 2. Example results on <strong class="bd la">ImageNet </strong>validation images, the left is the <strong class="bd la">masked image </strong>the middle is <strong class="bd la">MAE reconstruction </strong>and, the right is the <strong class="bd la">ground-truth. </strong>Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/f3f2f4ef12df8f3cd5f5ef9946dc13f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0b2WGTH5ORuwTZmHPzqUSA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Fig. 3. Example results on <strong class="bd la">COCO </strong>validation images, using an <strong class="bd la">MAE</strong> <strong class="bd la">trained </strong>on ImageNet. Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/cbe0540391108482149ca2e42e1c01b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*5Cs60X-1HrSO8BGUE02GJQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Fig. 4. Reconstructions of <strong class="bd la">ImageNet</strong> validation images, using an <strong class="bd la">MAE pre-trained </strong>(masking ratio 75–95%). Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><h2 id="d809" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">MAE编码器</h2><p id="15af" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">这里的编码器是<strong class="jq hi"> ViT </strong> ( <a class="ae jn" href="https://arxiv.org/pdf/2010.11929" rel="noopener ugc nofollow" target="_blank"> <em class="lv">视觉转换器</em> </a>)并且仅仅应用于<em class="lv">可见的、未被遮盖的补丁</em>。本研究中的编码器通过使用具有<strong class="jq hi"><em class="lv"/></strong>位置嵌入的<strong class="jq hi"><em class="lv"/></strong>来嵌入补丁，然后通过一系列<strong class="jq hi"> <em class="lv">变换块</em> </strong>来操作结果集。顺便说一下，在这项研究中，编码器只对全部数据的25%进行了处理。由于这个原因，<strong class="jq hi">屏蔽补丁</strong>被消除了，没有使用make令牌，这让我们只用大数据集的很小一部分来训练非常大的编码器。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ms mt l"/></div></figure><h2 id="88c2" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">MAE解码器</h2><p id="cceb" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">解码器的输入是全套的<strong class="jq hi">令牌，</strong>包括(i) <strong class="jq hi">编码的可见补丁</strong>。掩码令牌可以被描述为显示要预测的<strong class="jq hi"> <em class="lv">缺失补丁</em> </strong>的存在的<strong class="jq hi">共享倾斜向量</strong>。由于将<strong class="jq hi">位置嵌入<em class="lv"> </em> </strong>添加到记号，掩模记号将具有关于它们在图像中的位置(位置)的<strong class="jq hi">信息。</strong></p><ul class=""><li id="c49b" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj lw kq kr ks bi translated"><strong class="jq hi"> <em class="lv">解码器</em> </strong>仅用于<strong class="jq hi">预训练</strong>时进行图像重建，<strong class="jq hi"> <em class="lv">编码器</em> </strong>仅用于<strong class="jq hi">生成图像表示</strong>进行识别。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ms mt l"/></div></figure><h2 id="b584" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">重建目标</h2><p id="6579" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">解码器的输出是代表一个小块的像素值的矢量，解码器的最后一层是一个<em class="lv">线性投影。</em>使用的损失函数是在<em class="lv">像素空间</em>中的<strong class="jq hi">重建的</strong>和<strong class="jq hi">原始图像</strong>之间的<em class="lv"> MSE </em> ( <a class="ae jn" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">均方误差</strong> </a> <strong class="jq hi"> ) </strong>。</p><h2 id="a261" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">简单实现</h2><p id="7758" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">实施<em class="lv"> MAE预培训非常简单，步骤如下:</em></p><ol class=""><li id="cde5" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated"><strong class="jq hi">为每个补丁生成</strong>一个<strong class="jq hi">令牌</strong>(怎么做？通过具有附加位置嵌入的线性投影)</li><li id="197e" class="kk kl hh jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated"><strong class="jq hi">随机洗牌</strong>令牌列表<strong class="jq hi"> <em class="lv">随机</em> </strong>然后，<strong class="jq hi">删除</strong>列表的最后一部分(基于屏蔽比率)</li></ol><ul class=""><li id="9409" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj lw kq kr ks bi translated">这一过程生成<em class="lv">一小部分令牌</em>(没有替换的采样补丁)</li></ul><p id="2eca" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">3.在编码之后，掩码标记的列表被添加到编码补丁的列表中，并且<em class="lv">去混洗</em>这个完整的列表(颠倒随机混洗操作)以与它们的目标相等。</p><ul class=""><li id="497f" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj lw kq kr ks bi translated">不需要特定的任务(<em class="lv">，例如稀疏操作</em>)。这使得我们的工作更快。</li></ul><h1 id="85f3" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">ImageNet实验</h1><p id="3e47" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">作者在<strong class="jq hi"> ImageNet-1K (IN1K) </strong>上做了<strong class="jq hi">自我监督预训练</strong>，用<strong class="jq hi">端到端微调</strong>或<strong class="jq hi">线性探测来评估表示。</strong></p><p id="6896" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">基线:</strong><a class="ae jn" href="https://arxiv.org/pdf/2010.11929" rel="noopener ugc nofollow" target="_blank"><strong class="jq hi">ViT-Large</strong></a><strong class="jq hi"/>(Vision Transformer(ViT)模型在ImageNet-21k (14M图像，21，843类)上以224x224的分辨率预训练，并在ImageNet 2012 (1M图像，1，000类)上以384x384的分辨率微调)用作脊线。</p><h2 id="12d8" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">主要属性</h2><h2 id="ddfe" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated"><strong class="ak">掩蔽比:</strong></h2><p id="4ff2" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">从图5中我们可以理解掩蔽比的影响作用。微调和线性探测的比率都是75%(与BERT相反，掩蔽比率为15%)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/982d7768ee1c36917a29f7093345b9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Qrx2Z_HL2RNkNCaGE3r5MA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Fig.5 <strong class="bd la">Masking ratio. </strong>Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="4920" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">从<em class="lv">图5 </em>中，我们可以得出结论，线性探测和微调结果遵循<em class="lv">不同的</em>趋势。对于微调，它更像一个对比率不太敏感的倒U形，而对于线性探测，精度逐渐上升，直到达到最佳百分比。</p><h2 id="004f" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">解码器的设计</h2><p id="9562" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated">我们可以灵活地设计解码器的结构。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mv"><img src="../Images/e6decce4f5524d716199501a82f3bffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4BQGvGxXoerqP31rBtIlA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="85f7" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi"><em class="lv"/></strong>(一)足够深的解码器对于线性探测是严重的。我们可以将它解释为像素重建任务和识别任务之间的空白空间:自动编码器中的最后几层更专门用于重建，但与识别更不相关。合适的深度解码器可以计算重构特殊化，将潜在表示留在更抽象的层次上。</p><p id="8a17" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi"><em class="lv"/></strong>(b)解码器宽度是通道的数量。作者使用了在微调和线性探测下工作良好的512-d。不太宽(窄)的解码器也可以很好地配合微调。</p><h2 id="5977" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">掩码令牌</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/24ef98b07dfc2601ed33e77f05c09d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*Y4XbYREYjCZUh-9z2qiNoQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="5f9f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">MAE设计的关键是<strong class="jq hi">跳过<strong class="jq hi">编码器</strong>中的</strong>掩模toke，然后将其应用于<strong class="jq hi">轻量解码器</strong>。这种跳过大大降低了训练计算成本。整体FLOPs减少3.3倍<strong class="jq hi"> <em class="lv"> (c) </em> </strong>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/aa920548771b32fbdb009dcf930bf9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*Dy5AVNy98DDpCGzw88FY_g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Table 2. <strong class="bd la">Wall-clock time</strong> of the MAE training (800 epochs). Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="da33" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">挂钟时间加速比一个模块(较小的解码器)、一个ViT-H(较大的编码器)或两者都大。</p><h2 id="0f10" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">重建目标</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/0504bc67b44caea0362f8dd546c5b69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*-HTi0WHctWRqngnzgekj1A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="f0ca" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">多种结构用于比较。结果是在<strong class="jq hi">像素</strong>上用<strong class="jq hi">没有归一化</strong>。这提高了局部对比度<strong class="jq hi"/>。</p><h2 id="3797" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">数据扩充</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/6fcada965c9231b218e6264aac7df128.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*Q_hFP22G8gv_3SM3WxWsdw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="5e2c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">在这里，我们可以实现<strong class="jq hi"><em class="lv"/></strong>的<strong class="jq hi">对模型的影响。研究人员在两种状态下都使用了<strong class="jq hi"><em class="lv"/></strong><em class="lv">固定</em>或<em class="lv">随机大小</em>。</strong></p><h2 id="9392" class="ly kz hh bd la lz ma mb le mc md me li jx mf mg lk kb mh mi lm kf mj mk lo ml bi translated">掩模取样</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/25eb09524752bcf8d3bf90f5a2fdafc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*FaFZ6Mj_HaHq_BE9YkOzMg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">[1]</a></figcaption></figure><p id="bab3" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">下面是不同掩码采样策略的比较结果。简单随机抽样优于MAE它允许更高的掩蔽比(这提供了显著的<strong class="jq hi">加速</strong>和良好的<strong class="jq hi">精度</strong>)。</p><h1 id="a936" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">结论</h1><p id="515d" class="pw-post-body-paragraph jo jp hh jq b jr lq ii jt ju lr il jw jx ls jz ka kb lt kd ke kf lu kh ki kj ha bi translated"><strong class="jq hi">计算成本</strong>是<strong class="jq hi"> NLP </strong>和<strong class="jq hi">计算机视觉</strong>任务的一个基本因素。<strong class="jq hi">不复杂的模型</strong>是<strong class="jq hi">可扩展的</strong>是工程师和科学家的<strong class="jq hi">主要关注点</strong>。<strong class="jq hi">在NLP </strong>，<strong class="jq hi">简单的自我监督学习算法</strong>从指数缩放模型中获益。<strong class="jq hi">在计算机视觉</strong>中，<strong class="jq hi">实际预训练样本</strong>被有效监督。</p><p id="e82b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">在这个<strong class="jq hi">研究</strong>中，研究人员提出了一个变通方法，随机消除<strong class="jq hi">补丁</strong> ( <strong class="jq hi">不是</strong>消除<strong class="jq hi">对象</strong>)，也就是<strong class="jq hi">最有可能</strong> <em class="lv">不是</em>形成一个语义段。此外，提出的MAE <strong class="jq hi">重建</strong>像素(这些像素是<em class="lv">而不是</em>语义实体)。这种主动方法在现实世界的计算机视觉任务中是有益和实用的，特别是对于复杂程度为<strong class="jq hi">的<strong class="jq hi">繁重任务</strong>。通过消除随机补丁，该模型需要<strong class="jq hi">更少的能量</strong>和<strong class="jq hi">更少的输入</strong>用于<strong class="jq hi">预训练</strong>。</strong></p><p id="dccd" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">参考资料:</p><ol class=""><li id="fcc9" class="kk kl hh jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">He，k .等人，<em class="lv">掩蔽自动编码器是可伸缩视觉学习器。arXiv预印本arXiv:2111.06377，2021。</em></li></ol><blockquote class="na nb nc"><p id="7c79" class="jo jp lv jq b jr js ii jt ju jv il jw nd jy jz ka ne kc kd ke nf kg kh ki kj ha bi translated"><strong class="jq hi">请注意，这篇帖子是为了我将来可能的研究在没有完全阅读</strong> <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">论文</strong> </a> <strong class="jq hi">的情况下，回看和复习关于这个题目的材料。本文使用的所有图片来源均为</strong> <a class="ae jn" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi">原文</strong> </a> <strong class="jq hi">。</strong></p><p id="26fc" class="jo jp lv jq b jr js ii jt ju jv il jw nd jy jz ka ne kc kd ke nf kg kh ki kj ha bi translated">如果发现任何错误，请告诉我。同时，你可以在Twitter <a class="ae jn" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>或者LinkedIn <a class="ae jn" href="https://www.linkedin.com/in/reza-yazdanfar-b69055156/" rel="noopener ugc nofollow" target="_blank">这里</a>联系我。最后，如果你有任何想法，我都愿意讨论，你唯一需要的就是在LinkedIn或twitter上给我发消息。🙂</p></blockquote><div class="ng nh ez fb ni nj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">medium.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx jh nj"/></div></div></a></div></div></div>    
</body>
</html>