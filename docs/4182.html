<html>
<head>
<title>All you need to know about Linear Regression (Theory)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于线性回归(理论)你需要知道的</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/all-you-need-to-know-about-linear-regression-theory-9594f62515?source=collection_archive---------3-----------------------#2022-12-21">https://medium.com/mlearning-ai/all-you-need-to-know-about-linear-regression-theory-9594f62515?source=collection_archive---------3-----------------------#2022-12-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归是一种机器学习(ML)算法，用于估计定量变量之间的关系。它基于寻找输入变量和输出变量之间的线性关系的概念。</p><p id="9911" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用线性回归来:</p><ol class=""><li id="6d22" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">理解从属特征和独立特征之间关系的强度。</li><li id="da40" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">对于给定的自变量数值，预测因变量的数值。</li></ol><p id="66e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归的例子:</p><ol class=""><li id="b1d4" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">假设我们有一个只有两列(身高和体重)的数据集。目的是创建一个以身高为输入并预测体重的模型。</li><li id="58ac" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">创建一个以房间数量为输入并预测房价的模型。</li></ol><h2 id="bb57" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">线性回归的图形直觉</h2><p id="4274" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kn ir is it ko iv iw ix kp iz ja jb ha bi translated">线性回归的主要目的是找到最佳拟合线，使得实际点和预测点之间的差值之和<strong class="ig hi">最小</strong>。预测点和真实点之间的差异称为残差或误差。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/289d5e209fc72cc3c85202a6da102029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dXAn_8uNn6jFEkx3HIbr-Q.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Linear Regression</figcaption></figure><p id="f2f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，在上图中，蓝色的点是预测点，预测点与真实点(红线上的点)之间的差异称为残差或误差。线性回归试图减少这些误差或残差。所以在线性回归中，误差或残差的总和应该是最小的。</p><h2 id="8c73" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">直线方程</h2><p id="ee11" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kn ir is it ko iv iw ix kp iz ja jb ha bi translated">为了理解线性回归，首先理解线性方程或直线方程的概念是很重要的。</p><ul class=""><li id="c85e" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lg ji jj jk bi translated"><strong class="ig hi"> y = mx + b，</strong> m是斜率，b是截距。</li><li id="1668" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lg ji jj jk bi translated">它也可以写成，在大多数研究论文中，它被写成:</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lh"><img src="../Images/66b2e71eb2d031c354616a685a68f0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*XAFTH7o84LqPEiz-SxkA6w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Equation of a Straight Line</figcaption></figure><ol class=""><li id="b3ca" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">截距:</strong>在最佳拟合直线中，当自变量(x)的值等于零时，最佳拟合直线与y轴相交的点称为截距。</li><li id="28ec" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi">斜率:</strong>它告诉我们随着自变量(x)的单位运动，因变量(y)的运动是什么</li></ol><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es li"><img src="../Images/0292c31d3e255b0a70470142d6ba21a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK2PfoaPjOHsEc2tWvB1eg.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Image Source: <a class="ae lj" href="https://www.w3schools.com/datascience/ds_linear_slope.asp" rel="noopener ugc nofollow" target="_blank">w3schools</a></figcaption></figure><h2 id="c6a7" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">价值函数</h2><p id="4e26" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kn ir is it ko iv iw ix kp iz ja jb ha bi translated">线性回归的成本函数用于评估模型根据自变量的值预测因变量的能力。它还有助于找到模型参数(斜率和截距)的最佳值，使因变量的预测值和实际值之间的误差最小化。<strong class="ig hi">成本函数是模型性能的衡量标准。</strong></p><p id="377b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">示例:</strong>假设我们有一个只有两列高度和年龄的数据集。我们想根据人们的年龄来预测他们的身高。我们可以通过对数据进行直线拟合来使用线性回归。</p><p id="a60e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，成本函数将用于测量人的预测身高和实际身高之间的差异。目标是找到模型参数值，使预测高度和实际高度之间的差异最小。θ0和θ1是参数。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lk"><img src="../Images/4c954839081a5bb42ee477c3e48e2e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*MT1O5EtDJfZvJOzz4Atd_A.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Cost Function</figcaption></figure><h2 id="a6d5" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">梯度下降</h2><p id="ac37" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kn ir is it ko iv iw ix kp iz ja jb ha bi translated">如前所述，成本函数用于测量预测点和实际点之间的差异或残差，因此我们需要降低成本函数。这可以使用<strong class="ig hi">梯度下降来完成。</strong></p><p id="6222" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降是一种优化算法，用于查找模型参数(如斜率和截距)的值，以最小化预测数据点和实际数据点之间的误差。它的工作原理是从参数的初始值开始，然后在使误差最小化的方向上或在全局最小值的方向上调整它们。我们需要接近全局极小点或最小成本点，只有这样成本函数才会降低。</p><div class="kr ks kt ku fd ab cb"><figure class="ll kv lm ln lo lp lq paragraph-image"><img src="../Images/06b7c314eb0604f710947e5956510a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*S-AV7aX00K1oMTQHidgw_g.png"/></figure><figure class="ll kv lr ln lo lp lq paragraph-image"><img src="../Images/c10be491fc6bda61996247b12ae0acf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nlvblFZKBOJVCyb7yDuQlQ.png"/><figcaption class="lc ld et er es le lf bd b be z dx ls di lt lu">Source Image1: <a class="ae lj" href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">Saugatbhattarai</a>, Source Image2: <a class="ae lj" href="https://www.javatpoint.com/gradient-descent-in-machine-learning" rel="noopener ugc nofollow" target="_blank">Javapoint</a></figcaption></figure></div><p id="9638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">举例:</strong>假设你站在山顶，试图找到到达山脚的最快路径。你可以选择一条直接下山的路线，这可能不是最快最安全的路线，或者你可以选择一系列较小的步骤，每次根据你面前的地形稍微调整你的方向。这类似于梯度下降的工作原理。</p><h2 id="7e97" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">学习率</h2><p id="439e" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip kn ir is it ko iv iw ix kp iz ja jb ha bi translated">学习率决定收敛速度，并用于确定调整模型参数时优化算法所采用的步长。如果学习率太高，将导致剧烈的更新，并且收敛可能不会发生。如果学习率太小，将需要大量的时间来达到收敛。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lv"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Source: <a class="ae lj" href="https://www.niser.ac.in/~smishra/teach/cs460/2020/lectures/lec8/" rel="noopener ugc nofollow" target="_blank">Niser.ac.in</a></figcaption></figure><h2 id="ad85" class="jq jr hh bd js jt ju jv jw jx jy jz ka ip kb kc kd it ke kf kg ix kh ki kj kk bi translated">成本函数</h2><ul class=""><li id="3372" class="jc jd hh ig b ih kl il km ip lw it lx ix ly jb lg ji jj jk bi translated"><strong class="ig hi">均方误差(MSE): </strong>取预测点和实际点的平方差之和，然后除以观测总次数计算得出。它由以下公式给出:</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lz"><img src="../Images/d1058fbed805d1eabee4f40a7f84543c.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*lQ-ukyQKDKiyWenD5dl0sQ.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Mean Squared Error</figcaption></figure><ul class=""><li id="bf21" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lg ji jj jk bi translated"><strong class="ig hi">均方根误差(RMSE): </strong>取MSE的平方根给出。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ma"><img src="../Images/5aa9853862b92fe56246ab5c66764a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*wKh-xfk9NocFDspBnB6K4Q.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Root Mean Squared Error</figcaption></figure><ul class=""><li id="aa1d" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lg ji jj jk bi translated"><strong class="ig hi">平均绝对误差(MAE): </strong>平均绝对误差的公式与MSE非常相似，但我们不是取实际值和预测值之差的平方根，而是取其绝对值。它由下面的公式给出。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es mb"><img src="../Images/177a9d5ffa8cb8bcc01ac0eb07ef231a.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*hDVmGPjGZiurq-YFhqTK7Q.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Mean Absolute Error</figcaption></figure><p id="00d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mc">感谢阅读本文！如果你有任何问题，请在下面留言。可以关注我的</em><a class="ae lj" href="https://www.linkedin.com/in/devsachin0879/" rel="noopener ugc nofollow" target="_blank"><em class="mc">Linkedin</em></a><em class="mc">和</em><a class="ae lj" href="https://github.com/devsachin0879" rel="noopener ugc nofollow" target="_blank"><em class="mc">GitHub</em></a><em class="mc">。</em></p><div class="md me ez fb mf mg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hi fi z dy ml ea eb mm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu la mg"/></div></div></a></div></div></div>    
</body>
</html>