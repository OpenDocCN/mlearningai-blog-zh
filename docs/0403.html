<html>
<head>
<title>Bert &amp; Tricks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特和诡计</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/bert-tricks-5950007180e2?source=collection_archive---------4-----------------------#2021-04-05">https://medium.com/mlearning-ai/bert-tricks-5950007180e2?source=collection_archive---------4-----------------------#2021-04-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="96f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个博客旨在详细描述伯特和他使用的所有非凡的技巧。我们将把博客分成两部分:</p><ul class=""><li id="c760" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">变压器和层标准化</li><li id="15eb" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae jq" href="https://zzd2012victor.medium.com/bert-standing-on-the-shoulders-of-giants-23c8f2679447" rel="noopener">伯特&amp;阿尔伯特(伯特的轻量级版本)</a></li></ul></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><blockquote class="jy jz ka"><p id="381f" class="ie if kb ig b ih ii ij ik il im in io kc iq ir is kd iu iv iw ke iy iz ja jb ha bi translated"><strong class="ig hi"> LSTM死了。变形金刚万岁！</strong></p></blockquote><p id="868d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个非常有趣的关于变形金刚的youtube视频的标题。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kk kl l"/></div></figure><h1 id="91e0" class="km kn hh bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">动机</h1><p id="7bee" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">和RNN一起训练时，我们只能一个条目一个条目地计算。这样，我们就不可能用GPU来提高它的训练时间了。如果我们使用CNN来处理长序列问题，计算复杂度随着句子中两个单词之间距离的增加而增加。</p><p id="da4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，以前大多数注意力机制都是与RNN一起使用的。因此，RNN的局限性仍然存在。注意力比以前更像一个帮助者。</p><p id="a9b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在《变形金刚》中，注意力成了唯一被利用的东西。它可以直接链接序列中的任意两个节点，而不考虑输入和输出中的单词距离。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="2f1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与Transformer之前的主流模型不同，Transformer完全基于自我关注机制来绘制输入和输出之间的全局依赖关系，完全无需递归和卷积。</p><blockquote class="jy jz ka"><p id="c2eb" class="ie if kb ig b ih ii ij ik il im in io kc iq ir is kd iu iv iw ke iy iz ja jb ha bi translated">为什么在处理NLP问题时，自我关注比RNN(递归神经网络)和CNN(卷积神经网络)强大得多？<a class="ae jq" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">论文</a>的作者将自我关注层的不同方面与RNN和CNN进行了比较，以将(x_1，…，x_n)映射到(z_1，…，z_n)。他们发现有三个原因:</p></blockquote><ul class=""><li id="608d" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">每层的总计算复杂度更低。</li><li id="aed2" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">更多可以并行完成的计算量。</li><li id="87c9" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">网络中长程相关性之间的路径长度更短</li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/fbfbd54cd49abdf1341c3862a9f4cc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUUotHqDkbM0iZf10_Z7pQ.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Cited from <a class="ae jq" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a></figcaption></figure><p id="24b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上表可以看出，相比轮回层，自我注意有O(1)序贯操作，但轮回层有O(n)序贯操作。这是因为自我注意是对整个句子进行操作的，而循环层是对条目进行操作的。很明显，当序列长度“n”小于表征维度“d”时，自我注意每层的总计算复杂度较小。这是机器翻译中最先进的模型非常常用的设置。此外，正如我们所知，RNN的顺序计算机制阻止我们在GPU上并行化计算，自我关注放弃了顺序计算机制。它可以很容易地在GPU上并行化。</p><p id="bfb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与卷积层相比，自关注的主要优点是它可以像卷积层一样不考虑核的大小而进行映射。对于卷积层，大小为k &lt; n does not connect all pairs of input and output positions. However, a very large kernel generally means needs of more computing resource.</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="a439" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">The model architecture of transformers is basically an encoder-decoder model.</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es ma"><img src="../Images/004d8d990bce7906ddb02cff3b6565c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*krAB6b27e54w-4x8"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model Architecture of Transformers</figcaption></figure><p id="c360" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">The encoder is composed of 6 identical sublayers — multihead-attention followed by an simple feed-forward layer.</p><p id="9d02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">The decoder is also composed of 6 identical sublayers — two multihead-attention followed by an simple feed-forward layer. Note that the second multihead attention layer takes the output from the encoder as part of its input.</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mb"><img src="../Images/b9692cfa8e2048231f4cdec930f6c39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oF6WwtnQXoNt3OUXidhQGg.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model Architetcure shown in transformer paper</figcaption></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="9289" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">What is the basic idea of self-attention? As we can see below, the attention can figure out the relationships in between different parts of a sentence. Like here, it figures out the second <em class="kb"> it </em>的核由动物<em class="kb">表示。在我看来，它的表现就像人类一样。作为人类，我们使用这种技术来帮助我们理解一个句子。</em></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mc"><img src="../Images/3678bb3707ac27c63c443e93f3f623e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ekvuutqUUkFDlgca"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">self-attention</figcaption></figure><p id="e0e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，自我关注是如何实现的呢？它使用从输入中生成的三个值—键、查询和值。它只是将输入的嵌入乘以三个不同的参数矩阵——W _ K、W_Q、W_V。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es md"><img src="../Images/75ccef0820c26df86f32485f16343d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fzz17IfTmcFkOnLH"/></div></div></figure><p id="9e7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，它会执行以下操作:</p><ol class=""><li id="cf77" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb me ji jj jk bi translated">计算分数，它决定了对句子其他部分(输入)的关注程度。</li><li id="0992" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated">然后将分数除以sqrt(模型维度)。为什么？作者怀疑，对于较大的dk值，点积的大小会变大，从而将softmax函数推到梯度极小的区域。为了抵消这种影响，他们通过<em class="kb"> 1/sqrt(模型尺寸)来缩放产品。</em></li><li id="e275" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated">在分数的softmax和值之间做矩阵乘法。然后把它们加在一起。这是这个自我关注块的最终输出。</li></ol><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mf"><img src="../Images/64f05d40f97366624e3b0eeb04f59be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m4bnhP0xgzJRt4Bw"/></div></div></figure><p id="7b4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个过程在论文中用下图描述:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mg"><img src="../Images/941fd22a004dd107c5be93888aa63bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*Icrh4yvR9QTCbDM8xbG11A.png"/></div></figure><pre class="kf kg kh ki fd mh mi mj mk aw ml bi"><span id="077b" class="mm kn hh mi b fi mn mo l mp mq"># Linear projections<br/>Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)<br/>K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)<br/>V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)<br/>        <br/># Split and concat<br/>Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) <br/>K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) <br/>V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h)<br/>————————————————<br/>Link：<a class="ae jq" href="https://blog.csdn.net/dakenz/article/details/85150676" rel="noopener ugc nofollow" target="_blank">https://blog.csdn.net/dakenz/article/details/85150676</a></span></pre></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="c814" class="km kn hh bd ko kp mr kr ks kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj bi translated">请注意！</h1><p id="b98e" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">我们将在《变形金刚》一文中介绍最好的想法——多头注意力。这一点的总体思路如下所示:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mw"><img src="../Images/1c54bead1f380557867106c492f39253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-MyanFXRUmUYpfqY"/></div></div></figure><p id="3ac7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如作者所宣称的，多头注意力允许模型在不同位置共同注意来自不同表征子空间的信息。因此，模型可以学习更多的关系，而不仅仅是一个关系。这对模型理解一句话肯定是有帮助的。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="545a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于在这个模型中没有递归和卷积，为了利用序列的顺序，作者引入了“位置编码”,它注入了关于序列中每个记号的相对位置的一些信息。</p><p id="6311" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在论文中，他们选择了不同频率的正弦和余弦函数:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mx"><img src="../Images/1c3dc6d3c9430fa14612f0a7cd3952cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/0*lVVboayoco076AYH"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">sine and cosine functions of different frequencies</figcaption></figure><p id="3022" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们从上面的模型架构图中看到的，位置编码被直接添加到输入嵌入中。</p><p id="9962" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">代码:</p><pre class="kf kg kh ki fd mh mi mj mk aw ml bi"><span id="4fb8" class="mm kn hh mi b fi mn mo l mp mq">def positional_encoding(inputs,<br/>                        num_units,<br/>                        zero_pad=True,<br/>                        scale=True,<br/>                        scope="positional_encoding",<br/>                        reuse=None):<br/>    '''Sinusoidal Positional_Encoding.<br/>    Args:<br/>      inputs: A 2d Tensor with shape of (N, T).<br/>      num_units: Output dimensionality<br/>      zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero<br/>      scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)<br/>      scope: Optional scope for `variable_scope`.<br/>      reuse: Boolean, whether to reuse the weights of a previous layer<br/>        by the same name.<br/>    Returns:<br/>        A 'Tensor' with one more rank than inputs's, with the dimensionality should be 'num_units'<br/>    '''</span><span id="5e44" class="mm kn hh mi b fi my mo l mp mq">N, T = inputs.get_shape().as_list()<br/>    with tf.variable_scope(scope, reuse=reuse):<br/>        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])</span><span id="c14e" class="mm kn hh mi b fi my mo l mp mq"># First part of the PE function: sin and cos argument<br/>        position_enc = np.array([<br/>            [pos / np.power(10000, 2.*i/num_units) for i in range(num_units)]<br/>            for pos in range(T)])</span><span id="e914" class="mm kn hh mi b fi my mo l mp mq"># Second part, apply the cosine to even columns and sin to odds.<br/>        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i<br/>        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1</span><span id="ce61" class="mm kn hh mi b fi my mo l mp mq"># Convert to a tensor<br/>        lookup_table = tf.convert_to_tensor(position_enc)</span><span id="ab6d" class="mm kn hh mi b fi my mo l mp mq">if zero_pad:<br/>            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),<br/>                                      lookup_table[1:, :]), 0)<br/>        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)</span><span id="9327" class="mm kn hh mi b fi my mo l mp mq">if scale:<br/>            outputs = outputs * num_units**0.5</span><span id="30bd" class="mm kn hh mi b fi my mo l mp mq">return outputs<br/>————————————————<br/>Link：<a class="ae jq" href="https://blog.csdn.net/dakenz/article/details/85150676" rel="noopener ugc nofollow" target="_blank">https://blog.csdn.net/dakenz/article/details/85150676</a></span></pre><p id="4fe2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您想查看位置编码的效果:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mz"><img src="../Images/203cde254245aceb4ea4a9bb66a02a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ewmv7A7EjqFcdoYS"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">effect of positional encodings</figcaption></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="4a75" class="km kn hh bd ko kp mr kr ks kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj bi translated">图层规范化与批量规范化</h1><p id="720c" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">BN对每一批做归一化处理。假设我们有一批尺寸为10和3的特征。我们计算10个样本上每个特征的平均值和标准偏差，并进行归一化。</p><p id="7606" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">相反，LN对每个样本上的特征进行归一化。比如每个人都有{体重，身高，年龄}。我们计算1个样本的这三个特征的平均值和标准差。我们做标准化。</p><p id="a8be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这似乎是如此有线做这样的正常化。为什么要用？</p><p id="0574" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个原因是，批处理规范化在处理非常小的批处理或序列模型(如RNN)时表现很差。然而，LN没有这样的限制，因为它在每一层上进行标准化。LN不关心网络有多长，批量有多小。</p><p id="b302" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二个原因是在NLP问题中，BN是在对每个位置上的每个单词做归一化处理。这是违反直觉的吧？我们显然应该对一句话进行规范化。那是LN做的。</p><ul class=""><li id="62ba" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">C =序列长度(句子长度)</li><li id="a2ed" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">N =批量大小</li><li id="efba" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">h，W是嵌入维数。</li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es na"><img src="../Images/ffda88170810c2348f954fb5d40c8ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eOodBMoKvQPuzver"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Layer Norm vs. Batch Norm</figcaption></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="8ceb" class="km kn hh bd ko kp mr kr ks kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj bi translated">计算机视觉中的注意力</h1><p id="5eff" class="pw-post-body-paragraph ie if hh ig b ih lk ij ik il ll in io ip lm ir is it ln iv iw ix lo iz ja jb ha bi translated">目前，注意机制不仅应用于自然语言处理任务，也应用于计算机视觉任务。例如，在这篇<a class="ae jq" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，使用了<em class="kb">递归注意卷积神经网络</em>。它可以循环分析区域信息并从这些信息中提取特征。作者还使用了<em class="kb">注意力提议子网络</em> (APN)。APN从整体形象出发。然后，它循环生成子图像，并对这些子图像进行必要的预测。然后它用这些预测来做最后的预测。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es nb"><img src="../Images/3b7366cb81271ece80b438c87ff3a622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wbLYsMXMWSVrJjHf1rSbQ.png"/></div></div></figure><p id="cfaf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者举了一个例子，用注意力对鸟类进行分类。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es nc"><img src="../Images/875299962fe40555ccaa46fd5664545e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*iNg5M8KrWnFa4QZqF8RJRg.png"/></div></figure><p id="dd70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了整个图像的信息之外，当对鸟进行分类时，还包括像鸟的颜色、鸟的头部、鸟的喙的形状等信息。比花草树木等环境重要得多。环境信息应该像助手一样对待，否则我们应该忽略它们。本文中的注意机制可以帮助模型找出重要的信息，并给予这些特征更多的权重。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="f227" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们看看关于伯特的更多细节。</p><p id="245c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><ol class=""><li id="02ef" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb me ji jj jk bi translated">你所需要的只是关注:<a class="ae jq" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762.pdf</a></li><li id="f8b7" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated">图层归一化:<a class="ae jq" href="https://arxiv.org/pdf/1607.06450.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1607.06450.pdf</a></li><li id="16a5" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated"><a class="ae jq" href="https://blog.csdn.net/jiaowoshouzi/article/details/89073944" rel="noopener ugc nofollow" target="_blank">https://blog.csdn.net/jiaowoshouzi/article/details/89073944</a></li><li id="2ce7" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb me ji jj jk bi translated">凑近点看更好:递归注意卷积神经网络进行细粒度图像识别:<a class="ae jq" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ cvpr _ 2017/papers/Fu _ Look _ Closer _ to _ CVPR _ 2017 _ paper . pdf</a></li></ol></div></div>    
</body>
</html>