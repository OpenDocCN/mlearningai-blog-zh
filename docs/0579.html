<html>
<head>
<title>What are the differences in Pre-Trained Transformer-base models like BERT, DistilBERT, XLNet, GPT, XLNet, …</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预先训练好的基于变压器的模型有什么不同，比如BERT、DistilBERT、XLNet、GPT、XLNet等等</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/what-are-the-differences-in-pre-trained-transformer-base-models-like-bert-distilbert-xlnet-gpt-4b3ea30ef3d7?source=collection_archive---------0-----------------------#2021-05-19">https://medium.com/mlearning-ai/what-are-the-differences-in-pre-trained-transformer-base-models-like-bert-distilbert-xlnet-gpt-4b3ea30ef3d7?source=collection_archive---------0-----------------------#2021-05-19</a></blockquote><div><div class="ds gz ha hb hc hd"/><div class="he hf hg hh hi"><div class=""/><p id="0093" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">这篇文章是基于Transformer的著名模型的备忘单，并试图解释它们的独特性(尽管它们都基于相同的架构)。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/55dabd9b7b14b9e38dcee679a5185ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1xcM4aFdZieyAD2uM_OY2g.png"/></div></div></figure><p id="b327" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">Transformer架构和迁移学习的结合正在主宰自然语言处理世界。那里…</p></div></div>    
</body>
</html>