# 5 使用 Q 函数的深度强化学习

> 原文：<https://medium.com/mlearning-ai/5-deep-reinforcement-learning-with-q-functions-c79ee8ab4b2?source=collection_archive---------4----------------------->

这是我总结 Sergey Levine 教授主持的 CS285 讲座系列文章的第五篇。所有图片均取自他的讲座。[我写的这篇文章](https://samuelebolotta.medium.com/1-an-introduction-to-deep-reinforcement-learning-c5ab792af013)是对深度强化学习的介绍。[演员-评论家算法](https://samuelebolotta.medium.com/3-actor-critic-algorithms-779f14465b74)建立在我们在[这篇文章](https://samuelebolotta.medium.com/2-deep-reinforcement-learning-policy-gradients-5a416a99700a)中讨论的政策梯度框架之上。最重要的是，它们还增加了学习价值函数和 Q 函数，我们在本文中讨论过[。](https://samuelebolotta.medium.com/4-value-function-methods-17f2c898a00d)

虽然我们知道基于值的方法通常不能保证收敛，但实际上我们可以从中获得一些非常强大和非常有用的强化学习算法，这就是我们今天要讨论的内容。

## 在线 Q 迭代算法的问题

![](img/a86d1d4f7da006c71a986db419b109a3.png)

首先，我们了解到步骤 3 中的更新，即使它看起来像梯度更新，实际上并不是任何明确定义的目标的梯度——因此 Q-learning 不是梯度下降。

第二，当您一次对一个转换进行采样时，连续的转换是高度相关的。因此，我在时间步长 t 看到的状态很可能与我在时间步长 t+1 看到的状态非常相似，这意味着当我在步骤 3 中对这些样本采取梯度步骤时，我是在对高度相关的跃迁采取梯度步骤。这违反了随机梯度方法的普遍假设，并且有一个非常直观的原因不能很好地工作。为了理解为什么，让我们通过将目标值的等式直接插入梯度更新来重写算法。

![](img/5091fa979e8e9958271afba466314572.png)

你看到的一个接一个的状态将是强相关的，你的目标值总是在变化，所以即使你的优化过程看起来像监督回归，在某种意义上它有点像在追自己的尾巴——它试图赶上自己，然后从自己下面变出来，梯度没有解释这种变化。这可能是一个问题的原因是，如果你想象这是你的轨迹，你得到一些转换，你会局部过度适应这些转换，因为你看到非常相似的转换:

![](img/1d143c7dd1c60ea0a3ac871ea901edfa.png)

然后你会得到一些其他的过渡，然后你会对这些过度一点:

![](img/0f771b5b1429a64247e41ac21936e715.png)

诸如此类。然后，当你开始一个新的轨迹时，你的函数逼近器将会在它过度拟合到前一个轨迹的终点时停止，并且会再次变坏。因此，如果它一次看到所有的转换，它可能实际上已经准确地拟合了所有的转换，但是因为它一次看到这个非常局部的高度相关的窗口，它只有足够的时间来过度拟合每个窗口，而没有足够的更广泛的上下文来准确地拟合整个函数。

我们可以借用演员-评论家中的相同想法:我们讨论的特定解决方案是让多个工作人员收集不同的过渡，从所有这些工作人员中收集一批样本，更新该批并重复。这个过程原则上可以解决顺序状态高度相关的问题；顺序状态仍然是相关的，但是现在你从不同的工人那里得到了一批不同的转换，并且在工人之间，它们希望是不相关的。所以不能完全解决问题，但是可以缓解。当然，就像我们在 actor-critic 中了解到的那样，您可以拥有这个配方的异步版本，其中各个工作人员不等待同步点，而是向参数服务器查询最新的参数，然后按照他们自己的速度继续进行。

![](img/8a83ba8eaec3b0e470daff88b50faa3f.png)

事实上，通过 Q-learning，这种方法在理论上应该会更好，因为您甚至不需要工作人员使用最新的策略，所以您可能有一个稍微旧一点的策略正在对一些工作人员执行，这在理论上不是一个大问题。然而，有另一种解决相关样本问题的方法，这种方法很容易使用，并且在实践中效果很好:重放缓冲区。

## 重放缓冲区

让我们回想一下完全拟合的 Q 迭代算法:我们将使用我们想要的任何策略来收集转换数据集，然后我们将对转换数据集进行多次更新。因此，我们会用目标值标记所有转换，然后我们可能会对这些目标值进行大量梯度步骤回归，然后我们甚至可能会在收集更多数据之前返回并计算新的目标值:

![](img/aafb2c2b5709e46f4b609e7f2b75f75a.png)

在线 Q-learning 算法只是这种情况的特例，当 k 设置为 1 时，我们在第三步中采取一个梯度步骤，当然，任何收集策略都将起作用，只要它有广泛的支持。事实上，我们甚至可以完全省略第一步；我们可以从缓冲区加载数据。这种观点将 Q-learning 呈现为一种数据驱动的方法，其中我们只有一大堆转换，我们并不真正关心这些转换来自哪里，只要它们很好地覆盖了所有可能性的空间，我们只是在这些转换上进行越来越多的更新，在计算目标值和回归这些目标值之间交替。因此，如果我们愿意，我们仍然可以在第三步中采取一个梯度步骤，然后我们会得到一些看起来很像在线 Q-learning 的东西，只是没有数据收集。这给了我们一个带有重放缓冲器的 Q 学习算法的修改版本。

![](img/8cd8fbddf6527a6d1f0bd7eda5a61501.png)

在第一步中，我们不是在真实世界中实际采取步骤，而是简单地从缓冲区中采样一批多个跃迁，然后在第二步中，我们对该批中所有条目的梯度求和。所以每次在这个 for 循环中，我们可能会从缓冲区中抽取不同的批次，IID(独立同分布)。

我们在批次中有多个样本，所以我们有一个低的方差梯度，我们的样本不再相关，这满足随机梯度方法的假设。不幸的是，我们仍然没有真正的梯度，因为我们没有计算第二项的导数，但至少样本是不相关的。数据从哪里来？我们需要做的是，我们需要定期馈送重放缓冲区，因为最初我们的策略会非常糟糕，可能我们最初非常糟糕的策略不会访问空间中所有感兴趣的区域，所以我们仍然希望偶尔通过使用我们的最新策略来馈送重放缓冲区，可能通过一些 epsilon greedy 探索来收集一些数据，以实现更好的覆盖范围。

因此，我们有一个转换缓冲区，我们在其上进行策略外 Q-learning，然后定期将一些策略部署回世界，例如带有 epsilon greedy exploration 的 greedy 策略，以收集更多的数据，并带回更多的转换以添加回我们的缓冲区。这将刷新我们的行为缓冲区，有望获得更好的覆盖率。

把所有这些放在一起，我们可以设计一个带有重放缓冲区的完整的 Q 学习配方。第一步，使用一些策略收集转换数据集，并将这些数据添加到您的缓冲区中。第二步，从 B 中抽取一批样本，然后对该批样本进行学习，对该批样本进行 Q 学习伪梯度求和。因此，我们将计算批次中每个条目的目标值，然后我们将计算当前 Q 值减去目标值乘以导数，然后对整个批次求和。因为我们对整个批次求和，所以我们得到了较低的方差梯度，并且因为我们从缓冲区对批次 IID 进行采样，所以只要我们的缓冲区足够大，我们的样本就会去相关。我们可以重复这个过程 k 次。

## 目标网络

在这一点上，我们几乎已经开发了一个实用的深度 Q 学习算法，我们可以实际使用，但我们需要讨论另一个组件，以真正获得一个稳定可靠的过程。我们通过引入一个重放缓冲区来处理样本相关的问题，在该缓冲区中，我们存储迄今为止收集的所有数据，并且每次我们必须对 Q 函数的参数采取步骤时，我们实际上是使用从缓冲区中采样的一批跃迁来采取该步骤的。

但是我们还有另外一个问题要解决，那就是 Q 学习不是梯度下降，它有一个移动的目标。你可以把它看作是平方误差回归，除了回归目标本身一直在变化，它在我们下面变化，这使得学习过程很难收敛。

## Q-learning 和回归真的有什么关系？

在我之前描述的完全拟合 Q 迭代算法中，步骤 3 执行看起来很像监督学习的操作，本质上是回归到目标值。

![](img/7d9506a89c2a7a765358fac109f347cd.png)

事实上，完全拟合 Q 迭代算法中的第三步会收敛，但之后你的目标会改变，所以也许你不想让它收敛。从本质上来说，在错误的目标上进行收敛交易并不一定是一件好事，这就是为什么在实践中我们经常使用数量少得多的梯度步长，少到只有一个梯度步长，然后我们就有了这个移动目标问题，每个梯度步长我们的目标都在变化，而我们的梯度并没有考虑到目标的变化。

因此，直觉上，我们想要解决这个问题的是一个介于两者之间的东西:我们可以拥有完全拟合的 Q 迭代算法的一些稳定性，但同时不实际训练收敛。下面是我们如何使用重放缓冲区和目标网络进行 Q 学习——这看起来像是在线 Q 学习算法和完全批量拟合 Q 学习算法的一种混合。

我们将使用某种策略收集数据集，并将该策略添加到我们的缓冲区中(这是第二步，第一步将在后面介绍)。我们将从这个缓冲区中抽取一批样本，然后更新 Q 函数的参数。这个更新看起来很像之前的带有重放缓冲区的更新，但是现在参数和最大值是不同的参数向量。当然，在我完成 k 次来回更新后，我会出去收集更多的数据。然后我有一个大的外部循环，在 n 步数据收集后，我将实际更新φ'并将其设置为φ。这看起来很像我以前的拟合 Q 迭代过程，因为本质上我是用相同的目标值进行多次更新(如果φ'保持不变，那么整个目标值保持不变)，除了我可能仍然在那个内部循环中收集更多的数据。在第二步中，数据收集现在包含在对φ'的更新中，这样做的原因是因为在实践中，您通常希望尽可能多地收集数据，而为了稳定性，您通常不希望过于频繁地更新您的目标网络参数。因此，k 可能在 1 到 4 之间，所以每次收集更多数据时，我们可能会采取 1 到 4 个步骤，但 n 可能在一万左右，所以在我们改变目标值之前，可能会采取多达一万个步骤。这是为了确保我们没有试图击中一个移动的目标，因为用监督回归很难击中一个移动的目标。最初，我们将φ和φ'初始化为基本上随机的初始化，然后在前 n 步之后，我们将更新φ'，将其设置为等于φ，但是在接下来的 n 步中，φ'将保持不变。这意味着第四步开始看起来更像监督回归，因此第四步更容易做到，更稳定，你更有可能得到一个学习有意义的 Q 函数的算法。所以，你的目标在内循环中不会改变，这意味着本质上第二步、第三步和第四步看起来很像监督回归，唯一真正的区别是你可能会收集更多的数据，这些数据可以使用你最新的，例如 epsilon greedy 策略来收集。

基于这个通用配方，我们可以衍生出一种经典的深度 Q 学习算法，这种算法有时被称为 DQN。这个特殊的特例看起来是这样的:在第一步中，采取一个动作并观察结果转换，然后将其添加到缓冲区。所以，这看起来很像在线 Q-learning。在第二步中，从您的缓冲区中随机均匀地抽取一个小批量，因此这个小批量甚至可能不包含您刚刚在现实世界中获得的过渡。第三步，计算小批量中每个元素的目标值，并使用目标网络 qφ’计算这些目标值。在第四步中，通过将梯度回归到那些目标值上来更新当前网络参数φ。现在，注意到目前为止，φ还没有在其他地方使用过，除非在第一步，如果你使用ε梯度策略，然后在第五步，每隔 n 步，通过替换φ或φ来更新φ。

![](img/d0411dbf3936472b1e23cb3e6ee97ff4.png)

需要注意的是，这个过程基本上是顶部更一般的过程的一个特例:如果选择 k=1，就会得到这个算法。步骤的编号稍微重新安排了一下，但基本上是同一个方法。

还有一些其他的方法来处理目标网络，这些方法已经在文献中使用过，值得尝试。这种更新目标网络的方式有点奇怪，这里有一些直觉来说明一些奇怪之处。假设我采样了我的跃迁，然后更新了我的φ，然后我采样了另一个跃迁，再次更新了我的φ。蓝框是第一步，绿框是第二、第三和第四步:

![](img/ebca2609cec68b9fa4e3f7c811c537fc.png)

在这里，在这一步，也许我的目标网络是从第一步获得的。所以也许在第一步，当我开始时，φ'等于φ，所以在第三步，我从第一步得到我的目标值。

![](img/729d47c7f0267b0cd9f1f17f7feb96ab.png)

在第二步，我从第一步得到它们:

![](img/88b9ad4dd0dbda6c3e3519f75811ffd4.png)

在第四步，我从第一步得到它们:

![](img/5f77a794716201eaa88e6146d34e2591.png)

然后，如果第四步是我将φ’更新为等于φ(所以基本上如果我的 n 等于 4)，那么在第五步，我从上一步得到我的φ’。因此，看起来在不同的阶段，目标值的滞后量非常不同。如果你在 n 之后的那一步，你的目标网络只有一步的历史，如果你在一个翻转之前，那么它是 n-1 步的历史。所以看起来在不同的时间点，你的目标值看起来比其他的更像移动的目标；如果你在φ'到φ的点之后，那么你的目标看起来真的像一个移动的目标，如果已经过了很长时间，那么它看起来真的不像一个移动的目标。这实际上不是什么大问题，但感觉有点不对劲。那么你可以做的一个常见选择是使用不同类型的更新，这有点类似于 Poliak 平均:一个流行的替代方法是在每一步将φ'设置为τ乘以旧φ'加 1 减去τ乘以新φ。

![](img/51f25e5b0ccc00e11c517a1a022ea48b.png)

所以，你可以把这想象成，φ'在它的旧值和φ定义的新值之间逐渐插值。你会选择τ为一个相当大的数字，例如，你可能会选择它为 0.999，这意味着一千分之一基本上来自φ，其余来自φ’。当然，需要注意的是，这只有在φ'与φ相似时才有意义；因为你在逐渐让φ'越来越类似于φ，这个过程其实是没问题的。

## Q-learning 概述

在这里，我想讨论一下我们对这些 Q 学习算法的另一种看法，这可能会提供一个更统一的视角。

下面是我之前讨论过的使用重放缓冲器和目标网络的一般 Q 学习:

![](img/8fae000f004c6892741671338c7a0021.png)

这是拟合的 Q 迭代算法:

![](img/022978a1a2afc59d2098ca72624c63ca.png)

它们实际上是同一种东西，如果你将它们视为以不同的速率并行运行的相同的基本过程，那么所有这些方法都可以统一到一种并行框架中。

在拟合的 Q-算法中，内部内环只是 SGD，而我们之前的 DQN 方法是一种特殊情况，其中 n 等于 1，k 等于 1，但所有这些都只是这种更普遍观点的特殊情况。我们有我们的转换数据集，我们的重放缓冲区:

![](img/ca5dc4f3161b2eb094f3c714c7cd4f81.png)

我们周期性地与世界互动，当我们与世界互动时，我们通常做的是我们最新的向量φ，我们从φ中构造一些策略，例如使用ε-greedy 或 Boltzmann 探索，我们将它发送到世界中，它带回一个或多个转换。

![](img/fba3f84db1d6148a09b3542c9ae0be77.png)

你可以认为这不是我们定期做的一个独立的决定，而是一个持续不断的过程。所有这些过程都是步骤 1:数据收集过程，它在环境中采取步骤，并且它采取的每一步都发送回我们的重放缓冲区。我们的重放缓冲区是有限大小的，我们不能永远不停地向它添加东西，所以我们还有另一个进程，一个驱逐进程，当它变得太大时，它会定期将东西抛出缓冲区:

![](img/494ae31d7045c0d6ecfb0487cf91d5ee.png)

一个非常简单而合理的选择是简单地将重放缓冲区构造成一个环形缓冲区，当有新的内容加入时，缓冲区中最老的内容将被丢弃。然后是目标参数φ'，用于计算这些目标值，还有当前参数φ，这些参数是您要提供给进程 1 的参数，以便构建 epsilon 贪婪策略来收集更多数据。在这两个参数之间是进程二，它更新目标参数，通常非常慢:

![](img/db61ae5acd8f8f91d80425a9480d02c8.png)

然后是过程 3，这是一种主要的学习过程。它从重放缓冲区加载一批转换，加载目标参数φ’，使用目标参数计算被采样的一批转换的目标值，使用该值更新当前参数φ，然后将它们保存回当前参数:

![](img/c1f7193f1c8ded754a19e13654c9e1fe.png)

这是一个普通 Q 学习方法的图形描述，包含了我们讨论过的所有算法。它们本质上都可以被实例化为这个一般过程的特例。

*   在线 Q-learning 是一个特殊的例子，你立即驱逐，这意味着你的缓冲区的大小是 1。然后进程 1、2 和 3 都以相同的速度运行，它们都按顺序执行一个步骤。所以过程一采取一个步骤，这意味着你收集一个转换；过程二采取一个步骤，这意味着您的目标值总是使用最新的参数计算；过程三采取一个步骤，这意味着你做一个梯度更新。
*   我们之前提到的 DQN 算法也非常相似:进程一和进程三以相同的速度运行，然后进程二非常慢，重放缓冲区非常大(因此您可能存储多达一百万个转换)。这开始看起来如此奇怪的部分原因是，当您的重放缓冲区很大时，进程 1 和进程 3 的解耦程度相当高，因为一旦它足够大，您对刚刚收集的转换进行采样的概率就会变得相当低。事实证明，快速收集数据确实非常重要，因此，如果收集数据的速度不够快，Q-learning 算法的性能会迅速下降，但尽管如此，进程 1 和进程 3 之间仍有相当大的缓冲空间。
*   拟合 Q 迭代算法也可以看作是这种情况的一个特例。流程三在流程二的内循环中，而流程二又在流程一的内循环中。在这种算法中，你做你的回归一直到收敛，然后你更新你的目标网络参数，你可能会交替几次；然后在外循环中，你一路弹出，收集更多的数据

但是这些真的没有什么不同，它们只是关于我们运行所有这些不同进程的速率的特定选择，当然这其中有一些更深层次的东西，因为如果进程二和进程一完全停止，那么进程三就面临着一个标准的收敛监督学习问题。通过改变这些不同过程的速率，让它们以不同的速率运行，我们基本上减轻了非平稳性的影响。因为如果进程二的速率，例如，与进程三的速率非常不同，那么对于进程三(它运行得快得多)来说，看起来一切都几乎是静止的。这就是为什么这三个不同的过程以不同的速率运行可以帮助 Q-学习算法更有效地收敛的更深层次的原因。

## 改进 Q-learning

让我们讨论一些我们在实际实现 Q-learning 算法时需要考虑的实际问题，然后讨论一些可以使它们工作得更好的改进。

## 我们的 Q 值真的准确吗？

我们可以把 Q 值想象成这种抽象的对象，我们在强化学习中使用它来帮助我们改进我们的政策并获得 argmax，但是 Q 函数也是一种预测:它是对你在未来将获得的总回报的预测，如果你从一个特定的状态行动开始，然后遵循你的政策。所以，问这些预测是否是准确的预测是有意义的。它们与现实相符吗？

![](img/5a74cd7cf0f43747411ace97101e6001.png)

如果我们看看基本的学习曲线，其中 x 轴是 Q 学习的迭代次数，y 轴是每集的平均奖励，我们看看一些雅达利游戏，我们会发现所有这些雅达利游戏的每集平均奖励都在上升。所以，事情正在好转。如果我们观察预测的平均 Q 值，这是右边的两个图，我们会看到随着训练的进行，Q 函数预测的 Q 值越来越大。这在直觉上是有道理的:随着训练的进行，我们的政策变得更好，它得到了更高的回报，所以我们的 Q 函数也应该预测它会得到更高的 Q 值。

有趣的是，不同状态和动作的相对 Q 值在成功的训练运行中似乎是有意义的，但是它们的实际绝对值实际上并不能很好地预测真实值。特别是，Q 函数似乎系统地认为它将获得比实际更大的回报。这个问题在 Q-learning 中有时被称为高估，它实际上有一个相当直接和直观的原因。

## 高估

让我们看看我们是如何计算目标值的。当你计算你的目标值时，你得到你当前的目标 Q 函数，你得到这个 Q 函数相对于这个动作的最大值。这个 max 就是问题所在:

![](img/e4714076158bfb290f6459017e1b2861.png)

现在，让我们暂时忘记 Q 值，假设你有两个随机变量，x1 和 x2。你可能认为 x1 和 x2 是正态分布的随机变量，所以它们有一些真值和一些范数。你可以证明 x1 和 x2 的最大值的期望值总是大于或等于 x1 的期望值和 x2 的期望值的最大值。

![](img/6813b1f847303260fc35a0247459d392.png)

直觉告诉我们，当取 x1 和 x2 的最大值时，实际上是选取了噪声较大的值，尽管 x1 和 x2 的噪声可能为零均值，但两个零均值噪声的最大值通常不是零均值。你可以想象一个噪声是正的或者负的百分之五十，另一个是正的或者负的百分之五十，但是当你取两个中的最大值，如果其中一个是正的，你会得到一个正的答案。所以两个噪声中有一个是正的概率，肯定会很高。现在，如果你想象你的 Q 函数有一些噪声，那么当你在目标值中取这个最大值时，你就是这样做的。想象一下，不同动作的 Qφ代表该动作的真实 Q 值加上一些噪声，当你取目标值的最大值时，你实际上选择了正误差，出于同样的原因，x1 和 x2 的最大值的期望值大于或等于其期望值的最大值，动作的最大值将系统地选择正方向的误差，这意味着它将系统地高估真实 Q 值，即使你的 Q 函数最初没有系统地具有正或负的误差。为此，max 优先选择正方向的误差。

我们如何解决这个问题？如果我们回想一下 Q 迭代，我们得到这个最大值的方法基本上是通过修改策略迭代过程。我们有我们的贪婪策略，然后我们把 argmax 发送回我们的 Q 函数来获取它的值。这只是另一种说法:

![](img/c2c37a9f3410c415a8c3ef1a6b4f9c9e.png)

这实际上是我们将要用来尝试减轻这个问题的观察结果。你看，问题是我们根据 Qφ'选择我们的行动，所以如果 Qφ'错误地认为一些行动因为一些噪音而变得更好，那么这就是我们将选择的行动，然后我们将用于目标值的值是具有相同噪音的相同行动的值。但是，如果我们能够以某种方式将行为选择机制中的噪声与价值评估机制中的噪声去相关，那么这个问题可能会消失。因此，问题是该值也来自同一个 Qφ'，它与我们用来选择动作的规则具有相同的噪声。

## 双 Q 学习

如果给出值的函数与选择动作的函数不相关，那么原则上这个问题应该会消失。所以我们的想法是，不要用同一个网络来选择行动，就像我们用来评估价值的网络一样。双 Q 学习使用两个网络。一个网络称为φa，另一个网络称为φb。φa 使用来自φb 的值来评估目标值，但根据φa 选择动作。因此，如果假设φb 和φa 去相关，那么φa 为 argmax 选择的动作将被一些噪声破坏，但该噪声将不同于φb 的噪声，这意味着当φb 评估该动作时，如果该动作是因为它有正噪声而被选择的，那么φb 实际上会给它一个较低的值。因此，系统将是一种自我校正，然后类似地，通过使用φb 作为实际选择规则，使用φa 作为其目标网络来更新φb。这就是双 Q 学习的精髓。

![](img/92df0059a9c7e844b09a6f43416ec9fe.png)

在实践中，我们可以实现双 Q 学习的方法是实际上不添加另一个 Q 函数，而是实际上使用我们已经有的两个 Q 函数。我们可以使用当前网络和目标网络。

如果这是标准 Q-learning:

![](img/9cb5afbf306f9c8ceefd497362b910b5.png)

这是双 Q 学习:

![](img/704d143f69f8133bec12f541da4a2fb5.png)

我们使用 Qφ选择动作，但使用 Qφ'评估它。只要φ'和φ不太相似，那么这些就会去相关。你可以说，我们仍然有一点点移动目标的问题，因为随着我们的φ变化，我们的动作也变化，但 argmax 的变化可能是非常突然的离散变化，它不会一直发生，所以如果你有三个不同的动作，argmax 不会经常变化。当然，φ'和φ并不是完全分开的，因为你会定期将φ'设置为等于φ，但在实践中，它实际上工作得很好，并且缓解了大部分高估问题。

## 多步退货

我们还可以使用另一个技巧来改进 Q-学习算法，它类似于我们在演员-评论家文章中看到的东西。是多步回归的运用。这是我们的 Q-learning 目标:

![](img/e46a91517da0fc5e8515f79757bcfa72.png)

信号从哪里来？如果你最初的 Q 函数很差，那么你几乎所有的学习都来自奖励，第二项只是贡献噪声。另一方面，如果 Q 函数是好的，那么目标值通常是很重要的。在训练的早期，你的 Q 函数相当糟糕，所以几乎所有的学习信号都来自于奖励。在后来的训练中，你的 Q 函数变得更好，Q 值的大小比奖励大得多，所以在后来的训练中，Q 值占主导地位。但是如果你的 Q 函数不好，你最初的学习阶段会非常慢，因为这个目标值主要是由 Q 值决定的。这与我们在 actor-critic 中看到的非常相似，当时我们谈到使用奖励加上下一个值的 actor-critic 风格更新具有较低的方差，但它不是无偏的，因为如果值函数是错误的，那么您的优势值就会完全混乱。Q-学习也是一样:如果 Q-函数是错的，那么你的目标值就真的乱了，你就不会有太大的学习进步。我们在演员-评论家讲座中的另一个选择是使用蒙特卡洛奖励总和，因为奖励总是真实的；它们只是更高的方差，因为它们代表了单一样本的估计。我们可以在 Q-learning 中使用相同的基本思想。因此，Q-learning 默认情况下会后退一步，具有最大偏差和最小方差，但你可以像演员评论家一样构建一个多步目标:

![](img/bc51cd94b07e11738556402d6bfe56dd.png)

构建多步目标的方法不仅仅是使用一个奖励，而是从 t' = t 到 t+n-1 之间做一个小的累加。您可以验证，如果 n = 1，那么您可以完全恢复我们用于 Q-learning 的标准规则，但对于 n 大于 1 的情况，您将多个奖励值相加，然后将目标网络用于 t + n 步乘以γ的 n 次方。因此，这有时被称为 n 步回报估计器，因为您不是对一步的奖励求和，而是对 n 步求和。就像演员-评论家一样，权衡的结果是，由于 r 的单样本估计，它会给出更高的方差，但偏差较低，因为即使 Q 函数不正确，现在它会乘以γ的 n 次方，对于较大的 n 值，γ的 n 次方可能是一个非常小的数字。所以让我们来谈谈 Q-learning 的 n 个 n 步回报:它的偏差更小，因为 Q 值乘以一个小数字，它通常在早期更快，因为当目标值不好时，这些奖励的总和真的会给你很多有用的学习信号。不幸的是，一旦你使用 n 步回归，当你有一个政策样本时，这实际上只是对 Q 值的正确估计。这样做的原因是，如果您使用不同的策略收集样本，那么第二步 t+1 实际上可能与您的新策略不同，因为如果在第二步您采取不同的操作，这将与您从即时回报中获得的结果相匹配。所以，如果没有保单数据，n 步退货在技术上是不正确的，而如果有保单数据，技术上只允许使用 n 等于 1。因此，只有在学习 on-policy 时才是正确的，因为 off-policy 最终会使用示例中的操作来完成这些中间步骤，而这不是您的新策略会采取的操作。我们如何解决这个问题？

*   我们可以忽略这个问题
*   我们可以动态地切断跟踪，因此我们可以选择 n 来只获取策略数据。本质上，我们可以看看我们确定性的贪婪策略会做什么，我们可以看看我们在样本中实际做了什么，我们可以选择 n 作为最大值，这样所有的行为都与我们的策略会做的完全匹配，这也将消除偏差。但是，当数据主要是关于政策，而行动空间非常小时，这种方法很有效。
*   我们可以做的另一件事是重要性抽样:我们可以构建一个随机策略，并对这些 n 步回报估计量进行重要性加权。
*   您可以用一些其他附加信息来限制 Q 函数，从而使其偏离策略。

## q-连续动作学习

将 Q-learning 程序扩展到我们有连续动作的情况实际上是可能的，但有点复杂。

连续动作有什么问题？问题是当你选择你的动作时，你需要执行 argmax over 动作。

![](img/68146136f77035806c6788cd068c382f.png)

在离散的动作上做这件事很简单:你只需评估每一个可能动作的 Q 值，然后选择最好的一个。但是当你有连续动作的时候，就难多了。这在两个地方更难:a)当评估 argmax 策略时和当计算目标值时，这需要 max(或者在双 Q 学习的情况下也需要 argmax)。

![](img/b5740734e83516576d8569f7c6b40578.png)

此外，目标值 max 尤其成问题，因为这发生在训练的内部循环中，所以你真的希望非常快和非常有效。那么在有连续动作的情况下，如何执行这个 max 呢？有三种选择:

**选项 1:优化**

选项一是使用连续优化程序，例如梯度下降。梯度下降本身可能相当慢，因为它需要多个步骤来创建计算，并且它发生在外环学习过程的内环中；所以，我们有更好的选择。另外，我们的动作空间通常是低维的，所以在某种意义上，它比我们通常用 SGD 处理的那种问题更容易一些。

因此，事实证明，为了用优化来计算最大值，一个特别好的选择是使用一个无导数的随机优化过程。一个非常简单的解决方案是简单地将连续动作的最大值近似为随机采样的一组离散动作的最大值。

![](img/3f5b978736a3cae73aef00668bf0a9d3.png)

例如，您可以从一组有效的动作中随机抽取一组 n 个动作，然后取这些动作中最大的 Q 值。这不会给你一个精确的最大值，它会给你一个非常近似的最大值，但如果你的行动空间是非常低维的，你可以用足够的样本轰炸它，这个最大值实际上可能相当不错。当然，如果高估是你的问题，这可能实际上遭受高估更少，因为最大值不太有效。这也有非常简单的优势，并行化非常有效，因为你可以使用你最喜欢的深度学习框架，只需将这些不同的动作视为小批量的不同点，然后并行评估所有这些动作。问题是它不是很准确，尤其是当动作空间的维度变大时。这种随机抽样方法实际上并不能给你一个非常准确的最大值。

如果你想要一个更精确的解，有更好的算法是基于基本相同的原理。

*   交叉熵方法(CEM)是一种简单的迭代随机优化方案，它由一组采样动作组成，就像上面的简单解决方案一样，但交叉熵方法不是简单地选取最佳的一个，而是细化从中采样的分布，然后在好的区域中采样更多的样本，然后重复。如果你愿意并行化，并且你有一个低维的动作空间，这也可以是一个非常非常快的算法。
*   CMA-ES:你可以把它看作是 CEM 的一个更加华丽的版本，虽然不那么简单，但是结构上非常相似。这些方法适用于多达 40 维的动作空间，所以如果你使用这些解决方案中的一个，你只需用它代替 argmax 来找到最佳动作，剩下的 Q-learning 过程基本上保持不变。

**选项 2:易于优化的函数类**

或者，您可以使用一个天生易于优化的函数类。因此，任意神经网络相对于它们的一个输入不容易优化，但是其他函数类对于它们的最优值有封闭形式的解决方案。这种函数类的一个例子是二次函数；例如，你可以把你的 Q 函数表示成一个作用量为二次的函数，二次函数的最优解是一个封闭形式的解。

![](img/cc501e13c8a252bd539a705aa6ee59b0.png)

你可以用一种叫做归一化优势函数(NAF)的东西来做这件事。您有一个输出三个量的神经网络:标量值偏差、向量值和矩阵值。向量和矩阵一起在作用中找到一个二次函数，所以这个函数在状态中是完全非线性的，它可以表示状态的任何函数，但是对于一个给定的状态，作用中 Q 值的形状总是二次的，当它总是二次的时候，你总是可以找到最大值。在这种情况下，最大值只是 f 的μφ，之所以称之为“归一化”,是因为如果你对它求幂，你会得到一个归一化的概率分布。现在，我们以减少 Q 函数的表示能力为代价，使这种最大化操作变得非常容易，因为如果真正的 Q 函数在动作中不是二次的，那么我们的问题当然是我们不能精确地表示它。所以算法没有变化，它和 Q-learning 一样有效，但是它失去了一些代表性

**选项 3:学习近似最大化器**

这将有点类似于选项一，只是不是为我们必须采用的每个 argmax 单独运行优化，而是实际上训练第二个神经网络来执行最大化。请记住:

![](img/be7d9a065adcc5be2d8b20012ef95d64.png)

所以只要我们能做到 argmax，我们就能进行 Q 学习。所以想法是训练另一个网络μφ(s)，使得μφ(s)近似为 Q-φ的 argmax。你也可以把μφ(s)看作是一种策略。因为它查看状态并输出动作，特别是 argmax 动作。为此，我们只需求解θ:

![](img/bedf72aba416a804e8951d93cf50951a.png)

这些是我们的目标:

![](img/56d8ca1316eebcbad75d1eaccadb976a.png)

因此，该算法如下所示:

首先，我们采取一些行动，我们得到一个观察，我们添加到缓冲区。然后，我们从缓冲区随机均匀地抽取一个小批量样本。第三步，用μφ和目标网 Qφ'计算目标值。第四步，就像在 Q-learning 中一样，我们对φ进行梯度更新。第五步，我们对θ进行梯度更新。θ上的梯度取 Q 值对作用的导数，然后乘以作用对θ的导数，这就是通过μ的反向传播。然后，我们将更新我们的目标参数φ'和 prime，例如使用 Polyak 平均。然后，我们将重复这个过程。

## 实施提示和示例

Q-learning 方法通常比策略梯度方法更难使用，所以它们在正确使用时需要更多的关注。稳定 Q-learning 算法需要一些小心，我建议从测试一些简单可靠的问题开始，你知道你的算法应该工作。这只是为了确保您的实现是正确的，因为本质上您必须经历故障排除的几个不同阶段。你首先要确保你没有错误，然后你必须确保你调整你的超参数，然后让它在你的实际问题上工作，所以你要在超参数调整之前做调试，这意味着你要在非常简单的问题上做，基本上任何正确的实现都应该真正工作。Q-learning 在不同的问题上表现非常不同。

大的重放缓冲区确实有助于提高稳定性，所以使用大约一百万大小的重放缓冲区是一个不错的选择。在这一点上，该算法真的开始看起来更像 fitted-Q 迭代，这可能是其稳定性提高的部分原因。最后，Q-learning 需要很多时间，所以要有耐心:在很长一段时间内，它可能不会比随机更好，而随机探索会找到好的转换，一旦找到这些好的转换，它可能会起飞。从高探索开始，从较大的ε值开始，然后逐渐减少探索，因为最初你的 Q 函数是垃圾，所以它是随机探索，将做大部分繁重的工作，然后一旦你的 Q 函数变得更好，那么你可以减少ε-所以它通常有助于制定一个计划。

## Q-learning 的更多高级技巧

*   贝尔曼误差的梯度可能非常大；有一点麻烦的是，如果你有一个非常糟糕的行为，你并不真正关心这个行为的价值，但是你的平方误差目标真正关心的是计算出它到底有多糟糕。因此，如果你有一些好的行动，比如+10、+9、+8，你有一些坏的行动，比如-1.000.000，坏的行动会产生一个巨大的梯度，尽管你真的在乎-1.000.000。换句话说，它也可以是 1.000，这将导致相同的政策，但你的 Q 函数目标真的很关心这个，这将导致大的梯度。你能做的是，要么剪切梯度，要么使用所谓的 Huber 损失:

![](img/13a3c7419f57adb6373885f29ff08987.png)

它是平方误差损失和绝对值损失之间的插值，因此远离最小值的 Huber 损失看起来像绝对值，而靠近最小值的 Huber 损失用二次曲线变平。

*   双 Q 学习在实践中很有帮助，它很简单而且没有缺点。
*   n 步回报也很有帮助，但也有一些缺点:它们会系统地偏离你的目标，特别是对于较大的 n 值。
*   计划探索和计划学习率，以及 Adam 优化也有帮助。
*   当调试你的算法时，确保运行多个随机种子，因为你会看到随机种子之间的许多变化，所以你应该运行几个不同的随机种子，以确保事情真的按照你期望的方式工作。

## 摘要

我们在实践中讨论了 Q-learning，我们如何使用重放缓冲区和目标网络来稳定它；我们从三个过程的角度讨论了拟合 Q 迭代的一般观点，我们讨论了双 Q 学习如何使 Q 学习算法工作得更好；我们讨论了多步 Q-learning，最后讨论了如何让 Q-learning 在持续的行动中发挥作用。

*随时给我留言或:*

1.  通过[LinkedIn](https://www.linkedin.com/in/samuele-bolotta-841b16160/) 和 [Twitter](https://twitter.com/SamBolotta) 联系我
2.  跟着我上[媒](/@samuelebolotta)