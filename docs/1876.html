<html>
<head>
<title>Convert your bulky Transformer models into lightweight high performance ONNX models!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将您笨重的变压器模型转换为轻量级高性能ONNX模型！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/convert-your-bulky-transformer-models-into-lightweight-high-performance-onnx-models-5b18bc25ee06?source=collection_archive---------4-----------------------#2022-02-07">https://medium.com/mlearning-ai/convert-your-bulky-transformer-models-into-lightweight-high-performance-onnx-models-5b18bc25ee06?source=collection_archive---------4-----------------------#2022-02-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="2bbe" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">作者</h1><p id="4c27" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">纳巴伦·巴鲁阿</p><p id="59ba" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/nabarunbaruaAIML" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/nabarun-barua-aiml-engineer/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@nabarun.barua">towards data science</a></p><p id="d9bb" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">阿尔琼·库姆巴卡拉</p><p id="5550" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/arjunKumbakkara" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/arjunkumbakkara/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@arjunkumbakkara">towards data science</a></p><blockquote class="kg kh ki"><p id="b970" class="jc jd kj je b jf ka jh ji jj kb jl jm kk kc jp jq kl kd jt ju km ke jx jy jz ha bi translated"><strong class="je hi">简介:</strong>我们如何将我们为文本分类训练的ALBERT模型转换到ONNX运行时，以及它如何从46.8 mb的大小突然增加到358.3 mb。箱重量文件)。</p></blockquote><p id="7fea" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi kn translated"><span class="l ko kp kq bm kr ks kt ku kv di">答</span>在转换阶段之前，我们对python进行了广泛的优化试验，之后我们意识到我们关注的是错误的领域。因此，我们认为解决的办法是将。onnx将权重转换为压缩的量化版本。其细节如下:</p><h1 id="2e01" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">为什么是ONNX？</h1><ul class=""><li id="53eb" class="kw kx hh je b jf jg jj jk jn ky jr kz jv la jz lb lc ld le bi translated">ONNX运行时:跨平台由于ONNX是所有流行的机器学习、人工智能模型的通用格式。将训练好的模型(权重)转换成符合ONNX的ONNX模型已经成为事实上的标准。例如，将Tensorflow或Pytorch中构建的任何模型转换成另一种格式是一项痛苦的任务。出现了许多前所未有的问题和异常，解决这些问题可能会耗尽您的开发/实验时间。因此，这是确保您的模型不会陷入框架锁的最佳方式。跨平台兼容性使部署变得容易，我们将在接下来的部分中讨论这一点</li></ul><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lf"><img src="../Images/a8a92cb8f3c3c830327ced3f7ac71275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVrJyZpdH4UyaCQJnCfxEg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">ONNX interoperability | Image by <a class="ae kf" href="https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92#:~:text=ONNX%20is%20the%20acronym%20that,giants%20Microsoft%2C%20Facebook%20and%20Amazon." rel="noopener" target="_blank">Fernando López</a> | Logos taken from the original source</figcaption></figure><ul class=""><li id="19c5" class="kw kx hh je b jf ka jj kb jn lv jr lw jv lx jz lb lc ld le bi translated">更快的推理:ONNX运行时中的推理更快，因为运行时本身是用C语言构建的，而且它是我们能得到的最接近机器的。它的执行速度非常快。</li><li id="3c33" class="kw kx hh je b jf ly jj lz jn ma jr mb jv mc jz lb lc ld le bi translated">与环境无关的部署:虽然你的训练阶段是在python中，但是ONNX现在可以灵活地将训练好的权重部署到多个其他栈或环境中，比如C#/C++/Java等。</li></ul><p id="dca4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">本质上，ONNX有助于实现架构和硬件之间的高度互操作性。</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lf"><img src="../Images/9559f3f52e15521c19643113847c4ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iaE86nXUprJqKWm3diCvyw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">Credit: opendatascience.com</figcaption></figure><h1 id="e4ea" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">转换为ONNX:</h1><p id="587c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">从下面可以看出，转换到ONNX运行时就是调用一个API (Pytorch)</p><figure class="lg lh li lj fd lk er es paragraph-image"><div class="er es md"><img src="../Images/1ae3158b4e4e213550936ef4599d34e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/0*8scawoSbG_2WKjxl.jpeg"/></div></figure><p id="5400" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了进一步了解，请随意航行到我们建立的这个培训管道，并从中检查ONNX阶段。点击<a class="ae kf" href="https://github.com/nabarunbaruaAIML/CML_with_DVC_on_Transformer_NLP/blob/main/src/stage_04_onnx.py" rel="noopener ugc nofollow" target="_blank"> ONNX </a> <a class="ae kf" rel="noopener" href="/mlearning-ai/continuous-machine-learning-on-huggingface-transformer-with-dvc-including-weights-biases-6a909983e48e">注意:如果你想了解整个流程，那么就去看这篇文章</a>，它解释了代码和架构的整个培训管道<a class="ae kf" rel="noopener" href="/mlearning-ai/continuous-machine-learning-on-huggingface-transformer-with-dvc-including-weights-biases-6a909983e48e">。</a></p><h2 id="adc0" class="me if hh bd ig mf mg mh ik mi mj mk io jn ml mm is jr mn mo iw jv mp mq ja mr bi translated">问题是:</h2><p id="53ac" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然而，这里的问题是，直接转换可能会增加模型的整体大小，如下面所附的图片所示(我们使用ALBERT Transformer作为示例)。模型训练后，获得的大小为46.8 mb，但在直接转换为ONNX运行时后，大小增加了40倍，这对任何推理活动来说都太多了。因此，为了让您更容易地过渡到ONNX运行时并有效地使用它，我们不得不进行一些工作，如下所述。</p><h1 id="29f5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">优化转换而不损失任何勇气的方法！</h1><h2 id="3494" class="me if hh bd ig mf mg mh ik mi mj mk io jn ml mm is jr mn mo iw jv mp mq ja mr bi translated">第一次压缩:ONNX模型大小压缩，通过删除可被视为重复的共享层。</h2><p id="7da6" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这只是减少onnx转换模型的一种简单的pythonic方式。这个实现是基于ONNX团队的一个提示。</p><p id="dc79" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">原始重量尺寸:</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ms"><img src="../Images/fe372011c306e21892ebfbfa0dea1eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VNa965Ugk5VKWxBI.png"/></div></div></figure><p id="7adc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然而，转换后的大小达到358.3 mb。</p><p id="5a31" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Onnx重量尺寸:</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mt"><img src="../Images/f39d9505d34c8e5d208f0bacde6fdb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_GdtIyVD-mVyqI2P.png"/></div></div></figure><h2 id="232f" class="me if hh bd ig mf mg mh ik mi mj mk io jn ml mm is jr mn mo iw jv mp mq ja mr bi translated">ONNX团队关于解决方案正确性的摘录:</h2><p id="47f4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">“作为BERT优化的一部分，ALBERT模型在各层之间共享权重。随着模型尺寸变大，export torch.onnx.export将权重输出到不同的张量。使用下面的python脚本，我们可以删除重复的权重，并减少模型大小，即，比较每对初始值设定项，当它们相同时，只需删除一个初始值设定项，并将它的所有引用更新为另一个初始值设定项。”</p><p id="374d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">" " ONNX Team @天雷屋" " "</strong></p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mu"><img src="../Images/25cccdff26774f9b2446b80804754a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LqbFQ1mj1QKaPJHj.png"/></div></div></figure><p id="29a9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">实施后:</p><p id="efad" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">压缩Onnx权重大小:</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mv"><img src="../Images/b4b0ce56c45fb469f3ad119704349420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pNdg43ymbBOwxOlp.png"/></div></div></figure><p id="243d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">供您参考的依赖项:</p><pre class="lg lh li lj fd mw mx my mz aw na bi"><span id="af2a" class="me if hh mx b fi nb nc l nd ne">pip install --upgrade transformers sentencepiece<br/>pip install --upgrade onnxruntime<br/>pip install --upgrade onnxruntime-tools</span></pre><h2 id="c802" class="me if hh bd ig mf mg mh ik mi mj mk io jn ml mm is jr mn mo iw jv mp mq ja mr bi translated">第二动态量化:通过对转换后的ONNX模型进行量化。</h2><p id="fb94" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">动态量化研究</p><pre class="lg lh li lj fd mw mx my mz aw na bi"><span id="1542" class="me if hh mx b fi nb nc l nd ne">.....<br/> quantize_dynamic(onnx_model_path,<br/>                     quantized_model_path,<br/>                     weight_type=QuantType.QInt8)......</span></pre><p id="e7e8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这里，onnx runtime . quantization . quantize对HuggingFace BERT模型应用量化。它支持使用IntegerOps的动态量化和使用QLinearOps的静态量化。对于激活ONNXRuntime目前只支持uint8格式，对于权重ONNXRuntime同时支持int8和uint8格式。这里我们对伯特模型使用动态量化，对权重使用int8。</p><figure class="lg lh li lj fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nf"><img src="../Images/44472c9ee44edb72ccf24a610568a1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OzXu2Wde0-oElkl4.png"/></div></div></figure><h2 id="c79e" class="me if hh bd ig mf mg mh ik mi mj mk io jn ml mm is jr mn mo iw jv mp mq ja mr bi translated">第三:ONNX运行时的优化器。</h2><p id="535b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然而，不建议这样做，因为num_heads，hidden_size如果选择不正确，会妨碍模型的训练。然而，它直接使用。</p><pre class="lg lh li lj fd mw mx my mz aw na bi"><span id="968f" class="me if hh mx b fi nb nc l nd ne">.....<br/># optimize transformer-based models with onnxruntime-tools<br/>from onnxruntime_tools import optimizer<br/>from onnxruntime_tools.transformers.onnx_model_bert import BertOptimizationOptions<br/># disable embedding layer norm optimization for better model size reduction<br/>opt_options = BertOptimizationOptions('bert')<br/>opt_options.enable_embed_layer_norm = False<br/>opt_model = optimizer.optimize_model(<br/>    'bert.onnx',<br/>    'bert', <br/>    num_heads=12,<br/>    hidden_size=768,<br/>    optimization_options=opt_options)<br/>opt_model.save_model_to_file('bert.opt.onnx')</span></pre><p id="6a61" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这可能会给你很多警告，因为“onnxruntime_tools”现在已被弃用。因此，我们建议您使用第一种方法，这是最简单的方法，而且非常有效！附注:我们测试了这些是否有不必要的偏差。结果发现根本没有，而且效果很好。</p><p id="c5bd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">每种方法都有其优点和缺点，请根据您的推断标准选择方法。</p><p id="3293" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果你喜欢这个博客，请表达你的爱，给我们一个大拇指，给我们加星，如果不喜欢，请在评论区给我们一个反馈。希望这能帮助你理解加速推理的重要性。</p><p id="adda" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了合作、帮助和共同学习——加入我们的Discord服务器:【https://discord.gg/Z7Kx96CYGJ T2】</p><p id="8ac7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">github Repo:<a class="ae kf" href="https://github.com/nabarunbaruaAIML/onnx_model_size_compression" rel="noopener ugc nofollow" target="_blank">https://github . com/nabarunbaraaiml/onnx _ model _ size _ compression</a></p><p id="c65f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">一路平安！</p><div class="ng nh ez fb ni nj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">medium.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx lp nj"/></div></div></a></div></div></div>    
</body>
</html>