<html>
<head>
<title>Object Detection Explained: YOLO v2.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体探测解释:YOLO v2。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/object-detection-explained-yolo-v2-3e3086789ffb?source=collection_archive---------1-----------------------#2021-10-03">https://medium.com/mlearning-ai/object-detection-explained-yolo-v2-3e3086789ffb?source=collection_archive---------1-----------------------#2021-10-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="354c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">YOLO v2-更好、更快、更强</p><p id="6e26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我的目标检测系列之前，我讨论并介绍了YOLO v1架构。对于YOLO v2，我总结了作者添加的修改的所有要点。作者引入了许多修改，但我希望你非常熟悉YOLO v1，因为它将帮助你更快、更好、更强地理解YOLO v2。</p><p id="9618" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">原文:YOLO9000:更好更快更强:【https://arxiv.org/pdf/1612.08242v1.pdf<a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="17a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">上一个</strong>:</p><p id="1aa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://towardsdatascience.com/object-detection-explained-r-cnn-a6c813937a76" rel="noopener" target="_blank"> RCNN </a></p><p id="e7b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-fast-r-cnn-bc11e607411f">快速RCNN </a></p><p id="de8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-feature-pyramid-networks-cf2621c8f7cc"> FPN </a></p><p id="90de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-faster-r-cnn-23e7ab57991d">更快的RCNN </a></p><p id="0bfe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-single-shot-multibox-detector-c45e6a7af40">固态硬盘</a></p><p id="c0fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-yolo-v1-fb4bcd3d87a1"> YOLO v1 </a></p><h1 id="2dee" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">较好的</h1><p id="c7e2" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者表示，与Fast-RCNN相比，YOLO v1会产生更多的定位错误，并且它的召回率也相对较低。因此，为了解决上述问题，他们引入了以下修改:</p><h2 id="2bac" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">1.批量标准化(BN)</h2><p id="0284" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">BN层是在YOLO v1中的每个卷积层之后引入的，因此作者在mAP方面获得了大约2%的改进。</p><h2 id="d414" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">2.高分辨率分类器</h2><p id="5b94" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">YOLO v1用224 × 224的图像分辨率训练分类器，并提高到448用于检测。然而，在开始训练网络进行检测之前，YOLO v2首先在ImageNet上将它的分类器直接微调为448 × 448分辨率10个时期。这使得平均动脉压提高了4%。</p><h2 id="b4e2" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">3.带锚盒的卷积</h2><p id="28c1" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者从YOLO v1中移除了完全连接的层(你可以参考我之前关于YOLO v1的文章)，并使用锚框来预测边界框。此外，他们删除了一个池层，并将输入分辨率从448×448更改为416输入图像。这样做是因为他们需要在我们的特征地图上有奇数个位置，所以只有一个中心单元。结果，他们在mAP方面的性能略有下降，但在召回率方面有很大提高，大约为7%。</p><h2 id="aefa" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">4.维度群</h2><p id="cad0" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">锚箱在YOLO使用时有两个问题。首先，我们需要为网络选择好的先验，即锚盒，这样网络就更容易学习。因此，作者在训练集包围盒上采用K-Means聚类。</p><p id="02ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">事情是这样的:</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/882cd2c868a75056c83feb72d149bcd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*fej5X0e_24tu26XMOgKq4A.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><p id="7a5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">a)他们选择的距离函数如下:d(框，形心)= 1iou(框，形心)。</p><p id="9e30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b)他们使用不同的<em class="lg"> k </em>值运行K-Means，并发现k=5在模型复杂性和高召回率之间的<em class="lg">T5之间给出了一个很好的折衷。</em></p><p id="f11d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 5。直接位置预测</strong></p><p id="1a19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">遇到的第二个问题是模型不稳定。因此，在过去，区域建议网络<em class="lg"> t_x </em>和<em class="lg"> t_y </em>，而中心坐标<em class="lg"> (x，y) </em>计算如下:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lh"><img src="../Images/7f389720203339b068048fa6f12af297.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*IMxj9Tugp_scEf-oDumo-A.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><p id="8fb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，这个公式是不受约束的，因此它导致不稳定性。因此，作者使用以下公式:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es li"><img src="../Images/950cf5bf660e3adba2e35eb0a03659f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*0i9lojFOkmNTv8R1evvGSg.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><p id="80c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中网络预测<em class="lg"> t_x，t_y，t_w，t_h，t_o </em>(对象性)。同时，(c_x，c_y)从图像的左上角偏移，并且<em class="lg"> p_w </em>和<em class="lg"> p_h </em>是先前的边界框的宽度和高度。可以看到，地面真值是以[0，1]的范围为界的(sigmoid激活)。</p><p id="fc68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于维度聚类以及直接预测包围盒<em class="lg">中心位置，</em>作者获得了大约5%的改进。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/7d97cbf389fd574e279409b02be7e067.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*r98TUCEhdKTxp2TPjGVlhg.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><h2 id="7b88" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">6.多尺度训练</h2><p id="f8fd" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">网络不是固定输入图像大小，而是每10个时期从以下32的倍数中随机选择不同的输入分辨率:{320，352，…，608}。这种制度鼓励网络在各种输入维度下表现良好。此外，它在速度和准确性之间提供了一个简单的折衷。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lo"><img src="../Images/12595b2a97e3d2d465cab017b9211555.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*AkmsPmhr2xTtk8oglqXSCw.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><h1 id="e7de" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">更快的</h1><h2 id="e76e" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">1.暗网-19</h2><p id="b961" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者提出了一种新的主干网Darknet-19，它有19个卷积层和5个最大池层。处理一张图像需要55.8亿次运算；然而，在ImageNet上，最高准确率达到72.9%，最高准确率达到91.2%。</p><h2 id="75e2" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">2.分类培训</h2><p id="b4a9" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者利用标准的扩充。首先，他们以224 × 224的输入分辨率训练他们提出的主干，并在10个时期内以更大的尺寸448对其进行微调。更多细节请参考原论文。</p><h1 id="2cda" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">更强壮的</h1><p id="59bd" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">有多个数据集用于分类和检测。他们能结合在一起吗？请注意，作者提出YOLO9000，而不是YOLO v2，只是出于这个原因。他们合并了两个数据集，获得了9000多个类，因此这一部分是关于YOLO9000如何被训练的。</p><h2 id="2ddd" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">1.结合</h2><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lp"><img src="../Images/c46afef0a4ac186046215b752245d6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*WMRnhQquvIHpOhTFahRbEw.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><ul class=""><li id="115e" class="lq lr hh ig b ih ii il im ip ls it lt ix lu jb lv lw lx ly bi translated"><strong class="ig hi">微软COCO </strong>包含<strong class="ig hi"> </strong> 100k的图片，有80个类，检测标签，类比较通用，比如“狗”或者“船”。</li><li id="2e54" class="lq lr hh ig b ih lz il ma ip mb it mc ix md jb lv lw lx ly bi translated"><strong class="ig hi"> ImageNet </strong>有<strong class="ig hi"/>1300万张图片，有22k个类别，分类标签，类别更具体像“诺福克梗”、“约克夏梗”，或者“贝德灵顿梗”。</li></ul><p id="fd45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，有些种类如“狗”和“诺福克梗”是相互排斥的。</p><p id="d1ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了合并，使用了单词树:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es me"><img src="../Images/ac4cbeba8e462b0f960dfd05f1a1355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*RBIgeuH0-THuO1y5cQlp9g.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure><p id="7063" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上所示，作者使用WordTree构建了一个视觉概念的层次树。所以，“诺福克梗”也被贴上“狗”和“哺乳动物”的标签。总共有9418个班级。</p><h2 id="7364" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">2.联合分类和检测</h2><ul class=""><li id="3e61" class="lq lr hh ig b ih kb il kc ip mf it mg ix mh jb lv lw lx ly bi translated">作者使用3个先验，而不是5个，来限制输出大小。</li><li id="e345" class="lq lr hh ig b ih lz il ma ip mb it mc ix md jb lv lw lx ly bi translated">对于检测图像，损失通常反向传播。</li><li id="3247" class="lq lr hh ig b ih lz il ma ip mb it mc ix md jb lv lw lx ly bi translated">对于分类图像，只有分类损失在标签的相应级别或以上被反向传播。</li></ul><h2 id="86ac" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">3.结果</h2><ul class=""><li id="f2f9" class="lq lr hh ig b ih kb il kc ip mf it mg ix mh jb lv lw lx ly bi translated">实现了19.7%的mAP。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mi"><img src="../Images/e27c80dd2eaffaeffca7519c9ce27e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*ULJtdRiKJnk43UA7HLK08w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from the paper: <a class="ae jc" href="https://arxiv.org/pdf/1612.08242v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242v1.pdf</a></figcaption></figure></div></div>    
</body>
</html>