<html>
<head>
<title>Data sampling methods for imbalanced data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据的数据采样方法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/data-sampling-methods-in-machine-learning-17c77bbf0579?source=collection_archive---------6-----------------------#2022-01-23">https://medium.com/mlearning-ai/data-sampling-methods-in-machine-learning-17c77bbf0579?source=collection_archive---------6-----------------------#2022-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="74b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们面临分类问题时，大多数机器学习算法都被设计为对每一类的相同比例的观察值进行工作。因此，当某个类的观测值比多数类少很多时(不平衡数据集)，算法会忽略该类，从而降低其性能和应用程序容量。为了改进它，一个解决方案是使用数据采样算法。数据采样方法提供了几种技术来平衡两类的体积，既增加少数类(过采样)又减少多数类(欠采样)。</p><p id="1e7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在图1中可以看到一个模型中不平衡数据的影响。在这种情况下，我们有三个类:类2是多数类，类1和0是少数类。由于我们对从这些数据中学习感兴趣，并且这是一个多分类问题，如果我们考虑一个线性SVC模型来分类我们的数据集，我们可以看到在关于模型分类的少数类上添加更多数据的效果。在第一个训练数据集，不平衡数据是突然的，分类器只能预测两个类别。即使有一个人，像你一样，看到这个数据，并认为如果我们可以做一个更好的模型，我们会说不，因为没有足够的数据，我们必须区分三个类别。在相反的情况下，在最后一个例子中(右下角的图像)，所有零件都有相同的体积，大约330个例子。在这种情况下，该模型工作得相当好，并且像线性SVC这样的线性模型足以保证良好的性能来划定这三个类之间的边界。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/592bbfe7724614594accb58771d59880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uZU0bPrcT1YyG40-vnnLXQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Fig. 1: Effect of imbalanced data on machine learning algorithms</figcaption></figure><p id="3293" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果少数类有足够的数据进行统计处理并生成合成数据，或者对多数类采样不足，不平衡数据就不会成为问题。在大多数严重不平衡的数据集中，最大的挑战在于，您应该小心地应用数据采样技术，以不改变数据的原始分布或原始信息。</p><p id="af5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据采样提供了一组转换训练数据集以实现更好平衡的技术。没有最好的数据采样方法:正如生活中一样，有许多数据采样方法，每种方法都更好，这取决于项目中使用的应用程序或模型。因此，开展抽样方法对机器学习模型影响的敏感性研究是非常重要的。我们可以区分两类数据采样方法:<strong class="ig hi">过采样</strong>和<strong class="ig hi">欠采样</strong>技术。</p><h1 id="9b6d" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">过采样</h1><p id="0a1a" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">过采样技术从少数类的例子中合成新的例子。最常用的过采样方法有:</p><p id="a307" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机过采样:</strong>这种技术包括从少数数据集中随机复制样本。这对于具有偏态分布的数据集可能是有效的，在这种情况下，重复的示例可以提高模型性能。然而，读者应该注意过度拟合模型，因为这种技术复制了示例。因此，对训练和测试数据集进行敏感性研究并进行度量评估以防止过度拟合是很重要的。使用<em class="kv"> imblearn </em>库中的<em class="kv"> RandomOverSampler </em>类实现随机过采样。</p><p id="e036" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">合成少数过采样技术(SMOTE): </strong>该技术随机选择一个少数类实例，并找到其k个最近的少数类邻居。然后，通过随机选择k个最近邻居b中的一个并连接a和b以在特征空间中形成线段，来创建合成实例。合成实例生成为两个选定实例a和b的凸组合。读者应注意应用smote后的数据分布，因为原始数据对此策略有重要影响。此外，k-neighbors是一个待测试的超参数，用于改进模型学习。SMOTE是使用<em class="kv"> imblearn </em>库中的<em class="kv"> SMOTE </em>类实现的。</p><p id="a9e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">边界线-SMOTE: </strong>在这种情况下，它由SMOTE技术的修改组成，通过使用KNN，聚焦于两个类的边界。分类问题主要在于确定不同类别之间的界线。正如我们在前面的图片中看到的，可能是边界线没有很好地定义，它是为了区分该线中的类，包括分类模型。因此，该算法包括在类之间的边界周围应用SMOTE技术，以改进它们的限制并提高模型性能。使用<em class="kv"> imblearn </em>库中的BorderlineSMOTE类实现Borderline-SMOTE。</p><p id="a381" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">边界线-SMOTE SVM: </strong>该技术包括非常类似于先前技术的方法论，边界线-SMOTE，不同之处在于边界线不是由数据集本身施加的，而是由SVM算法、先验学习施加的。因此，SVM用于定位边界线，SMOTE算法基于该边界线工作。SVM SMOTE是通过使用<em class="kv"> imblearn </em>库中的SVM SMOTE类实现的。</p><p id="81c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自适应合成采样(ADASYN): </strong>该技术涉及生成与少数类中的样本密度成反比的合成样本。该技术在少数样本密度低的区域空间中生成合成样本，而在样本密度高的区域空间中生成较少的合成样本。ADASYN是通过使用<em class="kv"> imblearn </em>库中的ADASYN类实现的。</p><p id="3a7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">甘:</strong>生成对抗网络可以作为一种过采样工具。这是一个新话题，文献中有许多文章解释了使用GAN技术进行过采样的好处。</p><p id="e2fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">数据扩充:</strong>它包括生成额外的数据来提高模型的性能。它主要用于图像和语言处理。关于数据扩充的一些技术如下:添加噪声、裁剪、翻转、缩放、小波变换等等。</p><h1 id="fa80" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">欠采样</h1><p id="34f6" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">欠采样技术<strong class="ig hi"> </strong>从多数类中删除样本子集。最常用的技术是:</p><p id="5111" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机欠采样:</strong>该技术包括从多数类中随机选择样本，以从训练数据集中删除样本。通常，随机减少多数类数据集，以使两个类的体积更加均匀。在这种情况下，读者应该注意不要删除重要的例子。如果多数类足够大并且均匀分布，随机移除样本不会改变统计的基本原理，应用这种技术会很有趣。使用<em class="kv"> imblearn </em>库中的<em class="kv"> RandomUnderSampler </em>类实现随机欠采样。</p><p id="6e68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">新的缺失欠采样:</strong>在这种情况下，文献中有三种不同类型的这些技术:NearMiss-1从多数类中选择与来自少数类的三个最近的样本具有最小平均距离的样本，NearMiss-2从多数类中选择与来自少数类的三个最远的样本具有最小平均距离的样本，NearMiss-3涉及为最接近的少数类中的每个样本选择给定数量的多数类样本。读者应该关心这种方法的应用，因为它可以大大减少多数阶级的例子的数量，并集中在少数阶级附近。这种方法的效果可能是从多数类中急剧减少大量信息。NewarMiss是使用<em class="kv"> imblearn </em>库中的<em class="kv"> NearMiss </em>类实现的。</p><p id="6c8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">浓缩最近邻法则(CNN): </strong>这个技巧和之前的有很大不同。它包括使用KNN算法对不同的类进行分类，并且不能被正确分类的多数集合被递增地添加到存储中。因此，它从少数类的一个存储开始，通过添加不能正确分类的多数类的例子来工作。这是一种欠采样技术，它寻找样本集合的子集，不会导致KNN算法的模型性能损失。读者应该注意以下几点:这种技术集中在类之间的边界线上，没有保存来自其他部分的数据集。condensednearestneighbor是使用<em class="kv"> imblearn </em>库中的<em class="kv">condensednearestneighbor</em>类实现的。</p><p id="b4ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Tomek链接欠采样:</strong>该技术基于CNN，但经过修改。CNN技术的一个问题是样本是随机选取的。这种技术的修改是选择成对的例子，每个类一个，具有彼此最小的欧几里德距离。一旦对的体积在算法中保持不变，它们就被从多数类中移除。TomekLinks是使用<em class="kv"> imblearn </em>库中的<em class="kv"> TomekLinks </em>类实现的。</p><p id="b3eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编辑过的最近邻规则(ENN): </strong>该规则涉及使用k=3个最近邻来定位数据集中那些被错误分类的样本，然后在应用k=1分类规则之前移除这些样本。<strong class="ig hi">它也适用于少数类中的每个例子，其中那些被错误分类的例子从多数类中删除了它们最近的邻居。读者应该知道，这种技术对于去除数据集中的模糊和有噪声的例子是有用的。editednearestneighbors是使用<em class="kv">imb learn</em>库中的editednearestneighbors类实现的。</strong></p><p id="4d4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">单边选择(OSS): </strong>这种技术结合了Tomek Links和CNN。Tomek链接是边界上的模糊点，在多数类中被识别和移除。然后使用CNN方法从多数类中移除远离决策边界的冗余样本。OneSidedSelection是使用<em class="kv"> imblearn </em>库中的<em class="kv"> OneSidedSelection </em>类实现的。</p><p id="f085" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">邻域清理规则(NCR): </strong>它结合了CNN规则来移除多余的示例，以及ENN规则来移除有噪声或模糊的示例。CNN以一步的方式被应用，然后根据KNN分类器被错误分类的例子被移除，如ENN规则。<strong class="ig hi"> </strong>与OSS不同，较少的冗余实例被删除，更多的注意力放在清理那些保留的实例上。这样做的原因是为了关注保留在多数类中的例子的质量(明确性)。neighborhood cleaning rule是使用<em class="kv"> imblearn </em>库中的<em class="kv">neighborhood cleaning rule</em>类实现的。</p><p id="e240" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">图像压缩技术:</strong>有许多压缩技术，如奇异值分解(PCA)、傅立叶变换、小波基变换、卷积技术，以及许多可用于压缩数据集的压缩信息技术，始终牢记这种压缩对模型的影响。</p><h1 id="f96c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">组合数据采样方法</h1><p id="79ba" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">过采样和欠采样方法的组合已经被证明是有效的技术，并且一起可以被认为是采样技术。</p><h2 id="e96b" class="kw jt hh bd ju kx ky kz jy la lb lc kc ip ld le kg it lf lg kk ix lh li ko lj bi translated"><strong class="ak">随机过采样和欠采样</strong></h2><p id="5701" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">它们结合起来会很有效。随机过采样包括随机复制少数类中的样本，而随机欠采样包括从多数类中随机删除样本。这是两种技术的中间点。</p><h2 id="2cfd" class="kw jt hh bd ju kx ky kz jy la lb lc kc ip ld le kg it lf lg kk ix lh li ko lj bi translated"><strong class="ak">平滑和随机欠采样</strong></h2><p id="fd12" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">SMOTE的工作方式是选择特征空间中接近的例子，在这些例子之间画一条线，并沿着这条线创建一个新的样本作为一个点。建议对少数类使用SMOTE，然后对多数类使用欠采样技术。</p><h2 id="1328" class="kw jt hh bd ju kx ky kz jy la lb lc kc ip ld le kg it lf lg kk ix lh li ko lj bi translated"><strong class="ak"> SMOTE和Tomek链接欠采样</strong></h2><p id="2552" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">SMOTE是一种过采样方法，它综合了少数类中新的似是而非的例子。Tomek链接是指一种用于识别数据集中具有不同类的最近邻对的方法。移除这些对中的一个或两个示例(例如多数类中的示例)具有使训练数据集中的决策边界不那么嘈杂或模糊的效果。使用Tomek链接的目的是在分类问题中更好地定义类之间的边界。</p><h2 id="1705" class="kw jt hh bd ju kx ky kz jy la lb lc kc ip ld le kg it lf lg kk ix lh li ko lj bi translated"><strong class="ak">斯莫特和ENN </strong></h2><p id="70e9" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">SMOTE可能是最流行的过采样技术，可以与许多不同的欠采样技术结合使用。另一种非常流行的欠采样方法是编辑最近邻法，或ENN。该规则涉及使用k = 3个最近邻来定位数据集中被错误分类并随后被移除的那些例子。它可以应用于所有的课程，也可以只应用于大多数课程中的例子。数据采样方法的其他组合如下:</p><blockquote class="lk ll lm"><p id="fcfa" class="ie if kv ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">浓缩最近邻+ Tomek链接</p><p id="0823" class="ie if kv ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">SMOTE + Tomek链接</p><p id="c120" class="ie if kv ig b ih ii ij ik il im in io ln iq ir is lo iu iv iw lp iy iz ja jb ha bi translated">SMOTE +编辑最近邻</p></blockquote><h1 id="333f" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论和建议</h1><p id="2484" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">读者可以看到，在机器学习项目中，有不同的技术来处理数据采样。先验地，不知道哪种技术最适合于应用，甚至不知道哪种技术最适合于所使用的模型。因此，有必要使用不同的技术，将欠采样技术与过采样技术相结合，并测量它们对模型训练的影响。重要的是，这些技术应该在培训步骤中使用。还需要在不使用数据采样技术的情况下，测量测试和验证步骤的不同模型性能指标，以查看模型如何正常工作以及模型如何学习。此外，应该强调的是，没有一种方法比另一种更好，因为取决于数据集以及模型和应用，一种技术会比另一种更有趣。总是需要测试不同的技术，将欠采样与过采样结合起来，并执行灵敏度分析，以了解它们如何改进模型。读者可以深入查阅一些书籍，如《用Python实现不平衡分类》和Kaggle中的一些项目。最后，读者永远不要忘记享受和快乐地工作。知道人生最重要的是开心。所以，我希望你喜欢这篇文章，并从中得到乐趣！</p><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh jm lt"/></div></div></a></div></div></div>    
</body>
</html>