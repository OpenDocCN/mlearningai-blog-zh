# 梅塔的西塞罗:在外交上击败人类

> 原文：<https://medium.com/mlearning-ai/metas-cicero-beating-humans-at-diplomacy-298fd1b365d7?source=collection_archive---------1----------------------->

一个能够在信任和背叛的游戏中与你交谈、说服并击败你的模型

![](img/af37fc47d540808a12b3eab567285c7d.png)

image by [Andy Kelly](https://unsplash.com/@askkell) at unsplash.com

Meta 这几天推出了一个新的模型叫做 Cicero。新模型在一个名为“外交”的游戏中有着类似人类的表现。拥有 AlphaGo 和 AlphaZero 的 DeepMind 已经让我们习惯了模型战胜最好的玩家。西塞罗为什么批判？因为这是一个不同的故事？在本文中我们一起来探讨一下。

[**外交**](https://en.wikipedia.org/wiki/Diplomacy_(game)) 是一款战略棋类游戏，由 Allan B. Callhamer 于 1954 年创作。在这个游戏中，七名玩家竞争控制地图上称为供应中心(SCs)的城市。控制最多的玩家赢得游戏(或者所有玩家可以决定平局，或者如果结束了就轮到得分)。与典型的棋类战争游戏不同的是谈判阶段。其实玩家大部分时间都在谈判联盟(也可能被背叛)。**谈判是游戏不可或缺的一部分，对玩家的胜利至关重要。**

![](img/ba09e89a55bba4e62bfed6077b0fbb34.png)

diplomacy board. image source: [here](https://en.wikipedia.org/wiki/Diplomacy_(game))

到目前为止，[强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)算法已经被用来教一个模型学习一个游戏，打败人类。一个革命是，为了加速训练，模型可以与自己对抗(自我游戏)。国际象棋、围棋、扑克和星际争霸等游戏被称为两人零和(2p0s)设置，通过拥有足够的模型容量，人工智能可以学习如何超越。

> “人工智能领域的一个主要长期目标是建立能够用自然语言与人类进行规划、协调和谈判的智能体。”— [来源](https://www.science.org/doi/10.1126/science.ade9097)

然而，在像外交这样的游戏中，合作是取胜的关键。自我游戏方法不起作用，因为模型可能会收敛于与人类交互不兼容的策略(毕竟，模型学习与另一个人工智能游戏)。事实上，尽管模型理解游戏，但之前的尝试产生了一种无法解释的语言(在游戏的 2p0s 版本中工作良好的代理在人类玩家在场时表现很差)。

> “在像外交这样战略复杂的游戏中，一个可以在人类层面上玩的代理人是合作人工智能的真正突破。”— Yann LeCun ( [来源](https://ai.facebook.com/research/cicero/))

在外交中，关键是代理人不仅能玩游戏(沟通失误导致失败)。代理人发出的信息也必须清楚，因为玩家可能不理解，要求进一步解释，或者与其他玩家合作反对代理人。

正如作者进一步指出的:

> 最后，外交是一个特别具有挑战性的领域，因为成功需要在一个鼓励玩家不要相信任何人的环境中与他人建立信任。每个回合的行动都是在无约束力的私下协商后同时发生的。为了成功，经纪人必须考虑到球员可能不会信守诺言，或者其他球员自己可能会怀疑经纪人的诚实的风险。出于这个原因，对他人的信仰、目标和意图进行推理的能力，以及通过对话说服和建立关系的能力是外交中的强大技能。——[来源](https://www.science.org/doi/10.1126/science.ade9097)

# **车型**

西塞罗结合了战略推理模块和对话模块(还有一个过滤器来消除低质量的消息)。

![](img/1e1cfac865261254578ae262739f0b61.png)

Image source: [original article](https://www.science.org/doi/10.1126/science.ade9097)

简而言之:

*   Cicero 使用了一个预先训练好的模型，这个模型已经在外交对话(在人类之间的在线比赛中获得的对话)上进行了微调。
*   **战略推理**，该模块使用强化学习算法，根据游戏状态和对话来理解其他玩家的意图和行动。同时，它计划下一步的行动
*   **消息过滤**，有几种过滤器可以避免使用无意义、质量差或没有战略价值的消息

具体来说，研究人员获得了一个包含 125，000 个在线游戏(webDiplomacy.net)的数据集，其中包含近 50，000 个对话，玩家之间总共约有 1，300 万条消息。

用于对话模块的模型是 [R2C2](https://arxiv.org/abs/2203.13224) (基于 2.7B 参数转换器的编码器-解码器模型)，然后在获得的消息数据集上对其进行微调。**为了防止模型模仿用户之间的消息，研究人员使模型可控**(模型根据特定的计划生成条件消息)。为此，消息被标注了特定的相应动作(与策略相关的动作)。

![](img/f4958235ce9b4161b7c578d3b3b1f980.png)

“Pictured are three different possible intents in the same game situation. In each case, we show a message generated by Cicero (England, pink) to France (blue), Germany (orange) and Russia (purple) conditioned on these intents. Each intent leads to quite different messages consistent with the intended actions.” Image source: [original article](https://www.science.org/doi/10.1126/science.ade9097)

然而，这个过程比看起来要复杂得多，因为玩家可以发送内容不真实的消息，或者改变他的计划。研究人员考虑了这两个方面，并相应地对数据集进行了注释。

战略模块最大限度地提高诚实度和协调能力。然而，为了防止模型泄露关于其意图的信息(例如，它想要攻击的领土)，使用了缓解策略。此外，该模型被训练成在各种合作提议中进行选择，从而为自己带来最高的预期价值。

![](img/757e1ed71759e5b37b290fdee77c1604.png)

**“Illustration of the training and inference process for intent-controlled dialogue.** Actions are specified as strings of orders for units, e.g., “NTH S BEL — HOL” means that North Sea will support Belgium to Holland. (**A**) An ‘intent model’ was trained to predict actions for a pair of players based on their dialogue. Training data was restricted to a subset where dialogue is deemed ‘truthful’ (see sup:latent_intents). (**B**) Each message in the dialogue training dataset was annotated with the output of the intent model on the dialogue up to that point, with an agreement message injected at the end. © The dialogue model was trained to predict each dataset message given the annotated intent for the target message. (**D**) During play, intents were supplied by the planning module instead.” Image source: [original article](https://www.science.org/doi/10.1126/science.ade9097)

**结果表明，该模型能够产生与游戏状态一致的消息，也能产生与代理人的计划一致的消息，此外，这些消息被人类认为是高质量的。**

![](img/ab5793e5bf8b55b479dee969bb71db98.png)

Image source: [original article](https://www.science.org/doi/10.1126/science.ade9097)

策略模块考虑选择行动，包括游戏的状态和交换的信息。西塞罗还考虑了其他参与者的潜在政策(该模型使用迭代 piKL 算法，而不是监督学习，以避免学习虚假的相关性)。如文章图片所示，该算法使用对话的结果来选择最佳策略:

![](img/80ba5bc2010fba33676e6329707e6523.png)

**此外，西塞罗考虑到其他玩家可能会对他们的计划进行欺骗。**且不说西塞罗无法获得其他玩家的消息，但模型也为他们的政策建模。

研究人员在一个在线外交网站上使用西塞罗，玩了 40 个游戏。

> Cicero 在参加一场以上比赛的参与者中排名前 10%,在参加 5 场以上比赛的 19 名参与者中排名第二。在所有 40 场比赛中，西塞罗的平均得分为 25.8%，是其 82 个对手平均得分 12.4%的两倍多。作为联赛的一部分，西塞罗参加了一场 8 场比赛的比赛，共有 21 名参与者，其中 6 人至少打了 5 场比赛。参与者最多可以玩 6 个游戏，他们的排名由他们最好的 3 个游戏的平均值决定。西塞罗在这次比赛中获得第一名。

不同的玩家对该模型进行了评论:

> “许多人类玩家会软化他们的方法，或者他们会开始受到报复的激励，但西塞罗从来不这样做。它只是按照自己看到的情况来发挥作用。因此，它在执行战略时很无情，但也不会无情到惹恼其他玩家。”——安德鲁·戈夫，三届外交世界冠军([来源](https://www.theregister.com/2022/11/23/metas_cicero_chatbot_can_probably/))

# 离别的思绪

AlphaGo 已经显示出它可以灵活地战胜人类对手。另一方面，要在现实世界中使用的代理人不仅要能移动棋子，还要能与人谈判、说服和合作(例如，如果代理人要用于谈判和销售)。西塞罗在一场外交游戏中展示了他可以通过合作来赢得游戏(展示了他是最好的 10%玩家之一)。

外交被认为是一个几乎不可能与人工智能玩的游戏:代理人不仅必须在盒子周围移动，还必须使用自然语言与其他玩家互动和谈判。**在外交上，** [**一个代理人必须考虑到，对手**](https://arxiv.org/abs/2210.05492) **可能会认为一种态度过于咄咄逼人而不予合作，虚张声势，决定与他人结盟，等等。**

西塞罗不仅掌握了自然语言，还掌握了策略，取得了惊人的成就:

> 西塞罗在与其他球员制定策略时能够清晰而有说服力地说话。例如，在一个演示游戏中，西塞罗要求一名玩家立即支持棋盘的一部分，同时要求另一名玩家在游戏后期考虑结盟。— [来源](https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/)

![](img/78aeb48a3de9adea1ba40008403f216c.png)

Cicero dialogue. image source: [original article](https://www.science.org/doi/10.1126/science.ade9097)

另一方面，模型还不完善，说明还有可以改进的地方。例如，在某些情况下，他发送无意义的信息或与以前的信息相矛盾的信息。**虽然总的来说，大多数用户并没有意识到他们在玩一个 AI。**

> 没有游戏中的消息表明玩家相信他们在和一个人工智能代理人一起玩。一名球员在赛后聊天中提到了对西塞罗的一个账户可能是机器人的怀疑，但这并没有导致西塞罗被联盟中的其他球员检测为 AI 代理人。

这种模式除了代表人工智能证明有能力的另一个游戏之外，还可以有其他应用。同时，它提供了一个研究互动的环境，在这种互动中，目标比简单地选择最佳行动更复杂:**在短期内妥协以保持盟友，从而能够提高自己的胜算。**

从长远来看，作者已经提供了代码，以便社区可以测试它。类似的模型可能会彻底改变视频游戏，在游戏中，非玩家角色(NPC)可以与人类玩家交谈和计划。Meta 还将 Cicero 这样的模型视为元宇宙棋盘上的棋子，用户可以在这里谈判、学习新技能，并处于一个更加身临其境的环境中。

另一方面，这种模式提出了决定性的伦理问题。这种模型可以模仿其他人，并说服人们做一些危险的事情或骗取钱财。你怎么看？你觉得是突破还是危险？请在评论中告诉我你的想法

代号:[此处](https://github.com/facebookresearch/diplomacy_cicero)，原创文章:[此处](https://www.science.org/doi/10.1126/science.ade9097)，官方博文:[此处](https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/)

# 如果你觉得有趣:

你可以寻找我的其他文章，你也可以 [**订阅**](https://salvatore-raieli.medium.com/subscribe) 在我发表文章时得到通知，你也可以在**[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**上连接或联系我。**感谢您的支持！**

**这是我的 GitHub 知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。**

**[](https://github.com/SalvatoreRa/tutorial) [## GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…

### 关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…

github.com](https://github.com/SalvatoreRa/tutorial) 

或者随意查看我在 Medium 上的其他文章:

[](/mlearning-ai/is-ai-changing-football-123386582c9b) [## AI 在改变足球吗？

### 数据科学已经来到了足球界。团队和公司如何使用它？

medium.com](/mlearning-ai/is-ai-changing-football-123386582c9b) [](https://towardsdatascience.com/how-artificial-intelligence-could-save-the-amazon-rainforest-688fa505c455) [## 人工智能如何拯救亚马逊雨林

### 亚马逊正处于危险之中，人工智能可以帮助保护它

towardsdatascience.com](https://towardsdatascience.com/how-artificial-intelligence-could-save-the-amazon-rainforest-688fa505c455) [](https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221) [## 说生命的语言:AlphaFold2 和公司如何改变生物学

### 人工智能正在重塑生物学研究，并开辟治疗的新领域

towardsdatascience.com](https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221) [](https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7) [## AI 新绽放？为什么布鲁姆模式可以改变游戏规则

### 我们现在已经习惯了大型语言模型，为什么这个如此特别呢？

pub.towardsai.net](https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7) [](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)**