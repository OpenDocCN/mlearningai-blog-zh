<html>
<head>
<title>Getting Contextualized Word Embeddings with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT获得上下文化的单词嵌入</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/getting-contextualized-word-embeddings-with-bert-20798d8b43a4?source=collection_archive---------1-----------------------#2022-05-29">https://medium.com/mlearning-ai/getting-contextualized-word-embeddings-with-bert-20798d8b43a4?source=collection_archive---------1-----------------------#2022-05-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/d0ffa6967217b2efaa7421bf39c03b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*mV-9qh8FhiRs3KG6.gif"/></div></figure><div class=""/><div class=""><h2 id="f4dd" class="pw-subtitle-paragraph il hn ho bd b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc dx translated">如何使用Python、PyTorch和transformers库在BERT中获得上下文化的单词嵌入。</h2></div><figure class="je jf jg jh fd hj er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jd"><img src="../Images/7e74dcc2123acd518039e95b35c08bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wqtb5IH-eydH9t1gnOjKxw.jpeg"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">[<a class="ae jq" href="https://unsplash.com/s/photos/words" rel="noopener ugc nofollow" target="_blank">Source</a>]</figcaption></figure><p id="27b2" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi kn translated">伯特、埃尔莫和GPT-2证明，语境化单词嵌入是自然语言处理中一项改变游戏规则的创新。使用上下文化的单词表示代替静态向量(例如，word2vec)已经改进了几乎每个NLP任务。</p><blockquote class="kw kx ky"><p id="a933" class="jr js kz jt b ju jv ip jw jx jy is jz la kb kc kd lb kf kg kh lc kj kk kl km ha bi translated">但是，这些表象首先是如何语境化的呢？</p></blockquote><p id="6516" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">想想这个词<em class="kz">‘老鼠’</em>。它有多重含义，其中一个指啮齿动物，另一个指一种装置。伯特能够正确地为每个词义建立一个“<em class="kz">鼠标</em>”表示吗？</p><figure class="je jf jg jh fd hj er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ld"><img src="../Images/f7c8b38efd6bc700d4287694a979a95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/0*VB6byprDMCGvKNdA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Multiple embeddings for the same word ‘<em class="le">mouse</em>’. [<a class="ae jq" href="http://ai.stanford.edu/blog/contextual/" rel="noopener ugc nofollow" target="_blank">Source</a>]</figcaption></figure></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="bdd1" class="lm ln ho bd lo lp lq lr ls lt lu lv lw iu lx iv ly ix lz iy ma ja mb jb mc md bi translated"><strong class="ak">用Python生成BERT嵌入</strong></h1><p id="6658" class="pw-post-body-paragraph jr js ho jt b ju me ip jw jx mf is jz ka mg kc kd ke mh kg kh ki mi kk kl km ha bi translated">查看我的Colab笔记本，获取完整代码。T11】</p><p id="5911" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">为了形象化语境化单词嵌入的概念，让我们看一个小例子。对于下面的文本语料库，如下面的所示，BERT用于为每个单词生成上下文化的单词嵌入。</p><figure class="je jf jg jh fd hj er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mj"><img src="../Images/69a09158055729b35ce0abf9b8462f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0J5r_MI8LWuy6OijbDH_Tw.png"/></div></div></figure><p id="393d" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">在本文中,“银行”一词有四种不同的含义。第一句话是上下文无关的。此后，“银行”一词指金融机构、河岸、银行金库和动词。</p><p id="392a" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated"><strong class="jt hp">无聊的Python玩意儿</strong></p><p id="a13e" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">导入包:</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="a936" class="mp ln ho ml b fi mq mr l ms mt">import pandas as pd<br/>import numpy as np<br/>import torch</span></pre><p id="0b11" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">接下来加载预训练的BERT模型和标记器:</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="2783" class="mp ln ho ml b fi mq mr l ms mt">from transformers import BertModel, BertTokenizer</span><span id="898a" class="mp ln ho ml b fi mu mr l ms mt">model = BertModel.from_pretrained('bert-base-uncased',<br/>           output_hidden_states = True,)<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</span></pre><p id="128a" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">我们必须将输入的文本转换成BERT可以阅读的特定格式。主要是我们在输入的开头加上` `[CLS]`` `,在结尾加上` `[SEP]``'。然后，我们将符号化的BERT输入转换为张量格式。</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="1c66" class="mp ln ho ml b fi mq mr l ms mt">def bert_text_preparation(text, tokenizer):<br/>  """<br/>  Preprocesses text input in a way that BERT can interpret.<br/>  """<br/>  marked_text = "[CLS] " + text + " [SEP]"<br/>  tokenized_text = tokenizer.tokenize(marked_text)<br/>  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)<br/>  segments_ids = [1]*len(indexed_tokens)</span><span id="4cfa" class="mp ln ho ml b fi mu mr l ms mt"># convert inputs to tensors<br/>  tokens_tensor = torch.tensor([indexed_tokens])<br/>  segments_tensor = torch.tensor([segments_ids])</span><span id="d3cc" class="mp ln ho ml b fi mu mr l ms mt">return tokenized_text, tokens_tensor, segments_tensor</span></pre><p id="b78f" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">为了获得实际的BERT嵌入，我们将预处理的输入文本(现在用张量表示)放入我们预先训练的BERT模型中。</p><p id="5bbb" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">哪一种向量最适合作为情境化嵌入？我认为这取决于任务。提出BERT的原始论文考察了六个选择。</p><p id="aeca" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">我选择了其中一个在他们的实验中运行良好的选项，这是模型最后四层的总和。</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="23e7" class="mp ln ho ml b fi mq mr l ms mt">def get_bert_embeddings(tokens_tensor, segments_tensor, model):<br/>    """<br/>    Obtains BERT embeddings for tokens.<br/>    """<br/>    # gradient calculation id disabled<br/>    with torch.no_grad():<br/>      # obtain hidden states<br/>      outputs = model(tokens_tensor, segments_tensor)<br/>      hidden_states = outputs[2]</span><span id="83c5" class="mp ln ho ml b fi mu mr l ms mt">    # concatenate the tensors for all layers<br/>    # use "stack" to create new dimension in tensor<br/>    token_embeddings = torch.stack(hidden_states, dim=0)</span><span id="1f91" class="mp ln ho ml b fi mu mr l ms mt">    # remove dimension 1, the "batches"<br/>    token_embeddings = torch.squeeze(token_embeddings, dim=1)</span><span id="4266" class="mp ln ho ml b fi mu mr l ms mt">    # swap dimensions 0 and 1 so we can loop over tokens<br/>    token_embeddings = token_embeddings.permute(1,0,2)</span><span id="97c5" class="mp ln ho ml b fi mu mr l ms mt">    # intialized list to store embeddings<br/>    token_vecs_sum = []</span><span id="c330" class="mp ln ho ml b fi mu mr l ms mt">    # "token_embeddings" is a [Y x 12 x 768] tensor<br/>    # where Y is the number of tokens in the sentence</span><span id="4f1b" class="mp ln ho ml b fi mu mr l ms mt">    # loop over tokens in sentence<br/>    for token in token_embeddings:</span><span id="4180" class="mp ln ho ml b fi mu mr l ms mt">    # "token" is a [12 x 768] tensor</span><span id="20ab" class="mp ln ho ml b fi mu mr l ms mt">    # sum the vectors from the last four layers<br/>        sum_vec = torch.sum(token[-4:], dim=0)<br/>        token_vecs_sum.append(sum_vec)</span><span id="f3fe" class="mp ln ho ml b fi mu mr l ms mt">return token_vecs_sum</span></pre><p id="9876" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">现在我们可以为一组上下文创建上下文嵌入。</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="3acd" class="mp ln ho ml b fi mq mr l ms mt">sentences = ["bank",<br/>         "he eventually sold the shares back to the bank at a premium.",<br/>         "the bank strongly resisted cutting interest rates.",<br/>         "the bank will supply and buy back foreign currency.",<br/>         "the bank is pressing us for repayment of the loan.",<br/>         "the bank left its lending rates unchanged.",<br/>         "the river flowed over the bank.",<br/>         "tall, luxuriant plants grew along the river bank.",<br/>         "his soldiers were arrayed along the river bank.",<br/>         "wild flowers adorned the river bank.",<br/>         "two fox cubs romped playfully on the river bank.",<br/>         "the jewels were kept in a bank vault.",<br/>         "you can stow your jewellery away in the bank.",<br/>         "most of the money was in storage in bank vaults.",<br/>         "the diamonds are shut away in a bank vault somewhere.",<br/>         "thieves broke into the bank vault.",<br/>         "can I bank on your support?",<br/>         "you can bank on him to hand you a reasonable bill for your   <br/>services.",<br/>         "don't bank on your friends to help you out of trouble.",<br/>         "you can bank on me when you need money.",<br/>         "i bank on your help."<br/>         ]</span><span id="3879" class="mp ln ho ml b fi mu mr l ms mt">from collections import OrderedDict</span><span id="003c" class="mp ln ho ml b fi mu mr l ms mt">context_embeddings = []<br/>context_tokens = []</span><span id="fdaa" class="mp ln ho ml b fi mu mr l ms mt">for sentence in sentences:<br/>  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)<br/>  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)</span><span id="e9c3" class="mp ln ho ml b fi mu mr l ms mt">  # make ordered dictionary to keep track of the position of each   word<br/>  tokens = OrderedDict()</span><span id="5296" class="mp ln ho ml b fi mu mr l ms mt">  # loop over tokens in sensitive sentence<br/>  for token in tokenized_text[1:-1]:<br/>    # keep track of position of word and whether it occurs multiple times<br/>    if token in tokens:<br/>      tokens[token] += 1<br/>    else:<br/>      tokens[token] = 1</span><span id="a447" class="mp ln ho ml b fi mu mr l ms mt">  # compute the position of the current token<br/>    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]<br/>    current_index = token_indices[tokens[token]-1]</span><span id="3805" class="mp ln ho ml b fi mu mr l ms mt">  # get the corresponding embedding<br/>    token_vec = list_token_embeddings[current_index]<br/>    <br/>    # save values<br/>    context_tokens.append(token)<br/>    context_embeddings.append(token_vec)</span></pre><h1 id="64e2" class="lm ln ho bd lo lp mv lr ls lt mw lv lw iu mx iv ly ix my iy ma ja mz jb mc md bi translated"><strong class="ak">可视化结果</strong></h1><p id="f0ea" class="pw-post-body-paragraph jr js ho jt b ju me ip jw jx mf is jz ka mg kc kd ke mh kg kh ki mi kk kl km ha bi translated">我们可以使用TensorFlow的<a class="ae jq" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>来可视化我们的多维单词嵌入。为了做到这一点，我们首先必须将上面生成的BERT嵌入保存为。tsv文件。</p><pre class="je jf jg jh fd mk ml mm mn aw mo bi"><span id="3502" class="mp ln ho ml b fi mq mr l ms mt">import os</span><span id="ced9" class="mp ln ho ml b fi mu mr l ms mt">filepath = os.path.join('gdrive/My Drive/projections/')</span><span id="f113" class="mp ln ho ml b fi mu mr l ms mt">name = 'metadata_small.tsv'</span><span id="6ec5" class="mp ln ho ml b fi mu mr l ms mt">with open(os.path.join(filepath, name), 'w+') as file_metadata:<br/>  for i, token in enumerate(context_tokens):<br/>    file_metadata.write(token + '\n')</span><span id="6086" class="mp ln ho ml b fi mu mr l ms mt">import csv</span><span id="ce0c" class="mp ln ho ml b fi mu mr l ms mt">name = 'embeddings_small.tsv'</span><span id="7ef9" class="mp ln ho ml b fi mu mr l ms mt">with open(os.path.join(filepath, name), 'w+') as tsvfile:<br/>    writer = csv.writer(tsvfile, delimiter='\t')<br/>    for embedding in context_embeddings:<br/>        writer.writerow(embedding.numpy())</span></pre><p id="d2ec" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">现在我们可以将这些文件上传到<a class="ae jq" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>上，以获得我们结果的可视化。</p><figure class="je jf jg jh fd hj er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es na"><img src="../Images/e2d17227d345647efb0cf20becaeb4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wWsEUN7K5I5mSYMNNEoqUQ.gif"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx"><a class="ae jq" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/arminmirrezai/2c59f675a997fef20d59851291f8c268/raw/c542be898dc77aad2d593ebcd7b60ac302b71359/gistfile1.txt" rel="noopener ugc nofollow" target="_blank">The results can be seen here.</a></figcaption></figure><p id="8051" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated">我们看到了单词“bank”及其最近邻词的上下文化单词嵌入的可视化。显而易见，与指代金融银行的单词embedding相比，指代河岸的单词embedding在嵌入空间中具有不同的位置。</p></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><p id="d3d3" class="pw-post-body-paragraph jr js ho jt b ju jv ip jw jx jy is jz ka kb kc kd ke kf kg kh ki kj kk kl km ha bi translated"><strong class="jt hp">关注我:<br/> </strong> <a class="ae jq" href="https://twitter.com/r3d_robot" rel="noopener ugc nofollow" target="_blank">推特:@ r3d _ robot</a><br/><a class="ae jq" href="https://www.youtube.com/channel/UC-47UN9znQBo3ItNj8Ghspw/featured" rel="noopener ugc nofollow" target="_blank">Youtube:r3d _ robot</a></p><figure class="je jf jg jh fd hj er es paragraph-image"><div class="er es nb"><img src="../Images/fbc444d1b670a3a0a27684f29fada884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*usC4ZdxuD-d_aYum.gif"/></div></figure><div class="hg hh ez fb hi nc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hp fi z dy nh ea eb ni ed ef hn bi translated">Mlearning.ai提交建议</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">medium.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq hk nc"/></div></div></a></div></div></div>    
</body>
</html>