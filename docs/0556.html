<html>
<head>
<title>The forest of amazon reviews, part 1 (an NLP story)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊森林评论，第1部分(NLP的故事)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-forest-of-the-amazon-reviews-part-1-an-nlp-story-fa98efa892b8?source=collection_archive---------3-----------------------#2021-05-14">https://medium.com/mlearning-ai/the-forest-of-the-amazon-reviews-part-1-an-nlp-story-fa98efa892b8?source=collection_archive---------3-----------------------#2021-05-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/896fe448bbfa74024d3ab3e73d723029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qo05VkcCRPyT_Wox-li1mw.jpeg"/></div></div></figure><p id="bf03" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇文章中，我们将进入<a class="ae jn" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP ( <em class="jo">自然语言处理</em> ) </a>的世界，研究计算机和人类语言之间的互动。</p><p id="142d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">文本分析是机器学习算法的一个主要应用领域。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/d27ba14ef57c45f0b9f17f42fd0adddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*nBc0DhmFn9d78v_YkN4uBA.png"/></div></figure><p id="9617" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将解释如何分析从亚马逊产品评论中获得的文本，我们称之为<strong class="ir hi">情感分析。</strong></p><p id="a18a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在亚马逊评论中，客户在他们的评论上打星号(1到5)，在这种情况下，我们只考虑好的(4.5)或坏的(1.2)评论，而不是中性的。</p><p id="3565" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后应用预测模型并评估其性能。我们将分两部分进行:</p><p id="da1e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将通过删除对分析文本无用的内容来清理文本。我们将使用NLTK删除停用词并对单词进行词干分析。我们将创建一个矢量器，允许我们创建一个重复单词的矩阵。</p><p id="9fb9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第2部分</strong>)我们将把一个叫做随机森林的模型应用到我们在第1部分中获得的数据帧中。然后，我们将使用不同的指标评估它的性能，并使用GridSearchCV测试模型的不同配置</p><p id="be65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，解释太多了！！，我建议你在阅读我将解释的每个概念及其代码时，打开你的jupyter，打开这篇文章专用的笔记本，自己尝试一下。</p><h1 id="24bf" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">导入库</h1><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="e083" class="kx jv hh kt b fi ky kz l la lb">import pandas as pd<br/>import nltk<br/>import re<br/>import string</span></pre><p id="db98" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从nltk我们将需要:停用词和PorterStemmer，但是它们是干什么用的呢？</p><p id="725f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">停用词</strong></p><p id="2325" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">包含一组无意义的单词，如冠词、代词、介词等。在处理之前或之后被过滤。例如:“我”、“我”、“我的”、“我自己”、“我们”、“我们的”、“我们的”、“我们自己”、“你”、“你是”、…</p><p id="c49f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">词干化和词汇化</strong></p><p id="65d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PorterStemmer是词干的一种:“<em class="jo">词干(词根)是单词中你加上屈折(变化/派生)词缀的部分，如</em><strong class="ir hi"><em class="jo"/></strong><em class="jo">(-ed，-ize，-s，-de，-ing) </em> <strong class="ir hi"> <em class="jo">。</em> </strong> <em class="jo">因此，对一个单词或句子进行词干分析可能会产生不是实际单词的单词。词干是通过去掉一个词的后缀或前缀而产生的。</em>(<strong class="ir hi">datacamp.com</strong>)</p><p id="e9f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"/>另一个类似的技术是词汇化:“<em class="jo">词汇化，不同于词干化，适当地减少了词形变化，确保词根属于该语言。在引理化中，词根叫做</em> <strong class="ir hi">引理</strong> <em class="jo">。一个词条(复数词条或词条)是一组单词的规范形式、词典形式或引用形式</em><a class="ae jn" href="http://datacamp.com/" rel="noopener ugc nofollow" target="_blank">(<strong class="ir hi">datacamp.com</strong>)</a></p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/0b5ee201251eee87f7e6ce2fe0cb7fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ia4Fiysk-snfHoAgqsG3Dw.png"/></div></div></figure><p id="6328" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，词干化和词尾化都会产生词根形式的词形变化。区别在于词干可能不是一个真实的单词，而lemma是一个真实的语言单词。两者都是在文本挖掘中广泛使用的好技术。</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="1b20" class="kx jv hh kt b fi ky kz l la lb"># nltk libraries<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>from nltk import PorterStemmer</span><span id="68fd" class="kx jv hh kt b fi ld kz l la lb">stopwords = stopwords.words('english')<br/>ps = PorterStemmer()</span></pre><h1 id="ca5e" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">获取数据</h1><p id="ef79" class="pw-post-body-paragraph ip iq hh ir b is le iu iv iw lf iy iz ja lg jc jd je lh jg jh ji li jk jl jm ha bi translated">我们从kaggle ( <a class="ae jn" href="https://www.kaggle.com/bittlingmayer/amazonreviews" rel="noopener ugc nofollow" target="_blank"> <em class="jo">亚马逊评论进行情感分析</em> </a>)获取数据。数据在用bz2压缩的txt文件中，格式如下:</p><p id="775e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> __label__ ReviewText </strong>，其中标签可以是<em class="jo"> __label__1 </em>(差评)或<em class="jo"> __label__2 </em>(好评)</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="d3eb" class="kx jv hh kt b fi ky kz l la lb">import bz2 #unzip and load<br/>train_file = bz2.BZ2File('train.ft.txt.bz2')<br/>test_file = bz2.BZ2File('test.ft.txt.bz2')<br/>train_lines = train_file.readlines()<br/>test_lines = test_file.readlines()<br/>print('Train line: {}'.format(train_lines[11]))</span></pre><p id="e405" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">火车线路:b'__label__2伟大的书:这是一本伟大的书，我简直爱不释手，可以……</em></p><p id="307a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有360万条火车线，40万条试验线！！</p><p id="faf7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于处理时间的原因，我们将只提取一小部分数据</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="2727" class="kx jv hh kt b fi ky kz l la lb">train_part = train_lines[:10000]<br/>test_part = test_lines[:2000]<br/># Put all together<br/>amz_rev_part = train_part + test_part</span></pre><h1 id="3b9e" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">准备数据</h1><p id="7d0a" class="pw-post-body-paragraph ip iq hh ir b is le iu iv iw lf iy iz ja lg jc jd je lh jg jh ji li jk jl jm ha bi translated">现在我们将获得X(评论)和y(标签)的值，在X中有一些清理和转换任务</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="40c5" class="kx jv hh kt b fi ky kz l la lb"># Get label text and return 0 (__label__1=bad review) and 1 (__label__1=good_review)<br/>def reviewToY(review): <br/>    return 0 if review.split(' ')[0] == '__label__1' else 1</span><span id="a688" class="kx jv hh kt b fi ld kz l la lb"># Get review string feature<br/># delete the last char (\n) and transform to lower<br/>def reviewToX(review):<br/>    review = review.split(' ', 1)[1][:-1].lower()<br/>    return review</span><span id="2571" class="kx jv hh kt b fi ld kz l la lb">def splitReviewsLabels(lines): <br/>    reviews = []<br/>    labels = []<br/>    for review in tqdm(lines):<br/>        rev = reviewToX(review)<br/>        label = reviewToY(review)<br/>        # only get the first 512 chars for review<br/>        reviews.append(rev[:512])<br/>        labels.append(label)<br/>    return reviews, labels</span><span id="048a" class="kx jv hh kt b fi ld kz l la lb">X_amz_rev_part, y_amz_rev_part = splitReviewsLabels(amz_rev_part)</span></pre><p id="d773" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来创建一个方法来标记我们的评论(<em class="jo">获取单词列表</em>)，移除停用词并对单词进行词干化:</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="e43c" class="kx jv hh kt b fi ky kz l la lb"># Tokenize sentence, then delete stopwords and stemming<br/>def remove_stopword_and_stem(text):<br/>    tokens = re.split('\W+', text)<br/>    text = [ps.stem(word) for word in tokens if word not in stopwords]<br/> return text</span></pre><h1 id="fb4c" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">用计数矢量器创建单词包</h1><p id="5175" class="pw-post-body-paragraph ip iq hh ir b is le iu iv iw lf iy iz ja lg jc jd je lh jg jh ji li jk jl jm ha bi translated">什么是一袋单词？</p><p id="55c5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">BoW通过计算一个单词在文档中出现的次数，将文本转换为固定长度的向量。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/b3cc1b6904db83daeefdf2e72aba64a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*cNEEuxQs443qpPQvU1Z7iw.png"/></div></figure><p id="3a8e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在Scikit Learn中，有几个生成弓形的矢量器，其中最常用的是CountVectorizer和TfidfVectorizer。</p><p id="9dc7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有关该主题的详细信息，请阅读<a class="ae jn" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank">sk learn-feature-extraction</a>中的“文本特征提取”</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="3730" class="kx jv hh kt b fi ky kz l la lb"># create vectorizer<br/>count_vect = CountVectorizer(analyzer=remove_stopword_and_stem)<br/># train the vectorizer<br/>vec = count_vect.fit(X_amz_rev_part)<br/># create my bag of words (X_cv) with transform()<br/>X_cv = vec.transform(X_amz_rev_part)<br/># view BoW<br/>print(X_cv.shape)<br/>print(count_vect.get_feature_names()[3000:3010])</span></pre><p id="362b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> (12000，31803) </em></p><p id="f867" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> ['bock '，' bode '，' bodi '，' bodic '，' bodo '，' bodybuild '，' bodyfin '，' bodygroom '，'保镖'，' bodyment'] </em></p><p id="49e5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我的弓的类型和形状:</p><p id="13e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> &lt; 12000x31803以压缩稀疏行格式&gt; </em>存储了351918个元素的“&lt;类“numpy . int 64”&gt;类型的稀疏矩阵】</p><p id="d39e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们用我的弓信息创建一个新的数据帧:</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="5aa3" class="kx jv hh kt b fi ky kz l la lb"># build dataframe with the sparse matrix <br/>X_features_df = pd.DataFrame(X_cv.toarray())<br/># Rename columns with the word<br/>X_features_df.columns = count_vect.get_feature_names()<br/>X_features_df</span></pre><p id="2635" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然我们已经有了从哪里获得X(特性)的数据框架，我们就可以根据已经有的数据添加新的列，比如一个特性，它具有每个评论的字母数。</p><p id="d841" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们获得我们的原始df ( <em class="jo"> X_amz_rev_part </em>)并对评论字母进行计数，然后在新df ( <em class="jo"> X_features_df </em>)中创建新特征(<em class="jo"> review_len </em>):</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="0aaa" class="kx jv hh kt b fi ky kz l la lb"># Adding new features , For ex. = len(review) <br/>X_amz_df = pd.DataFrame(X_amz_rev_part, columns = ['Review'])<br/>X_features_df['review_len'] = X_amz_df['Review'].apply(lambda x: len(x) - x.count(" "))<br/>X_features_df['review_len']</span></pre><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/744a7cad96abc9b03100f4cadc205579.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*NvGgybBWzEJFqb7_yWXjAQ.png"/></div></figure><p id="f5ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好的，很多数字，矩阵和向量，如果我们把一些人类更容易理解的东西可视化会怎么样？:)</p><p id="99f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">比如《我的弓》中的十大常用词:</p><pre class="jq jr js jt fd ks kt ku kv aw kw bi"><span id="8d51" class="kx jv hh kt b fi ky kz l la lb"># Sum words of my Bag of Words, and get an ordered freq. of that<br/>sum_words = X_cv.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]<br/>words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)</span><span id="ee88" class="kx jv hh kt b fi ld kz l la lb"># get the top 10 common words<br/>common_words = words_freq[:10]<br/># create df with top words and count<br/>df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])</span><span id="a120" class="kx jv hh kt b fi ld kz l la lb"># build a bar graph and plot<br/>import matplotlib.pyplot as plt<br/>fig = plt.gcf()<br/>fig.set_size_inches( 20, 8)<br/>ax = df.plot.bar(x='ReviewText', y='count', rot=0)</span></pre><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/a4e5355be18e9ec51ebdab74bcf270c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*gRE_p7diEZQw5o7G7E4xlw.png"/></div></figure><p id="206e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，伙计们，这是我们对亚马逊评论情感分析的第一部分。我们已经获得了机器学习算法所需的数字形式的数据。</p><p id="1f38" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<strong class="ir hi">第二部分</strong>中，我们将对这些数据应用一个叫做<strong class="ir hi">随机森林</strong>的ML分类器，我们希望它能够最精确地预测评论是好是坏。</p><p id="c2b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">像往常一样，github链接附有完整的<a class="ae jn" href="https://github.com/jrercoli/rf_amazon_reviews_part1" rel="noopener ugc nofollow" target="_blank">情绪分析jupyter nb </a>以便你可以自己验证代码。也请关注我的数据科学博客<a class="ae jn" href="https://rorjor.wixsite.com/empoweredatascience" rel="noopener ugc nofollow" target="_blank">赋能数据科学</a></p><p id="4db6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的评论和/或您的喜欢；)</p></div></div>    
</body>
</html>