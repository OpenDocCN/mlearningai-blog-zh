<html>
<head>
<title>Scaling large language models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">扩展大型语言模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/scaling-large-language-models-83f43705f7e6?source=collection_archive---------1-----------------------#2022-05-04">https://medium.com/mlearning-ai/scaling-large-language-models-83f43705f7e6?source=collection_archive---------1-----------------------#2022-05-04</a></blockquote><div><div class="ds gz ha hb hc hd"/><div class="he hf hg hh hi"><div class=""/><p id="821d" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">新型的NLP模型胜过GPT-3。将PaLM模型视为一种方法。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/7e9c0eca6352e120f28a13feb3e9e286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PfjtQVj4OuXg2QW6"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@ryunosuke_kikuno?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ryunosuke Kikuno</a> on <a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7f7e" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated"><strong class="ik hm">简介</strong></p><p id="6fb4" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">OpenAI于2020年6月发布了GPT-3。现在，两年后，我们看到竞争对手出现了。</p><p id="74de" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">与GPT-3相比，他们中的一些人使用新的方法来实现最先进的性能。</p></div></div>    
</body>
</html>