# 无模型 v .基于模型的强化学习

> 原文：<https://medium.com/mlearning-ai/model-free-v-model-based-reinforcement-learning-3f33b49e5753?source=collection_archive---------2----------------------->

所以你想学习强化学习？好吧，公平的警告。做好带着困惑进入这个领域的准备。

方法一个接一个。使解释混乱的词语和术语。该怎么办呢？

好吧，让我们来理解强化学习的大类*实际上是什么*，以及它们之间的区别。从那里，我们可以理解属于某些类别的方法的重要特征，并且能够拓宽我们对该领域的整体理解！

![](img/16bfc83d32837c32cf9b74131b6f5cc0.png)

上次，我们谈到了[政策内方法和政策外方法的区别。](/mlearning-ai/on-policy-v-off-policy-reinforcement-learning-explained-89054a6cc6)这次，我们讨论的是无模型和基于模型的强化学习。

通常，所有深度强化学习方法都可以分为无模型或基于模型。幸运的是，这两个类别并不太难理解。我们来分解一下。

那么无模型和基于模型到底是什么意思呢？简单地说，无模型方法**没有**环境模型，而基于模型的方法**有**环境模型。

够简单吧？好吧，让我们定义一个环境的模型实际上*是什么*，这样我们可以更好地理解有一个和没有一个的方法实际上是什么样子。

我假设你熟悉强化学习的基础:一个*代理*驻留在一个环境中，并且拥有该环境的当前*状态*。代理在此状态下采取*动作*，并接收*奖励*和*下一个状态。*

因为代理人的目标是最大化所有未来的回报，所以它必须在行动的同时牢记其行动的后果:眼前的回报和下一个状态。

事实上，我们的代理人希望能够预测未来，并了解其行为将带来的回报和未来状态。“模型”就是这样做的。

在给定当前状态和动作的情况下，该模型明确地定义了结束于状态 t+1 的概率。用数学术语来说:

![](img/ac018a1582aee234f4794f6f2d093ab7.png)

Otherwise known as transition probability

该模型是一个显式函数——我们输入当前状态和我们采取的行动，它会给出我们将结束的下一个状态。

一般来说，模型可以是*已知的*(即近似的)，也可以是*已知的*。

**已知型号**给代理；他们不需要训练。一个很好的例子就是游戏规则，比如国际象棋。国际象棋的模型是已知的——代理不必学习它。这就是游戏规则。

代理知道在它移动一个棋子之后，下一个状态看起来完全一样，除了它移动的棋子将在不同的位置。如果这个动作抓住了一个棋子，代理人将会知道拿走这个棋子的回报是什么。毕竟，这些只是游戏规则。

或者，模型可以是模拟器中的物理定律。假设代理是一个虚拟人，试图行走。已知的物理模型将让代理知道，通过用 *x* 的力向下推一条腿，结果将是同样大小的力向上推虚拟人。

在这两种情况下，模型将计算下一个状态的概率，但概率每次都是 100%。它肯定知道下一个状态是什么。

**学习模型**是没有给代理的模型。相反，代理必须通过经验来学习模型。最常见的是，该模型由神经网络表示。神经网络将学习输出下一个状态的有效猜测。

在这两种情况下，重要的是要注意模型与代理是分离的。代理将显式地引用该模型来计算出下一个状态是什么，但是然后它将决定在给定该信息的情况下应该采取哪个动作。

因此，**基于模型的**方法有一个明确的模型，它可以是一个近似下一个状态的神经网络，也可以是一个已知的模型(硬代码)，它将给出确定的下一个状态。代理参考这个模型来查看下一个状态是什么，并基于该知识做出决策。

**无模型**方法在决定行动时仍然需要能够考虑未来。但是它们没有明确地定义预测的下一个状态。相反，神经网络近似某种状态或动作的值。这个值*隐含地*考虑了未来的奖励和状态。然而，还没有模型可以确定下一个状态是什么。

如果成功，基于模型的方法比无模型的方法更有效。能够自信地预测下一个状态允许代理更快地确定最佳行为。

然而，在必须学习模型的情况下，问题变得明显。学习准确的模型可能是困难的，并且模型中的小的不一致可能使学习代理的最优策略的过程不可能。

每种类型都有自己的优点和缺点，在大多数情况下，自信地说一种类型比另一种类型好是不准确的。基于模型的方法需要较少的样本，但可能不准确，而无模型的方法需要更多的样本，但具有更稳定和可靠的学习。

![](img/b9d2dce75cee32934b5d221eaf25ef96.png)

我希望这消除了关于无模型和基于模型的强化学习的任何不确定性。如果你想了解政策内和政策外方法的区别，看看这篇文章！

否则，祝你在强化学习的丛林之旅中好运；)*如果你有任何关于 RL 相关的问题，请随时联系我！推特:@jereminuer*

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)