<html>
<head>
<title>An overview of VGG16 and NiN models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VGG16和NiN模型概述</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-overview-of-vgg16-and-nin-models-96e4bf398484?source=collection_archive---------0-----------------------#2021-03-25">https://medium.com/mlearning-ai/an-overview-of-vgg16-and-nin-models-96e4bf398484?source=collection_archive---------0-----------------------#2021-03-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="dc89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章旨在简要介绍两个经典的卷积神经网络，VGG16和NiN(又名网络中的网络)。我们将发现他们的架构以及他们在Keras平台上的实现。可以参考我之前的一些相关主题的博客:<a class="ae jc" href="https://lekhuyen.medium.com/convolutional-neural-networks-3f00c165c9d9" rel="noopener">卷积神经网络</a>、<a class="ae jc" href="https://lekhuyen.medium.com/lenet-and-mnist-handwritten-digit-classification-354f5646c590" rel="noopener"> LeNet </a>、<a class="ae jc" href="https://lekhuyen.medium.com/alexnet-and-image-classification-8cd8511548b4" rel="noopener"> Alexnet </a>模型。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="5b7d" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">一. VGG模式</h1><h2 id="0957" class="ki jl hh bd jm kj kk kl jq km kn ko ju ip kp kq jy it kr ks kc ix kt ku kg kv bi translated"><strong class="ak"> <em class="kw"> 1。</em>简介</strong></h2><p id="a57e" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">VGG是一种深度卷积神经网络，由卡伦·西蒙扬和安德鲁·齐泽曼提出[1]。VGG是他们小组名称的首字母缩写，来自牛津大学的视觉几何小组。该模型在ILSVRC-2014竞赛中获得第二名，实现了92.7%的分类性能。VGG模型使用非常小的卷积滤波器(3 × 3)来研究层的深度，以处理大规模图像。作者发布了一系列不同层长度的VGG模型，从11层到19层，如下表所示。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/05cfec885627f7d90909539db622ba35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PEvSaceAL8nNLTco_SYieg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Table 1: Different configurations of VGG. <a class="ae jc" href="https://pub.towardsai.net/the-architecture-and-implementation-of-vgg-16-b050e5a5920b" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9c37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总而言之:</p><ul class=""><li id="96e5" class="ls lt hh ig b ih ii il im ip lu it lv ix lw jb lx ly lz ma bi translated">VGG的所有构型都是块状结构。</li><li id="0ec5" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">每个VGG块由一系列卷积层组成，其后是最大池层。相同的内核大小(3 × 3)应用于所有卷积层。此外，作者使用填充大小1来保持每个卷积层之后的输出大小。还应用了大小为2 × 2、步长为2的最大池，以便在每个块之后将分辨率减半</li><li id="f207" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">每个VGG模型都有两个完全连接的隐藏层和一个完全连接的输出层。</li></ul><p id="4400" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们只关注VGG16的部署、它的架构以及它在Keras上的实现。其他配置的构造类似。</p><p id="4f61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">VGG16的结构如下图所示:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/1728fc6bfb07391ebce2b8df4cdf8b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNifzsJ7tD2kAfBXt3AzEg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Figure 1<strong class="bd jm">: The architecture of VGG16</strong>. Source: Researchgate.net</figcaption></figure><p id="9c65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">VGG16由13个卷积层、5个最大池层和3个全连接层组成。因此，具有可调参数的层数是16 (13个卷积层和3个全连接层)。这就是型号名称为VGG16的原因。第一块中的滤波器数量是64，然后在后面的块中这个数量加倍，直到它达到512。该模型由两个完全连接的隐藏层和一个输出层完成。两个完全连接的层具有相同的神经元数量，即4096。输出层由1000个神经元组成，对应于Imagenet数据集的类别数。在下一节中，我们将在Keras上实现这个架构。</p><h2 id="c79c" class="ki jl hh bd jm kj kk kl jq km kn ko ju ip kp kq jy it kr ks kc ix kt ku kg kv bi translated"><strong class="ak"> <em class="kw"> 2。在Keras</em>T3上实现VGG16</strong></h2><p id="23e5" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">首先，我们需要导入一些必要的库:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="48e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦所有必要的库都准备好了，就可以通过下面的函数来实现该模型:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="f1e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们来看看模型每一层的详细信息:</p><pre class="ld le lf lg fd mj mk ml mm aw mn bi"><span id="a22f" class="ki jl hh mk b fi mo mp l mq mr"><strong class="mk hi"><em class="ms">vgg16_model = VGG16()</em></strong></span><span id="069a" class="ki jl hh mk b fi mt mp l mq mr"><strong class="mk hi"><em class="ms">vgg16_model.summary()</em></strong></span><span id="9333" class="ki jl hh mk b fi mt mp l mq mr">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d (Conv2D)              (None, 224, 224, 64)      1792      <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     <br/>_________________________________________________________________<br/>max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    <br/>_________________________________________________________________<br/>max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    <br/>_________________________________________________________________<br/>conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         <br/>_________________________________________________________________<br/>conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   <br/>_________________________________________________________________<br/>conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   <br/>_________________________________________________________________<br/>conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   <br/>_________________________________________________________________<br/>max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         <br/>_________________________________________________________________<br/>conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   <br/>_________________________________________________________________<br/>conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   <br/>_________________________________________________________________<br/>conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   <br/>_________________________________________________________________<br/>max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 25088)             0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 4096)              102764544 <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 4096)              0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 4096)              16781312  <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 4096)              0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 1000)              4097000   <br/>=================================================================<br/>Total params: 138,357,544<br/>Trainable params: 138,357,544<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="d1b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着过滤器的数量随着模型深度增加，因此在后面的层中参数的数量显著增加。特别地，两个全连接隐藏层中的参数数量非常大，分别有102，764，544和16，781，312个参数。它占整个模型参数的86.4%。</p><p id="f8a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大量参数可能会降低模型性能。有时，它会导致过度拟合。一个自然的问题出现了:<strong class="ig hi"> <em class="ms">有没有可能用一些东西来代替完全连接的层来降低模型的复杂度？我们将在下一节讨论的NiN模型是这个问题的恰当答案。</em></strong></p><h1 id="4576" class="jk jl hh bd jm jn mu jp jq jr mv jt ju jv mw jx jy jz mx kb kc kd my kf kg kh bi translated">二。NiN模型</h1><h2 id="071e" class="ki jl hh bd jm kj kk kl jq km kn ko ju ip kp kq jy it kr ks kc ix kt ku kg kv bi translated"><strong class="ak"> <em class="kw"> 1。</em>简介</strong></h2><p id="fae9" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">网络中的网络(NiN)是由、、水城范等提出的一种深度卷积神经网络[2]。这个网络的结构不同于经典的CNN模型:</p><ul class=""><li id="96b9" class="ls lt hh ig b ih ii il im ip lu it lv ix lw jb lx ly lz ma bi translated">经典模型使用线性卷积层，这些层之后是一个激活函数来扫描输入，而NiN使用<strong class="ig hi">多层感知器卷积层</strong>，其中每一层都包括一个微网络。</li></ul><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mz"><img src="../Images/1b40b06562c82c95371cda0cbf8e6488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQZhggEHrL4yXIP8l-KZdg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Figure 2: Comparison of Linear convolutional layer and Multilayer perception convolutional layer (Mlpconv layer). <a class="ae jc" href="https://arxiv.org/abs/1312.4400" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="f94b" class="ls lt hh ig b ih ii il im ip lu it lv ix lw jb lx ly lz ma bi translated">经典模型在模型末端应用完全连接的层来分类对象，而NiN在将输出馈送到softmax层之前使用一个<a class="ae jc" href="https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">全局平均池层</strong> </a> <strong class="ig hi"> </strong>。与传统的全连接层相比，<strong class="ig hi"> </strong>全局平均池层具有一些优势。首先，通过加强特征图和类别之间的对应，它更适合于卷积结构。其次，在全局平均池层中没有要优化的参数，因此有助于避免过拟合现象。最后，使用全局平均池图层对输入的空间转换更具鲁棒性，因为它汇总了空间信息。</li></ul><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es na"><img src="../Images/1c068596fbc70a8bed790232f951bfee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWGsLkUnDaWz7KbIlRt9Hg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Figure 3: The overall structure of NiN: three MLP convolutional layers and one global average pooling layer. <a class="ae jc" href="https://arxiv.org/pdf/1312.4400.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="e4d1" class="ki jl hh bd jm kj kk kl jq km kn ko ju ip kp kq jy it kr ks kc ix kt ku kg kv bi translated"><strong class="ak">T23】2。在Keras上实现NiN</strong></h2><p id="dfbd" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">最初的NiN网络由四个NiN块组成。每个块包括三个卷积层:</p><ul class=""><li id="9472" class="ls lt hh ig b ih ii il im ip lu it lv ix lw jb lx ly lz ma bi translated">第一层使用的是形状属于{11 × 11，5 × 5，3 × 3}的滤波窗口。</li><li id="fc66" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">后两层是1 × 1卷积层。</li></ul><p id="5db0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个NiN块后面是一个最大池层，池大小为3 × 3，跨度为2。除了最后一个块后面是全局平均池层。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nb"><img src="../Images/658e3574f0534ec6c66f9761cc515082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSDO7sJqoqxLrg00LSuIpA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Figure 4: The architecture of the NiN model. <a class="ae jc" href="https://d2l.ai/_images/nin.svg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="bc99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个模型很容易通过下面的函数实现:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="9456" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们考虑模型每一层的详细信息(即输出大小和参数数量):</p><pre class="ld le lf lg fd mj mk ml mm aw mn bi"><span id="d1a3" class="ki jl hh mk b fi mo mp l mq mr"><strong class="mk hi"><em class="ms">NiN_model = NiN()<br/>NiN_model.summary()</em></strong></span><span id="76d5" class="ki jl hh mk b fi mt mp l mq mr">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d_13 (Conv2D)           (None, 54, 54, 96)        34944     <br/>_________________________________________________________________<br/>conv2d_14 (Conv2D)           (None, 54, 54, 96)        9312      <br/>_________________________________________________________________<br/>conv2d_15 (Conv2D)           (None, 54, 54, 96)        9312      <br/>_________________________________________________________________<br/>max_pooling2d_5 (MaxPooling2 (None, 26, 26, 96)        0         <br/>_________________________________________________________________<br/>conv2d_16 (Conv2D)           (None, 22, 22, 256)       614656    <br/>_________________________________________________________________<br/>conv2d_17 (Conv2D)           (None, 22, 22, 256)       65792     <br/>_________________________________________________________________<br/>conv2d_18 (Conv2D)           (None, 22, 22, 256)       65792     <br/>_________________________________________________________________<br/>max_pooling2d_6 (MaxPooling2 (None, 10, 10, 256)       0         <br/>_________________________________________________________________<br/>conv2d_19 (Conv2D)           (None, 8, 8, 384)         885120    <br/>_________________________________________________________________<br/>conv2d_20 (Conv2D)           (None, 8, 8, 384)         147840    <br/>_________________________________________________________________<br/>conv2d_21 (Conv2D)           (None, 8, 8, 384)         147840    <br/>_________________________________________________________________<br/>max_pooling2d_7 (MaxPooling2 (None, 3, 3, 384)         0         <br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 3, 3, 384)         0         <br/>_________________________________________________________________<br/>conv2d_22 (Conv2D)           (None, 1, 1, 10)          34570     <br/>_________________________________________________________________<br/>conv2d_23 (Conv2D)           (None, 1, 1, 10)          110       <br/>_________________________________________________________________<br/>conv2d_24 (Conv2D)           (None, 1, 1, 10)          110       <br/>_________________________________________________________________<br/>global_average_pooling2d (Gl (None, 10)                0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 1000)              11000     <br/>=================================================================<br/>Total params: 2,026,398<br/>Trainable params: 2,026,398<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="7306" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，全局平均池层没有参数。因此，使用这一层而不是完全连接的层有助于显著降低模型的复杂性。该模型的参数数量比VGG模型的参数数量少得多。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><p id="9c6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong>我们发现了VGG和NiN模型的架构。VGG模型的构造与之前的模型类似，<a class="ae jc" href="https://lekhuyen.medium.com/lenet-and-mnist-handwritten-digit-classification-354f5646c590" rel="noopener"> LeNet </a>和<a class="ae jc" href="https://lekhuyen.medium.com/alexnet-and-image-classification-8cd8511548b4" rel="noopener"> Alexnet </a>。它们都由卷积层、汇集层和全连接层组成。VGG的合理深度扩展使它优于以前的。然而，全连接层中的参数数量太大，尤其是在大规模图像处理的情况下。NiN通过用全局平均池层替换这些层来克服这个缺点。这一层有一些优点，它更自然地加强了特征映射和类别之间的对应关系。除此之外，这一层没有任何参数需要优化。因此，使用该层有助于在训练模型时避免过度拟合。NiN模型的出现也是我们将在ne xt帖子中讨论的后来现代CNN模型的构建的灵感。</p><p id="d183" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望这篇帖子对你有帮助！不要犹豫，关注我的<a class="ae jc" href="https://lekhuyen.medium.com/" rel="noopener">中博客</a>接收相关话题。</p><p id="6cd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢阅读！</p><p id="1d1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="ms"> Github代码:</em></strong><a class="ae jc" href="https://github.com/KhuyenLE-maths/An_overview_of_VGG_and_NiN_models/blob/main/An_overview_of_VGG16_and_NiN_models.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/khu yenle-maths/An _ overview _ of _ VGG _和_ NiN _ models/blob/main/An _ overview _ of _ vgg 16 _和_NiN_models.ipynb </a></p><p id="a107" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="ms">我的博客页面:</em></strong><a class="ae jc" href="https://lekhuyen.medium.com/" rel="noopener">https://lekhuyen.medium.com/</a></p><p id="8bb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">______________________________________________________</p><p id="5a26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献:</strong></p><p id="e804" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]西蒙扬，卡伦，和安德鲁齐塞曼。“用于大规模图像识别的非常深的卷积网络。”<em class="ms"> arXiv预印本arXiv:1409.1556 </em> (2014)。</p><p id="43ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]林，闵，，严水成.“网络中的网络。”<em class="ms"> arXiv预印本arXiv:1312.4400 </em> (2013)。</p></div></div>    
</body>
</html>