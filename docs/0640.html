<html>
<head>
<title>Reinforcement Learning on Tetris</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">俄罗斯方块的强化学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/reinforcement-learning-on-tetris-707f75716c37?source=collection_archive---------0-----------------------#2021-06-04">https://medium.com/mlearning-ai/reinforcement-learning-on-tetris-707f75716c37?source=collection_archive---------0-----------------------#2021-06-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4fa9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我决定重写这篇日志，并发表一篇关于这个兼职项目的简洁文章。代码可以在这里找到<a class="ae jc" href="https://github.com/zeroize318/tetris_ai" rel="noopener ugc nofollow" target="_blank"> tetris_ai </a>。以下是这本日记的大纲:<br/> 1。当前结果视频<br/> 2。强化学习的算法<br/> 3。应用详情<br/> 4。神经网络<br/> 5。结果和讨论<br/> 6。我尝试过的想法和变体</p><p id="99c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文章第二部分:<a class="ae jc" href="https://rex-l.medium.com/reinforcement-learning-on-tetris-2-f12f74f70788" rel="noopener">https://rex-l . medium . com/reinforcement-learning-on-Tetris-2-f12f 74 f 70788</a></p><h1 id="1993" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">当前结果视频</strong></h1><figure class="kb kc kd ke fd kf"><div class="bz dy l di"><div class="kg kh l"/></div><figcaption class="ki kj et er es kk kl bd b be z dx">AI playes Tetris that focuses on T-Spin</figcaption></figure><figure class="kb kc kd ke fd kf"><div class="bz dy l di"><div class="km kh l"/></div><figcaption class="ki kj et er es kk kl bd b be z dx">AI plays customized Tetris</figcaption></figure></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><h1 id="348c" class="jd je hh bd jf jg ku ji jj jk kv jm jn jo kw jq jr js kx ju jv jw ky jy jz ka bi translated">强化学习算法</h1><p id="e7c5" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><em class="le">本节大部分内容并非来自真实资源(提供出处的除外)。因此，如果你注意到一些错误或者你不同意，你非常欢迎指出并讨论它。</em></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/e787282ee448caac2a2cde843a52f7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLBXHgdBBD-gkomEOYfN6g.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">Source: <a class="ae jc" href="https://youtu.be/w_IIP-swuVo" rel="noopener ugc nofollow" target="_blank">https://youtu.be/w_IIP-swuVo</a></figcaption></figure><p id="91cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q-learning在俄罗斯方块上应用的主要问题是许多步骤将是琐碎的。游戏的动作{a}包括{ <em class="le">向左、向右、向下、向左转、向右转、保持、软降和硬降</em> }，但教AI学习如何移动棋子不是我们的优先任务。人类在玩俄罗斯方块的时候，考虑<strong class="ig hi">如何</strong>将棋子移动到某个位置是小事一桩；相反，我们考虑<strong class="ig hi">到</strong>到<strong class="ig hi">我们应该把</strong>移到哪里。因此，最好提供当前棋子可以掉落的所有可能位置，并让AI选择最佳位置。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lm"><img src="../Images/b9b84d5a09e2b336b0dfa9dc7c0adbec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*WZycFyCt9hx-E9CIb1H3pQ.gif"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">All possible states and rewards: [(r, s’)], where r’ is simply {score’-score}. If you are not familiar with T-spin, please check this short video <a class="ae jc" href="https://youtu.be/FI39WJqTLvA" rel="noopener ugc nofollow" target="_blank">https://youtu.be/FI39WJqTLvA</a></figcaption></figure><p id="b9a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这种调整，不再涉及动作{a}，奖励{r}与未来状态{s'}绑定。Q值由V值代替，V值基本上是给定可能状态池{[s']}的最大Q值。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es ln"><img src="../Images/00db1944590fae1c483120d5fb65e66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABUgREJ-j1LCKQEOYeI0sA.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">Φ is the the trainable variables of the Neural Network for V(s)</figcaption></figure><p id="f9b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是人工智能的巨大优势:首先，人工智能不需要学习如何移动棋子。否则，预计会看到人工智能卡在来回移动一块，左转，然后右转；第二，数据集大小减少到大约1/7，其中7是移动棋子的估计平均步数；最后，人工智能不需要学习如何得分，例如，做一个T型旋转——人工智能只需要学习如何为T型旋转设置一块板。V值可以解释为得分的可能性，由于γ的原因，预计得分会低于实际得分。所以，当AI被提供了[(r，s')]，它就把它处理成一列数[r+V(s ')]；高分移动几乎肯定具有最大值，因此策略会选择它。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><h1 id="2be8" class="jd je hh bd jf jg ku ji jj jk kv jm jn jo kw jq jr js kx ju jv jw ky jy jz ka bi translated">申请详情</h1><ul class=""><li id="5cf0" class="lo lp hh ig b ih kz il la ip lq it lr ix ls jb lt lu lv lw bi translated">外循环—样本收集{( <em class="le"> s，s’，r </em> )}: <br/>对于每一步，环境(游戏)都有当前状态<em class="le"> s </em>，并提供所有可能状态的列表及其对应的奖励[(<em class="le">s’，r </em> )]。神经网络对V值进行评估，使其成为一个数字列表[r+V(s')]，找到最高的<em class="le"> r+V(s') </em>值，并将其命名为最佳未来状态。AI将当前状态、最佳未来状态及其回报(<em class="le"> s，s’，r </em>)附加到样本中。然后，它生成一个介于0和1之间的随机数，如果它大于ε(比如说0.05)，那么人工智能将选择最佳未来状态进行开发；否则，它将探索，选择一个随机的未来状态。最后，环境的当前状态<em class="le"> s </em>被所选择的<em class="le">s’替换。<br/> </em>下一步…重复直到游戏结束。</li><li id="66e6" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">内循环—更新目标y: <br/>这个简单来说就是:<em class="le"> y_i=γ[r_i+V(s'_i)]。<br/> </em>我把<em class="le"> r_i </em>放在括号里面是因为我觉得它和未来状态绑定了。因此，这是未来的奖励。<br/>有一点要注意，当<em class="le"> s'_i </em>游戏结束时，<em class="le"> y_i=γ[r_i] </em>，一定不能包含<em class="le"> V(s'_i) </em>因为这个值<em class="le"> V(s'_i) </em>永远不会被求值；它的价值是无限的。<br/>另一个小细节:<em class="le"> s </em>包含当前棋子、保持棋子和4个下一个棋子。<em class="le">s’</em>包含未来当前片段、保持片段、3个下一个片段和未知的第4个下一个片段，因此输入给定为[0，0，0，0，0，0，0]。</li></ul><p id="756d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">拟合—训练神经网络:<br/>好吧，让机器学习包的库函数来处理。<br/>我使用tensorflow.keras，这里我唯一想指出的是<em class="le"> model(inputs，training=False) </em>比<em class="le"> model.predict(inputs，batch_size=batch_size) </em>快得多。虽然，我需要为<em class="le">模型(输入)</em>手动编写小批量的代码。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><h1 id="2ee2" class="jd je hh bd jf jg ku ji jj jk kv jm jn jo kw jq jr js kx ju jv jw ky jy jz ka bi translated"><strong class="ak">神经网络</strong></h1><p id="a2a8" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">输入:每个柱子的高度是为了给AI一种危险感。洞的位置是为了给AI一种棋盘不整齐的感觉。输入[0]是一个2D阵列，它将被馈送到一个卷积神经网络。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mc"><img src="../Images/4ab29e05f210cfb0262f19fac32f3bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gChlOBDHqu9odc6ZeP1Hvg.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">Inputs for the Neural Networks. Possible piece types are ‘S’, ‘Z’, ‘I’, ‘O’, ’T’, ’L’, ’J’; 7 of them in total</figcaption></figure><p id="ae1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">型号:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es md"><img src="../Images/491fd83e35808ef08f301ff18852bb90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swhH9NaZiiNT-XbXcvk6qQ.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">Neural Network Model</figcaption></figure><p id="4e34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一些参数:<br/>缓冲区大小:50000 <br/>内循环:5 <br/>历元:5 <br/>批量大小:512<br/>opi mizer = keras . opi mizer . Adam(0.001)<br/>loss = ' Huber _ loss '<br/>metrics = ' mean _ squared _ error '</p><h1 id="297d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结果和讨论</h1><p id="f828" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">人工智能学习生存的速度惊人地快。在这个视频<a class="ae jc" href="https://youtu.be/FTDZN4pPhwA" rel="noopener ugc nofollow" target="_blank"> AI玩定制的俄罗斯方块</a>中，只用了20个外圈就达到了这个玩法水平。我的桌面训练AI的每个循环花费了大约400秒，也就是说，如果定制游戏的难度合理，训练一个全新的AI几乎无限期地存活下来总共需要大约2.5小时。我发现人工智能玩这个定制游戏真的很好。</p><p id="b9f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，人工智能不擅长有效得分，即每步得分。它经常完成一行、两行或三行。我还没有找到训练它达到每2000件更高分数的方法——人工智能只是在某个地方停滞不前或变得更糟。我相信这是由于一个传统的俄罗斯方块有循环出现的状态，但是不同的政策会导致不同的奖励。</p><p id="1d21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">举个例子，给定γ=0.95，完成1行奖励10分而完成2行奖励30分，</p><p id="a46e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">策略一:<br/>当前状态<em class="le"> s </em> —降1枚—降1枚—降1枚完成1行—降1枚—降1枚完成1行—相同状态<em class="le">s</em><br/>v[s]= 10 *γ+γ⁵*(10+v[s】)<br/>v[s]= 72.11</p><p id="568f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">策略二:<br/>当前状态<em class="le"> s </em> —掉4块—掉1块完成2行—相同状态<em class="le">s</em>t14】v[s]= γ⁵(30+v[s)<br/>v[s]= 102.61</p><ul class=""><li id="16f3" class="lo lp hh ig b ih ii il im ip me it mf ix mg jb lt lu lv lw bi translated">有了好的γ值，从理论上讲，AI会追求每件作品更高的分数。但是对于AI来说，一旦它已经为当前的策略进行了训练，就很难找到更好的策略——它需要AI保持许多步骤(&gt; 3)才能完成线条并立即得分！而且即使它选择探索，它选择一个不可怕的举动的机会就更低了，而且这种幸运的巧合要发生几次才能遇到一次更好的政策，而这个样本很可能会淹没在糟糕政策的样本池中。(当我写这篇文章时，我意识到也许给ε-贪婪方法一个分布概率而不是均匀随机将会改善这一点。)</li><li id="0045" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">换句话说，现在的AI真的不太可能转投另一种策略。<a class="ae jc" href="https://youtu.be/BbruZtUte00" rel="noopener ugc nofollow" target="_blank">https://youtu.be/BbruZtUte00</a>看这个视频，AI开发了不同的策略，尽管他们的设置完全相同。一些人最终总是尝试做T形旋转，而另一些人一次也没做过。</li><li id="c3bd" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">示例策略还揭示了另一个问题，V值可能在某种程度上独立于状态，只要它们在几个碎片掉落之后出现。这意味着所有状态，如果没有给出下一个片段，<em class="le">可能</em>具有相同的V值！疯了！</li><li id="191d" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">结果对选定的γ高度敏感。我想象γ，奖励和惩罚构成边界条件。状态场中的每一个值都依赖于边界条件。它们至关重要。举例来说，如果γ太低，人工智能会更喜欢现在抓取点，而不是保存到将来更高的点。如果γ太高，V值可能会高得离谱，因为状态会重复出现。</li></ul><h1 id="e0ac" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">我尝试过的想法和变体</h1><ul class=""><li id="b21a" class="lo lp hh ig b ih kz il la ip lq it lr ix ls jb lt lu lv lw bi translated">【试过】N-return:结果不好。我不知道为什么。</li><li id="3092" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[尝试]对所有可能的状态进行采样，而不是只对最佳状态进行采样— <em class="le"> {(s，[s '，r])}。</em>然后，在外部循环和内部循环之间添加另一个循环，该循环选择与当前模型最佳的[s '，r]。这样做的好处是所有的样本都可以永久使用，因为它是独立于策略的。然而，在模型拟合期间有一个问题，当选择另一个[s '，r]时，V(s ')可能是无界的，因为游戏可能还没有经历过那条路线。</li><li id="0833" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">【尚未尝试】基于以上分析，当是勘探而不是开采时，我是否应该不添加样本(<em class="le"> s，s’，r </em>)。请注意，无论如何，最好的<em class="le">s’</em>将被记录，而不是实际选择的那个。</li><li id="7150" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[已尝试]重放缓冲:似乎不能提高收敛。</li><li id="c132" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[尚未尝试]集合模型:这有一些很大的潜力，因为，如前所述，早期的训练真的很快(~2.5小时)，但他们随后就被他们当前的政策卡住了。有些人可能制定了更有效的政策(T-spin ),但这是随机的。也许，参考几十个已经训练了3个小时的模型，然后用他们收集的样本，训练一些其他的新模型会很好。</li><li id="959c" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[已尝试]自定义图层:已尝试。这里说的太多了。目前我将不得不跳过它。目前，它看到一个小的改进，但仍然有同样的限制。</li><li id="bf92" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[尚未尝试]进行探索，使用基于[V(s')+r]的分布概率来选择移动。</li><li id="6cf8" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">【试过，有点】真过渡函数:s’的最后一段输入[1]可以是任意一种可能，[1，0，0，0，0，0，0]，[0，1，0，0，0，0，0]… [0，0，0，0，0，0，0，1]。v(s’)并取它们的平均值。这导致更多的计算时间，我没有发现明显的改善。然而，这对于用表格方法测试算法是绝对重要的。(我在一个3x3的游戏板上测试了该算法，并证明它与表格方法一致:&gt;)</li><li id="042f" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[尝试]操纵奖励功能:我希望看到人工智能做更多的4线完成和双T旋转，而不是1，2或3线完成，所以我操纵奖励功能，让它不同于增加的分数。这很好，但当我过度压缩1、2或3行完成的分数时，人工智能会以一种有趣而合理的方式感到困惑。人工智能会把木板堆到3/4高，然后为了生存而完成1或2行。一旦它能够在3/4高度生存足够长的时间，它永远不会发现保持干净的棋盘会更好，因为在大量的步骤之后，比如说200，γ ⁰⁰基本上是0，游戏结束的惩罚将不会应用于这样的状态。</li><li id="0b18" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">[试用]多处理:非常适合收集样本。该算法完全适合V-learning。因此，异步训练是不必要的，是吗？</li><li id="d5fc" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">【试过但失败】GPU:我觉得这个AI最好的办法是用CPU采集样本，用GPU训练模型。但是我没能在它们之间转换。在样本收集期间，输入的批量很小，大约是当前状态的40个可能的未来状态。因此，在一个内核中使用model(inputs)比model.predict(inputs)要快，我认为这是由于开销的原因。我可以让每个内核存储自己的模型，这样多处理的规模就接近完美。然而，GPU更擅长在V拟合期间训练它，在我的桌面上大约快3倍。我试图在“:/cpu”和“:/gpu”之间切换，但总是出现致命错误。无论如何，训练的效率不是这个人工智能的主要问题，我对目前的速度已经很满意了。</li></ul><h1 id="f079" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">问题</h1><ul class=""><li id="8f3b" class="lo lp hh ig b ih kz il la ip lq it lr ix ls jb lt lu lv lw bi translated">你知道如何改进这个人工智能吗？</li><li id="5573" class="lo lp hh ig b ih lx il ly ip lz it ma ix mb jb lt lu lv lw bi translated">你想看人工智能玩不同的定制俄罗斯方块游戏吗？你可以告诉我你想看的片段，我可以训练它演奏。在我的代码中，当一个片段太淘气时，比如“H”和“Donut ”,我会降低它们出现的机会。否则，游戏将无法进行。</li></ul></div></div>    
</body>
</html>