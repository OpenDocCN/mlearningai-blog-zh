<html>
<head>
<title>A Little About Perceptrons and Activation Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一点关于感知器和激活函数</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-little-about-perceptrons-and-activation-functions-aed19d672656?source=collection_archive---------4-----------------------#2021-06-05">https://medium.com/mlearning-ai/a-little-about-perceptrons-and-activation-functions-aed19d672656?source=collection_archive---------4-----------------------#2021-06-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7151" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常情况下，科学突破和工程壮举的灵感来源于自然界早已存在的东西，远在我们想到它们之前。从类似的鸟和飞机到象鼻启发的机器人手臂，我们不需要更多的证据就可以同意自然是我们今天所拥有的东西的主要灵感来源。</p><p id="e21d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计算机科学当然也不例外。尽管对人脑的研究已经有很长时间了，但在1943年，沃伦麦卡洛克和沃尔特皮茨推出了第一个使用电路的神经网络模型。神经网络的工作非常像人脑，其中它们的<em class="jc">感知器</em>类似于我们的<em class="jc">神经元，</em>因此得名。</p><p id="685f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当开始研究机器学习时，大多数人首先会了解过度拟合和欠拟合的概念，显然这两者都是我们希望在我们的模型中避免的。我们想要的是一个能很好地/恰当地适合我们的数据的模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/db41996b576e6334484b53f9a02174ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f5d_m1XTSHxH3tf8.jpg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Underfit, Good fit, and Overfit example. Image retrieved from julienharbulot.com</figcaption></figure><p id="183c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在回归问题中，我们希望能够在给定输入的情况下预测输出的值。神经网络能够用它们的感知器做到这一点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/5183fc121a26822bc0dea6e0490cdfd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eMcJxrc8Qmcm8Lhr.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">The simplest mathematical model of a neuron called the Perceptron. Image retrieved from researchgate.com uploaded by Zafeirios Fountas</figcaption></figure><p id="9628" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们在X-Y平面上有一组点，假设X代表每周平均睡眠时间，Y是相应学生的测试分数。如果我们找出了这个模式，我们就可以根据一个学生每周的平均睡眠时间来预测他的考试成绩。</p><p id="37d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们来谈谈感知器。它们类似于人脑内部的神经元，基本上是构建回归线的构件。假设我们有一个简单的神经网络，有一个隐藏层，也就是感知机所在的层，有三个感知机。一个感知器接受输入(可能不止一个)，每个输入乘以一个叫做<em class="jc">权重的因子</em>加在一起，再加上一个叫做<em class="jc">偏差</em>的数字。将加权和与偏差相加后，该值被插入到<em class="jc">激活函数中。这个激活函数的结果实际上就是感知器的输出。对于具有一个隐藏层的神经网络，然后将每个感知器的所有输出值加在一起，作为原始输入的预测值。</em></p><p id="6951" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们的睡眠分数示例中，神经网络接受学生每周平均睡眠时间的值作为输入，每个感知器将该值乘以每个感知器的相应权重，并通过将已经相乘的输入求和并添加偏差值来预测该学生的考试分数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/239788e485f31d86a1971f417ea21b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvn_x7iwnrEamwid4h-a4g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">To create a prediction as depicted with the blue line in the graph, a neural network with 3 perceptrons in a single hidden layer is constructed</figcaption></figure><p id="bfe4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计算出正确的权重和偏差是创建准确的回归模型所必需的，这由反向传播算法来完成。但是本文不会讨论该算法的工作原理和机制。相反，我们将更多地讨论激活函数的种类。</p><h1 id="c82b" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">激活功能</h1><p id="902f" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">大多数时候，我们的数据点在X-Y平面上形成的模式可能不是直线模式。如果是这种情况，一个简单的直线线性回归的形式</p><blockquote class="ky kz la"><p id="3803" class="ie if jc ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated">y = mx + b</p></blockquote><p id="9996" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不足以预测x的值。如果我们强迫自己在这样的情况下使用线性回归，我们会有一个不适合的模型。</p><p id="a9c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，我们需要一个工具来模拟我们的数据点，使我们能够弯曲回归线。我们使用激活函数来实现。简而言之，激活函数使我们的回归变得非线性。一些常用的激活功能如下。</p><h2 id="604b" class="le jw hh bd jx lf lg lh kb li lj lk kf ip ll lm kj it ln lo kn ix lp lq kr lr bi translated">Sigmoid激活函数</h2><p id="5ded" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">sigmoid函数的图形如下所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/52d434d14a2f4adf463fbcb2e065b312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YgSud-Xy30FA1nQ4.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Sigmoid function. Image retrieved from wikipedia.com</figcaption></figure><p id="4dc9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的sigmoid函数的“S”形图形的方程式如下。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/6d5c894daf78adea9297d93125101e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/0*YQtaDDR1Nq-HM_lJ.png"/></div></figure><p id="a760" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于sigmoid函数的范围仅从0到1，因此它通常用于预测概率。sigmoid函数也是可微的，这意味着可以计算导数。然而，当它接近正负无穷大时，导数越来越接近0。小梯度值不适合反向传播。此外，逻辑sigmoid函数会导致神经网络在训练时停滞。</p><p id="3872" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Softmax激活函数</strong>另一方面，是用于多类分类的更一般化的逻辑激活函数。这意味着softmax可用于解决涉及两个或更多类别的分类问题。</p><h2 id="ff9b" class="le jw hh bd jx lf lg lh kb li lj lk kf ip ll lm kj it ln lo kn ix lp lq kr lr bi translated">双曲正切激活函数</h2><p id="7f45" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">双曲正切函数与sigmoid函数非常相似。tanh激活函数也具有S形，但不同之处在于tanh的范围是从-1到1。与sigmoid激活函数并排比较的tanh激活函数的图形和方程如下。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/6536751f5ac0fd892c94e6ec981697a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xfAi23O-dm9-Zqkg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Sigmoid and Tanh compared side by side. Image retrieved from https://stats.stackexchange.com answer by ekoulier</figcaption></figure><p id="2681" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Tanh通常应用于两个类之间的分类问题。像sigmoid激活函数一样，双曲正切函数是可微分的，但结果是，当梯度值越来越接近两个无限末端时，梯度值也越来越接近0，这对于反向传播来说是不希望的。当人们必须在乙状结肠和Tanh之间做出选择时，tanh是有利的。双曲正切函数的公式如下。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/1d69d26dac16ead8c5a9d1786725ea17.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*oxF7YETLMa72XJWP.png"/></div></figure><h2 id="0ad9" class="le jw hh bd jx lf lg lh kb li lj lk kf ip ll lm kj it ln lo kn ix lp lq kr lr bi translated">ReLU和泄漏ReLU激活功能</h2><p id="c8f6" class="pw-post-body-paragraph ie if hh ig b ih kt ij ik il ku in io ip kv ir is it kw iv iw ix kx iz ja jb ha bi translated">ReLU代表整流线性单元，是神经网络中最常用的激活函数。ReLU激活函数的范围从0到无穷大，0表示小于等于0的值，即0表示负值。总的来说，ReLU激活功能比Sigmoid和Tanh更有利。但是，它可能仍然不能正确地映射负值，因为它会立即将负值转换为0。</p><p id="cc03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是漏ReLU的用武之地。泄漏的ReLU是作为一种尝试，以解决垂死的ReLU问题。Leaky ReLU不会将负值设置为零，而是遵循一个带有梯度的线性方程，通常为0.01。当梯度不为0.01时，称为随机化ReLU。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/5d616a0d0f62fdae86b927de54dc0650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aoERmpmfVgF9asia.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">ReLU vs Leaky ReLU graph. Image retrieved from towardsdatascience.com by Sagar Sharma</figcaption></figure></div><div class="ab cl lx ly go lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ha hb hc hd he"><h1 id="20b3" class="jv jw hh bd jx jy me ka kb kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks bi translated">参考</h1><div class="mj mk ez fb ml mm"><a href="https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">机器学习中的欠适应和过适应</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">让我们考虑我们正在设计一个机器学习模型。一个模型被认为是一个好的机器学习模型，如果…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="mv l"><div class="mw l mx my mz mv na jn mm"/></div></div></a></div><div class="mj mk ez fb ml mm"><a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" rel="noopener follow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">支持向量机——机器学习算法简介</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">从零开始的SVM模式</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="nb l mx my mz mv na jn mm"/></div></div></a></div><div class="mj mk ez fb ml mm"><a href="https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">提高神经网络的性能</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">神经网络是机器学习算法，提供许多用例的准确性状态。但是，很多…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">www.kdnuggets.com</p></div></div><div class="mv l"><div class="nc l mx my mz mv na jn mm"/></div></div></a></div><div class="mj mk ez fb ml mm"><a href="https://www.hebergementwebs.com/tensorflow-tutorial/tensorflow-single-layer-perceptron" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">张量流-单层感知器</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">Tensorflow教程要理解单层感知器，理解人工神经网络很重要…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">www.hebergementwebs.com</p></div></div><div class="mv l"><div class="nd l mx my mz mv na jn mm"/></div></div></a></div><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/@opam22/menilik-activation-functions-7710177a54c9"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Menilik激活函数</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">我的生活充满了希望。Sayyidina Ali bin Abi Thalib公司</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="ne l mx my mz mv na jn mm"/></div></div></a></div><div class="mj mk ez fb ml mm"><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener follow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">神经网络中的激活函数</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">Sigmoid，tanh，Softmax，ReLU，Leaky ReLU解释！！！</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="nf l mx my mz mv na jn mm"/></div></div></a></div><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ng nh l"/></div></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ng nh l"/></div></figure></div></div>    
</body>
</html>