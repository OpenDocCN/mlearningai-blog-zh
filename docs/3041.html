<html>
<head>
<title>ATTENTION Explained Clearly and in Depth ｜Natural Language Processing (NLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">清晰深入地解释注意力|自然语言处理(NLP)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/attention-to-natural-language-processing-nlp-explained-clearly-and-in-depth-b5c32217ec41?source=collection_archive---------0-----------------------#2022-07-12">https://medium.com/mlearning-ai/attention-to-natural-language-processing-nlp-explained-clearly-and-in-depth-b5c32217ec41?source=collection_archive---------0-----------------------#2022-07-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/a93322ba12cdf1f8338f14871f077ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*6CZ6zaYsTKOmAqchQL3dvQ.png"/></div></figure><p id="6007" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">“注意力”通常用于NLP库中，被认为是自然语言处理的突破，对于理解BERT和GPT-3等前沿技术至关重要。</p><p id="c2b6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这一页，我们想深入浅出地解释“注意”。</p><h2 id="75d6" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">目录</h2><ul class=""><li id="2556" class="ke kf hh in b io kg is kh iw ki ja kj je kk ji kl km kn ko bi translated">什么是注意力？</li><li id="5176" class="ke kf hh in b io kp is kq iw kr ja ks je kt ji kl km kn ko bi translated">为什么它被称为自然语言世界的突破</li><li id="a3f1" class="ke kf hh in b io kp is kq iw kr ja ks je kt ji kl km kn ko bi translated">对注意力如何与基于原始论文的数学公式一起工作的深刻解释</li></ul><h1 id="e617" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">1.1什么是注意力？</h1><p id="b34b" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">注意力是一种机制，它告诉AI句子中的哪些单词对于一项任务来说是必不可少的，以及在执行任务时应该注意(注意)哪些单词。注意力的发展克服了之前模型的致命缺陷，即对于较长的句子不准确。</p><p id="5d52" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">既然注意力非常接近人类的思维方式，直观上也容易理解，那就用一个具体的例子来理解吧。</p><p id="68f8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">例如，假设您有一个将英语翻译成西班牙语的任务，如下面的句子所示。</p><h2 id="baef" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">英语句子</h2><blockquote class="lo lp lq"><p id="3182" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated">艾米丽在苹果农场出生长大，所以她每天都吃苹果，当然，艾米丽爱吃苹果。</p></blockquote><h2 id="90db" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">翻译句子</h2><blockquote class="lo lp lq"><p id="6fa1" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated">埃米莉·娜西奥和她在曼萨纳斯的一个村庄，因为她的名字叫曼萨纳斯。</p></blockquote><p id="5683" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当然，答案是“恩坎坦”。你在心里是怎么翻译的？</p><h2 id="58fc" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">“艾米丽爱”是要翻译的部分，所以“恩坎坦”是要翻译的词。</h2><p id="5c16" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">你大概就是这样想出“恩坎坦”的。在这个翻译中，“艾米丽喜欢苹果。</p><p id="d29f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你觉得，“要翻译，就“艾米丽喜欢苹果”这部分就够了。”那你很明智。这是一个关键的注意点。</p><p id="6c20" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力是让AI理解人类无意识做什么的东西，“这部分对这个翻译很重要”。就像你刚才的感觉一样。</p><p id="7146" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们比较一下常规方法和注意力模型，如下图所示。</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lv"><img src="../Images/bd6d7829e7743ee17fd75ab1f8e23523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x9FU7OIdbhkVG1jqGzSzuw.png"/></div></div></figure><p id="1cc7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如图所示，传统方法按原样传递句子。如果句子很短，可以准确翻译，但如果句子很长，就像这种情况，准确性就会下降。该论文指出，常规方法的准确度在30个字符之后急剧下降。</p><p id="7549" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">另一方面，注意力突出了诸如“艾米丽”和“我喜欢”这样的基本词汇。这使得读者能够集中注意力在关键的单词上，因此即使是长句也能准确翻译。</p><h2 id="ae52" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">要点总结</h2><ul class=""><li id="18fb" class="ke kf hh in b io kg is kh iw ki ja kj je kk ji kl km kn ko bi translated">注意力是一种机制，它告诉人工智能哪些词是重要的，应该在任务中注意哪些词。</li><li id="2d13" class="ke kf hh in b io kp is kq iw kr ja ks je kt ji kl km kn ko bi translated">由于这种机制，即使在长句子中，准确性也不会受到影响。</li></ul><h1 id="de86" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">1.2为什么注意力被认为是自然语言世界的一个突破</h1><p id="232d" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">注意力之所以被认为是自然语言世界的一个突破，是因为它是一个重要的机制，导致了伯特、GPT-3和其他最先进的库。</p><p id="9eb2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2015年宣布关注。从那里开始，自然语言处理世界迅速发展起来。</p><p id="b88d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">《注意力》出版两年后，《建立在注意力上的变形金刚》出版了。Transformer的论文标题是“注意力是你所需要的”，并提出了一个只使用注意力的模型。</p><p id="4c8e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">正如你们中的许多人可能知道的，Transformer是创建像BERT和GPT-3这样强大的库的基础。</p><p id="a67d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力是近几年NLP快速发展的根源。因此，集中被认为是自然语言世界的一个突破。</p><p id="9d26" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">将自然语言世界的发展与交通工具从马车发展到汽车的过程相比较，就很容易理解了。</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lv"><img src="../Images/aa791e5985ea0dba4366383d48f3feb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCFOq8CrtU8_x5m887naVQ.png"/></div></div></figure><p id="841c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">前注意模式的缺点是不能处理长句，但这是通过引入一种新的注意机制来克服的。此外，它显示了与统计翻译相同水平的翻译准确性，统计翻译是当时占主导地位的翻译方法。</p><p id="2f49" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">基于关注，几年后开发了Transformer。</p><p id="bd15" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">各种包含变形金刚的强大模型被创造出来。这些强大的模型是我们通常使用的模型，如BERT、GPT-3和PaLm。</p><p id="26d5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第1章的要点总结</p><p id="c1b6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力是一种机制，它告诉人工智能“哪些单词对于这项任务是必不可少的。这种机制的准确性不再妥协，即使是长句子。</p><p id="36b6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力之所以被称为自然语言世界的突破，是因为注意力是近年来NLP快速发展的根源。</p><h1 id="86d1" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">2.对注意力如何与基于原始论文的数学公式一起工作的深刻解释</h1><p id="7b56" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">现在你已经理解了注意力的轮廓，让我解释一下注意力是如何工作的。</p><p id="bc50" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从这一点开始，我们将在原始论文的基础上解释对理解注意力很重要的公式。当然，这一章有点难懂，但即使没有数学背景的人也能看懂。(那些认为只做一个概述就够了的人到此为止都没问题。别忘了跟着拍。)</p><p id="8058" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你能彻底阅读和理解这一章，你应该能理解和解释数学公式中的注意。</p><p id="7d0a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你打算为了研究或其他目的阅读原始论文，在阅读原始报告之前阅读这一章将使你了解大致内容并顺利阅读原始文章。</p><p id="5e99" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">本章的流程将解释如下。</p><p id="49ce" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.1总体流程说明</p><p id="1e4f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.2针对注意力重要部分的解释</p><p id="c49e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你理解了这一章，你就可以用公式来理解和解释注意力。</p><h1 id="818e" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">2.1注意力的整体流动</h1><p id="b9de" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">首先，我们将解释注意力和注意力的整体流动。</p><p id="ef75" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">原始论文中引入的包含注意力的模型被称为“RNNsearch”该模型基于先前研究的简单编码器-解码器模型“RNN编码器-解码器”</p><p id="18cf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">换句话说，RNNsearch是对RNN编码器-解码器模型的改进，但增加了注意力。</p><p id="ad44" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力本身不是模型；注意力是一种机制(架构)，RNNsearch是一种融入注意力的模型。</p><blockquote class="lo lp lq"><p id="27fa" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated"><strong class="in hi">简要回顾:</strong>什么是编码器和解码器？</p><p id="24d5" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated"><strong class="in hi">编码器:</strong>转换输入的句子、单词等。变成人工智能可以处理的东西。</p><p id="ef7f" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated"><strong class="in hi">解码器</strong>:处理并输出转换后的值。</p></blockquote><p id="4b08" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从这里开始，我们将分别解释编码器和解码器的注意模型，并与以前的研究进行比较。(我们将使用与原文相同的翻译任务)</p><p id="eb8e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">之前的工作:RNN编码器-解码器</p><p id="73ef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">编码器用RNN把输入字(x)转换成a。</p><p id="faf3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">解码器输出RNN的字(p(yi))，具有输入语义向量、隐藏层(Si)和前一个字(yi-1)</p><h2 id="cb74" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">编码器</h2><p id="92f0" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">考虑到前一个和下一个上下文，用双向RNN将输入(x)转换成单词向量(hi)</p><p id="8546" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">原始论文的词向量(hi)称为注释。</p><h2 id="2676" class="jj jk hh bd jl jm jn jo jp jq jr js jt iw ju jv jw ja jx jy jz je ka kb kc kd bi translated">解码器</h2><p id="7a0e" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">使用注意机制将单词向量(hi)转换为上下文向量(ci)。解码器注意机制将单词向量(hi)转换成上下文向量(ci)并加权(α)哪些单词应该被注意(注意)。这就是关注。</p><p id="6ae4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">改编自原始论文</p><p id="e188" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用上下文向量(ci)、隐藏层(Si)和先前单词(yi-1)作为输入，RNN输出单词概率(p(yi))</p><p id="c3a4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">总之，先前的研究为所有单词创建了相同固定维度的语义向量，并将其直接用于RNN。</p><p id="9595" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">相比之下，注意力模型为每个单词创建一个上下文向量(ci ),考虑要关注哪些单词，并在RNN中使用它。</p><p id="962f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">通过将语义向量改为上下文向量(ci)，我们克服了“句子越长，准确率越低”的问题，这个问题在那之前一直是个问题。</p><p id="d28f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下一章将用数学公式解释这个上下文向量(ci)。</p><h1 id="de26" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">2.2注意重要公式:用数学公式理解上下文向量(CI)</h1><p id="30d2" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">正如上一节所解释的，我们将解释获得上下文向量(ci)的公式，这是注意力的关键。</p><p id="64a2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">首先，我们将组织每个系数的作用。</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es me"><img src="../Images/878ae5a9567ab1db7632ded9962995c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fm6sqXQy8nO1OzUna4I7PA.png"/></div></div></figure><p id="ddcf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ci:上下文向量</p><p id="22fd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它考虑了上下文并包含关于“哪些单词对任务重要”的信息。在翻译的情况下，它包含了“主题”和“喜欢”对于翻译“我喜欢”很重要的信息。"</p><p id="94b4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">αij:单词重要性</p><p id="2f2b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">权重表示单词对任务的重要程度，取值范围从0到1。详见下文。</p><p id="1b4f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">hj:词向量</p><p id="0131" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用编码器的双向RNN，不仅包含单词信息，还包含单词前后的信息的向量。</p><p id="e611" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">换句话说，找到上下文向量(ci)</p><p id="8f84" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">通过单词的重要性(αij)对单词向量(hj)进行加权，并且产生加权值(σ)的和。</p><p id="de7c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">就是这个意思。</p><p id="61bd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">补充:解释αij(单词重要性)的公式</p><p id="a118" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">作为补充，我们也简单解释一下αij。αij的程序如下。</p><p id="1a1f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是一个softmax公式，因此您可以看到eij是使用softmax计算的。</p><p id="95f9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">eij然后用于计算隐藏层(Si-1)和单词向量(hj ),以给出“单词j周围的输入和单词I的输出匹配程度的分数”。</p><p id="b40d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你想了解更多关于eij公式和其他细节，请阅读原文(链接附在底部)。</p><h1 id="8fa4" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">第二章概要</h1><ul class=""><li id="37aa" class="ke kf hh in b io kg is kh iw ki ja kj je kk ji kl km kn ko bi translated">我们改进了以前的模型(RNN编码器-解码器),并添加了注意机制。</li><li id="f9d8" class="ke kf hh in b io kp is kq iw kr ja ks je kt ji kl km kn ko bi translated">传统模型对每个单词使用相同的语义向量，而注意力对每个单词使用上下文向量(ci)。</li><li id="ba6e" class="ke kf hh in b io kp is kq iw kr ja ks je kt ji kl km kn ko bi translated">上下文向量(ci)是单词加权的重要性向量(α)。</li></ul><h1 id="0e0b" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">结论。</h1><p id="ef9a" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">注意力是一种机制，它告诉人工智能“哪些单词对于任务来说是必不可少的。由于这种机制，即使在长句子中，准确率也不会下降。</p><p id="da67" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意力之所以被称为自然语言世界的突破，是因为注意力是近年来NLP快速发展的根源。</p><p id="222e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">传统上，每个单词使用相同的语义向量，而注意力使用上下文向量(ci)。</p><p id="02cd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">上下文向量(ci)是每个单词重要性的α加权。</p><h1 id="7739" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">感谢您的阅读！请鼓掌并跟我来！我是用通俗易懂，图文并茂的方式写NLP的！</h1><h1 id="1c00" class="ku jk hh bd jl kv kw kx jp ky kz la jt lb lc ld jw le lf lg jz lh li lj kc lk bi translated">参考文献和参考资料</h1><p id="aa30" class="pw-post-body-paragraph il im hh in b io kg iq ir is kh iu iv iw ll iy iz ja lm jc jd je ln jg jh ji ha bi translated">原始文件</p><blockquote class="lo lp lq"><p id="456d" class="il im lr in b io ip iq ir is it iu iv ls ix iy iz lt jb jc jd lu jf jg jh ji ha bi translated">通过联合学习对齐和翻译的神经机器翻译</p></blockquote><p id="5a16" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">原创日语博客</p><div class="mf mg ez fb mh mi"><a href="http://nlpillustration.tech/?p=185" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi">【超図解】自然言語処理(NLP)のAttentionをわかりやすく、そして深く解説 - イラストでわかるNLP｜自然言語処理</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx">自然言語処理ライブラリの解説などで度々見かける「Attention」。Attentionは自然言語界のブレイクスルーと言われ、BERTやGPT-3といった最先端技術を理解する上で必須の仕組みです。このページでは「Attention」について…</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">nlpillustration.tech</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw ij mi"/></div></div></a></div><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="mx l mt mu mv mr mw ij mi"/></div></div></a></div></div></div>    
</body>
</html>