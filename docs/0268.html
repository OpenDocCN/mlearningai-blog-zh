<html>
<head>
<title>Implementing Neural Networks from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/implementing-neural-networks-from-scratch-23d2d9b25e1d?source=collection_archive---------4-----------------------#2021-03-13">https://medium.com/mlearning-ai/implementing-neural-networks-from-scratch-23d2d9b25e1d?source=collection_archive---------4-----------------------#2021-03-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="9ec6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">人工神经网络</h1><h2 id="e9ab" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">介绍</h2><p id="a16e" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">ANN是神经网络最基本的结构。基本的人工神经网络结构被称为感知器。感知器是一个简单的带有激活函数的线性回归。线性回归用于寻找输入和输出之间的线性关系。但是大多数真实数据本质上是非线性的，所以为了使回归通用，我们使用带有激活的感知器。激活功能增加了输出的非线性，使其对非线性输入更加灵活。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/8739150e65cebb78dbb6ed6dfd5be97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*Wb1zK3ASn6vL6ZLiTVSDEw.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Perceptron acts as a linear regressor with an activation function</figcaption></figure><h2 id="5110" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">激活功能</h2><p id="a351" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">在我们神经上的刺激越过特定阈值之前，神经元不会激活，也不会向我们的大脑发送任何信号。类似地，只有当层的输出超过阈值时，神经元才被激活。激活函数用于激活一个神经元或一组神经元的输出。不同的激活函数激活神经元的方式不同。要获得更多关于激活功能的信息，请查看这篇<a class="ae la" href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lb"><img src="../Images/9a5f6821b74d7a0fd52f3ee0a88aca5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*RiFf3mPXJVUYykpaR0-m5w.png"/></div></figure><p id="a8b7" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">抱歉，如果这个理论很无聊。现在让我们快速进入实施阶段！！！！</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lh"><img src="../Images/18aa6843bfb38b512d008050c74a341f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Fw9kdz_voFAZlO88LF8-3Q.png"/></div></figure><p id="ca49" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><strong class="jv hi">导入我们的库</strong></p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="cd9e" class="jc if hh lj b fi ln lo l lp lq">from sklearn.datasets import fetch_openml #Downloading the MNIST dataset<br/>from keras.utils.np_utils import to_categorical #One hot encoding of labels <br/>import numpy as np #For linear algebra<br/>from sklearn.model_selection import train_test_split #Spliting the dataset  <br/>import time #For keeping track of time</span></pre><p id="136a" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">首先，让我们快速地将数据标准化并分成测试和训练</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="727e" class="jc if hh lj b fi ln lo l lp lq">x, y = fetch_openml('mnist_784', version=1, return_X_y=True) # Fetching the flattened mnist data 28x28 to 768 <br/>x = (x/255).astype('float32')# Normalising the values<br/>y = to_categorical(y) # One hot encoding<br/># Train and validation split <br/>x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)</span></pre><p id="b006" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">我们将编写一个通用的代码来创建N个隐藏层(因为大多数实现被限制在一定数量的隐藏层)。这可能看起来有点复杂，但不要担心，我们可以很容易地实现它。为了让我们的代码更流畅，我们需要使用一个类。这个类的名字是Ann，它将执行这些操作</p><ul class=""><li id="518c" class="lr ls hh jv b jw lc ka ld jj lt jm lu jp lv kn lw lx ly lz bi translated">初始化</li><li id="4f70" class="lr ls hh jv b jw ma ka mb jj mc jm md jp me kn lw lx ly lz bi translated">正向传播</li><li id="c9b0" class="lr ls hh jv b jw ma ka mb jj mc jm md jp me kn lw lx ly lz bi translated">反向传播</li><li id="3ddf" class="lr ls hh jv b jw ma ka mb jj mc jm md jp me kn lw lx ly lz bi translated">更新权重</li><li id="f591" class="lr ls hh jv b jw ma ka mb jj mc jm md jp me kn lw lx ly lz bi translated">计算验证准确度</li></ul><p id="f941" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><strong class="jv hi">创建我们的ANN类</strong></p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="cd36" class="jc if hh lj b fi ln lo l lp lq">#Creating the Ann class<br/>class Ann():<br/>  '''<br/>  Input-&gt;<br/>  sizes: list/array<br/>  epoches: Int<br/>  learning rate: float<br/>  '''<br/>  # Constructor with default epochs and learning rate<br/>  def __init__(self,sizes,epochs=10,lr=0.001):<br/>    self.sizes = sizes<br/>    self.epochs = epochs<br/>    self.lr = lr</span><span id="3d85" class="jc if hh lj b fi mf lo l lp lq">    self.params = self.initialization()# initializing the parameters</span></pre><p id="50fe" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">Ann类将把<code class="du mg mh mi lj b">size</code>、<code class="du mg mh mi lj b">epoch</code>和<code class="du mg mh mi lj b">lr</code>作为3个变量。size变量将是一个包含隐藏层大小的列表或数组。<code class="du mg mh mi lj b">epoch</code>和<code class="du mg mh mi lj b">lr</code>分别代表历元数和学习率。</p><p id="d9ca" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">现在我们将初始化网络的权重。我们要把我们所有的重量保存在字典里。其中键是权重矩阵的名称，值是一个数组。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="4c22" class="jc if hh lj b fi ln lo l lp lq"># Method for initializing the weight matrices<br/>  def initialization(self):<br/>    params = {} # Dictionary for  storing the parameters <br/>    for i in range(len(self.sizes)-1):<br/>      wstr = "W"+str(i+1) # Giving names to each weight matrices as W1,W2,W3<br/>      params[wstr]=np.random.randn(self.sizes[i+1],self.sizes[i])*np.sqrt(1./self.sizes[i+1]) #initializing the weights<br/>    return params</span></pre><p id="7db9" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">在我们写正向传播函数之前，让我们定义我们的激活函数。我们将定义带导数和不带导数的sigmoid。带导数的Sigmoid将用于我们的反向传播函数。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="69a9" class="jc if hh lj b fi ln lo l lp lq">def sigmoid(self, x, derivative=False):<br/>    if derivative:<br/>        return (np.exp(-x))/((np.exp(-x)+1)**2)<br/>    return 1/(1 + np.exp(-x))</span><span id="3804" class="jc if hh lj b fi mf lo l lp lq">def softmax(self, x):<br/>    # Numerically stable with large exponentials<br/>    exps = np.exp(x - x.max())<br/>    return exps / np.sum(exps, axis=0)</span></pre><p id="d04a" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">我们正在调整softmax的稳定性。要了解更多关于softmax函数checkout的稳定性<a class="ae la" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank">这个</a></p><h2 id="3226" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">正向传播</h2><p id="c802" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">我们将创建一个名为<code class="du mg mh mi lj b">forward_pass()</code>的函数来执行FP。这是一个简单的函数，它使用所有的权重，并将它们与前一层给出的输入相乘。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="6b9a" class="jc if hh lj b fi ln lo l lp lq"># Forward propogation(FP)<br/>  def forward_pass(self,x_train):<br/>    '''<br/>    Function which will take the input and will perform FP <br/>    '''<br/>    params = self.params # Accessing the initial parameters <br/>    n_weights = len(params)-1 #number of loops for forward pass <br/>    params["A0"] = x_train.T #Input layer<br/>    for i in range(n_weights):<br/>      wstr="W"+str(i+1) # Weights string- W1,W2... so on<br/>      zstr="Z"+str(i+1) # output of each layer before activation- Z1,Z2,Z3..... and so on<br/>      astr="A"+str(i+1) # layers output after activation - A1,A2,A3<br/>      params[zstr] = np.dot(params[wstr],params["A"+str(i)]) #dot product of weights and layers<br/>      params[astr] = self.sigmoid(params[zstr]) # Applyng the activation function<br/>        # Last layer with softmax activation<br/>    params["Z"+str(i+2)] = np.dot(params["W"+str(i+2)],params["A"+str(i+1)]) <br/>    params["A"+str(i+2)]= self.softmax(params["Z"+str(i+2)])</span><span id="6b18" class="jc if hh lj b fi mf lo l lp lq">    return params["A"+str(i+2)]</span></pre><h2 id="66b5" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">反向传播</h2><p id="6b4b" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated"><code class="du mg mh mi lj b">backward_pass</code>将执行反向传播，并将发现权重的变化。我们正在分别寻找最后一层的导数，因为最后一层使用了softmax激活函数。其余层使用sigmoid激活函数。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="c8fc" class="jc if hh lj b fi ln lo l lp lq"># Backward propogation(BP)<br/>  def backward_pass(self,y_train,output):<br/>    ''' <br/>    Function for BP which will be used for updating the parameters. <br/>    It will update the last weight matrix first and so on.<br/>    '''<br/>    assert output.shape==y_train.shape, f"Output shape doesnt match: output shape{output.shape} and y_train{y_train.shape}"<br/>    output=output<br/>    params = self.params<br/>    change_w = {} #Dictionary for storing the change in weights<br/>    n = len(self.sizes)-1<br/>    # computing update for Wn i.e the last weight matrix<br/>    error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z'+str(n)])<br/>    change_w['W'+str(n)] = np.outer(error, params['A'+str(n-1)])<br/>    # Loop for updating the remaining weights<br/>    for i in range(n):<br/>      error = np.dot(params['W'+str(n-i)].T, error) * self.sigmoid(params['Z'+str(n-1-i)], derivative=True)<br/>      change_w['W'+str(n-1-i)] = np.outer(error, params['A'+str(n-2-    i)])<br/>    return change_w</span></pre><h2 id="d702" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">更新参数</h2><p id="2ee7" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">在这一步中，我们将使用BP提供的导数值来更新我们的权重。我们将使用随机梯度下降(SGD)方法更新权重。用于更新权重的表达式。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es mj"><img src="../Images/1a775f7c170dc9aadf186eebeee770dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/1*tC5fgR9vq58iMEEhpgJKmg.gif"/></div></figure><p id="ffe6" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><em class="mk"> w </em>是权重矩阵，α是学习率。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="4704" class="jc if hh lj b fi ln lo l lp lq">#updating the parameters<br/>  def parameter_update(self,change_w):<br/>    '''<br/>    function for updating parameters using stocastich gradient descent<br/>    takes the change_w dictionary created by BP and returns the updated parameters<br/>    '''<br/>    for key, value in change_w.items():# accessing the change in weights <br/>        for w_arr in self.params[key]:<br/>            w_arr -= self.l_rate * value #updating the weights using SGD</span></pre><p id="46e7" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">计算训练准确度</p><p id="b93b" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">跟踪验证准确性的功能。我们还可以计算训练精度，但是验证精度是我们用来判断模型性能的。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="abf3" class="jc if hh lj b fi ln lo l lp lq"># Function for computing the accuracy on validation set<br/>  def compute_accuracy(x_val,y_val):<br/>    '''<br/>      Function for computing the validation accuracy<br/>    '''<br/>    predictions=[]<br/>    for x,y in zip(x_val,y_val):<br/>      predict = self.forard_pass(xval) #prediction using the ANN<br/>      predictions.append(y_val==predict) #comparing the prediction with the real value<br/>    return np.mean(predictions)</span></pre><h2 id="4c1c" class="jc if hh bd ig jd je jf ik jg jh ji io jj jk jl is jm jn jo iw jp jq jr ja js bi translated">训练模型</h2><p id="107f" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">最后也是最重要的一步</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="6c9c" class="jc if hh lj b fi ln lo l lp lq"># the traning function<br/>  def train(self,x_train,y_train,x_val,y_val):<br/>    '''<br/>    Function for training the network using all the methods created above<br/>    does the FP<br/>    does the BP<br/>    updates the parameters<br/>    checks for the accuracy<br/>    '''<br/>    start_time = time.time() #for computing the amount of time taken to do the whole task<br/>    for iterations in range(self.epochs): #Runnig loop for number of epochs<br/>      for x,y in zip(x_train,y_train): <br/>        out = self.forward_pass(x_train) # FP<br/>        change_w = self.backward_pass(y_train,out) #BP<br/>        self.parameter_update(change_w) #updation of weights</span><span id="377c" class="jc if hh lj b fi mf lo l lp lq">    accuracy = self.compute_accuracy # Computing the accuracy<br/>    print(f"Epochs: {iteration+1} , validation Accuracy: {accuracy} , duration: {time.time() - start.time()}")</span></pre><p id="9b89" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><code class="du mg mh mi lj b">train</code>功能将使用<code class="du mg mh mi lj b">forward_pass</code>、<code class="du mg mh mi lj b">backward_pass</code>和<code class="du mg mh mi lj b">parameter_update</code>功能训练模型。FP将给出模型的输出。基于实际产量和预测产量之间的差异，BP将计算权重的变化。使用BP提供的导数值来更新权重。</p><p id="648b" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">就是这样。现在把所有东西放在一起然后跑。</p><pre class="kp kq kr ks fd li lj lk ll aw lm bi"><span id="bd7b" class="jc if hh lj b fi ln lo l lp lq"># Creating a class instance<br/>NN=Ann(sizes=[784, 8, 8, 10],epochs=1)<br/># Training the model<br/>NN.train(xtrain,ytrain,xval,yval)</span></pre><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ml"><img src="../Images/740978ab521feee63fac3261f343db11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*de_4kMxID9FegnZe.jpg"/></div></figure><h1 id="d5a4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">可能的问题和修复</h1><p id="aa18" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated">从头开始创建人工神经网络的目的是了解整体结构是如何工作的。尤其是FP和BP的直觉。并且真正体会到现有的软件包是多么有用。所以，这不是实现神经网络的最佳方式。根据隐藏层的数量和长度，可能会有一些内存问题。最好的方法是开始使用较少的隐藏层。</p><p id="fb1e" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated">有很多功能在同时工作。对于调试，尝试单独运行这些函数，然后将它们放回到主类中。别指望这整件事会像魔法一样管用。如果你发现任何问题，请联系我，也想提出一些改进意见，随时准备改变。</p><h1 id="2cbb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">感谢阅读！！！❤</h1><h1 id="df2e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="2dad" class="pw-post-body-paragraph jt ju hh jv b jw jx jy jz ka kb kc kd jj ke kf kg jm kh ki kj jp kk kl km kn ha bi translated"><a class="ae la" href="https://mlfromscratch.com/neural-network-tutorial/#/" rel="noopener ugc nofollow" target="_blank">https://mlfromscratch.com/neural-network-tutorial/#/</a></p><p id="22c7" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><a class="ae la" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank">https://Eli . the green place . net/2016/the-soft max-function-and-derivative/</a></p><p id="44d9" class="pw-post-body-paragraph jt ju hh jv b jw lc jy jz ka ld kc kd jj le kf kg jm lf ki kj jp lg kl km kn ha bi translated"><a class="ae la" href="https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights" rel="noopener ugc nofollow" target="_blank">https://brilliant . org/wiki/back propagation/#:~:text = back propagation % 2C % 20 short % 20 for % 20% 22 backward % 20 propagation，to % 20% 20 eural % 20 network % 20 weights</a>。</p></div></div>    
</body>
</html>