# ä½¿ç”¨ Transformers trainer å’Œ pipeline è¿›è¡Œ NLP çš„è¿ç§»å­¦ä¹ 

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d?source=collection_archive---------2----------------------->

æ·±åº¦å­¦ä¹ å¦‚ä»Šæœ‰å¾ˆå¤šæœ‰è¶£çš„åº”ç”¨ï¼Œ[æ™ºèƒ½å†°ç®±](https://www.businessinsider.com/samsung-lg-unveil-artificial-intelligence-equipped-smart-fridges-2020-1)ï¼Œ[æ™ºèƒ½ç”µæ¢¯](https://digital.hbs.edu/platform-digit/submission/thyssenkrup-steelmaker-transforms-elevators-using-ai/)ï¼Œ[æ·±é…¿](https://towardsdatascience.com/deep-brew-transforming-starbucks-into-an-ai-data-driven-company-8eb2e370af7b)ï¼Œ[è‡ªåŠ¨é©¾é©¶](https://en.wikipedia.org/wiki/Self-driving_car)ï¼Œä»…ä¸¾å‡ ä¾‹ã€‚

[](https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/) [## 7 æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨-æœºå™¨å­¦ä¹ æŒæ¡

### è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ­£ä»ç»Ÿè®¡æ–¹æ³•è½¬å‘ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚æœ‰â€¦

machinelearningmastery.com](https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/) 

ç„¶è€Œï¼Œé€šå¸¸ä½ éœ€è¦å¤§é‡æ•°æ®æ¥è®­ç»ƒ [SOTA](https://pub.towardsai.net/state-of-the-art-models-in-every-machine-learning-field-2021-c7cf074da8b2) (æœ€å…ˆè¿›çš„)æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¹Ÿè®¸ä½ éœ€è¦è®¡ç®—èƒ½åŠ›ï¼Œä½†æ˜¯åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®ã€‚å¦‚æœæ²¡æœ‰é‚£ä¹ˆå¤šæ•°æ®ï¼Œä½ èƒ½åšä»€ä¹ˆï¼Ÿè¿›å…¥[è¿ç§»å­¦ä¹ ](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)çš„ä¸–ç•Œã€‚

åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œè¿ç§»å­¦ä¹ æ˜¯é‡ç”¨é€šè¿‡è§£å†³å…¶ä»–é—®é¢˜è·å¾—çš„â€œçŸ¥è¯†â€(è¿™äº›çŸ¥è¯†æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œå¹¶è¾¾åˆ° SOTA æ°´å¹³)ï¼Œä»¥æœ‰æ•ˆåœ°è§£å†³ä¸€ä¸ªä¸åŒä½†ç›¸å…³çš„é—®é¢˜ï¼Œè€Œä¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ã€‚

[](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) [## æ·±åº¦å­¦ä¹ è¿ç§»å­¦ä¹ çš„æ¸©å’Œä»‹ç»â€”â€”æœºå™¨å­¦ä¹ æŒæ¡

### è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸ºä¸€é¡¹ä»»åŠ¡å¼€å‘çš„æ¨¡å‹è¢«é‡æ–°ç”¨ä½œä¸€é¡¹ä»»åŠ¡çš„èµ·ç‚¹

machinelearningmastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) [](https://www.seldon.io/transfer-learning/) [## æœºå™¨å­¦ä¹ çš„è¿ç§»å­¦ä¹ 

### æœºå™¨å­¦ä¹ çš„è¿ç§»å­¦ä¹ æ˜¯æŒ‡é¢„è®­ç»ƒæ¨¡å‹çš„å…ƒç´ åœ¨æ–°çš„æœºå™¨å­¦ä¹ ä¸­è¢«é‡ç”¨â€¦

www.seldon.io](https://www.seldon.io/transfer-learning/) [](https://research.aimultiple.com/transfer-learning/) [## 2022 å¹´çš„è¿ç§»å­¦ä¹ :å®ƒæ˜¯ä»€ä¹ˆ&å¦‚ä½•å·¥ä½œ

### è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚è®­ç»ƒç®—æ³•å¯èƒ½ä¸ä¼šåƒâ€¦

research.aimultiple.com](https://research.aimultiple.com/transfer-learning/) 

åœ¨ä¸€ç³»åˆ—æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³ä¸ºå¼€å‘äººå‘˜æä¾›åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ä¸­ä½¿ç”¨è¿ç§»å­¦ä¹ çš„å¿«é€Ÿå…¥é—¨ã€‚

å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œ [transformers æ¶æ„](https://towardsdatascience.com/transformers-89034557de14)æ˜¯è§£å†³ä¸åŒé—®é¢˜çš„é¦–é€‰æ¨¡å‹ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ã€æ–‡æœ¬ç”Ÿæˆã€é—®é¢˜å›ç­”ç­‰ã€‚å¤šäºäº†[æ‹¥æŠ±è„¸](https://huggingface.co)å’Œå®ƒçš„ç”Ÿæ€ç³»ç»Ÿï¼Œ[ç”¨å˜å½¢é‡‘åˆš](https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5)è½¬ç§»å­¦ä¹ å·²ç»å˜å¾—éå¸¸å®¹æ˜“å¼€å§‹äº†ã€‚

è®©æˆ‘ä»¬æ¥å…³æ³¨ä¸€ä¸‹å˜å½¢é‡‘åˆšçš„è¿ç§»å­¦ä¹ ï¼Œä¸»è¦æ˜¯å¦‚ä½•ä»å˜å½¢é‡‘åˆšåº“ä¸­å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å˜å½¢é‡‘åˆšåº“æä¾›äº†[è®­ç»ƒå™¨](https://huggingface.co/docs/transformers/training)å’Œ[æµæ°´çº¿](https://huggingface.co/docs/transformers/quicktour)ï¼Œè®©è®­ç»ƒå’Œé¢„æµ‹å˜å¾—çœŸæ­£å®¹æ˜“ã€‚

# æ–‡æœ¬åˆ†ç±»

## åŠ è½½æ•°æ®é›†

```
from datasets import load_datasetraw_datasets = load_dataset("imdb")
```

## åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®

ç›®çš„æ˜¯å°†æ–‡æœ¬æ ‡è®°ä¸ºæ¨¡å‹ç¨åå¯è¯»çš„æ ¼å¼ã€‚é‡è¦çš„æ˜¯è¦åŠ è½½ä¸è®­ç»ƒæ¨¡å‹æ—¶ç›¸åŒçš„åˆ†è¯å™¨ï¼Œä»¥ç¡®ä¿æ­£ç¡®åœ°å¯¹å•è¯è¿›è¡Œåˆ†è¯ã€‚

```
from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

ç†è§£æ•°æ®çš„å½¢çŠ¶å¯¹æˆ‘ä»¬æ¥è¯´ä¹Ÿå¾ˆé‡è¦ã€‚å¦‚æœä½ çœ‹çœ‹ tokenized_datasets å†…éƒ¨ï¼Œå®ƒæ˜¯

```
DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 25000
    })
```

æ–‡æœ¬å’Œæ ‡ç­¾åœ¨åŸå§‹åŸå§‹æ•°æ®é›†ä¸­ã€‚è¦åšåˆ†ç±»ï¼Œä½ éœ€è¦â€œæ ‡ç­¾â€æ¥å‘Šè¯‰åœ°é¢çœŸç›¸ã€‚â€œinput _ idsâ€æ˜¯è®°å·ç´¢å¼•ï¼Œè®°å·çš„æ•°å­—è¡¨ç¤ºæ„æˆå°†è¢«æ¨¡å‹ç”¨ä½œè¾“å…¥çš„åºåˆ—ã€‚â€œattention_maskâ€æŒ‡ç¤ºä»¤ç‰Œç´¢å¼•çš„ä½ç½®ï¼Œä½¿å¾—æ¨¡å‹ä¸å…³æ³¨å®ƒä»¬(é€šå¸¸ 1 æŒ‡ç¤ºåº”è¯¥å…³æ³¨çš„å€¼)ã€‚â€œtoken_type_idsâ€è¢«ä¸€äº›æ¨¡å‹ç”¨æ¥è¯†åˆ«åºåˆ—çš„ç±»å‹(æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [Token Type IDs](https://huggingface.co/docs/transformers/glossary#token-type-ids) )ã€‚äº†è§£æ›´å¤šå…³äº[é¢„å¤„ç†](https://huggingface.co/docs/transformers/preprocessing)çš„ä¿¡æ¯ã€‚

## åˆ†å‰²è®­ç»ƒï¼Œæµ‹è¯•æ•°æ®é›†

raw_datasets å¯¹è±¡æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæœ‰ä¸‰ä¸ªé”®:â€œtrainâ€ã€â€œtestâ€å’Œâ€œunsupervisedâ€(å¯¹åº”äºæ•°æ®é›†çš„ä¸‰ä¸ªæ‹†åˆ†)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨â€œè®­ç»ƒâ€åˆ†å‰²è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨â€œæµ‹è¯•â€åˆ†å‰²è¿›è¡ŒéªŒè¯ã€‚

```
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
full_train_dataset = tokenized_datasets["train"]
full_eval_dataset = tokenized_datasets["test"]
```

## å®šä¹‰é¢„è®­ç»ƒæ¨¡å¼

è¯·æ³¨æ„ï¼Œæ¨¡å‹åç§°ä¸ tokenizer ç›¸åŒ

```
from transformers import AutoModelForSequenceClassificationmodel = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")
```

## è¿åŠ¨é‹

ç°åœ¨æœ€ç²¾å½©çš„éƒ¨åˆ†æ¥äº†ã€‚å®šä¹‰æ‚¨çš„è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†åˆ›å»ºè®­ç»ƒå™¨

```
from transformers import TrainingArgumentstraining_args = TrainingArguments("test_trainer")from transformers import Trainertrainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)
```

## å¼€å§‹è®­ç»ƒ

```
trainer.train()
```

è¿™ä¸ª API éå¸¸ç®€å•(å½“ç„¶ä½ å¯ä»¥åšå¾ˆå¤šå®šåˆ¶ï¼Œæ¯”å¦‚è®­ç»ƒæ—¶é—´ï¼Œå­¦ä¹ é€Ÿåº¦)ã€‚å¦åˆ™ï¼Œåœ¨æœ¬æœº Pytorch ä¸­å°†ä¼šå‡ºç°å¦‚ä¸‹å†…å®¹:

```
from transformers import AdamWoptimizer = AdamW(model.parameters(), lr=5e-5)from transformers import get_schedulernum_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)import torchdevice = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)from tqdm.auto import tqdmprogress_bar = tqdm(range(num_training_steps))model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

å°†è¿™äº›æ ·æ¿ä»£ç æ”¾å…¥ä¸€ä¸ªæ›´å¹²å‡€çš„ç•Œé¢æ˜¯å¾ˆå¥½çš„ã€‚

## ç®¡é“

ç°åœ¨æˆ‘ä»¬ç”¨å¾®è°ƒåçš„æ¨¡å‹æ¥åšé¢„æµ‹ã€‚é¦–å…ˆï¼Œä¿å­˜æ¨¡å‹

```
trainer.save_model()
```

å¹¶é¢„æµ‹ã€‚å˜å½¢é‡‘åˆšåº“æä¾›äº†åŸºäºä»»åŠ¡çš„ç®¡é“æ¥ç®€åŒ–æˆ‘ä»¬çš„ä»»åŠ¡ã€‚

```
from transformers import pipeline# load from previously saved model
pipe = pipeline("text-classification", model="test_trainer", tokenizer="bert-base-cased")
pipe("This restaurant is awesome")
```

è¾“å‡º

```
[{'label': 'LABEL_1', 'score': 0.9312610626220703}]
```

# é—®ç­”
åŠ è½½æ•°æ®é›†

```
from datasets import load_datasetsquad = load_dataset("squad")
```

## åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®

```
from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )# offset_mapping gives corresponding start and end character in the original text for each token in our input IDsoffset_mapping = inputs.pop("offset_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])# sequence_ids helps distinguish which parts of the offsets correspond to the question and which part correspond to the context
        sequence_ids = inputs.sequence_ids(i)# Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1# If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputstokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)
```

è®°å·åŒ– _ å°é˜Ÿçš„å½¢çŠ¶

```
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],
        num_rows: 87599
    })
```

åŸå§‹çš„å°é˜Ÿæ•°æ®é›†æœ‰ä¸Šä¸‹æ–‡ã€é—®é¢˜ã€ç­”æ¡ˆï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦åœ¨æ ‡è®°ä¸­æ‰¾åˆ°è¿™äº›ç­”æ¡ˆçš„ç¡®åˆ‡å¼€å§‹å’Œç»“æŸä½ç½®ã€‚è¿™å°±æ˜¯â€œèµ·å§‹ä½ç½®â€ã€â€œç»“æŸä½ç½®â€å‘Šè¯‰æˆ‘ä»¬çš„ã€‚

## è´Ÿè½½æ¨¡å‹

```
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainermodel = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")
```

## ç›¸åŒçš„åŸ¹è®­å¸ˆæ¨¡å¼

```
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01
)trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)
```

## ç«è½¦

```
trainer.train()
```

## ç®¡é“é¢„æµ‹

é¦–å…ˆä¿å­˜æ¨¡å‹

```
trainer.save_model()
```

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨â€œé—®ç­”â€ç®¡é“æ¥å®Œæˆé—®ç­”ä»»åŠ¡

```
context  = r"""
The Mars Orbiter Mission (MOM), also called Mangalyaan ("Mars-craft", from Mangala, "Mars" and yÄna, "craft, vehicle")
is a space probe orbiting Mars since 24 September 2014? It was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO). It is India's first interplanetary mission
and it made India the fourth country to achieve Mars orbit, after Roscosmos, NASA, and the European Space Company. and it made India the first country to achieve this in the first attempt.
The Mars Orbiter took off from the First Launch Pad at Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08 UTC on 5 November 2013.
The launch window was approximately 20 days long and started on 28 October 2013.
The MOM probe spent about 36 days in  Earth orbit, where it made a series of seven apogee-raising orbital maneuvers before trans-Mars injection
on 30 November 2013 (UTC).[23] After a 298-day long journey to the Mars Orbit, it was put into Mars orbit on 24 September 2014."""
nlp = pipeline("question-answering")
result = nlp(question="When did Mars Mission Launched?", context=context)
print(result['answer'])
```

# æ‘˜è¦

## åŠ è½½æ•°æ®é›†

```
from datasets import load_datasetraw_datasets = load_dataset("xsum")
```

## åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®

```
model_checkpoint = "t5-small"
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""max_input_length = 1024
max_target_length = 128def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)# Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True)model_inputs["labels"] = labels["input_ids"]
    return model_inputstokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
```

ç¬¦å·åŒ–æ•°æ®é›†çš„å½¢çŠ¶

```
DatasetDict({
    train: Dataset({
        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 204045
    })
```

â€œæ–‡æ¡£â€ã€â€œæ‘˜è¦â€ã€â€œidâ€æ¥è‡ªåŸå§‹æ•°æ®é›†ï¼Œè¡¨ç¤ºåŸå§‹æ–‡æ¡£ã€æ–‡æ¡£æ‘˜è¦ã€æ–‡æ¡£ç´¢å¼•å·ã€‚â€œæ ‡ç­¾â€æ˜¯æ ‡è®°åŒ–æ‘˜è¦çš„â€œè¾“å…¥æ ‡è¯†â€ã€‚

## è´Ÿè½½æ¨¡å‹

```
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainermodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

## åˆ›å»ºåŸ¹è®­å¸ˆ

```
batch_size = 16
model_name = model_checkpoint.split("/")[-1]
args = Seq2SeqTrainingArguments(
    f"{model_name}-finetuned-xsum",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True
)data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)
```

## ç«è½¦

```
trainer.train()
```

## ç®¡é“é¢„æµ‹

é¦–å…ˆä¿å­˜æ¨¡å‹

```
trainer.save_model()
```

## ä½¿ç”¨ç®¡é“é¢„æµ‹

```
# use bart in pytorch
from transformers import pipeline
summarizer = pipeline("summarization", model=f"{model_name}-finetuned-xsum",tokenizer=model_checkpoint)
summarizer("Designing a proper application architecture is the difference between a project that flourishes and one that gets pigeonholed into a set of inextensible features. When it comes to an open source project like Ostia, it becomes especially important, as the â€˜activation energyâ€™ for a developer to get up to speed on a project is a huge barrier.", min_length=5, max_length=20)
#[{'summary_text': 'Ostia is an open source project that flourishes and gets pigeonhole'}]
```

# å·¥ä½œæµæ¨¡å¼

å¯¹äºè¿™äº›ä»»åŠ¡ï¼Œæ¨¡å¼æ˜¯

1.  åŠ è½½åŸå§‹æ•°æ®é›†
2.  åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®
3.  å¯é€‰:åˆ†ç±»ä»»åŠ¡æ—¶æ‹†åˆ†æ•°æ®
4.  å®šä¹‰åŸ¹è®­å‚æ•°å’ŒåŸ¹è®­å¸ˆ
5.  å¼€å§‹è®­ç»ƒ/å¾®è°ƒ
6.  ä¿å­˜æ¨¡å‹ï¼Œç„¶ååŠ è½½å¸¦æœ‰ç®¡é“å’Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹
7.  ä½¿ç”¨ç®¡é“é¢„æµ‹

# é™„å½•

## è¿ç§»å­¦ä¹ 

[](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751) [## ä»é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ä¸­è½¬ç§»å­¦ä¹ 

### å¦‚ä½•å¿«é€Ÿç®€å•åœ°è§£å†³ä»»ä½•å›¾åƒåˆ†ç±»é—®é¢˜

towardsdatascience.com](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751) 

## ç®¡é“

[](https://www.analyticsvidhya.com/blog/2021/12/all-nlp-tasks-using-transformers-package/) [## ä½¿ç”¨ Transformers Pipeline-Analytics vid hya çš„æ‰€æœ‰ NLP ä»»åŠ¡

### 1.ç†è§£å˜å‹å™¨åŒ… 2ã€‚æ–‡æœ¬åˆ†ç±» 3ã€‚é—®é¢˜å›ç­” 4ã€‚æ–‡æœ¬æ‘˜è¦ 5ã€‚è¯­è¨€â€¦

www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2021/12/all-nlp-tasks-using-transformers-package/) [](https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/) [## æ‹¥æŠ±è„¸å˜å‹å™¨ç®¡é“åŠŸèƒ½|é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†

### è¿™ç¯‡æ–‡ç« ä½œä¸ºæ•°æ®ç§‘å­¦åšå®¢çš„ä¸€éƒ¨åˆ†å‘è¡¨ã€‚ç›®çš„è¿™ç¯‡åšå®¢æ–‡ç« å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨â€¦

www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/) [](/geekculture/pipeline-object-in-transformers-using-hugging-face-6577f57a4c18) [## å˜å½¢é‡‘åˆšä¸­çš„ç®¡é“å¯¹è±¡ğŸ¤—

### ä¸Šå‘¨æˆ‘å®Œæˆäº†ä¸€ä¸ªå…è´¹çš„æ‹¥æŠ±è„¸è¯¾ç¨‹ï¼Œåœ¨é‚£é‡Œæˆ‘å­¦åˆ°äº†å¾ˆå¤šæ–°ä¸œè¥¿ï¼Œæ¯”å¦‚å˜å½¢é‡‘åˆšâ€¦

medium.com](/geekculture/pipeline-object-in-transformers-using-hugging-face-6577f57a4c18) [](https://huggingface.co/docs/transformers/glossary) [## è¯æ±‡è¡¨

### è‡ªåŠ¨ç¼–ç æ¨¡å‹:è§ MLM è‡ªå›å½’æ¨¡å‹:è§ CLM CLM:å› æœè¯­è¨€å»ºæ¨¡ï¼Œä¸€ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œå…¶ä¸­â€¦

huggingface.co](https://huggingface.co/docs/transformers/glossary) [](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)