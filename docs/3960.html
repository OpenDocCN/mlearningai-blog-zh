<html>
<head>
<title>Distributed batch inference with Hugging Face on Amazon Sagemaker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Amazon Sagemaker上带拥抱脸的分布式批量推理</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/distributed-batch-inference-with-hugging-face-on-amazon-sagemaker-51980e460ec1?source=collection_archive---------3-----------------------#2022-11-15">https://medium.com/mlearning-ai/distributed-batch-inference-with-hugging-face-on-amazon-sagemaker-51980e460ec1?source=collection_archive---------3-----------------------#2022-11-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="588b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用SageMaker处理作业，通过Hugging Face的Transformer模型轻松地对大型数据集进行推理</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/a3d9e4ad016ba857d8f32f70e2ec7515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XLcyF1qxUgLigTur"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@burntime?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alex Kulikov</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ba18" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这篇博客将向您完整地介绍如何对生产中的大量数据运行分布式批量推理。我们将使用亚马逊Sagemaker，一种完全托管的机器学习服务。借助Amazon SageMaker，数据科学家和开发人员可以快速构建和训练机器学习模型，然后将其部署到生产就绪的托管环境中。</p><p id="e681" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">假设我们有数Pb的数据，我们想使用一个预训练的模型进行批量推理。Sagemaker处理作业是最简单的方法之一，因为它专注于抽象出所需的基础设施。它还要求对我们现有的代码进行最小的更改。在本教程中，我们将使用来自Huggin Face的预训练模型来计算一对句子之间的语义相似度。</p><p id="8b3d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">这篇博文将涵盖以下主题:</strong></p><ul class=""><li id="46df" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">修改SageMaker处理作业的代码</li><li id="bab3" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">用代码和依赖项构建docker容器，并将其推送到Amazon弹性容器注册(Amazon ECR)存储库</li><li id="e526" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">使用自定义docker图像启动处理作业</li></ul></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="e996" class="le lf hh bd lg lh li lj lk ll lm ln lo in lp io lq iq lr ir ls it lt iu lu lv bi translated">修改代码</h1><p id="4968" class="pw-post-body-paragraph jn jo hh jp b jq lw ii js jt lx il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">创建处理作业需要指定一个亚马逊简单存储服务(亚马逊S3) URI来下载数据，并在docker容器中指定一个路径来下载数据。处理容器中的路径必须以<code class="du mb mc md me b">/opt/ml/processing/</code>开头。稍后将对此进行更多讨论。</p><blockquote class="mf mg mh"><p id="4b28" class="jn jo mi jp b jq jr ii js jt ju il jv mj jx jy jz mk kb kc kd ml kf kg kh ki ha bi translated">注意:<code class="du mb mc md me b">/opt/ml</code>及其所有子目录由SageMaker保留。在构建处理Docker映像时，不要将容器所需的任何数据放在这些目录中。</p></blockquote><p id="bc3c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们需要修改我们的脚本从这个路径读取数据<code class="du mb mc md me b">/opt/ml/processing/</code></p><p id="aa6b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在本例中，我们使用来自上游数据管道的数据，该管道将数据分成多个拼花文件，并将它们存储在S3存储桶中。如果所选实例上有多个GPU，我们将使用每个GPU并行推断每个文件。这里我们使用了一个来自拥抱脸的预训练句子转换器(更多信息<a class="ae jm" href="https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2" rel="noopener ugc nofollow" target="_blank">这里</a>)，但是我们可以通过简单地修改下面的脚本来使用任何模型。</p><p id="01d6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">示例脚本从输入目录中读取文件，从句子嵌入中添加一个具有余弦相似性的新列，并将其保存在输出目录中。如下所示，可以从处理作业中覆盖这些参数。</p><pre class="ix iy iz ja fd mm me mn bn mo mp bi"><span id="0711" class="mq lf hh me b be mr ms l mt mu">import os<br/>import argparse<br/>import logging<br/>import numpy as np<br/>import pandas as pd<br/>from numpy.linalg import norm<br/>import torch.multiprocessing as mp<br/>from sentence_transformers import SentenceTransformer<br/><br/># set up logger<br/>logging.basicConfig(format="%(asctime)s:%(levelname)s:%(name)s:%(message)s")<br/>logger = logging.getLogger("HuggingFace")<br/>logger.propagate = True<br/>logger.setLevel(logging.INFO)<br/><br/><br/>def get_model_output(cfg, model, df, device_idx):<br/><br/>    # sanity check<br/>    text1 = df["text1"].values<br/>    text2 = df["text2"].values<br/><br/>    # Create the trainer for inference.<br/>    logger.info("Predicting model")<br/>    # Compute embedding for both lists<br/>    embeddings1 = model.encode(<br/>        text1,<br/>        batch_size=cfg.batch_size,<br/>        convert_to_numpy=True,<br/>        device=device_idx,<br/>    )<br/>    embeddings2 = model.encode(<br/>        text2,<br/>        batch_size=cfg.batch_size,<br/>        convert_to_numpy=True,<br/>        device=device_idx,<br/>    )<br/>    df["semanticScore"] = np.hstack(<br/>        [<br/>            (emb1 @ emb2.T) / (norm(emb1) * norm(emb2))<br/>            for emb1, emb2 in zip(embeddings1, embeddings2)<br/>        ]<br/>    )<br/>    return df<br/><br/><br/>def do_infer(cfg, device_idx):<br/><br/>    # Load the model.<br/>    logger.info("Loading model")<br/>    model = SentenceTransformer(cfg.model, device=device_idx)<br/><br/>    # Init file path that this process needs to process.<br/>    if os.path.isdir(cfg.input_dir):<br/>        filepath_list = [<br/>            filepath<br/>            for filepath in os.listdir(cfg.input_dir)<br/>            if filepath.endswith(".parquet")<br/>        ]<br/>        filepath_list = sorted(filepath_list)<br/>        filepath_list = [<br/>            filepath_list[idx]<br/>            for idx in range(len(filepath_list))<br/>            if idx % cfg.num_gpu == device_idx<br/>        ]<br/>    else:<br/>        filepath_list = (<br/>            [cfg.input_dir] if device_idx == 0 else []<br/>        )  # only use the first GPU.<br/><br/>    # Get the input and output filepath.<br/>    input_list = [os.path.join(cfg.input_dir, filepath) for filepath in filepath_list]<br/>    output_list = [os.path.join(cfg.output_dir, filepath) for filepath in filepath_list]<br/>    logger.info(f"Device {device_idx} input: {input_list}")<br/>    logger.info(f"Device {device_idx} output: {output_list}")<br/><br/>    # Start processing.<br/>    for in_filename, out_filename in zip(input_list, output_list):<br/>        # Read the dataset.<br/>        logger.info("Reading file {}".format(in_filename))<br/>        prod_df = pd.read_parquet(in_filename)<br/><br/>        # Run the inference.<br/>        output_df = get_model_output(cfg, model, prod_df, device_idx)<br/><br/>        # Save the results<br/>        logger.info(f"Writing the result to {out_filename}")<br/>        output_df.to_parquet(out_filename)<br/>        logger.info(f"Done writing the result to {out_filename}")<br/><br/><br/>def do_multiprocessing_infer(cfg):<br/><br/>    processes_list = [<br/>        mp.Process(target=do_infer, args=(cfg, device_idx))<br/>        for device_idx in range(cfg.num_gpu)<br/>    ]<br/><br/>    for process in processes_list:<br/>        process.start()<br/><br/>    for process in processes_list:<br/>        process.join()<br/><br/><br/>if __name__ == "__main__":<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument("--num_gpu", type=int, default=4)<br/>    parser.add_argument("--batch_size", type=int, default=128)<br/>    parser.add_argument(<br/>        "--input_dir", type=str, default="/opt/ml/processing/data/input"<br/>    )<br/>    parser.add_argument(<br/>        "--output_dir", type=str, default="/opt/ml/processing/data/output"<br/>    )<br/>    parser.add_argument(<br/>        "--model",<br/>        type=str,<br/>        default="sentence-transformers/distiluse-base-multilingual-cased-v2",<br/>    )<br/>    args = parser.parse_args()<br/>    do_multiprocessing_infer(args)</span></pre><h1 id="eb38" class="le lf hh bd lg lh mv lj lk ll mw ln lo in mx io lq iq my ir ls it mz iu lu lv bi translated">构建docker映像并将其推送到Amazon ECR</h1><p id="40d3" class="pw-post-body-paragraph jn jo hh jp b jq lw ii js jt lx il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">一旦我们完成了代码，我们需要构建一个docker容器。下面是一个DOCKER文件示例。在这个例子中，我们所有的代码都在src目录中</p><pre class="ix iy iz ja fd mm me mn bn mo mp bi"><span id="e522" class="mq lf hh me b be mr ms l mt mu"><br/>FROM python:3.8<br/><br/>RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \<br/>         wget \<br/>         python3 \<br/>    &amp;&amp; rm -rf /var/lib/apt/lists/*<br/><br/>RUN wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \<br/>        rm -rf /root/.cache<br/><br/>COPY requirements.txt /opt/program/<br/>COPY src/ /opt/program/src/<br/>WORKDIR /opt/program<br/><br/>RUN pip install -r requirements.txt<br/><br/><br/># Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard<br/># output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE<br/># keeps Python from writing the .pyc files which are unnecessary in this case. We also update<br/># PATH so that the train and serve programs are found when the container is invoked.<br/>ENV PYTHONUNBUFFERED=TRUE<br/>ENV PYTHONDONTWRITEBYTECODE=TRUE</span></pre><p id="2e6a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一旦我们有了DOCKER文件，我们需要构建它来创建一个图像，并在将它推送到Amazon ECR之前标记该图像。ECR是一个类似Docker Hub的容器注册中心，我们可以在这里托管容器映像。</p><p id="1c78" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">示例bash脚本将处理所有与AWS相关的认证，创建一个名为<strong class="jp hi"> <em class="mi"> sm-semantic-similarity、</em> </strong>的存储库，并对其进行标记，最后将其推送到Amazon ECR存储库。我们将得到一个独特的链接，如"&lt;<strong class="jp hi">ACCOUNT-ID&gt;. dkr . ECR . us-east-1 . Amazon AWS . com/sm-lexical-similarity:v1</strong>"</p><pre class="ix iy iz ja fd mm me mn bn mo mp bi"><span id="5ce9" class="mq lf hh me b be mr ms l mt mu"># Name of algo -&gt; ECR<br/>algorithm_name=sm-semantic-similarity<br/><br/>account=$(aws sts get-caller-identity --query Account --output text)<br/><br/># Region, defaults to us-east-1<br/>region=$(aws configure get region)<br/>region=${region:-us-east-1}<br/><br/>fullname="${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:v1"<br/><br/># If the repository doesn't exist in ECR, create it.<br/>aws ecr describe-repositories --repository-names "${algorithm_name}" --region ${region}&gt; /dev/null 2&gt;&amp;1<br/><br/>if [ $? -ne 0 ]<br/>then<br/>    aws ecr create-repository --repository-name "${algorithm_name}" --region ${region}&gt; /dev/null<br/>fi<br/><br/># Get the login command from ECR and execute it directly<br/>aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}<br/><br/># Build the docker image locally with the image name and then push it to ECR<br/># with the full name.<br/><br/>docker build -t ${algorithm_name} .<br/>docker tag ${algorithm_name} ${fullname}<br/>docker push ${fullname}</span></pre><h1 id="3d9d" class="le lf hh bd lg lh mv lj lk ll mw ln lo in mx io lq iq my ir ls it mz iu lu lv bi translated">使用自定义容器运行SageMaker处理作业</h1><p id="6794" class="pw-post-body-paragraph jn jo hh jp b jq lw ii js jt lx il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">Amazon SageMaker从S3复制数据，然后提取容器。群集资源在作业期间调配，并在作业完成时清理。处理作业的输出存储在参数中指定的S3存储桶中。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es na"><img src="../Images/13906fd9a72770b1f79e092c128dc113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHwBSYtk4p4WzSgTsEtsfQ.png"/></div></div></figure><p id="ba91" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><code class="du mb mc md me b">Processor</code>类需要以下参数:</p><ul class=""><li id="0103" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated"><strong class="jp hi">角色</strong>:AWS IAM角色名或ARN</li><li id="8bfb" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated"><strong class="jp hi">image _ uri:</strong>Docker图像的唯一链接</li><li id="567e" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated"><strong class="jp hi"> instance_count: </strong>运行处理作业的实例数。</li><li id="b723" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated"><strong class="jp hi"> instance_type: </strong>用于处理的EC2实例的类型</li><li id="36e8" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated"><strong class="jp hi">入口点:</strong>构成入口点命令的字符串列表。这里我们可以根据instance_type传递<code class="du mb mc md me b">num_gpus</code>参数。例如<code class="du mb mc md me b">ml.p3.16xlarge</code>有8个GPU</li><li id="172f" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated"><strong class="jp hi"> volume_size_in_gb </strong>:以gb为单位的大小，用于在处理作业期间存储数据</li></ul><p id="e2ae" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当运行处理作业时，我们需要提供作为<code class="du mb mc md me b"><a class="ae jm" href="https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi">ProcessingInput</strong></a></code>对象的输入文件列表和作为输出的<code class="du mb mc md me b"><a class="ae jm" href="https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi">ProcessingOutput</strong></a></code>对象列表。请注意参数<code class="du mb mc md me b"><strong class="jp hi">s3_data_distribution_type</strong></code>，它可以是“<strong class="jp hi">full replicated</strong>或“<strong class="jp hi"> ShardedByS3Key </strong>”，其中full replicated将使给定数据集的副本在每个实例中可用，ShardedByS3Key将把[<strong class="jp hi"># of data files</strong>]/[<strong class="jp hi"># of instances</strong>]条数据复制到每个实例中。<br/>更多信息请参考此处的<a class="ae jm" href="https://sagemaker.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank">和</a>。</p><pre class="ix iy iz ja fd mm me mn bn mo mp bi"><span id="6c8f" class="mq lf hh me b be mr ms l mt mu">import boto3<br/>import sagemaker<br/>from sagemaker import get_execution_role<br/>from sagemaker.processing import Processor<br/>from sagemaker.processing import ProcessingInput, ProcessingOutput<br/><br/>region = boto3.session.Session().region_name<br/>role = get_execution_role()<br/><br/>ecr_image = '&lt;ACCOUNT-ID&gt;.dkr.ecr.us-east-1.amazonaws.com/sm-lexical-similarity:v1'<br/><br/>huggingface_processor = Processor(role=role,<br/>                                  entrypoint=['python', '-u', 'src/infer.py', '--num_gpus=8'],<br/>                                  image_uri=ecr_image,<br/>                                  instance_type='ml.p3.16xlarge',<br/>                                  instance_count=50,<br/>                                  volume_size_in_gb=600,<br/>                                  base_job_name = 'preprocess-semantic'<br/>                                 )<br/><br/>huggingface_processor.run(<br/>    inputs=[<br/>        ProcessingInput(<br/>            source='&lt;s3_uri or local path&gt;',<br/>            s3_data_distribution_type='ShardedByS3Key',<br/>            destination='/opt/ml/processing/data/input')<br/>    ],<br/>    <br/>    outputs=[<br/>        ProcessingOutput(<br/>          source='/opt/ml/processing/data/output/',<br/>          destination='&lt;s3_uri&gt;,<br/>          s3_upload_mode='Continuous'<br/>        )<br/>    ]<br/>                     <br/>)</span></pre><h1 id="7135" class="le lf hh bd lg lh mv lj lk ll mw ln lo in mx io lq iq my ir ls it mz iu lu lv bi translated">结论</h1><p id="cd4f" class="pw-post-body-paragraph jn jo hh jp b jq lw ii js jt lx il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">在这篇博客中，我们介绍了一个端到端的分布式处理作业，它使用了来自Hugging Face的预训练的transformer模型。我们利用Amazon SageMaker抽象出资源供应。我们学习了如何打包我们的代码，创建docker容器，并将其上传到Amazon ECR。这里是官方<a class="ae jm" href="https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#learn-more" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">亚马逊SageMaker处理类文档</strong> </a>了解更多。</p><p id="19b6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">希望你喜欢这个教程，觉得有用。如果你有任何想法、评论或问题，请在下面留下评论或在<a class="ae jm" href="https://www.linkedin.com/in/ratulghosh1/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。快乐阅读🙂</p><div class="nb nc ez fb nd ne"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jg ne"/></div></div></a></div></div></div>    
</body>
</html>