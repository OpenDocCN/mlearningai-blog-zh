# 通往出口的路:GPT 模式中的巴纳姆效应或语言理解

> 原文：<https://medium.com/mlearning-ai/this-way-to-the-egress-barnum-effect-or-language-understanding-in-gpt-type-models-597c27094f35?source=collection_archive---------4----------------------->

语言建模已经产生了像 [GPT-2](https://openai.com/blog/better-language-models/) 、 [GPT-3](https://arxiv.org/abs/2005.14165) 、[地鼠](https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf)和其他系统，这些系统使用巨大的书面人类语言和深度学习神经网络来学习语言的本质，可以说。大规模语言建模使用一种类似于自回归的方法，根据序列的早期部分来预测序列中未来的标记(例如单词)。这些系统在数十亿个文本标记上被训练，并且它们使用数百万、数十亿或甚至数万亿个参数来编码包含在它们被训练的文本中的标记共现模式。例如，Gopher 在具有 2，048 个标记的上下文窗口的 3000 亿个标记上被训练，因此它根据 2，048 个附近的文本标记来学习每个文本标记的概率。

给定一些文本的“提示”,它们会产生与提示有意义相关的后续文本。例如，Gopher 接受了 152 项语言相关任务的测试，其中大部分都达到了很高的准确率。

包括 GPT-3 在内的这些模型被媒体描述为人工智能领域的重大突破。甚至哲学家 David Chalmers 也说过“GPT-3 是有史以来最有趣和最重要的人工智能系统之一。……更引人注目的是，GPT 3 号显示出了一般智力的迹象。”

虽然我同意大型语言模型是一个重大的成就，但我不认为它们提供了任何类似一般智能的东西。围绕这些模型的炒作被过分夸大了。

与这些模型受到的热情赞扬形成对比的是，它们也不是没有批评者。它们并不代表概念上的突破，相反，它们只做了一件事——根据其他文本标记提供的上下文对文本标记的概率进行建模。在另一篇文章中，我将阐述这种模型在多大程度上可以理解它们产生的令牌串。这里我想考虑模型输出对读者理解的贡献。为了客观地看待这个问题，我们需要考虑巴纳姆效应。简而言之，在这种情况下，无论文本是否存在，人们都会在文本中找到意义。

巴纳姆效应(Paul Meehl 在 1956 年命名)主要是在人格测试的背景下研究的，但它延伸得更远。同样的效果是某些声称的精神现象的核心，例如“[冷读术](https://en.wikipedia.org/wiki/Cold_reading)”在一本冷读本中，一个自称的灵媒提到了模糊的“事实”，而他们的受害者认为这些“事实”是重要的。对这一现象的最初研究是由 Ross Stagner (1947)完成的，他给人事经理做了一个性格测试。测试结束后，他给了他们所有与他们在测试中选择的答案无关的反馈。反馈包括适用于任何人的概括，但经理们普遍认为它非常准确地描述了他们。第二年，Bertram Forer 对大学生进行了更系统的研究。所有学生都收到了相同的反馈，其中包括[陈述](https://en.wikipedia.org/wiki/Barnum_effect)，如:

1.你非常需要别人喜欢和欣赏你。

2.你倾向于对自己吹毛求疵。

3.你有大量未使用的能力，你没有把它们变成你的优势。

4.虽然你有一些性格上的弱点，但你通常能够弥补它们。

平均而言，学生们认为反馈准确地描述了他们个人，给了 4.3 分(满分 5 分)，尽管他们都收到了完全相同的反馈。

学生、人事经理和心理受害者认为这些模糊的陈述与他们自己有关。他们提供了解决歧义的意义，而解决方案倾向于确认他们已经认为它应该意味着什么。类似地，大型语言模型输出的观众应用意义，当机器像灵媒一样简单地产生高概率猜测时，此外，这些猜测通常被挑选出来听起来更有意义。

为了让你们对生成的文本有所了解，这里有一些我在[https://huggingface.co/EleutherAI/gpt-neo-1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B?text=he+struck+out+for+chicago)用这个模型生成的文本，它有 13 亿个参数。我提供的提示以粗体突出显示。

*   **他先走了一步，然后出发去芝加哥。印第安人队在比赛的第一局被怀特一垒安打得分，导致印第安人队进入第二局，但印第安人队以 4 比 5 输掉了比赛。**
*   **他先走了一步，然后出发去芝加哥。在芝加哥，他被发现藏在一辆汽车的后备箱里，随后被控私藏枪支，并因涉嫌与这起谋杀案有关而试图从警方手中逃脱，从而妨碍司法公正**
*   他先走了一步，然后出发去芝加哥。“这是我一生中第一次也是唯一一次为赢得一场比赛而如此兴奋，”他说。“这绝对是偶然发生的事情。我真的很想赢
*   他先走了一步，然后出发去芝加哥。他大约 9 点到达芝加哥；00 开始。他回到房间后不久，雨就停了。他到达了

当然，并不是每个人都对这些模型赞不绝口。例如， [OpenAI](https://openai.com/blog/better-language-models/) 认识到这些模型有局限性。在讨论他们自己的 GPT-2 时，他们写道:

> 总的来说，我们发现需要几次尝试才能得到一个好的样本，尝试的次数取决于模型对环境的熟悉程度。当提示数据中高度呈现的主题(英国退出欧盟、麦莉·赛勒斯、指环王等等)时，它似乎能够在大约 50%的时间内生成合理的样本。

较大的模型(例如，GPT-3 对 GPT2)在选择高概率单词方面可能比这些示例可能暗示的更好，但是它们没有比较小的模型具有的关于单词模式的任何不同信息。他们只是拥有更多的信息(更多的例子，更多的上下文，更多的参数)。与 Chalmer 的观点相反，这些模型没有显示出任何普遍智力的证据。它们被设计成根据上下文从词汇表中选择单词。共现模式最多只是意义的一部分，但不是全部。他们作品中的许多表面意义是由读者贡献的，而不是机器。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)