<html>
<head>
<title>Boosting , Bagging, Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">助推，装袋，随机森林</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/boosting-bagging-random-forest-126fef1d1e74?source=collection_archive---------0-----------------------#2021-06-02">https://medium.com/mlearning-ai/boosting-bagging-random-forest-126fef1d1e74?source=collection_archive---------0-----------------------#2021-06-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c46f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可能存在无法计算数据集标准偏差的情况，在这种情况下<strong class="ig hi"> Bootstrap </strong>变得有用，可以改进决策树等统计方法。</p><p id="746e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有时，决策树也可能具有很高的方差，也就是说，如果我们将训练数据集分成两半，并对它们运行决策树算法，两个输出可能会给出完全不同的结果。另一方面，当在不同的数据集上重复应用时，线性回归往往给出较低的方差。在这样的<strong class="ig hi">场景下，自举聚合或打包</strong>是一种有用且有效的技术。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/dafd4cb7dc4f78580268d2822c79c6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*UJoUX8YUB-tMSc4MMkxkyw.png"/></div></figure><p id="dd9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为一般概念，我们知道对一组观察值进行平均会减少方差。然而，在实际场景中，这可能是不可实现的，因为我们可能无法访问多个训练集。在这种情况下，我们通过从相同的训练集中获取重复的样本来引导，从而产生B个不同的引导训练集。然后，我们在中的bth自举训练集上训练我们的方法，最后对所有预测值进行平均。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jk"><img src="../Images/94ccc536dde5f56f49e13c034767d46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*zLBL-r0rJOiTtaP2U5Y6dg.png"/></div></figure><p id="94ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Bagging可以改进预测，在决策树中特别有用。实际上，我们使用一个大的B，这样误差就稳定下来了。还有一种非常直接的方法来估计袋装模型的测试误差，甚至进行交叉验证。这被称为自付费用(OOB)。这是因为装袋的关键是重复拟合观测值的自举子集，装袋的树通常使用大约2/3的观测值，剩余的1/3不用于拟合装袋的树。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jl"><img src="../Images/4315f564df4c206d904f177f7ac0f422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*NDSf_HkEkjqyOvh2dPxC-w.png"/></div></figure><p id="ab9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树最大的优势之一就是它的图形化表示。然而，当我们用袋子装大量的树时，这就被扭曲了。当我们提高预测准确性时，我们放弃了可解释性。虽然袋装树的集合比单一的树更难解释，但是我们可以使用RSS(回归树)或Gini指数(分类树)很好地总结每个预测值的重要性。对于回归树，我们计算RSS由于在集合上的分割和在所有B树上的平均而减少的总量。大值表示是一个重要的预测值。而在分类树的情况下，我们得到基尼指数并在所有B树上平均它。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jm"><img src="../Images/5b78bab3b5789d27449cc7c9537a36da.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*i0GdfhVquiMt_r-pqhmBNA.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">Each time we split a random sample of predictors is chosen typically where chosen</figcaption></figure><p id="8225" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们假设我们应用了bagging，并在数据集中获得了一个非常强的预测器和一些中等强度的预测器。然后在收集袋装树时，大多数树会在顶部使用这种强预测器。因此，我们将有一个高度相关的袋装树。为了克服这种相关性，我们使用<strong class="ig hi">随机森林。</strong>我们在训练样本上建立了许多决策树。但是每次我们分开的时候，一个随机的预测样本会被选择</p><p id="a94f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这也主要是因为平均许多高度相关的量不会导致方差的大幅减少，因为平均许多不相关的量并因此装袋不会显著减少方差。如果我们能够管理每次分割，以处理数据集的子集，我们就有可能克服这一挑战。这就是典型的随机森林所做的。我们对这些树进行去相关处理，从而使平均值的可变性更小，更加可靠。bagging和随机森林的主要区别在于预测子集大小m的选择<strong class="ig hi">当m = p时是bagging，当m=√p时是随机森林。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jr"><img src="../Images/c4128160e2d8caac5fbdd049b5b46825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*LQuZo5QPI0r5yzbDDQM9Eg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">A plot to find the no of trees ; how error decreases with no. of tree</figcaption></figure><p id="704c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机森林因此可以被认为是决策树的集合。它构建并组合多个决策树，以提高准确性。之所以命名为random，是因为预测器是随机选择的；之所以命名为forest，是因为使用了多个决策树来进行预测/决策。随机森林有助于克服过度拟合，并通过其特性使模型健壮。</p><p id="7dd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为随机森林算法的简单方法</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es js"><img src="../Images/342236a6208929a72ddc77fcb099c3d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*a5TGLT5ex4QHn54ePsbMgQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">A simple R code approach</figcaption></figure><ol class=""><li id="00ea" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">从训练集中随机抽取m个观察值。</li><li id="3075" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">根据引导示例制作决策树。</li><li id="9f96" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">在每个节点随机选择<strong class="ig hi"> <em class="kh"> f </em> </strong>特性。</li><li id="27dd" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">根据目标函数(例如信息增益)基于产生最佳分裂的特征来分裂节点，</li><li id="16c0" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">重复上述步骤k次(k是我们想要使用子集创建的树的数量)</li><li id="486d" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">聚集从每个树中为新的数据点导出的预测值，以形成新的数据点，从而通过多数投票来分配类别标签。</li></ol><p id="2941" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机森林的一些<strong class="ig hi">优缺点</strong></p><ol class=""><li id="8d0b" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">易于计算</li><li id="f3ea" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">可以有效地处理数据</li><li id="68e7" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">缺失值、异常值，不要妨碍输出</li></ol><p id="f5a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而:证明过度拟合并且不能预测超出训练数据集范围的值。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ki"><img src="../Images/4b4f93eafd96d69050796fa9235b0229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pj_LnmpOScDhlwHpoBMHyw.png"/></div></div></figure><p id="8010" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Boosting </strong>是另一种改进决策树预测的方法。虽然一般方法与bagging相同，但在bagging中，每棵树都建立在bootstrap数据集上，而在boosting中，树是按顺序生长的，这意味着每棵树都是基于前一棵树生长的。</p><p id="ec1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">升压有三个调谐参数:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kn"><img src="../Images/18be360b16050654e5d53345fcab7734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*KQPRDtLpsl_HzXyf-Um7IQ.png"/></div></figure><ol class=""><li id="5c3b" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">树的数目b .我们使用交叉验证来选择b。</li><li id="bda5" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">收缩参数(λ):一个小正数，控制模型学习的速率。</li><li id="d597" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">每棵树的分裂数d。</li></ol><p id="8a81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">— — — — — — — — — — — — @ — — — — — — — — — — — — — — — -</p></div></div>    
</body>
</html>