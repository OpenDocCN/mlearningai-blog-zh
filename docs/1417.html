<html>
<head>
<title>Big Data Regression — Complete Pipeline — Part 1/2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据回归—完整的管道—第1/2部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/big-data-regression-complete-pipeline-part-1-2-b54c9aa27e80?source=collection_archive---------3-----------------------#2021-12-12">https://medium.com/mlearning-ai/big-data-regression-complete-pipeline-part-1-2-b54c9aa27e80?source=collection_archive---------3-----------------------#2021-12-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/188e54a69d2943052f459671005b40e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTwZq7ceSHClNuAconBNig.jpeg"/></div></div></figure><h1 id="de52" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="3175" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">这篇由两部分组成的文章全面介绍了一个真实的回归大数据问题，包括代码和详细的分析。第1部分(本文)主要关注EDA和浅层学习建模，而第2部分主要关注深度学习建模。</p><h1 id="76d5" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated"><strong class="ak">议程</strong></h1><ol class=""><li id="e6b5" class="kl km hh jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">问题和数据集描述</li><li id="606a" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">探索性数据分析</li><li id="4a94" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">用浅层技术建模</li></ol></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="ba7d" class="ip iq hh bd ir is lg iu iv iw lh iy iz ja li jc jd je lj jg jh ji lk jk jl jm bi translated">1.问题和数据集描述</h1><p id="dca6" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">为了尊重客户隐私，将原始问题一般化。500个不同的传感器以1Hz的速率(每秒)对给定场景进行采样，并引用2个数字标签(想象它们是产量、质量等。).所要求的结果是基于感觉数据预测这两个标签。</p><p id="e0c7" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated"><strong class="jp hi">数据集描述</strong>:</p><ul class=""><li id="c529" class="kl km hh jp b jq ll ju lm jy lq kc lr kg ls kk lt kr ks kt bi translated"><em class="lu"> var0 </em> ➔时间戳</li><li id="5d0d" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated"><em class="lu"> var1 </em> … <em class="lu"> var501 </em> ➔感官样本(用缺失值估算)</li><li id="66f2" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated"><em class="lu">结果502 </em>，<em class="lu">结果502 </em> ➔标签(待定)</li><li id="7bb4" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">大约300万行，从2021年9月1日到2021年10月4日(大约1个月)</li></ul><p id="c16e" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">数据存储在一个巨大的(~7GB) CSV文件中，并应用<a class="ae lv" href="https://docs.dask.org" rel="noopener ugc nofollow" target="_blank"> Dask </a>以高效的方式加载数据:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="22ec" class="mf iq hh mb b fi mg mh l mi mj">import dask.dataframe as dd</span><span id="0e97" class="mf iq hh mb b fi mk mh l mi mj">data_df = dd.read_csv(path.join(base_path, 'raw_data.csv'))<br/>data_df = data_df.drop('Unnamed: 0', axis=1)</span><span id="857a" class="mf iq hh mb b fi mk mh l mi mj">display(data_df.head())</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ml"><img src="../Images/ba4fb0333fc51b6b8d17539bffb0d679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfMHz_2zPuF4AK0ouOuYWw.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">Dataset overview — ~3 million rows, each comprises 1 timestamp, 500 samples and 2 labels</figcaption></figure><p id="48a2" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">注意，用<a class="ae lv" href="https://pandas.pydata.org" rel="noopener ugc nofollow" target="_blank"> Pandas </a>读取和操作数据在大多数机器上是不可能的，因为数据集不适合内存。</p><p id="fcf8" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">以下是本部分提供的以下代码所需的所有初步导入:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="ddc9" class="mf iq hh mb b fi mg mh l mi mj">!pip install sweetviz dask[dataframe] dask-ml dask_xgboost lazypredict==0.2.9</span><span id="8efc" class="mf iq hh mb b fi mk mh l mi mj">import pickle<br/>import numpy as np<br/>import pandas as pd<br/>from os import path<br/>import seaborn as sns<br/>import sweetviz as sv<br/>from glob import glob<br/>import dask.dataframe as dd<br/>import matplotlib.pyplot as plt<br/>from sklearn.utils import shuffle<br/>from dask.distributed import Client<br/>from sklearn.metrics import r2_score<br/>from sklearn.pipeline import Pipeline<br/>from dask.diagnostics import ProgressBar<br/>from dask_ml.xgboost import XGBRegressor<br/>from dask_ml.decomposition import IncrementalPCA<br/>from dask_ml.preprocessing import StandardScaler<br/>from lazypredict.Supervised import LazyRegressor<br/>from sklearn.model_selection import train_test_split<br/>from dask_ml.model_selection import train_test_split as dd_train_test_split</span></pre></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="1162" class="ip iq hh bd ir is lg iu iv iw lh iy iz ja li jc jd je lj jg jh ji lk jk jl jm bi translated">2.探索性数据分析</h1><h2 id="4554" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.1.数据类型</h2><p id="2433" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">通过应用dask的<em class="lu"> describe </em>方法探索每个列数据类型:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="3184" class="mf iq hh mb b fi mg mh l mi mj">display(data_df.describe())</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/8084c6b6aa297b92ebfb16fd390c36f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGCGg9a7fSLdZeacmST_4A.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Data-type check</figcaption></figure><p id="6d13" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，所有的列都是“float64”类型。</p><h2 id="7beb" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.2.样品数量</h2><p id="e5da" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">通过计算Dask的数据帧形状，探索样本量:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="19f8" class="mf iq hh mb b fi mg mh l mi mj">with ProgressBar():<br/>    print("Data contains %d samples" % data_df.shape[0].compute())</span></pre><p id="8abe" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">运行上面的代码返回了2923189个样本的报告，这实际上与1个月的1Hz速率采样(~ 31x24x60x60)相匹配。</p><h2 id="2302" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.3.标签检查</h2><p id="f843" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">绘制两个标签，在此称为'<em class="lu">结果502 </em>和'<em class="lu">结果503 </em>':</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="04ea" class="mf iq hh mb b fi mg mh l mi mj">results_list = ['var0', "result 502", "result 503"]</span><span id="efc4" class="mf iq hh mb b fi mk mh l mi mj">with ProgressBar():<br/>   results_df = data_df[results_list].compute().set_index('var0')</span><span id="e7d3" class="mf iq hh mb b fi mk mh l mi mj">fig, axes = plt.subplots(2, 1, figsize=(20,12))<br/>results2_df = results_df.iloc[:int(1e5),:]<br/>results_df.plot(title='Labels (raw) - full', grid=True, ax=axes[0])<br/>results2_df.plot(title='Labels (raw) - zoom', grid=True, ax=axes[1])<br/>plt.show()</span></pre><p id="82ad" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">对数据运行上面的代码，最终会得到下图:</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/c84b95696832b2d953d07b3671cefafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZr95pRsD08hyc0mPJvx8g.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Labels inspection, zoom-out (top) and zoom-in (bottom, first 100K samples)</figcaption></figure><p id="00da" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，这两个标签相关，而“<em class="lu">结果503 </em>”似乎以不连续的方式起作用，即周期性地下降到零。该观察结果表明，根据➔的“结果503 ”状态，是否可以将数据(样本)分为两组，这可能值得检查。参见第2.6节。</p><h2 id="3328" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.4.样品检验</h2><p id="1b28" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">检查缺失值的数量(无、NaN、NaT):</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="8661" class="mf iq hh mb b fi mg mh l mi mj">missing_values = data_df.isnull().sum()<br/>percent_missing = ((missing_values / data_df.index.size) * 100)</span><span id="7c95" class="mf iq hh mb b fi mk mh l mi mj">with ProgressBar():<br/>   percent_missing_s = percent_missing.compute()</span><span id="eddd" class="mf iq hh mb b fi mk mh l mi mj">print('Missing Values mean: %.2f%%' % percent_missing_s.mean())</span><span id="71c1" class="mf iq hh mb b fi mk mh l mi mj">title_str = 'Missing Values [%]'<br/>percent_missing_s.plot(title=title_str, grid=True, figsize=(20,5))</span></pre><p id="a166" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">对数据运行上面的代码，结果如下图所示:</p><p id="441c" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated"><em class="lu">缺失值均值:15.71% </em></p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/c68897d2211bd750b15b3d14434b13bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yfr1HD7XN-O_4__J4_FL3w.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Missing values among the samples, in percentage [%]</figcaption></figure><p id="fbf5" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">下面是几个代表性样本(代码+数字)的“放大”，以便更好地理解缺失值行为:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="7420" class="mf iq hh mb b fi mg mh l mi mj">with ProgressBar():<br/>   few_features_list = ['var100', 'var200', 'var300', 'var400', 'var500']<br/>   few_features_df = data_df[['var0'] + few_features_list]<br/>   few_features_df = few_features_df.compute().set_index('var0')</span><span id="94ea" class="mf iq hh mb b fi mk mh l mi mj">fig, axes = plt.subplots(5, 1, figsize=(20,25))<br/>for k, features in enumerate(few_features_df.iteritems()):<br/>   title_str = '%s (raw)' % features[0]<br/>   features[1].plot(title=title_str, grid=True, ax=axes[k])<br/>plt.show()</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/a24b4266b9c644cfa267ec29f27dd98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P6Z9-F9ABN-4_8-mIteWxA.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Few representative samples, for exploring the missing-values behavior</figcaption></figure><p id="737a" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，缺失值代表相应信号中的大液滴段，这意味着应考虑零填充策略。原因是“缺失数据”可以被视为“关闭时间”,因此零填充似乎是一个合适的选择。</p><p id="ca20" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">注意，有时采用不同的策略是有意义的，比如用previous、mean、median等填充。在某些情况下，删除缺少值的要素可能是有意义的。在应用这种策略时，下面的代码可能会有用(删除丢失值超过50%的列):</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="1627" class="mf iq hh mb b fi mg mh l mi mj">columns_to_drop = percent_missing_s[percent_missing_s &gt;= 50].index</span><span id="6855" class="mf iq hh mb b fi mk mh l mi mj">data_cleaned_df = data_df.drop(list(columns_to_drop), axis=1)</span><span id="7052" class="mf iq hh mb b fi mk mh l mi mj">print('Columns number reduced from %d to %d after clean-up' % \<br/>      (len(data_df.columns), len(data_cleaned_df.columns)))</span></pre><h2 id="d696" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.5.相关分析</h2><p id="7709" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><a class="ae lv" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关</a>(“r”)系数根据协方差衡量两个连续变量之间的统计关系(又名关联)。它的范围在-1和+1之间，量化了两个变量之间线性关联的方向和强度。<br/> <a class="ae lv" href="https://doi.org/10.2466%2Fpms.1996.82.3.988" rel="noopener ugc nofollow" target="_blank">伊文思(1996) </a>建议如下，为绝对值:</p><ul class=""><li id="8c41" class="kl km hh jp b jq ll ju lm jy lq kc lr kg ls kk lt kr ks kt bi translated">0.0 &lt;= r &lt; 0.2 ➔ “very weak”</li><li id="1f06" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">0.2 &lt;= r &lt; 0.4 ➔ “weak”</li><li id="ebc0" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">0.4 &lt;= r &lt; 0.6 ➔ “moderate”</li><li id="2586" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">0.6 &lt;= r &lt; 0.8 ➔ “strong”</li><li id="90cd" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">0.8 &lt;= r &lt; 1.0 ➔ “very strong”</li></ul><p id="d54d" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">Following is the <strong class="jp hi">相关矩阵</strong>(代码+数字)，由Dask的<em class="lu"> corr </em>方法生成，用matplotlib的<em class="lu"> matshow </em>方法绘制:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="1886" class="mf iq hh mb b fi mg mh l mi mj">with ProgressBar():<br/>   corr_pearson_df = data_df.corr(method='pearson').compute()</span><span id="d49e" class="mf iq hh mb b fi mk mh l mi mj">matfig = plt.figure(figsize=(10,10))<br/>plt.matshow(corr_pearson_df.corr(), fignum=matfig.number)<br/>plt.show()</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/8f533d4d0e860ddaaed058d1f70dfa08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*LkBdo_3J-_7Zw6xNO9kJow.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Correlation Matrix, a yellow shade implies on a relatively high correlation</figcaption></figure><p id="4385" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，在相关矩阵中几乎没有黄色斑点。这意味着样本之间的相互关联，以及将它们组合成联合特征和/或考虑降维技术(例如PCA、t-SNE等)的可能性。)关于利用样本➔之间的互信息，参见第3.1节。</p><p id="0da8" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">接下来，有趣的是关注最后2行/列，它们代表了样本和标签之间的相关性:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="3a45" class="mf iq hh mb b fi mg mh l mi mj">CORRELATION_THR = 0.4</span><span id="ddae" class="mf iq hh mb b fi mk mh l mi mj">correlation_dict = {}</span><span id="e3f2" class="mf iq hh mb b fi mk mh l mi mj">fig, axes = plt.subplots(2, 1, figsize=(20,10))</span><span id="eea1" class="mf iq hh mb b fi mk mh l mi mj">for k, label_type in enumerate(('502', '503')):</span><span id="cb7f" class="mf iq hh mb b fi mk mh l mi mj">   corr_s = corr_pearson_df.loc['result %s' % label_type]<br/>   corr_s = correlation_s.drop(['result 502', 'result 503'])</span><span id="aa38" class="mf iq hh mb b fi mk mh l mi mj">   print('\nSignificant Feature for results %s (&gt;%.1f):' % \<br/>         (label_type, CORRELATION_THR))<br/>   print(corr_s[corr_s &gt; CORRELATION_THR].index.values)</span><span id="e5fe" class="mf iq hh mb b fi mk mh l mi mj">   title_str = 'Result %s - Pearson Correlation' % label_type<br/>   corr_s.plot(ax=axes[k], grid=True, title=title_str)</span><span id="5774" class="mf iq hh mb b fi mk mh l mi mj">   correlation_dict[label_type] = corr_s</span><span id="d1c3" class="mf iq hh mb b fi mk mh l mi mj">plt.show()</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/ca3ec0a98c648c264d5fd9661a163421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HlJAWlX_BZ84E2NHRlbwA.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — Correlation between the samples and the 2 labels</figcaption></figure><p id="5dc1" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，在几个样品中，两种标记都具有“中等”相关性并超过(0.4+)，这是令人鼓舞的。</p><p id="fe44" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">以下代码计算并存储高度相关的要素:</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="8da8" class="mf iq hh mb b fi mg mh l mi mj">fig, axes = plt.subplots(2, 1, figsize=(20,15))</span><span id="3325" class="mf iq hh mb b fi mk mh l mi mj">high_correlated_dict = {}</span><span id="6f7e" class="mf iq hh mb b fi mk mh l mi mj">for k, label_type in enumerate(('502', '503')):</span><span id="2e90" class="mf iq hh mb b fi mk mh l mi mj">   corr_s = correlation_dict[label_type]<br/>   columns_to_keep = ['var0', 'result 502', 'result 503'] + \<br/>                     corr_s[corr_s &gt; CORRELATION_THR].index.tolist()</span><span id="eb7f" class="mf iq hh mb b fi mk mh l mi mj">   with ProgressBar():<br/>      df = data_df[columns_to_keep].compute().set_index('var0')</span><span id="ceed" class="mf iq hh mb b fi mk mh l mi mj">   print('\nHigh Correlated features %s:' % str(df.shape))      <br/>   display(df.head())</span><span id="196c" class="mf iq hh mb b fi mk mh l mi mj">   title_str = 'High Correlated Features (result %s)' % label_type<br/>   plot_df = corr_pearson_df.loc[df.columns]['result %s'%label_type]<br/>   plot_df = plot_df.drop(['result 502', 'result 503'])<br/>   plot_df.plot(kind='bar', title=title_str, grid=True, ax=axes[k])<br/>   high_correlated_dict[label_type] = df</span><span id="c3a4" class="mf iq hh mb b fi mk mh l mi mj">plt.show()</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nj"><img src="../Images/3533c752b8ba299c057b82260aca8286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ySmwA5O64TcSJsBxWkKxQ.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — High Correlated Features</figcaption></figure><p id="4766" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">第一标签(“结果502”)以8个高度相关的样本结束，而第二标签(“结果503”)以34个高度相关的样本结束。</p><p id="4a13" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">这是一个重要的收获，因为在建模阶段坚持使用这些高度相关的样本可能是有益的，主要是在浅层学习阶段，其中特征提取通常是以线性方式完成的，没有内在的组合。</p><h2 id="6d3b" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.6.活动与非活动</h2><p id="d028" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">本节利用了前面几节的内容。根据'<em class="lu">结果503 </em>'状态，即“活动”或“非活动”,检查第2.3节中的2个标签表明数据(样本)或许可以分为2组。此外，第2.5节暗示只有一小部分样本与标签相关，因此关注这些特征可能是有益的(计算方面和存储方面，主要用于大数据分析)。</p><p id="60d9" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">以下代码应用<a class="ae lv" href="https://pypi.org/project/sweetviz/" rel="noopener ugc nofollow" target="_blank"> SweetViz </a>的<em class="lu"> compare_intra </em>函数来实现该分析。该函数采用一个布尔序列作为参数之一，并采用一个显式的“name”元组来命名(true，false)结果数据集。请注意，在内部，这将创建2个单独的数据帧来表示每个结果组，即“活动”和“非活动”。</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="ce7f" class="mf iq hh mb b fi mg mh l mi mj">for k, label_type in enumerate(('502', '503')):<br/>   <br/>   html_log = 'report_%s.html' % label_type</span><span id="65bf" class="mf iq hh mb b fi mk mh l mi mj">   feature_config = sv.FeatureConfig(<br/>      force_num=high_correlated_df.columns.tolist()<br/>   )</span><span id="ca00" class="mf iq hh mb b fi mk mh l mi mj">   report = sv.compare_intra(high_correlated_df,<br/>                             high_correlated_df['result 503'] == 0,<br/>                             ['Stop', 'Active'],<br/>                             'result %s' % label_type,<br/>                             feature_config,<br/>                             pairwise_analysis='auto')</span><span id="7307" class="mf iq hh mb b fi mk mh l mi mj">   report.show_html(html_log)</span></pre><p id="1991" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">运行上面的代码最终得到2个HTML报告，每个标签一个(“502”和“503”)。每个报告都提供了对每个样本的深入了解，以及强大的成对分析视图(充当关联矩阵)。</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nk"><img src="../Images/f980e3806306cfe315c5e1d99b942e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgH0b3K90stqbGfouA2TxA.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA — SweetViz compare_intra analysis for “Active” vs “InActive” (aka “Stop”) groups, per each label</figcaption></figure><p id="0e94" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">上图来自两个HTML报告。左侧是指“<em class="lu">结果502 </em>报告，右侧是指“<em class="lu">结果503 </em>报告。顶行提供了两组之间的高级统计比较。中间部分关注一个相关性最高的样本("<em class="lu"> var109" </em>)，底部部分介绍每组的成对相关矩阵。请注意，SweetViz <em class="lu"> compare_intra </em>在处理大量特征时表现不佳，因此将其应用于选定的子集(例如最相关的子集)非常重要。</p><h2 id="269e" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">2.7.降维</h2><p id="e707" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">因此，主成分分析(PCA)被用于开发最大方差，保留最少的成分。Dask增量PCA用于处理内存受限条件下的大量数据。</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="dd00" class="mf iq hh mb b fi mg mh l mi mj">lr = Pipeline(steps=[<br/>     ('scale', StandardScaler()),<br/>     ('pca', IncrementalPCA()),<br/>])</span><span id="e7a3" class="mf iq hh mb b fi mk mh l mi mj">dropped_list = ['var0', 'result 502', 'result 503']<br/>data_filt_df = data_df.drop(dropped_list, axis=1).fillna(0)</span><span id="0e4a" class="mf iq hh mb b fi mk mh l mi mj">X_arr = data_filt_df.to_dask_array(lengths=True).astype(float)</span><span id="e59c" class="mf iq hh mb b fi mk mh l mi mj">lr_fitted = lr.fit(X_arr)</span><span id="dd88" class="mf iq hh mb b fi mk mh l mi mj">pca = lr_fitted['pca']</span><span id="21a0" class="mf iq hh mb b fi mk mh l mi mj">print('PCA explained variance (%d components):' % pca_components_num)</span><span id="057b" class="mf iq hh mb b fi mk mh l mi mj">print(pca.explained_variance_ratio_)<br/>print(pca.singular_values_)</span><span id="3554" class="mf iq hh mb b fi mk mh l mi mj">explained_variance = 0.9</span><span id="daa6" class="mf iq hh mb b fi mk mh l mi mj">cumsum_var = pca.explained_variance_ratio_.cumsum() &gt; explained_variance</span><span id="444d" class="mf iq hh mb b fi mk mh l mi mj">idx = cumsum_var.argmax()</span><span id="d55c" class="mf iq hh mb b fi mk mh l mi mj">print('Number of components needed for having at least %.2f is equal to %d' % (explained_variance, idx))</span><span id="89a6" class="mf iq hh mb b fi mk mh l mi mj">pd.Series(pca.explained_variance_ratio_.cumsum()).plot(title='PCA explained variance (n=%d)' % pca_components_num, grid=True, figsize=(20,5))</span></pre><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nl"><img src="../Images/4596e480d0552b9c453eaf521f27fa7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeMBzJ8pXeIFjwzzQUDJcQ.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">EDA - PCA Explained Variance</figcaption></figure><p id="c30f" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，需要121个分量来解释90%的方差。目前，我们正在对所有数据进行主成分分析，用于EDA目的。稍后，可以如下应用带建模的PCA:</p><ul class=""><li id="d079" class="kl km hh jp b jq ll ju lm jy lq kc lr kg ls kk lt kr ks kt bi translated">仅在训练集上安装PCA➔<em class="lu">PCA . fit(x _ train)</em></li><li id="c839" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk lt kr ks kt bi translated">将映射(转换)应用于训练集和测试集➔x _ train =<em class="lu">PCA . transform(x _ train)，x _ test = PCA . transform(x _ test)</em></li></ul></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="08a0" class="ip iq hh bd ir is lg iu iv iw lh iy iz ja li jc jd je lj jg jh ji lk jk jl jm bi translated">3.用浅层技术建模</h1><h2 id="ed7d" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">3.1.特征工程</h2><p id="4bb8" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">第2节提供的EDA将<strong class="jp hi">样品</strong> (' <em class="lu"> var1 </em>，…，'<em class="lu"> var501 </em>')视为<strong class="jp hi">特征</strong>。在这一点上，当处理特征工程时，可以考虑以下方案(可以应用一个或多个):</p><ol class=""><li id="6901" class="kl km hh jp b jq ll ju lm jy lq kc lr kg ls kk kq kr ks kt bi translated">将特征定义为样本，即1:1映射</li><li id="75bd" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">将特征定义为降维后的样本，例如通过应用PCA、t-SNE、相关阈值等。</li><li id="3aa2" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">通过处理样本来定义特征，线性或非线性，也称为基于核的映射。</li><li id="0935" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">考虑添加将样本组合在一起的特征，获得标签样本之间的交互信息</li></ol><p id="a1f6" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">通常建议从最轻的方案开始，如果有必要的话，从武器库中较重的工具开始。因此，我们将从上述列表中的方案1+2开始，即将特征定义为高相关性样本。</p><h2 id="2f65" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">3.2.懒惰预测</h2><p id="e71d" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><a class="ae lv" href="https://lazypredict.readthedocs.io/en/latest" rel="noopener ugc nofollow" target="_blank"> Lazy Predict </a>有助于用很少的代码构建许多基本模型，并有助于了解哪些模型在没有任何参数调整的情况下工作得更好。不幸的是，Lazy Predict在大数据上表现不佳(内存消耗太高)，但假设数据在一段时间内大致类似地分布，将它应用于一小部分数据可能是有教育意义的。这里的目的基本上是提供特征如何与标注“交互”的粗略感觉，这可能暗示如何进行进一步的分析。因此，允许在结构化期间混洗数据，这通常不太适合时间序列分析。当时间序列特征被认为不太重要时，这可能表现得很好。在其他情况下，可以跳过混洗数据。</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="8028" class="mf iq hh mb b fi mg mh l mi mj">for label_type in ('502', '503'):</span><span id="8528" class="mf iq hh mb b fi mk mh l mi mj">   # only small portion, due to memory limitations:<br/>   df = high_correlated_dict[label_type].iloc[:int(1e4),:]</span><span id="0afb" class="mf iq hh mb b fi mk mh l mi mj">   # Fill missing values with zeros:<br/>   df = df.fillna(0)</span><span id="03d8" class="mf iq hh mb b fi mk mh l mi mj">   # Structure the data (+shuffle):<br/>   X, y = shuffle(df.drop(['result 502', 'result 503'], axis=1),<br/>                  df['result %s' % label_type],<br/>                  random_state=42)</span><span id="75bc" class="mf iq hh mb b fi mk mh l mi mj">   # Split data into train and test sets (80:20 ratio):<br/>   cv_tuple = train_test_split(X, y, test_size=.2, random_state=0)<br/>   X_train, X_test, y_train, y_test = cv_tuple</span><span id="ae1b" class="mf iq hh mb b fi mk mh l mi mj">   # Create an instance of the estimator, and fit it to the data:<br/>   reg = LazyRegressor(verbose=0,<br/>                       ignore_warnings=False,<br/>                       custom_metric=None,<br/>                       predictions=True)</span><span id="1a4d" class="mf iq hh mb b fi mk mh l mi mj">   lazy_models, lazy_predictions = reg.fit(X_train, <br/>                                           X_test,<br/>                                           y_train,<br/>                                           y_test)</span><span id="f658" class="mf iq hh mb b fi mk mh l mi mj">   # Results analysis:<br/>   print('Lazy Models (label = result %s)' % label_type)<br/>   display(lazy_models)</span><span id="7eeb" class="mf iq hh mb b fi mk mh l mi mj">   if not lazy_models.empty:</span><span id="742b" class="mf iq hh mb b fi mk mh l mi mj">      fig, axs = plt.subplots(1, 1, figsize=(20, 5), sharex=True)<br/>      sns.set_theme(style="whitegrid")<br/>      sns.barplot(x=lazy_models.index,<br/>                  y="R-Squared", <br/>                  data=lazy_models,<br/>                 ax=axs)<br/>      axs.set(ylim=(0, 1))<br/>      axs.set(title='Lazy Models (label = result %s)' % label_type)<br/>      plt.xticks(rotation=90)<br/>      plt.show()</span></pre><p id="e43a" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">运行上述代码最终会得到以下结果:</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/7fe0ad5ca93a1c6186dc8d27f97f742f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPP8xDrTBGL6OHH0RkTugg.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">Modeling — Lazy Predict, over small portion of the big-data (+shuffling)</figcaption></figure><p id="62bd" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，一些模型在高R平方精度方面表现良好，这是令人鼓舞的。</p><h2 id="d91d" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">3.3.XGBoost回归器</h2><p id="f29c" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">前一节(3.2)暗示了浅层学习方案可能对数据表现良好。这一节重点介绍了流行且强大的XGBoost回归模型。在此，不应用洗牌，使用Dask的<em class="lu">客户端</em>、<em class="lu"> train_test_split </em>和<em class="lu"> XGBRegressor </em>机器处理全部数据。正在生成两个模型，每个标签一个。</p><pre class="lw lx ly lz fd ma mb mc md aw me bi"><span id="a7ca" class="mf iq hh mb b fi mg mh l mi mj">client = Client(processes=True, threads_per_worker=1)<br/>#client = Client('scheduler-address:8786')</span><span id="3d93" class="mf iq hh mb b fi mk mh l mi mj">for label_type in ('502', '503'):</span><span id="1310" class="mf iq hh mb b fi mk mh l mi mj">   # Structure the data (high-correlated features only):<br/>   df = high_correlated_dict[label_type]<br/>   high_correlated_dd = dd.from_pandas(df , npartitions=3)</span><span id="e091" class="mf iq hh mb b fi mk mh l mi mj">   X = high_correlated_dd.drop(['result 502', 'result 503'], axis=1)<br/>   y = high_correlated_dd['result %s' % label_type]</span><span id="1b07" class="mf iq hh mb b fi mk mh l mi mj">   # Split data into train and test sets (80:20 ratio):<br/>   cv_tpl = dd_train_test_split(X, y, test_size=.2, random_state=0)<br/>   X_train, X_test, y_train, y_test = cv_tpl</span><span id="1191" class="mf iq hh mb b fi mk mh l mi mj">   # Fit the model<br/>   est = XGBRegressor()<br/>   est.fit(X_train, y_train)</span><span id="5819" class="mf iq hh mb b fi mk mh l mi mj">   # Results analysis:<br/>   test_df = pd.DataFrame(y_test.compute())<br/>   test_df['predicted'] = est.predict(X_test).compute()</span><span id="4bad" class="mf iq hh mb b fi mk mh l mi mj">   accuracy_r2 = r2_score(test_df['result %s' % label_type],<br/>                          test_df['predicted'])</span><span id="c2e5" class="mf iq hh mb b fi mk mh l mi mj">   print('accuracy=%.2f' % accuracy_r2)</span><span id="ec30" class="mf iq hh mb b fi mk mh l mi mj">   feature_import_df = pd.DataFrame(index=X_test.columns,<br/>                                    data=est.feature_importances_)</span><span id="fdfc" class="mf iq hh mb b fi mk mh l mi mj">   title_str = 'Result %s - Features Import. (XGboost)' % label_type<br/>   feature_import_df.plot(title=title_str, <br/>                          kind='bar',<br/>                          legend=None,<br/>                          figsize=(20,5))</span><span id="2058" class="mf iq hh mb b fi mk mh l mi mj">   fig, axes = plt.subplots(2, 1, figsize=(20,12))<br/>   title_str = 'Result %s - Act. vs. Predicted - full' % label_type<br/>   test_df.plot(title=, grid=True, ax=axes[0])<br/>   title_str = 'Result %s - Act. vs. Predicted - first 10K samples'    <br/>   test_df_zoom = test_df.iloc[:int(1e4),:]<br/>   test_df_zoom.plot(title=title_str, grid=True, ax=axes[1])<br/>   plt.show()</span></pre><p id="2f84" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">运行上述代码最终会得到以下结果:</p><figure class="lw lx ly lz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nm"><img src="../Images/9c7a714b42089eb4aef50ef73c24d188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3QU5kdh8qEunrhTuzldEg.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx">Modeling — XGBoost Regressor</figcaption></figure><p id="76b1" class="pw-post-body-paragraph jn jo hh jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ha bi translated">可以观察到，建模结束得相当好，因为'<em class="lu">结果502 </em>'和'<em class="lu">结果503 </em>'模型分别引入了0.89和0.99的R2平方精度。顶部的特征重要性条形图也有意义，因为重要的特征与高度相关的样本相匹配，这些样本在第2.5节中检索。</p></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="9886" class="ip iq hh bd ir is lg iu iv iw lh iy iz ja li jc jd je lj jg jh ji lk jk jl jm bi translated">摘要</h1><p id="a239" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">一个真正的大数据回归问题被介绍，伴随着代码和定性分析。第一部分涉及以有效的方式加载数据。接下来的部分集中在EDA的各个方面。最后，第三部分介绍了建模细节，包括特性工程考虑，最终在R2平方精度方面取得了令人满意的结果。</p><h2 id="16c2" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">下一步是什么？</h2><ol class=""><li id="3227" class="kl km hh jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">探索其他降维技术，或者替代当前的高相关性阈值方案，或者在当前的高相关性阈值方案之上</li><li id="1efd" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">探索其他浅层学习模型，如轻型GBM等。</li><li id="723c" class="kl km hh jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">深度学习技术的转换(下一部分文章)，例如LSTM/RNN，自动编码器，变压器等。</li></ol><h2 id="c4b0" class="mf iq hh bd ir mq mr ms iv mt mu mv iz jy mw mx jd kc my mz jh kg na nb jl nc bi translated">最后一个音符</h2><p id="5acf" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在处理大数据时，pickle是您的好朋友……通常建议将中间(和最终)对象存储在指定的Pickle文件中，只在没有它们的情况下重复繁重的计算。</p><div class="nn no ez fb np nq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hi fi z dy nv ea eb nw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">medium.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe in nq"/></div></div></a></div></div></div>    
</body>
</html>