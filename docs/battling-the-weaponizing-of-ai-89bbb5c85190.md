# 对抗人工智能的武器化

> 原文：<https://medium.com/mlearning-ai/battling-the-weaponizing-of-ai-89bbb5c85190?source=collection_archive---------2----------------------->

![](img/7fd815516ef57929e06159c4d5606bd0.png)

Photo Credit: Ian Frome, Unsplash.com

“我不再用脸书了，”她说。“他们破坏了民主。”

当她说出这句话时，我正在主持一个新手机应用程序设计的可用性会议。几年前，我是 IDEO 的设计研究主管，当时我们正在为一家电信公司进行服务设计项目。

我们向她展示的设计概念有一个无害但又无处不在的功能——使用脸书登录的能力。

但这位 20 多岁、不到 40 岁的年轻女士对这个功能犹豫不决，并继续告诉我她为什么不再信任社交网络。当然，这次会议是在 2016 年总统选举之后举行的。在这次选举中，一个被许多人认为是电视上的奇观，被认为是浮夸的骗子的人刚刚被选为我们的最高官员。

尽管现在是 2020 年，我们的民主仍然完好无损。它不是没有暴力和叛逆的挑战。她的挑衅性声明让我想知道——你如何阻止一个由人工智能推动的流氓技术平台摧毁民主、真理、正义和道德等社会支柱？

如果脸书的政纲真的“破坏了民主”，那么有没有办法对抗这种伤害呢？或者我们一定要相信，正如脸书所说，总会有坏演员，你不能阻止他们表演。“坏艾”是否已经走上了大规模枪击和恐怖主义行为的道路——遭到普遍谴责，但似乎无法阻止？

她的评论将我从“我们需要更有道德的人工智能”的谴责性讲座推向了“我们如何才能创造人工智能，并更好地意识到它在新形成的平台上使用的后果？”

所以让我们解剖一下人工智能武器化最著名的案例之一——社交媒体和 2016 年总统选举。并详细说明如何与之对抗。

# 一系列不幸的事件

## 一个信号被忽略，一个偏见被点燃，一个变化的叙述

让我们回到 2016 年 2 月。

**一个信号被忽略**

多亏了一篇[非常有研究的电讯文章](https://www.wired.com/story/inside-facebook-mark-zuckerberg-2-years-of-hell/)，我们知道这是关于脸书投资者和内部人士[罗伯特·麦克纳米](https://en.wikipedia.org/wiki/Roger_McNamee)注意到脸书的新闻内容发生了一些奇怪的事情。他看到当时的总统候选人伯尼·桑德斯(后来再次成为总统候选人伯尼·桑德斯)发布的消息，似乎是关闭的。麦克纳米告诉《连线》:

“我观察到的迷因表面上来自一个与桑德斯竞选团队有关的脸书组织，但不可能来自桑德斯竞选团队，”他回忆道，“然而它们的组织和传播方式表明有人有预算。我坐在那里想，'这真的很奇怪。我的意思是，这并不好。"

麦克纳米的“奇怪的新闻反馈信号”是由算法、数据泄露和俄罗斯坏人共同创造的新宇宙的缩影。同年，[脸书也解雇了其趋势新闻专栏的“人工编辑”，转而支持一种算法，此前](https://www.businessinsider.com/facebook-human-news-editors-2016-9)[指控人工编辑压制保守新闻](https://gizmodo.com/former-facebook-workers-we-routinely-suppressed-conser-1775461006)。

但事实上，对于脸书以及随后的 Twitter、YouTube 和几乎所有其他依赖用户生成内容的社交网络来说，压制真实新闻的算法才是更大的问题。

坏演员正在创建虚假的社交网络团体，自称是从黑人生活问题到疫苗专家的每个人，但实际上是白人至上主义者和医学快客。(点击此处了解更多关于这些团体的信息[。](https://www.wired.com/story/facebook-groups-are-destroying-america/))2016 年，脸书甚至向联邦调查局报告称，其安全团队注意到俄罗斯演员试图“窃取记者和公众人物的证件”但是没有任何结果，新闻报道还在继续。

内容工厂可能是从坏演员开始的，但正是普通老人用越来越多的危言耸听的内容淹没了社交网络。仅仅通过几个“训练”广告，俄罗斯黑客就利用脸书的算法，用虚假的、未经证实的、耸人听闻的、但受欢迎的内容来攻击共和党人和右倾观众。其他坏演员利用脸书的 PageRank 算法对喜欢、分享和评论的偏好，在社交媒体上像野火一样传播虚假、阴谋性的内容。”

尽管上述真实情况在知识分子聚会上成为了很好的笑点，但当新冠肺炎·疫情抵达美国海岸时，错误信息从有趣变成了致命，社交媒体内容的罪恶开始影响其用户的生活。2016 年开始的噩梦般的选举狂笑[变成了 2021 年对](https://www.nytimes.com/2021/01/11/arts/television/capitol-riot-graphic-videos.html)美国国会大厦的全面恐怖袭击。

在此之后，r [esearch](https://www.wired.com/story/russian-facebook-ads-targeted-us-voters-before-2016-election/) 和[调查](https://www.npr.org/2017/09/08/549284183/facebook-acknowledges-russian-ads-in-2016-election-will-investigations-follow)表明，如果该平台的人工智能模型没有被设计成篡夺人类行为，那么许多出错的事情就永远不会发生 ***。*** 这不是偶然事件。

这些模型的设计是基于一些最普遍的、激励人类行为的偏见。人工智能没有出错——它按照设计的功能运行。社交网络平台设计他们的人工智能模型，利用我们的偏见来对付我们。

他们这样做是有意还是无意，还有待观察。但最后都无所谓了。因为 2016 年向我们展示了人工智能推动的内容可以被操纵和武器化的速度有多快。现在是我们对抗它的时候了。

公平地说，脸书和 YouTube 后来修改了它们的机器学习模型，以减少对极端内容的过滤泡沫。但是这些模型仍然基于这样一种观念，即我所说的和我所做的是绝对真实的，任何研究人类心理学的人都知道这不是真的。

因此，我们必须在对抗人工智能模型的武器化方面更进一步。但是怎么做呢？很简单，从问题的源头开始——我们的思想。

# 思维敏捷；缓慢跳跃

因为我有人机交互的背景，我倾向于关注数字设计如何影响人类的行为和决策。钻研这个话题意味着要绕道进入心理学和认知偏差的世界。

由阿莫斯·特沃斯基和丹尼尔·卡恩门创造的[认知偏差](https://en.wikipedia.org/wiki/Cognitive_bias)仅仅是一种思维方式，它导致人们“系统性偏离”，偏离正常的决策模式。这是你的大脑走捷径做决定，用你的潜意识作为它的向导。

在他的书《思考快与慢》中，Kahnmen 将几十年来对人类行为和决策的研究浓缩成面向公众的精辟见解。这本书(你必须读一读)讲述了我们的大脑如何在两条轨道上运行——快速思维轨道采用许多捷径来快速得出结论，而“缓慢”思维轨道让我们通过记忆和知识进行深思熟虑的推理来做出决定。

猜猜我们大部分时间都在思考哪条赛道？

你猜对了——我们缺乏认知负荷承受能力，这让我们大多数人大部分时间都生活在快车道上。

这种时尚思维最著名的例子之一来自《琳达问题》一书，描述了[合取谬误](https://en.wikipedia.org/wiki/Conjunction_fallacy)。

琳达 31 岁，单身，直言不讳，非常聪明。她主修哲学。作为一名学生，她深切关注歧视和社会正义问题，还参加了反核示威。

*哪个可能性更大？*

*琳达是银行出纳员。*

琳达是一名银行出纳员，积极参与女权运动。

正如 Kahnmen 和 Tversky 的研究证明的那样，大多数人选择 Linda 是一名银行出纳员和一名活动家。

现在，如果你停下来想一分钟，你会推理出概率定律清楚地表明，一个特征为真的可能性比两个特征在一起的可能性大得多。从逻辑上讲，让 Linda 只符合一个标准更容易。但是为什么绝大多数人会选择后者呢？因为他们的思维捷径告诉他们，琳达一定是一名银行出纳员*和*一名女权主义者，因为如果她参加示威游行并关心社会正义问题，这两个特征一定是真实的。但从数学上来说，它们不是。

认知偏见是植入我们头脑中的小小种子，来自生活经验和知识的相关时刻。就其本身而言，认知偏差既不是消极的也不是积极的。但是，当它们与现实世界的刺激相互作用时，这些种子可能会长成危险的混乱思维处理丛林，从而导致一些灾难性的结果。

脸书现在正在创建程序来防止滥用它的平台，但是为了解决这个问题，必须发展一些基本的信念。

**偏压点燃**

目前，心理学上已知有 [180+认知偏差](https://commons.wikimedia.org/wiki/File:Cognitive_bias_codex_en.svg)影响人类行为。

如果你不认为潜意识偏见会影响我们的决策，试着参加隐性项目的[隐性偏见测试](https://implicit.harvard.edu/implicit/research/)。这保证是一次令人大开眼界的经历。现在回到那个讨厌的脸书破坏民主的指控。

如果没有一个名为“可用性级联”的非常真实的认知偏差，俄罗斯的广告可能会失败。

简而言之，可用性级联是人们仅仅因为看到某件事一遍又一遍地重复而相信它的倾向。这是[可用性启发式](https://en.wikipedia.org/wiki/Availability_heuristic)的副产品，这是一种认知偏差，让我们在评估想法、概念或决定时依赖于脑海中的任何想法。

谚语“重复某事足够长的时间，它就会成真，”描述了人们仅仅因为听到很多就不得不相信他们所听到的倾向。

俄罗斯人发送了关于希拉里的电子邮件、非法移民和其他热点问题的针对性问题广告。这些广告被阅读、点击、分享。参与内容的人越多，算法显示的内容就越多。算法显示的内容越多，参与的人就越多。

你看到这里出现的模式了吗？

但那又怎样？这只是一个广告。

嗯，这是事实，但目标广告并不是人们在社交媒体平台上消费内容的唯一方式。

让我们来分析一下。

区区 10 万美元的广告费用怎么可能扰乱并改变总统选举的结果呢？！

# 叙述改变了

## 学、听、跳

在我之前的[数据宣言文章](https://www.linkedin.com/pulse/my-data-design-ethics-manifesto-ovetta-sampson/)中，我用一个算法搜索单词“种子”的例子来分析算法是如何工作的让我们回到搜索单词“种子”的算法上来让我们用“非法移民”、“退伍军人福利”和“自由议程”来代替它。

现在，这些词似乎没有太多的共同点，但算法并不关心。它的一项工作就是找到你想要的内容并提供给你。

假设你点击了那个有针对性的俄罗斯广告。

例如在脸书，你可以像分享新闻文章或帖子一样分享广告。现在算法正在被训练和学习。当你点击和分享它的时候，它的度量意味着你喜欢它。对算法来说这很好。但是对于那些曾经监测到可疑内容激增的人来说，这并不好。这一点也不好。

所以现在它开始为你提供更多的广告，比如“移民”和“退伍军人”但它也显示了 DACA 和奥巴马总统的梦想家行政命令的广告。你不能点击这些。所以最终它会停止向你发送附属内容。

它从倾听开始。然后它学习。最后，智能系统开始采取，我称之为“跳跃取悦”

与此同时，您的朋友正在分享来自 let's say Breitbart News 的内容，其中包含“非法移民”等词语所以现在智能系统选择显示这些内容，因为“移民”这个词

这是在测试你是否喜欢这个内容。等你点击。一个类似公制的。你分享！另一个类似的度量。你得到的圣杯！你喜欢这篇文章，你评论它。

所以现在智能系统就像这样，“好吧，让我们开始提供更多这样的服务。”请记住，智能系统会在您每次点赞、分享或评论时获得奖励。就像一只受过训练的狗，它寻求从你那里得到赞扬。你参与得越多，它就越认为你喜欢它给你的输出。

很快你就会听说非法移民从一个叫亚历克斯·琼斯的家伙那里获得健康福利。你从未听说过他，但当你点击一篇博文后，它会邀请你喜欢他的脸书页面。

现在你在消费谈论非法移民谋杀无辜美国人的内容。

你变得惊慌。你想脱离——但你不能。你的新闻订阅现在充斥着关于“非法移民”的文章、广告和迷因你不再喜欢——算法会喂你更多。你停止分享——它试图让你得到更多。它不是向你展示替代内容，而是双倍返还你已经奖励的内容。

就像任何受过训练的狗一样——用一个算法来旋转是极其困难的；反悔是很难的。你不能改变方向。你有点卡住了，因为这些算法就是这样设计的。但他们不必如此。稍后会详细介绍。

很快你就会看到很多文章写着“非法移民”但是他们也有其他热门词汇。

与此同时，脸书一直在存储这些数据。它一直把你归类为这个年龄，种族，性别，现在是政治的人。

现在让我们假设这些数据被盗，脸书保证这些数据是私人的，不会与你联系在一起。脸书出现了数据泄露，数百万像你一样的人的数据现在掌握在为政治活动工作的人手中。

让我们假设他们使用自己的算法寻找那些用“移民”、“退伍军人福利”和“自由议程”这些词消费脸书内容的人算法只需几秒钟就能找到你的姓名、年龄、家乡和性别。

对照你所在城镇的登记选民名单，一周后你会收到一张传单，上面写着“非法移民”、“犯罪”和“自由议程”。

同样是这位政治操作人员，在你们的报纸、电视，当然还有脸书，用更多的广告淹没你们。

根据《连线》杂志上这篇关于脸书选举内容的精彩文章，研究员 Jonathan Albright 透露，来自六个俄罗斯宣传账户的这些帖子在该平台上被分享了 3.4 亿次！

**这就是放大算法效应。**

你会反复听到同样的信息。你在你朋友的墙上看到的。你看到它被喜欢、分享和支持。现在来源已经不重要了。你的大脑会想一件事:“我一直看到它，所以它一定是真的。”

到了选举日，耶稣本人可能会下来告诉保守派，无证移民很少犯罪，人们不会相信他！

在 Facebooks 算法、俄罗斯广告买家、窃取数据的政治间谍和玩心理游戏的认知偏见之间，不知情的选民永远没有机会获得真相。

社交媒体平台能预见到这一点吗？

不是以他们过去的态度。他们说他们太天真了。他们没有准备好面对操纵他们产品的坏人。这是真的。但是，数据泄露部分不仅仅是坏演员，对吗？

然后还有那些算法。它们一定要被设计成那样吗？不，他们没有。

# 反对武器化

坏演员是的，但可预防的设计缺陷可能和某人应该知道得更好——绝对。

以下是我发现有助于处理和阻止意外后果的人工智能设计原则，这些意外后果肯定会在人类认知偏差与强大的机器学习模型交互时出现。

没有无害的算法。任何技术都可以武器化。无论你是霍布斯团队，相信人类本质上是邪恶的，还是卢梭阵营，相信我们生来是善良的，都不重要。我们创造的任何东西都有可能造成伤害。句号。在这一点上没有谈判的余地，机器学习模型这些人工智能的构建模块也不能幸免于这种潜力。因此，创造无害人工智能的第一步是知道所有人工智能都可以武器化，不管人们的意图如何。

**倾听意外后果**。认为一个智能系统的所有负面影响都可以在设计过程中被预见是疯狂的。你可以做一些练习和活动来最大限度地减少伤害([我在 IDEO](https://www.ideo.com/blog/ai-needs-an-ethical-compass-this-tool-can-help) 和现在的微软帮助创造了许多这样的练习和活动)，但你不会得到所有的练习和活动。算法上的伤害不能被忽视，但它可以而且应该被反复解决。智能系统不应该为静态行为而设计。它被称为机器学习是有原因的。它一直在学习。因此，我们应该设计由机器学习支持的体验，以便随时倾听。倾听“为了取悦而跳跃”，从常规到极端的算法跳跃——这几乎发生在每个智能系统中，所以为什么不为这种必然性而构建呢。

将意想不到的后果作为设计的机会。IDEO Chicago 的管理合伙人 Travis 给了我这块宝石。没有人第一次就能猜对。我们应该以迭代的心态来设计这些产品。当意想不到的后果出现时——比如俄罗斯黑客散布假消息——利用这些机会更好地设计你的智能系统。

**为人际关系建立智能系统，而不是任务清单。**我们大多数的智能系统都被设计用来“接管”不必要的人类任务。对于人工智能的第一个十年来说，这可能是伟大的，但我们现在已经过去了。智能系统可以由算法构建者***和*** 训练，这些人参与最终的产品或服务。设计用户可以告诉他们的智能系统的方式——不，以及如何改变，何时转向。

设计与人工智能系统的互动，类似于一种新的关系；[我称之为“有意识的人工智能”，](https://yourstory.com/2019/11/ai-design-ovetta-sampson-microsoft)人类和智能系统合作行动。随着时间的推移，你可以让人工智能系统变得不那么动态。但是在开始的时候，当我们开始互相了解的时候，建立一些方法让我教人工智能系统我所期望的。让模型能够学习，而不是预测权威。没有绝对的真理——尤其是对人类而言。我们是一群善变的人，我们他们构建算法的方式应该考虑到这个事实。

**让大人有代理权*。*不设则已，不设则已。不要以为你是模型制作人，即使模型一旦成型，他也知道。就我而言，我不明白为什么我们要建造智能系统，好像它们实际上是石头做的一样。人们谈论算法“强加”给我们，“统治我们”，好像他们是上帝，而我们只是凡人没有权力。任何智能系统都应该是动态构建的，而不是一成不变的。灵活，不是一成不变的。适应性强而不是一成不变。因为我们人类就是这样。**

以多元化的心态构建智能系统。目前，许多推荐引擎本质上都很简单。他们是在狭窄的数据集上接受训练的，并且经常被搁置一旁。为什么？我们应该建立智能系统，使用各种输入来学习，是的，理解产品或服务的正确输出。我喜欢 Spotify 用于智能音乐推荐系统的[分层机器学习技术。他们将用户体验(一种设计)构建到他们的 ML 模型中，以影响推荐引擎。很可爱。脸书肯定会努力。](https://read.hyperight.com/how-spotify-knows-what-music-you-like-hint-with-machine-learning/)[在这篇文章](https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/)中，他们解释了他们的推荐引擎有多复杂，以及它用来决定向一个名叫胡安的虚构用户推荐什么的许多输入和计算。自 2016 年以来，他们取得了长足的进步！他们的模型绝对是多层次的，这是一件好事。然而，对我来说，这一切都归结为一种极其简单的人性观

*“因为 Juan 与该内容的制作人有联系，或者他已经选择关注该内容的制作人，所以这些内容很可能与他相关或对他感兴趣…”*

但是只要读一读 Kahnmen 的书，你就会发现人类并没有那么简单。我们中的一些人想要阅读我们厌恶的内容，只是因为我们想要全面的体验。或者我们错误地反复点击与我们无关的东西。或者我们的家庭可能都热爱葛培理，但我们最近碰巧抛弃了与葛培理相反的东西。从人类是非理性的前提出发，内容可能更相关。

**给人一个喘息的机会。**如上所述，即使是受过良好教育和最善意的内容消费者也可能被骗进一个 Q-non 兔子洞。我们是人类，这意味着我们很脆弱。社交网络尤其不擅长扰乱正念，但越来越多的这种迫使人们依赖于他们的设备或应用程序的坏习惯正在渗透到用户体验设计的各个角落。提醒、通知是新的启示，实际上不必设计到体验中。作为设计师，我们更了解。我们应该保护人类的脆弱，而不是利用它。当你建造智能系统来对抗我们的偏见时，你就是在利用我们的人性。这是不允许的。

**不要双重偏见**:当我们设计智能系统时，有一些认知偏见是我们应该特别设计的。其中包括:

***自动化偏差—*** 人类倾向于支持来自自动化决策系统的建议，并忽略未经自动化处理的矛盾信息，即使这些信息是正确的。人们不信任智能系统，这是一个神话。[所有的研究都指向相反的方向。人类最初可能会对自动化系统表示不信任，但随着时间的推移，他们的行为表明并非如此。他们太信任他们了。我们需要围绕这个缺陷进行设计。](https://www.researchgate.net/publication/340605601_Human_trust_in_artificial_intelligence_Review_of_empirical_research_Academy_of_Management_Annals_in_press)

***锚定偏差—*** 人类倾向于在所有后续决策中过于依赖最早的信息

*****确认偏差—*** 相信某事是因为它确认了我们已经相信的事情。**

**像其他东西一样，人工智能可以被武器化，用来对付我们。作为以人为中心的设计师，我们站在伤害和满足之间的人行横道上。现在是我们更严肃地对待这一职责的时候了。**