<html>
<head>
<title>Fundamentals of Deep Learning — Activation Functions and When to Use Them?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的基础——激活函数以及何时使用它们？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fundamentals-of-deep-learning-activation-functions-and-when-to-use-them-e7d8e2cb231b?source=collection_archive---------1-----------------------#2021-05-21">https://medium.com/mlearning-ai/fundamentals-of-deep-learning-activation-functions-and-when-to-use-them-e7d8e2cb231b?source=collection_archive---------1-----------------------#2021-05-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3ba9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活功能是深度学习最基本的构建模块。如果你像我一样是深度学习的初学者，你一定有过类似“为什么我们有这么多激活功能？”，“为什么一个比另一个好用？”，“我们怎么知道用哪一个？”，“我应该是数学专家才能理解它们吗？”在你的脑海中飘荡。在这篇文章中，我将回答所有这些是什么，为什么和如何。让我们开始吧。</p><p id="021f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我强烈建议你在浏览这个博客之前理解什么是人工神经网络&amp;它是如何工作的。这里有一个资源可以给你一个坚实的基础。</p><div class="jc jd ez fb je jf"><a href="https://becominghuman.ai/artificial-neural-networks-and-deep-learning-a3c9136f2137" rel="noopener  ugc nofollow" target="_blank"><div class="jg ab dw"><div class="jh ab ji cl cj jj"><h2 class="bd hi fi z dy jk ea eb jl ed ef hg bi translated">人工神经网络和深度学习</h2><div class="jm l"><h3 class="bd b fi z dy jk ea eb jl ed ef dx translated">自从特别是计算机和一般技术出现以来，创造智能系统的想法…</h3></div><div class="jn l"><p class="bd b fp z dy jk ea eb jl ed ef dx translated">becominghuman.ai</p></div></div><div class="jo l"><div class="jp l jq jr js jo jt ju jf"/></div></div></a></div><p id="9a99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">那么激活功能是什么呢？</strong></p><p id="f0f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活函数是附属于人工神经网络中每个神经元的数学函数。该函数计算输入的加权和，并进一步向其添加偏差，然后确定是否应该激活神经元。更简单地说，在每一层中，只有那些与模型预测相关的信息的神经元才会被激发或激活。</p><p id="c9e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">—激活功能也有助于将每个神经元的输出标准化，以安排在1 &amp; 0之间或-1和+1之间。</p><p id="7221" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">—激活函数的主要目的是<strong class="ig hi">将非线性</strong>引入神经元的输出。</p><p id="ff2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你下面看到的网络是一个人工神经网络，由不同层的相互连接的神经元组成。每个神经元由它的权重、偏置和激活功能来表征。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es jv"><img src="../Images/ae0d1e331e20beec19c2f00da8da8df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*CoNnxJlmETLqk-diAAujrA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Source: <a class="ae kg" href="https://orbograph.com/wp-content/uploads/2019/01/DeepLearn.png" rel="noopener ugc nofollow" target="_blank">https://orbograph.com/wp-content/uploads/2019/01/DeepLearn.png</a></figcaption></figure><p id="fc7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当输入被馈送到输入层时，神经元使用权重和偏差执行线性变换。之后，激活功能被应用于输出。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es kh"><img src="../Images/3e0208710651a2351ff6585aa18c635d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/1*fyRyx3mv_9q9zahDMR2CuQ.gif"/></div></figure><p id="92cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么我们甚至需要一个激活功能？</strong></p><p id="189a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活功能的主要优势如下:</p><ol class=""><li id="01b7" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated">他们在神经网络中引入了非线性。</li></ol><p id="f6e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我说过激活函数的主要优点是将非线性引入网络。但是什么是非线性呢？</p><p id="4aff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">比如说X₂….的X₁Xₙ是神经网络的输入，W₁、W₂,…和Wₙ是与神经元相关联的相应权重。如果没有激活功能，每一层之后的输出只是:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es ks"><img src="../Images/2b89793bb6430155c97012252161e329.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/1*spUoelcw72-Gf6hsLaprkg.gif"/></div></figure><p id="fccf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个方程看起来不像线性回归算法中回归线的方程吗？</p><p id="634b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了更好地理解这一点，让我们像下面这样在3D平面中可视化数据集</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/58207a21d93fcad0112682f57f377501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZdqUaFi7KMwrYYReqg-U_A.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">Source: <a class="ae kg" href="https://www.geeksforgeeks.org/3d-scatter-plotting-in-python-using-matplotlib/" rel="noopener ugc nofollow" target="_blank">GeeksforGeeks</a></figcaption></figure><p id="1439" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用x轴、y轴和z轴上的三个特征的线性分类器可以给我们一条穿过3D平面的线，但是它将永远不能准确地学习将产生准确预测的模式，因为定义该分类的模式仅仅是<strong class="ig hi">非线性的。</strong>现在，当我们建立一个没有激活函数的神经网络时，我们的输出基本上是<strong class="ig hi"> y= wᵢ*xᵢ +偏差。</strong>但这是不好的，因为<strong class="ig hi"> w*x也有1度</strong>，因此是线性的，这基本上等同于线性分类器(线性回归模型)。</p><blockquote class="ky kz la"><p id="5e9a" class="ie if kr ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated">因此，如果没有激活函数，权重和偏差将只有线性转换，或者神经网络只是一个线性回归模型，它很容易解决，但在解决我们在计算机视觉或NLP中经常遇到的复杂问题的能力方面有限。</p></blockquote><p id="cbeb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.非线性激活函数也非常重要，因为它们是可微分的，并且使得反向传播(梯度下降)成为可能。反向传播使误差最小化，并提高模型的准确性或效率。</p><p id="ef8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望我让你相信了激活函数在深度神经网络中的重要性。现在，让我们看看我们有哪些不同类型的激活功能，以及我们可以在哪里应用它们:</p><ol class=""><li id="6f31" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated"><strong class="ig hi">二元阶跃函数</strong></li></ol><p id="7b3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是最简单的激活函数，其中如果输出超过某个阈值，则神经元被激活，或者不被激活。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es le"><img src="../Images/f42f609cd3c4454574cbe2edb6bb3812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a38x-jcymhJJKEP_O1amZg.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">Source: <a class="ae kg" href="https://towardsdatascience.com/activation-functions-in-neural-networks-58115cda9c96" rel="noopener" target="_blank">Deval Shah</a></figcaption></figure><p id="7f75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二进制步长函数在创建二进制分类器时非常有用，但在处理多类分类问题时却显得力不从心。</p><p id="2be1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二进制阶跃函数的另一个缺点是其导数为0，这意味着由于消失梯度问题，反向传播是不可能的。</p><p id="0bb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2。乙状结肠或逻辑函数</strong></p><p id="de4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">sigmoid非线性函数具有如下数学形式:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lf"><img src="../Images/d671b58494aaa0dd6c5d8644731cbe50.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/1*NYHIWMvZse1wfS_kNiqj2w.gif"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">sigmoid or logistic activation function</figcaption></figure><p id="e6fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中“e”是一个数学常数，是自然对数的底数。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lg"><img src="../Images/342652b4c7ca103bb7e5978024d0adc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ts0PhqsoTor4JSZVsZijVg.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">Source: <a class="ae kg" rel="noopener" href="/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e">Mehmet Toprak</a></figcaption></figure><p id="cfa6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Sigmoid函数将任何实数值作为输入，并输出0到1范围内的值。它特别适用于我们必须<strong class="ig hi">预测概率</strong>作为输出的模型。由于任何观察的概率存在于范围<strong class="ig hi"> 0和1之间，</strong>sigmoid函数是正确的选择。</p><p id="84c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">sigmoid函数也成为反向传播的消失梯度问题的牺牲品。如果你注意到，在sigmoid函数的两端，Y值往往对x的变化反应很小，这意味着什么呢？梯度变得非常小或者几乎消失。所以网络已经停止学习超过这一点。</p><p id="a815" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，sigmoid函数在分类问题中仍然很受欢迎。</p><p id="2999" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3。双曲正切或双曲正切激活函数</strong></p><p id="2fc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">双曲正切函数与sigmoid函数非常相似，但更好。唯一的区别是tanh只将负输入映射为负数量，范围在<strong class="ig hi"> -1到1之间。</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lh"><img src="../Images/16ee5301b668b37cbdfd6a1dc847057e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/1*D8cDF7vM7ssw9eWwjnXR7g.gif"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">tanh activation function</figcaption></figure><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es li"><img src="../Images/8c0a3773f3f5c03dc11b1d3f2e41cc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*NK0AjxnpDfULnQIxkuq_VQ.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Source: <a class="ae kg" href="https://www.i2tutorials.com/explain-all-zero-centered-activation-functions/" rel="noopener ugc nofollow" target="_blank">i2tutorials</a></figcaption></figure><p id="298d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">tanh优于sigmoid的一个优点是tanh的梯度比sigmoid更强，并且其导数也更陡。</p><p id="f611" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">双曲正切函数也有渐变消失的问题。</p><p id="3006" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 4。整流线性单元或ReLU激活功能</strong></p><p id="eded" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ReLU的公式看似简单:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lj"><img src="../Images/68f44631de0fa29ff70c947a189e658b.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/1*hfizeREB3gTyMkVyZYX_4A.gif"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">ReLU</figcaption></figure><p id="264e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也就是说，如果x为正，则输出x，其他值为0。ReLU的范围是[0，∞)</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lk"><img src="../Images/9da59753a921fe961048b9dd6649f0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIgTWE1223EGTqmi8lYBlA.png"/></div></div></figure><p id="d242" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与sigmoid和tanh相比，ReLU的主要优势在于它不会同时激活所有神经元。正如我们从上图中看到的，对于负输入值，结果为零，这意味着这些神经单元没有被激活，这反过来使ReLU的计算开销明显低于tanh和sigmoid，因为它涉及更简单的数学运算。</p><p id="cf30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但ReLU的问题是，对于所有负值，梯度变为零，这大大降低了模型根据数据进行适当拟合或训练的能力，因为这些神经元停止进一步响应。这就是所谓的<em class="kr">【垂死挣扎】</em>问题。Leaky ReLU是解决这个问题的另一个激活函数。</p><p id="aa28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 5。泄漏的ReLU </strong></p><p id="c6ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所讨论的，leaky ReLU通过简单地制作水平线(对于所有输出&lt; 0) non-horizontal .</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es ll"><img src="../Images/8cfc0205a53378f6b4e6477d6aa5c1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*KphsdJyD_23HuHLV_-FyVQ.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Leaky ReLU</figcaption></figure><p id="4e88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">For example y = 0.01x for x&lt;0 will make it a slightly inclined line rather than horizontal line. This is leaky ReLu. The main goal here is to let the gradient be non-zero.</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lm"><img src="../Images/1dcafa677d10355fb7f4a110e576a1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nLjmCTPZdF6YiEGWAZiIaQ.jpeg"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">Leaky ReLU activation function. Source: <a class="ae kg" href="https://knowhowspot.com/technology/ai-and-machine-learning/artificial-neural-network-activation-function/" rel="noopener ugc nofollow" target="_blank">https://knowhowspot.com/technology/ai-and-machine-learning/artificial-neural-network-activation-function/</a></figcaption></figure><p id="8304" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 6)来解决死亡ReLU的问题。Softmax功能</strong></p><p id="f330" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们要学习的最后一个函数叫做softmax函数，它被描述为多个sigmoid函数的组合。我们知道signoid函数广泛用于二分类问题。</p><p id="80af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Softmax通过计算每个目标类在所有可能的目标类中的概率，将sigmoid函数扩展到多类分类。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es ln"><img src="../Images/329faf89b76af553b59e31a9606a896b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*BCgPyYe5RUBUPz4732qCxQ.jpeg"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">softmax function</figcaption></figure><p id="131c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您想为多类分类问题构建一个神经网络，那么输出层的神经元数量将与目标层中的类数量一样多。例如，如果你有三个类，在输出层会有三个神经元。假设你得到神经元的输出为[1.2，0.9，0.75]。</p><p id="5713" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对这些值应用softmax函数，您将得到以下结果— [0.42，0.31，0.27]。这些表示数据点属于每个类别的概率。请注意，所有值的总和是1。</p><h2 id="2f28" class="lo lp hh bd lq lr ls lt lu lv lw lx ly ip lz ma mb it mc md me ix mf mg mh mi bi translated">关键要点:</h2><p id="4a85" class="pw-post-body-paragraph ie if hh ig b ih mj ij ik il mk in io ip ml ir is it mm iv iw ix mn iz ja jb ha bi translated">&gt; Sigmoid函数广泛用于二元分类问题</p><p id="d867" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">&gt; Softmax函数理想地用在分类器的输出层，我们实际上试图获得概率来定义每个输入的类别。</p><p id="0bd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">&gt;如果有疑问，请使用ReLU。</p><p id="920a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">确保将ReLU仅应用于隐藏层。</p><p id="81d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">&gt;由于消失梯度问题，有时会避免使用Sigmoid &amp; tanh函数。</p><p id="b506" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">&gt;泄漏ReLU由于其线性，不能用于复杂的分类问题。</p><h2 id="64e7" class="lo lp hh bd lq lr ls lt lu lv lw lx ly ip lz ma mb it mc md me ix mf mg mh mi bi translated">参考资料:</h2><p id="2d03" class="pw-post-body-paragraph ie if hh ig b ih mj ij ik il mk in io ip ml ir is it mm iv iw ix mn iz ja jb ha bi translated">[1] <a class="ae kg" href="https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/" rel="noopener ugc nofollow" target="_blank">通过分析激活功能Vidya </a></p><p id="08b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] <a class="ae kg" href="https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253" rel="noopener" target="_blank">关于深度学习模型中的“激活函数”你需要知道的一切</a></p><p id="2498" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae kg" rel="noopener" href="/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">【万物理论】激活函数</a></p></div><div class="ab cl mo mp go mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ha hb hc hd he"><p id="76a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<a class="ae kg" href="https://www.linkedin.com/in/sindhuseelam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae kg" href="https://twitter.com/SindhuSeelam_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上与我联系，获取更多关于机器学习的教程和文章</p></div></div>    
</body>
</html>