<html>
<head>
<title>Convolutional neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/convolutional-neural-networks-3f00c165c9d9?source=collection_archive---------4-----------------------#2021-03-08">https://medium.com/mlearning-ai/convolutional-neural-networks-3f00c165c9d9?source=collection_archive---------4-----------------------#2021-03-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c6ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积神经网络(CNN)是一种深度神经网络，通常用于处理结构化数据阵列，如图像。CNN广泛应用于计算机视觉。它们在图像和视频识别、图像分类、自然语言处理等方面有着广泛的应用。</p><p id="6d6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积神经网络包括输入层、隐藏层和输出层。</p><ul class=""><li id="79fe" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi"> <em class="jl">输入</em> </strong>为形状张量<em class="jl">(图像数)×(图像高)×(图像宽)×(通道数)</em></li><li id="ddf7" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><strong class="ig hi"> <em class="jl">隐藏层</em> </strong>位于输入层和输出层之间。它们包括<em class="jl">卷积层</em>，其后是其他层，如<em class="jl">汇集层、归一化层、</em>和<em class="jl">全连接层。</em></li><li id="6146" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><strong class="ig hi"> <em class="jl">输出层</em> </strong>是产生程序最终结果的网络的最后一层。</li></ul><p id="d223" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将发现一个CNN模型的基本结构，以及它在识别MNIST手写数字图像问题上的应用。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="069a" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">一.卷积层</h1><p id="41a6" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">在这一层中，输入通过互相关运算与核相乘，然后结果与定标器偏置相加以产生输出。这一层的参数包括<em class="jl">内核</em>和<em class="jl">标量偏移</em>。</p><h2 id="57c8" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated"><strong class="ak"> 1。互相关运算。</strong></h2><p id="d437" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">让我们在一个简单的例子中看看这个操作是如何工作的，其中我们有一个输入，它是一个大小为4× 4的二维矩阵(即大小为4 × 4× 1的张量)。内核窗口是大小为3× 3的正方形矩阵:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es lp"><img src="../Images/33ce548435d911c5dc028a92724e2996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*0MJsC8hlvohG5edvJs75Rw.png"/></div></figure><p id="3d7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积窗口从输入张量的顶部开始，它从左到右和从输入的顶部到底部滑动。在每个位置，该窗口内的输入的子张量与核逐元素相乘，然后对结果求和以获得该位置的输出元素。</p><p id="d138" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">0×0 + 1×2 + 2×1 + 3×1 + 0×3 + (-1)×2 + (-2)×2 + 0×0 + 1×(-1) = 0</p><p id="53b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">1×0 + 2×2 + 4×1 + 0×1 + (-1)×3 + 5×2 + 0×2 + 1×0 + 3×(-1) = 12</p><p id="f9b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">3×0 + 0×2 + (-1)×1 + (-2)×1 + 0×3 + 1×2 + 1×2 + 0×0 + 9×(-1) = -8</p><p id="8a19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">0×0 + 1×2 + (-5)×1 + 0×1 + 1×3 + 3×2 + 0×2 + 9×0 + 0×(-1) = 12.</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es lx"><img src="../Images/1e1e37b2cb5b18679443ec8f1c479bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/1*cFMF_uWgUFdVRMZAZ0Bfzg.gif"/></div><figcaption class="ly lz et er es ma mb bd b be z dx"><a class="ae mc" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="bc85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设输入是大小为n₁ × n₂的矩阵，内核大小为k₁ × k₂，则输出大小确定为:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es md"><img src="../Images/4899b8d5d573b76699b7b4ba45011a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/0*VIvxuh6u7zULOlOr"/></div></figure><h2 id="1456" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">2.填料</h2><p id="0c52" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">由于内核大小通常大于1，因此输出小于输入。这意味着当内核大时，我们的输入图像会丢失很多像素。为了克服这个缺点，我们可以在输入图像的边界周围添加更多的像素。通常，我们将额外像素的值设置为0。如果我们添加额外像素的p₁行和p₂列，则输出的新大小由下式给出:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es me"><img src="../Images/627f8e7c9e3825ddff125697780ceb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*mZqBCbOJOEFSMh0g"/></div></figure><p id="0821" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，当p₁= k₁–1.时，输入和输出具有相同的大小</p><p id="2dd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们回到上面的例子，如果我们将0像素的2行(p₁ = 2)和2列(p₂ = 2)添加到输入中，那么在这种情况下，输出大小为4 × 4，等于原始输入大小。</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es mf"><img src="../Images/e28e4572e9991d5221e86dc952869685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*tWTc1ANR9q8fxYxIG7rTcw.png"/></div></figure><h2 id="b52c" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">3.进展</h2><p id="c1a9" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">在上面的例子中，我们在输入张量上将卷积窗口扩大了1个像素。有时，有必要将输出压缩成较小的尺寸。因此，我们需要增加步幅。在我们的示例中，如果我们将填充图像的所有垂直和水平方向移动3个像素，则输出大小会减少到2 × 2:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es mg"><img src="../Images/19fef7fcfe6cf1abac9a5512bf0a1d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*-dsk81I9LuvG8LRb3Sh5mw.png"/></div></figure><p id="75c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们根据输入中的垂直和水平来表示s₁和s₂的步幅，然后输出大小由下式确定:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es mh"><img src="../Images/ab7f2d6bbf86a60470af71c24764773e.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/1*oXWuLip00YkYTf33KVaWsw.gif"/></div></figure><p id="b330" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">综上所述，padding和stride是两种功能不同的操作。填充操作在输入中添加额外的0像素，以避免在输出中丢失信息，而步长操作旨在压缩输出。这两种操作可以结合起来，以获得包括关键像素的所需大小的输出。下图说明了不同填充和步长组合的卷积层的输出。这张图片收集自<a class="ae mc" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank"> Github账户</a>:</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mi"><img src="../Images/b73cc47a4a5fc431ccc01f50726ff871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MzVn9M7T8Z0sNPMbLUxsvg.gif"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx"><a class="ae mc" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="1998" class="jy jz hh bd ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr mr kt ku kv bi translated">二。池层</h1><p id="c4b3" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">池层是卷积神经网络的构造块。它的功能是在保留最重要信息的同时降低卷积层的输出维数。</p><p id="bb48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像卷积层一样，汇集层由一个固定形状的窗口组成(也称为<strong class="ig hi"> <em class="jl">汇集窗口</em> </strong>)。该窗口从左到右、从上到下跨越输入的所有区域。在每个位置，它计算汇集窗口内所有输入元素的最大值或平均值。这些操作符称为<em class="jl">最大池(max pooling) </em>和<em class="jl">平均池。</em>但与卷积层不同的是，池层不包含任何参数(既没有内核，也没有偏向)。</p><p id="0a51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们通过以下示例来了解<em class="jl">最大池</em>和<em class="jl">平均池</em>的工作原理:</p><ul class=""><li id="0292" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">如果我们对输入图像应用大小为3 × 3的最大池窗口，则输出大小将为2 × 2，值如下图所示:</li></ul><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es ms"><img src="../Images/55034a5f6b347c433f8e17bb946b5bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Ofa1u5bIdPN0o1UeAgebLQ.png"/></div></figure><ul class=""><li id="28d2" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">当大小为2 × 2的平均池窗口应用于具有2个步长的输入时，输出大小也是2 × 2:</li></ul><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es mt"><img src="../Images/2597b4dd7384c14ec94b7ffdd3387cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*w2VUZr0O4B9rvbwMQHRxbg.png"/></div></figure><h1 id="66a1" class="jy jz hh bd ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr mr kt ku kv bi translated">三。全连接层</h1><p id="dc69" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">全连接层是卷积神经网络的最后一级。网络中可能有一层或多层这样的层。当前层的所有神经元都与下一层的每个神经元相连。这就是它们被称为全连接层的原因。</p><p id="aa5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个全连接层获取前一层(池层的卷积层)的输出，并将它们展平为单个向量。在输出层中，节点的数量对应于类(或标签)的数量。这一层给出了每一类的概率。例如，在对MNIST数据进行分类的问题中，我们有10个标签，它们是从0到9的数字。因此，输出节点的数量是10。如果输入是数字2的图像，那么类2给出的概率应该是最大的。</p><figure class="lq lr ls lt fd lu er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mu"><img src="../Images/cfba1032191beec19e6eb288b3c651df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZR7V-aDKE_xo7yoWXcz7g.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx">[Source: Toward Data Science]</figcaption></figure><h1 id="96b7" class="jy jz hh bd ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr mr kt ku kv bi translated">四。例子</h1><p id="165a" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">在本节中，我们将在Keras中构建一个简单的卷积神经网络来对MNIST数据进行分类。</p><h2 id="d993" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">1.手写数字的MNIST数据集</h2><p id="88a2" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">该数据集由训练集中的60，000个样本和测试集中的10，000个样本组成。这些数字已经过大小标准化，并以28 × 28的固定大小居中。</p><p id="4d1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该数据库可在<a class="ae mc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">页面</a>上获得。也可以从Keras的库<strong class="ig hi">数据集</strong>中加载:</p><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><pre class="lq lr ls lt fd mx my mz na aw nb bi"><span id="89b3" class="lb jz hh my b fi nc nd l ne nf"> X_train shape (60000, 28, 28) X_test shape (10000, 28, 28)</span></pre><p id="f408" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机可视化训练集中的一些图像:</p><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="ab fe cl ng"><img src="../Images/ab345b77a5810cac5e0226b98d942bf0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*n1zI5gPeLIapbNBprBneGg.png"/></div></figure><h2 id="3dd0" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">2.预处理数据</h2><p id="6c73" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">该任务包括以下步骤:</p><ul class=""><li id="5145" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">将图像重塑为所需的Keras大小</li><li id="6426" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated">将整数值转换为浮点值</li><li id="22e3" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated">标准化数据</li><li id="67c1" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated">一键编码标签</li></ul><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="ec5d" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">3.建立一个CNN模型；</h2><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="be8c" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">4.培训模式</h2><p id="f2a2" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">定义培训功能</p><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="1703" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总结培训历史:</p><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="f2db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">描述模型的架构以及每层中的参数数量:</p><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><pre class="lq lr ls lt fd mx my mz na aw nb bi"><span id="ca2c" class="lb jz hh my b fi nc nd l ne nf">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d_4 (Conv2D)            (None, 28, 28, 32)        832       <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)            (None, 28, 28, 32)        25632     <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         <br/>_________________________________________________________________<br/>conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     <br/>_________________________________________________________________<br/>conv2d_7 (Conv2D)            (None, 14, 14, 64)        36928     <br/>_________________________________________________________________<br/>max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 3136)              0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 128)               401536    <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 10)                1290      <br/>=================================================================<br/>Total params: 484,714<br/>Trainable params: 484,714<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="d964" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练模型(在训练集上拟合模型，并使用X_test作为验证集</p><pre class="lq lr ls lt fd mx my mz na aw nb bi"><span id="bbc4" class="lb jz hh my b fi nc nd l ne nf"><strong class="my hi">train_model(model,X_train, y_train, X_test, y_test)</strong></span><span id="1f91" class="lb jz hh my b fi nh nd l ne nf">Epoch 1/50<br/>468/468 [==============================] - 5s 10ms/step - loss: 1.6875 - accuracy: 0.4765 - val_loss: 34.9615 - val_accuracy: 0.9225<br/>Epoch 2/50<br/>468/468 [==============================] - 4s 9ms/step - loss: 0.2491 - accuracy: 0.9239 - val_loss: 18.6345 - val_accuracy: 0.9582<br/>............<br/>............<br/>............<br/>Epoch 49/50<br/>468/468 [==============================] - 4s 9ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 7.9952 - val_accuracy: 0.9897<br/>Epoch 50/50<br/>468/468 [==============================] - 4s 9ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 8.4591 - val_accuracy: 0.9888<br/>313/313 [==============================] - 1s 2ms/step - loss: 8.4456 - accuracy: 0.9888</span></pre><figure class="lq lr ls lt fd lu er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es ni"><img src="../Images/49c7d5c1aa7b69fc34a00aa0748945bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkkrpNjaHjLVqnBITf11IQ.png"/></div></div></figure><h2 id="8563" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">5.做预测</h2><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="c63c" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">6.确定混淆矩阵</h2><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="ab fe cl ng"><img src="../Images/085a7824ebd00f51eb20209b358d5a90.png" data-original-src="https://miro.medium.com/v2/format:webp/1*9rA_0VIdjGcl4-OOofdn2A.png"/></div></figure><h2 id="c20b" class="lb jz hh bd ka lc ld le ke lf lg lh ki ip li lj km it lk ll kq ix lm ln ku lo bi translated">7.随机可视化一些图像及其预测标签</h2><figure class="lq lr ls lt fd lu"><div class="bz dy l di"><div class="mv mw l"/></div></figure><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="ab fe cl ng"><img src="../Images/774c6294e873a5b56f6237499619dbd8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-n6WpoT1wRg_kcyIigAN0Q.png"/></div></figure><h1 id="6a30" class="jy jz hh bd ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr mr kt ku kv bi translated"><strong class="ak">五、结论</strong></h1><p id="db87" class="pw-post-body-paragraph ie if hh ig b ih kw ij ik il kx in io ip ky ir is it kz iv iw ix la iz ja jb ha bi translated">我们已经发现了卷积神经网络的基本架构，包括卷积层、池层和全连接层。CNN在计算机视觉方面有很多应用。在这篇文章中，我们建立了一个简单的CNN模型来识别手写数字图像。结果，我们在训练集和验证集上都获得了98.88%的性能。虽然这个结果已经足够好了，但是你也可以试着改变一些模型的超参数，比如内核大小，过滤器的数量，填充，或者激活函数等等，看看能不能找到一个更好的结果。:-)</p><p id="7323" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望这篇文章对你有帮助。不要犹豫，在medium 上找到我<a class="ae mc" href="https://lekhuyen.medium.com/" rel="noopener">，在接下来的博客中发现类似的内容。</a></p><p id="6b2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>