<html>
<head>
<title>Everything You Need To Know About Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于强化学习你需要知道的一切</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/everything-you-need-to-know-about-reinforcement-learning-c7c2d333ed7a?source=collection_archive---------0-----------------------#2021-06-07">https://medium.com/mlearning-ai/everything-you-need-to-know-about-reinforcement-learning-c7c2d333ed7a?source=collection_archive---------0-----------------------#2021-06-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5e8ea7dcb96db031b4e581bb7ff2d8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BnrBxVVM2D7oBex1"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@patkrupa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Pat Krupa</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0bbe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">强化学习是教育机器学习模型做出一系列判断的过程。在一个不确定的，可能是复杂的环境中，代理人学习达到一个目标。在强化学习中，人工智能被置于类似游戏的环境中。计算机通过反复试验找到了解决问题的方法。为了说服计算机去做程序员所期望的事情，人工智能会为它所做的行为得到补偿。它的目标是最大化整体报酬。虽然开发人员制定了奖励政策，也就是游戏规则，但他没有向模型提供任何关于如何完成游戏的提示或建议。由模型来发现如何完成工作以最大化回报，从完全随机的试验开始，发展到复杂的策略和超人的能力。</p><h1 id="a90c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习科学</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/59e6713675237e26f409b4dbb0a47ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPF16t2St9uWT-fpNv37ew.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Standard scenario of reinforcement learning</figcaption></figure><ul class=""><li id="f307" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated"><strong class="iw hi">代理</strong>——它是一个虚构的实体，为了获得报酬而在某个环境中行动。</li><li id="8e84" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><strong class="iw hi">环境(e) </strong> —代理必须处理的情况。</li><li id="f70c" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><strong class="iw hi">奖励(R) </strong> —为完成特定行动或活动的代理人提供的即时奖励。</li><li id="6f3b" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><strong class="iw hi">状态</strong> — <strong class="iw hi"> </strong>术语“状态”是指环境所指示的当前情况。</li></ul><h1 id="eb89" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的过程是怎样的？</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/2dd68f5e538100055c8c61a0330d7266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eN0KsKCMi64bPzpxzu0lWQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Reinforcement learning example</figcaption></figure><p id="7f60" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看一些简单的例子来帮助你理解强化学习机制。考虑训练一只狗新把戏的场景。</p><ul class=""><li id="e8cb" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">我们不能指导狗做什么，因为它不理解英语或任何其他人类语言。相反，我们使用不同的方法。</li><li id="ee0a" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">我们模拟一种情况，狗试图以各种方式做出反应。如果狗以期望的方式回应，我们会奖励它一份礼物。</li><li id="ac2a" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">当狗再次暴露在同样的环境中时，它会更加兴奋地做类似的活动，希望得到额外的奖励(食物)。</li><li id="a25d" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">这类似于狗如何通过有利的经历来学习“做什么”。</li><li id="6b5f" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">同时，狗知道当面对不利事件时不应该做什么。</li></ul><h1 id="d6ea" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习算法</h1><p id="b1aa" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">强化学习算法可以通过三种方式实现。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/f00551b61ce611431d50ed13833c0bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*BygULKy6O4QpMPejuvJ_Mw.png"/></div></figure><h2 id="f5ab" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">基于模型的方法</h2><p id="5f5b" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">一种基本方法:如果我们不知道MDP，我们可以从数据中估计它。代理在环境中行为(根据一些策略)并观察状态、动作和奖励顺序。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/5b8d8b6c38805a2331e58b0b601a2248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmAraS_cYpqwhVR0m8uFow.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">reward sequence</figcaption></figure><p id="0b02" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用计数，创建MDP的经验估计值。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/6e6ba40e6df15a347decdf74cb016502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTiOnY1W2In3va6Jg5aTZg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">empirical estimate</figcaption></figure><p id="8a9e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后通过例如值迭代求解MDP M̂ = (𝒮,𝒜,𝑃̂,𝑅̂,𝛾)</p><h2 id="e276" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">基于价值的方法</h2><p id="f06f" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">你应该努力在基于价值的强化学习方法中优化价值函数V(s)。在这个策略中，代理人预期在策略π下的当前状态的长期返回。</p><h2 id="35f3" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">基于策略的方法</h2><p id="5880" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">在基于策略的RL方法中，你努力设计一个策略，在每个状态下采取的每个行动都有助于你在未来获得最大的回报。</p><p id="9d75" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有两种基于策略的方法:</p><ul class=""><li id="26a2" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">确定性的</li></ul><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/9cbcc0ed21052162753ca8b535e5b675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UScTUD6--YAiy-xud8JJQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Deterministic</figcaption></figure><ul class=""><li id="d854" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">随机的</li></ul><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/3c33aa223751a79913aa38943acc7d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6M7fNslYB_PnM6KX6mhf4A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Stochastic</figcaption></figure><h1 id="a3cd" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习种类</h1><p id="de77" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">正强化和负强化是强化学习的两种类型。</p><h2 id="f3a0" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">正面强化</h2><p id="8820" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">当预测的行为模式被证明时，鼓励或添加一些东西以提高相同行为被重复的机会的技术被称为积极强化学习。</p><p id="0a67" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，如果一个孩子在考试中表现很好，他们可以得到一个冰淇淋作为奖励。</p><h2 id="9239" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">负强化</h2><p id="c676" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">负面强化需要通过消除负面环境来提高特定行为再次发生的可能性。</p><p id="97d6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，如果一个孩子考试不及格，他或她可能会因为不能玩电子游戏而受到负面强化。这并不完全是因为考试不及格而惩罚孩子，而是消除可能导致孩子考试不及格的负面情况(在这个例子中，是电子游戏)。</p><h1 id="06a8" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习模型</h1><p id="1ed7" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">在强化学习中，有两种主要的学习模型。</p><h2 id="ad44" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">马尔可夫决策过程</h2><p id="1e51" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">马尔可夫决策过程是一个元组</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/6ecb09280903b19bc1bbe431bafcde7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*x4iYWhQ1WfsA9fsC4zQ64w.png"/></div></figure><p id="fe25" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">包括:</p><ul class=""><li id="14bc" class="kv kw hh iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">s是状态的集合。(例如，在自主直升机飞行中，S可以是所有潜在直升机位置和方向的集合。)</li><li id="b52a" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">是动作的集合。(例如，直升机操纵杆可能被推动的所有可能方向的集合。)</li><li id="4b7c" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">状态转移的概率用Psa表示。Psa是跨越每个状态s ε S和动作a ε A的状态空间的分布。Psa表示如果我们在状态a中执行动作a，我们将迁移到的状态的分布。</li><li id="570c" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">γε[0，1]称为discout因子。</li><li id="5e39" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">r:s×a→ℝ是奖励函数。(奖励有时也被写成仅是状态a的函数，在这种情况下，我们会得到R : S → ℝ)</li></ul><p id="e2f8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">用于映射强化学习中的解决方案的数学方法被重构为马尔可夫决策过程或MDP。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/ae6c36cc49f034b6352d9db7c43807bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*dpKw8IuqQBG-EfbCUUKpBg.png"/></div></figure><h2 id="1826" class="lq jt hh bd ju lr ls lt jy lu lv lw kc jf lx ly kg jj lz ma kk jn mb mc ko md bi translated">q学习</h2><p id="6cdb" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">Q-learning是一种用于时间差分学习的非策略RL算法。时间差异学习方法比较时间上连续的预测。它学习值函数Q (S，a)，该函数描述了在给定状态“S”下执行动作“a”有多好</p><p id="a843" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">贝尔曼方程可以用来计算Q学习的值。考虑下面的贝尔曼方程:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/d17c072feea49e1dfbf9ff85e2861e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*SOMd4XQfFmcDu217_s9LCQ.png"/></div></figure><p id="2b80" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">等式中有几个部分，包括奖励、折扣因子(Y)、概率和最终状态s’。但是，没有提供Q值，所以检查下图:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ml"><img src="../Images/a8fd964b6ae6be36281d9a90c2679d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82_1hBhnQ2dRvQ6E75r_Xw.png"/></div></div></figure><p id="e926" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上图中，我们可以看到一个代理有三个值选项:V(s1)、V(s2)和V(s3)。因为这里是MDP，代理只关心当前和未来的状态。因为代理可以向任何方向行走(向上、向左或向右)，所以他必须选择最佳路径。在这种情况下，代理将根据概率进行移动并修改状态。然而，如果我们想要任何精确的运动，我们必须在Q值方面做一些改变。考虑下面的图像:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ml"><img src="../Images/16d8a02af9bbff655003dc834c4cb460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BPzRWXDYk-fwD8f8Gqw63g.jpeg"/></div></div></figure><p id="69b2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">代表每个州的表演质量水平。因此，我们将使用一对状态和动作，即Q. (s，a)，而不是在每个状态使用一个值。Q值描述了哪些动作比其他动作更具润滑性，代理人根据最佳Q值做出下一步行动。可以使用贝尔曼方程来计算Q值。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/117ecf5a4034c679e792b8a215960c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*thEgpjMx4U0Js-kGwFAR-Q.png"/></div></figure><p id="2b9a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，我们可以说，<strong class="iw hi"><em class="mn">V(s)= max【Q(s，a)】</em></strong></p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/56365fe47338576cbd7b75dce44a2e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*aeGKKNP3zkcxcJW1U2dQGw.png"/></div></figure><p id="010a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">下面的流程图展示了Q-learning是如何工作的:</strong></p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es mp"><img src="../Images/84159f009d6252c79710e3a234809823.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*jm2D-IKRmHUaGqGZvIb6HA.png"/></div></figure><h1 id="b98d" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习和监督学习有什么区别？</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/c602ce26ede0c076e354673fdf07937e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5BzZ-mBZ8gfitgluhvKrDQ.png"/></div></div></figure><h1 id="6252" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的应用</h1><ul class=""><li id="c3dd" class="kv kw hh iw b ix lk jb ll jf mr jj ms jn mt jr la lb lc ld bi translated"><a class="ae it" href="https://arxiv.org/pdf/2002.00444.pdf" rel="noopener ugc nofollow" target="_blank">自动驾驶汽车</a></li><li id="1f7f" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae it" href="https://arxiv.org/ftp/arxiv/papers/1803/1803.03916.pdf" rel="noopener ugc nofollow" target="_blank">交易和金融</a></li><li id="165d" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae it" href="https://www.researchgate.net/publication/318740604_Coarse-to-Fine_Question_Answering_for_Long_Documents" rel="noopener ugc nofollow" target="_blank">自然语言处理</a></li><li id="c73d" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">医疗保健(<a class="ae it" href="https://arxiv.org/pdf/1908.08796.pdf" rel="noopener ugc nofollow" target="_blank">动态治疗方案(DTRs </a>)</li><li id="3e60" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae it" href="https://engineering.fb.com/2018/11/01/ml-applications/horizon/" rel="noopener ugc nofollow" target="_blank">工程</a></li><li id="d57a" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae it" href="http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf" rel="noopener ugc nofollow" target="_blank">新闻推荐</a></li><li id="85b4" class="kv kw hh iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae it" href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank">机器人操纵</a></li></ul><h1 id="4ee7" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="b690" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">强化学习解决了具有很少或没有数据的自主代理学习控制方法的困难。因为收集和标记大量样本模式的成本高于数据本身，所以RL算法在机器学习中很有用。RL总是在学习，所以它在手头的任务上成长得越来越好。在监督学习下，学习一个国际象棋游戏可能是一项耗时的工作，但RL在同样的工作上工作很快。在这种情况下，以最大化长期回报为目的尝试一项任务的试错策略可以产生更好的结果。马尔可夫决策过程的动态规划技术与强化学习(MDP)密切相关。</p></div><div class="ab cl mu mv go mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ha hb hc hd he"><div class="kr ks kt ku fd nb"><a href="https://www.linkedin.com/in/rkavishwara/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hi fi z dy ng ea eb nh ed ef hg bi translated">ravindu kavishwara - CTF球员-黑盒子| LinkedIn</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">我想象有一天我们生活在一个一切都自动化的社会。人工智能处理一切。我…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">www.linkedin.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np in nb"/></div></div></a></div></div></div>    
</body>
</html>