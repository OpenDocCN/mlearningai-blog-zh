<html>
<head>
<title>PyTorch implementation of Geoffrey Hinton’s Forward-Forward algorithm and analysis of performance VS backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Geoffrey Hinton前向-前向算法的PyTorch实现及性能与反向传播的分析</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/pytorch-implementation-of-forward-forward-algorithm-by-geoffrey-hinton-and-analysis-of-performance-7e4f1a26d70f?source=collection_archive---------0-----------------------#2022-12-20">https://medium.com/mlearning-ai/pytorch-implementation-of-forward-forward-algorithm-by-geoffrey-hinton-and-analysis-of-performance-7e4f1a26d70f?source=collection_archive---------0-----------------------#2022-12-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6511ea59b0562e10ade2e75c07b8ce61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwX3pdEu-4OM3XX97z7k-A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by Author</figcaption></figure><p id="7598" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你是一个渴望测试Hinton的<a class="ae jr" href="https://www.cs.toronto.edu/~hinton/FFA13.pdf" rel="noopener ugc nofollow" target="_blank">向前向前算法</a>的人工智能研究人员吗？我也是，但是找不到任何完整的实现，所以我决定自己从头开始编码。好消息——您可以在这里访问GitHub上完整的PyTorch实现。如果你喜欢⭐项目，别忘了留下一颗星</p><p id="dba4" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><a class="ae jr" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward" rel="noopener ugc nofollow" target="_blank">https://github . com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward _ forward</a></p><p id="96fd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我一读这篇论文，就开始想知道AI如何受益于Hinton的FF算法(FF = Forward-Forward)。我对以下概念特别感兴趣:</p><ul class=""><li id="2089" class="js jt hh iv b iw ix ja jb je ju ji jv jm jw jq jx jy jz ka bi translated">本地培训。每一层都可以通过比较正流和负流的输出来训练。</li><li id="2fa3" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated">无需存储激活。在反向传播过程中需要激活来计算梯度，但通常会导致严重的内存不足错误。</li><li id="a3fb" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated">更快的权重层更新。一旦计算出一个层的输出，就可以立即更新权重，即不需要等待完整的向前(和部分向后)过程完成。</li><li id="5ca3" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated">可选的良好度量标准。Hinton的论文使用输出的平方和作为质量度量，但我预计在未来几个月内，科学文献中会出现替代的度量。</li></ul><p id="6c7b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Hinton的论文提出了两种不同的前向-前向算法，我称之为基本算法和递归算法。让我们检查这些算法的性能的3个关键方面，并与它们的替代方案反向传播进行比较。</p><figure class="kh ki kj kk fd ii er es paragraph-image"><div class="er es kg"><img src="../Images/6703c41f2b81f7df3f2877e45a864fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*ezIZvfjpJU5lqvjf3WJXVA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Performance comparison of backprop, base, and recurrent Forward-Forward. Author’s image</figcaption></figure><h1 id="aec8" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">1 —基本FF增加内存使用</h1><p id="95d8" class="pw-post-body-paragraph it iu hh iv b iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ln jo jp jq ha bi translated">第一个有趣的发现是，前向-前向算法的内存使用仍然会随着层数的增加而增加，但相对于反向传播算法而言会明显减少。这是因为前向-前向算法的内存使用量的增加只与网络参数的数量有关:每层包含2000×2000个参数，当使用Adam优化器对其进行训练时，大约占用64 MB。n_layers=2和n_layers=47之间的总内存使用差异约为2.8 GB，相当于64MB * 45层。</p><h1 id="ffc3" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2 —对于瘦型号，Base FF的内存使用情况比backprop差</h1><p id="729c" class="pw-post-body-paragraph it iu hh iv b iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ln jo jp jq ha bi translated">从图中我们看到，对于少数层，前向-前向算法比后向算法占用更多的内存(大约2GB对400MB)。这可以通过考虑前向-前向算法的结构得到部分解释。对于FF，我们需要按照可能的类的数量(在MNIST是10)复制每个输入，这意味着有效的批量大小变成了原来的10倍。现在让我们来做一下数学计算:在评估网络时，我们以一个独特的批次(10，000张图像)为模型提供整个验证集。考虑一个2000的隐藏维度，每个隐藏状态占用的内存是80 MB(我们以32位精度运行模型)。这意味着有效的批量大小变为100，000个图像，推断过程中占用的内存约为800MB。这种快速计算已经表明，与瘦模型的backprop相比，FF使用了更高的内存，但它没有产生测试期间获得的2 GB以上的结果。需要进一步调查来解释FF的确切内存使用情况。</p><h1 id="f75e" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3 —递归FF没有很大的内存使用优势</h1><p id="805c" class="pw-post-body-paragraph it iu hh iv b iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ln jo jp jq ha bi translated">与基本FF不同，递归FF与深度网络(15层以上)的backprop相比没有明显的内存优势。这是故意的，因为递归网络必须在时间t保存每个中间步骤，以便在时间t+1计算随后的和先前的层输出。虽然在科学上是相关的，但循环FF在内存方面的性能明显不如基本FF。</p><h1 id="e926" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">下一步是什么？</h1><p id="d99c" class="pw-post-body-paragraph it iu hh iv b iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ln jo jp jq ha bi translated">前向-前向算法实际上可以进一步优化，因为它不需要在训练时加载整个网络。事实上，前向-前向算法可用于单独训练网络的每一层，这意味着该算法的内存使用将只与被训练层的参数数量有关。</p><h1 id="35de" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">参考</h1><ul class=""><li id="40df" class="js jt hh iv b iw lj ja lk je lo ji lp jm lq jq jx jy jz ka bi translated"><a class="ae jr" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward" rel="noopener ugc nofollow" target="_blank"> PyTorch在GitHub上的实现</a></li><li id="f5da" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.cs.toronto.edu/~hinton/FFA13.pdf" rel="noopener ugc nofollow" target="_blank">前向-前向算法:一些初步研究</a></li></ul><h1 id="8ce1" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">推荐读物</h1><ul class=""><li id="4739" class="js jt hh iv b iw lj ja lk je lo ji lp jm lq jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/chatllama-0-0-2-release-notes" rel="noopener ugc nofollow" target="_blank">认识ChatLLaMA:用有限的计算资源构建一个类似ChatGPT的助手</a></li><li id="fd64" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps" rel="noopener ugc nofollow" target="_blank">如何利用NVIDIA多进程服务(MPS)提高Kubernetes中的GPU利用率</a></li><li id="96cd" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/reinforcement-learning-from-human-feedback-rlhf-a-simplified-explanation" rel="noopener ugc nofollow" target="_blank">人类反馈强化学习(RLHF)——一个简化的解释</a></li><li id="9a02" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/metas-llama-a-small-language-model-beating-giants" rel="noopener ugc nofollow" target="_blank">梅塔的美洲驼:打败巨人的小语种模型</a></li><li id="4fdc" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/geoffrey-hinton-forward-forward" rel="noopener ugc nofollow" target="_blank"> PyTorch实现Geoffrey Hinton的前向-前向算法</a></li><li id="b0c5" class="js jt hh iv b iw kb ja kc je kd ji ke jm kf jq jx jy jz ka bi translated"><a class="ae jr" href="https://www.nebuly.com/blog/top-10-newsletters-on-machine-learning-and-ai-in-2023" rel="noopener ugc nofollow" target="_blank">2023年关于机器学习和人工智能的10大最佳时事通讯</a></li></ul><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hi fi z dy lz ea eb ma ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi in lu"/></div></div></a></div></div></div>    
</body>
</html>