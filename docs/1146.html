<html>
<head>
<title>The Higgs Boson Machine Learning Challenge in the CERN Large Hadron Collider</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">欧洲粒子物理研究所大型强子对撞机中的希格斯玻色子机器学习挑战</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-higgs-boson-machine-learning-challenge-in-the-cern-large-hadron-collider-8520b709686d?source=collection_archive---------3-----------------------#2021-10-11">https://medium.com/mlearning-ai/the-higgs-boson-machine-learning-challenge-in-the-cern-large-hadron-collider-8520b709686d?source=collection_archive---------3-----------------------#2021-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/7ade0db92f9be8739c46930838c2903a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjeyz84C_eJ68_Rgq1mNbA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The Large Hadron Collider</figcaption></figure><h1 id="87bf" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">1.介绍</h1><p id="d610" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">欧洲核子研究组织，被称为T2 CERN，是一个欧洲研究组织，运营着世界上最大的粒子物理实验室。CERN也是万维网(WWW)的发源地。LHC实验每年产生约90千兆字节的数据，另外25千兆字节的数据来自欧洲粒子物理研究所的其他(非LHC)实验。有20多个实验——国际合作。</p><p id="4b9d" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">你可能已经知道，欧洲粒子物理研究所是著名的<strong class="jt hi">希格斯玻色子</strong>被发现的地方。它是一个粒子，解释了为什么物体有质量，没有它，物体就没有质量。正如我们所知，这意味着没有恒星，没有星系，甚至可能没有生命。</p><h2 id="c352" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">1.1问题</h2><p id="efd3" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">这个问题的简单定义是，我们试图将事件分为两类:</p><ul class=""><li id="5a76" class="li lj hh jt b ju kp jy kq kc lk kg ll kk lm ko ln lo lp lq bi translated">(1)<strong class="jt hi">信号</strong>事件和</li><li id="209c" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">(2) <strong class="jt hi">背景</strong>事件。</li></ul><p id="4a01" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">这里的“信号”事件是“希格斯玻色子的ττ衰变”,而“背景”事件是每一个其他希格斯玻色子发生的非ττ衰变。它本质上是一个二元分类挑战，就像分类肿瘤是良性还是恶性的机器学习挑战一样。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/d4b0e5c1d27be5c0b03cfb237d922697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5YYEn4sxtShGAA-KW21aCQ.jpeg"/></div></div></figure><p id="a8b2" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">正如我们所看到的，希格斯玻色子衰变为两个τ轻子(τ-τ)只是希格斯玻色子实际衰变的6.3%。实际上，希格斯玻色子在几乎60%的时间里衰变为两个底夸克(<em class="mb"> bb </em>)。然而，数据集<strong class="jt hi">而不是</strong>仅包含6%的信号事件和94%的背景事件。背景事件少了很多。三分之一的事件被标记为信号，剩余的三分之二的事件被标记为背景。</p><h2 id="8495" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">1.2数据集</h2><p id="cee8" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">数据集是从<a class="ae mc" href="http://opendata.cern.ch/" rel="noopener ugc nofollow" target="_blank"> CERN开放数据门户</a>获得的。在<a class="ae mc" href="https://github.com/Brighton94/higgs-boson-machine-learning-project" rel="noopener ugc nofollow" target="_blank">笔记本</a>中，我列出了每个特性的含义。</p><p id="0b68" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">但是，这里有一些开始使用的细节:</p><ul class=""><li id="d124" class="li lj hh jt b ju kp jy kq kc lk kg ll kk lm ko ln lo lp lq bi translated">所有变量都是浮点数，除了PRI_jet_num是整数</li><li id="611c" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">以PRI为前缀的变量(用于基元)是由探测器测量的关于束团碰撞的“原始”量。</li><li id="e955" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">以DER为前缀的变量(衍生的)是从原始特征计算的量，这些特征是由ATLAS的物理学家选择的</li><li id="7f4a" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">可能发生的情况是，对于某些条目，某些变量是无意义的或无法计算的；在这种情况下，它们的值为999.0，超出了所有变量的正常范围</li></ul><p id="79b4" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><em class="mb">看起来像</em> <strong class="jt hi"> <em class="mb">这样的值丢失值</em> </strong> <em class="mb">可能已经被CERN的科学家用一个异常值代替了。特别是，我注意到丢失的值或</em> <strong class="jt hi"> <em class="mb"> NAN </em> </strong> <em class="mb">条目(如果有)可能已被替换为不在特征值范围内的异常值；使用的异常值为999。</em></p><p id="40bb" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">为了将粒子对撞机实验中收集的大量数据转化为对自然的精确测量，粒子物理学家必须在高维数据中找到微妙的模式。为此，粒子物理学家使用复杂的计算机模拟，这些模拟编码了我们对基本粒子碰撞、附加辐射模式、粒子衰变、强子化、与探测器的相互作用以及传感器读数的理解。<em class="mb">不幸的是，用计算机模拟分析粒子碰撞数据经常面临一个根本性的挑战。</em></p><p id="9df6" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><em class="mb">模拟实现了一个正向过程:给定一个物理参数值，它们可以生成合成观测值。然而，在科学过程中，物理学家感兴趣的是逆问题:给定观测数据，物理参数最可能的值是什么？在频率主义者和贝叶斯统计框架中，这种推断所需的关键量是似然函数，或作为参数函数的观察数据的概率密度。然而，一般来说，计算似然函数是不可能的——这种困难是数据分析的一个基本问题。</em></p><p id="40dc" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">多年来，已经开发了不需要易处理的似然函数的参数推断的不同方法。这些方法统称为基于模拟或无似然推理技术。</p><p id="9dcd" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><strong class="jt hi">LHC粒子物理实验的主要阶段有:</strong></p><ul class=""><li id="6a4f" class="li lj hh jt b ju kp jy kq kc lk kg ll kk lm ko ln lo lp lq bi translated">从氢中产生质子，加速到0，99999999 c(这里的“c”是光速)</li><li id="abcc" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">将一束质子撞在一起(碰撞被称为“事件”)</li><li id="1001" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">将原始检测机信息转换为“点击数”(像素)</li><li id="b099" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">重建粒子轨迹——“轨迹”、“喷射”、“簇射”</li><li id="73e5" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">识别粒子的类型</li><li id="8aee" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">重建衰变结构</li><li id="bdfb" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">过滤有意义的事件</li><li id="74a0" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">分析统计属性</li></ul><p id="4791" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">从下面的转折点开始，抛物线的右半部分也突出显示了这些主要阶段:</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/ab8e71a60255e2259fbf6514b7fc9391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R--l-0xhzhDwTsUBpLQHrA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Picture Based on F.Carminati, K.Cranmer and V.Innocente</figcaption></figure><p id="57ec" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">请注意，如果你从“原始数据”转折点沿抛物线向右走<em class="mb">，你会沿着我在上面描述的路径。这张图片来自探测器记录和CERN科学家收集的原始信息→重建点→轨道段→轨道候选者和顶点→某种衰变。同时，有非常强大的软件和模块可以用来模拟发生在探测器层面和亚原子层面的每一步。实际上，有一些事件发生器可以产生类似模式的信号，它们允许您估计您可以从检测器获得什么样的响应。下面是我把我听说过的常见模拟软件包整理出来的一张图片:<em class="mb"> Geant </em>、<em class="mb"> Genie </em>和<em class="mb">皮媞亚</em>。</em></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/5abdaafc7c49fc5d9c3d9a6d5b7a0fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4yVB-YX8znBJvZKnn8Z6Pg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Pictures obtained from the simulation package links: Genie — <a class="ae mc" href="http://www.genie-mc.org/" rel="noopener ugc nofollow" target="_blank">http://www.genie-mc.org/</a>, Pythia — <a class="ae mc" href="https://pythia.org/" rel="noopener ugc nofollow" target="_blank">https://pythia.org/</a>, Geant — <a class="ae mc" href="https://geant4.web.cern.ch/" rel="noopener ugc nofollow" target="_blank">https://geant4.web.cern.ch/</a>. More info about simulation packages at <a class="ae mc" href="https://arxiv.org/pdf/1101.2599.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1101.2599.pdf</a></figcaption></figure><p id="a51d" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">用于模拟数据的方法也可以应用于真实数据。</p><h1 id="9821" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">方法学</h1><p id="d362" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">在这一部分中，我将讨论和描述我所做的探索性数据分析(EDA)、我所执行的推断性统计测试，以及使用了什么机器学习及其原因。</p><p id="4a8c" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">Kaggle上过去可用的数据和假设最大化的指标被称为<em class="mb">近似中值显著性</em> (AMS):</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/307c3c77b04f1886495b7972863da375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*FP1sq-3DtG62xR4CUjOsBg.png"/></div></figure><p id="95bd" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">我选择使用<strong class="jt hi">对数损失度量</strong>而不是AMS度量，这是为比赛设计的。对数损失度量是基于概率的最重要的分类度量之一。虽然很难解释原始的对数损失值，但是对数损失仍然是比较模型的一个很好的度量。对于任何给定的问题，较低的测井损失值意味着更好的预测。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/3fb01f4c1647b42c67bc9b30823657df.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*b3SCFrhYT6RiqWogwP8Dug.png"/></div></figure><p id="2871" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">其中<em class="mb"> y </em>是<strong class="jt hi">真标签</strong>，而<em class="mb"> p </em>是<em class="mb"> y </em> = 1的概率。</p><p id="c784" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><strong class="jt hi">注意，与AMS指标不同，对数损失严重惩罚了预测非常错误的模型</strong>。例如，如果<em class="mb"> y </em> = 1，并且一个模型预测<em class="mb"> p </em>是<em class="mb">近似为零</em>，那么就只剩下括号内的第一项(因为1—<em class="mb">y</em>= 0)，如果<em class="mb"> p </em>几乎为零，那么第一项就会很大。这是因为log(0)是负无穷大<em class="mb"/>，括号外的负号意味着–log(0)是正无穷大<em class="mb">。</em></p><h2 id="790d" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">探索性数据分析</h2><p id="63b6" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">从我执行的EDA中，我发现没有缺失值，这可能是因为CERN的科学家可能用不在要素值范围内的异常值替换了缺失值。特别是，他们似乎已经用–999取代了无意义和缺失的值(如果有的话)。</p><p id="2b38" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">总的来说，每列有818238行数据，包含用异常值-999替换的数据的11列是:</p><ol class=""><li id="4352" class="li lj hh jt b ju kp jy kq kc lk kg ll kk lm ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">DER_mass_MMC</code>":有124602行值= -999</li><li id="c71b" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">DER_deltaeta_jet_jet</code> ": 580253行值= -999</li><li id="313b" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">DER_mass_jet_jet</code> ": 580253行值= -999</li><li id="8262" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">DER_prodeta_jet_jet</code> " : 580253行值= -999</li><li id="874b" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">DER_lep_eta_centrality</code> ": 580253行值= -999</li><li id="9ad3" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_leading_pt</code> ": 327371行值= -999</li><li id="0c34" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_leading_eta</code> ": 327371行值= -999</li><li id="0d17" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_leading_phi</code> ": 327371行值= -999</li><li id="67cb" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_subleading_pt</code> ": 580253行值= -999</li><li id="31d8" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_subleading_eta</code> ": 580253行值= -999</li><li id="4904" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko mh lo lp lq bi translated">"<code class="du mi mj mk ml b">PRI_jet_subleading_phi</code> ": 580253行值= -999</li></ol><p id="ba3e" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">正如人们所料，这个数据集有一个类不平衡。在现实生活中，几乎总是存在阶级不平衡。正如我们在下图中看到的那样，衰变为两个τ轻子的希格斯玻色子也不能幸免于这个问题:</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/b379820183c06251fb05c5faa8aafce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*WG3s_rvhuPnhKeuzubEiDg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd iv">Class frequency plot: </strong>this class imbalance suggests that we should use a stratified-shuffle splitting validation strategy.</figcaption></figure><p id="e599" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">我还计算并绘制了<strong class="jt hi">互信息</strong>分数。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mn"><img src="../Images/76db287517a6ca3bf722a1e0ce6a2d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_27beWK-qDuZ9pBj7v4wMQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd iv">Mutual information scores:</strong> What’s surprising here is that most of the variables prefixed with DER (for DERived) are variables that contain the most outliers (the outlier -999). Some of these variables have about two-thirds of such outliers, yet they contain so much more information about the target variable than the variables that neither have outliers nor missing values. This is a somewhat surprising result and, luckily, I didn’t drop the columns because they contain mostly outliers!</figcaption></figure><p id="1c64" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">正如我们所看到的，‘<code class="du mi mj mk ml b">Weight</code>’特性包含了更多关于目标变量的信息，这也可以被认为是存在数据泄漏(稍后在‘我发现了什么’一节中有更多关于这一点的信息)。</p><h2 id="d8b7" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">思考模型</h2><p id="9a4a" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">由于明显的类别不平衡，我使用的数据分割策略是<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hi">分层混洗分割</strong> </a>，以在我训练我的模型之前，在训练和验证数据中保持相同的类别比例。</p><p id="1c8d" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">基本上有4种常见的机器学习模型。线性的、基于树的、基于kNN的和神经网络模型。作为我的第一次尝试，我尝试了线性模型，因此我必须检查特征之间的相关性，即我必须检查所谓的<strong class="jt hi">多重共线性</strong>。</p><p id="f5c8" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated"><strong class="jt hi">多重共线性</strong>发生在多元回归模型中的一个预测变量可以通过其他预测变量以高精度进行线性预测的时候。这可能导致扭曲或误导的结果。幸运的是，决策树和提升树算法天生对多重共线性免疫。当他们决定分割时，树将只选择一个完全相关的特征。然而，其他算法如逻辑回归或线性回归也不能避免这个问题。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/cee75ad6a8a70fd8e1bd8e859a6494b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1XWopk4C9Qfa_VzXVlMzw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The absolute correlation here is just the absolute value of the correlation.</figcaption></figure><p id="fa1b" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">正如我们所看到的，大多数特征之间的相关性为零，这对我的线性模型来说是个好消息，但我发现有496对特征显示出强正/负相关性(即绝对相关值大于0.8)。</p><p id="91fe" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">另一个模式，似乎是一个不错的选择，是KNN模式。然而，我很快意识到，如果我从1到40的范围内训练KNN模型，将需要很长时间来确定最佳k值。KNN模型的缺点是，由于距离计算，它们需要花费大量时间来训练和进行预测。然而，尽管有这个缺点，我仍然使用了KNN模型，它实际上比所有的线性回归模型都要好。</p><h1 id="3bbb" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">我发现了什么</h1><h2 id="c792" class="ku iu hh bd iv kv kw kx iz ky kz la jd kc lb lc jh kg ld le jl kk lf lg jp lh bi translated">数据泄漏和质量指标</h2><p id="6d0e" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated"><code class="du mi mj mk ml b">'Weight'</code>特征包含关于目标变量<code class="du mi mj mk ml b">'Label'</code>的信息。在我们的模型中包含这个特性会导致<a class="ae mc" href="https://www.kaggle.com/alexisbcook/data-leakage" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hi">数据泄露</strong> </a>。如果在训练模型之前没有去除<code class="du mi mj mk ml b">Weight</code>特征，这将导致<strong class="jt hi">完美的分类器</strong>，因此你将得到的是模型过于乐观。特别是，选择逻辑回归模型并使用L1正则化(LASSO)或L2正则化(RIDGE)技术将产生完美的分类器。数据泄露可以用我<a class="ae mc" href="https://github.com/Brighton94/higgs-boson-machine-learning-project/blob/main/atlas-higgs-challenge.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>开头定义的以下两个公式来解释:</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/c5b6d33a675e12c369e60b284e311174.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*BWtTDu_skRuwbGdCdK7Ofg.png"/></div></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mp"><img src="../Images/87166df7809c24ae52537cd6a89d787c.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*yzutpm0whA2ACLZBzB9olw.png"/></div></figure><p id="6d61" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">从上面两个公式可以看出，权重<em class="mb"> w_i </em>告诉你一些关于目标变量的事情(事件标签是<em class="mb"> s </em>还是<em class="mb"> b </em>)。出于这个原因，<code class="du mi mj mk ml b">Weight</code>列被简单地从特性列表中删除，以提供给我的模型。以下是没有数据泄漏的正确混淆矩阵:</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/2fbdd71e518f264a95c171c2536678c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlRilVvigtoaGgldu9eBtQ.png"/></div></div></figure><p id="bf55" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">如我们所见，使用L1正则化(Lasso)的逻辑回归模型比使用L2正则化(Ridge)技术的表现稍好；使用正则化比仅使用正则逻辑回归模型(Lr)稍有改进。线性模型可以通过使用主成分分析(PCA)等降维技术来改进，但问题是我们的大多数值都是弱相关的，如果特征之间有很强的相关性，PCA是有用的，所以我怀疑PCA不会有帮助或产生巨大的影响。所以这个项目的主要焦点很快转向了基于树的模型和神经网络。</p><p id="e53d" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">目前，我使用了一个基于树的模型——梯度推进(GB)和一个简单的神经网络(NN)。梯度推进模型是目前编写时最好的模型。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/6e655aa5a2fc9194323674be7496eb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJM79vEuuVqBrj_LxD_fpQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The error, in this case the log-loss, started to plateu between 200 and 400 trees.</figcaption></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/affcabbcda494b00ff15868d9c9b749a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*bYbFv6eVXcBL1T94HNvIpQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">The gradient boosting classifier confusion matrix.</figcaption></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/dd7be79ea921d8c8e8b154d107dd8030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*QHorbPXackqB33CeXXkZ1A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd iv">The ROC Curve of the Gradient Boosted Classifier. NB:</strong> In the machine learning community, the ROC curve is plotted as a dependence of the true positive rate (TPR) from the false positive rate (FPR). However, in the high energy physics community, the ROC curve is plotted as the dependence of the one minus false positive rate (1–FPR) from the true positive rate. The 1–FPR is called the <strong class="bd iv">background rejection</strong> and the TPR is called the <strong class="bd iv">signal efficiency</strong>.</figcaption></figure><p id="4acd" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">正如我们可以从ROC曲线中看到的，梯度增强分类器表现良好，并且远远优于线性模型，即使没有对超参数进行任何微调或使用4重交叉验证。然而，神经网络没有梯度推进模型做得好，但它比我尝试过的任何线性模型都好。也许一些预处理、特征工程、添加更多隐藏层和超参数调整可能会使我的keras神经网络的性能更好。也许比梯度推进模型更好。</p><p id="a0ee" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">结论:梯度增强分类器是用于确定事件是背景还是信号的最佳分类器；准确度为0.84。使用的第二好的分类器是两个隐藏层的keras神经网络；一个隐藏层有100个神经元，另一个隐藏层有60个神经元。两个隐藏层都使用了ReLu激活函数，精度为0.83。</p><h1 id="5a30" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">后续步骤</h1><ul class=""><li id="49b7" class="li lj hh jt b ju jv jy jz kc mu kg mv kk mw ko ln lo lp lq bi translated">改进keras神经网络。</li><li id="a511" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">特征工程(需要一些粒子物理知识)。</li></ul><h1 id="6028" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">我没做的事</h1><ul class=""><li id="9753" class="li lj hh jt b ju jv jy jz kc mu kg mv kk mw ko ln lo lp lq bi translated">模型组装。虽然梯度推进是一个集合，我没有做各种各样的模型集合。</li><li id="f4cd" class="li lj hh jt b ju lr jy ls kc lt kg lu kk lv ko ln lo lp lq bi translated">尝试为我的基于kNN的模型找到最佳k值。我想在1到40的范围内找到最好的k值。这意味着我必须训练41个kNN模型，训练它们需要很多时间，因为距离的计算。也许正确实现本文中的步骤将有助于加快确定最佳k值的过程:<a class="ae mc" href="https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb" rel="noopener" target="_blank">用20行代码使kNN比Scikit-learn快300倍！</a>。或者，你可以试着正确地实现这个<a class="ae mc" href="https://pub.towardsai.net/knn-k-nearest-neighbors-is-dead-fc16507eb3e" rel="noopener ugc nofollow" target="_blank">的代码和思想，KNN (K近邻)死了！</a>博客文章—作者声称:</li></ul><blockquote class="mx my mz"><p id="9529" class="jr js mb jt b ju kp jw jx jy kq ka kb na kr ke kf nb ks ki kj nc kt km kn ko ha bi translated">"<em class="hh">人工神经网络(近似最近邻)万岁，他们比sklearn的KNN快了380倍，同时提供了99.3%的相似结果</em>"</p></blockquote><p id="a0a5" class="pw-post-body-paragraph jr js hh jt b ju kp jw jx jy kq ka kb kc kr ke kf kg ks ki kj kk kt km kn ko ha bi translated">谢谢你看我的博客。点击<a class="ae mc" href="https://github.com/Brighton94/higgs-boson-machine-learning-project/blob/main/atlas_higgs_challenge.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jt hi">此处</strong> </a>查看我在github上的笔记本。</p></div></div>    
</body>
</html>