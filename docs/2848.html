<html>
<head>
<title>Breast Cancer Detection with Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于决策树的乳腺癌检测</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/breast-cancer-detection-with-decision-trees-f66637ac482e?source=collection_archive---------1-----------------------#2022-06-18">https://medium.com/mlearning-ai/breast-cancer-detection-with-decision-trees-f66637ac482e?source=collection_archive---------1-----------------------#2022-06-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e389" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于如何使用scikit-learn找出决策树算法的最佳参数的指南。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/460b130c3cee6a8c5371d46be7ef0773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I4sKj35ygT4IeoF6"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@jplenio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Johannes Plenio</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dc6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树是最常用的机器学习算法之一。在这篇文章中，我将讨论以下主题，</p><ul class=""><li id="2c65" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">什么是决策树？</li><li id="52d5" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">决策树的一些优点和缺点</li><li id="d509" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">数据预处理</li><li id="bc4d" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">构建模型</li><li id="8360" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">模型评估</li><li id="4f87" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">网格搜索超参数调谐</li></ul><p id="8edf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更多关于机器学习的内容，可以关注我们的<a class="ae js" href="https://youtube.com/c/tirendazacademy" rel="noopener ugc nofollow" target="_blank"> Tirendaz Academy </a> YouTube频道。</p><p id="dda2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们开始吧！</p><h1 id="b2ba" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">什么是决策树？</h1><p id="e909" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">决策树是一种非参数的监督学习。这种技术广泛用于分类和回归任务。这种方法的目标是创建一个模型来预测目标变量的值。换句话说，决策树编码了一系列if-then-else规则。树中的每个节点都包含一个条件。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lk"><img src="../Images/4c1ea23e60e2b9a9d47a0c715c70e429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n478b-JcJInucGXvEhR87Q.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx"><a class="ae js" href="https://scikit-learn.org/stable/modules/tree.html#tree" rel="noopener ugc nofollow" target="_blank">Decision tree for iris dataset</a></figcaption></figure><h1 id="0797" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">决策树的一些优点和缺点</h1><p id="a68c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">像其他机器学习评估器一样，决策树有一些优点和缺点。通过考虑以下问题，您可以构建一个良好的决策树模型。</p><p id="461c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们来看看决策树的一些优点。</p><ul class=""><li id="dc4d" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">决策树很容易理解和解释。</li><li id="b570" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">你可以很容易地想象树木。</li><li id="1b32" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">决策树几乎不需要数据预处理。</li><li id="a012" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">您可以使用这种技术处理数值和分类数据。</li></ul><p id="dae0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，决策树也有一些缺点。让我们来看看这些缺点。</p><ul class=""><li id="22f5" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">决策树学习者可以创建过于复杂的树，不能很好地概括数据。为了克服这个问题，您可以使用一些方法，例如设置树的最大深度，设置叶节点所需的最小样本数，以及修剪。</li><li id="4dc4" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">决策树可能不稳定。为了避免这个问题，您可以在集成中使用决策树。</li></ul><h1 id="2d6b" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">具有scikit学习的决策树</h1><p id="8d9c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">为了展示如何实现决策树算法，我将使用乳腺癌威斯康星州数据集。在加载数据集之前，让我导入熊猫。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="4433" class="lq ki hh lm b fi lr ls l lt lu">import pandas as pd</span></pre><p id="0d62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们加载数据集。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="8bc8" class="lq ki hh lm b fi lr ls l lt lu">df = pd.read_csv( “Breast Cancer Wisconsin.csv”)</span></pre><p id="a76d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在这里找到数据集<a class="ae js" href="https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data" rel="noopener ugc nofollow" target="_blank"/>。让我们看一下数据集的前五行。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="d2ff" class="lq ki hh lm b fi lr ls l lt lu">df.head()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lv"><img src="../Images/11f5064fa7fe5ba6777bb224fba1fb45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKUG0dw9JQAjofhnE5mFcA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">The first rows of the breast cancer dataset</figcaption></figure><p id="03d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该数据集由恶性和良性肿瘤细胞的例子组成。数据集中的第一列显示唯一的ID号，第二列显示诊断，假设M表示恶性，B表示良性。其余栏目是我们的特色。让我们来看看数据集的形状。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="9c8e" class="lq ki hh lm b fi lr ls l lt lu">df.shape</span><span id="8762" class="lq ki hh lm b fi lw ls l lt lu"># output:<br/>(569, 33)</span></pre><h1 id="20c2" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">数据预处理</h1><p id="44d2" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">现在，让我们创建输入和输出变量。为此，我将使用loc和drop方法。首先，让我创建我们的目标变量。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="a8a9" class="lq ki hh lm b fi lr ls l lt lu">y = df.loc[:,"diagnosis"].values</span></pre><p id="cc60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">太美了。我们创建了目标变量。让我们创建我们的特征变量。为此，我将使用drop方法。让我删除目标变量和不必要的列。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="e37b" class="lq ki hh lm b fi lr ls l lt lu">X = df.drop(["diagnosis","id","Unnamed: 32"],axis=1).values</span></pre><p id="60c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，我们的目标变量有两个类别，M和b。让我们用标签编码器对目标变量进行编码。首先，我要导入这个类。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="ae39" class="lq ki hh lm b fi lr ls l lt lu">from sklearn.preprocessing import LabelEncoder</span></pre><p id="f4cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我要从这个类创建一个对象。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="0ec4" class="lq ki hh lm b fi lr ls l lt lu">le = LabelEncoder()</span></pre><p id="b282" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们拟合并转换我们的目标变量。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="0d95" class="lq ki hh lm b fi lr ls l lt lu">y = le.fit_transform(y)</span></pre><p id="53f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在构建模型之前，让我们将数据集分为训练集和测试集。为此，我将使用train_test_split函数。首先，让我导入这个函数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="e9b7" class="lq ki hh lm b fi lr ls l lt lu">from sklearn.model_selection import train_test_split</span></pre><p id="a361" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们使用这个函数分割数据集。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="bd3c" class="lq ki hh lm b fi lr ls l lt lu">X_train, X_test, y_train, y_test = train_test_split(X, y,   <br/>                                                    stratify=y,<br/>                                                    random_state=0)</span></pre><h1 id="3d96" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">构建决策树模型</h1><p id="fb7a" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">让我们来看看如何构建决策树模型。首先，我要导入决策树分类器类。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="8087" class="lq ki hh lm b fi lr ls l lt lu">from sklearn.tree import DecisionTreeClassifier</span></pre><p id="b20e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从这个类创建一个对象。首先，我想使用默认值。因此，我将只使用random_state参数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="427f" class="lq ki hh lm b fi lr ls l lt lu">dt = DecisionTreeClassifier(random_state = 42)</span></pre><p id="f023" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们用训练集来建立模型。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="0353" class="lq ki hh lm b fi lr ls l lt lu">dt.fit(X_train, y_train)</span></pre><p id="4901" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">太棒了。我们建立了我们的模型。现在，让我们用这个模型来预测训练值和测试值。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="5040" class="lq ki hh lm b fi lr ls l lt lu">y_train_pred=dt.predict(X_train)<br/>y_test_pred=dt.predict(X_test)</span></pre><p id="264f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们看看模型在训练集和测试集上的性能。为此，我将使用accuracy_score函数。首先，让我导入这个函数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="0e68" class="lq ki hh lm b fi lr ls l lt lu">from sklearn.metrics import accuracy_score</span></pre><p id="47c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们来看看训练集和测试集的准确性分数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="c364" class="lq ki hh lm b fi lr ls l lt lu">tree_train = accuracy_score(y_train, y_train_pred)<br/>tree_test = accuracy_score(y_test, y_test_pred)</span></pre><p id="751e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们打印这些分数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="ad83" class="lq ki hh lm b fi lr ls l lt lu">print(f’Decision tree train/test accuracies: <br/>       {tree_train:.3f}/{tree_test:.3f}’)</span><span id="0876" class="lq ki hh lm b fi lw ls l lt lu">#Output:<br/>Decision tree train/test accuracies:1.000/0.951</span></pre><p id="c88f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如您所见，训练集上的分数是100%，但测试集上的分数是95%。这意味着我们的模型存在过拟合问题。请注意，决策树模型对训练集的学习非常好。所以，它只是记住了结果。但是，模型不能一概而论。请注意，当我们有一个复杂的模型时，会发生过度拟合。</p><p id="10c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了克服过度拟合问题，我们控制树的复杂性。要做到这一点，我们有多种方法。首先，让我们指定max_depth参数，它控制级别的最大数量。max_depth参数的默认值是None，这意味着树可以尽可能地增长。我们可以尝试较小的值，并比较结果。让我指定max_depth参数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="9dee" class="lq ki hh lm b fi lr ls l lt lu">dt = DecisionTreeClassifier(max_depth=2)<br/>dt.fit(X_train, y_train)</span></pre><p id="32b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们再来看看这个模型在训练集和测试集上的表现。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="d6ba" class="lq ki hh lm b fi lr ls l lt lu">y_train_pred=dt.predict(X_train)<br/>y_test_pred=dt.predict(X_test)<br/>tree_train = accuracy_score(y_train, y_train_pred)<br/>tree_test = accuracy_score(y_test, y_test_pred)<br/>print(f’Decision tree train/test accuracies: <br/>      {tree_train:.3f}/{tree_test:.3f}’)</span><span id="1054" class="lq ki hh lm b fi lw ls l lt lu">#Output:<br/>Decision tree train/test accuracies:0.951/0.923</span></pre><p id="601b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如您所见，训练集上的性能是100%，但现在只有95%。这意味着模型不再能够记住训练集中的所有结果。通过降低复杂性，我们提高了模型的泛化能力。</p><p id="f4fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，另一个问题是模型过于简单。为了做得更好，我们需要使用不同的参数来调整模型。为此，我将使用网格搜索技术。</p><h1 id="831a" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">网格搜索超参数调谐</h1><p id="6c1c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">你可以用网格搜索技术找出你的模型的最佳参数。让我们导入GridSearchCV类。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="94cc" class="lq ki hh lm b fi lr ls l lt lu">from sklearn.model_selection import GridSearchCV</span></pre><p id="bcf9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我将从DecisionTreeClassifier创建一个对象。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="143f" class="lq ki hh lm b fi lr ls l lt lu">dt = DecisionTreeClassifier(random_state = 42)</span></pre><p id="3844" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我创建一个参数变量，它包括max_depth和min_leaf_size的值，这是另一个重要的参数。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="35c9" class="lq ki hh lm b fi lr ls l lt lu">parameters = {"max_depth":[1, 2, 3, 4, 5, 7, 10],  <br/>              "min_samples_leaf": [1, 3, 6, 10, 20]}</span></pre><p id="9c1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">太棒了。我们指定了参数的值。为了找到最佳参数，我将从GridSearchCV创建一个对象。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="1c3d" class="lq ki hh lm b fi lr ls l lt lu">clf = GridSearchCV(dt, parameters, n_jobs= 1)</span></pre><p id="dce8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的模型可以训练了。接下来，我将使用训练集来拟合我们的模型。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="4b57" class="lq ki hh lm b fi lr ls l lt lu">clf.fit(X_train, y_train)</span></pre><p id="9618" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，为了查看最佳参数，我将使用best_params_ attribute。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="6160" class="lq ki hh lm b fi lr ls l lt lu">print(clf.best_params_)</span><span id="c382" class="lq ki hh lm b fi lw ls l lt lu">#Output:<br/>{'max_depth': 3, 'min_samples_leaf': 1}</span></pre><p id="24b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们执行这个单元时，你可以看到最好的参数。最大深度为3，最小样本叶为1。</p><h1 id="478c" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">评估模型</h1><p id="0a55" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">现在，我要预测用这些参数训练的这个模型。注意，我们不需要再次训练我们的模型。因为找到最佳参数后，模型就训练好了。让我们预测训练值和测试值。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="a97e" class="lq ki hh lm b fi lr ls l lt lu">y_train_pred=clf.predict(X_train)<br/>y_test_pred=clf.predict(X_test)<br/>tree_train = accuracy_score(y_train, y_train_pred)<br/>tree_test = accuracy_score(y_test, y_test_pred)<br/>print(f’Decision tree train/test accuracies:<br/>      {tree_train:.3f}/{tree_test:.3f}’)</span><span id="e2ce" class="lq ki hh lm b fi lw ls l lt lu">#Output:<br/>Decision tree train/test accuracies:0.974/0.958</span></pre><p id="39dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给你。根据最佳参数找到准确度分数。请注意，我们的模型在训练集上的得分接近测试集上的得分。此外，两者的准确度分数都接近1。因此，我们获得了最佳参数，并使用这些参数预测了训练集和测试集中的值。</p><h1 id="e716" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="ab2c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">决策树是一种非参数的监督学习方法。您可以使用决策树算法执行分类和回归任务。在这篇文章中，我谈到了决策树以及如何用scikit learn实现这项技术。最后，我展示了如何用网格搜索技术找出最佳参数。你可以在这里找到这个笔记本<a class="ae js" href="https://www.kaggle.com/code/tirendazacademy/breast-cancer-detection-with-decision-trees" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="5421" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就是这样。感谢阅读。我希望你喜欢它。别忘了在<a class="ae js" href="https://www.youtube.com/channel/UCFU9Go20p01kC64w-tmFORw" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae js" href="https://github.com/tirendazacademy" rel="noopener ugc nofollow" target="_blank">GitHub</a>|<a class="ae js" href="https://twitter.com/TirendazAcademy" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae js" href="https://www.kaggle.com/tirendazacademy" rel="noopener ugc nofollow" target="_blank">ka ggle</a>|<a class="ae js" href="https://www.linkedin.com/in/tirendaz-academy" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>上关注我们。</p><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/mlearning-ai/ensemble-learning-adaboost-with-python-8332778fbb61"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">集成学习—使用Python的AdaBoost</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">关于如何用scikit-learn实现自适应增强(AdaBoost)算法的指南。</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo jm ma"/></div></div></a></div><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/geekculture/top-8-machine-learning-algorithms-df30277b2056"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">8种最佳机器学习算法</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">数据科学家和机器学习工程师应该知道的最好的机器学习算法。</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mp l ml mm mn mj mo jm ma"/></div></div></a></div><p id="1354" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果这篇文章有帮助，请点击拍手👏按钮几下，以示支持👇</p><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mq l ml mm mn mj mo jm ma"/></div></div></a></div></div></div>    
</body>
</html>