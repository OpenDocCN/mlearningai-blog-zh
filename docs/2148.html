<html>
<head>
<title>Quick Guide to Deal with Imbalanced Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">处理不平衡数据的快速指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/quick-guide-to-deal-with-imbalanced-data-77014a44f4bb?source=collection_archive---------5-----------------------#2022-03-15">https://medium.com/mlearning-ai/quick-guide-to-deal-with-imbalanced-data-77014a44f4bb?source=collection_archive---------5-----------------------#2022-03-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/6a86be3a1c65cbddb48a4481b82accea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q32OK8b7kWpL-qyjh0fRLA.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Meme by author | Generated through <a class="ae ip" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/memegenerator</a> | Photo source belongs to Universal Pictures — Despicable Me movies</figcaption></figure><p id="96dc" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">作为数据科学家，我们生活在一个有趣的时代。我们已经看到了云技术和自动化的指数级增长。突然间，每个人都可以成为数据科学家——不再需要有人拥有理学硕士或博士学位，除非这个角色涉及特定的新问题。虽然现在进入该领域的门槛较低，但我坚信，作为数据科学家，我们必须至少掌握经典机器学习算法的本质以及一些技术概念。</p><p id="0007" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">根据我作为数据科学家的经验，我发现不平衡的类是最被低估的技术概念之一，尤其是对于新的数据科学家。在这篇文章中，我们将讨论为什么不平衡的班级是一个问题，以及一些我认为有用的解决方案。</p><h2 id="0c6b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">问题的症结</h2><p id="e882" class="pw-post-body-paragraph iq ir hh is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn ha bi translated">要理解为什么不平衡的班级是一个问题，我们必须回到根本。机器学习算法是如何工作的？所有的机器学习算法，不管它们有多花哨，都旨在最小化<strong class="is hi"> <em class="ko">成本函数</em> </strong>。因为算法只是算法，它们不像我们，没有对错的概念。因此，我们在训练过程中使用成本函数，以允许算法学习概念。不同的算法将有它们自己的方式来最小化函数，例如用于线性模型的<strong class="is hi"> <em class="ko">梯度下降</em> </strong>和用于神经网络的<strong class="is hi"> <em class="ko">反向传播</em> </strong>。</p><p id="e14f" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">简而言之，这意味着所有的最大似然算法都寻求最小化它们预测中的误差。虽然这是一个显而易见的事实，但当数据集中存在不平衡的类时，这就会成为一个问题。考虑一个场景，我们想要创建一个模型来预测很少发生的欺诈活动(我知道一个非常老套的例子)。假设在1000个观察值的数据集中，只有20个正值(欺诈性的)和980个负值(非欺诈性的)。为了获得良好的性能，ML算法可以将所有观察结果分类为非欺诈性的(即使ML算法也讨厌出错！)</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es kp"><img src="../Images/c696fa268db0fdf6666616098565e65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/1*A-OXER-E1uxc7kurw4fEOg.gif"/></div></figure><p id="f3f3" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">通过使用准确性作为性能的衡量标准，我们将获得:</p><p id="e468" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">(0 + 980) / 1000 = 0.98精度！</p><p id="ac3f" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这看起来令人印象深刻，但非常误导。如果模型的目标是预测欺诈活动，那么这个特定的模型绝对是垃圾，因为它根本没有正确地对欺诈活动进行分类<strong class="is hi"/>。班级越不平衡，误解就越严重。你们当中更有经验的数据科学家将会意识到，首先，在存在不平衡类别的情况下，通过精确度来衡量最大似然模型的性能是一个糟糕的选择。你是对的！</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es ku"><img src="../Images/71409a7b64be1b4c9c30fb8a9fce8760.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/1*DWhy6sfOFrtrzFN9PBH0pw.gif"/></div></figure><p id="0e83" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">通常，“修补程序”会选择更合适的性能指标，例如召回，尽量减少误报，等等。然而，这个补丁只会帮助我们更好地了解不平衡职业对模型的影响，这只是第一步。在我们上面的例子中，模型将得到一个召回分数:</p><p id="82c2" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi">0 / (0 + 20) = 0</p><p id="0bc1" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">因为它没有对任何欺诈活动进行正确分类。我们还可以在训练ML算法之前使用分层采样，以便在存在不平衡数据的情况下获得更准确的性能视图。然而，解决不平衡问题的理想方法是收集更多的数据，使少数群体或多或少地与大多数群体平等。但是，在实际项目中，由于时间限制或数据收集过程过于昂贵，这通常是不可行的。那么我们能做什么呢？</p><h2 id="144c" class="jo jp hh bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">解决方案1 —阈值分类</h2><p id="a05a" class="pw-post-body-paragraph iq ir hh is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn ha bi translated">我们可以做的一件事是为训练好的模型定义定制的概率阈值。这是最简单却被忽视的方法之一。ML模型的输出要么是概率，要么是一些表示类成员的分数。底线是，即使在分类的情况下，输出也总是连续的值。那么我们的模型如何输出标签呢？还是上课？答案是<strong class="is hi"> <em class="ko">决定阈值</em> </strong>。</p><p id="8c6b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">再次考虑我们的欺诈活动示例，其中有两个类别。一个自然的决策阈值是将其设为0.5。例如，如果观察的概率小于0.5，我们会将其归类为欺诈活动，如果不是，则会将其归类为非欺诈活动。<em class="ko">。例如，来自<strong class="is hi"> scikit-learn </strong>包的predict() </em>方法使用这个缺省值进行二进制分类。当处理不平衡数据集时，这又是次优的或者完全错误的。我们可以做的一件事是将决策阈值从0.5“移动”到其他值。为了找到正确的决策阈值，我们可以:</p><ul class=""><li id="4cd9" class="kv kw hh is b it iu ix iy jb kx jf ky jj kz jn la lb lc ld bi translated">咨询主题专家或</li><li id="e871" class="kv kw hh is b it le ix lf jb lg jf lh jj li jn la lb lc ld bi translated">在概率范围内使用网格搜索(在0和1之间)</li></ul><p id="6ded" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">要使用<strong class="is hi"> scikit-learn </strong>实现这一点，请使用<em class="ko">。predict_proba() </em>方法输出概率而不是标签。</p><h2 id="2b42" class="jo jp hh bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">解决方案2 —采样算法</h2><p id="a916" class="pw-post-body-paragraph iq ir hh is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn ha bi translated">第二个解决方案是创建一个更加平衡的数据集。虽然数据收集过程很昂贵，但我们可以使用采样算法来代替。例如，我们可以使用<strong class="is hi"> bootstrapping </strong>及其变体来产生更多的少数类。然而，bootstrapping的一个缺点是它减少了数据的变化，这可能会导致模型不能很好地一般化。我个人最喜欢使用SMOTE(过采样算法)和Tomek's Links(欠采样算法)的组合。这两种方法<strong class="is hi">必须</strong>一起使用才能产生最佳效果。为了理解为什么我们将在高层次上探索这两种算法。</p><p id="dae9" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">击杀</strong></p><p id="f283" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">合成少数过采样技术(SMOTE)是一种生成合成数据的技术。这个特殊的算法在“特征”空间而不是“数据”空间工作。这意味着SMOTE用于已经过预处理并准备好输入ML算法的数据。SMOTE使用k近邻的概念，因此，我们必须预先确定近邻的数量。邻居数量的默认值是5。为了生成合成数据，下面的等式其中<em class="ko"> x </em>是被研究的数据点并且<em class="ko"> n </em>是它的邻居:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/2d6ba81e884404ea165ebb17e552d03d.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/1*HL6wVqAplqFWzUstP092zw.gif"/></div></figure><p id="a7a8" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在高层次上，这个等式意味着我们只是在原始数据点上添加了噪声。不过有一个小小的警告。通过使用上面的公式，合成数据点将<strong class="is hi">仅</strong>沿着被研究的数据点和它的邻居之间的直线生成。下面的图可以用来说明:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/d143803df3152e41e3ba14bb076da0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*ASOcLf26G2d0ovOGN1irOQ.png"/></div><figcaption class="il im et er es in io bd b be z dx">Image by Author | generated using scikit-learn python library</figcaption></figure><p id="2299" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">假设我们使用具有3个最近邻居的SMOTE算法。对于特定的数据点，我们将使用距离度量来识别少数类的3个最近邻。根据过采样率，我们将从已识别的3个最近邻中选择随机邻，并沿线创建合成数据(每条线一个数据点)。过采样率取决于所选择的k个最近邻居的数量。例如，如果我们使用默认值5最近邻，我们可以将少数类的数据点增加到其初始数量的500%。</p><p id="9d45" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这种技术优于普通的自举，因为它增加了合成数据的变化。同时，它必须与欠采样算法一起使用，特别是Tomek的链接，因为否则，它将“模糊”并移动决策边界。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/b0eaac7dc254f06919c6aa68debc1928.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*yE2stUIGRc_qLDi07pXBTA.png"/></div><figcaption class="il im et er es in io bd b be z dx">Before SMOTE | Image by Author | Generated using scikit-learn package</figcaption></figure><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/b94f807720d968749c1008032a9e4203.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*QlEPYe3L3eeRnG0UgUbQMw.png"/></div><figcaption class="il im et er es in io bd b be z dx">After SMOTE | Image by Author | Generated using imbalanced-learn package</figcaption></figure><p id="695d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">正如我们所看到的，在使用SMOTE算法之后，区分两个类的判定边界不太清楚，因为更多的蓝色点与橙色点混合在一起。这可能产生次优的ML模型。为了缓解这个问题，可以利用Tomek的链接。</p><p id="1a3d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">托梅克的链接</strong></p><p id="efaa" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">不同类别的两个数据点之间的托梅克链接被定义为使得对于任何样本<em class="ko"> z </em>，</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/e517ab88855e1341d08e74da4bdebaa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*DdlixKR4vJB6hBfzTXhPOg.png"/></div></figure><p id="7b11" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">通俗地说，如果两个数据点属于不同的类，并且是彼此最近的邻居，则这两个数据点具有托梅克链接。目的是沿着决策边界寻找点。下面的情节可以作为例证:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/d9c3de0cec85bf09d4930be65d44fbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*8ty-o8z5EL8qIOYCTcGe9A.png"/></div><figcaption class="il im et er es in io bd b be z dx">Image from <a class="ae ip" href="https://imbalanced-learn.org/stable/under_sampling.html" rel="noopener ugc nofollow" target="_blank">https://imbalanced-learn.org/stable/under_sampling.html</a></figcaption></figure><p id="f0e1" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">一旦我们从各种不同的对中识别出Tomek的链接，我们就可以从对中的多数类中仅移除<strong class="is hi">的</strong>数据点，或者移除所有样本(移除所有Tomek的链接)。这将使我们能够删减SMOTE算法产生的合成数据。例如，在下图中，我们删除了所有已识别的Tomek链接。因此，较少的蓝点与多数类别混合，从而在两个类别之间产生总体上更清晰的区分。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/eab021257e81c271a4d6252f49d62224.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*_L_UXFZSORyu8H_AG7EkJA.png"/></div><figcaption class="il im et er es in io bd b be z dx">SMOTE + Tomek’s Links | Image by Author | Generated using scikit-learn and imbalanced-learn package</figcaption></figure><p id="de29" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">幸运的是，这些算法已经通过<strong class="is hi">不平衡学习</strong>包实现并可以立即使用。该软件包还包含SMOTE算法的许多其他变体，并有全面的文档。一个重要的注意事项是，我们应该<strong class="is hi">只</strong>对<strong class="is hi">训练数据集</strong>使用采样算法！</p><p id="8e8d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">[1]<a class="ae ip" href="https://arxiv.org/pdf/1106.1813.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1106.1813.pdf</a></p><p id="218a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;ar number = 4309452</p><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh ij lt"/></div></div></a></div></div></div>    
</body>
</html>