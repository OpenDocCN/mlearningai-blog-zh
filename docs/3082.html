<html>
<head>
<title>MNIST Image Classification via Federated Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于联邦学习的MNIST图像分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/mnist-image-classification-via-federated-learning-c922f6ee2d80?source=collection_archive---------1-----------------------#2022-07-18">https://medium.com/mlearning-ai/mnist-image-classification-via-federated-learning-c922f6ee2d80?source=collection_archive---------1-----------------------#2022-07-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es il"><img src="../Images/1aec5de3b27999a45bf68d17a295bdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEUxSGQGOFgwobm6br9WCg.jpeg"/></div></div></figure><p id="2104" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">什么是联合学习？ <br/>联合学习(Federated learning)是机器学习中的一种技术，通过这种技术，我们可以在不同设备或保存多个本地数据样本的单个服务器上传播的数据上训练模型。简而言之，联邦学习的目标是在分散的数据集上训练机器学习模型，而不制作它的任何副本。这样，联合学习也为我们提供了更好的隐私和资源管理。</p><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es jv"><img src="../Images/ebc709705f07dcdff35ee3c4fd9da103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7fzD6tix6s1z1p9RtIbbw.jpeg"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig. How federated learning works</figcaption></figure><p id="223d" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">在这篇博客中，我们将使用联合学习技术训练一个MNIST图像分类模型。MNIST数据集包括单通道60，000个范围从0到9的单个数字的手写图像，尺寸为28×28。</p><p id="e785" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">该数据集还包含10，000张具有相同规格的测试图像。</p><blockquote class="ka kb kc"><p id="8c1e" class="ix iy kd iz b ja jb jc jd je jf jg jh ke jj jk jl kf jn jo jp kg jr js jt ju ha bi translated"><strong class="iz hi">工作</strong></p></blockquote><p id="cea9" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">数据分布在4个不同的客户端。这样，我们就有了一个分散的数据集。模型权重在客户端和主服务器模型之间传递。</p><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es kh"><img src="../Images/0c61f1ecc22fc0be93ee58a14ba5829c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfxEhyvX55r8IA6_GOMP2Q.jpeg"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig. federated learning among 4 different (local) clients</figcaption></figure><blockquote class="ka kb kc"><p id="bf68" class="ix iy kd iz b ja jb jc jd je jf jg jh ke jj jk jl kf jn jo jp kg jr js jt ju ha bi translated"><strong class="iz hi">模型架构</strong></p></blockquote><p id="abb8" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">一个具有两个卷积层和两个完全连接层的神经网络被用来实现这个任务。型号规格如下所示</p><figure class="im in io ip fd iq er es paragraph-image"><div role="button" tabindex="0" class="ir is di it bf iu"><div class="er es ki"><img src="../Images/bd9f1fa72343d6c60ec661785a17e378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQnuUyN8J8WJDmSJ5aF7Dg.jpeg"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig. Model architecture</figcaption></figure><h1 id="56fe" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">做代码</strong></h1><p id="e58d" class="pw-post-body-paragraph ix iy hh iz b ja lh jc jd je li jg jh ji lj jk jl jm lk jo jp jq ll js jt ju ha bi translated">首先，您需要为此获得所有必需的库和依赖项。我用python编写代码，并使用以下库</p><ol class=""><li id="068f" class="lm ln hh iz b ja jb je jf ji lo jm lp jq lq ju lr ls lt lu bi translated">Pytorch</li><li id="718b" class="lm ln hh iz b ja lv je lw ji lx jm ly jq lz ju lr ls lt lu bi translated">Pysyft</li><li id="354b" class="lm ln hh iz b ja lv je lw ji lx jm ly jq lz ju lr ls lt lu bi translated">Numpy</li><li id="589c" class="lm ln hh iz b ja lv je lw ji lx jm ly jq lz ju lr ls lt lu bi translated">简易词典</li><li id="b0a0" class="lm ln hh iz b ja lv je lw ji lx jm ly jq lz ju lr ls lt lu bi translated">火炬视觉</li><li id="ddfd" class="lm ln hh iz b ja lv je lw ji lx jm ly jq lz ju lr ls lt lu bi translated">Matplotlib</li></ol><p id="5cb6" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">安装所需的依赖关系</strong></p><p id="541e" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">另外，确保你按照我提到的顺序安装它们，因为有一些版本问题。</p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="3850" class="mf kk hh mb b fi mg mh l mi mj">pip install syft -f <a class="ae mk" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/torch_stable.html</a><br/>pip  install torchvision<br/>pip install syft==0.2.9<br/>pip install easydict<br/>pip install matplotlib</span></pre><p id="3f6b" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">链接Pytorch和Pysyft </strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="3ad0" class="mf kk hh mb b fi mg mh l mi mj"># A Hook which Overrides Methods on PyTorch Tensors. Sending the tensors from one virtual machine / client to another</span><span id="ad21" class="mf kk hh mb b fi ml mh l mi mj"># The purpose of this class is to: extend torch methods to allow for the moving of tensors from one</span><span id="6d95" class="mf kk hh mb b fi ml mh l mi mj">hook = sy.TorchHook(torch)</span></pre><p id="3ba7" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">定义保存数据的客户端/虚拟机</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="16c7" class="mf kk hh mb b fi mg mh l mi mj"># Defining virtual clients<br/># sending the hook we previously created into these clients so the model knows which virtual machines to send data ( move tensors between )</span><span id="acd1" class="mf kk hh mb b fi ml mh l mi mj">client0 = sy.VirtualWorker(hook, id="client0")<br/>client1 = sy.VirtualWorker(hook, id="client1")<br/>client2 = sy.VirtualWorker(hook, id="client2")<br/>client3 = sy.VirtualWorker(hook, id="client3")</span></pre><p id="7a39" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">在一个简单的字典中定义参数</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="4709" class="mf kk hh mb b fi mg mh l mi mj">args = edict({<br/>"batch_size" : 64,<br/>"epochs" : 10,<br/>"learning_rate" : 0.01,})</span></pre><p id="9082" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">制作联合数据集</strong></p><p id="66a6" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">我们的MNIST数据集没有在我们之前创建的客户端之间划分，因此它是一个集中式数据集。让我们把它变成一个分散的/联合的数据集。</p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="718e" class="mf kk hh mb b fi mg mh l mi mj">#For the training dataset, we need to divide it across the virtual clients we had created previously</span><span id="2824" class="mf kk hh mb b fi ml mh l mi mj">#so instead of using a normal dataloader, using a Federated dataloader that loads the MNIST data and also dividies it across the created clients</span><span id="482d" class="mf kk hh mb b fi ml mh l mi mj">federated_train_loader = sy.FederatedDataLoader(<br/>datasets.MNIST('../data', train=True, download=True,<br/>transform=transforms.Compose([<br/>transforms.ToTensor(),<br/>transforms.Normalize((0.1307,), (0.3081,))<br/>])).federate((client0,client1,client2,client3)),<br/>batch_size=args.batch_size, shuffle=True)</span><span id="2a0d" class="mf kk hh mb b fi ml mh l mi mj">#As for the testing dataset, it will be tested on the global model so no need to divide it across the clients</span><span id="533b" class="mf kk hh mb b fi ml mh l mi mj">test_loader = torch.utils.data.DataLoader(<br/>datasets.MNIST('../data', train=False, transform=transforms.Compose([<br/>transforms.ToTensor(),<br/>transforms.Normalize((0.1307,), (0.3081,))<br/>])),<br/>batch_size=args.batch_size, shuffle=True)</span></pre><p id="5670" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">让我们定义模型</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="c1b0" class="mf kk hh mb b fi mg mh l mi mj"># model architecture</span><span id="cc90" class="mf kk hh mb b fi ml mh l mi mj"># convolutional layer 1 -&gt; maxpool(2,2) -&gt; relu</span><span id="43cc" class="mf kk hh mb b fi ml mh l mi mj"># convolutional layer 2 -&gt; maxpool(2,2) -&gt; relu</span><span id="1ad6" class="mf kk hh mb b fi ml mh l mi mj"># fullly connected layer 1 -&gt; relu</span><span id="a95e" class="mf kk hh mb b fi ml mh l mi mj"># output layer -&gt; log_softmax</span><span id="77a1" class="mf kk hh mb b fi ml mh l mi mj">class Net(nn.Module):</span><span id="39b7" class="mf kk hh mb b fi ml mh l mi mj">def __init__(self):</span><span id="17f2" class="mf kk hh mb b fi ml mh l mi mj">super(Net, self).__init__()<br/>self.conv1 = nn.Conv2d(1, 20, 5, 1)<br/>self.conv2 = nn.Conv2d(20, 50, 5, 1)<br/>self.fc1 = nn.Linear(4*4*50, 500)<br/>self.fc2 = nn.Linear(500, 10)</span><span id="fafc" class="mf kk hh mb b fi ml mh l mi mj">def forward(self, x):</span><span id="182d" class="mf kk hh mb b fi ml mh l mi mj">x = F.relu(self.conv1(x))<br/>x = F.max_pool2d(x, 2, 2)<br/>x = F.relu(self.conv2(x))<br/>x = F.max_pool2d(x, 2, 2)<br/>x = x.view(-1, 4*4*50)<br/>x = F.relu(self.fc1(x))<br/>x = self.fc2(x)</span><span id="a9a6" class="mf kk hh mb b fi ml mh l mi mj">return F.log_softmax(x, dim=1)</span><span id="a109" class="mf kk hh mb b fi ml mh l mi mj"># Computing log_softmax() is slightly less likely to fail due to arithmetic overflow or underflow than computing softmax().</span><span id="c93b" class="mf kk hh mb b fi ml mh l mi mj"># Using log_softmax() is slightly more efficient than using softmax() when computing negative log likelihood loss (also called cross entropy error).</span></pre><p id="86bc" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">定义培训和测试功能</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="59d8" class="mf kk hh mb b fi mg mh l mi mj">def train(args, model, federated_train_loader, optimizer, epoch,train_loss_batch,train_loss_epoch,id0,id1,id2,id3):</span><span id="8160" class="mf kk hh mb b fi ml mh l mi mj">model.train()</span><span id="ad81" class="mf kk hh mb b fi ml mh l mi mj">client = None</span><span id="02a3" class="mf kk hh mb b fi ml mh l mi mj">t_loss = 0</span><span id="cb28" class="mf kk hh mb b fi ml mh l mi mj">total = 0</span><span id="f4db" class="mf kk hh mb b fi ml mh l mi mj">for batch_idx, (data, target) in enumerate(federated_train_loader):</span><span id="3176" class="mf kk hh mb b fi ml mh l mi mj">#The dataset we are loading is federated/distributed, means it is divided into different clients</span><span id="56ba" class="mf kk hh mb b fi ml mh l mi mj">#We need to send each of the batch to the corresponding client</span><span id="76b9" class="mf kk hh mb b fi ml mh l mi mj">if client == None :<br/>client = data.location.id</span><span id="99dd" class="mf kk hh mb b fi ml mh l mi mj">print('Processed at :',client)</span><span id="2d38" class="mf kk hh mb b fi ml mh l mi mj">elif client != data.location.id :</span><span id="43db" class="mf kk hh mb b fi ml mh l mi mj">client = data.location.id</span><span id="4231" class="mf kk hh mb b fi ml mh l mi mj">print('Processed at :',client)</span><span id="3eef" class="mf kk hh mb b fi ml mh l mi mj">model.send(data.location) </span><span id="508c" class="mf kk hh mb b fi ml mh l mi mj">optimizer.zero_grad()</span><span id="1585" class="mf kk hh mb b fi ml mh l mi mj">output = model(data)</span><span id="07e5" class="mf kk hh mb b fi ml mh l mi mj">loss = F.nll_loss(output, target)</span><span id="92bf" class="mf kk hh mb b fi ml mh l mi mj">loss.backward()</span><span id="24bb" class="mf kk hh mb b fi ml mh l mi mj">optimizer.step()</span><span id="b92b" class="mf kk hh mb b fi ml mh l mi mj">model.get() # recieve the model back from the client<br/>if batch_idx % 30 == 0:<br/>loss = loss.get() # getting the loss back from the last client<br/>t_loss += loss<br/>total += 1</span><span id="0e3c" class="mf kk hh mb b fi ml mh l mi mj">train_loss_batch.append(loss.item())</span><span id="463b" class="mf kk hh mb b fi ml mh l mi mj">if client == 'client0':<br/>id0.append(loss.item())<br/>elif client == 'client1':<br/>id1.append(loss.item())<br/>elif client == 'client2':<br/>id2.append(loss.item())<br/>elif client == 'client3':<br/>id3.append(loss.item())</span><span id="7a93" class="mf kk hh mb b fi ml mh l mi mj">print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(<br/>epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,<br/>100. * batch_idx / len(federated_train_loader), loss.item()))<br/>t_loss /= total</span><span id="558c" class="mf kk hh mb b fi ml mh l mi mj">train_loss_epoch.append(t_loss.item())</span><span id="38a4" class="mf kk hh mb b fi ml mh l mi mj">def test(args, model, test_loader,testing_loss):</span><span id="0d0d" class="mf kk hh mb b fi ml mh l mi mj">model.eval()</span><span id="7e0b" class="mf kk hh mb b fi ml mh l mi mj">test_loss = 0</span><span id="18ea" class="mf kk hh mb b fi ml mh l mi mj">correct = 0</span><span id="f568" class="mf kk hh mb b fi ml mh l mi mj">with torch.no_grad():</span><span id="065e" class="mf kk hh mb b fi ml mh l mi mj">for data, target in test_loader:</span><span id="1f10" class="mf kk hh mb b fi ml mh l mi mj">output = model(data)</span><span id="2594" class="mf kk hh mb b fi ml mh l mi mj">test_loss += F.nll_loss(output, target, reduction='sum').item() #reduction means that we just sum up the loss of the whole batch</span><span id="6fa3" class="mf kk hh mb b fi ml mh l mi mj">pred = output.argmax(1, keepdim=True)</span><span id="3c9b" class="mf kk hh mb b fi ml mh l mi mj">correct += pred.eq(target.view_as(pred)).sum().item()<br/>test_loss /= len(test_loader.dataset)<br/>testing_loss.append(test_loss)<br/>print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(</span><span id="0950" class="mf kk hh mb b fi ml mh l mi mj">test_loss, correct, len(test_loader.dataset),<br/>100. * correct / len(test_loader.dataset)))</span></pre><p id="ff55" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">主要功能</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="22e1" class="mf kk hh mb b fi mg mh l mi mj">model = Net()</span><span id="6efa" class="mf kk hh mb b fi ml mh l mi mj">optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)</span><span id="3ba6" class="mf kk hh mb b fi ml mh l mi mj">train_loss_batch = []<br/>train_loss_epoch = []</span><span id="76f2" class="mf kk hh mb b fi ml mh l mi mj">id0 = []<br/>id1 = []<br/>id2 = []<br/>id3 = []</span><span id="1276" class="mf kk hh mb b fi ml mh l mi mj">testing_loss = []</span><span id="cf2f" class="mf kk hh mb b fi ml mh l mi mj">for epoch in range(0, args.epochs):<br/>train(args, model, federated_train_loader, optimizer, epoch,train_loss_batch,train_loss_epoch,id0,id1,id2,id3)</span><span id="6362" class="mf kk hh mb b fi ml mh l mi mj">test(args, model, test_loader,testing_loss)</span></pre><p id="19d9" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">标绘结果</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="32c8" class="mf kk hh mb b fi mg mh l mi mj">from matplotlib import pyplot as plt</span><span id="e5a1" class="mf kk hh mb b fi ml mh l mi mj">plt.plot(train_loss_batch)</span><span id="50f1" class="mf kk hh mb b fi ml mh l mi mj">plt.title("Training Loss in batches")</span><span id="60eb" class="mf kk hh mb b fi ml mh l mi mj">plt.show()</span><span id="92d5" class="mf kk hh mb b fi ml mh l mi mj">plt.plot(train_loss_epoch)<br/>plt.title("Training Loss per epoch")</span><span id="edde" class="mf kk hh mb b fi ml mh l mi mj">plt.show()</span><span id="3c43" class="mf kk hh mb b fi ml mh l mi mj">f = plt.figure()</span><span id="6835" class="mf kk hh mb b fi ml mh l mi mj">f.set_figwidth(10)</span><span id="3c3f" class="mf kk hh mb b fi ml mh l mi mj">f.set_figheight(8)</span><span id="9dba" class="mf kk hh mb b fi ml mh l mi mj">plt.plot(id0,'r',label = 'client 0')<br/>plt.plot(id1,'g',label = 'client 1')<br/>plt.plot(id2,'b',label = 'client 2')<br/>plt.plot(id3,'y',label = 'client 3')</span><span id="2744" class="mf kk hh mb b fi ml mh l mi mj">plt.legend()</span><span id="c13c" class="mf kk hh mb b fi ml mh l mi mj">plt.title("Client Losses during training wrt batches")</span><span id="8754" class="mf kk hh mb b fi ml mh l mi mj">plt.show()</span></pre><p id="08e7" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated"><strong class="iz hi">保存模型</strong></p><pre class="im in io ip fd ma mb mc md aw me bi"><span id="b126" class="mf kk hh mb b fi mg mh l mi mj">torch.save(model.state_dict(), "model.pt")</span></pre><blockquote class="ka kb kc"><p id="3662" class="ix iy kd iz b ja jb jc jd je jf jg jh ke jj jk jl kf jn jo jp kg jr js jt ju ha bi translated"><strong class="iz hi">结果</strong></p></blockquote><p id="629c" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">以下是模型训练后的结果</p><figure class="im in io ip fd iq er es paragraph-image"><div class="er es mm"><img src="../Images/6c21787061bc8ec9f4f4985e4c16409b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*B_dsZtH5sOaP9P2yKUe_vQ.png"/></div></figure><figure class="im in io ip fd iq er es paragraph-image"><div class="er es mn"><img src="../Images/bb19b0b0f3d31b54e957af0061e9fcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*d-z-_kJ6TziO1UO3lJpRYQ.png"/></div></figure><figure class="im in io ip fd iq er es paragraph-image"><div class="er es mo"><img src="../Images/455cae39f7ce5472a8d5664e9506d603.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*VbIqAzzHg0LtcFf58RJ5kg.png"/></div></figure><figure class="im in io ip fd iq er es paragraph-image"><div class="er es mo"><img src="../Images/814ce7778266d1b44eac045d16a1a0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*gRq7yvboWTRLqITRTbnMrA.png"/></div></figure><p id="c930" class="pw-post-body-paragraph ix iy hh iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju ha bi translated">总之，联合学习是一种使我们能够在分散的数据集上训练模型的技术。如果处理得当，这有助于确保更好的数据隐私以及更好的资源管理。谷歌智能键盘(Gboard)也采用了这种技术。联合学习还有助于公司在不共享数据或建立集中式数据中心的情况下相互训练模型。</p><blockquote class="ka kb kc"><p id="98dc" class="ix iy kd iz b ja jb jc jd je jf jg jh ke jj jk jl kf jn jo jp kg jr js jt ju ha bi translated"><strong class="iz hi">演职员表和参考资料</strong></p></blockquote><div class="mp mq ez fb mr ms"><a href="https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">用于图像分类的联合学习|张量流联合</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">此colab已经过验证，可与注:tensorflow_federated pip包的最新发布版本一起使用…</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">www.tensorflow.org</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng iv ms"/></div></div></a></div><div class="mp mq ez fb mr ms"><a href="https://colab.research.google.com/github/tensorflow/federated/blob/v0.3.0/docs/tutorials/federated_learning_for_image_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">谷歌联合实验室</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">编辑描述</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">colab.research.google.com</p></div></div><div class="nb l"><div class="nh l nd ne nf nb ng iv ms"/></div></div></a></div><div class="mp mq ez fb mr ms"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hi fi z dy mx ea eb my ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">medium.com</p></div></div><div class="nb l"><div class="ni l nd ne nf nb ng iv ms"/></div></div></a></div></div></div>    
</body>
</html>