# 双重下降

> 原文：<https://medium.com/mlearning-ai/double-descent-8f92dfdc442f?source=collection_archive---------1----------------------->

机器学习的突破正在迅速改变社会，但偏差-方差权衡，机器学习中最基本的概念之一，似乎与更现代的模型中观察到的现象不一致，如神经网络和梯度推进。最近的两篇论文[【1】](https://arxiv.org/pdf/1912.02292.pdf)和[【2】](https://arxiv.org/pdf/1812.11118.pdf)试图通过在统一的性能曲线内调和经典理解和现代实践来弥合这一鸿沟。

# 传统观点——“越大的模型越差”

统计学中的传统观点是“模型越大越差”，这由以下偏差方差权衡曲线显示:

![](img/670e9dded9142495a4a9f5ec307b400c.png)

当我们增加假设类(H)的容量时，即当我们使我们的模型更复杂时，模型过度拟合训练数据，因此训练风险/误差达到 0。测试风险/错误也遵循相同的趋势，直到我们到达“甜蜜点”，它描述了我们用例的最佳模型，在这一点之后，测试风险增加。

机器学习中的传统智慧建议通过平衡欠拟合和过拟合，基于偏差-方差权衡来控制函数类 H 的容量。

*   如果 H 太小，H 中的所有预测器可能不符合训练数据(即，具有大的经验风险)，因此对新数据的预测很差。
*   如果 H 太大，经验风险最小化器可能过度拟合训练数据中的虚假模式，导致新示例的准确性差(经验风险小，真实风险大)。

这条曲线的教科书推论是:

> 训练误差为零的模型会过度拟合训练数据，并且通常会概括得很差

然而，实践者通常使用越来越大的神经网络来提高测试集的准确性，而不改变任何其他参数，为什么它会提高准确性呢？现代机器学习方法不遵循偏差方差曲线吗？事实上，许多实践者试图达到零训练误差，这与现代的过度适应有着经典的关联。那么到底是什么改变了呢？

# 双重下降

随着我们增加假设类的复杂性，现代机器学习方法表现出稍微不同的现象。最初的 U 形曲线符合经典的理解，但是超过某个点(称为插值阈值)，测试风险再次开始降低。米哈伊尔·贝尔金等人[【1】](https://arxiv.org/pdf/1812.11118.pdf)的论文将这种现象描述为“双重下降”。

本文将曲线分为两个区域，经典区域(基于经典理解)也称为欠参数区域，现代插值区域也称为“过参数”区域。现代偏差-方差权衡曲线见下图:

![](img/dd106ec8f73498efaf10a7dd2a51e96d.png)

Double Descent Curve

在现代机器学习方法(例如深度神经网络)中，模型的复杂性也与模型中可学习参数的数量有关。超过某个阈值，如果我们增加可学习参数的数量，我们将达到这种现代插值机制，其中曲线显示测试风险/误差的下降趋势。

# 达到插值阈值需要多少个参数？

虽然它取决于输入特征和许多其他因素，但本文观察到，当参数的数量大致等于样本的数量时，经典的偏差方差权衡 U 曲线的峰值出现，此后我们可以观察到现代的双重下降机制。

通常使用具有大量参数的神经网络。但是为了实现单个输出(回归或两类分类)的插值，人们期望至少需要与数据点一样多的参数。此外，如果预测问题有一个以上的输出(如在多类分类中)，则所需参数的数量应乘以输出的数量。对于神经网络来说，这确实是经验上的情况。因此，例如，像 ImageNet 这样大的数据集(有 1，000，000 个示例和 103 个类)可能需要具有 1，000，000，000 个参数的网络来实现插值；这比 ImageNet 的许多神经网络模型都要大。

# 真实例子的结果

本文进行了几个实验来证明这种双下降曲线的存在，其中一个实验是在著名的 MNIST 数据集上进行的，作者观察了下面的误差与参数数量的关系图:

![](img/578c8f1d539fab198f7303c2e31821a7.png)

对于 MNIST，n(样本数)= 4000，d(维数)= 784，K(类别数)= 10。作者根据预测的参数数量(n.K)观察到插值阈值(在上图中用虚线表示)。(前面提到过。)

同样如预期的，测试误差在插值阈值之后开始减小。

# 双下降只对神经网络观察到吗？

该论文还指出，类似的现象也见于其他一些机器学习方法，如 AdaBoost 和 Random Forrest。更一般地，作者指出，有证据表明，通过增强决策树和随机福里斯特探索的函数族也有类似的行为。

Abraham J. Wyner 等人[【4】](https://www.jmlr.org/papers/volume18/15-240/15-240.pdf)的一篇期刊给出了经验证据，即当 AdaBoost 和随机森林与最大(插值)决策树一起使用时，拟合方法的灵活性产生的插值预测值比刚性非插值方法(例如，AdaBoost 或具有浅树的随机森林)产生的预测值对训练数据中的噪声更鲁棒。据说这反过来会产生更好的概括。(近)插值树的平均确保了所得到的函数比任何单独的树都要平滑得多，这与许多现实世界问题兼容的归纳偏差相一致。

# 随机福里斯特的结果

作者再次使用 MNIST 数据集的子集进行分类，并观察到以下误差与参数数量的关系图:

![](img/926674da4a26ca2721e89f2eeff769c8.png)

随机森林的复杂度由树的数量(Ntree)和每棵树允许的最大叶子数量(Nmaxleaf)控制。从上图可以看出，在达到一定的模型复杂度(插值阈值)后，测试误差开始下降。

# 更正式的方法

Preetum Nakkiran 等人[【2】](https://arxiv.org/pdf/1912.02292.pdf)的论文表明，双重下降是一种健壮的现象，出现在各种任务、架构和优化方法中。除此之外，本文还展示了一个更一般的双下降概念，它不仅仅依赖于参数的数量。

# 有效模型复杂性(EMC)

本文将训练过程的有效模型复杂度定义为模型达到接近 0 训练误差的最大样本数。有效模型的复杂性不仅取决于数据分布和分类器的架构，还取决于训练过程，特别是增加训练时间会增加 EMC。

形式上有效的模型复杂性定义为:

![](img/ba5cc6c04b76bb4cf8277f2014bd5138.png)

这里，训练过程/程序 T 被定义为接受样本列表并输出将数据映射到标签的分类器 T(S)的任何程序。

非正式地说，EMC 可以分为三个部分:

*   在参数化状态下:如果 EMC 充分小于 n，增加其有效复杂性的训练过程 T 的任何扰动将减少测试误差。
*   临界状态:如果 EMC 大约等于训练样本的数量 n，那么增加其有效复杂性的训练过程 T 的扰动可能减少或增加测试误差。
*   过参数化状态:如果 EMC 充分大于 n，增加其有效复杂性的训练过程 T 的任何扰动将减少测试误差。

这是 Belkin 等人在第一篇论文中观察到的结果的延伸。

# 更多标签噪声的影响

当训练完成时(对于固定的大量优化步骤)，该论文执行了几个实验来研究增加规模的模型的测试误差。临界区域在插值点周围表现出明显不同的测试行为，并且通常在测试误差中存在峰值，该峰值在具有标签噪声的设置中变得更加突出。这可以在下图中看到(设置—在 Resnet18 上训练的 Cifar10):

![](img/81e908b7a96baa8989ed2fe5d09e74c8.png)

# 有时候数据多了会疼？

该论文描述了在没有添加标签噪声的情况下对语言翻译任务执行的实验，其中添加更多数据实际上会在特定机制中损害性能。如下图所示:

![](img/5e77b7cca9bf574d1c3b8ead1a516f47.png)

增加样本数量会使曲线向更低的测试误差方向下移。然而，由于更多的样本需要更大的模型来拟合，因此增加样本数量也会将插值阈值(和测试误差峰值)向右移动。

对于中等大小的模型(红色箭头)，这两种影响结合在一起，我们看到对 4.5 倍以上的样本进行训练实际上会损害测试性能。

# 训练时间更长可以逆转过度拟合？

该论文指出，更长时间的训练实际上可以扭转过度拟合！

![](img/801524ef954d528e02cf106f9d68bcc4.png)

上面的图表显示了作为模型大小和优化步骤数量的函数的测试和训练误差。对于给定数量的优化步骤(固定的 y 坐标)，测试和训练误差呈现出模型大小的双重下降。对于给定的模型尺寸(固定的 x 坐标)，随着训练的进行，测试和训练误差减小、增大、再减小；论文称这种现象为**划时代双下降**。

> *一般来说，当模型勉强能够适应训练集时，测试误差的峰值会系统性地出现。*

# 结论

这两篇论文都提出了一个非常有趣的想法，并试图弥合机器学习方法的理论和实践之间的差距。Deep Double Descent 的论文进一步指出了某些传统方法可能不总是给出最佳结果的情况，作为一个例子，作者提到，与一般的看法(“数据越多越好”)相反，更多的数据可能实际上导致更差的测试准确性。研究人员应该明白，这种情况只发生在特定的范围内，进一步增加有效模型的复杂性实际上可能会导致更好的结果。

尽管完全理解双重下降背后的机制是一个开放的问题，我希望从业者和研究人员在建立他们的模型时记住这一现象。

# 参考

有关更多详细信息，请参考以下参考资料:

[[1]调和现代机器学习实践和偏差-方差权衡，Mikhail Belkin 等人](https://arxiv.org/pdf/1812.11118.pdf)

[[2]深度双重下降:更大的模型和更多的数据带来的伤害——Preetum nak kiran 等人](https://arxiv.org/pdf/1912.02292.pdf)

[[3]解释 adaboost 和随机森林作为插值分类器的成功。—大卫·米纳斯等人](https://www.jmlr.org/papers/volume18/15-240/15-240.pdf)

[4]https://openai.com/blog/deep-double-descent/