<html>
<head>
<title>Optimizing Machine Learning Models with GridSearchCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用GridSearchCV优化机器学习模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/optimizing-machine-learning-models-with-gridsearchcv-c3ff518c3a48?source=collection_archive---------2-----------------------#2022-08-20">https://medium.com/mlearning-ai/optimizing-machine-learning-models-with-gridsearchcv-c3ff518c3a48?source=collection_archive---------2-----------------------#2022-08-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1f98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">开发机器学习模型的主要挑战之一是选择最佳模型超参数。超参数是定义模型算法将如何学习的细节的设置。根据模型的不同，它们可以控制学习功能，例如算法如何迭代解决方案、计算内部功能或对预测进行加权。</p><p id="13e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然大多数ML模型都有很好的记录，超参数选择仍然很迟钝。选择最佳设置通常需要迭代不同的选项，看看它们如何影响模型精度，这个过程称为<em class="jc">网格搜索</em>。幸运的是，Scikit-learn已经开发了<code class="du jd je jf jg b"><a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=grids#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code> <a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=grids#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank">工具</a>来自动化这个过程。<code class="du jd je jf jg b">GridSearchCV</code>还会在遍历选项时存储超参数组合的精确度分数，从而方便地探索这些选项对模型的影响。让我们看看它是如何工作的。</p><p id="7ad8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个工作流程是在Python 3.10中创建的，相应的Jupyter笔记本可以在<a class="ae jh" href="https://github.com/RDhoelzle/gridsearchcv_optimization" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl ji jj go jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="ha hb hc hd he"><h1 id="78a0" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">1.导入和处理数据</h1><p id="2430" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">该项目使用了<a class="ae jh" href="https://www.kaggle.com/datasets/mdmahmudulhasansuzan/students-adaptability-level-in-online-education" rel="noopener ugc nofollow" target="_blank"> kaggle </a>上的“在线教育中的学生适应性水平”公共数据集，并在<em class="jc"> 2021年第12届国际计算、通信和网络技术会议(ICCCNT) </em>会议录(DOI:10.1109/ICC CNT 51525 . 2021 . 579741)中发表为<a class="ae jh" href="https://ieeexplore.ieee.org/document/9579741" rel="noopener ugc nofollow" target="_blank">使用机器学习方法的在线教育中的学生适应性水平预测</a>。在这里，作者收集了一系列在线学习学生的人口统计、经济和其他背景数据，以及他们对在线学习适应性的个人得分。然后作者利用这些数据，通过建立机器学习分类器模型来预测新生的适应水平。他们比较了一系列模型的准确性，包括<code class="du jd je jf jg b">K-Nearest Neighbors</code>、<code class="du jd je jf jg b">Decision Tree</code>、<code class="du jd je jf jg b">Random Forest</code>、<code class="du jd je jf jg b">Support Vector Machine</code>、<code class="du jd je jf jg b">Artificial Neural Network</code>和<code class="du jd je jf jg b">Naive Bayes</code>，为我们开发和比较自己的分类器模型提供了很好的参考点。</p><p id="2f1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从下载并导入数据作为一个<code class="du jd je jf jg b">pandas</code>数据帧开始。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="7000" class="la jq hh jg b fi lb lc l ld le">#download and import data<br/>import wget<br/>import os.path<br/>import pandas as pd</span><span id="1214" class="la jq hh jg b fi lf lc l ld le">file_path = 'data/students_adaptability_level_online_education.csv'<br/>if not os.path.isfile(file_path):<br/>    url = '<a class="ae jh" href="https://www.kaggle.com/datasets/mdmahmudulhasansuzan/students-adaptability-level-in-online-education?select=students_adaptability_level_online_education.csv'" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/mdmahmudulhasansuzan/students-adaptability-level-in-online-education?select=students_adaptability_level_online_education.csv'</a><br/>    wget.download(url, out = file_path)<br/>    <br/>df = pd.read_csv(file_path)<br/>df.head()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lg"><img src="../Images/66662805f7ea1314a94e8df71ac7df2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3c-PZx7xAXTWDyQ32W_JA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><p id="4cbd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看起来我们所有的变量都是对象。让我们在处理数据之前确认一下。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="6083" class="la jq hh jg b fi lb lc l ld le">#check data structure<br/>df.dtypes</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ls"><img src="../Images/49c555028b80364d87d87dc8a068af2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*A6aLyNefsGXKHpx_xXvZXg.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><p id="6bd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了构建和探索一系列分类器模型，我们需要将数据转换为编码整数。首先，让我们按字母顺序对每个变量的<code class="du jd je jf jg b">value_counts</code>进行排序，并将它们保存到变量列表的字典中。然后，我们将使用每个值的列表索引作为新编码数据帧中的整数。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="9f90" class="la jq hh jg b fi lb lc l ld le">#check distribution of all data values and list values in dict<br/>values_dict = {}<br/>for column in df:<br/>    values_dict[column] = df[column].value_counts().sort_index().index.to_list()</span><span id="ade1" class="la jq hh jg b fi lf lc l ld le">#inspect values<br/>values_dict</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es lt"><img src="../Images/5c45dc705a040c62aa414a0837a6bd1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*7RM7YAgFzhtgoihYxhdPGw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><p id="7409" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">年龄</strong>、<strong class="ig hi">教育程度</strong>、<strong class="ig hi">财务状况</strong>依次为。也就是说他们有一个自然的等级顺序。由于这些值当前是按字母顺序排列的，因此当前列出的顺序与它们的自然排序不匹配。虽然这不会影响我们的分类器模型的准确性，但它确实使数据探索和模型解释变得复杂。为了让接下来的事情变得更容易，让我们在处理最终的编码数据帧之前，手动对这些值进行重新排序。请记住，在构建实时模型时，所有新数据都需要以同样的方式进行预处理。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="4e54" class="la jq hh jg b fi lb lc l ld le">#reorder lists<br/>order = [0,5,1,2,3,4]<br/>values_dict[‘Age’] = [values_dict[‘Age’][i] for i in order]</span><span id="2260" class="la jq hh jg b fi lf lc l ld le">order = [1,0,2]<br/>values_dict[‘Education Level’] = [values_dict[‘Education Level’][i] for i in order]<br/>values_dict[‘Financial Condition’] = [values_dict[‘Financial Condition’][i] for i in order]</span><span id="6513" class="la jq hh jg b fi lf lc l ld le">order = [1,0]<br/>values_dict[‘Institution Type’] = [values_dict[‘Institution Type’][i] for i in order]<br/>values_dict[‘Load-shedding’] = [values_dict[‘Load-shedding’][i] for i in order]</span><span id="38c5" class="la jq hh jg b fi lf lc l ld le">order = [1,2,0]<br/>values_dict[‘Adaptivity Level’] = [values_dict[‘Adaptivity Level’][i] for i in order]</span><span id="0afe" class="la jq hh jg b fi lf lc l ld le">#build integer-coded dataframe<br/>coded_dict = {}<br/>for column in df:<br/>    coded_dict[column] = []<br/>    for i in range(0,df.shape[0]):<br/>        coded_dict[column].append(values_dict[column].index(df[column][i]))</span><span id="193a" class="la jq hh jg b fi lf lc l ld le">coded_df = pd.DataFrame(coded_dict)<br/>coded_df.head()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lg"><img src="../Images/1a795f7b3e99ef6d462f67cfcc05cef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xU9xgMK_09eqdQFnzBog-w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure></div><div class="ab cl ji jj go jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="ha hb hc hd he"><h1 id="3ff0" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2.数据探索</h1><p id="703e" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">既然数据是整数编码的，我们可以在数据集中寻找任何明显的趋势。我总是喜欢从相关矩阵开始，它可以快速可视化相关变量。在这里，我们将使用<em class="jc"> Spearmans的等级相关性</em>来说明这些是分级整数的事实，并且我们将把结果矩阵绘制成热图。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="f230" class="la jq hh jg b fi lb lc l ld le">#plot correlation matrix<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="5929" class="la jq hh jg b fi lf lc l ld le">cmap = sns.color_palette(“vlag”, as_cmap=True).reversed()<br/>sns.heatmap(coded_df.corr(method=’spearman’), cmap=cmap, vmin=-1, vmax=1)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es lu"><img src="../Images/a7f48dd70f59c05d6f054f2ed47efcdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*xIlz14ztQZUbycos63P53g.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Correlation heatmap of coded data.</figcaption></figure><p id="9974" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">鉴于获得教育需要时间(我们大多数人进步的速度都差不多)，毫不奇怪<strong class="ig hi">的年龄</strong>和<strong class="ig hi">的教育水平</strong>相互之间有很强的相关性。然而，与<strong class="ig hi">适应性水平</strong>没有很强的个体相关性，表明需要进行多变量探索。值得注意的是，响应变量与<strong class="ig hi">课时</strong>、<strong class="ig hi">财务状况</strong>和<strong class="ig hi">位置</strong>之间存在微弱的正相关，这似乎表明，生活在城镇中、接受更多教师互动的资源丰富的学生适应得最好。与<strong class="ig hi">机构类型</strong>也存在微弱的负相关，表明民办学校比公办学校具有更高的适应性。</p><p id="5e82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于没有一个变量与<strong class="ig hi">适应性水平</strong>有很强的相关性，让我们纵坐标(维度减少)数据，看看在整个数据结构中是否有任何聚类。首先，我们将数据标准化。然后我们将使用<code class="du jd je jf jg b">Principal Component Analysis</code>进行排序，它沿着可变性最大的维度投影全部数据(在这里阅读更多关于排序的信息<a class="ae jh" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank">)。</a></p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="f1e8" class="la jq hh jg b fi lb lc l ld le">#split X and Y, and standardize X<br/>import numpy as np<br/>import sklearn<br/>from sklearn import preprocessing</span><span id="5402" class="la jq hh jg b fi lf lc l ld le">Y = coded_df[‘Adaptivity Level’].to_numpy()<br/>X = coded_df.loc[:, coded_df.columns != ‘Adaptivity Level’].to_numpy()<br/>xform = preprocessing.StandardScaler()<br/>X_z = xform.fit(X).transform(X)</span><span id="4795" class="la jq hh jg b fi lf lc l ld le">#define pca plot fuction<br/>def plot_pca(ordi,lab,y):<br/>    '''<br/>    Generates biplot of 1st and 2nd axes from an ordination model<br/>    '''<br/>    plt.figure()<br/>    plt.scatter(ordi[y==0, 0], ordi[y==0, 1], color='red', label='Low')<br/>    plt.scatter(ordi[y==1, 0], ordi[y==1, 1], color='blue', label='Moderate')<br/>    plt.scatter(ordi[y==2, 0], ordi[y==2, 1], color='green', label='High')<br/>    plt.xlabel(lab[0])<br/>    plt.ylabel(lab[1])<br/>    plt.legend()<br/>    plt.show()</span><span id="2470" class="la jq hh jg b fi lf lc l ld le">#train and plot PCA<br/>from sklearn.decomposition import PCA</span><span id="a9ef" class="la jq hh jg b fi lf lc l ld le">pca = PCA()<br/>X_pca = pca.fit_transform(X_z)<br/>labels = [f"PCA1 ({pca.explained_variance_ratio_[0]*100:.1f}%)",<br/>          f"PCA2 ({pca.explained_variance_ratio_[1]*100:.1f}%)"]<br/>plot_pca(X_pca,labels,Y)</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es lv"><img src="../Images/7e349e4cb4c484106a2154da64e23886.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*y3wgQE8_K41hkLiz5awbzg.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Principle component analysis biplot, colored by <strong class="bd jr">Adaptivity Level</strong></figcaption></figure><p id="e9e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然有一个轻微的明显趋势，即<em class="jc">向左上方的适应性低</em>和<em class="jc">向右下方的适应性高</em>，但是这个趋势是微弱的和高度混合的。第一个和第二个<em class="jc">主成分</em>仅占总数据可变性的39.0%，因此我们无法从简单排序的二维数据中获得有意义的分离。还有其他排序方法，它们应用更结构化的数据投影，试图最大化固有的数据结构。<code class="du jd je jf jg b">t-Distributed Stochastic Neighbor Embedding</code>就是这样一种尝试最大化自然数据聚类的方法。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="950d" class="la jq hh jg b fi lb lc l ld le">#train and plot tSNE<br/>from sklearn.manifold import TSNE</span><span id="3e61" class="la jq hh jg b fi lf lc l ld le">tsne = TSNE(n_components=2, learning_rate=300, perplexity=30, early_exaggeration=12, init=’random’, random_state=2019)<br/>X_tsne = tsne.fit_transform(X_z)<br/>labels = [“tSNE1”,”tSNE2"]<br/>plot_pca(X_tsne,labels,Y)</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es lw"><img src="../Images/f7f1a97c08698d07bc71596aa4c8ed25.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*QDYqTMLv3KTN753R1gZgpQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">t-Distributed stochastic neighbor embedding biplot, colored by <strong class="bd jr">Adaptivity Level</strong></figcaption></figure><p id="88dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">显然有许多相似的学生群，但他们并不根据适应性水平来区分。让我们看看我们能用一些常见的ML分类算法做些什么。在这里，我们将使用<code class="du jd je jf jg b">Logistic Regression</code>、<code class="du jd je jf jg b">K-Nearest Neighbors</code>、<code class="du jd je jf jg b">Decision Tree</code>、<code class="du jd je jf jg b">Random Forest</code>、<code class="du jd je jf jg b">Support Vector Machine</code>和<code class="du jd je jf jg b">Artificial Neural Network</code>。在最初的出版物中，作者没有使用<code class="du jd je jf jg b">Logistic Regression</code>，尽管它允许多分类，但对于n &gt; 2类来说，它的性能通常不太好。然而，他们确实建立了一个<code class="du jd je jf jg b">Naive Bayes</code>模型，该模型假设独立变量之间互不影响。然而，我们可以合理地假设某些变量之间的相互作用，例如受教育程度越高的学生年龄越大，经济富裕的学生越有可能负担得起更高的网速和更昂贵的设备。因此，对于该数据集来说，这不是一个有效的假设，因此我们将不包括<code class="du jd je jf jg b">Naive Bayes</code>模型。</p></div><div class="ab cl ji jj go jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="ha hb hc hd he"><h1 id="7986" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3.基于GridSearchCV的模型优化</h1><p id="e143" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">所有机器学习算法都有一系列超参数，这些参数会影响它们如何构建模型。其中包括<code class="du jd je jf jg b">regularization parameters</code>、<code class="du jd je jf jg b">scaling values</code>、<code class="du jd je jf jg b">solver algorithms</code>、<code class="du jd je jf jg b">tree depth</code>和<code class="du jd je jf jg b">number of neighbors</code>等等。这些超参数的最佳设置很少是显而易见的，因此有必要在这些设置的范围内迭代和测量模型精度。<code class="du jd je jf jg b">GridSearchCV</code>通过运行这些超参数的所有组合的复制<code class="du jd je jf jg b">cross-validations</code>，然后选择具有最高模型精度的超参数集，来自动化该过程。</p><p id="5d7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先将数据随机分为训练集(80%)和测试集(20%)。<code class="du jd je jf jg b">GridSearchCV</code>还将测试训练分割输入的训练数据，并因此报告样本内训练精度的准确度。为了比较和选择最佳的整体模型，我们还需要用测试集测量样本外的准确性。我们还将过滤掉警告消息，将打印输出限制为仅显示进度消息。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="f6a7" class="la jq hh jg b fi lb lc l ld le">#split train and test data sets<br/>from sklearn.model_selection import train_test_split</span><span id="d49a" class="la jq hh jg b fi lf lc l ld le">X_train, X_test, Y_train, Y_test = train_test_split(X_z, Y, test_size=0.2, random_state=2)</span><span id="8460" class="la jq hh jg b fi lf lc l ld le">#ignore all warnings<br/>from warnings import simplefilter</span><span id="11da" class="la jq hh jg b fi lf lc l ld le">simplefilter(action='ignore')</span></pre><p id="9ff9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们可以开始优化我们的第一个模型。先从<code class="du jd je jf jg b">Logistic Regression</code>说起吧。在建立新模型之前，最好先查看一下<a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">用户指南</a>中的超参数描述。对于我们的数据集，我们将优化<code class="du jd je jf jg b">C</code>、<code class="du jd je jf jg b">penalty</code>和<code class="du jd je jf jg b">solver</code>超参数，同时将<code class="du jd je jf jg b">multi_class</code>设置为<code class="du jd je jf jg b">multinomial</code>。这是使用<code class="du jd je jf jg b">parameters</code>字典完成的。然后，我们使用<code class="du jd je jf jg b">parameters</code>和<code class="du jd je jf jg b">Logisitic Regression</code>模型对象以及定义数量的<code class="du jd je jf jg b">cross-validations</code>创建一个<code class="du jd je jf jg b">GridSearchCV</code>对象(这里我们使用10个)。最后，我们通过拟合训练数据来优化我们的模型。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="940b" class="la jq hh jg b fi lb lc l ld le">#train logistic regression<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.linear_model import LogisticRegression</span><span id="150a" class="la jq hh jg b fi lf lc l ld le">parameters = {'C': np.logspace(-2, 0, 20),<br/>              'penalty': ['none', 'l2', 'l1', 'elasticnet'],<br/>              'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],<br/>              'multi_class': ['multinomial']}<br/>lr = LogisticRegression()<br/>grid_search = GridSearchCV(lr, parameters, cv=10, verbose=0)<br/>logreg_cv = grid_search.fit(X_train, Y_train)</span></pre><p id="783b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦<code class="du jd je jf jg b">GridSearchCV</code>完成优化我们的模型，我们可以检查我们的最终超参数值、样本内精度和样本外精度。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="dd51" class="la jq hh jg b fi lb lc l ld le">#test logistic regression<br/>print(“Tuned hpyerparameters (best parameters):”, logreg_cv.best_params_)<br/>print(“Train accuracy:”, logreg_cv.best_score_)<br/>print(“Test accuracy:”, logreg_cv.best_estimator_.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lx"><img src="../Images/a6663f4776e6d6ec90cf5ee85e404986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4oYtwReBnXKZ_KSpUUWPw.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><p id="39e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以提取每个测试超参数组合的样本内得分，以探索每个参数如何影响模型准确性。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="b1c9" class="la jq hh jg b fi lb lc l ld le">#plot logreg scores<br/>logreg_cv_df = pd.DataFrame(logreg_cv.cv_results_[‘params’])<br/>logreg_cv_df[‘score’] = logreg_cv.cv_results_[‘mean_test_score’]</span><span id="e819" class="la jq hh jg b fi lf lc l ld le">sns.lineplot(data=logreg_cv_df, x=’C’, y=’score’, hue=’penalty’)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ly"><img src="../Images/65db038d8af1f8e24ed8c60c44fdcaf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*HpKJc90x8sDPpAGeSoC0rw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Lineplot of <strong class="bd jr">C</strong> by <strong class="bd jr">score</strong> for different <strong class="bd jr">penalty</strong> values.</figcaption></figure><p id="afa5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最精确的<code class="du jd je jf jg b">Logistic Regression</code>模型是用l1和l2 <code class="du jd je jf jg b">pentalty</code>项以及大约为0.4的<code class="du jd je jf jg b">regularization</code>项实现的。尽管如此，在最大准确率为70%的情况下，这远远低于原始出版物中的最佳模型(使用<code class="du jd je jf jg b">Random Forest</code>模型获得了89.63%的准确率)。</p><p id="1e22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看使用<code class="du jd je jf jg b">K-Nearest Neighbors</code>是否能得到更好的结果。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="602c" class="la jq hh jg b fi lb lc l ld le">#train k-nearest neighbors<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="c826" class="la jq hh jg b fi lf lc l ld le">parameters = {'n_neighbors': list(range(1, 20)),<br/>              'weights': ['uniform', 'distance'],<br/>              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],<br/>              'p': [1,2]}<br/>knn = KNeighborsClassifier()<br/>grid_search = GridSearchCV(knn, parameters, cv=10, verbose=0)<br/>knn_cv = grid_search.fit(X_train, Y_train)</span><span id="a2e5" class="la jq hh jg b fi lf lc l ld le">#test k-nearest neighbors<br/>print("Tuned hpyerparameters (best parameters):", knn_cv.best_params_)<br/>print("Train accuracy:", knn_cv.best_score_)<br/>print("Test accuracy:", knn_cv.best_estimator_.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lz"><img src="../Images/e2827ac11eb4574232586b86311652e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxSAXdvnxCjHLdfqlz65Iw.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="6d9e" class="la jq hh jg b fi lb lc l ld le">#plot knn scores<br/>knn_cv_df = pd.DataFrame(knn_cv.cv_results_[‘params’])<br/>knn_cv_df[‘score’] = knn_cv.cv_results_[‘mean_test_score’]</span><span id="13ff" class="la jq hh jg b fi lf lc l ld le">sns.lineplot(data=knn_cv_df, x=’n_neighbors’, y=’score’, hue=’weights’, style=’algorithm’)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ly"><img src="../Images/650cfa729d755cf49eac5a4abda86c6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*80-31PdYCkOr-MQ1CYAMzQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Lineplot of <strong class="bd jr">n_neighbors</strong> by <strong class="bd jr">score</strong> for different <strong class="bd jr">weights</strong> and <strong class="bd jr">algorithm</strong> values.</figcaption></figure><p id="f40b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">邻域<code class="du jd je jf jg b">weighting</code>方法是对<code class="du jd je jf jg b">K-Nearest Neighbors</code>模型准确性影响最大的超参数。计算类型<code class="du jd je jf jg b">algorithm</code>在较低的<code class="du jd je jf jg b">n_neighbor</code>值时也发挥了重要作用，但其影响在大约10个邻居以上减弱。该模型获得了88.80%的样本外准确率，接近原作者用<code class="du jd je jf jg b">Random Forest</code>模型获得的89.63%的准确率。</p><p id="c330" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们来造一个<code class="du jd je jf jg b">Decision Tree</code>。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="6acc" class="la jq hh jg b fi lb lc l ld le">#train decision tree<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="48e7" class="la jq hh jg b fi lf lc l ld le">parameters = {'criterion': ['gini', 'entropy', 'log_loss'],<br/>              'splitter': ['best', 'random'],<br/>              'max_depth': [2*n for n in range(1,10)],<br/>              'max_features': ['auto', 'sqrt', 'log2'],<br/>              'min_samples_leaf': [1, 2, 4],<br/>              'min_samples_split': [2, 5, 10]}<br/>tree = DecisionTreeClassifier()<br/>grid_search = GridSearchCV(tree, parameters, cv=10, verbose=0)<br/>tree_cv = grid_search.fit(X_train, Y_train)</span><span id="d60d" class="la jq hh jg b fi lf lc l ld le">#test decision tree<br/>print("Tuned hpyerparameters (best parameters):", tree_cv.best_params_)<br/>print("Train accuracy:", tree_cv.best_score_)<br/>print("Test accuracy:", tree_cv.best_estimator_.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lx"><img src="../Images/99e25ba7828748c6e6472b90bed125c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7nuCasB-HrWKPgCvezHTqg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="f032" class="la jq hh jg b fi lb lc l ld le">#plot decision tree scores<br/>tree_cv_df = pd.DataFrame(tree_cv.cv_results_[‘params’])<br/>tree_cv_df[‘score’] = tree_cv.cv_results_[‘mean_test_score’]</span><span id="b4fa" class="la jq hh jg b fi lf lc l ld le">sns.lineplot(data=tree_cv_df, x=’max_depth’, y=’score’, hue=’min_samples_leaf’, style=’min_samples_split’)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ly"><img src="../Images/e2f18b97d35a787123f0ed2a39cfee91.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*X_lNFNxneAy6oKfJzqEYhw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Lineplot of <strong class="bd jr">max_depth</strong> by <strong class="bd jr">score</strong> for different <strong class="bd jr">min_samples_leaf</strong> and <strong class="bd jr">min_samples_split</strong> values.</figcaption></figure><p id="fa21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用最小的<code class="du jd je jf jg b">min_samples_leaf</code>和<code class="du jd je jf jg b">min_samples_split</code>值和最高的树<code class="du jd je jf jg b">max_depth</code>时，<code class="du jd je jf jg b">Decision Tree</code>模型表现最佳。这表明该模型需要进行许多小的划分，以便对样品进行分类，这与通过排序产生聚类的困难是一致的。该模型仍然达到了88.38%的高精度，尽管这种高深度、细粒度的变量分裂不是非常有效。</p><p id="4541" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在到了<code class="du jd je jf jg b">Random Forest</code>，这是一个<code class="du jd je jf jg b">Decision Tree</code>“元估计器”。这也给了原作者他们最好的结果。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="a044" class="la jq hh jg b fi lb lc l ld le">#train random forest<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="74e5" class="la jq hh jg b fi lf lc l ld le">parameters = {'criterion': ['gini', 'entropy', 'log_loss'],<br/>              'max_depth': [2*n for n in range(1,10)],<br/>              'max_features': ['auto', 'sqrt', 'log2'],<br/>              'min_samples_leaf': [1, 2, 4],<br/>              'min_samples_split': [2, 5, 10]}<br/>forest = RandomForestClassifier()<br/>grid_search = GridSearchCV(forest, parameters, cv=10, verbose=0)<br/>forest_cv = grid_search.fit(X_train, Y_train)</span><span id="3b31" class="la jq hh jg b fi lf lc l ld le">#test random forest<br/>print("Tuned hpyerparameters (best parameters):", forest_cv.best_params_)<br/>print("Train accuracy:", forest_cv.best_score_)<br/>print("Test accuracy:", forest_cv.best_estimator_.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lx"><img src="../Images/a0f2c75f8f66ca8352766324a9b83804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fc312lUfHhb_jVkxhpBl2Q.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="3fd4" class="la jq hh jg b fi lb lc l ld le">#plot random forest scores<br/>forest_cv_df = pd.DataFrame(forest_cv.cv_results_[‘params’])<br/>forest_cv_df[‘score’] = forest_cv.cv_results_[‘mean_test_score’]</span><span id="ad31" class="la jq hh jg b fi lf lc l ld le">sns.lineplot(data=forest_cv_df, x=’max_depth’, y=’score’, hue=’min_samples_leaf’, style=’min_samples_split’)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ly"><img src="../Images/64bf146b5d622ff2e2a773a24e610f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*yvHFNfzg8p8Zx6DUTU4U7Q.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Lineplot of <strong class="bd jr">max_depth</strong> by <strong class="bd jr">score</strong> for different <strong class="bd jr">min_samples_leaf</strong> and <strong class="bd jr">min_samples_split</strong> values.</figcaption></figure><p id="a2ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的<code class="du jd je jf jg b">Random Forest</code>模型，从100个独立的决策树中获得一致<em class="jc">投票</em>，仅仅略微提高了原始<code class="du jd je jf jg b">Decision Tree</code>的准确性。它仍然依赖于最小的<code class="du jd je jf jg b">min_samples_leaf</code>和<code class="du jd je jf jg b">min_samples_split</code>值，尽管将<code class="du jd je jf jg b">max_depth</code>减少到了14。令人惊讶的是，我们的<code class="du jd je jf jg b">Random Forest</code>与原始出版物相比表现不佳，尽管作者没有描述他们选择了哪些超参数，我们也没有调整这里所有可能的参数。</p><p id="7867" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将稍微不同地优化我们的<code class="du jd je jf jg b">Support Vector Machine</code>模型。支持向量机找到一个超平面函数来最大化类别之间的分离，以一种逆排序的方式向数据添加更高维度。与其他模型相比，这是一个计算量很大的模型，我的机器(32GB RAM，3600Mhz/8CPU)在试图一次运行它时停滞不前。这里使用的嵌套for-loop方法是对<code class="du jd je jf jg b">GridSearchCV</code>的手动迭代，一次只执行整个网格搜索的一小部分。这仍然花了我的机器4天时间来完成，所以我建议用<code class="du jd je jf jg b">verbose</code>设置来启用一些进度消息，以保持你的理智。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="1110" class="la jq hh jg b fi lb lc l ld le">#train support vector machine<br/>from sklearn.svm import SVC</span><span id="397e" class="la jq hh jg b fi lf lc l ld le">svm_params = []<br/>svm_scores = np.empty(0)<br/>for i in np.logspace(-3, 3, 10):<br/>    for j in range(2,5):<br/>        for k in np.logspace(-3, 1, 10):<br/>            parameters = {'C': [i],<br/>                          'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],<br/>                          'degree': [j],<br/>                          'gamma': [k]}<br/>            svm = SVC()<br/>            grid_search = GridSearchCV(svm, parameters, cv=10, verbose=1)<br/>            svm_cv = grid_search.fit(X_train, Y_train)<br/>            <br/>            svm_params = svm_params + svm_cv.cv_results_['params']<br/>            svm_scores = np.append(svm_scores, svm_cv.cv_results_['mean_test_score'])</span><span id="108e" class="la jq hh jg b fi lf lc l ld le">#test support vector machine<br/>print("Tuned hpyerparameters (best parameters):", svm_params[np.argmax(svm_scores)])<br/>print("Train accuracy:", np.max(svm_scores))<br/>svm_cv = SVC(C=svm_params[np.argmax(svm_scores)]['C'],<br/>             degree=svm_params[np.argmax(svm_scores)]['degree'],<br/>             gamma=svm_params[np.argmax(svm_scores)]['gamma'],<br/>             kernel=svm_params[np.argmax(svm_scores)]['kernel'])<br/>svm_cv.fit(X_train, Y_train)<br/>print("Test accuracy:", svm_cv.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ma"><img src="../Images/e38120077ceb659e371f9feb5e380c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYpY7Cav7oSW5cv8gLXf-w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="680c" class="la jq hh jg b fi lb lc l ld le">#plot svm scores<br/>svm_cv_df = pd.DataFrame(svm_params)<br/>svm_cv_df['score'] = svm_scores</span><span id="f7ad" class="la jq hh jg b fi lf lc l ld le">from mpl_toolkits.mplot3d import Axes3D<br/>fig = plt.figure()<br/>ax = fig.gca(projection='3d')<br/>surf = ax.plot_trisurf(svm_cv_df[(svm_cv_df['C']==0.001) &amp; (svm_cv_df['kernel']=='poly')]['degree'],<br/>                       svm_cv_df[(svm_cv_df['C']==0.001) &amp; (svm_cv_df['kernel']=='poly')]['gamma'],<br/>                       svm_cv_df[(svm_cv_df['C']==0.001) &amp; (svm_cv_df['kernel']=='poly')]['score'],<br/>                       cmap=plt.cm.viridis, linewidth=0.2)<br/>ax.view_init(18,300)<br/>ax.set_xlabel('degree')<br/>ax.set_ylabel('gamma')<br/>ax.set_zlabel('score')<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es mb"><img src="../Images/d8715d7ceafd3cd233c4b7ce6e678b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*4Q69sMpyxKx5Un1NLJLs4Q.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Surface plot of <strong class="bd jr">gamma</strong> and <strong class="bd jr">degree</strong> by <strong class="bd jr">score</strong>.</figcaption></figure><p id="8afb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jd je jf jg b">Support Vector Machine</code>以89.21%的准确率提供了最好的分类。尽管这仍然稍微低于原始出版物中的模型。多项式<code class="du jd je jf jg b">degree</code>和<code class="du jd je jf jg b">gamma</code>标度是最重要的超参数。</p><p id="10a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们来看一款<code class="du jd je jf jg b">Artificial Neural Net</code>车型。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="3b0b" class="la jq hh jg b fi lb lc l ld le">#train artificial neural network<br/>from sklearn.neural_network import MLPClassifier</span><span id="53c3" class="la jq hh jg b fi lf lc l ld le">parameters = {'activation': ['identity', 'logistic', 'tanh', 'relu'],<br/>              'solver': ['lbfgs', 'sgd', 'adam'],<br/>              'alpha': np.logspace(-5, 0, 10),<br/>              'learning_rate': ['adaptive'],<br/>              'max_iter': [1000]}<br/>ann = MLPClassifier()<br/>grid_search = GridSearchCV(ann, parameters, cv=10, verbose=0)<br/>ann_cv = grid_search.fit(X_train, Y_train)</span><span id="34ba" class="la jq hh jg b fi lf lc l ld le">#test artificial neural network<br/>print("Tuned hpyerparameters (best parameters):", ann_cv.best_params_)<br/>print("Train accuracy:", ann_cv.best_score_)<br/>print("Test accuracy:", ann_cv.best_estimator_.score(X_test, Y_test))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lg"><img src="../Images/893d1c671d9a17cfc60e7bb31ed21477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lvJ6q_j-_DGB2BJ5ksRYQ.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="04a0" class="la jq hh jg b fi lb lc l ld le">#plot ann scores<br/>ann_cv_df = pd.DataFrame(ann_cv.cv_results_['params'])<br/>ann_cv_df['score'] = ann_cv.cv_results_['mean_test_score']</span><span id="9bcb" class="la jq hh jg b fi lf lc l ld le">sns.lineplot(data=ann_cv_df, x='alpha', y='score', hue='solver', style='activation')<br/>plt.legend(loc='lower right')<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es ly"><img src="../Images/418751fd9cd7261b986148982342c54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*io8HYpTkP5NQ0AbX32KUDQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Lineplot of <strong class="bd jr">alpha</strong> by <strong class="bd jr">score</strong> for different <strong class="bd jr">solver</strong> and <strong class="bd jr">activation</strong> values.</figcaption></figure><p id="6e07" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jd je jf jg b">Artificial Neural Network</code>比<code class="du jd je jf jg b">Support Vector Machine</code>具有更高的样本内准确率，但相当于89.21%的样本外准确率。<code class="du jd je jf jg b">solver</code>超参数对模型精度的影响最大，lbfgs表现最好，尤其是使用tanh和relu <code class="du jd je jf jg b">activation</code>函数。这与<a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlpclassifier#sklearn.neural_network.MLPClassifier" rel="noopener ugc nofollow" target="_blank"> ANN文档</a>一致，后者推荐lbfgs在小数据集上更快收敛。</p><p id="28f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jd je jf jg b">GridSearchCV</code>还能够比SVM更快地优化人工神经网络，优化时间为1小时，而优化时间为4天，这使得人工神经网络成为该数据集更高效的实施选项。</p></div><div class="ab cl ji jj go jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="ha hb hc hd he"><h1 id="1502" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">4.最终模型比较</h1><p id="5c7d" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">最后，我们将直接比较模型的准确性，并构建混淆矩阵，以查看每个模型的优势和不足。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="31ab" class="la jq hh jg b fi lb lc l ld le">#compare models<br/>print("Logistic Regression: {}\nKNN: {}\nDecision Tree: {}\nRandom Forest: {}\nSVM: {}\nANN: {}".format(<br/>    logreg_cv.best_estimator_.score(X_test, Y_test),<br/>    knn_cv.best_estimator_.score(X_test, Y_test),<br/>    tree_cv.best_estimator_.score(X_test, Y_test),<br/>    forest_cv.best_estimator_.score(X_test, Y_test),<br/>    svm_cv.score(X_test, Y_test),<br/>    ann_cv.best_estimator_.score(X_test, Y_test)))</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es mc"><img src="../Images/9253edf5a04b886a2e2e754fb04fb0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*gWr_E_bqLfWEEP9g6n_wHg.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Screenshot by author</figcaption></figure><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="8599" class="la jq hh jg b fi lb lc l ld le">#plot confusion matrices using model predictions<br/>from sklearn.metrics import confusion_matrix</span><span id="e9b7" class="la jq hh jg b fi lf lc l ld le">logreg_cm = confusion_matrix(Y_test, logreg_cv.predict(X_test))<br/>knn_cm = confusion_matrix(Y_test, knn_cv.predict(X_test))<br/>tree_cm = confusion_matrix(Y_test, tree_cv.predict(X_test))<br/>forest_cm = confusion_matrix(Y_test, forest_cv.predict(X_test))<br/>svm_cm = confusion_matrix(Y_test, svm_cv.predict(X_test))<br/>ann_cm = confusion_matrix(Y_test, ann_cv.predict(X_test))</span><span id="9e64" class="la jq hh jg b fi lf lc l ld le">vmax = max(np.amax(logreg_cm), np.amax(knn_cm), np.amax(tree_cm), np.amax(forest_cm), np.amax(svm_cm), np.amax(ann_cm))</span><span id="2b58" class="la jq hh jg b fi lf lc l ld le">fig, axs = plt.subplots(2, 3, constrained_layout=True)</span><span id="f8b1" class="la jq hh jg b fi lf lc l ld le">sns.heatmap(logreg_cm, annot=True, cbar=False, ax=axs[0,0], cmap='mako', fmt='g', vmin=0, vmax=vmax)<br/>sns.heatmap(knn_cm, annot=True, cbar=False, ax=axs[1,0], cmap='mako', fmt='g', vmin=0, vmax=vmax)<br/>sns.heatmap(tree_cm, annot=True, cbar=False, ax=axs[0,1], cmap='mako', fmt='g', vmin=0, vmax=vmax)<br/>sns.heatmap(forest_cm, annot=True, cbar=False, ax=axs[1,1], cmap='mako', fmt='g', vmin=0, vmax=vmax)<br/>sns.heatmap(svm_cm, annot=True, cbar=False, ax=axs[0,2], cmap='mako', fmt='g', vmin=0, vmax=vmax)<br/>sns.heatmap(ann_cm, annot=True, cbar=False, ax=axs[1,2], cmap='mako', fmt='g', vmin=0, vmax=vmax)</span><span id="2b97" class="la jq hh jg b fi lf lc l ld le">axs[0,0].set_title('Logistic Regression')<br/>axs[1,0].set_title('K-Nearest Neighbors')<br/>axs[0,1].set_title('Decision Tree')<br/>axs[1,1].set_title('Random Forest')<br/>axs[0,2].set_title('Support Vector Machine')<br/>axs[1,2].set_title('Artificial Neural Network')</span><span id="200a" class="la jq hh jg b fi lf lc l ld le">axs[0,0].xaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,0].xaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[0,1].xaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,1].xaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[0,2].xaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,2].xaxis.set_ticklabels(['Low', 'Mod', 'High'])</span><span id="c98b" class="la jq hh jg b fi lf lc l ld le">axs[0,0].yaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,0].yaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[0,1].yaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,1].yaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[0,2].yaxis.set_ticklabels(['Low', 'Mod', 'High'])<br/>axs[1,2].yaxis.set_ticklabels(['Low', 'Mod', 'High'])</span><span id="ff58" class="la jq hh jg b fi lf lc l ld le">fig.supxlabel('Predicted Adaptability')<br/>fig.supylabel('True Adaptability')</span><span id="fc36" class="la jq hh jg b fi lf lc l ld le">plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es md"><img src="../Images/b798cd7e2195eac897df99cff466594d.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*1w_U1K6QmLjs6Pl1Jkev2w.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Confustion matrices showing performances of all models on the test data.</figcaption></figure><p id="f94b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我们可以看到来自<code class="du jd je jf jg b">Logistic Regression</code>的糟糕结果是由二元预测的趋势导致的，将除了一个学生之外的所有学生的<strong class="ig hi">适应性</strong>分类为<em class="jc">低</em>或<em class="jc">中等</em>，尽管<code class="du jd je jf jg b">mutli_class</code>参数被优化为<code class="du jd je jf jg b">multinomial</code>。其他每个模型几乎都是一样的，只是分类不足的学生数量不同。</p><p id="328c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们来比较一下每个模型的样本内和样本外精度如何变化。</p><pre class="ks kt ku kv fd kw jg kx ky aw kz bi"><span id="678e" class="la jq hh jg b fi lb lc l ld le">#compare model accuracies<br/>model_acc = {'Model': ['LogReg', 'KNN', 'Tree', 'Forest', 'SVM', 'ANN',<br/>                       'LogReg', 'KNN', 'Tree', 'Forest', 'SVM', 'ANN'],<br/>             'Accuracy': [logreg_cv.best_score_,<br/>                          knn_cv.best_score_,<br/>                          tree_cv.best_score_,<br/>                          forest_cv.best_score_,<br/>                          np.max(svm_scores),<br/>                          ann_cv.best_score_,<br/>                          logreg_cv.best_estimator_.score(X_test, Y_test),<br/>                          knn_cv.best_estimator_.score(X_test, Y_test),<br/>                          tree_cv.best_estimator_.score(X_test, Y_test),<br/>                          forest_cv.best_estimator_.score(X_test, Y_test),<br/>                          svm_cv.score(X_test, Y_test),<br/>                          ann_cv.best_estimator_.score(X_test, Y_test)],<br/>             'Data': ['GridSearch', 'GridSearch', 'GridSearch', 'GridSearch', 'GridSearch', 'GridSearch',<br/>                      'Test', 'Test', 'Test', 'Test', 'Test', 'Test']}</span><span id="bf77" class="la jq hh jg b fi lf lc l ld le">model_df = pd.DataFrame(model_acc)</span><span id="27d5" class="la jq hh jg b fi lf lc l ld le">sns.catplot(kind='bar', y='Accuracy', x='Model', hue='Data', data=model_df)<br/>plt.ylim(0.65, None)<br/>plt.xlabel('Model', fontsize=13)<br/>plt.ylabel('Accuracy', fontsize=13)<br/>plt.show()</span></pre><figure class="ks kt ku kv fd lh er es paragraph-image"><div class="er es me"><img src="../Images/44b5ab0cba1ee0955460771bd8ba0893.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*rUn7VWJqSxWrzkKPspRVpw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx">Barplot of the in- and out-of-sample accuracies of all models.</figcaption></figure><p id="c940" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个模型的样本内精度都高于样本外精度，证明了使用不同数据训练和验证模型的重要性。除了<code class="du jd je jf jg b">Logistic Regression</code>之外，不同模型的精确度非常具有可比性。我们最终的模型选择归结为在<code class="du jd je jf jg b">Support Vector Machine</code>和<code class="du jd je jf jg b">Artificial Nueral Network</code>模型之间的训练效率选择，尽管<code class="du jd je jf jg b">K-Nearest Neighbors</code>、<code class="du jd je jf jg b">Decision Tree</code>或<code class="du jd je jf jg b">Random Forest</code>每个都比ANN快得多，并且在这个数据集上的准确性仅稍差。根据您的计算资源，更好的训练效率可以合理地将最终的模型选择转向其他三个模型中的任何一个。</p></div><div class="ab cl ji jj go jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="ha hb hc hd he"><h1 id="426a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">5.结论</h1><p id="342d" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">正如我们所看到的，<code class="du jd je jf jg b">GridSearchCV</code>是机器学习模型选择不可或缺的工具，允许在多个模型的超参数之间进行快速方便的比较。然而，对于像<code class="du jd je jf jg b">Support Vector Machines</code>这样更加资源密集型的模型，有时需要手工迭代<code class="du jd je jf jg b">GridSearchCV</code>才能完成。对于for循环，这很容易做到。</p><p id="4161" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在决定和部署您的模型之前，通过重新采样您的训练和测试数据来交叉验证您的模型准确性总是一个好主意。这可以用<a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn的</a> <code class="du jd je jf jg b"><a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank">cross_val_score</a></code> <a class="ae jh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" rel="noopener ugc nofollow" target="_blank">工具</a>来完成，我可能会在以后的帖子中介绍。</p><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw lm mi"/></div></div></a></div></div></div>    
</body>
</html>