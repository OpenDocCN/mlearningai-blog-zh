<html>
<head>
<title>How to build high performance model serving with AWS Sagemaker &amp; Nvidia Triton Inference Server</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何构建服务于AWS Sagemaker &amp; Nvidia Triton推理服务器的高性能模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/how-to-build-high-performance-model-serving-with-aws-sagemaker-nvidia-triton-inference-server-8e2001ae5e8?source=collection_archive---------2-----------------------#2022-09-05">https://medium.com/mlearning-ai/how-to-build-high-performance-model-serving-with-aws-sagemaker-nvidia-triton-inference-server-8e2001ae5e8?source=collection_archive---------2-----------------------#2022-09-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1407" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="b315" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">机器学习已经成为IT系统数字化和现代化不可或缺的一部分。我们正在将越来越多的机器学习任务从POC转移到生产中。企业正在采用ML预测/建议来解决影响人类生活和商业投资回报率的各种关键问题。</p><p id="4cb6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，我们必须将重点放在能够与组织的IT系统轻松集成的ML系统的设计上。DevOps机器学习实践(称为MLOps)允许数据科学团队创建这样的编排。</p><p id="dc00" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在接下来的几节中，我们将介绍MLOps的概念，参考AWS Sagemaker平台中端到端MLOps管道的架构，并最终在Sagemaker中部署一个用于模型服务的Nvidia Triton推理服务器。</p><p id="d53f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">注意:要了解机器学习平台AWS Sagemaker，请查看我的另一篇博客:在AWS Sagemaker中构建ML模型。</p><p id="fbfa" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">注意:要了解AWS Sagemaker，一个先进的机器学习平台，请查看我的另一个博客:<a class="ae kf" href="https://www.analyticsvidhya.com/blog/2022/02/building-ml-model-in-aws-sagemaker/" rel="noopener ugc nofollow" target="_blank">在AWS Sagemaker中构建ML模型</a></p><h1 id="9e57" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是MLOps</h1><p id="d105" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">MLOps代表机器学习操作。MLops的核心功能是在生产中运行ML系统&amp;管理ML代码、数据、ML工件、跟踪和监控模型性能、数据治理、数据漂移、偏差和其他日常操作。</p><p id="ad53" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下图显示了MLOps实践。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/93e8171c6212d63bea4357d3d4faf30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViYkx0wt7bQdVuJuCTTDjg.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx"><a class="ae kf" href="https://aws.amazon.com/blogs/machine-learning/architect-and-build-the-full-machine-learning-lifecycle-with-amazon-sagemaker/" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/blogs/machine-learning/architect-and-build-the-full-machine-learning-lifecycle-with-amazon-sagemaker/</a></figcaption></figure><h1 id="4a03" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">为什么选择AWS Sagemaker</h1><p id="fc81" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">构建MLOps解决方案非常复杂，它需要MLOps工具、基础架构、流程编排技术和生产后维护方面的知识。这耗费了数据科学团队大量的时间，从而延迟了模型实验和开发周期。基于云的ML平台是托管服务，有助于数据科学团队专注于数据分析、模型开发和优化，以便及时为最终用户带来最佳价值。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/e438f8a29916d66b8111115fba9ed848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxtYJoZVTK6GhyDYjxmG-Q.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx"><a class="ae kf" href="https://aws.amazon.com/sagemaker/mlops/" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/sagemaker/mlops/</a> and Author’s own addition</figcaption></figure><p id="ca9c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">AWS Sagemaker是一个端到端的机器学习平台，它提供了在生产中部署模型所需的所有服务。</p><h1 id="0b9b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">AWS Sagemaker MLOps参考架构</h1><p id="037f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">参考架构展示了如何在AWS服务上实现流程编排，MLOps工具最终为终端用户实施了端到端解决方案。</p><p id="1f8c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> Sagemaker端点是模型服务模块的核心服务，将模型结果带到现实世界。</strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kx"><img src="../Images/454d67ad13d63323f614503419a65550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NUFeIomptdU0olYzqVt-Q.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx"><a class="ae kf" href="https://aws.amazon.com/blogs/apn/taming-machine-learning-on-aws-with-mlops-a-reference-architecture/" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/blogs/apn/taming-machine-learning-on-aws-with-mlops-a-reference-architecture/</a></figcaption></figure><h1 id="6d65" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是模特服务</h1><p id="8545" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">模型预测或推理服务是机器学习管道的重要部分。模型在验证和优化后部署在推理服务器上。我们可以使用部署的模型来预测使用API服务或Kubernetes集群的未知数据集。</p><p id="c501" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">模型结果对业务ROI有直接影响，因此这对于设计符合业务需求的推理服务解决方案是很重要的。</p><p id="5929" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">模型结果作为推理服务器提供给外部用户。</p><p id="9103" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在许多情况下，该模型进行实时预测，数百万人点击API来获得预测或建议，例如电子商务网站、OTT平台等。人们不会等待建议，因此性能是关键指标。</p><p id="479c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Sagemaker型号服务选项</p><p id="0ace" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">萨格马克推论- </strong></p><p id="7835" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">1.实时端点</p><p id="029c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">2.批量转换</p><p id="1be6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">3.无服务器推理</p><p id="d5de" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">4.异步推理</p><p id="e970" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">5.英伟达Triton推论(Nividia AI产品)</p><p id="b0ba" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Sagemaker实时端点是满足高性能、低延迟模型服务需求的最佳解决方案。但是，为了实现服务SLA，需要监控一些重要的性能指标。</p><p id="056c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下图显示了适用于Sagemaker实时端点性能的一些关键指标。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/46b9da6439020469274a5e686795ba6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vzl6-i0Dpx19UcgAG5I6HA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx"><a class="ae kf" href="https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/" rel="noopener ugc nofollow" target="_blank">https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/</a></figcaption></figure><h1 id="1c93" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">为什么选择Nvidia Triton推理服务器</h1><p id="afac" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">托管机器学习(ML)模型对于企业来说可能具有关键的挑战性性能SLA。使用案例类似于医疗保健中的推荐引擎、欺诈发现和任务关键型应用的预测，电子商务网站需要一种具有高吞吐量的服务容量模型，在这种情况下，毫秒级数据足以维持巨大的实时流量。</p><p id="00e9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">需要满足严格的延迟和性能SLA，并且需要大规模部署该模型，以便在一毫秒内处理数百万个请求。</p><p id="c3a2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通常，我们构建基于DAG(有向无环图)的工作流来消费请求并服务于端到端ML流水线。随着复杂性的增加，ML架构使用复杂的模型、大量的数据、多个来源和各种mlops工具，这导致了高响应时间和差的用户体验。</p><p id="4a1e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通过在同一个实例中托管多个模型，我们可以缩短响应时间并满足总体吞吐量SLA。</p><p id="3c2b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">降低业务逻辑的复杂性，并将所有模型和应用程序逻辑封装在同一个实例的同一个或多个容器中(为各种流程提供隔离)也有助于减少延迟。</p><p id="891c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">但是，推理服务器中的应用程序逻辑、模型优化、计算、存储和网络等基础设施以及使用推理请求的底层web服务器都会影响整体延迟。</p><p id="1553" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Nvidia Triton推理服务器是一种开源技术，为生产级机器学习模型提供高吞吐量和低延迟的推理解决方案。托管机器学习(ML)模型对于企业来说可能具有关键的挑战性性能SLA。类似于医疗保健、电子商务网站中的推荐引擎、欺诈检测和任务关键型应用预测的使用案例。这些应用程序需要一个高吞吐量、低延迟的模型服务API，这需要毫秒级的时间。</p><p id="3590" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们建立基于DAG(有向无环图)的工作流，为端到端的ML管道服务。随着复杂性的增加，ML架构使用复杂的模型、大量的数据、多个来源和mlops工具，这导致了高响应时间和差的用户体验。</p><p id="8c11" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通过在同一个实例中托管多个模型，我们可以缩短响应时间并满足总体吞吐量SLA。</p><p id="55c2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">降低业务逻辑的复杂性，并将所有模型和应用程序逻辑封装在位于同一实例的同一个或多个容器中。这项技术为正在运行的进程提供了隔离，也有助于减少延迟。</p><p id="ac8d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">但是，推理服务器中的应用程序逻辑、模型优化、计算、存储和网络等基础设施以及使用推理请求的底层web服务器都会影响整体延迟。</p><p id="af1b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Nvidia Triton推理服务器是一种开源技术，为生产级机器学习模型提供高吞吐量和低延迟的推理解决方案。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kz"><img src="../Images/3ca2b9edeec54e0e16d95c3bd0486e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tR-Um4LZydXAG9zAi4KKgw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Author’s own image. Text Ref : <a class="ae kf" href="https://developer.nvidia.com/nvidia-triton-inference-server" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/nvidia-triton-inference-server</a> and content : <a class="ae kf" href="https://developer.nvidia.com/nvidia-triton-inference-server" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/nvidia-triton-inference-server</a></figcaption></figure><h1 id="29a7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">解决方案概述</h1><p id="9516" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该解决方案将重点关注使用Nvidia Triton推理服务器容器的Sagemaker实时模型部署。我们不打算讨论如何在Sagmaker中构建和训练模型，这可以在我的另一个博客中参考，如引言中所述。</p><p id="d600" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们将使用Sagemaker推理在Nvidia Triton服务器上部署一个PyTorch预训练RESNET50模型。</p><p id="7b2e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">RESNET50模型将使用PyTorch docker-container从torchvision下载。</p><p id="0f15" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">设置环境&amp; IAM角色，Sagemaker将使用该角色访问来自S3的Triton ECR图像和模型工件。</p><p id="4302" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该解决方案运行在sagemaker笔记本GPU类型的实例。</p><p id="1a9f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Nvidia Triton参考架构如下所示，以帮助您了解该解决方案的组件。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es la"><img src="../Images/cdfb05916f946f0b1c309ee852d21783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t319tBFnTJamQ8fWn1XROw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx"><a class="ae kf" href="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/11/05/ML-6284-image001.png" rel="noopener ugc nofollow" target="_blank">https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/11/05/ML-6284-image001.png</a></figcaption></figure><h1 id="6e8e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">设置环境</h1><p id="574c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们将安装所需的包，如boto3、sagemaker、nvidia-pyindex和tritonclient，以构建环境。之后，帐户id将被映射，我们将设置容器图像。</p><p id="e17d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们还将设置Sagemaker运行时和执行角色。</p><p id="0ab0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">！pip install-qU pip AWS CLI boto 3 sagemaker<br/>！pip安装nvidia-pyindex <br/>！pip安装tritonclient[http]</p><p id="c8fa" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">从sagemaker导入boto3，json，sagemaker，time <br/>导入get _ execution _ role<br/>sm _ client = bot O3 . client(service _ name = " sage maker ")<br/>runtime _ sm _ client = bot O3 . client(" sage maker-runtime ")<br/>sage maker _ session = sage maker。Session(boto_session=boto3。会话())<br/>角色=获取_执行_角色()</p><p id="66c7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">account_id_map = { <br/>'美国-东-1': '785573368785 '，<br/>'美国-东-2': '007439368137 '，<br/>'美国-西-1': '710691900526 '，<br/>'美国-西-2': '301217895009 '，<br/>'欧盟-西-1 ':' 802834088</p><p id="8b48" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">区域= boto3。会话()。region_name <br/>如果区域不在account _ id _ map . keys():<br/>raise("不支持的区域")</p><p id="f021" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为triton ECR图像设置图像uri</p><p id="92c1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">base = " Amazon AWS . com . cn " if region . starts with(" cn-")else " Amazon AWS . com "<br/>triton _ image _ uri = " { account _ id } . . dkr . ECR . { region }。{ base }/sagemaker-triton server:21.08-py3 "。格式(<br/>account _ id = account _ id _ map[region]，region=region，base=base <br/>)</p><p id="5346" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们需要实用程序方法来准备请求负载。</p><p id="fdc5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">将numpy作为np从PIL导入Image<br/>S3 _ client = boto 3 . client(' S3 ')<br/>S3 _ client . download _ file(<br/>" sage maker-sample-files "，<br/>" datasets/Image/pets/shiba _ inu _ dog . jpg "，<br/> "shiba_inu_dog.jpg" <br/>)</p><p id="88d6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在Triton服务器上设置用于推理的样本图像</p><p id="4c3a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">def get _ sample _ image():<br/>image _ path = "。/shiba _ inu _ dog . jpg "<br/>img = image . open(image _ path)。convert(" RGB ")<br/>img = img . resize((224，224)) <br/> img = (np.array(img)。astype(NP . float 32)/255)—NP . array(<br/>[0.485，0.456，0.406]，dtype=np.float32 <br/>)。reshape(1，1，3) <br/> img = img / np.array([0.229，0.224，0.225]，dtype=np.float32)。reshape(1，1，3) <br/> img = np.transpose(img，(2，0，1)) <br/> return img.tolist()</p><p id="b4e4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">导入triton客户端包，该包将为有效负载构建实用程序方法。我们还将推理请求转换为二进制格式，以提高推理服务的性能。</p><p id="f521" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">将tritonclient.http导入为http client<br/>def _ get _ sample _ image _ binary(input _ name，output _ name):<br/>inputs =[]<br/>outputs =[]<br/>inputs . append(http client。InferInput(input_name，[1，3，224，224]，" FP32 ")<br/>input _ data = NP . array(get _ sample _ image()，dtype = NP . float 32)<br/>input _ data = NP . expand _ dims(input _ data，axis=0) <br/> inputs[0]。set_data_from_numpy(input_data，binary _ data = True)<br/>outputs . append(http client。inferequestedoutput(output _ name，binary _ data = True))<br/>request _ body，header_length = httpclient。推理serverclient . generate _ request _ body(<br/>inputs，outputs = outputs<br/>)<br/>return request _ body，header _ length<br/>def get _ sample _ image _ binary _ pt():<br/>return _ get _ sample _ image _ binary(" INPUT _ _ 0 "，" OUTPUT _ _ 0 ")<br/>def get _ sample _ image _ binary _ TRT():<br/>return _ get _ sample _ image _ binary(" INPUT "，" OUTPUT ")</p><p id="d14d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">docker运行generate_models.sh脚本，该脚本将从torchvision下载pytorch resnet50模型，并部署在sagemaker上的triton推理服务器上。</p><p id="57db" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">！docker run—GPU = all—RM-it \</p><p id="959e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">-v ` pwd `/ workspace:/workspace nvcr.io/nvidia/pytorch:21.08-py3 \</p><p id="1a21" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">/bin/bash生成_模型. sh</p><p id="7b27" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">来自AWS帐户的作者自己的图像</p><p id="80ea" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">来自AWS帐户的作者自己的图像</p><p id="b9ee" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们在这篇博客中使用了预先训练好的模型来简化解决方案。一旦模型被下载，它将在下面的目录结构下可用。</p><p id="5e84" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我在Sagemaker笔记本实例— Jupyterlab中运行了这个模型。屏幕截图如下所示。</p><p id="b8b9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者图片取自自己的AWS账户</p><h1 id="27c2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">在S3进行模型打包和上传</h1><p id="cc2d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在本节中，我们将使用sagemaker_session.upload_data()函数将下载的预训练模型上传到S3桶中。</p><p id="8de7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型将被部署在sagemaker端点中，用于模型服务。</p><p id="0b65" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型与从torchvision容器下载的所有工件打包在一起。它们使用tar.gz压缩并保存在“triton-serve-pt/resnet/1/”目录中。压缩包被设置为使用下面的代码模拟uri路径。</p><p id="d454" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">！mkdir-p triton-serve-pt/resnet/1/<br/>！mv-f workspace/model . pt triton-serve-pt/resnet/1/<br/>！tar-C triton-serve-pt/-czf model.tar.gz resnet<br/>model _ uri = sage maker _ session . upload _ data(path = " model . tar . gz "，key _ prefix = " triton-serve-pt ")<br/>print(model _ uri)</p><p id="4b39" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">打印将显示-&gt; S3://sagemaker-us-east-1–999999999/triton-serve-pt/model . tar . gz</p><h1 id="a0d1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">正在创建Sagemaker端点</h1><p id="a1e8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在，我们创建一个端点配置。我们将指定端点使用的实例类型和实例数量。我们用的是gpu类型“g4dn.4xlarge”。</p><p id="ed5d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型由datetime进行版本控制。</p><p id="d841" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">“sage maker _ TRITON _ DEFAULT _ MODEL _ NAME”是设置为= resnet的环境变量。请注意，这必须与s3中上传的文件夹名称相匹配。</p><p id="e5a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们还可以设置sage maker _ TRITON _ BUFFER _ MANAGER _ THREAD _ COUNT &amp;</p><p id="bfa1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">SAGEMAKER_TRITON_THREAD_COUNT优化线程计数。</p><p id="6160" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下面的代码将实现这些步骤。</p><p id="361b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">sm _ MODEL _ NAME = " TRITON-resnet-pt-"+time . strftime(" % Y-% M-% d-% H-% M-% S "，time . gmtime())<br/>container = {<br/>" Image ":TRITON _ Image _ uri，<br/> "ModelDataUrl": model_uri，<br/>" Environment ":{ " sage maker _ TRITON _ DEFAULT _ MODEL _ NAME ":" resnet " }，<br/>}<br/>create _ MODEL _ response = sm _ client . create _ MODEL(<br/></p><p id="2561" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Endpoint _ Config _ name = " triton-resnet-pt-"+time . strftime(" % Y-% M-% d-% H-% M-% S "，time . gmtime())<br/>create _ Endpoint _ Config _ response = sm _ client . create _ Endpoint _ Config(<br/>Endpoint Config name = Endpoint _ Config _ name，<br/>production variants =[<br/>{<br/>" instance type ":" ml . g4dn . 4x large "，<br/> "InitialVariantWeight": 1，【T11</p><p id="2c9d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出:</p><p id="46c0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">端点配置Arn:Arn:AWS:sage maker:us-east-1:9999999:endpoint-Config/triton-resnet-pt-2022–08–01–15–12–24</p><p id="6d43" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在我们将使用上面的端点配置创建sagemaker emdpoint。等到端点处于服务状态。</p><p id="b89d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Endpoint _ name = " triton-resnet-pt-"+time . strftime(" % Y-% M-% d-% H-% M-% S "，time . gmtime())<br/>create _ Endpoint _ response = sm _ client . create _ Endpoint(<br/>Endpoint name = Endpoint _ name，Endpoint configname = Endpoint _ config _ name<br/>)<br/>print(" Endpoint Arn:"+create _ Endpoint _ response[" Endpoint Arn "])</p><p id="de19" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出-&gt;端点Arn:Arn:AWS:sage maker:us-east-1:9999999:Endpoint/triton-resnet-pt-2022–08–01–15–12–32</p><p id="cac0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">检查端点的状态</p><p id="be6a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">resp = sm _ client . describe _ endpoint(endpoint name = endpoint _ name)<br/>Status = resp[" endpoint Status "]<br/>print(" Status:"+Status)<br/>while Status = = " Creating ":<br/>time . sleep(60)<br/>resp = sm _ client . describe _ endpoint(endpoint name = endpoint _ name)<br/>Status = resp[" endpoint Status "]<br/>print(" Status:"+Status)<br/>print(" Arn:" "</p><p id="ff0a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出-&gt;</p><p id="1a7d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="bd30" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="f4ca" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="7f13" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="9207" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="3d54" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:创建</p><p id="63a4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:运行中</p><p id="98e7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">arn:arn:AWS:sage maker:us-east-1:999999:endpoint/triton-resnet-pt-2022–08–01–15–12–32</p><p id="d4d5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">状态:运行中</p><p id="73ad" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者图片来自她自己的AWS账户</p><p id="5cff" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">一旦端点进入“服务中”状态，我们将运行推理。</p><p id="b2d9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者图片来自她自己的AWS账户</p><p id="e372" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者图片来自她自己的AWS账户</p><p id="8b42" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">调用端点和推理运行</p><p id="b5ed" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">接下来，我们将调用端点来预测并使用之前加载的示例图像运行推理。</p><p id="3b4f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们将定义有效载荷输入，定义invoke_endpoint函数。</p><p id="184e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">payload = {<br/>" inputs ":[<br/>{<br/>" name ":" INPUT _ _ 0 "，<br/> "shape": [1，3，224，224]，<br/> "datatype": "FP32 "，<br/> "data": get_sample_image()，<br/> } <br/> ] <br/> }</p><p id="b9ed" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">response = runtime _ sm _ client . invoke _端点(</p><p id="1011" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">EndpointName=endpoint_name，content type = " application/octet-stream "，Body = JSON . dumps(payload)<br/>)<br/>print(JSON . loads(response[" Body "]。阅读()。解码(" utf8 ")))</p><p id="f93b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">request_body，header _ length = get _ sample _ image _ binary _ pt()<br/>response = runtime _ sm _ client . invoke _ endpoint(<br/>endpoint name = endpoint _ name，<br/>content type = " application/vnd . sage maker-triton . binary+JSON；json-header-size={} "。格式(<br/> header_length <br/>)，<br/> Body=request_body，<br/>)</p><p id="f83b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">header _ length _ prefix = " application/vnd . sage maker-triton . binary+JSON；json-header-size= "</p><p id="7160" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">header _ length _ str = response[" content type "][len(header _ length _ prefix):]</p><p id="882f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">阅读回复正文</p><p id="823c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">result = httpclient。推理服务器客户端. parse_response_body( <br/>响应["Body"])。read()，header _ length = int(header _ length _ str)<br/>)<br/>OUTPUT 0 _ data = result . as _ numpy(" OUTPUT _ _ 0 ")<br/>print(OUTPUT 0 _ data)</p><h1 id="1657" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="851b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Sagemaker通过提供完全托管的ML平台，帮助所有行业的数据科学团队轻松构建、培训和部署机器学习和深度学习模型，并快速从实验转向生产。</p><p id="1e2c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在本文中，我们检查了如何将Triton推理服务器与Sagemaker集成。</p><ul class=""><li id="4034" class="lb lc hh je b jf ka jj kb jn ld jr le jv lf jz lg lh li lj bi translated">借助Sagemaker和Nvidia Triton推理服务器(容器集成)，我们可以在多个框架(Pytorch、Tensorflow、ONNX等)上使用高性能的GPU和CPU，通过单一推理服务解决方案进一步简化ML部署。</li><li id="a533" class="lb lc hh je b jf lk jj ll jn lm jr ln jv lo jz lg lh li lj bi translated">然而，Triton并不是所有推理需求的最佳解决方案。</li><li id="aa75" class="lb lc hh je b jf lk jj ll jn lm jr ln jv lo jz lg lh li lj bi translated">我们必须了解问题陈述、技术前景以及最终用户对基准当前和目标性能SLA的期望。</li><li id="c94c" class="lb lc hh je b jf lk jj ll jn lm jr ln jv lo jz lg lh li lj bi translated">在将任何推理工作负载托管到Nvidia Triton推理服务器或设计新的Triton推理解决方案之前，成本效益分析和ROI计算非常重要。</li></ul><h1 id="54cb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">清理AWS资源</h1><p id="598a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一旦项目完成，清理端点、任何正在运行的作业、停止所有笔记本实例。</p><p id="4d6f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">你可以使用下面的代码来清理资源。我们也可以使用AWS管理控制台来清理。</p><p id="94f2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">sm _ client . delete _ model(model name = sm _ model _ name)<br/>sm _ client . delete _ endpoint _ config(endpoint configname = endpoint _ config _ name)<br/>sm _ client . delete _ endpoint(endpoint name = endpoint _ name)</p><h1 id="11bc" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考:</h1><p id="8228" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/aws/amazon-sagemaker-examples/tree/435506384dc397daf8307f5fb74ac97ade386dcd/sagemaker-triton/resnet50" rel="noopener ugc nofollow" target="_blank">https://github . com/AWS/Amazon-sage maker-examples/tree/435506384 DC 397 daf 8307 F5 FB 74 AC 97 ade 386 DCD/sage maker-triton/resnet 50</a></p><p id="e67e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/aws/amazon-sagemaker-examples/tree/435506384dc397daf8307f5fb74ac97ade386dcd/sagemaker-triton/resnet50" rel="noopener ugc nofollow" target="_blank">https://github . com/AWS/Amazon-sage maker-examples/tree/435506384 DC 397 daf 8307 F5 FB 74 AC 97 ade 386 DCD/sage maker-triton/resnet 50</a></p><div class="lp lq ez fb lr ls"><a href="https://github.com/aws/amazon-sagemaker-examples/tree/435506384dc397daf8307f5fb74ac97ade386dcd/sagemaker-triton/resnet50" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">亚马逊-sage maker-examples/sage maker-triton/resnet 50 at 435506384 DC 397 daf 8307 f 5 FB 74 AC 97 ade 386 DCD…</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">例子📓展示如何使用🧠亚马逊建立、训练和部署机器学习模型的Jupyter笔记本电脑…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">github.com</p></div></div><div class="mb l"><div class="mc l md me mf mb mg kq ls"/></div></div></a></div><div class="lp lq ez fb lr ls"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">medium.com</p></div></div><div class="mb l"><div class="mh l md me mf mb mg kq ls"/></div></div></a></div></div></div>    
</body>
</html>