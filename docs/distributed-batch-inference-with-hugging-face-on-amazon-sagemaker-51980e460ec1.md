# Amazon Sagemaker ä¸Šå¸¦æ‹¥æŠ±è„¸çš„åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/distributed-batch-inference-with-hugging-face-on-amazon-sagemaker-51980e460ec1?source=collection_archive---------3----------------------->

## ä½¿ç”¨ SageMaker å¤„ç†ä½œä¸šï¼Œé€šè¿‡ Hugging Face çš„ Transformer æ¨¡å‹è½»æ¾åœ°å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œæ¨ç†

![](img/a3d9e4ad016ba857d8f32f70e2ec7515.png)

Photo by [Alex Kulikov](https://unsplash.com/@burntime?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

è¿™ç¯‡åšå®¢å°†å‘æ‚¨å®Œæ•´åœ°ä»‹ç»å¦‚ä½•å¯¹ç”Ÿäº§ä¸­çš„å¤§é‡æ•°æ®è¿è¡Œåˆ†å¸ƒå¼æ‰¹é‡æ¨ç†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨äºšé©¬é€Š Sagemakerï¼Œä¸€ç§å®Œå…¨æ‰˜ç®¡çš„æœºå™¨å­¦ä¹ æœåŠ¡ã€‚å€ŸåŠ© Amazon SageMakerï¼Œæ•°æ®ç§‘å­¦å®¶å’Œå¼€å‘äººå‘˜å¯ä»¥å¿«é€Ÿæ„å»ºå’Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç„¶åå°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§å°±ç»ªçš„æ‰˜ç®¡ç¯å¢ƒä¸­ã€‚

å‡è®¾æˆ‘ä»¬æœ‰æ•° Pb çš„æ•°æ®ï¼Œæˆ‘ä»¬æƒ³ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚Sagemaker å¤„ç†ä½œä¸šæ˜¯æœ€ç®€å•çš„æ–¹æ³•ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒä¸“æ³¨äºæŠ½è±¡å‡ºæ‰€éœ€çš„åŸºç¡€è®¾æ–½ã€‚å®ƒè¿˜è¦æ±‚å¯¹æˆ‘ä»¬ç°æœ‰çš„ä»£ç è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª Huggin Face çš„é¢„è®­ç»ƒæ¨¡å‹æ¥è®¡ç®—ä¸€å¯¹å¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

**è¿™ç¯‡åšæ–‡å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜:**

*   ä¿®æ”¹ SageMaker å¤„ç†ä½œä¸šçš„ä»£ç 
*   ç”¨ä»£ç å’Œä¾èµ–é¡¹æ„å»º docker å®¹å™¨ï¼Œå¹¶å°†å…¶æ¨é€åˆ° Amazon å¼¹æ€§å®¹å™¨æ³¨å†Œ(Amazon ECR)å­˜å‚¨åº“
*   ä½¿ç”¨è‡ªå®šä¹‰ docker å›¾åƒå¯åŠ¨å¤„ç†ä½œä¸š

# ä¿®æ”¹ä»£ç 

åˆ›å»ºå¤„ç†ä½œä¸šéœ€è¦æŒ‡å®šä¸€ä¸ªäºšé©¬é€Šç®€å•å­˜å‚¨æœåŠ¡(äºšé©¬é€Š S3) URI æ¥ä¸‹è½½æ•°æ®ï¼Œå¹¶åœ¨ docker å®¹å™¨ä¸­æŒ‡å®šä¸€ä¸ªè·¯å¾„æ¥ä¸‹è½½æ•°æ®ã€‚å¤„ç†å®¹å™¨ä¸­çš„è·¯å¾„å¿…é¡»ä»¥`/opt/ml/processing/`å¼€å¤´ã€‚ç¨åå°†å¯¹æ­¤è¿›è¡Œæ›´å¤šè®¨è®ºã€‚

> æ³¨æ„:`/opt/ml`åŠå…¶æ‰€æœ‰å­ç›®å½•ç”± SageMaker ä¿ç•™ã€‚åœ¨æ„å»ºå¤„ç† Docker æ˜ åƒæ—¶ï¼Œä¸è¦å°†å®¹å™¨æ‰€éœ€çš„ä»»ä½•æ•°æ®æ”¾åœ¨è¿™äº›ç›®å½•ä¸­ã€‚

æˆ‘ä»¬éœ€è¦ä¿®æ”¹æˆ‘ä»¬çš„è„šæœ¬ä»è¿™ä¸ªè·¯å¾„è¯»å–æ•°æ®`/opt/ml/processing/`

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ªä¸Šæ¸¸æ•°æ®ç®¡é“çš„æ•°æ®ï¼Œè¯¥ç®¡é“å°†æ•°æ®åˆ†æˆå¤šä¸ªæ‹¼èŠ±æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ S3 å­˜å‚¨æ¡¶ä¸­ã€‚å¦‚æœæ‰€é€‰å®ä¾‹ä¸Šæœ‰å¤šä¸ª GPUï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¯ä¸ª GPU å¹¶è¡Œæ¨æ–­æ¯ä¸ªæ–‡ä»¶ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªæ¥è‡ªæ‹¥æŠ±è„¸çš„é¢„è®­ç»ƒå¥å­è½¬æ¢å™¨(æ›´å¤šä¿¡æ¯[è¿™é‡Œ](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2))ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°ä¿®æ”¹ä¸‹é¢çš„è„šæœ¬æ¥ä½¿ç”¨ä»»ä½•æ¨¡å‹ã€‚

ç¤ºä¾‹è„šæœ¬ä»è¾“å…¥ç›®å½•ä¸­è¯»å–æ–‡ä»¶ï¼Œä»å¥å­åµŒå…¥ä¸­æ·»åŠ ä¸€ä¸ªå…·æœ‰ä½™å¼¦ç›¸ä¼¼æ€§çš„æ–°åˆ—ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨è¾“å‡ºç›®å½•ä¸­ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œå¯ä»¥ä»å¤„ç†ä½œä¸šä¸­è¦†ç›–è¿™äº›å‚æ•°ã€‚

```
import os
import argparse
import logging
import numpy as np
import pandas as pd
from numpy.linalg import norm
import torch.multiprocessing as mp
from sentence_transformers import SentenceTransformer

# set up logger
logging.basicConfig(format="%(asctime)s:%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger("HuggingFace")
logger.propagate = True
logger.setLevel(logging.INFO)

def get_model_output(cfg, model, df, device_idx):

    # sanity check
    text1 = df["text1"].values
    text2 = df["text2"].values

    # Create the trainer for inference.
    logger.info("Predicting model")
    # Compute embedding for both lists
    embeddings1 = model.encode(
        text1,
        batch_size=cfg.batch_size,
        convert_to_numpy=True,
        device=device_idx,
    )
    embeddings2 = model.encode(
        text2,
        batch_size=cfg.batch_size,
        convert_to_numpy=True,
        device=device_idx,
    )
    df["semanticScore"] = np.hstack(
        [
            (emb1 @ emb2.T) / (norm(emb1) * norm(emb2))
            for emb1, emb2 in zip(embeddings1, embeddings2)
        ]
    )
    return df

def do_infer(cfg, device_idx):

    # Load the model.
    logger.info("Loading model")
    model = SentenceTransformer(cfg.model, device=device_idx)

    # Init file path that this process needs to process.
    if os.path.isdir(cfg.input_dir):
        filepath_list = [
            filepath
            for filepath in os.listdir(cfg.input_dir)
            if filepath.endswith(".parquet")
        ]
        filepath_list = sorted(filepath_list)
        filepath_list = [
            filepath_list[idx]
            for idx in range(len(filepath_list))
            if idx % cfg.num_gpu == device_idx
        ]
    else:
        filepath_list = (
            [cfg.input_dir] if device_idx == 0 else []
        )  # only use the first GPU.

    # Get the input and output filepath.
    input_list = [os.path.join(cfg.input_dir, filepath) for filepath in filepath_list]
    output_list = [os.path.join(cfg.output_dir, filepath) for filepath in filepath_list]
    logger.info(f"Device {device_idx} input: {input_list}")
    logger.info(f"Device {device_idx} output: {output_list}")

    # Start processing.
    for in_filename, out_filename in zip(input_list, output_list):
        # Read the dataset.
        logger.info("Reading file {}".format(in_filename))
        prod_df = pd.read_parquet(in_filename)

        # Run the inference.
        output_df = get_model_output(cfg, model, prod_df, device_idx)

        # Save the results
        logger.info(f"Writing the result to {out_filename}")
        output_df.to_parquet(out_filename)
        logger.info(f"Done writing the result to {out_filename}")

def do_multiprocessing_infer(cfg):

    processes_list = [
        mp.Process(target=do_infer, args=(cfg, device_idx))
        for device_idx in range(cfg.num_gpu)
    ]

    for process in processes_list:
        process.start()

    for process in processes_list:
        process.join()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_gpu", type=int, default=4)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument(
        "--input_dir", type=str, default="/opt/ml/processing/data/input"
    )
    parser.add_argument(
        "--output_dir", type=str, default="/opt/ml/processing/data/output"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="sentence-transformers/distiluse-base-multilingual-cased-v2",
    )
    args = parser.parse_args()
    do_multiprocessing_infer(args)
```

# æ„å»º docker æ˜ åƒå¹¶å°†å…¶æ¨é€åˆ° Amazon ECR

ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ª docker å®¹å™¨ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª DOCKER æ–‡ä»¶ç¤ºä¾‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„ä»£ç éƒ½åœ¨ src ç›®å½•ä¸­

```
 FROM python:3.8

RUN apt-get -y update && apt-get install -y --no-install-recommends \
         wget \
         python3 \
    && rm -rf /var/lib/apt/lists/*

RUN wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py && \
        rm -rf /root/.cache

COPY requirements.txt /opt/program/
COPY src/ /opt/program/src/
WORKDIR /opt/program

RUN pip install -r requirements.txt

# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard
# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE
# keeps Python from writing the .pyc files which are unnecessary in this case. We also update
# PATH so that the train and serve programs are found when the container is invoked.
ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
```

ä¸€æ—¦æˆ‘ä»¬æœ‰äº† DOCKER æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºå®ƒæ¥åˆ›å»ºä¸€ä¸ªå›¾åƒï¼Œå¹¶åœ¨å°†å®ƒæ¨é€åˆ° Amazon ECR ä¹‹å‰æ ‡è®°è¯¥å›¾åƒã€‚ECR æ˜¯ä¸€ä¸ªç±»ä¼¼ Docker Hub çš„å®¹å™¨æ³¨å†Œä¸­å¿ƒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œæ‰˜ç®¡å®¹å™¨æ˜ åƒã€‚

ç¤ºä¾‹ bash è„šæœ¬å°†å¤„ç†æ‰€æœ‰ä¸ AWS ç›¸å…³çš„è®¤è¯ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º ***sm-semantic-similarityã€*** çš„å­˜å‚¨åº“ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ ‡è®°ï¼Œæœ€åå°†å…¶æ¨é€åˆ° Amazon ECR å­˜å‚¨åº“ã€‚æˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªç‹¬ç‰¹çš„é“¾æ¥ï¼Œå¦‚"<**ACCOUNT-ID>. dkr . ECR . us-east-1 . Amazon AWS . com/sm-lexical-similarity:v1**"

```
# Name of algo -> ECR
algorithm_name=sm-semantic-similarity

account=$(aws sts get-caller-identity --query Account --output text)

# Region, defaults to us-east-1
region=$(aws configure get region)
region=${region:-us-east-1}

fullname="${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:v1"

# If the repository doesn't exist in ECR, create it.
aws ecr describe-repositories --repository-names "${algorithm_name}" --region ${region}> /dev/null 2>&1

if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name "${algorithm_name}" --region ${region}> /dev/null
fi

# Get the login command from ECR and execute it directly
aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}

# Build the docker image locally with the image name and then push it to ECR
# with the full name.

docker build -t ${algorithm_name} .
docker tag ${algorithm_name} ${fullname}
docker push ${fullname}
```

# ä½¿ç”¨è‡ªå®šä¹‰å®¹å™¨è¿è¡Œ SageMaker å¤„ç†ä½œä¸š

Amazon SageMaker ä» S3 å¤åˆ¶æ•°æ®ï¼Œç„¶åæå–å®¹å™¨ã€‚ç¾¤é›†èµ„æºåœ¨ä½œä¸šæœŸé—´è°ƒé…ï¼Œå¹¶åœ¨ä½œä¸šå®Œæˆæ—¶æ¸…ç†ã€‚å¤„ç†ä½œä¸šçš„è¾“å‡ºå­˜å‚¨åœ¨å‚æ•°ä¸­æŒ‡å®šçš„ S3 å­˜å‚¨æ¡¶ä¸­ã€‚

![](img/13906fd9a72770b1f79e092c128dc113.png)

`Processor`ç±»éœ€è¦ä»¥ä¸‹å‚æ•°:

*   **è§’è‰²**:AWS IAM è§’è‰²åæˆ– ARN
*   **image _ uri:**Docker å›¾åƒçš„å”¯ä¸€é“¾æ¥
*   **instance_count:** è¿è¡Œå¤„ç†ä½œä¸šçš„å®ä¾‹æ•°ã€‚
*   **instance_type:** ç”¨äºå¤„ç†çš„ EC2 å®ä¾‹çš„ç±»å‹
*   **å…¥å£ç‚¹:**æ„æˆå…¥å£ç‚¹å‘½ä»¤çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥æ ¹æ® instance_type ä¼ é€’`num_gpus`å‚æ•°ã€‚ä¾‹å¦‚`ml.p3.16xlarge`æœ‰ 8 ä¸ª GPU
*   **volume_size_in_gb** :ä»¥ gb ä¸ºå•ä½çš„å¤§å°ï¼Œç”¨äºåœ¨å¤„ç†ä½œä¸šæœŸé—´å­˜å‚¨æ•°æ®

å½“è¿è¡Œå¤„ç†ä½œä¸šæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æä¾›ä½œä¸º`[**ProcessingInput**](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput)`å¯¹è±¡çš„è¾“å…¥æ–‡ä»¶åˆ—è¡¨å’Œä½œä¸ºè¾“å‡ºçš„`[**ProcessingOutput**](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput)`å¯¹è±¡åˆ—è¡¨ã€‚è¯·æ³¨æ„å‚æ•°`**s3_data_distribution_type**`ï¼Œå®ƒå¯ä»¥æ˜¯â€œ**full replicated**æˆ–â€œ **ShardedByS3Key** â€ï¼Œå…¶ä¸­ full replicated å°†ä½¿ç»™å®šæ•°æ®é›†çš„å‰¯æœ¬åœ¨æ¯ä¸ªå®ä¾‹ä¸­å¯ç”¨ï¼ŒShardedByS3Key å°†æŠŠ[**# of data files**]/[**# of instances**]æ¡æ•°æ®å¤åˆ¶åˆ°æ¯ä¸ªå®ä¾‹ä¸­ã€‚
æ›´å¤šä¿¡æ¯è¯·å‚è€ƒæ­¤å¤„çš„[å’Œ](https://sagemaker.readthedocs.io/en/stable/index.html)ã€‚

```
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.processing import Processor
from sagemaker.processing import ProcessingInput, ProcessingOutput

region = boto3.session.Session().region_name
role = get_execution_role()

ecr_image = '<ACCOUNT-ID>.dkr.ecr.us-east-1.amazonaws.com/sm-lexical-similarity:v1'

huggingface_processor = Processor(role=role,
                                  entrypoint=['python', '-u', 'src/infer.py', '--num_gpus=8'],
                                  image_uri=ecr_image,
                                  instance_type='ml.p3.16xlarge',
                                  instance_count=50,
                                  volume_size_in_gb=600,
                                  base_job_name = 'preprocess-semantic'
                                 )

huggingface_processor.run(
    inputs=[
        ProcessingInput(
            source='<s3_uri or local path>',
            s3_data_distribution_type='ShardedByS3Key',
            destination='/opt/ml/processing/data/input')
    ],

    outputs=[
        ProcessingOutput(
          source='/opt/ml/processing/data/output/',
          destination='<s3_uri>,
          s3_upload_mode='Continuous'
        )
    ]

)
```

# ç»“è®º

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„åˆ†å¸ƒå¼å¤„ç†ä½œä¸šï¼Œå®ƒä½¿ç”¨äº†æ¥è‡ª Hugging Face çš„é¢„è®­ç»ƒçš„ transformer æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨ Amazon SageMaker æŠ½è±¡å‡ºèµ„æºä¾›åº”ã€‚æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ‰“åŒ…æˆ‘ä»¬çš„ä»£ç ï¼Œåˆ›å»º docker å®¹å™¨ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ° Amazon ECRã€‚è¿™é‡Œæ˜¯å®˜æ–¹ [**äºšé©¬é€Š SageMaker å¤„ç†ç±»æ–‡æ¡£**](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#learn-more) äº†è§£æ›´å¤šã€‚

å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œè§‰å¾—æœ‰ç”¨ã€‚å¦‚æœä½ æœ‰ä»»ä½•æƒ³æ³•ã€è¯„è®ºæˆ–é—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢ç•™ä¸‹è¯„è®ºæˆ–åœ¨ [LinkedIn](https://www.linkedin.com/in/ratulghosh1/) ä¸Šè”ç³»æˆ‘ã€‚å¿«ä¹é˜…è¯»ğŸ™‚

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)