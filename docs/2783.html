<html>
<head>
<title>K-Means Simplified: A Beginner’s Guide to the K-Means Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means简化版:K-Means算法初学者指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/clustering-with-k-means-c967ef5c292a?source=collection_archive---------4-----------------------#2022-06-10">https://medium.com/mlearning-ai/clustering-with-k-means-c967ef5c292a?source=collection_archive---------4-----------------------#2022-06-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="ae54" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">数据科学|机器学习|算法</h2><div class=""/><div class=""><h2 id="a730" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">学习最简单的无监督聚类算法之一的逐步方法</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/6876e8c107cf69f42f7356617adda438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNqdyr92fH-67Y8JvSpnDA.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Credits: <a class="ae jv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/</a></figcaption></figure><p id="7db5" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">K-means聚类属于无监督学习算法家族。它旨在将相似的对象分组以形成集群。K-means聚类中的K指的是您确定的所需聚类数。</p><p id="3746" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">当数据集包含一些要素和标注时，可以使用监督学习算法，如逻辑回归、支持向量机等。这些算法学习将输入要素映射到标注的函数。但是在有<strong class="jy hr">无</strong> <strong class="jy hr">标签的情况下，</strong>我们就不能用监督学习了。这就是分组和聚类概念派上用场的地方。处理无标注数据集的一个简单方法是在数据中找到彼此相似的观察值，并将它们组合在一起形成聚类。</p><p id="0c34" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">现在我们对K-Means和聚类有了一些基本的了解，让我们看看K-Means聚类中涉及的步骤。</p><p id="221a" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">让我们用一个例子来理解这一点:</p><p id="caff" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">图1显示了二维绘制的原始数据。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ks"><img src="../Images/6ee1ee2fadebc5837c12d19e52caec23.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*pb5Ig_9gWMjf-AIA91hh8g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 1: Original dataset — Image created by the author</figcaption></figure><p id="79fe" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">第一步:</strong>选择聚类数K(你决定)。对于这个例子，我们将选择k = 2。</p><p id="5b2c" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">步骤2: 该算法随机初始化质心。对于k =2，两个质心将被随机初始化。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kt"><img src="../Images/88a50ff40e1670965b4667d3d36ed274.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*HhK3O7GZxG0bWdseQZPBeA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 2: Centroid Initialization — Image created by the author</figcaption></figure><p id="cb3e" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">步骤3: </strong>通过计算质心和数据点之间的欧几里德距离，将每个数据点分配到最近的质心。</p><p id="c531" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">二维空间中两点(X₁，Y₁)和(X₂，Y₂)之间的欧氏距离</strong>计算为√(X₂-X₁) + (Y₂-Y₁)</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ku"><img src="../Images/e200bd524b90c37a9379270f0159ad7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*E1dpIdUstwn2Q9xHZOaE9g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 3: Cluster Formation — Image created by the author</figcaption></figure><p id="a56c" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">C1和(P1、P2、P3、P4)之间的距离。。。P12)、C2和(P1、P2、P3、P4、。。。P12 ),并且与任一质心具有相对较短欧几里得距离的那些点将被分配给该特定质心。例如，如果C1-P1距离是10个单位，C2-P1距离是15个单位，P1将被分配给质心1。计算完所有距离后，将形成两个集群，如下图4所示。</p><p id="afcb" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">注意:与其他基于距离的算法一样，标准化数据点总是更好，因为不同的变量可能有不同的标度，从而影响误差平方和。</p><p id="c732" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">步骤4: </strong>通过取聚类的平均值来更新质心。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kv"><img src="../Images/3fdd61a5f8cfcb731ec3f5396978c382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kUNWXRFKfJI48LH198NGA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 4: Centroids Updated — Image created by the author</figcaption></figure><p id="fd25" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">一旦这些点与它们最近的质心相关联，该算法就计算聚类的平均值。例如，聚类1的平均值将是(1.8 + 2 + 3 + 1.7 + 0.5 + 1 ) / 6，(3.5 + 2.5 + 1 + 1 + 1.5 + 2.5 )/6，这是近似值。<strong class="jy hr"> ( 1.6，2 ) </strong>。因此，聚类1中的质心更新为(1.6，2)。对于所有集群，此过程并行发生。</p><p id="7c9e" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">步骤5: </strong>重复步骤3和4，直到质心位置没有变化。</p><p id="f466" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">除非数据点成为新聚类的一部分，否则由于质心和欧几里德距离的更新，聚类的平均值不会改变，质心也不会进一步更新。</p><p id="86f0" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">关于这个算法的详细实现，请访问我的Github知识库，这里:【https://github.com/swapnilin/K-Means-Clustering T2】</p><p id="4883" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated"><strong class="jy hr">评估指标</strong></p><p id="af87" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">正如您在上面注意到的，对于K-means聚类，第一步是决定K的值，该值应该在训练模型之前已知。它是一个超参数，而不是模型参数。我们可以用侧影测量法或肘法来找出k的最佳值。</p><p id="7269" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">“<strong class="jy hr">剪影</strong>值是一个<strong class="jy hr">度量值</strong>，用于衡量一个对象与其自己的聚类(内聚)相比与其他聚类(分离)有多相似。轮廓<strong class="jy hr">的范围</strong>从1到+1。接近+1的轮廓系数表明样本远离相邻的聚类。值为0表示样本位于或非常接近两个相邻聚类之间的判定边界，负值表示这些样本可能被分配到错误的聚类。— <em class="kw">维基百科</em></p><p id="e414" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">表1显示了不同k值下获得的侧影得分和平方和(WSS)得分。根据获得的侧影值，您可以绘制侧影图来确定聚类数。在这种情况下，k = 5将是聚类的最佳值。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kx"><img src="../Images/5d8b9b6009af4b858b8c710d4b631468.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*sekMTBMe_1_UgVUdmqdf9g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Table 1: WSS and Silhouette Scores — <a class="ae jv" href="https://github.com/swapnilin/K-Means-Clustering" rel="noopener ugc nofollow" target="_blank">https://github.com/swapnilin/K-Means-Clustering</a></figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ky"><img src="../Images/ae3bd433a5dbec3835cde65d76818ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*PqVqcD-OCnwe6dHADgx0Xg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 5: Silhouette Plot — <a class="ae jv" href="https://github.com/swapnilin/K-Means-Clustering" rel="noopener ugc nofollow" target="_blank">https://github.com/swapnilin/K-Means-Clustering</a></figcaption></figure><p id="2143" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">“肘形法<strong class="jy hr">包括绘制解释的变化作为聚类数的函数，并选择曲线的肘形作为要使用的聚类数。”— <em class="kw">维基百科</em></strong></p><p id="5022" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">图3显示了通过使用表1中的WSS分数绘制的肘图。正如我们所看到的，弯头是在k = 5时创建的，因此我们将选择k的最佳值为5。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kz"><img src="../Images/b182e9dd33ba90b306dcc7a959fce45d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*9ZhTwSEA9Xip7_Cch_1bxw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Fig 6: Elbow Plot — <a class="ae jv" href="https://github.com/swapnilin/K-Means-Clustering" rel="noopener ugc nofollow" target="_blank">https://github.com/swapnilin/K-Means-Clustering</a></figcaption></figure><p id="dd72" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">对于使用Python的K-Means集群的<strong class="jy hr">实现</strong>，我建议检查我的<a class="ae jv" href="https://github.com/swapnilin/K-Means-Clustering" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库上的代码。</p><p id="3223" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">感谢您的阅读。如果您有更多问题，请通过LinkedIn联系。</p><div class="la lb ez fb lc ld"><a href="https://swapnilin.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="le ab dw"><div class="lf ab lg cl cj lh"><h2 class="bd hr fi z dy li ea eb lj ed ef hq bi translated">Swapnil Kangralkar</h2><div class="lk l"><h3 class="bd b fi z dy li ea eb lj ed ef dx translated">Swapnil Kangralkar。我是加拿大渥太华的一名数据科学家。</h3></div><div class="ll l"><p class="bd b fp z dy li ea eb lj ed ef dx translated">swapnilin.github.io</p></div></div><div class="lm l"><div class="ln l lo lp lq lm lr jp ld"/></div></div></a></div><div class="la lb ez fb lc ld"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="le ab dw"><div class="lf ab lg cl cj lh"><h2 class="bd hr fi z dy li ea eb lj ed ef hq bi translated">Mlearning.ai提交建议</h2><div class="lk l"><h3 class="bd b fi z dy li ea eb lj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ll l"><p class="bd b fp z dy li ea eb lj ed ef dx translated">medium.com</p></div></div><div class="lm l"><div class="ls l lo lp lq lm lr jp ld"/></div></div></a></div></div></div>    
</body>
</html>