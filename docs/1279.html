<html>
<head>
<title>Speech Recognition by using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的语音识别</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/speech-recognition-by-using-deep-learning-56455dae3d74?source=collection_archive---------5-----------------------#2021-11-11">https://medium.com/mlearning-ai/speech-recognition-by-using-deep-learning-56455dae3d74?source=collection_archive---------5-----------------------#2021-11-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7ae4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">语音识别指的是机器将口头语言转换成可读文本的能力。在这篇博客中，我将提到由谷歌大脑组织的一场kaggle比赛- <strong class="ig hi"> TensorFlow语音识别挑战赛(</strong><a class="ae jc" href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge" rel="noopener ugc nofollow" target="_blank">)https://www . ka ggle . com/c/tensor flow-Speech-Recognition-Challenge</a><strong class="ig hi">)</strong>。这些数据包含了65，000个由30个短词组成的一秒钟长的话语，由数千个不同的人完成。</p><p id="5445" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我处理了</p><ol class=""><li id="3f24" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">创建有用的功能</li><li id="aaae" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">模型架构</li></ol><p id="7521" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们手头的第一项任务是将语音转换成一种数学形式，以便在此基础上建立进一步的机器学习模型。音频文件具有音频波形，该波形包含每一时刻的波的振幅(模拟)。为了进行计算，我们需要将其离散化，或者换句话说，我们需要决定在所有时刻(间隔)输出音频波形(数字)。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/36b97d844290cebe51a467b451f9f961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7-6_zcjqlZMWbjCKv1K2A.png"/></div></div></figure><p id="3f2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要提供采样率来获得这种离散化。例如，如果音频文件为1秒，采样率为16000，那么在这种情况下将提取16000个点。<strong class="ig hi">数据点数=采样率*时间</strong></p><p id="c29d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们有了一个音频文件的数字解释，但我们想进一步提取一些有用的特征。声音主要有三个特征<strong class="ig hi">响度、音高和音色</strong>。现在，这些特征必须用数学方法提取出来进行计算。</p><p id="1518" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">响度</strong> -响度与振幅直接相关。如上所述，该信息已经存在于文件中</p><p id="8212" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">音高- </strong>音高与频率有关。这种关系是由熔融标度确定</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kd"><img src="../Images/b9249fd5a899c479d2986f83670c5d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*ejp5s39OGLcOxyj9cEWFRg.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">Relation between Frequency vs Pitch in mels</figcaption></figure><p id="51e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更多关于熔化规模的细节可以从https://www.sfu.ca/sonic-studio-webdav/handbook/Mel.html的<a class="ae jc" href="https://www.sfu.ca/sonic-studio-webdav/handbook/Mel.html" rel="noopener ugc nofollow" target="_blank">研究。我们将使用傅立叶变换来获得频率。</a></p><p id="1781" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">音色——</strong>音色取决于波形。音色是声音的质量，有助于区分不同的声音。</p><p id="f820" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上所述，我们需要频率，但我们得到的是每个时刻的幅度，因此我们需要将时域信号转换为频域信号。谢天谢地，这可以通过傅立叶变换来实现。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ki"><img src="../Images/20757624bd13dbed009ae5e0af8c63df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hc0IirUAf6t0nlVTSchwaw.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx">Fourier Transform</figcaption></figure><p id="2187" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果有人想了解傅立叶变换背后的数学，那么我推荐这个Youtube频道-<a class="ae jc" href="https://www.youtube.com/watch?v=iCwMQJnKk2c&amp;list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0" rel="noopener ugc nofollow" target="_blank">【https://www.youtube.com/watch?v=iCwMQJnKk2c】T21&amp;list = PL-watfeyamnqiee 7 ch 3 q 1 BH 4 qjfaaeven 0</a></p><p id="1f50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是通过傅立叶变换，所有关于时间序列的信息都丢失了。为此，使用频谱图，该频谱图将整个时间分成多个窗口，并对每个窗口执行傅立叶变换。(<a class="ae jc" rel="noopener" href="/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53">https://medium . com/analytics-vid hya/understanding-the-Mel-spectrogram-fc a2 AFA 2 ce 53</a>)</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kj"><img src="../Images/670a625ed531885820d9cb5b424996e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*pksgTHBBFptQOINfpvzRhA.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">image from https://www.mathworks.com/help/dsp/ref/dsp.stft.html</figcaption></figure><p id="fe23" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经具备了开始准备模型的所有特性。让我们看看如何用python实现这些东西</p><pre class="js jt ju jv fd kk kl km kn aw ko bi"><span id="3b56" class="kp kq hh kl b fi kr ks l kt ku">samples, sample_rate = librosa.load('path', sr=16000)<br/>spectrum = librosa.feature.melspectrogram(y=samples, sr=16000, n_mels=128)<br/>samples = librosa.power_to_db(S=spectrum, ref=np.max)</span></pre><p id="f713" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">n_mels是指mel数，它决定了需要对哪些频率进行傅立叶变换计算。</p><p id="162d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一个参数是hop_length，它是每个时间窗口需要有多少数据点。默认值为512，因此，如果音频文件的长度为1秒，选择的n_mels为64，采样率为16000，则在这种情况下，频谱图的输出将为(n_mels，(时间*采样率)/hop_length)，即(64，(16000*1)/512)=(64，32)</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kv"><img src="../Images/114b5d9bae32464dfad38601eecc1157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*AGG_s_ZjMPcwBu_OAt_Djg.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">Mel spectogram Example</figcaption></figure><h1 id="3b7d" class="kw kq hh bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated"><strong class="ak">模型建筑</strong></h1><p id="355d" class="pw-post-body-paragraph ie if hh ig b ih lt ij ik il lu in io ip lv ir is it lw iv iw ix lx iz ja jb ha bi translated">如上所述，mel频谱图的输出是(n_mels，t ),因此需要做出的一个重要决定是，我们应该原样使用它，还是应用转置并使其成为(t，n_mels)。我建立了不同的模型来检查哪个更好。</p><p id="79bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了建立模型，我从conv2d开始参考这个网址【https://www.tensorflow.org/tutorials/audio/simple_audio T4】但是因为这是时间序列数据，我想用lstm，然后我看到了这个博客</p><p id="f55a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://towardsdatascience.com/tensorflow-speech-recognition-challenge-solution-outline-9c42dbd219c9" rel="noopener" target="_blank">https://towards data science . com/tensor flow-speech-recognition-challenge-solution-outline-9 c 42 DBD 219 c 9</a>其中conv2d和lstm都用了。这是一个非常好的想法，因为这是作为图像处理和识别其中的模式，还利用了mel光谱图的时间序列功能。这听起来比只使用lstm或conv2d要好。现在所有这些都需要付诸试验。</p><p id="1fbf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了检查，我做了4个模型</p><ol class=""><li id="6c16" class="jd je hh ig b ih ii il im ip jf it jg ix jh jb ji jj jk jl bi translated">带有conv2d和lstm的Mel (n_mels，t)</li><li id="3d9f" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">只有lstm的Mel (n_mels，t)</li><li id="762a" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">带有conv2d和lstm的Mel (t，n_mels)</li><li id="7842" class="jd je hh ig b ih jm il jn ip jo it jp ix jq jb ji jj jk jl bi translated">只有lstm的Mel (t，n_mels)</li></ol><p id="2840" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我使用以下网络作为基本模型来尝试这些迭代</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ly"><img src="../Images/5642beea67f2fb7eb9ef424ea21d3a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*scRxfwdhSoZW09FzLND_mQ.png"/></div></div></figure><h1 id="e7fa" class="kw kq hh bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated"><strong class="ak">结果</strong></h1><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es lz"><img src="../Images/aef5ae53e8512d7e9712bf71b585410e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*yclefiAgrKPXKcviou1o8g.png"/></div></figure><p id="dc52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这表明转置mel谱图与conv2d+lstm网络一起使用效果更好。</p></div></div>    
</body>
</html>