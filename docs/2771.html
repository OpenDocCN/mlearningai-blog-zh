<html>
<head>
<title>On-Policy v. Off-Policy Reinforcement Learning Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了政策上与政策外强化学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/on-policy-v-off-policy-reinforcement-learning-explained-89054a6cc6?source=collection_archive---------1-----------------------#2022-06-09">https://medium.com/mlearning-ai/on-policy-v-off-policy-reinforcement-learning-explained-89054a6cc6?source=collection_archive---------1-----------------------#2022-06-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9b1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在大而复杂的深度强化学习领域，很容易迷失方向。</p><p id="2a3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个领域中，您会遇到一些细微差别的术语和分类，它们很难相互解析。<em class="jc">什么是基于策略和基于价值的方法？无模型和基于模型的对比呢？这些东西是相互排斥的吗？</em></p><p id="20ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是，作为一个勇敢的探索者，你应该了解这个领域的不同部分，以及不同方法之间的区别。我是来帮你的！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d6aa4a07b13b0f84ca7f6de4109b908b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvhIbw0KAT1oTSKInfe2cw.png"/></div></div></figure><p id="aad1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，没有比区分<strong class="ig hi">政策外和政策内方法</strong>更好的起点了。这两个术语用于描述<em class="jc">如何</em>在一般意义上，代理在培训阶段学习和表现，以及代理学习/表现的两种主要方式。</p><p id="0749" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是首先，重要的是要问:<strong class="ig hi">什么是策略？</strong></p><p id="5678" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用最简单的术语来说，策略<em class="jc">包括代理应该为每个可能的状态</em>采取的建议动作。</p><p id="af6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以把保单想象成一个概率分布。对于每个状态，策略是我们在该状态下采取任何可能行动的概率。该策略可能确定在某个州，有60%的机会采取行动1，30%的机会改变采取行动2，以及10%的机会采取行动3。只是，政策并不能决定概率。这是概率。</p><p id="b831" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">毫不夸张地说，策略<em class="jc">是</em>对于每个状态s ∈ S采取每个动作a ∈ A的概率。策略由<strong class="ig hi"> π </strong>表示。</p><p id="8f04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在任何给定时刻，我们的代理都遵循一个策略<strong class="ig hi">π</strong>T20】k。随着我们的代理人接受培训，这一政策也在不断变化和完善。每次策略稍有变化(我们经过一个梯度步骤，神经网络的权重不同)，现在策略已经变成了<strong class="ig hi">π</strong>T24】k+1。即使两种政策之间的唯一区别是现在更有可能被选择的单一行动，它们也是两种完全不同的政策。</p><p id="4d62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一切都清楚了吗？很好。</p><p id="108d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一个重要的澄清是<strong class="ig hi">行为策略</strong>和<strong class="ig hi">更新策略</strong>之间的定义和区别。</p><p id="225b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">行为政策很简单。这正是我在以上段落中描述的政策。这是一个代理在任何给定的状态下如何<em class="jc">行动</em>(概率等)。).</p><p id="9487" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更新策略是<em class="jc">代理在计算状态-动作对的值时想象它将如何动作</em>。为了更好地理解这一点，让我们首先考虑代理如何计算状态动作对的值(在特定状态下采取特定动作的值)。</p><p id="ca21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个状态-行为对的值就是<em class="jc">预期收益</em>。预期回报是代理人认为在所述状态下采取所述行动后的总体回报，并继续向前直到环境被重置。代理人在想"<em class="jc">好吧，如果我在这个状态采取这个行动，我可能会在状态s结束。如果我在状态</em> s <em class="jc">，我可能会采取行动</em> a <em class="jc">，然后我可能会得到奖励r，并在状态s</em>′<em class="jc">结束。</em>“代理人不断地遵循这个思路。</p><p id="734d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但这里有一个问题:代理人如何知道处于状态s时会采取什么行动？你看，预期收益是在假设代理人遵循特定政策的前提下计算的。该策略是<em class="jc">更新策略。</em></p><p id="8c9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当一个代理计算一个状态-动作对的值时，它在想“如果我采取这个动作，然后遵循更新策略<strong class="ig hi"> π </strong>向前，累积的回报会是什么？”</p><p id="407a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这些解释，我们终于可以达到这篇文章的目的了。这里输入了<em class="jc">符合策略的</em>和<em class="jc">不符合策略的</em>方法之间的区别。</p><p id="0ddf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在基于策略的方法中，行为策略和更新策略是同一个<strong class="ig hi">T21。在非政策方法中，它们是不同的。</strong></p><p id="e7a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在基于策略的方法中，假设代理将遵循<em class="jc">当前行为策略</em>向前，计算状态-动作对的值。因此，该值仅根据此特定策略来定义。</p><p id="d804" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在非策略方法中，情况并非如此。让我们使用Q-Learning，一种非策略方法，来展示这将是什么样子。</p><p id="f903" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Q-Learning中，通常使用一种叫做<em class="jc">ε-贪婪的策略。在这种策略中，代理人的行为开始时完全是随机的，随着时间的推移，慢慢地开始变得更加贪婪和剥削。这将是行为政策。</em></p><p id="d76d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">只是，当我们在Q-Learning中计算一个状态-动作对的值时，我们假设在每个状态下，我们选择的动作使它认为它可以获得的未来回报最大化。我们的更新政策是一个贪婪的政策。</p><p id="3daa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Q-Learning中，我们的更新策略假设我们在遵循一个贪婪的策略，即我们选择我们认为会给我们带来最大回报的行动。</p><p id="2dd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，我们是半随机地行动，而<strong class="ig hi">并不是</strong>一直贪婪。我们的行为方针不是完全贪婪。</p><p id="cb9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正因为如此，我们的更新策略(一个<em class="jc">贪婪</em>策略)不同于我们的行为策略(一个<em class="jc">ε-贪婪策略)</em>使得Q-Learning成为一种非策略方法。</p></div><div class="ab cl jp jq go jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="ha hb hc hd he"><p id="b472" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">了解这些区别是很重要的，我相信你会继续探索RL领域，掌握这些新知识并准备好向前探索！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/1b59406e5522eaa78bf542a807563418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HT6SWJdZotOcDIb85LVLnw.jpeg"/></div></div></figure><p id="1282" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">顺便说一句，如果你仍然对这个话题感到不清楚，请在twitter @jereminuer上给我发个DM，我很乐意解释更多！</em></p><div class="jx jy ez fb jz ka"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kb ab dw"><div class="kc ab kd cl cj ke"><h2 class="bd hi fi z dy kf ea eb kg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kh l"><h3 class="bd b fi z dy kf ea eb kg ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ki l"><p class="bd b fp z dy kf ea eb kg ed ef dx translated">medium.com</p></div></div><div class="kj l"><div class="kk l kl km kn kj ko jn ka"/></div></div></a></div></div></div>    
</body>
</html>