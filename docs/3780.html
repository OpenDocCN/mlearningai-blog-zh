<html>
<head>
<title>K-Means to K-Modes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-均值到K-模式</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/k-means-to-k-modes-f7af5f1a3aa0?source=collection_archive---------3-----------------------#2022-10-20">https://medium.com/mlearning-ai/k-means-to-k-modes-f7af5f1a3aa0?source=collection_archive---------3-----------------------#2022-10-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/719455cfd53422328834c9eac39e5b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CuZeeuhNOTeM6hC58NGNng.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" href="https://www.pexels.com/@polina-tankilevitch/" rel="noopener ugc nofollow" target="_blank">Polina Tankilevitch</a></figcaption></figure><div class=""/><p id="d895" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">蔡寨村坐落在中国山区的河两岸。这个村子以生产豆腐而闻名。以前，专业人员会仔细检查作物，决定它们是否适合做豆腐。然而，随着一个新成立的合作社的运行，该村每天加工大约100公斤大豆。这意味着每天早上要仔细检查近80万颗种子。没有一个最努力工作的团队能够每天检查这么多的大豆。</p><p id="e416" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这种情况下，机器学习非常方便。使用无监督学习聚类算法，我们将检查相对较小的一批大豆，并定义不健康作物的群体。为此，我们将使用K-Modes聚类算法。为了更好地解释K-模式，我将首先提到K-均值，它是K-模式的前身。</p><h1 id="01a4" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">什么是K均值，为什么它不是分类数据集的一个选项？</h1><ol class=""><li id="e085" class="kq kr hx iw b ix ks jb kt jf ku jj kv jn kw jr kx ky kz la bi translated">K-Means是一种无监督聚类算法，它使用欧几里德距离(来自几何类)来测量两个向量(也称为数据点)之间的差异。这种类型的测量在彼此没有任何明确关系的分类对象的空间中是没有意义的。</li><li id="37a5" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">K-Means通过改变聚类的均值来最小化成本函数，或者简单地说，它使用原始聚类中所有对象的均值作为中心数据点，并基于该均值更新聚类。想象一下，在四月、五月和六月之间找到一个平均值。</li></ol><h1 id="84ef" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">k-表示循序渐进</h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="4c3f" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># generate some artificial numerical data for classification <br/></strong>from sklearn.datasets import make_blobs<br/>points, center = make_blobs(n_samples=100, centers=3, n_features=2)</span><span id="75dc" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># create a dataframe<br/></strong>df = pd.DataFrame(dict(x=points[:,0], y=points[:, 1], label=center))</span></pre><p id="d3af" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是我得到的。</p><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="7014" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，为了实验清楚起见，我必须删除集群标签。</p><p id="a956" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">步骤一:</strong>生成随机点作为每个聚类初始中心点。这里我们有三个</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="bf78" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># drop labels</strong><br/>points = df.drop('label', axis=1)</span><span id="8fa0" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># generate initial centroids</strong><br/>centroids = points.sample(3)<br/>centroids = centroids.reset_index(drop=True)</span></pre><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="16a8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第二步:</strong>对于数据集中的每个点，计算到每个中心点的距离。将每个变量分配到其最近的中心点。</p><p id="980f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下面是一个很小的函数:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="263f" class="lp jt hx ll b fi lq lr l ls lt">def get_nearest_centroid(df, centroids, iter):<br/><em class="lx">'''<br/>For each point in the dataset calculates<br/>nearest cluster<br/>'''</em></span><span id="3562" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># make a copy of the data-frame</strong><br/>df_new = pd.DataFrame(df[['x', 'y']])</span><span id="43aa" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># find distances</strong><br/>distances = distance.cdist(df, centroids, 'euclidean')</span><span id="9bab" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># find nearest centroid</strong><br/>nearest_centroid = np.argmin(distances, axis=1)</span><span id="9894" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># create a new column with updated centroids</strong><br/>df_new['cluster_'+iter] = nearest_centroid</span><span id="864a" class="lp jt hx ll b fi lu lr l ls lt">return df_new</span></pre><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="b155" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第三步:</strong>用新形成的集群取代旧的中心。这是一个更新中心的函数</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="59a1" class="lp jt hx ll b fi lq lr l ls lt">def get_centroids(df, column_header):<br/><em class="lx">'''<br/>Averages clusters by mean to get<br/>updated centroids<br/>'''</em></span><span id="2950" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># group by cluster to find a mean</strong><br/>new_centroids = df.groupby(column_header).mean()</span><span id="f4d1" class="lp jt hx ll b fi lu lr l ls lt">return new_centroids</span></pre><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="e506" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第四步:</strong>迭代<strong class="iw hy"> </strong>直到集群不再更新</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a3d8" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># write a for i loop to calculate same centroids in less code</strong><br/>pass_i = get_nearest_centroid(points, centroids, str(0))<br/>centroids_i = get_centroids(pass_i, 'cluster_'+str(0))</span><span id="3b19" class="lp jt hx ll b fi lu lr l ls lt">for i in range(1, 10):<br/>  pass_before_i = pass_i.copy()<br/>  centroids_before_i = centroids_i.copy()<br/>  pass_i = get_nearest_centroid(points, centroids_i, str(i))<br/>  centroids_i = get_centroids(pass_i, 'cluster_'+str(i))<br/>    if pass_before_i['cluster_'+str(i - 1)].equals(pass_i['cluster_'+str(i)]) and           centroids_before_i.equals(centroids_i):<br/>    break</span></pre><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="a3d7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用<em class="lx"> sklearn </em>库，你可以用更少的代码获得同样的结果。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="e164" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># load k-means algorithm</strong><br/>from sklearn.cluster import KMeans</span><span id="aed7" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># instantiate</strong> <br/>kmeans = KMeans(n_clusters=3)</span><span id="b1da" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># fit</strong><br/>kmeans.fit(points)</span><span id="ca3f" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># retrieve labels and center points</strong><br/>labels = kmeans.labels_<br/>cluster_centers = kmeans.cluster_centers_<br/>cluster_centers=pd.DataFrame(cluster_centers, columns=[['x', 'y']])</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ly"><img src="../Images/60c0ab3323983676f1d28adbab83afbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPCL-9d-SdH4qdGHMDSOIg.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx">Clusters calculated by hand and clusters calculated using sklearn</figcaption></figure><figure class="lg lh li lj fd hj"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="290e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以，现在，我已经解释了K-Means算法背后的数学，我可以更深入地研究K-Modes。</p><h1 id="b27b" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">为什么K-Modes适用于分类特征？</h1><ol class=""><li id="acf9" class="kq kr hx iw b ix ks jb kt jf ku jj kv jn kw jr kx ky kz la bi translated">K-Modes使用两个向量变量之间的差异数来定义它们有多接近。</li><li id="4c0b" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">K-Modes用模式代替聚类的均值。</li><li id="96bb" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">K-Modes使用基于频率的方法来寻找集群的模式。</li></ol><p id="f158" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae hu" href="https://archive.ics.uci.edu/ml/datasets/Soybean+(Large)" rel="noopener ugc nofollow" target="_blank">来自UCI的著名大豆数据集</a>完全符合我改进大豆选择过程的目标。</p><p id="6619" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我加载了数据集，并删除了所有的未知数(空值)。我最终得到了三个主要的集群，每个集群由40个变量和12个较小的变量组成。和K-Means的例子一样，我决定只坚持三个主要的聚类，所以我放弃了其他的。我还对数据集进行了重新采样，以便所有聚类均匀分布。为了便于翻译，我给每堂课都分配了一个数字。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="93f9" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># filter the dataset to only include three main clusters</strong><br/>filters = list(df_large['class'].value_counts()[:3].index)<br/>df = df_large[df_large['class'].isin(filters)].copy()</span><span id="27af" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># assign a number to each class</strong><br/>df['class_num'] = df['class'].replace({'brown-spot':0, 'alternarialeaf-spot':1, 'frog-eye-leaf-spot':2})</span><span id="5a12" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># resample dataset</strong><br/>df = df.sample(120)<br/>df = df.reset_index(drop=True)<br/>print(df.shape)<br/>df.head()</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lz"><img src="../Images/39f718e841454bf7de975423571a4c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QTW1RAJ5325sctiQCp5J2Q.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx">Resampled Soybean (Large) Data Set</figcaption></figure><p id="9105" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我将无法支持带有可视化绘图的代码，因为数据集有3个以上的特征，但逻辑几乎是相同的。</p><p id="c0bf" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第一步:</strong>删除包含集群标签的列，<strong class="iw hy"> </strong>设置初始模式</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="7018" class="lp jt hx ll b fi lq lr l ls lt"><strong class="ll hy"># generate initial modes</strong><br/>clusters = df_new.sample(3)<br/>clusters = clusters.reset_index(drop=True)<br/>clusters</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ma"><img src="../Images/b559c7220dede945322813ec3c5c165a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5HaXI_wRdeMNYRVTxwWPrA.png"/></div></div></figure><p id="442f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第二步:</strong>对于数据集中的每个点，计算它与每个模式之间的差异数。将三种模式中的一种分配给数据集中的每个变量。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="aaf2" class="lp jt hx ll b fi lq lr l ls lt">def get_nearest_cluster(df, clusters, iter):<br/>'''<br/>For each column in the dataframe calculates<br/>nearest cluster<br/>'''<br/><strong class="ll hy"># make a copy of initial dataframe, drop iteration column if there</strong><br/>df_new = df[df.columns.drop(list(df.filter(regex='iter')))].copy()</span><span id="526c" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># create a list of differences</strong><br/>df_diffs = []</span><span id="162c" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># iterate through rows and clusters</strong><br/>for index, row in df_new.iterrows():<br/>  cluster_differences = []<br/>    for index_c, cluster in clusters.iterrows():<br/><strong class="ll hy"># count number of differences</strong><br/>    diff = np.array(cluster) - np.array(row)<br/>    non_zero_diffs = np.count_nonzero(diff)<br/>    cluster_differences.append(non_zero_diffs)<br/>  df_diffs.append(cluster_differences)</span><span id="2aee" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># find a minimum difference and add it to the dataset</strong><br/>df_new['iter_'+iter] = np.argmin(df_diffs, axis=1)<br/>return df_new</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mb"><img src="../Images/67da875b58eaf100222b025e2a7ea834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-CT8O5LkG-0M285amNHbA.png"/></div></div></figure><p id="9373" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第三步:更新模式</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f59b" class="lp jt hx ll b fi lq lr l ls lt">def new_modes(df, column_header):<br/><em class="lx">'''<br/>Udates modes of the clusters based on<br/>prior clusters and given datapoints<br/>'''</em><br/><strong class="ll hy"># iterate clusters</strong><br/>modes = []<br/>for i in df[column_header].unique():<br/><strong class="ll hy"># cluster modes</strong><br/>  modes_i = []<br/>  df_sub = df[df[column_header] == i]<br/><strong class="ll hy"># iterate columns</strong><br/>  for col in df_sub:<br/>    mode = df_sub[col].value_counts().index[0]<br/>    modes_i.append(mode)<br/>  modes.append(modes_i)</span><span id="9095" class="lp jt hx ll b fi lu lr l ls lt"><strong class="ll hy"># turn into a dataframe</strong><br/>modes = pd.DataFrame(modes, columns=df.columns)<br/>modes = modes.drop(column_header, axis=1)<br/>return modes</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mc"><img src="../Images/edb336a4c8a6546cc90691d8692d7d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8oPgR2-y0mS4cyreQN7xw.png"/></div></div></figure><p id="9f4c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hy">第四步:</strong>迭代<strong class="iw hy"> </strong>直到集群不再更新</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="fb42" class="lp jt hx ll b fi lq lr l ls lt">pass_i = get_nearest_cluster(df_new, initial_centroids, '0')<br/>centroids_i = new_modes(pass_i, 'iter_0')</span><span id="9ebc" class="lp jt hx ll b fi lu lr l ls lt">for i in range(1, 100):<br/><strong class="ll hy"># make a copy to compare to<br/>  </strong>pass_before_i = pass_i.copy()<br/>  centroids_before_i = centroids_i.copy()<br/><strong class="ll hy"># calculate new clusters and centroids<br/>  </strong>pass_i = get_nearest_cluster(pass_before_i, centroids_before_i, str(i))<br/>  centroids_i = new_modes(pass_i, 'iter_'+str(i))<br/>  if centroids_i.equals(centroids_before_i):<br/>    print(f'It tool {i+1} iterations to find clusters')<br/>    break</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es md"><img src="../Images/ff197169d9963e0ee7ef77030b9331aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2pys_y9O91IE9PxKEO7hA.png"/></div></div></figure><p id="3e9d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">同样，你可以用更少的代码用<strong class="iw hy"> kmodes </strong>库获得同样的结果。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="1fa1" class="lp jt hx ll b fi lq lr l ls lt">from kmodes.kmodes import KModes</span><span id="de77" class="lp jt hx ll b fi lu lr l ls lt">k = KModes(n_jobs = -1,n_clusters=3,init='Huang', random_state = 0)<br/>clusters = k.fit_predict(df_new)</span></pre><figure class="lg lh li lj fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/7df3791c9c5252c718a914b921336bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXzLUdXf_dosHE66kDhiNA.png"/></div></div></figure></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><p id="95b0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后要提到的一点是，当您手动计算k模式时，同一数据集的准确度分数可能会随时间而变化。发生这种情况是因为每个集群可能有多种模式，除非对算法进行编码，否则无法控制选择哪种算法。</p><p id="d001" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我希望这篇文章能帮助你更好地理解K-Means和K-Modes。祝你的数据集群好运！</p><p id="6ac0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">完整代码可在<a class="ae hu" href="https://github.com/sugarfreecode/DS-Unit-1-Sprint-3-Data-Storytelling/blob/master/k_means_to_k_modes.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><p id="f274" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">文章是根据z .黄。<em class="lx"> </em> <a class="ae hu" href="https://cse.hkust.edu.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lx">对具有分类值的大型数据集进行聚类的k-Means算法的扩展</em> </a> (1998)。数据挖掘和知识发现。2(3): 283–304</p><p id="3e4d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<a class="ae hu" href="https://www.theguardian.com/artanddesign/2021/mar/24/chinas-rural-revolution-architects-rescuing-villages-oblivion-tofu-rice-wine-lotus-tea" rel="noopener ugc nofollow" target="_blank">卫报</a>上阅读关于<a class="ae hu" href="https://www.archdaily.com/943412/tofu-factory-dna" rel="noopener ugc nofollow" target="_blank">中国蔡寨村一个新成立的村合作社</a></p><div class="hg hh ez fb hi mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hy fi z dy mr ea eb ms ed ef hw bi translated">Mlearning.ai提交建议</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na ho mm"/></div></div></a></div></div></div>    
</body>
</html>