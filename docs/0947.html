<html>
<head>
<title>Disastrous Tweets Classification using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT的灾难性推文分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/disastrous-tweets-classification-using-bert-ceb8e7be6c36?source=collection_archive---------1-----------------------#2021-08-30">https://medium.com/mlearning-ai/disastrous-tweets-classification-using-bert-ceb8e7be6c36?source=collection_archive---------1-----------------------#2021-08-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/38b0e4f40b3f78c6b823461284c67305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fsZ39nHRMPzvcMsJ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.google.com/search?q=trees&amp;rlz=1C1CHBD_enIN956IN956&amp;sxsrf=ALeKk00a1i4Yr3ps-Ry__s7DrhfWiwB8uw:1630002116227&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwiEqPuIp8_yAhVJzjgGHXrOAkYQ_AUoAXoECAIQAw&amp;biw=1280&amp;bih=577#imgrc=QzhuPn-k-uepRM" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><blockquote class="iu iv iw"><p id="64e1" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="ja hi">概述</strong></p></blockquote><p id="24bb" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在这篇文章中，我将使用最先进的模型来区分推文是灾难性的还是非灾难性的。我将使用BERT在KTrain的keras API的帮助下对推文进行分类。如果您以前使用过tensorflow，那么您可能已经知道keras是tensorflow的包装器，就像KTrain是keras和tensorflow的包装器一样。KTrain内部包含各种预处理模块，使NLP任务更加简单。如果你对KTrain比较好奇那就别忘了访问<a class="ae it" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank"> <strong class="ja hi">这个链接</strong> </a> <strong class="ja hi">。</strong>我采集的数据集由各种推文评论组成，并被分为两个标签，其中<strong class="ja hi">标签1 </strong>描述推文是灾难性的，而<strong class="ja hi">标签0 </strong>表明推文不是灾难性的。</p><p id="164b" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤1:导入必要的库</strong></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="40f4" class="ki kj hh ke b fi kk kl l km kn">import pandas as pd<br/>import numpy as np<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.layers import<br/>                            Embedding,Dense,SpatialDropout1D,Dropout<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.optimizers import Adam<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span></pre><p id="2d95" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤2:-数据理解</strong></p><p id="0909" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在这一步中，我们将清楚地了解数据集的不同方面。如空值、类的数量(是二进制类还是多类)、数据集的形状等。</p><p id="9483" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">打印数据帧中的所有列</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="9911" class="ki kj hh ke b fi kk kl l km kn">data.columns </span><span id="c337" class="ki kj hh ke b fi ko kl l km kn">[out]&gt;&gt;Index(['id', 'keyword', 'location', 'text', 'target'],  dtype='object')</span></pre><p id="522f" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">数据集的形状，即:-数据集中有多少行和列</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="20f5" class="ki kj hh ke b fi kk kl l km kn">data.shape <br/>[out]&gt;&gt; (7613, 5)  #dataset has 7613 rows and 5 columns</span></pre><p id="9111" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">获取与数据集相关的所有信息，如有多少空值。每列的数据类型是什么等等。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="fdf9" class="ki kj hh ke b fi kk kl l km kn">data.info </span><span id="20f4" class="ki kj hh ke b fi ko kl l km kn">[out]&gt; &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7613 entries, 0 to 7612 Data columns (total 5 columns):  #   Column    Non-Null Count  Dtype  ---  ------    --------------  -----   <br/>0   id        7613 non-null   int64   <br/>1   keyword   7552 non-null   object  <br/>2   location  5080 non-null   object  <br/>3   text      7613 non-null   object <br/>4   target    7613 non-null   int64  dtypes: int64(2), object(3) memory usage: 297.5+ KB</span></pre><p id="ce02" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">数字和分类数据的统计描述。通过使用<code class="du kp kq kr ke b">data.describe()</code>,我们将获得数据集的统计描述，如平均值、中位数、众数、百分位等。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="9ff7" class="ki kj hh ke b fi kk kl l km kn">data.describe() #description of numerical data</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/36f642c1427c46e9960b0168b20314a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*ZOo8OD4-NVGTy64NSmJffA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 1</figcaption></figure><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="d06e" class="ki kj hh ke b fi kk kl l km kn">cat_data=(data.dtypes[data.dtypes=='object']).describe()<br/>cat_data #description of categorical features</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/a41b5db24e87a301930069e99bdad88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*t1LJQNMj4xsBHMH6ZciLXA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 2</figcaption></figure><p id="419b" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">统计<strong class="ja hi">灾难性(1) </strong>和<strong class="ja hi">非灾难性(0) </strong>推文的数量。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="68b3" class="ki kj hh ke b fi kk kl l km kn">daat['target'].value_counts()</span><span id="9f49" class="ki kj hh ke b fi ko kl l km kn">[out]&gt;&gt;    0    4342 <br/>           1    3271<br/>           Name: target, dtype: int64</span></pre><p id="f6d6" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">第三步:-探索性数据分析</strong></p><p id="e144" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">绘制灾难性和非灾难性推文的分布。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="ed3b" class="ki kj hh ke b fi kk kl l km kn">plt.figure(figsize=(8,6))<br/>sns.set_style(style='darkgrid')<br/>sns.countplot(data['target'])<br/>plt.title('Disastrous and Non-Disastrous Tweets')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ku"><img src="../Images/059941acb4a26969c913858f18d05cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*digGvXa_cK0uVhrjhlwofw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 3</figcaption></figure><p id="f095" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们用饼状图看看灾难性和非灾难性推文的百分比贡献。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="5f74" class="ki kj hh ke b fi kk kl l km kn">plt.figure(figsize=(6,8))<br/>sns.set_style("darkgrid")data['target'].value_counts().plot.pie(autopct='%0.2f%%')<br/>plt.title("Percentage Contribution")<br/>plt.xlabel("percent contribution")<br/>plt.ylabel("target")<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/b0bb2f41431774e7ba3529637b4dce9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*R1aCsk2hys9DR1GOq4cJgg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 4</figcaption></figure><p id="f114" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们看看推文中的字符分布数量，在此之前，我们需要对推文进行预处理。我们将删除不需要的文本，即:- url，特殊符号，如@，！、#等。我们还将计算字数，字符数，平均单词长度等。为此，我在这里导入了预处理文件<a class="ae it" href="https://github.com/laxmimerit/preprocess_kgptalkie" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="1235" class="ki kj hh ke b fi kk kl l km kn">import preprocess_kgptalkie as akhil<br/>df=akhil.get_basic_features(data)<br/>df.head()</span></pre><p id="2be0" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们做更多的绘图来理解数据框中文本的本质</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="1336" class="ki kj hh ke b fi kk kl l km kn">sns.kdeplot(df['char_counts'],shade=True,color='green')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kw"><img src="../Images/882c3e1dd80515d3df82eabf88c0787f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*mFqAwtJTiwxnmNToJa7vBw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 5</figcaption></figure><p id="9086" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们来看看推文的长度是如何根据它们的性质而变化的。即:-对tweet字符进行计数，并将它们与灾难性和非灾难性tweet进行比较，并将比较哪种情况下tweet的长度更长。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="8477" class="ki kj hh ke b fi kk kl l km kn">plt.figure(figsize=(6,8))<br/>sns.kdeplot(df[df['target']==1]<br/>            ['char_counts'],color='red',shade=True)<br/>sns.kdeplot(df[df['target']==0]<br/>            ['char_counts'],color='green',shade=True)<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/310711ded943e2cb134d89958eccd04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*O0zxMczREuEN69ursZHBCQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Figure 6</figcaption></figure><p id="a015" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">灾难性的推文比非灾难性的推文更长。因此，如果推文很长，那么这些推文可能与抱怨或不满有关。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="2d10" class="ki kj hh ke b fi kk kl l km kn">#Distribution of stop words on both the classes<br/>sns.boxplot(data['target'],y=data['stopwords_counts'])<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/e22ac92602220044597131e58351773d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*D1z89OZBa1mNn87YcS2T0w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 7</figcaption></figure><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="c3af" class="ki kj hh ke b fi kk kl l km kn"><br/>#let's see how random the hastag has been used in both the situaton<br/>sns.violinplot(x=data['target'],y=data['hastag_counts'])<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/bb6a85ded98144988fc2d0fdde87891b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*fp3fQOQC1LRU8qTibwaGyg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 8</figcaption></figure><p id="99c9" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">如果我们看到上图，我们会发现在灾难性的推文中使用的标签会稍微多一点。</p><p id="61bd" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们看看数据集中最常用和最不常用的单词是什么。基于这些常用词，我们可以了解推文的整体性质。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="ab0f" class="ki kj hh ke b fi kk kl l km kn">freq_occuring=akhil.get_word_freqs(data,'text')<br/>top_20=freq_occuring[:20]<br/>sns.barplot(top_20.index,top_20.values)<br/>plt.xticks(rotation=70)<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es la"><img src="../Images/340fbd355e44b7310be0d908208096e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*6IVjLR0e6O7J7WLKGia3ag.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 9</figcaption></figure><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="3a06" class="ki kj hh ke b fi kk kl l km kn">#least 20 occurring words<br/>least_20=freq_occuring[:20]<br/>sns.barplot(least_20.index,least_20.values)<br/>plt.xticks(rotation=70)<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/46b35a1af67d9a0d15f8e12fa2216360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MhXr6HiaLPg8jFcSLdF51g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 10</figcaption></figure><p id="1f55" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤4:-数据清理</strong></p><p id="5fbe" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">众所周知，tweets包含标签，特别symbols(@,#,$,%,^,&amp;,*！)，网址，号码等。我们需要清除所有那些不需要的文字。因此，在这一节中，我们将对文本进行分类，以便它可以是人类可读的格式，没有任何特殊字符。</p><p id="a71d" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">导入必要的文本清理库</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="a2a1" class="ki kj hh ke b fi kk kl l km kn">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem.porter import PorterStemmer<br/>ps=PorterStemmer()<br/>nltk.download('stopwords')<br/>stopwords=set(stopwords.words('english'))</span></pre><p id="9460" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们清理数据…</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="d88a" class="ki kj hh ke b fi kk kl l km kn">def cleaner(text):<br/>    cleaned=text.replace("//"," ").replace("."," ")<br/>    cleaned=re.sub(r'[^a-zA-Z]'," ",cleaned)<br/>    cleaned=cleaned.strip() #removing whitespace<br/>    cleaned=cleaned.lower() #converting into lower case words<br/>    cleaned=re.sub(r'\w+\d+'," ",cleaned)#remove alphanumeric words<br/>    cleaned=ps.stem(cleaned) #stemming <br/>    cleaned=[word for word in cleaned.split if len(word)&gt;2]<br/>    cleaned=" ".join(cleaned)<br/>    return cleaned<br/>data['text']=data['text'].apply(lambda text:cleaner(text))<br/>#let's check for some text<br/>data['text'][0:10]</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/7ff60c5ef27bb49bff0751f427f5de58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQ4pOO4UkMmyF-xxE78DLg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 11</figcaption></figure><p id="9d1d" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在我们将进行最常见的可视化，即:类<strong class="ja hi"> 0 </strong>和类<strong class="ja hi"> 1 </strong>的文字云可视化。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="8b19" class="ki kj hh ke b fi kk kl l km kn">from wordcloud import WordCloud,STOPWORDS<br/>dataset=akhil.get_word_freqs(data[data['target']==1],'text')<br/>print(dataset.index)<br/>dataset=" ".join(dataset.index)<br/>word_cloud=WordCloud(max_font_size=60,background_color='white').generate(dataset)<br/>plt.imshow(word_cloud)<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es ld"><img src="../Images/bfd944234d819d4b4a8dbd190b141b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*Dp3pc1GRSRW7D7REQHcW5A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 12</figcaption></figure><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="62d5" class="ki kj hh ke b fi kk kl l km kn">from wordcloud import WordCloud,STOPWORDS<br/>dataset=akhil.get_word_freqs(data[data['target']==0],'text')<br/>print(dataset.index)<br/>dataset=" ".join(dataset.index)<br/>word_cloud=WordCloud(max_font_size=60,background_color='white').generate(dataset)<br/>plt.imshow(word_cloud)<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div class="er es le"><img src="../Images/7ac6dd4c906e556d2a8da892a33250ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*o-72uk9uwdnfDRao0h8Now.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">figure 13</figcaption></figure><p id="0dad" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="ja hi">步骤5:-微调BERT模型</strong></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="dbbc" class="ki kj hh ke b fi kk kl l km kn">import ktrain<br/>from ktrain import text<br/>(x_train, y_train),<br/>   (x_test,y_test),preprocess=text.texts_from_df(data,  <br/>                                text_column='text'<br/>                                ,label_columns='target',maxlen=50,<br/>                                 preprocess_mode='bert')</span></pre><p id="8916" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们一点一点地理解上面的代码…</p><p id="41b2" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">ktrain将对数据帧中的数据进行预处理，并从中返回五个变量，它们是<code class="du kp kq kr ke b">(x_train,y_train)(x_test,y_test)</code>和<code class="du kp kq kr ke b">preprocess</code>。<code class="du kp kq kr ke b">text_from_df</code>里面的参数是:</p><p id="bf2e" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><code class="du kp kq kr ke b">data</code>已经进行运算的数据集。<code class="du kp kq kr ke b">text_column</code>数据帧/数据集中存在文本列。<code class="du kp kq kr ke b">label_columns</code>数据集中存在目标/输出列。<code class="du kp kq kr ke b">maxlen</code>一个句子中可以出现的单词的最大长度在BERT的情况下，我们可以取最大长度<code class="du kp kq kr ke b">512</code>，如果我们取超过<code class="du kp kq kr ke b">512</code>的句子长度，就会产生错误。<code class="du kp kq kr ke b">preprocess_mode</code>这说明了预处理是如何完成的，在我的例子中，我已经使用BERT预处理了文本数据。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="7a1b" class="ki kj hh ke b fi kk kl l km kn">model=text.text_classifier('bert',train_data=(x_train,y_train) <br/>                                             ,preproc=preprocess)</span></pre><p id="fe2d" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在我们将使用<code class="du kp kq kr ke b">get_learner</code>,这将包装模型和数据，然后进一步用于结果的最终预测。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="3f33" class="ki kj hh ke b fi kk kl l km kn">learner=ktrain.get_learner(model,train_data=<br/>         (x_train,y_train),val_data=(x_test,y_test),batch_size=64)</span></pre><p id="fde0" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们将为模型给出一些学习速率和时期，以预测可能的最佳结果。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="5d08" class="ki kj hh ke b fi kk kl l km kn">learner.for_onecycle(lr=1e-5,epochs=4)</span></pre><figure class="jz ka kb kc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lf"><img src="../Images/8c41e743715116bf68d85cca7d4d4521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LIVoXlT6SAXsx1iy2PFxHA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Figure 14</figcaption></figure><p id="fb36" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我得到了大约58%的准确率。我可能需要更多地关注预处理和清理方面，以提高模型的准确性。让我们预测热门推文的结果。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="2f28" class="ki kj hh ke b fi kk kl l km kn">prediction=ktrain.get_predictor(learner.model,preprocess)<br/>data=["US did this! Loudly crying faceLoudly crying face"]<br/>prediction.predict(data)</span><span id="9b30" class="ki kj hh ke b fi ko kl l km kn">[out]&gt;&gt; 'not-Disastrous'</span></pre><p id="b807" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">结论:-</p><p id="3b7f" class="pw-post-body-paragraph ix iy hh ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这是一个关于如何解决NLP模型的基本介绍性方法，请耐心等待，稍后会有更多使用张量流和Pytorch进行BERT微调的博客。请在评论框中提出你的宝贵意见。</p></div></div>    
</body>
</html>