<html>
<head>
<title>ElegantRL Demo: Stock Trading Using DDPG (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ElegantRL演示:使用DDPG进行股票交易(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/elegantrl-demo-stock-trading-using-ddpg-part-i-e77d7dc9d208?source=collection_archive---------2-----------------------#2021-03-28">https://medium.com/mlearning-ai/elegantrl-demo-stock-trading-using-ddpg-part-i-e77d7dc9d208?source=collection_archive---------2-----------------------#2021-03-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="ac26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">深度确定性策略梯度算法教程(DDPG)</p><p id="3c56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由Steven Li、<a class="ae jc" href="https://twitter.com/XiaoYangLiu10" rel="noopener ugc nofollow" target="_blank">、</a>和Zeng撰写的这篇文章描述了在ElegantRL中使用深度确定性策略梯度(DDPG)算法实现一个股票交易应用程序[1]。股票交易在投资中起着至关重要的作用，开发一个能够在动态的股票市场中获利的自动化代理是一个挑战。为了展示ElegantRL的出色表现，我们展示了如何训练一个有效的交易代理。</p><p id="28f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请查看ElegantRL库的介绍性<a class="ae jc" href="https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b" rel="noopener" target="_blank">文章</a>和【1】的<a class="ae jc" href="https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b" rel="noopener" target="_blank">媒体博客</a>。</p><p id="e485" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一部分的<strong class="ig hi">中，我们将讨论结合股票交易任务的问题公式的DDPG算法。在<strong class="ig hi">第二部分</strong>中，我们将讨论该应用的实现细节及其易于定制的特性。这篇文章之后，你可能会设计自己的股票交易代理，并开始赚钱！</strong></p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="b683" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">股票交易任务</strong></h1><p id="1376" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">股票交易被认为是机器学习中最热门的话题之一，因为一个有利可图的人工智能代理对几乎每个人来说都是不可抗拒的。使用智能交易代理自动操纵股票账户，你唯一需要做的就是打开投资组合，躺在沙发上，数美元。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es kn"><img src="../Images/54749fc6b0a1da1ae0bd8cec856b6322.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*LCJmeYc_63MaZA7CkbcaEQ.gif"/></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 1. The bunny counts the money. [Image from the <a class="ae jc" href="https://tenor.com/view/bugs-bunny-looney-tunes-cash-money-counting-money-gif-17544086" rel="noopener ugc nofollow" target="_blank">link</a>].</figcaption></figure><p id="295f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">深度强化学习(DRL)已经证明了它在交易领域的能力。在这篇文章中，我们的目标是展示一个受过股票市场数据训练的DDPG经纪人可以在回溯测试中获利。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="5ad2" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">问题表述</strong></h1><p id="7524" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">DRL方法有两个核心组成部分:<strong class="ig hi">交易代理</strong>和<strong class="ig hi">环境</strong>。交易代理和市场环境之间的相互作用如图2所示:</p><ol class=""><li id="5f1f" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">代理观察市场环境的当前状态。</li><li id="bf98" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">在当前状态下，代理根据其策略进行操作。</li><li id="62dc" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">环境基于动作和状态转换向前迈出一步，即转换，然后产生奖励。</li><li id="4d11" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">代理收到奖励，并使用转换来更新其策略。</li></ol><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/f0156c5bec6f4c81112281a3e7ca4e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ra8Y9Rt2Eu-2-werdNmXlQ.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 2. Overview of stock trading using a DRL agent. [Image by authors].</figcaption></figure><p id="d1fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">形式上，我们将股票交易建模为马尔可夫决策过程(MDP)，并将交易目标表述为期望回报的最大化:</p><ul class=""><li id="7129" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb ls lf lg lh bi translated">state<em class="lt">s</em>=【b，<strong class="ig hi">T3】p，<strong class="ig hi"> <em class="lt"> h </em> </strong>:一个向量，包含余额b，股票价格<strong class="ig hi"> <em class="lt"> p </em> </strong>，股票份额<strong class="ig hi"> <em class="lt"> h </em> </strong>。<strong class="ig hi"> <em class="lt"> p </em> </strong>和<em class="lt"/></strong>是维度为<em class="lt"> D </em>的向量，其中<em class="lt"> D </em>表示股票的数量。</li><li id="d3fa" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">动作<strong class="ig hi"> <em class="lt"> a </em> </strong>:对<em class="lt"> D </em>股票的动作向量。每只股票允许的操作包括<em class="lt">卖出</em>、<em class="lt">买入</em>或<em class="lt">持有</em>，分别导致<strong class="ig hi"><em class="lt"/></strong>中股票份额的减少、增加或不变。</li><li id="51b6" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">奖励<em class="lt"> r </em> ( <em class="lt"> s，</em> <strong class="ig hi"> <em class="lt"> a </em> </strong> <em class="lt">，s’</em>):采取行动<strong class="ig hi"> <em class="lt"> a </em> </strong>处于状态<em class="lt"> s </em>到达新状态<em class="lt">s’</em>的资产价值变化。</li><li id="7420" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">策略π( <em class="lt"> s </em>):状态<em class="lt"> s </em>的交易策略，是状态<em class="lt"> s </em>的行动概率分布。</li><li id="089a" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">Q-函数<em class="lt"> Q </em> ( <em class="lt"> s </em>，<strong class="ig hi"> <em class="lt"> a </em> </strong>):采取行动<strong class="ig hi"> <em class="lt"> a </em> </strong>在状态<em class="lt"> s </em>下的预期收益(报酬)遵循政策π。</li><li id="3d7c" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">状态转换:在采取动作<strong class="ig hi"><em class="lt"/></strong>后，股票数量<strong class="ig hi"><em class="lt"/></strong>被修改，如图3所示，新的投资组合是余额和股票总价值的总和。</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lu"><img src="../Images/f377924abf3d3d37104c739715b6413a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GXNQWOBEr8uDQvwL"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 3. State transition. A starting portfolio value with three actions results in three possible portfolios. Note that “hold” may lead to different portfolio values due to the changing stock prices. [Image from [2]].</figcaption></figure></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="0195" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">我们为什么选择DDPG算法？</strong></h1><p id="7f1c" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">深度确定性策略梯度(DDPG)算法[3]是行动者-批评家框架下的无模型非策略算法，它可以被认为是深度Q网络(DQN)和策略梯度的结合。我们选择DDPG算法的主要原因如下:</p><ul class=""><li id="f2eb" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb ls lf lg lh bi translated">与其他先进的(SOTA)算法相比，它非常简单，是ElegantRL中DRL算法的一个很好的例子。由于简单，用户可以更专注于股票交易策略，并从回测中选择最佳算法。</li><li id="4ee3" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">与DQN不同，它能够处理连续的而不是离散的状态和动作空间，因此可以交易大量股票。</li></ul><p id="e705" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">至此，人们可能会问一个问题:<strong class="ig hi">为什么要用连续动作空间进行股票交易？</strong></p><p id="aa69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先定义一个股票交易例子的状态空间和动作空间，假设我们的投资组合总共有30只股票:</p><ul class=""><li id="d98c" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb ls lf lg lh bi translated"><strong class="ig hi">状态空间</strong>:我们用一个由七部分信息组成的181维向量来表示多只股票交易环境的状态空间:[b，<strong class="ig hi"> <em class="lt"> p </em> </strong>，<strong class="ig hi"> <em class="lt"> h </em> </strong>，<strong class="ig hi"> <em class="lt"> M </em> </strong>，<strong class="ig hi"> <em class="lt"> R </em> </strong>，<strong class="ig hi"> <em class="lt"> C </em> </strong>，<strong class="ig hi"> <em class="lt"> X </em> </strong> ] <strong class="ig hi"> <em class="lt"> h </em> </strong>为股数，<strong class="ig hi"> <em class="lt"> M </em> </strong>为均线收敛发散(MACD)，<strong class="ig hi"> <em class="lt"> R </em> </strong>为相对强弱指数(RSI)，<strong class="ig hi"> <em class="lt"> C </em> </strong>为商品通道指数(CCI)，<strong class="ig hi"> <em class="lt"> X </em> </strong>为平均方向性指数(ADX)。</li><li id="8036" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated"><strong class="ig hi">行动空间</strong>:概括一下，我们有三种类型的行动:<em class="lt">卖出</em>、<em class="lt">买入</em>和<em class="lt">持有</em>一只股票。我们用负值表示卖出，正值表示买入，零表示持有。在这种情况下，动作空间定义为{- <em class="lt"> k </em>，…，-1，0，1，…, <em class="lt"> k </em> }，其中<em class="lt"> k </em>为每笔交易中买入或卖出的最大份额。</li></ul><p id="1e19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回到问题，我们交易的股票数量是整数，我们卖出、买入或持有的股票数量也是整数。直观上，股票交易的动作空间更可能是离散的，而不是连续的。然而，动作空间的复杂性随着股票数量d成指数增长。</p><p id="00dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，对于5支股票，假设代理被允许在每笔交易中卖出、买入或持有多达50支股票。在这种情况下，动作空间是(50+50+1)⁵，大约是10 ⁰.如果我们将股票数量从5个增加到30个，那么10⁶⁰.的行动空间就增加了由于如此大的动作空间几乎不可能离散地表示出来，所以我们假设股票交易的动作空间是连续的。</p><p id="800e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了DDPG处理连续空间，它还有以下优点:</p><ul class=""><li id="acf3" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb ls lf lg lh bi translated">演员-评论家框架:DDPG遵循标准的演员-评论家框架，它包含一个产生行动的演员网络和一个估计预期回报的评论家网络。这样的标准框架对于初学者来说入门是很清楚的。</li><li id="9c8e" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated">熟悉招数的可重用性 : DDPG运用了很多已经在DQN使用的招数，比如经验回放，冻结目标网络，探索噪音。在这种情况下，DDPG的元素很容易理解。</li></ul></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="7b6b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">算法细节和伪代码:</strong></h1><p id="ab9b" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">DDPG算法[3]可以分为四个部分:</p><ul class=""><li id="f057" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb ls lf lg lh bi translated"><strong class="ig hi">初始化</strong>:初始化变量和网络。</li><li id="cbd1" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated"><strong class="ig hi">采样</strong>:通过行动者网络(策略)与环境的交互获得变迁。</li><li id="7871" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated"><strong class="ig hi">计算</strong>:计算相关变量，如目标Q值。</li><li id="3ffe" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb ls lf lg lh bi translated"><strong class="ig hi">更新</strong>:基于损失函数更新演员和评论家网络。</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lv"><img src="../Images/a1aaadacabde17351e0877ead0e4179e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHSvHDR7aWm6dasJA6Zy6Q.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 4. Overview of the DDPG algorithm. [Image by authors].</figcaption></figure><p id="73ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">初始化:</strong></p><ol class=""><li id="3685" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">我们初始化演员和评论家(Q函数)网络。Actor network对每只股票采取行动，决定买入、卖出或持有的股票数量。批评家网络从当前的状态和行动中估计期望的回报。</li><li id="67f2" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们为作为目标网络的每个网络制作一个副本。</li></ol><p id="4012" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">采样:</strong></p><ol class=""><li id="1b71" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">给定状态，我们使用Actor网络输出相应的动作。</li><li id="c4cf" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们坚定地跟随行动，并修改我们对每只股票的持股数量，以使股市环境向前迈进。</li><li id="3378" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们观察我们账户的资产价值变化(余额和股票的总价值)，然后计算报酬。</li><li id="6614" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们将转换(s，a，s’，r)存储到重放缓冲器中，用于将来的训练。</li></ol><p id="5fbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">计算:</strong></p><ol class=""><li id="d9ad" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">我们从重放缓冲区随机抽取一批样本。</li><li id="695e" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们计算目标值，用于最小化均方贝尔曼误差(MSBE)。</li></ol><p id="7a19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">更新:</strong></p><ol class=""><li id="25f1" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">我们用目标值在MSBE上使用梯度下降来更新Critic网络。</li><li id="e209" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">我们使用梯度上升来更新行动者网络，以找到使回报最大化的行动策略。</li><li id="f3d5" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">演员目标网络和评论家目标网络通过软更新来同步。</li></ol><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lw"><img src="../Images/e411dd5d75a5d70bf7030dd5ace03885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fHIKWGba662WYhl0"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 5. The pseudocode of the DDPG algorithm. [Image from [4]].</figcaption></figure><p id="c6f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">股票市场环境的详细实施、培训流程的实现以及绩效评估将很快在<strong class="ig hi">第二部分中提供。</strong>感兴趣的用户可以在FinRL中测试<a class="ae jc" href="https://github.com/AI4Finance-LLC/FinRL-Library/blob/master/FinRL_stock_trading_NeurIPS_2018.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>，我们将很快在ElegantRL中提供类似的。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="daf8" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">参考文献:</strong></h1><p id="7f44" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">[1] Z .熊，，s .钟，杨红阳，a .瓦利德.<a class="ae jc" href="https://arxiv.org/abs/1811.07522" rel="noopener ugc nofollow" target="_blank">股票交易实用深度强化学习方法</a>。NeurIPS关于人工智能在金融服务中的挑战和机遇的研讨会:公平性、可解释性、准确性和隐私的影响，2018年。</p><p id="5ee9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] H .杨，(合初级)，s .钟，a .瓦利德.<a class="ae jc" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996" rel="noopener ugc nofollow" target="_blank">自动化股票交易的深度强化学习:一种集成策略</a>。2020年ACM金融人工智能国际会议。</p><p id="e7e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]利利拉普、T.P .、亨特、J.J .、普里策尔、a .、赫斯、n .、埃雷兹、塔萨、y .、西尔弗和维斯特拉..<a class="ae jc" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">深度强化学习的连续控制</a>。ICLR 2016。</p><p id="5db4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4] OpenAI在深度RL中旋转起来，<a class="ae jc" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id1" rel="noopener ugc nofollow" target="_blank">深度确定性政策梯度</a>。</p><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo kt ma"/></div></div></a></div><p id="98db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">🟠 <a class="ae jc" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ig hi">成为ML作家</strong> </a></p></div></div>    
</body>
</html>