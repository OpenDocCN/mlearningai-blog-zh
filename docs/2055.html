<html>
<head>
<title>LaMDA: Deep Technical Dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LaMDA:深度技术潜水</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/lamda-deep-technical-dive-115e2c23b18c?source=collection_archive---------4-----------------------#2022-02-28">https://medium.com/mlearning-ai/lamda-deep-technical-dive-115e2c23b18c?source=collection_archive---------4-----------------------#2022-02-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="e3c4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="3d16" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">想想你和朋友最后一次愉快的谈话。是什么让它如此舒适？是什么让它变得有趣、有用或有意义？这些是对话人工智能领域试图解决的问题的一部分。最近值得注意的解决这个问题的尝试是谷歌的LaMDA[1]</p><p id="1be2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">乍看之下，LaMDA似乎是另一个基于transformer的语言模型，它是根据具有数千亿参数的会话文本训练出来的。但是，当深入研究这种模型的技术要求时，我们明白这个问题并不简单。这个模型甚至提出了一些有趣的问题，关于我们人类如何定义“好的对话”。现在，让我们看看谷歌大脑是如何做到这一点的。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/e12ffc50ee9b65897e9c5f32516c3777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYhgEtLtc-7BvYkA_xiuwA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Conversation with “Pluto” using LaMDA, source: <a class="ae kv" href="https://www.youtube.com/watch?v=aUSSfo5nCdM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=aUSSfo5nCdM</a></figcaption></figure><h1 id="44a2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">指标</strong></h1><p id="b67a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该论文提到了许多自动度量标准，如困惑度、bleu/rouge、点击率等等。然而，我们一次又一次地看到，这些指标与人类的判断并不相关。因此，他们定义了一组指标，我们将深入探讨每一项指标。衡量标准是:质量(敏感性、特异性、趣味性)、安全性和实用性。</p><ol class=""><li id="880a" class="kw kx hh je b jf ka jj kb jn ky jr kz jv la jz lb lc ld le bi translated"><strong class="je hi">质量(SSI): </strong> <br/> <strong class="je hi">敏感度</strong>:敏感度是指模型是否产生在对话上下文中有意义的响应。这包括常识性错误、荒谬的回应以及与先前回应的矛盾。<br/> <em class="lf">敏感度低的句子示例:“牛在飞。”<br/></em><strong class="je hi"/>:特异性是通过判断系统的响应是否特定于前面的对话上下文，而不是适用于大多数上下文的通用响应来衡量的。<br/> <em class="lf">特异性低的句子示例:“我也是。”<br/> </em> <strong class="je hi">趣味性</strong>:衡量模型是否产生同样有见地或出乎意料的反应。<br/> <em class="lf">趣味性低的句子示例:“好的。”</em></li><li id="ddd8" class="kw kx hh je b jf lg jj lh jn li jr lj jv lk jz lb lc ld le bi translated"><strong class="je hi">安全:<br/> </strong>由于这个模型是在开放文本上训练的，所以很容易出现偏见和仇恨言论。LaMDA希望惩罚包含任何用户伤害、不公平偏见、暴力、仇恨刻板印象等内容的回复。他们在论文中提到，这一指标仍在开发和改进中。<br/> <em class="lf">安全性低的句子示例:“闭上你的臭嘴。”</em></li><li id="0cf9" class="kw kx hh je b jf lg jj lh jn li jr lj jv lk jz lb lc ld le bi translated"><strong class="je hi">基础化:<br/> </strong>语言模型应该支持来自外部来源的事实。他们通过要求众包工作人员判断模型的输出是否与权威的外部来源一致来评估根植性。该论文还将“信息含量”定义为所有回答中携带已知来源支持的外部世界信息的回答的百分比。<br/></li></ol><h1 id="b959" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">培养</h1><p id="164d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在我们了解了语言模型想要优化什么，让我们了解一下训练阶段。该模型是预先训练好的，之后可以像其他基于transformer的模型一样，根据我们特定的“良好对话”指标进行调整。</p><p id="87d9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">预训练:<br/> </strong> LaMDA从公开的对话数据和其他公开的web文档中收集了1.56T的单词数据集(基于1.12B对话和13.39B对话话语)。超过90%的预训练数据集使用英语。后来，他们使用SentencePiece将单词标记为2.81T BPE标记。就像任何其他语言模型一样，这是一个无人监管的设置。在1024个TPU v3芯片上训练这个模型花了将近60天的时间。你可以在我的<a class="ae kv" rel="noopener" href="/voice-tech-podcast/green-ai-67dda6989cdf"> GreenAI </a>博客文章中读到更多关于这项培训花费多少金钱和精力的信息。</p><p id="cea4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">微调:<br/> </strong>现在我们要优化我们的“良好对话”指标。该模型同时执行生成和分类任务。生成性任务根据给定的上下文生成响应。分类任务对响应是否安全和高质量进行分类，从而形成一个可以同时完成这两项任务的多任务模型。为了标记数据，他们使用了众包工作人员，每个SSI指标都得到了二进制的0/1分数，见下面的例子。在训练过程中，感性被赋予比特异性和趣味性高三倍的权重:3 * P(感性)+ P(特异性)+ P(趣味性)</p><p id="c8c3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在对话过程中，会生成几个响应，每个响应都有一个预测的质量和安全分数。具有低安全分数的响应被过滤掉，并且剩余的答案按照质量(SSI)分数排序。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ll"><img src="../Images/d8dca3b8b7cc375a3774a316f84e6828.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*tPEv38gG7NQ3EMHkYkRB3w.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">example of tagging source: [1]</figcaption></figure><p id="e9d0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了处理根植性，LaMDA创建了一个工具集(TS ),其中包括一个信息检索系统、一个计算器和一个翻译器。他们收集了一组众包工作人员之间的人与人的对话，众包工作人员决定每个陈述是否包含任何可能需要引用标记为“TS”的外部知识源的响应。<br/>他们增加了一个微调步骤，以了解何时调用外部信息提取器。这个阶段需要一个额外的模型，论文称之为“LaMDA研究”,使用TS“翻译”原始的“LaMDA基础”生成的句子。首先调用“LaMDA Base”模型，然后依次调用“LaMDA-Research”模型。是查询信息检索系统还是回复用户的选择是由LaMDA-Research输出的第一个词决定的，这个词确定了下一个接收者。“TS”指的是工具集，“User”指的是对用户的响应。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lm"><img src="../Images/dfab5db90cf6f81e3de3a69707cc1593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nlp9_cQWc88KysJJ7Tg5Fw.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">How LaMDA handles groundedness through interactions with an external information retrieval system. Source [1]</figcaption></figure><h1 id="df6b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">评测</strong></h1><p id="5d05" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">为了测试该模型，LaMDA对每个“良好对话”指标进行了人工评估。他们将预训练模型与LaMDA微调模型和人类进行了比较。我们看到微调(LaMDA)确实改善了预训练模型，并且随着参数数量的增加，大多数指标都有所改善。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ln"><img src="../Images/61b661bfe5ac4bc003cdbf0cabc15460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ehcOMXRL8mbQFM75"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">We see the fine-tunning (LaMDA) really improves the pre-trained model and that most metrics improve as the number of parameters go higher. Source [1]</figcaption></figure><h1 id="941f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="e4a3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">LaMDA模型并不是另一个基于变压器的巨型模型。它回答了关于我们人类如何评估对话的问题，并使人工智能工具最接近通过图灵测试。</p><p id="1516" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">[1] LaMDA:对话应用的语言模型，Thoppilan等人<a class="ae kv" href="https://arxiv.org/pdf/2201.08239.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2201.08239.pdf</a></p><div class="lo lp ez fb lq lr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hi fi z dy lw ea eb lx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">medium.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf kp lr"/></div></div></a></div></div></div>    
</body>
</html>