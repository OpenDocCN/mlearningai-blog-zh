<html>
<head>
<title>PPO — Intuitive guide to state-of-the-art Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PPO——最新强化学习的直观指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b?source=collection_archive---------0-----------------------#2022-12-15">https://medium.com/mlearning-ai/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b?source=collection_archive---------0-----------------------#2022-12-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="ebd6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="3461" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">(这个故事也可以作为一个<a class="ae ka" href="https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>)。</p><p id="dedc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi"> P </strong>近似<strong class="je hi"> P </strong>策略<strong class="je hi"> O </strong>优化(<strong class="je hi"> PPO </strong>)自从在论文<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">近似策略优化算法</strong></a>(Schulman et al .艾尔。, 2017).这种优雅的算法可以并且已经被用于各种任务。最近，它也被用于ChatGPT的训练，这是目前最热门的机器学习模型。</p><p id="a01c" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPO不仅在RL社区中被广泛使用，而且它还是通过深度学习(DL)模型解决RL问题的优秀入门。</p><p id="4177" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在这篇文章中，我给出了强化学习领域的快速概述，解决RL问题的算法的分类，以及在<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的PPO算法的评论。最后分享一下<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations" rel="noopener ugc nofollow" target="_blank">我自己在PyTorch中对PPO算法的实现</a>，对得到的结果进行评论，并以结论结束。</p><h1 id="7539" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">强化学习</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/230d7f9136896e3de729fe2339f5dae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVgSzd0-ES-7A3_0YLGS9A.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">ChatGPT’s answer to the prompt: “Give an overview on the field of Reinforcement Learning”. While I asked help to ChatGPT for the introduction to the field of RL which was used to train ChatGPT itself (quite meta), I promise that everything in this article apart from this picture is written by me.</figcaption></figure><p id="8695" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">首先向接近RL的人展示的经典图片如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/a2cbef0b3f99d799b6a4ce04ff2d9f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l6nFVmkoeFAaTRfp"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Reinforcement Learning framework. Image from <a class="ae ka" href="https://neptune.ai/blog/reinforcement-learning-agents-training-debug" rel="noopener ugc nofollow" target="_blank">neptune.ai</a></figcaption></figure><p id="0961" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在每个时间戳，环境向代理提供奖励和对当前状态的观察。给定这些信息，代理在环境中采取行动，并以新的奖励和状态等做出响应。这个非常通用的框架可以应用于各种领域。</p><p id="3378" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们的目标是创造一个能最大化所获回报的代理人。特别是，我们通常对折扣奖励的总和最大化感兴趣</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kx"><img src="../Images/a9f1eb8818dc7dd5e9b9f841e3c92b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/0*dwOUuOx1i089pCXE"/></div></figure><p id="3f36" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">其中，γ是折扣因子，通常在[0.95，0.99]范围内，r_t是时间戳t的奖励。</p><h1 id="cf95" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">算法</h1><p id="e75e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">那么我们如何解决RL问题呢？有多种算法，但它们可以(针对马尔可夫决策过程或MDP)分为两类:<strong class="je hi">基于模型的</strong>(创建环境的模型)和<strong class="je hi">无模型的</strong>(只需了解给定一个状态要做什么)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/0374f13917271b72108dc4ec1be7066d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RyxxdI-JNSTwSoke4owtQ.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Taxonomy of Reinforcement Learning algorithms (from <a class="ae ka" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html" rel="noopener ugc nofollow" target="_blank">OpenAI spinning up</a>)</figcaption></figure><p id="9359" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">基于模型的</strong>算法使用一个环境模型，并使用这个模型来预测未来的状态和奖励。该模型要么是给定的(例如棋盘)，要么是学习的。</p><p id="8d24" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">无模型</strong>算法取而代之的是，直接学习如何针对训练中遇到的状态采取行动(策略优化或PO)，哪些状态-行动对产生良好的回报(Q-Learning)，或者两者同时进行。</p><p id="d5bd" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi"> PPO </strong>属于PO算法家族。因此，我们不需要环境模型来学习PPO算法。PO和Q-Learning算法之间的主要区别在于，PO算法可以用于具有连续动作空间的环境中(即，我们的动作具有真实值的环境中)，并且可以找到最优策略，即使该策略是随机的(即，概率性地动作)，而Q-Learning算法不能做这些事情。这是更喜欢PO算法的另一个原因。另一方面，Q学习算法往往更简单、更直观、更好训练。</p><h2 id="70c0" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">策略优化(基于梯度)</h2><p id="15ad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">PO算法尝试直接学习策略。为此，他们要么使用无梯度算法(如遗传算法)，要么使用更常见的基于梯度的算法。</p><p id="3dae" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">对于基于梯度的方法，我们指的是所有试图估计所学政策相对于累积回报的梯度的方法。如果我们知道这个梯度(或它的近似值)，我们可以简单地将政策的参数向梯度的方向移动，以使回报最大化。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ln"><img src="../Images/979b9dea533516e58766d689258a5b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O0oxrEZak8ko4xz8.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Objective to be maximized with PO algorithms. Image from <a class="ae ka" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" rel="noopener ugc nofollow" target="_blank">Lil’Log’s blog.</a></figcaption></figure><p id="2205" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，有多种方法可以估算梯度。在这里，我们发现列出了6个不同的值，我们可以选择它们作为我们的最大化目标:总回报、一次行动后的回报、减去基线版本的回报、状态-行动值函数、优势函数(在最初的PPO论文中使用)和时间差(TD)残差。原则上，它们都提供了我们感兴趣的真实梯度的估计。</p><h1 id="0dc4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">聚苯醚（Polyphenylene Oxide的缩写）</h1><p id="3c62" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> PPO </strong>是一种(无模型)策略优化的基于梯度的算法。该算法旨在学习一种策略，在训练期间给定经验的情况下，最大化所获得的累积奖励。</p><p id="2103" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">它由一个<strong class="je hi">演员</strong> <strong class="je hi"> πθ(。| st) </strong>，其输出给定时间戳t的状态下的下一个动作的概率分布，以及由<strong class="je hi">评论家</strong> <strong class="je hi"> V(st) </strong>，其估计来自该状态(标量)的预期累积回报。因为演员和评论家都将状态作为输入，所以可以在提取高级特征的两个网络之间共享主干架构。</p><p id="ad95" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPO的目的是使政策更有可能选择具有高度“优势”的行动，也就是说，具有比批评家所能预测的高得多的衡量累积回报。同时，我们不希望在一个步骤中更新策略太多，因为这可能会导致优化问题。最后，如果该策略具有高熵，我们将为其提供奖金，以激励探索而不是开发。</p><p id="77eb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">总损失函数(将被最大化)由三项组成:削波项、价值函数(VF)项和熵奖励。</p><p id="1bde" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">最终目标如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lo"><img src="../Images/4eee1d33404f1b2b761471a13990c1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*8WP3C1EQDbKvU_cI87otoA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">The loss function of PPO to be maximized.</figcaption></figure><p id="fce4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">其中，c1和c2是超参数，分别衡量策略的批评准确性和探索能力的重要性。</p><h2 id="88e4" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">剪辑术语</h2><p id="6d57" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如我们所说的，损失函数促使产生优势的行动的概率最大化(或者，如果行动产生负面优势，则使概率最小化):</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lp"><img src="../Images/ce776e6339715fda94243c3426efa1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*cr288gSfmdqMyRuGj4X8aQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">First loss term. We maximize and minimize the probability of picking</figcaption></figure><p id="1f9a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">其中:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/f7bac037dd0d4bf8900570879e4c7a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dranEeIvz8lW_4XjUY3mXg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Coefficient rt(θ). This is the term that gradients are going to go through.</figcaption></figure><p id="686f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">是一个比率，用于衡量我们现在(使用更新的策略)相对于以前执行以前的操作的可能性。原则上，我们不希望这个系数太高，因为这意味着政策突然改变。这就是为什么我们取它的最小值和[1+ϵ1-ϵ]之间的剪辑版本，其中ϵ是一个超参数。</p><p id="5ff2" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">优势计算如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lr"><img src="../Images/dcb251ae3343ef3432352da91060b17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v94VN_62M-FbWPQOoETLHw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Advantage estimate. We simply take a difference between what we estimated the cumulative reward would have been given the initial state and the real cumulative reward observed up to a step t plus the estimate from that state onward. We apply a stop-gradient operator to this term in the CLIP loss.</figcaption></figure><p id="1c85" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们看到，它只是简单地衡量了批评者对给定状态st的错误程度。如果我们获得了更高的累积奖励，优势估计将为正，我们将更有可能在这种状态下采取行动。反之亦然，如果我们期望更高的回报，而我们得到了更低的回报，优势估计将是负的，我们将减少在这一步采取行动的可能性。</p><p id="a432" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，如果我们一直到最后的状态sT，我们不需要依赖批评家本身，我们可以简单地将批评家与实际的累积回报进行比较。在这种情况下，对优势的估计就是真正的优势。这就是我们在实现车杆问题时要做的事情。</p><h2 id="b67b" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">价值函数项</h2><p id="719c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然而，为了对优势有一个好的估计，我们需要一个可以预测给定状态的值的评论家。该模型是以监督的方式学习的，具有简单的MSE损失:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ls"><img src="../Images/c99c51b76bff4edaad1cf444e252a799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cI0e2esKc8ewZ0HZ"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">The loss function for our critic is simply the Mean-Squared-Error between its predicted expected reward and the observed cumulative reward. We apply a stop-gradient operator only to the observed reward in this case and optimize the critic.</figcaption></figure><p id="ecfa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在每次迭代中，我们也更新critic，这样随着训练的进行，它将为我们提供越来越精确的状态值。</p><h2 id="934f" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">熵项</h2><p id="d98d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">最后，我们鼓励探索，对政策的产出分布的熵有一点奖励。我们考虑标准熵:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lt"><img src="../Images/d72cf63e4034c628f33a3bd4630c298b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/0*TRSWE2dVyRxpvG3g"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Entropy formula for the output distribution given by the policy model.</figcaption></figure><h1 id="c5c2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">履行</h1><p id="aacb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">不要担心这个理论是否仍然有点可疑。这个实现有望让一切变得清晰。</p><h2 id="d56b" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">独立于PPO的代码</h2><p id="f3f5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们从进口开始:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="c560" class="lz if hh lv b be ma mb l mc md">from argparse import ArgumentParser<br/><br/>import gym<br/>import numpy as np<br/>import wandb<br/><br/>import torch<br/>import torch.nn as nn<br/>from torch.optim import Adam<br/>from torch.optim.lr_scheduler import LinearLR<br/>from torch.distributions.categorical import Categorical<br/><br/>import pytorch_lightning as pl</span></pre><p id="a3ed" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPO的重要超参数是<em class="me">数量的演员</em>、<em class="me">视野</em>、<em class="me">ε</em>、每个优化阶段的<em class="me">次数</em>、学习率<em class="me">、折现因子<em class="me">γ</em>以及对不同损失项<em class="me"> c1 </em>和<em class="me"> c2 </em>进行加权的常数。我们通过程序参数收集这些信息。</em></p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="88b8" class="lz if hh lv b be ma mb l mc md">def parse_args():<br/>    """Pareser program arguments"""<br/>    # Parser<br/>    parser = ArgumentParser()<br/><br/>    # Program arguments (default for Atari games)<br/>    parser.add_argument("--max_iterations", type=int, help="Number of iterations of training", default=100)<br/>    parser.add_argument("--n_actors", type=int, help="Number of actors for each update", default=8)<br/>    parser.add_argument("--horizon", type=int, help="Number of timestamps for each actor", default=128)<br/>    parser.add_argument("--epsilon", type=float, help="Epsilon parameter", default=0.1)<br/>    parser.add_argument("--n_epochs", type=int, help="Number of training epochs per iteration", default=3)<br/>    parser.add_argument("--batch_size", type=int, help="Batch size", default=32 * 8)<br/>    parser.add_argument("--lr", type=float, help="Learning rate", default=2.5 * 1e-4)<br/>    parser.add_argument("--gamma", type=float, help="Discount factor gamma", default=0.99)<br/>    parser.add_argument("--c1", type=float, help="Weight for the value function in the loss function", default=1)<br/>    parser.add_argument("--c2", type=float, help="Weight for the entropy bonus in the loss function", default=0.01)<br/>    parser.add_argument("--n_test_episodes", type=int, help="Number of episodes to render", default=5)<br/>    parser.add_argument("--seed", type=int, help="Randomizing seed for the experiment", default=0)<br/><br/>    # Dictionary with program arguments<br/>    return vars(parser.parse_args())</span></pre><p id="d9d5" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，默认情况下，参数设置如本文所述。理想情况下，如果可能的话，我们的代码应该在GPU上运行，所以我们创建了一个简单的实用函数。</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="4b3c" class="lz if hh lv b be ma mb l mc md">def get_device():<br/>    """Gets the device (GPU if any) and logs the type"""<br/>    if torch.cuda.is_available():<br/>        device = torch.device("cuda")<br/>        print(f"Found GPU device: {torch.cuda.get_device_name(device)}")<br/>    else:<br/>        device = torch.device("cpu")<br/>        print("No GPU found: Running on CPU")<br/>    return device</span></pre><p id="8a27" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">当我们应用RL时，我们通常有一个缓冲区来存储当前模型遇到的状态、动作和奖励。这些用于更新我们的模型。我们创建一个效用函数<code class="du mf mg mh lv b">run_timestamps</code>，它将在给定的环境中运行给定的模型，运行固定数量的时间戳(如果剧集结束，则重新设置环境)。我们还使用了一个选项<code class="du mf mg mh lv b">render=False</code>,以防我们只是想看看经过训练的模型表现如何。</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="8ab5" class="lz if hh lv b be ma mb l mc md">@torch.no_grad()<br/>def run_timestamps(env, model, timestamps=128, render=False, device="cpu"):<br/>    """Runs the given policy on the given environment for the given amount of timestamps.<br/>     Returns a buffer with state action transitions and rewards."""<br/>    buffer = []<br/>    state = env.reset()[0]<br/><br/>    # Running timestamps and collecting state, actions, rewards and terminations<br/>    for ts in range(timestamps):<br/>        # Taking a step into the environment<br/>        model_input = torch.from_numpy(state).unsqueeze(0).to(device).float()<br/>        action, action_logits, value = model(model_input)<br/>        new_state, reward, terminated, truncated, info = env.step(action.item())<br/><br/>        # Rendering / storing (s, a, r, t) in the buffer<br/>        if render:<br/>            env.render()<br/>        else:<br/>            buffer.append([model_input, action, action_logits, value, reward, terminated or truncated])<br/><br/>        # Updating current state<br/>        state = new_state<br/><br/>        # Resetting environment if episode terminated or truncated<br/>        if terminated or truncated:<br/>            state = env.reset()[0]<br/><br/>    return buffer</span></pre><p id="05eb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">该函数的输出(不呈现时)是一个缓冲区，包含每个时间戳的状态、采取的动作、动作概率(logits)、估计的评论家值、奖励和所提供策略的终止状态。注意，这个函数使用了装饰器<strong class="je hi"> @torch.no_grad()，</strong>，所以我们不需要存储在与环境交互过程中所采取的动作的渐变。</p><h2 id="cec7" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">PPO的代码</h2><p id="dc34" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">既然我们已经解决了琐碎的事情，是时候实现核心算法了。</p><p id="b7ba" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">理想情况下，我们希望我们的<strong class="je hi">主</strong>函数看起来像这样:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="5330" class="lz if hh lv b be ma mb l mc md">def main():<br/>    # Parsing program arguments<br/>    args = parse_args()<br/>    print(args)<br/><br/>    # Setting seed<br/>    pl.seed_everything(args["seed"])<br/><br/>    # Getting device<br/>    device = get_device()<br/><br/>    # Creating environment (discrete action space)<br/>    env_name = "CartPole-v1"<br/>    env = gym.make(env_name)<br/><br/>    # Creating the model, training it and rendering the result<br/>    # (We are missing this part 😅)<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/>    training_loop(env, model, args)<br/>    model = load_best_model()<br/>    testing_loop(env, model)</span></pre><p id="80bc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们已经拿到大部分了。我们只需要定义PPO模型、训练和测试功能。</p><p id="6d90" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPO模型的架构不是这里有趣的部分。我们只需要两个在环境中表演的模特(演员和评论家)。当然，模型架构在更困难的任务中起着至关重要的作用，但是有了cart pole，我们可以确信一些MLP将完成这项工作。</p><p id="921a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">因此，我们可以创建一个包含演员和评论家模型的MyPPO类。可选地，我们可以决定两者之间的部分架构是共享的。当对某些状态运行forward方法时，我们返回参与者的抽样动作、每个可能动作的相对概率(logits)以及评论家对每个状态的估计值。</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="7675" class="lz if hh lv b be ma mb l mc md">class MyPPO(nn.Module):<br/>    """Implementation of a PPO model. The same backbone is used to get actor and critic values."""<br/><br/>    def __init__(self, in_shape, n_actions, hidden_d=100, share_backbone=False):<br/>        # Super constructor<br/>        super(MyPPO, self).__init__()<br/><br/>        # Attributes<br/>        self.in_shape = in_shape<br/>        self.n_actions = n_actions<br/>        self.hidden_d = hidden_d<br/>        self.share_backbone = share_backbone<br/><br/>        # Shared backbone for policy and value functions<br/>        in_dim = np.prod(in_shape)<br/><br/>        def to_features():<br/>            return nn.Sequential(<br/>                nn.Flatten(),<br/>                nn.Linear(in_dim, hidden_d),<br/>                nn.ReLU(),<br/>                nn.Linear(hidden_d, hidden_d),<br/>                nn.ReLU()<br/>            )<br/><br/>        self.backbone = to_features() if self.share_backbone else nn.Identity()<br/><br/>        # State action function<br/>        self.actor = nn.Sequential(<br/>            nn.Identity() if self.share_backbone else to_features(),<br/>            nn.Linear(hidden_d, hidden_d),<br/>            nn.ReLU(),<br/>            nn.Linear(hidden_d, n_actions),<br/>            nn.Softmax(dim=-1)<br/>        )<br/><br/>        # Value function<br/>        self.critic = nn.Sequential(<br/>            nn.Identity() if self.share_backbone else to_features(),<br/>            nn.Linear(hidden_d, hidden_d),<br/>            nn.ReLU(),<br/>            nn.Linear(hidden_d, 1)<br/>        )<br/><br/>    def forward(self, x):<br/>        features = self.backbone(x)<br/>        action = self.actor(features)<br/>        value = self.critic(features)<br/>        return Categorical(action).sample(), action, value</span></pre><p id="49a2" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，<code class="du mf mg mh lv b">Categorical(action).sample()</code>创建了一个带有动作逻辑的分类分布，并从中抽取了一个动作样本(针对每个州)。</p><p id="2520" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">最后，我们可以在<code class="du mf mg mh lv b">training_loop</code>函数中处理实际的算法。正如我们从论文中所知，函数的实际签名应该是这样的:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="4d67" class="lz if hh lv b be ma mb l mc md">def training_loop(env, model, max_iterations, n_actors, horizon, gamma, <br/>epsilon, n_epochs, batch_size, lr, c1, c2, device, env_name=""):<br/>  # TODO...</span></pre><p id="6e19" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">下面是本文中为PPO培训程序提供的伪代码:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mi"><img src="../Images/adf9438a5b6a7b4fab6124db0fcc8de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1ROKGqst4p8rXTBtDwa_A.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Pseudo code for PPO training provided in the <a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">original paper</a>.</figcaption></figure><p id="00b3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPO的伪代码相对简单:我们简单地通过我们的策略模型的多个副本(称为行动者)收集与环境的交互，并使用先前定义的目标来优化行动者和批评者网络。</p><p id="3340" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">由于我们需要衡量我们实际获得的累积奖励，我们创建了一个函数，给定一个缓冲区，用累积奖励替换每个时间戳的奖励:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="b59f" class="lz if hh lv b be ma mb l mc md">def compute_cumulative_rewards(buffer, gamma):<br/>    """Given a buffer with states, policy action logits, rewards and terminations,<br/>    computes the cumulative rewards for each timestamp and substitutes them into the buffer."""<br/>    curr_rew = 0.<br/><br/>    # Traversing the buffer on the reverse direction<br/>    for i in range(len(buffer) - 1, -1, -1):<br/>        r, t = buffer[i][-2], buffer[i][-1]<br/><br/>        if t:<br/>            curr_rew = 0<br/>        else:<br/>            curr_rew = r + gamma * curr_rew<br/><br/>        buffer[i][-2] = curr_rew<br/><br/>    # Getting the average reward before normalizing (for logging and checkpointing)<br/>    avg_rew = np.mean([buffer[i][-2] for i in range(len(buffer))])<br/><br/>    # Normalizing cumulative rewards<br/>    mean = np.mean([buffer[i][-2] for i in range(len(buffer))])<br/>    std = np.std([buffer[i][-2] for i in range(len(buffer))]) + 1e-6<br/>    for i in range(len(buffer)):<br/>        buffer[i][-2] = (buffer[i][-2] - mean) / std<br/><br/>    return avg_rew</span></pre><p id="b66b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">请注意，最后，我们将累积奖励标准化。这是一个标准的技巧，可以使优化问题更容易，训练更流畅。</p><p id="85f0" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">现在我们可以获得一个包含状态、采取的行动、行动概率和累积回报的缓冲区，我们可以编写一个函数，给定一个缓冲区，计算我们最终目标的三个损失项:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="f524" class="lz if hh lv b be ma mb l mc md">def get_losses(model, batch, epsilon, annealing, device="cpu"):<br/>    """Returns the three loss terms for a given model and a given batch and additional parameters"""<br/>    # Getting old data<br/>    n = len(batch)<br/>    states = torch.cat([batch[i][0] for i in range(n)])<br/>    actions = torch.cat([batch[i][1] for i in range(n)]).view(n, 1)<br/>    logits = torch.cat([batch[i][2] for i in range(n)])<br/>    values = torch.cat([batch[i][3] for i in range(n)])<br/>    cumulative_rewards = torch.tensor([batch[i][-2] for i in range(n)]).view(-1, 1).float().to(device)<br/><br/>    # Computing predictions with the new model<br/>    _, new_logits, new_values = model(states)<br/><br/>    # Loss on the state-action-function / actor (L_CLIP)<br/>    advantages = cumulative_rewards - values<br/>    margin = epsilon * annealing<br/>    ratios = new_logits.gather(1, actions) / logits.gather(1, actions)<br/><br/>    l_clip = torch.mean(<br/>        torch.min(<br/>            torch.cat(<br/>                (ratios * advantages,<br/>                 torch.clip(ratios, 1 - margin, 1 + margin) * advantages),<br/>                dim=1),<br/>            dim=1<br/>        ).values<br/>    )<br/><br/>    # Loss on the value-function / critic (L_VF)<br/>    l_vf = torch.mean((cumulative_rewards - new_values) ** 2)<br/><br/>    # Bonus for entropy of the actor<br/>    entropy_bonus = torch.mean(torch.sum(-new_logits * (torch.log(new_logits + 1e-5)), dim=1))<br/><br/>    return l_clip, l_vf, entropy_bonus</span></pre><p id="0fb1" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">注意，在实践中，我们使用一个<em class="me">退火</em>参数，它被设置为1，并在整个训练过程中向0线性衰减。这个想法是，随着训练的进展，我们希望我们的政策改变越来越少。还要注意的是，<em class="me">优势</em>变量是张量之间的简单差异，我们不跟踪梯度，不像<em class="me"> new_logits </em>和<em class="me"> new_values </em>。</p><p id="49cd" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">既然我们已经有了与环境交互和存储缓冲区、计算(真实)累积回报和获得损失项的方法，我们就可以编写最终的训练循环了:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="52f7" class="lz if hh lv b be ma mb l mc md">def training_loop(env, model, max_iterations, n_actors, horizon, gamma, epsilon, n_epochs, batch_size, lr,<br/>                  c1, c2, device, env_name=""):<br/>    """Train the model on the given environment using multiple actors acting up to n timestamps."""<br/><br/>    # Starting a new Weights &amp; Biases run<br/>    wandb.init(project="Papers Re-implementations",<br/>               entity="peutlefaire",<br/>               name=f"PPO - {env_name}",<br/>               config={<br/>                   "env": str(env),<br/>                   "number of actors": n_actors,<br/>                   "horizon": horizon,<br/>                   "gamma": gamma,<br/>                   "epsilon": epsilon,<br/>                   "epochs": n_epochs,<br/>                   "batch size": batch_size,<br/>                   "learning rate": lr,<br/>                   "c1": c1,<br/>                   "c2": c2<br/>               })<br/><br/>    # Training variables<br/>    max_reward = float("-inf")<br/>    optimizer = Adam(model.parameters(), lr=lr, maximize=True)<br/>    scheduler = LinearLR(optimizer, 1, 0, max_iterations * n_epochs)<br/>    anneals = np.linspace(1, 0, max_iterations)<br/><br/>    # Training loop<br/>    for iteration in range(max_iterations):<br/>        buffer = []<br/>        annealing = anneals[iteration]<br/><br/>        # Collecting timestamps for all actors with the current policy<br/>        for actor in range(1, n_actors + 1):<br/>            buffer.extend(run_timestamps(env, model, horizon, False, device))<br/><br/>        # Computing cumulative rewards and shuffling the buffer<br/>        avg_rew = compute_cumulative_rewards(buffer, gamma)<br/>        np.random.shuffle(buffer)<br/><br/>        # Running optimization for a few epochs<br/>        for epoch in range(n_epochs):<br/>            for batch_idx in range(len(buffer) // batch_size):<br/>                # Getting batch for this buffer<br/>                start = batch_size * batch_idx<br/>                end = start + batch_size if start + batch_size &lt; len(buffer) else -1<br/>                batch = buffer[start:end]<br/><br/>                # Zero-ing optimizers gradients<br/>                optimizer.zero_grad()<br/><br/>                # Getting the losses<br/>                l_clip, l_vf, entropy_bonus = get_losses(model, batch, epsilon, annealing, device)<br/><br/>                # Computing total loss and back-propagating it<br/>                loss = l_clip - c1 * l_vf + c2 * entropy_bonus<br/>                loss.backward()<br/><br/>                # Optimizing<br/>                optimizer.step()<br/>            scheduler.step()<br/><br/>        # Logging information to stdout<br/>        curr_loss = loss.item()<br/>        log = f"Iteration {iteration + 1} / {max_iterations}: " \<br/>              f"Average Reward: {avg_rew:.2f}\t" \<br/>              f"Loss: {curr_loss:.3f} " \<br/>              f"(L_CLIP: {l_clip.item():.1f} | L_VF: {l_vf.item():.1f} | L_bonus: {entropy_bonus.item():.1f})"<br/>        if avg_rew &gt; max_reward:<br/>            torch.save(model.state_dict(), MODEL_PATH)<br/>            max_reward = avg_rew<br/>            log += " --&gt; Stored model with highest average reward"<br/>        print(log)<br/><br/>        # Logging information to W&amp;B<br/>        wandb.log({<br/>            "loss (total)": curr_loss,<br/>            "loss (clip)": l_clip.item(),<br/>            "loss (vf)": l_vf.item(),<br/>            "loss (entropy bonus)": entropy_bonus.item(),<br/>            "average reward": avg_rew<br/>        })<br/><br/>    # Finishing W&amp;B session<br/>    wandb.finish()</span></pre><p id="b1b4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">最后，为了查看最终模型的效果，我们使用下面的<code class="du mf mg mh lv b">testing_loop</code>函数:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="c6b5" class="lz if hh lv b be ma mb l mc md">def testing_loop(env, model, n_episodes, device):<br/>    """Runs the learned policy on the environment for n episodes"""<br/>    for _ in range(n_episodes):<br/>        run_timestamps(env, model, timestamps=128, render=True, device=device)</span></pre><p id="56f3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们的主要计划很简单:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="507d" class="lz if hh lv b be ma mb l mc md">def main():<br/>    # Parsing program arguments<br/>    args = parse_args()<br/>    print(args)<br/><br/>    # Setting seed<br/>    pl.seed_everything(args["seed"])<br/><br/>    # Getting device<br/>    device = get_device()<br/><br/>    # Creating environment (discrete action space)<br/>    env_name = "CartPole-v1"<br/>    env = gym.make(env_name)<br/><br/>    # Creating the model (both actor and critic)<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/><br/>    # Training<br/>    training_loop(env, model, args["max_iterations"], args["n_actors"], args["horizon"], args["gamma"], args["epsilon"],<br/>                  args["n_epochs"], args["batch_size"], args["lr"], args["c1"], args["c2"], device, env_name)<br/><br/>    # Loading best model<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/>    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))<br/><br/>    # Testing<br/>    env = gym.make(env_name, render_mode="human")<br/>    testing_loop(env, model, args["n_test_episodes"], device)<br/>    env.close()</span></pre><p id="cb7a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">而这一切都是为了实现！如果你能走到这一步，恭喜你。您现在知道如何实现PPO算法了。</p><h1 id="98bb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结果</h1><p id="1750" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">权重和偏差日志让我们可以直观地看到记录的指标和损失。特别是，我们可以访问损失图及其项，以及每次迭代的平均回报。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/7de6c9dce20f3b6bf976714726b1d5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*kSSdUvHqCHqV0ncQIfL-hg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Training losses through training iterations. The total loss (blue) is the sum of L_CLIP (orange) minus the L_VF (pink) plus a small constant times the entropy bonus (green)</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/637d6b0a331a89cd568e26eed25e471b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*m7NeWEnlEUa_9djnuLsnGA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Average reward through iterations. PPO quickly learns to maximize the cumulative reward.</figcaption></figure><p id="0a2a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">由于推车杆环境并不极具挑战性，我们的算法很快就找到了问题的解决方案，在大约20步后最大化了平均奖励。此外，由于环境只有两种可能的行为，熵项基本上保持不变。</p><p id="1b64" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">最后，如果我们将最终策略付诸实施，我们会得到以下结果！</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mk"><img src="../Images/5f14ae3564f14fb000ba7fa324183eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/1*JVPhoji799kNzYT5dVEw8g.gif"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Trained PPO model balancing the cart pole</figcaption></figure><h1 id="d438" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="c20d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">PPO是一种最先进的RL策略优化(因此是无模型的)算法，因此，它几乎可以在任何环境中使用。此外，PPO具有相对简单的目标函数和相对较少的待调节的超参数。</p><p id="3813" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">如果你想玩这个动态算法，这里有一个链接指向<a class="ae ka" href="https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>。你可以在<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>下找到我个人最新的PPO算法的重新实现(作为. py文件)。您可以随意使用它或将其应用到您自己的项目中！</p><p id="5f2b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">如果你喜欢这个故事，请告诉我！请随时联系我们进行进一步的讨论。祝你和PPO✌️一起快乐黑客</p><h1 id="7915" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">更深入的链接:</h1><p id="ef86" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">舒尔曼等人。艾尔。■原文<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"><em class="me"/></a></p><p id="c872" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">OpenAI的<a class="ae ka" href="https://spinningup.openai.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> " <em class="me">旋转上升导至深度RL </em> " </a></p><p id="5efa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">莉莲翁的<a class="ae ka" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/" rel="noopener ugc nofollow" target="_blank"><em class="me">一(长)窥强化学习</em></a></p><p id="a275" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">翁莉莲的<a class="ae ka" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" rel="noopener ugc nofollow" target="_blank"><em class="me">政策梯度算法</em></a></p><div class="ml mm ez fb mn mo"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mp ab dw"><div class="mq ab mr cl cj ms"><h2 class="bd hi fi z dy mt ea eb mu ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mv l"><h3 class="bd b fi z dy mt ea eb mu ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mw l"><p class="bd b fp z dy mt ea eb mu ed ef dx translated">medium.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc kq mo"/></div></div></a></div></div></div>    
</body>
</html>