<html>
<head>
<title>PPO â€” Intuitive guide to state-of-the-art Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PPOâ€”â€”æœ€æ–°å¼ºåŒ–å­¦ä¹ çš„ç›´è§‚æŒ‡å—</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b?source=collection_archive---------0-----------------------#2022-12-15">https://medium.com/mlearning-ai/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b?source=collection_archive---------0-----------------------#2022-12-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="ebd6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ä»‹ç»</h1><p id="3461" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">(è¿™ä¸ªæ•…äº‹ä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ª<a class="ae ka" href="https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colabç¬”è®°æœ¬</a>)ã€‚</p><p id="dedc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi"> P </strong>è¿‘ä¼¼<strong class="je hi"> P </strong>ç­–ç•¥<strong class="je hi"> O </strong>ä¼˜åŒ–(<strong class="je hi"> PPO </strong>)è‡ªä»åœ¨è®ºæ–‡<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">è¿‘ä¼¼ç­–ç•¥ä¼˜åŒ–ç®—æ³•</strong></a>(Schulman et al .è‰¾å°”ã€‚, 2017).è¿™ç§ä¼˜é›…çš„ç®—æ³•å¯ä»¥å¹¶ä¸”å·²ç»è¢«ç”¨äºå„ç§ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå®ƒä¹Ÿè¢«ç”¨äºChatGPTçš„è®­ç»ƒï¼Œè¿™æ˜¯ç›®å‰æœ€çƒ­é—¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</p><p id="a01c" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPOä¸ä»…åœ¨RLç¤¾åŒºä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œè€Œä¸”å®ƒè¿˜æ˜¯é€šè¿‡æ·±åº¦å­¦ä¹ (DL)æ¨¡å‹è§£å†³RLé—®é¢˜çš„ä¼˜ç§€å…¥é—¨ã€‚</p><p id="4177" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ç»™å‡ºäº†å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„å¿«é€Ÿæ¦‚è¿°ï¼Œè§£å†³RLé—®é¢˜çš„ç®—æ³•çš„åˆ†ç±»ï¼Œä»¥åŠåœ¨<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">è®ºæ–‡</a>ä¸­æå‡ºçš„PPOç®—æ³•çš„è¯„è®ºã€‚æœ€ååˆ†äº«ä¸€ä¸‹<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations" rel="noopener ugc nofollow" target="_blank">æˆ‘è‡ªå·±åœ¨PyTorchä¸­å¯¹PPOç®—æ³•çš„å®ç°</a>ï¼Œå¯¹å¾—åˆ°çš„ç»“æœè¿›è¡Œè¯„è®ºï¼Œå¹¶ä»¥ç»“è®ºç»“æŸã€‚</p><h1 id="7539" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">å¼ºåŒ–å­¦ä¹ </h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/230d7f9136896e3de729fe2339f5dae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVgSzd0-ES-7A3_0YLGS9A.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">ChatGPTâ€™s answer to the prompt: â€œGive an overview on the field of Reinforcement Learningâ€. While I asked help to ChatGPT for the introduction to the field of RL which was used to train ChatGPT itself (quite meta), I promise that everything in this article apart from this picture is written by me.</figcaption></figure><p id="8695" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">é¦–å…ˆå‘æ¥è¿‘RLçš„äººå±•ç¤ºçš„ç»å…¸å›¾ç‰‡å¦‚ä¸‹:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/a2cbef0b3f99d799b6a4ce04ff2d9f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l6nFVmkoeFAaTRfp"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Reinforcement Learning framework. Image from <a class="ae ka" href="https://neptune.ai/blog/reinforcement-learning-agents-training-debug" rel="noopener ugc nofollow" target="_blank">neptune.ai</a></figcaption></figure><p id="0961" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">åœ¨æ¯ä¸ªæ—¶é—´æˆ³ï¼Œç¯å¢ƒå‘ä»£ç†æä¾›å¥–åŠ±å’Œå¯¹å½“å‰çŠ¶æ€çš„è§‚å¯Ÿã€‚ç»™å®šè¿™äº›ä¿¡æ¯ï¼Œä»£ç†åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ï¼Œå¹¶ä»¥æ–°çš„å¥–åŠ±å’ŒçŠ¶æ€ç­‰åšå‡ºå“åº”ã€‚è¿™ä¸ªéå¸¸é€šç”¨çš„æ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸã€‚</p><p id="3378" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›é€ ä¸€ä¸ªèƒ½æœ€å¤§åŒ–æ‰€è·å›æŠ¥çš„ä»£ç†äººã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é€šå¸¸å¯¹æŠ˜æ‰£å¥–åŠ±çš„æ€»å’Œæœ€å¤§åŒ–æ„Ÿå…´è¶£</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kx"><img src="../Images/a9f1eb8818dc7dd5e9b9f841e3c92b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/0*dwOUuOx1i089pCXE"/></div></figure><p id="3f36" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å…¶ä¸­ï¼ŒÎ³æ˜¯æŠ˜æ‰£å› å­ï¼Œé€šå¸¸åœ¨[0.95ï¼Œ0.99]èŒƒå›´å†…ï¼Œr_tæ˜¯æ—¶é—´æˆ³tçš„å¥–åŠ±ã€‚</p><h1 id="cf95" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ç®—æ³•</h1><p id="e75e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è§£å†³RLé—®é¢˜å‘¢ï¼Ÿæœ‰å¤šç§ç®—æ³•ï¼Œä½†å®ƒä»¬å¯ä»¥(é’ˆå¯¹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æˆ–MDP)åˆ†ä¸ºä¸¤ç±»:<strong class="je hi">åŸºäºæ¨¡å‹çš„</strong>(åˆ›å»ºç¯å¢ƒçš„æ¨¡å‹)å’Œ<strong class="je hi">æ— æ¨¡å‹çš„</strong>(åªéœ€äº†è§£ç»™å®šä¸€ä¸ªçŠ¶æ€è¦åšä»€ä¹ˆ)ã€‚</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/0374f13917271b72108dc4ec1be7066d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RyxxdI-JNSTwSoke4owtQ.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Taxonomy of Reinforcement Learning algorithms (from <a class="ae ka" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html" rel="noopener ugc nofollow" target="_blank">OpenAI spinning up</a>)</figcaption></figure><p id="9359" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">åŸºäºæ¨¡å‹çš„</strong>ç®—æ³•ä½¿ç”¨ä¸€ä¸ªç¯å¢ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥é¢„æµ‹æœªæ¥çš„çŠ¶æ€å’Œå¥–åŠ±ã€‚è¯¥æ¨¡å‹è¦ä¹ˆæ˜¯ç»™å®šçš„(ä¾‹å¦‚æ£‹ç›˜)ï¼Œè¦ä¹ˆæ˜¯å­¦ä¹ çš„ã€‚</p><p id="8d24" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">æ— æ¨¡å‹</strong>ç®—æ³•å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œç›´æ¥å­¦ä¹ å¦‚ä½•é’ˆå¯¹è®­ç»ƒä¸­é‡åˆ°çš„çŠ¶æ€é‡‡å–è¡ŒåŠ¨(ç­–ç•¥ä¼˜åŒ–æˆ–PO)ï¼Œå“ªäº›çŠ¶æ€-è¡ŒåŠ¨å¯¹äº§ç”Ÿè‰¯å¥½çš„å›æŠ¥(Q-Learning)ï¼Œæˆ–è€…ä¸¤è€…åŒæ—¶è¿›è¡Œã€‚</p><p id="d5bd" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi"> PPO </strong>å±äºPOç®—æ³•å®¶æ—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹æ¥å­¦ä¹ PPOç®—æ³•ã€‚POå’ŒQ-Learningç®—æ³•ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒPOç®—æ³•å¯ä»¥ç”¨äºå…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒä¸­(å³ï¼Œæˆ‘ä»¬çš„åŠ¨ä½œå…·æœ‰çœŸå®å€¼çš„ç¯å¢ƒä¸­)ï¼Œå¹¶ä¸”å¯ä»¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå³ä½¿è¯¥ç­–ç•¥æ˜¯éšæœºçš„(å³ï¼Œæ¦‚ç‡æ€§åœ°åŠ¨ä½œ)ï¼Œè€ŒQ-Learningç®—æ³•ä¸èƒ½åšè¿™äº›äº‹æƒ…ã€‚è¿™æ˜¯æ›´å–œæ¬¢POç®—æ³•çš„å¦ä¸€ä¸ªåŸå› ã€‚å¦ä¸€æ–¹é¢ï¼ŒQå­¦ä¹ ç®—æ³•å¾€å¾€æ›´ç®€å•ã€æ›´ç›´è§‚ã€æ›´å¥½è®­ç»ƒã€‚</p><h2 id="70c0" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">ç­–ç•¥ä¼˜åŒ–(åŸºäºæ¢¯åº¦)</h2><p id="15ad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">POç®—æ³•å°è¯•ç›´æ¥å­¦ä¹ ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬è¦ä¹ˆä½¿ç”¨æ— æ¢¯åº¦ç®—æ³•(å¦‚é—ä¼ ç®—æ³•)ï¼Œè¦ä¹ˆä½¿ç”¨æ›´å¸¸è§çš„åŸºäºæ¢¯åº¦çš„ç®—æ³•ã€‚</p><p id="3dae" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¯¹äºåŸºäºæ¢¯åº¦çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯æ‰€æœ‰è¯•å›¾ä¼°è®¡æ‰€å­¦æ”¿ç­–ç›¸å¯¹äºç´¯ç§¯å›æŠ¥çš„æ¢¯åº¦çš„æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“è¿™ä¸ªæ¢¯åº¦(æˆ–å®ƒçš„è¿‘ä¼¼å€¼)ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ”¿ç­–çš„å‚æ•°å‘æ¢¯åº¦çš„æ–¹å‘ç§»åŠ¨ï¼Œä»¥ä½¿å›æŠ¥æœ€å¤§åŒ–ã€‚</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ln"><img src="../Images/979b9dea533516e58766d689258a5b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O0oxrEZak8ko4xz8.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Objective to be maximized with PO algorithms. Image from <a class="ae ka" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" rel="noopener ugc nofollow" target="_blank">Lilâ€™Logâ€™s blog.</a></figcaption></figure><p id="2205" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯·æ³¨æ„ï¼Œæœ‰å¤šç§æ–¹æ³•å¯ä»¥ä¼°ç®—æ¢¯åº¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°åˆ—å‡ºäº†6ä¸ªä¸åŒçš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å®ƒä»¬ä½œä¸ºæˆ‘ä»¬çš„æœ€å¤§åŒ–ç›®æ ‡:æ€»å›æŠ¥ã€ä¸€æ¬¡è¡ŒåŠ¨åçš„å›æŠ¥ã€å‡å»åŸºçº¿ç‰ˆæœ¬çš„å›æŠ¥ã€çŠ¶æ€-è¡ŒåŠ¨å€¼å‡½æ•°ã€ä¼˜åŠ¿å‡½æ•°(åœ¨æœ€åˆçš„PPOè®ºæ–‡ä¸­ä½¿ç”¨)å’Œæ—¶é—´å·®(TD)æ®‹å·®ã€‚åŸåˆ™ä¸Šï¼Œå®ƒä»¬éƒ½æä¾›äº†æˆ‘ä»¬æ„Ÿå…´è¶£çš„çœŸå®æ¢¯åº¦çš„ä¼°è®¡ã€‚</p><h1 id="0dc4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">èšè‹¯é†šï¼ˆPolyphenylene Oxideçš„ç¼©å†™ï¼‰</h1><p id="3c62" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> PPO </strong>æ˜¯ä¸€ç§(æ— æ¨¡å‹)ç­–ç•¥ä¼˜åŒ–çš„åŸºäºæ¢¯åº¦çš„ç®—æ³•ã€‚è¯¥ç®—æ³•æ—¨åœ¨å­¦ä¹ ä¸€ç§ç­–ç•¥ï¼Œåœ¨è®­ç»ƒæœŸé—´ç»™å®šç»éªŒçš„æƒ…å†µä¸‹ï¼Œæœ€å¤§åŒ–æ‰€è·å¾—çš„ç´¯ç§¯å¥–åŠ±ã€‚</p><p id="2103" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å®ƒç”±ä¸€ä¸ª<strong class="je hi">æ¼”å‘˜</strong> <strong class="je hi"> Ï€Î¸(ã€‚| st) </strong>ï¼Œå…¶è¾“å‡ºç»™å®šæ—¶é—´æˆ³tçš„çŠ¶æ€ä¸‹çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»¥åŠç”±<strong class="je hi">è¯„è®ºå®¶</strong> <strong class="je hi"> V(st) </strong>ï¼Œå…¶ä¼°è®¡æ¥è‡ªè¯¥çŠ¶æ€(æ ‡é‡)çš„é¢„æœŸç´¯ç§¯å›æŠ¥ã€‚å› ä¸ºæ¼”å‘˜å’Œè¯„è®ºå®¶éƒ½å°†çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œæ‰€ä»¥å¯ä»¥åœ¨æå–é«˜çº§ç‰¹å¾çš„ä¸¤ä¸ªç½‘ç»œä¹‹é—´å…±äº«ä¸»å¹²æ¶æ„ã€‚</p><p id="ad95" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPOçš„ç›®çš„æ˜¯ä½¿æ”¿ç­–æ›´æœ‰å¯èƒ½é€‰æ‹©å…·æœ‰é«˜åº¦â€œä¼˜åŠ¿â€çš„è¡ŒåŠ¨ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå…·æœ‰æ¯”æ‰¹è¯„å®¶æ‰€èƒ½é¢„æµ‹çš„é«˜å¾—å¤šçš„è¡¡é‡ç´¯ç§¯å›æŠ¥ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›åœ¨ä¸€ä¸ªæ­¥éª¤ä¸­æ›´æ–°ç­–ç•¥å¤ªå¤šï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå¯¼è‡´ä¼˜åŒ–é—®é¢˜ã€‚æœ€åï¼Œå¦‚æœè¯¥ç­–ç•¥å…·æœ‰é«˜ç†µï¼Œæˆ‘ä»¬å°†ä¸ºå…¶æä¾›å¥–é‡‘ï¼Œä»¥æ¿€åŠ±æ¢ç´¢è€Œä¸æ˜¯å¼€å‘ã€‚</p><p id="77eb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ€»æŸå¤±å‡½æ•°(å°†è¢«æœ€å¤§åŒ–)ç”±ä¸‰é¡¹ç»„æˆ:å‰Šæ³¢é¡¹ã€ä»·å€¼å‡½æ•°(VF)é¡¹å’Œç†µå¥–åŠ±ã€‚</p><p id="1bde" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æœ€ç»ˆç›®æ ‡å¦‚ä¸‹:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lo"><img src="../Images/4eee1d33404f1b2b761471a13990c1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*8WP3C1EQDbKvU_cI87otoA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">The loss function of PPO to be maximized.</figcaption></figure><p id="fce4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å…¶ä¸­ï¼Œc1å’Œc2æ˜¯è¶…å‚æ•°ï¼Œåˆ†åˆ«è¡¡é‡ç­–ç•¥çš„æ‰¹è¯„å‡†ç¡®æ€§å’Œæ¢ç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚</p><h2 id="88e4" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">å‰ªè¾‘æœ¯è¯­</h2><p id="6d57" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ­£å¦‚æˆ‘ä»¬æ‰€è¯´çš„ï¼ŒæŸå¤±å‡½æ•°ä¿ƒä½¿äº§ç”Ÿä¼˜åŠ¿çš„è¡ŒåŠ¨çš„æ¦‚ç‡æœ€å¤§åŒ–(æˆ–è€…ï¼Œå¦‚æœè¡ŒåŠ¨äº§ç”Ÿè´Ÿé¢ä¼˜åŠ¿ï¼Œåˆ™ä½¿æ¦‚ç‡æœ€å°åŒ–):</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lp"><img src="../Images/ce776e6339715fda94243c3426efa1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*cr288gSfmdqMyRuGj4X8aQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">First loss term. We maximize and minimize the probability of picking</figcaption></figure><p id="1f9a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å…¶ä¸­:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/f7bac037dd0d4bf8900570879e4c7a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dranEeIvz8lW_4XjUY3mXg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Coefficient rt(Î¸). This is the term that gradients are going to go through.</figcaption></figure><p id="686f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ˜¯ä¸€ä¸ªæ¯”ç‡ï¼Œç”¨äºè¡¡é‡æˆ‘ä»¬ç°åœ¨(ä½¿ç”¨æ›´æ–°çš„ç­–ç•¥)ç›¸å¯¹äºä»¥å‰æ‰§è¡Œä»¥å‰çš„æ“ä½œçš„å¯èƒ½æ€§ã€‚åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬ä¸å¸Œæœ›è¿™ä¸ªç³»æ•°å¤ªé«˜ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ”¿ç­–çªç„¶æ”¹å˜ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å–å®ƒçš„æœ€å°å€¼å’Œ[1+Ïµ1-Ïµ]ä¹‹é—´çš„å‰ªè¾‘ç‰ˆæœ¬ï¼Œå…¶ä¸­Ïµæ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚</p><p id="5ff2" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ä¼˜åŠ¿è®¡ç®—å¦‚ä¸‹:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lr"><img src="../Images/dcb251ae3343ef3432352da91060b17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v94VN_62M-FbWPQOoETLHw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Advantage estimate. We simply take a difference between what we estimated the cumulative reward would have been given the initial state and the real cumulative reward observed up to a step t plus the estimate from that state onward. We apply a stop-gradient operator to this term in the CLIP loss.</figcaption></figure><p id="1c85" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬çœ‹åˆ°ï¼Œå®ƒåªæ˜¯ç®€å•åœ°è¡¡é‡äº†æ‰¹è¯„è€…å¯¹ç»™å®šçŠ¶æ€stçš„é”™è¯¯ç¨‹åº¦ã€‚å¦‚æœæˆ‘ä»¬è·å¾—äº†æ›´é«˜çš„ç´¯ç§¯å¥–åŠ±ï¼Œä¼˜åŠ¿ä¼°è®¡å°†ä¸ºæ­£ï¼Œæˆ‘ä»¬å°†æ›´æœ‰å¯èƒ½åœ¨è¿™ç§çŠ¶æ€ä¸‹é‡‡å–è¡ŒåŠ¨ã€‚åä¹‹äº¦ç„¶ï¼Œå¦‚æœæˆ‘ä»¬æœŸæœ›æ›´é«˜çš„å›æŠ¥ï¼Œè€Œæˆ‘ä»¬å¾—åˆ°äº†æ›´ä½çš„å›æŠ¥ï¼Œä¼˜åŠ¿ä¼°è®¡å°†æ˜¯è´Ÿçš„ï¼Œæˆ‘ä»¬å°†å‡å°‘åœ¨è¿™ä¸€æ­¥é‡‡å–è¡ŒåŠ¨çš„å¯èƒ½æ€§ã€‚</p><p id="a432" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯·æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬ä¸€ç›´åˆ°æœ€åçš„çŠ¶æ€sTï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¾èµ–æ‰¹è¯„å®¶æœ¬èº«ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ‰¹è¯„å®¶ä¸å®é™…çš„ç´¯ç§¯å›æŠ¥è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹ä¼˜åŠ¿çš„ä¼°è®¡å°±æ˜¯çœŸæ­£çš„ä¼˜åŠ¿ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨å®ç°è½¦æ†é—®é¢˜æ—¶è¦åšçš„äº‹æƒ…ã€‚</p><h2 id="b67b" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">ä»·å€¼å‡½æ•°é¡¹</h2><p id="719c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">ç„¶è€Œï¼Œä¸ºäº†å¯¹ä¼˜åŠ¿æœ‰ä¸€ä¸ªå¥½çš„ä¼°è®¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¯ä»¥é¢„æµ‹ç»™å®šçŠ¶æ€çš„å€¼çš„è¯„è®ºå®¶ã€‚è¯¥æ¨¡å‹æ˜¯ä»¥ç›‘ç£çš„æ–¹å¼å­¦ä¹ çš„ï¼Œå…·æœ‰ç®€å•çš„MSEæŸå¤±:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ls"><img src="../Images/c99c51b76bff4edaad1cf444e252a799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cI0e2esKc8ewZ0HZ"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">The loss function for our critic is simply the Mean-Squared-Error between its predicted expected reward and the observed cumulative reward. We apply a stop-gradient operator only to the observed reward in this case and optimize the critic.</figcaption></figure><p id="ecfa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ›´æ–°criticï¼Œè¿™æ ·éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›è¶Šæ¥è¶Šç²¾ç¡®çš„çŠ¶æ€å€¼ã€‚</p><h2 id="934f" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">ç†µé¡¹</h2><p id="d98d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æœ€åï¼Œæˆ‘ä»¬é¼“åŠ±æ¢ç´¢ï¼Œå¯¹æ”¿ç­–çš„äº§å‡ºåˆ†å¸ƒçš„ç†µæœ‰ä¸€ç‚¹å¥–åŠ±ã€‚æˆ‘ä»¬è€ƒè™‘æ ‡å‡†ç†µ:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lt"><img src="../Images/d72cf63e4034c628f33a3bd4630c298b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/0*TRSWE2dVyRxpvG3g"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Entropy formula for the output distribution given by the policy model.</figcaption></figure><h1 id="c5c2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">å±¥è¡Œ</h1><p id="aacb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">ä¸è¦æ‹…å¿ƒè¿™ä¸ªç†è®ºæ˜¯å¦ä»ç„¶æœ‰ç‚¹å¯ç–‘ã€‚è¿™ä¸ªå®ç°æœ‰æœ›è®©ä¸€åˆ‡å˜å¾—æ¸…æ™°ã€‚</p><h2 id="d56b" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">ç‹¬ç«‹äºPPOçš„ä»£ç </h2><p id="f3f5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">è®©æˆ‘ä»¬ä»è¿›å£å¼€å§‹:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="c560" class="lz if hh lv b be ma mb l mc md">from argparse import ArgumentParser<br/><br/>import gym<br/>import numpy as np<br/>import wandb<br/><br/>import torch<br/>import torch.nn as nn<br/>from torch.optim import Adam<br/>from torch.optim.lr_scheduler import LinearLR<br/>from torch.distributions.categorical import Categorical<br/><br/>import pytorch_lightning as pl</span></pre><p id="a3ed" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPOçš„é‡è¦è¶…å‚æ•°æ˜¯<em class="me">æ•°é‡çš„æ¼”å‘˜</em>ã€<em class="me">è§†é‡</em>ã€<em class="me">Îµ</em>ã€æ¯ä¸ªä¼˜åŒ–é˜¶æ®µçš„<em class="me">æ¬¡æ•°</em>ã€å­¦ä¹ ç‡<em class="me">ã€æŠ˜ç°å› å­<em class="me">Î³</em>ä»¥åŠå¯¹ä¸åŒæŸå¤±é¡¹<em class="me"> c1 </em>å’Œ<em class="me"> c2 </em>è¿›è¡ŒåŠ æƒçš„å¸¸æ•°ã€‚æˆ‘ä»¬é€šè¿‡ç¨‹åºå‚æ•°æ”¶é›†è¿™äº›ä¿¡æ¯ã€‚</em></p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="88b8" class="lz if hh lv b be ma mb l mc md">def parse_args():<br/>    """Pareser program arguments"""<br/>    # Parser<br/>    parser = ArgumentParser()<br/><br/>    # Program arguments (default for Atari games)<br/>    parser.add_argument("--max_iterations", type=int, help="Number of iterations of training", default=100)<br/>    parser.add_argument("--n_actors", type=int, help="Number of actors for each update", default=8)<br/>    parser.add_argument("--horizon", type=int, help="Number of timestamps for each actor", default=128)<br/>    parser.add_argument("--epsilon", type=float, help="Epsilon parameter", default=0.1)<br/>    parser.add_argument("--n_epochs", type=int, help="Number of training epochs per iteration", default=3)<br/>    parser.add_argument("--batch_size", type=int, help="Batch size", default=32 * 8)<br/>    parser.add_argument("--lr", type=float, help="Learning rate", default=2.5 * 1e-4)<br/>    parser.add_argument("--gamma", type=float, help="Discount factor gamma", default=0.99)<br/>    parser.add_argument("--c1", type=float, help="Weight for the value function in the loss function", default=1)<br/>    parser.add_argument("--c2", type=float, help="Weight for the entropy bonus in the loss function", default=0.01)<br/>    parser.add_argument("--n_test_episodes", type=int, help="Number of episodes to render", default=5)<br/>    parser.add_argument("--seed", type=int, help="Randomizing seed for the experiment", default=0)<br/><br/>    # Dictionary with program arguments<br/>    return vars(parser.parse_args())</span></pre><p id="d9d5" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå‚æ•°è®¾ç½®å¦‚æœ¬æ–‡æ‰€è¿°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘ä»¬çš„ä»£ç åº”è¯¥åœ¨GPUä¸Šè¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„å®ç”¨å‡½æ•°ã€‚</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="4b3c" class="lz if hh lv b be ma mb l mc md">def get_device():<br/>    """Gets the device (GPU if any) and logs the type"""<br/>    if torch.cuda.is_available():<br/>        device = torch.device("cuda")<br/>        print(f"Found GPU device: {torch.cuda.get_device_name(device)}")<br/>    else:<br/>        device = torch.device("cpu")<br/>        print("No GPU found: Running on CPU")<br/>    return device</span></pre><p id="8a27" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å½“æˆ‘ä»¬åº”ç”¨RLæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰ä¸€ä¸ªç¼“å†²åŒºæ¥å­˜å‚¨å½“å‰æ¨¡å‹é‡åˆ°çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ã€‚è¿™äº›ç”¨äºæ›´æ–°æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•ˆç”¨å‡½æ•°<code class="du mf mg mh lv b">run_timestamps</code>ï¼Œå®ƒå°†åœ¨ç»™å®šçš„ç¯å¢ƒä¸­è¿è¡Œç»™å®šçš„æ¨¡å‹ï¼Œè¿è¡Œå›ºå®šæ•°é‡çš„æ—¶é—´æˆ³(å¦‚æœå‰§é›†ç»“æŸï¼Œåˆ™é‡æ–°è®¾ç½®ç¯å¢ƒ)ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ªé€‰é¡¹<code class="du mf mg mh lv b">render=False</code>,ä»¥é˜²æˆ‘ä»¬åªæ˜¯æƒ³çœ‹çœ‹ç»è¿‡è®­ç»ƒçš„æ¨¡å‹è¡¨ç°å¦‚ä½•ã€‚</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="8ab5" class="lz if hh lv b be ma mb l mc md">@torch.no_grad()<br/>def run_timestamps(env, model, timestamps=128, render=False, device="cpu"):<br/>    """Runs the given policy on the given environment for the given amount of timestamps.<br/>     Returns a buffer with state action transitions and rewards."""<br/>    buffer = []<br/>    state = env.reset()[0]<br/><br/>    # Running timestamps and collecting state, actions, rewards and terminations<br/>    for ts in range(timestamps):<br/>        # Taking a step into the environment<br/>        model_input = torch.from_numpy(state).unsqueeze(0).to(device).float()<br/>        action, action_logits, value = model(model_input)<br/>        new_state, reward, terminated, truncated, info = env.step(action.item())<br/><br/>        # Rendering / storing (s, a, r, t) in the buffer<br/>        if render:<br/>            env.render()<br/>        else:<br/>            buffer.append([model_input, action, action_logits, value, reward, terminated or truncated])<br/><br/>        # Updating current state<br/>        state = new_state<br/><br/>        # Resetting environment if episode terminated or truncated<br/>        if terminated or truncated:<br/>            state = env.reset()[0]<br/><br/>    return buffer</span></pre><p id="05eb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯¥å‡½æ•°çš„è¾“å‡º(ä¸å‘ˆç°æ—¶)æ˜¯ä¸€ä¸ªç¼“å†²åŒºï¼ŒåŒ…å«æ¯ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€ã€é‡‡å–çš„åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡(logits)ã€ä¼°è®¡çš„è¯„è®ºå®¶å€¼ã€å¥–åŠ±å’Œæ‰€æä¾›ç­–ç•¥çš„ç»ˆæ­¢çŠ¶æ€ã€‚æ³¨æ„ï¼Œè¿™ä¸ªå‡½æ•°ä½¿ç”¨äº†è£…é¥°å™¨<strong class="je hi"> @torch.no_grad()ï¼Œ</strong>ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å­˜å‚¨åœ¨ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ‰€é‡‡å–çš„åŠ¨ä½œçš„æ¸å˜ã€‚</p><h2 id="cec7" class="kz if hh bd ig la lb lc ik ld le lf io jn lg lh is jr li lj iw jv lk ll ja lm bi translated">PPOçš„ä»£ç </h2><p id="dc34" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ—¢ç„¶æˆ‘ä»¬å·²ç»è§£å†³äº†çç¢çš„äº‹æƒ…ï¼Œæ˜¯æ—¶å€™å®ç°æ ¸å¿ƒç®—æ³•äº†ã€‚</p><p id="b7ba" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„<strong class="je hi">ä¸»</strong>å‡½æ•°çœ‹èµ·æ¥åƒè¿™æ ·:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="5330" class="lz if hh lv b be ma mb l mc md">def main():<br/>    # Parsing program arguments<br/>    args = parse_args()<br/>    print(args)<br/><br/>    # Setting seed<br/>    pl.seed_everything(args["seed"])<br/><br/>    # Getting device<br/>    device = get_device()<br/><br/>    # Creating environment (discrete action space)<br/>    env_name = "CartPole-v1"<br/>    env = gym.make(env_name)<br/><br/>    # Creating the model, training it and rendering the result<br/>    # (We are missing this part ğŸ˜…)<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/>    training_loop(env, model, args)<br/>    model = load_best_model()<br/>    testing_loop(env, model)</span></pre><p id="80bc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬å·²ç»æ‹¿åˆ°å¤§éƒ¨åˆ†äº†ã€‚æˆ‘ä»¬åªéœ€è¦å®šä¹‰PPOæ¨¡å‹ã€è®­ç»ƒå’Œæµ‹è¯•åŠŸèƒ½ã€‚</p><p id="6d90" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPOæ¨¡å‹çš„æ¶æ„ä¸æ˜¯è¿™é‡Œæœ‰è¶£çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬åªéœ€è¦ä¸¤ä¸ªåœ¨ç¯å¢ƒä¸­è¡¨æ¼”çš„æ¨¡ç‰¹(æ¼”å‘˜å’Œè¯„è®ºå®¶)ã€‚å½“ç„¶ï¼Œæ¨¡å‹æ¶æ„åœ¨æ›´å›°éš¾çš„ä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†æ˜¯æœ‰äº†cart poleï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿¡ä¸€äº›MLPå°†å®Œæˆè¿™é¡¹å·¥ä½œã€‚</p><p id="921a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ¼”å‘˜å’Œè¯„è®ºå®¶æ¨¡å‹çš„MyPPOç±»ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šä¸¤è€…ä¹‹é—´çš„éƒ¨åˆ†æ¶æ„æ˜¯å…±äº«çš„ã€‚å½“å¯¹æŸäº›çŠ¶æ€è¿è¡Œforwardæ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬è¿”å›å‚ä¸è€…çš„æŠ½æ ·åŠ¨ä½œã€æ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„ç›¸å¯¹æ¦‚ç‡(logits)ä»¥åŠè¯„è®ºå®¶å¯¹æ¯ä¸ªçŠ¶æ€çš„ä¼°è®¡å€¼ã€‚</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="7675" class="lz if hh lv b be ma mb l mc md">class MyPPO(nn.Module):<br/>    """Implementation of a PPO model. The same backbone is used to get actor and critic values."""<br/><br/>    def __init__(self, in_shape, n_actions, hidden_d=100, share_backbone=False):<br/>        # Super constructor<br/>        super(MyPPO, self).__init__()<br/><br/>        # Attributes<br/>        self.in_shape = in_shape<br/>        self.n_actions = n_actions<br/>        self.hidden_d = hidden_d<br/>        self.share_backbone = share_backbone<br/><br/>        # Shared backbone for policy and value functions<br/>        in_dim = np.prod(in_shape)<br/><br/>        def to_features():<br/>            return nn.Sequential(<br/>                nn.Flatten(),<br/>                nn.Linear(in_dim, hidden_d),<br/>                nn.ReLU(),<br/>                nn.Linear(hidden_d, hidden_d),<br/>                nn.ReLU()<br/>            )<br/><br/>        self.backbone = to_features() if self.share_backbone else nn.Identity()<br/><br/>        # State action function<br/>        self.actor = nn.Sequential(<br/>            nn.Identity() if self.share_backbone else to_features(),<br/>            nn.Linear(hidden_d, hidden_d),<br/>            nn.ReLU(),<br/>            nn.Linear(hidden_d, n_actions),<br/>            nn.Softmax(dim=-1)<br/>        )<br/><br/>        # Value function<br/>        self.critic = nn.Sequential(<br/>            nn.Identity() if self.share_backbone else to_features(),<br/>            nn.Linear(hidden_d, hidden_d),<br/>            nn.ReLU(),<br/>            nn.Linear(hidden_d, 1)<br/>        )<br/><br/>    def forward(self, x):<br/>        features = self.backbone(x)<br/>        action = self.actor(features)<br/>        value = self.critic(features)<br/>        return Categorical(action).sample(), action, value</span></pre><p id="49a2" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯·æ³¨æ„ï¼Œ<code class="du mf mg mh lv b">Categorical(action).sample()</code>åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰åŠ¨ä½œé€»è¾‘çš„åˆ†ç±»åˆ†å¸ƒï¼Œå¹¶ä»ä¸­æŠ½å–äº†ä¸€ä¸ªåŠ¨ä½œæ ·æœ¬(é’ˆå¯¹æ¯ä¸ªå·)ã€‚</p><p id="2520" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æœ€åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨<code class="du mf mg mh lv b">training_loop</code>å‡½æ•°ä¸­å¤„ç†å®é™…çš„ç®—æ³•ã€‚æ­£å¦‚æˆ‘ä»¬ä»è®ºæ–‡ä¸­æ‰€çŸ¥ï¼Œå‡½æ•°çš„å®é™…ç­¾ååº”è¯¥æ˜¯è¿™æ ·çš„:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="4d67" class="lz if hh lv b be ma mb l mc md">def training_loop(env, model, max_iterations, n_actors, horizon, gamma, <br/>epsilon, n_epochs, batch_size, lr, c1, c2, device, env_name=""):<br/>  # TODO...</span></pre><p id="6e19" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ä¸‹é¢æ˜¯æœ¬æ–‡ä¸­ä¸ºPPOåŸ¹è®­ç¨‹åºæä¾›çš„ä¼ªä»£ç :</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mi"><img src="../Images/adf9438a5b6a7b4fab6124db0fcc8de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1ROKGqst4p8rXTBtDwa_A.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Pseudo code for PPO training provided in the <a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">original paper</a>.</figcaption></figure><p id="00b3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">PPOçš„ä¼ªä»£ç ç›¸å¯¹ç®€å•:æˆ‘ä»¬ç®€å•åœ°é€šè¿‡æˆ‘ä»¬çš„ç­–ç•¥æ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬(ç§°ä¸ºè¡ŒåŠ¨è€…)æ”¶é›†ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå¹¶ä½¿ç”¨å…ˆå‰å®šä¹‰çš„ç›®æ ‡æ¥ä¼˜åŒ–è¡ŒåŠ¨è€…å’Œæ‰¹è¯„è€…ç½‘ç»œã€‚</p><p id="3340" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç”±äºæˆ‘ä»¬éœ€è¦è¡¡é‡æˆ‘ä»¬å®é™…è·å¾—çš„ç´¯ç§¯å¥–åŠ±ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œç»™å®šä¸€ä¸ªç¼“å†²åŒºï¼Œç”¨ç´¯ç§¯å¥–åŠ±æ›¿æ¢æ¯ä¸ªæ—¶é—´æˆ³çš„å¥–åŠ±:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="b59f" class="lz if hh lv b be ma mb l mc md">def compute_cumulative_rewards(buffer, gamma):<br/>    """Given a buffer with states, policy action logits, rewards and terminations,<br/>    computes the cumulative rewards for each timestamp and substitutes them into the buffer."""<br/>    curr_rew = 0.<br/><br/>    # Traversing the buffer on the reverse direction<br/>    for i in range(len(buffer) - 1, -1, -1):<br/>        r, t = buffer[i][-2], buffer[i][-1]<br/><br/>        if t:<br/>            curr_rew = 0<br/>        else:<br/>            curr_rew = r + gamma * curr_rew<br/><br/>        buffer[i][-2] = curr_rew<br/><br/>    # Getting the average reward before normalizing (for logging and checkpointing)<br/>    avg_rew = np.mean([buffer[i][-2] for i in range(len(buffer))])<br/><br/>    # Normalizing cumulative rewards<br/>    mean = np.mean([buffer[i][-2] for i in range(len(buffer))])<br/>    std = np.std([buffer[i][-2] for i in range(len(buffer))]) + 1e-6<br/>    for i in range(len(buffer)):<br/>        buffer[i][-2] = (buffer[i][-2] - mean) / std<br/><br/>    return avg_rew</span></pre><p id="b66b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¯·æ³¨æ„ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†ç´¯ç§¯å¥–åŠ±æ ‡å‡†åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æŠ€å·§ï¼Œå¯ä»¥ä½¿ä¼˜åŒ–é—®é¢˜æ›´å®¹æ˜“ï¼Œè®­ç»ƒæ›´æµç•…ã€‚</p><p id="85f0" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç°åœ¨æˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ªåŒ…å«çŠ¶æ€ã€é‡‡å–çš„è¡ŒåŠ¨ã€è¡ŒåŠ¨æ¦‚ç‡å’Œç´¯ç§¯å›æŠ¥çš„ç¼“å†²åŒºï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œç»™å®šä¸€ä¸ªç¼“å†²åŒºï¼Œè®¡ç®—æˆ‘ä»¬æœ€ç»ˆç›®æ ‡çš„ä¸‰ä¸ªæŸå¤±é¡¹:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="f524" class="lz if hh lv b be ma mb l mc md">def get_losses(model, batch, epsilon, annealing, device="cpu"):<br/>    """Returns the three loss terms for a given model and a given batch and additional parameters"""<br/>    # Getting old data<br/>    n = len(batch)<br/>    states = torch.cat([batch[i][0] for i in range(n)])<br/>    actions = torch.cat([batch[i][1] for i in range(n)]).view(n, 1)<br/>    logits = torch.cat([batch[i][2] for i in range(n)])<br/>    values = torch.cat([batch[i][3] for i in range(n)])<br/>    cumulative_rewards = torch.tensor([batch[i][-2] for i in range(n)]).view(-1, 1).float().to(device)<br/><br/>    # Computing predictions with the new model<br/>    _, new_logits, new_values = model(states)<br/><br/>    # Loss on the state-action-function / actor (L_CLIP)<br/>    advantages = cumulative_rewards - values<br/>    margin = epsilon * annealing<br/>    ratios = new_logits.gather(1, actions) / logits.gather(1, actions)<br/><br/>    l_clip = torch.mean(<br/>        torch.min(<br/>            torch.cat(<br/>                (ratios * advantages,<br/>                 torch.clip(ratios, 1 - margin, 1 + margin) * advantages),<br/>                dim=1),<br/>            dim=1<br/>        ).values<br/>    )<br/><br/>    # Loss on the value-function / critic (L_VF)<br/>    l_vf = torch.mean((cumulative_rewards - new_values) ** 2)<br/><br/>    # Bonus for entropy of the actor<br/>    entropy_bonus = torch.mean(torch.sum(-new_logits * (torch.log(new_logits + 1e-5)), dim=1))<br/><br/>    return l_clip, l_vf, entropy_bonus</span></pre><p id="0fb1" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ³¨æ„ï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª<em class="me">é€€ç«</em>å‚æ•°ï¼Œå®ƒè¢«è®¾ç½®ä¸º1ï¼Œå¹¶åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å‘0çº¿æ€§è¡°å‡ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œéšç€è®­ç»ƒçš„è¿›å±•ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ”¿ç­–æ”¹å˜è¶Šæ¥è¶Šå°‘ã€‚è¿˜è¦æ³¨æ„çš„æ˜¯ï¼Œ<em class="me">ä¼˜åŠ¿</em>å˜é‡æ˜¯å¼ é‡ä¹‹é—´çš„ç®€å•å·®å¼‚ï¼Œæˆ‘ä»¬ä¸è·Ÿè¸ªæ¢¯åº¦ï¼Œä¸åƒ<em class="me"> new_logits </em>å’Œ<em class="me"> new_values </em>ã€‚</p><p id="49cd" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†ä¸ç¯å¢ƒäº¤äº’å’Œå­˜å‚¨ç¼“å†²åŒºã€è®¡ç®—(çœŸå®)ç´¯ç§¯å›æŠ¥å’Œè·å¾—æŸå¤±é¡¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç¼–å†™æœ€ç»ˆçš„è®­ç»ƒå¾ªç¯äº†:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="52f7" class="lz if hh lv b be ma mb l mc md">def training_loop(env, model, max_iterations, n_actors, horizon, gamma, epsilon, n_epochs, batch_size, lr,<br/>                  c1, c2, device, env_name=""):<br/>    """Train the model on the given environment using multiple actors acting up to n timestamps."""<br/><br/>    # Starting a new Weights &amp; Biases run<br/>    wandb.init(project="Papers Re-implementations",<br/>               entity="peutlefaire",<br/>               name=f"PPO - {env_name}",<br/>               config={<br/>                   "env": str(env),<br/>                   "number of actors": n_actors,<br/>                   "horizon": horizon,<br/>                   "gamma": gamma,<br/>                   "epsilon": epsilon,<br/>                   "epochs": n_epochs,<br/>                   "batch size": batch_size,<br/>                   "learning rate": lr,<br/>                   "c1": c1,<br/>                   "c2": c2<br/>               })<br/><br/>    # Training variables<br/>    max_reward = float("-inf")<br/>    optimizer = Adam(model.parameters(), lr=lr, maximize=True)<br/>    scheduler = LinearLR(optimizer, 1, 0, max_iterations * n_epochs)<br/>    anneals = np.linspace(1, 0, max_iterations)<br/><br/>    # Training loop<br/>    for iteration in range(max_iterations):<br/>        buffer = []<br/>        annealing = anneals[iteration]<br/><br/>        # Collecting timestamps for all actors with the current policy<br/>        for actor in range(1, n_actors + 1):<br/>            buffer.extend(run_timestamps(env, model, horizon, False, device))<br/><br/>        # Computing cumulative rewards and shuffling the buffer<br/>        avg_rew = compute_cumulative_rewards(buffer, gamma)<br/>        np.random.shuffle(buffer)<br/><br/>        # Running optimization for a few epochs<br/>        for epoch in range(n_epochs):<br/>            for batch_idx in range(len(buffer) // batch_size):<br/>                # Getting batch for this buffer<br/>                start = batch_size * batch_idx<br/>                end = start + batch_size if start + batch_size &lt; len(buffer) else -1<br/>                batch = buffer[start:end]<br/><br/>                # Zero-ing optimizers gradients<br/>                optimizer.zero_grad()<br/><br/>                # Getting the losses<br/>                l_clip, l_vf, entropy_bonus = get_losses(model, batch, epsilon, annealing, device)<br/><br/>                # Computing total loss and back-propagating it<br/>                loss = l_clip - c1 * l_vf + c2 * entropy_bonus<br/>                loss.backward()<br/><br/>                # Optimizing<br/>                optimizer.step()<br/>            scheduler.step()<br/><br/>        # Logging information to stdout<br/>        curr_loss = loss.item()<br/>        log = f"Iteration {iteration + 1} / {max_iterations}: " \<br/>              f"Average Reward: {avg_rew:.2f}\t" \<br/>              f"Loss: {curr_loss:.3f} " \<br/>              f"(L_CLIP: {l_clip.item():.1f} | L_VF: {l_vf.item():.1f} | L_bonus: {entropy_bonus.item():.1f})"<br/>        if avg_rew &gt; max_reward:<br/>            torch.save(model.state_dict(), MODEL_PATH)<br/>            max_reward = avg_rew<br/>            log += " --&gt; Stored model with highest average reward"<br/>        print(log)<br/><br/>        # Logging information to W&amp;B<br/>        wandb.log({<br/>            "loss (total)": curr_loss,<br/>            "loss (clip)": l_clip.item(),<br/>            "loss (vf)": l_vf.item(),<br/>            "loss (entropy bonus)": entropy_bonus.item(),<br/>            "average reward": avg_rew<br/>        })<br/><br/>    # Finishing W&amp;B session<br/>    wandb.finish()</span></pre><p id="b1b4" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æœ€åï¼Œä¸ºäº†æŸ¥çœ‹æœ€ç»ˆæ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„<code class="du mf mg mh lv b">testing_loop</code>å‡½æ•°:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="c6b5" class="lz if hh lv b be ma mb l mc md">def testing_loop(env, model, n_episodes, device):<br/>    """Runs the learned policy on the environment for n episodes"""<br/>    for _ in range(n_episodes):<br/>        run_timestamps(env, model, timestamps=128, render=True, device=device)</span></pre><p id="56f3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬çš„ä¸»è¦è®¡åˆ’å¾ˆç®€å•:</p><pre class="kh ki kj kk fd lu lv lw bn lx ly bi"><span id="507d" class="lz if hh lv b be ma mb l mc md">def main():<br/>    # Parsing program arguments<br/>    args = parse_args()<br/>    print(args)<br/><br/>    # Setting seed<br/>    pl.seed_everything(args["seed"])<br/><br/>    # Getting device<br/>    device = get_device()<br/><br/>    # Creating environment (discrete action space)<br/>    env_name = "CartPole-v1"<br/>    env = gym.make(env_name)<br/><br/>    # Creating the model (both actor and critic)<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/><br/>    # Training<br/>    training_loop(env, model, args["max_iterations"], args["n_actors"], args["horizon"], args["gamma"], args["epsilon"],<br/>                  args["n_epochs"], args["batch_size"], args["lr"], args["c1"], args["c2"], device, env_name)<br/><br/>    # Loading best model<br/>    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)<br/>    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))<br/><br/>    # Testing<br/>    env = gym.make(env_name, render_mode="human")<br/>    testing_loop(env, model, args["n_test_episodes"], device)<br/>    env.close()</span></pre><p id="cb7a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è€Œè¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†å®ç°ï¼å¦‚æœä½ èƒ½èµ°åˆ°è¿™ä¸€æ­¥ï¼Œæ­å–œä½ ã€‚æ‚¨ç°åœ¨çŸ¥é“å¦‚ä½•å®ç°PPOç®—æ³•äº†ã€‚</p><h1 id="98bb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ç»“æœ</h1><p id="1750" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æƒé‡å’Œåå·®æ—¥å¿—è®©æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°è®°å½•çš„æŒ‡æ ‡å’ŒæŸå¤±ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®æŸå¤±å›¾åŠå…¶é¡¹ï¼Œä»¥åŠæ¯æ¬¡è¿­ä»£çš„å¹³å‡å›æŠ¥ã€‚</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/7de6c9dce20f3b6bf976714726b1d5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*kSSdUvHqCHqV0ncQIfL-hg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Training losses through training iterations. The total loss (blue) is the sum of L_CLIP (orange) minus the L_VF (pink) plus a small constant times the entropy bonus (green)</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mj"><img src="../Images/637d6b0a331a89cd568e26eed25e471b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*m7NeWEnlEUa_9djnuLsnGA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Average reward through iterations. PPO quickly learns to maximize the cumulative reward.</figcaption></figure><p id="0a2a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç”±äºæ¨è½¦æ†ç¯å¢ƒå¹¶ä¸æå…·æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¾ˆå¿«å°±æ‰¾åˆ°äº†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¤§çº¦20æ­¥åæœ€å¤§åŒ–äº†å¹³å‡å¥–åŠ±ã€‚æ­¤å¤–ï¼Œç”±äºç¯å¢ƒåªæœ‰ä¸¤ç§å¯èƒ½çš„è¡Œä¸ºï¼Œç†µé¡¹åŸºæœ¬ä¸Šä¿æŒä¸å˜ã€‚</p><p id="1b64" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æœ€åï¼Œå¦‚æœæˆ‘ä»¬å°†æœ€ç»ˆç­–ç•¥ä»˜è¯¸å®æ–½ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mk"><img src="../Images/5f14ae3564f14fb000ba7fa324183eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/1*JVPhoji799kNzYT5dVEw8g.gif"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Trained PPO model balancing the cart pole</figcaption></figure><h1 id="d438" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ç»“è®º</h1><p id="c20d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">PPOæ˜¯ä¸€ç§æœ€å…ˆè¿›çš„RLç­–ç•¥ä¼˜åŒ–(å› æ­¤æ˜¯æ— æ¨¡å‹çš„)ç®—æ³•ï¼Œå› æ­¤ï¼Œå®ƒå‡ ä¹å¯ä»¥åœ¨ä»»ä½•ç¯å¢ƒä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒPPOå…·æœ‰ç›¸å¯¹ç®€å•çš„ç›®æ ‡å‡½æ•°å’Œç›¸å¯¹è¾ƒå°‘çš„å¾…è°ƒèŠ‚çš„è¶…å‚æ•°ã€‚</p><p id="3813" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¦‚æœä½ æƒ³ç©è¿™ä¸ªåŠ¨æ€ç®—æ³•ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªé“¾æ¥æŒ‡å‘<a class="ae ka" href="https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colabç¬”è®°æœ¬</a>ã€‚ä½ å¯ä»¥åœ¨<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations" rel="noopener ugc nofollow" target="_blank"> GitHubåº“</a>ä¸‹æ‰¾åˆ°æˆ‘ä¸ªäººæœ€æ–°çš„PPOç®—æ³•çš„é‡æ–°å®ç°(ä½œä¸º. pyæ–‡ä»¶)ã€‚æ‚¨å¯ä»¥éšæ„ä½¿ç”¨å®ƒæˆ–å°†å…¶åº”ç”¨åˆ°æ‚¨è‡ªå·±çš„é¡¹ç›®ä¸­ï¼</p><p id="5f2b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•…äº‹ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼è¯·éšæ—¶è”ç³»æˆ‘ä»¬è¿›è¡Œè¿›ä¸€æ­¥çš„è®¨è®ºã€‚ç¥ä½ å’ŒPPOâœŒï¸ä¸€èµ·å¿«ä¹é»‘å®¢</p><h1 id="7915" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">æ›´æ·±å…¥çš„é“¾æ¥:</h1><p id="ef86" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">èˆ’å°”æ›¼ç­‰äººã€‚è‰¾å°”ã€‚â– åŸæ–‡<a class="ae ka" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"><em class="me"/></a></p><p id="c872" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">OpenAIçš„<a class="ae ka" href="https://spinningup.openai.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> " <em class="me">æ—‹è½¬ä¸Šå‡å¯¼è‡³æ·±åº¦RL </em> " </a></p><p id="5efa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è‰è²ç¿çš„<a class="ae ka" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/" rel="noopener ugc nofollow" target="_blank"><em class="me">ä¸€(é•¿)çª¥å¼ºåŒ–å­¦ä¹ </em></a></p><p id="a275" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç¿è‰è²çš„<a class="ae ka" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/" rel="noopener ugc nofollow" target="_blank"><em class="me">æ”¿ç­–æ¢¯åº¦ç®—æ³•</em></a></p><div class="ml mm ez fb mn mo"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mp ab dw"><div class="mq ab mr cl cj ms"><h2 class="bd hi fi z dy mt ea eb mu ed ef hg bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="mv l"><h3 class="bd b fi z dy mt ea eb mu ed ef dx translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="mw l"><p class="bd b fp z dy mt ea eb mu ed ef dx translated">medium.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc kq mo"/></div></div></a></div></div></div>    
</body>
</html>