<html>
<head>
<title>Demystifying DCNNs — the AlexNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开DCNNs的神秘面纱AlexNet</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/demystifying-dcnns-the-alexnet-6081eccb8f57?source=collection_archive---------2-----------------------#2022-10-15">https://medium.com/mlearning-ai/demystifying-dcnns-the-alexnet-6081eccb8f57?source=collection_archive---------2-----------------------#2022-10-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="b6b6" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">亲自实施AlexNet架构</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es iw"><img src="../Images/6e7a58182361f45ca3dfec32c03e672e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Gyks9uqO9ttK1W-ks_4yUQ.gif"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Source — gifs.com</figcaption></figure><h1 id="0f1c" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">Introduction🧧</h1><p id="1843" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">还记得2012年的ImageNet视觉识别挑战赛吗？</p><p id="9270" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">你当然知道！经过大量的反复试验和实验，研究员Alex Krizhevsky和他的合著者Ilya Sutskever和Geoffrey E. Hinton(他们从字面上理解了深度学习中的“深度”)推出了以第一作者命名的AlexNet架构，并赢得了ImageNet挑战赛，从而为深度卷积神经网络的对象检测带来了深度迁移学习的新时代！</p><p id="dace" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">在这篇博客中，我们将揭开AlexNet论文的神秘面纱，深入讨论它的架构，然后动手实践我们所学到的东西！</p><p id="c661" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">事不宜迟，我们开始吧。🤜🏼🤛🏼</p><h1 id="c689" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">先决条件⏮️</h1><p id="c219" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">本文假设您非常了解-</p><ol class=""><li id="586f" class="lb lc hh kc b kd kw kg kx kj ld kn le kr lf kv lg lh li lj bi translated">卷积运算</li><li id="28ae" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">最大池、步幅、内核、填充</li><li id="3473" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">神经网络(CNN)和激活函数(Softmax，ReLU)</li><li id="2d49" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">卷积神经网络</li></ol><h1 id="c85d" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">修改专区！🤔</h1><blockquote class="lp"><p id="be1f" class="lq lr hh bd ls lt lu lv lw lx ly kv dx translated">如果您已经非常了解先决条件，请跳过这一部分</p></blockquote><p id="38a4" class="pw-post-body-paragraph ka kb hh kc b kd lz ii kf kg ma il ki kj mb kl km kn mc kp kq kr md kt ku kv ha bi translated">我附上一些我学习深度学习基础的地方的资源，给你帮助。</p><ol class=""><li id="a5f8" class="lb lc hh kc b kd kw kg kx kj ld kn le kr lf kv lg lh li lj bi translated">卷积运算— <a class="ae me" href="https://www.youtube.com/watch?v=Etksi-F5ug8" rel="noopener ugc nofollow" target="_blank">看这个</a></li><li id="2119" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">卷积神经网络— <a class="ae me" href="https://www.youtube.com/watch?v=SH8D4WJBhms" rel="noopener ugc nofollow" target="_blank">看这个</a></li><li id="b38f" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">学习激活功能，请到我的<a class="ae me" rel="noopener" href="/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e">博客</a></li></ol><h1 id="b552" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">打开八层怪兽🤖</h1><p id="14f8" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">为了更容易理解复杂的架构，我画了一个更容易理解的模型架构。在整个博客中仔细遵循架构！</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mf"><img src="../Images/fede585c92a78fbf21dd3724b1991c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPStTSNdzjLGiPBShA7-hg.jpeg"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">AlexNet Architecture Simplified (Source-Author)</figcaption></figure><p id="a96a" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">让我们把它分解开来，一步一步地理解架构——</p><ol class=""><li id="6784" class="lb lc hh kc b kd kw kg kx kj ld kn le kr lf kv lg lh li lj bi translated">从上面绘制的架构中，观察224×224尺寸的<strong class="kc hi"> RGB图像以及3个通道</strong>(红色(R)、绿色(G)和蓝色(B))。它代表任何彩色图像，因此，我们的输入。</li><li id="6c97" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">第1层</strong> —在第1层中，我们应用步长大小为4的卷积运算，并且具有大小为11 x 11的96个内核。当应用这些操作时，会产生96个大小为55 x 55的图像(因为我们的内核有96个通道)。这只是一个三维张量。</li><li id="4acb" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">然后，我们应用内核大小为3x3、步长大小为2的最大池。这导致图像大小为27x27，有96个通道。</li><li id="1168" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">第2层</strong> —在AlexNet的第2层中，我们再次应用卷积，并用256个大小为5x5的内核进行填充。注意，我在图中的卷积旁边提到了“相同”。填充中的<strong class="kc hi">相同</strong>意味着过滤器将被应用于<strong class="kc hi">输入的所有</strong>元素，并且填充在模型训练阶段应被设置为相同。这确保了输出尺寸与输入尺寸相同。这再次导致具有256个这样的通道的大小为27×27的3D张量。</li><li id="7bf0" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">同样，我们应用了内核大小为3x3、步幅大小为2的max-pooling层，就像我们在第1层之后所做的那样。这产生了具有256个通道的13x13大小的图像。</li><li id="e4ce" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">第3层</strong> —在AlexNet的第3层中，我们再次使用相同的填充(如第2层所述)和384个大小为3x3的内核进行卷积。这导致具有384个通道的尺寸为<strong class="kc hi"> 13x13 </strong>的3D张量。</li><li id="45f6" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">第4层</strong> —在第4层中，我们用384个大小为3x3的内核应用相同填充的卷积。这再次导致具有384个通道的尺寸为<strong class="kc hi"> 13x13 </strong>的3D张量。</li><li id="6afd" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">第5层</strong> —在第5层中，我们使用256个大小为3x3的内核，应用具有相同填充的卷积。这再次导致具有256个通道的尺寸为<strong class="kc hi"> 13x13 </strong>的3D张量。</li><li id="48b5" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated">现在，我们再次应用步幅大小为2的最大池和3×3最大池过滤器(内核)。这产生了具有256个通道的6x6大小的图像。</li><li id="22c8" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">层6、7、8 — </strong>在6x6x256 (= 9216) 3D张量上应用展平操作(维度的简单乘积)时，我们简单地得到2个完全连接的层，然后是最终的softmax层，以输出我们的类。(请注意，该图有1000个输出类，因为它用于对ImageNet问题进行分类，其中有1000个要识别的类)</li></ol><p id="970c" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">因此，基本上，AlexNet有5个卷积层，然后是2个完全连接的层，最后是用于产生输出的Softmax层。</p><h1 id="ab93" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">为什么AlexNet这么酷？！</h1><blockquote class="lp"><p id="f7cf" class="lq lr hh bd ls lt lu lv lw lx ly kv dx translated">嘎吱嘎吱地嚼纸…</p></blockquote><p id="b011" class="pw-post-body-paragraph ka kb hh kc b kd lz ii kf kg ma il ki kj mb kl km kn mc kp kq kr md kt ku kv ha bi translated">让我们来讨论一些最重要的概念，这些概念在深度学习和物体检测中广泛使用，甚至在今天，这些概念是在AlexNet 的论文中介绍的</p><ol class=""><li id="4ce4" class="lb lc hh kc b kd kw kg kx kj ld kn le kr lf kv lg lh li lj bi translated">AlexNet paper是第一个使用强大的非线性激活函数概念的公司。</li><li id="d41e" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">退出</strong>用于防止过拟合，并确保学习中模型的鲁棒性和泛化能力。</li><li id="bff8" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">数据扩充</strong>用于将数据转换成各种形式(水平/垂直翻转、旋转等)。)来增强训练数据的多样化。</li><li id="c9c6" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><strong class="kc hi">多个GPU</strong>用于训练模型以获得良好的性能测量。</li></ol><blockquote class="lp"><p id="4a22" class="lq lr hh bd ls lt mk ml mm mn mo kv dx translated">除此之外，AlexNet论文还使用了一个名为<strong class="ak">局部反应归一化(LRN) </strong>的概念，尽管这是一个值得注意的概念，但它被批量归一化等高级技术进一步取代。</p></blockquote><p id="6b8f" class="pw-post-body-paragraph ka kb hh kc b kd lz ii kf kg ma il ki kj mb kl km kn mc kp kq kr md kt ku kv ha bi translated">就我个人而言，在解决深度学习问题时，我确实更喜欢批处理规范化，但LRN的概念如此美丽，以至于我在研究AlexNet论文时无法跳过它。</p><h1 id="981c" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">LRN将神经科学与人工智能联系起来🔮🪄</h1><p id="d797" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">我们知道，在使用任何激活函数之前，我们的神经网络的隐藏层希望我们将高维数据归一化为零均值和单位方差，以正确地对数据建模。规范化转换数据集中列的值并对其进行缩放，而不会扭曲值范围的差异或丢失信息。</p><p id="4db6" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">使用ReLU后，也就是f (x) = max (0，x)，你会发现使用ReLU后得到的值没有像tanh和sigmoid函数那样的值域。因此，在ReLU之后必须做一个规范。研究人员在AlexNet的论文中提到了一种叫做局部反应正常化的方法，这种方法对我来说似乎很有趣，因为它被发现是由神经科学中一个叫做“<strong class="kc hi">侧抑制</strong>的重要概念所激发的，它谈到了活跃神经元对其周围神经元的工作，对特定刺激做出反应。</p><p id="9c44" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">假设你已经知道ReLU的工作原理，数据应该在0和1之间归一化为任何&lt;0 is 0 in ReLU theories. For an unbounded function (no maximum value) like ReLU, LRN is used to normalize those unbounded activations given out by ReLU.</p><p id="3ad3" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">You will be surprised to know that the concept of LRN in artificial neural networks was adapted from a neuroscience concept called <strong class="kc hi">侧抑制</strong>。</p><blockquote class="lp"><p id="c83d" class="lq lr hh bd ls lt lu lv lw lx ly kv dx translated">侧抑制包括通过兴奋神经元来抑制远处的神经元。</p></blockquote><p id="6450" class="pw-post-body-paragraph ka kb hh kc b kd lz ii kf kg ma il ki kj mb kl km kn mc kp kq kr md kt ku kv ha bi translated">在我们的中枢神经系统中，兴奋或受刺激的神经元往往会抑制远处神经元的活动，这有助于增强我们的感官知觉(回想一下，当你的注意力集中在目标物体上时，你仍然可以模糊地识别周围的环境——当你看着一只鸟时，你仍然可以感觉到周围的树木)。这种视觉抑制反过来增强了视觉图像中的感知和对比度。它有助于改善我们观看时的视觉和视力(以及听觉、嗅觉等)。)!</p><p id="29a3" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated"><strong class="kc hi">但是，LRN是如何与侧抑制联系在一起的呢？</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mp"><img src="../Images/e8e860ef9917dd2fdadcf8737b3c5460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQ8uVctw9vzsuBGBApRSYA.jpeg"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">Source — Author</figcaption></figure><p id="65aa" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated"><strong class="kc hi">局部响应归一化</strong>也是一个涉及卷积神经网络输入特征(特征图)对比度增强的过程。<strong class="kc hi"> LCN在考虑到每个像素的特征图(图像)的局部邻域中执行。</strong></p><p id="a05d" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">总的概念是增强输入图像或特征图上的“峰值”和抑制“平坦”响应，因为峰值与目标物体或刺激的存在正相关，而平坦但高频的响应不给出太多涉及物体或刺激是否实际存在的数据。因此，LRN增加了神经对所需刺激的感觉灵敏度。</p><p id="d3c7" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">综上所述，我们可以推断出，如果与目标对象/刺激(峰值)存在强相关性，则图像的局部区域(或邻域)中的神经元之间的仿真使得<strong class="kc hi">强相关性将抑制弱相关性，从而增强峰值</strong>。但是如果有一个平坦但强的相关性，那么这些强相关性中的每一个都将几乎相等地抑制彼此，这样整个邻域都将被<strong class="kc hi">阻尼</strong>。</p><blockquote class="lp"><p id="b5e0" class="lq lr hh bd ls lt lu lv lw lx ly kv dx translated">因此，LRN过程增强了对象检测和识别。</p></blockquote><blockquote class="mq mr ms"><p id="b2a4" class="ka kb mt kc b kd lz ii kf kg ma il ki mu mb kl km mv mc kp kq mw md kt ku kv ha bi translated">LRN的概念被即将出现的层和批归一化的概念所掩盖(它的影响很小)，层和批归一化是指归一化特征图的整个层或创建小批特征图并归一化每一批，到目前为止在几乎每个神经网络上都工作得很好。</p></blockquote><p id="e7a7" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">有了这些，我们基本上完成了研究论文。现在让我们进入有趣的部分。</p><h1 id="4e6a" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">AlexNet的实际实施👩🏼‍💻</h1><p id="dadf" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">只要看一下架构，我们就可以很容易的对AlexNet的架构进行编码。下面是同样的代码—</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mx my l"/></div></figure><p id="a15b" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">这就对了。传说中的AlexNet触手可及！</p><h1 id="a046" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">参考🔃</h1><p id="d43a" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">我使用了以下资源及其链接，以供参考-</p><ol class=""><li id="d345" class="lb lc hh kc b kd kw kg kx kj ld kn le kr lf kv lg lh li lj bi translated"><a class="ae me" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet论文(2012) </a></li><li id="41c5" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Lateral_inhibition" rel="noopener ugc nofollow" target="_blank">侧向抑制</a></li><li id="68a8" class="lb lc hh kc b kd lk kg ll kj lm kn ln kr lo kv lg lh li lj bi translated"><a class="ae me" href="https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn" rel="noopener ugc nofollow" target="_blank"> LRN </a></li></ol><h1 id="a41a" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">结论🔚</h1><p id="4480" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">希望您在学习AlexNet的过程中感到愉快！如果你希望我涵盖任何其他神经网络架构或研究论文，请在评论中告诉我！</p><blockquote class="mq mr ms"><p id="6a7c" class="ka kb mt kc b kd kw ii kf kg kx il ki mu ky kl km mv kz kp kq mw la kt ku kv ha bi translated">如果你是数据科学和机器学习的初学者，并对数据科学/ML-AI、向数据科学的职业过渡指导、面试/简历准备有一些具体的疑问，或者甚至想在你的D-Day之前获得模拟面试，请随时在此预约1:1电话<a class="ae me" href="https://topmate.io/sukannya" rel="noopener ugc nofollow" target="_blank">。我很乐意帮忙！</a></p></blockquote><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mz"><img src="../Images/7ccf5c55e5765f2ac181b1891e18b416.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*97DEXH-NNPTuDTxYUV4heQ.jpeg"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Source — Author</figcaption></figure><p id="c591" class="pw-post-body-paragraph ka kb hh kc b kd kw ii kf kg kx il ki kj ky kl km kn kz kp kq kr la kt ku kv ha bi translated">快乐学习！😎</p><div class="na nb ez fb nc nd"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hi fi z dy ni ea eb nj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jc nd"/></div></div></a></div></div></div>    
</body>
</html>