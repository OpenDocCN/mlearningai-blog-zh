<html>
<head>
<title>One Stop for K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-均值聚类的一站式服务</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/one-stop-for-k-means-clustering-b58fa59334e5?source=collection_archive---------3-----------------------#2022-05-16">https://medium.com/mlearning-ai/one-stop-for-k-means-clustering-b58fa59334e5?source=collection_archive---------3-----------------------#2022-05-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f061c4070944b0d673d81ef051a6c0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxue67mTHcpC9uMBBJ5X1A.png"/></div></div></figure><p id="00ab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如何对相似的数据点进行聚类，使其有意义？嗯，K-Means是答案之一。这篇文章几乎囊括了K-Means聚类的所有内容。嗯，也就是说，我没有写代码。</p><p id="d703" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">大纲— </strong></p><ol class=""><li id="7b99" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#5c19">什么是K均值聚类</a></li><li id="738c" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#a46d">星团的属性</a></li><li id="8f14" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#892f">K均值聚类算法</a></li><li id="1467" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#091b">收敛/停止标准</a></li><li id="8bf2" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#b055">质心初始化:K-Means ++ </a></li><li id="9f73" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#9f73">选择最佳K </a></li><li id="6392" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/p/b58fa59334e5#497d">评估集群质量</a></li></ol><p id="8d31" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"><em class="jn">~物以类聚</em> </strong></p></div><div class="ab cl kd ke go kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ha hb hc hd he"><h1 id="5c19" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">1.什么是K-Means聚类？</h1><p id="12e2" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">K-Means聚类是一种无监督的学习算法，它可以帮助我们将数据中相似的数据点聚集成簇。这些聚类表示数据点共享的特征，这些特征标记了它们的相似性。</p><p id="7d64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简单来说，K-Means几乎就像<a class="ae jx" rel="noopener" href="/@priyanshsoni761/k-nearest-neighbors-knn-1606989b7ee0?source=user_profile---------3----------------------------"> KNN </a>，我们查看K-最近点的相似性。在K-Means聚类中，我们形成K个聚类，使得被分组在一个聚类中的点是相似的并且共享共同的特征。下图将使这一点更加清楚:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/c2518ccc51ea5aa3fc8d2b299b73e4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*_CwYQtRAzlN2CcxHtFuyFQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Ideal Clustering with 5 clusters</figcaption></figure><p id="eb0f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">K-Means通过将我们的数据作为输入参数来帮助我们获得如上图所示的图形。因为我们只是根据我们所拥有的数据点来形成聚类，所以我们不需要这些点的y标签。因此，聚类是一种无监督的学习算法，其中我们不需要数据点的标签。</p><p id="2e9f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn">聚类的应用— </em> </strong></p><p id="7e0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">1.客户细分<br/> 2。文档分割<br/> 3。图像分割<br/> 4。推荐系统等。</p><h1 id="a46d" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated">2.集群的属性</h1><p id="3a31" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">现在，这些集群有一些属性，遵循这些属性，它们变得有意义。</p><ol class=""><li id="0207" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi">一个聚类内的所有数据点应尽可能相似— </strong>一个聚类内的数据点彼此非常接近。这意味着在特征空间中，数据点代表相似的特征。此外，相似的点会更好地聚集在一起。因此，一个聚类中的点应该尽可能地相似，这样该聚类才有意义。</li><li id="5acd" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><strong class="ir hi">来自不同聚类的数据点应该尽可能的不同— </strong>来自不同聚类的数据点彼此相距较远。这意味着在特征空间中，这些点代表非常不同的特征。此外，不同的点永远不会形成集群。因此，来自不同聚类的点应该尽可能不同，以使聚类有意义。</li><li id="9ac2" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><strong class="ir hi">每个聚类都有一个质心— </strong>我们形成的每个聚类都有一个质心，所有的点都围绕着这个质心关联。这个质心是帮助我们形成聚类的点，并根据聚类内部点的平均值进行调整。</li></ol><p id="801d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">~好吧，不要为属性费心了，这只是蹩脚的理论。好玩的部分是下一个</em></p><h1 id="892f" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated">3.聚类算法</h1><p id="a64b" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">这就是我们深入研究这种算法如何生成漂亮的聚类的地方。</p><p id="47bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn"> Algo — </em> </strong></p><ul class=""><li id="c14f" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated"><strong class="ir hi">【步骤1】—选择我们想要的聚类数(K)——</strong>现在可以任意选择，因为我们将在以后决定如何选择K值。通常选择K=3的随机值来启动算法</li><li id="f332" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated"><strong class="ir hi">【步骤2】—选择K个随机点作为K个质心— </strong>这些点也是随机选择的，以形成每个聚类的质心。这些点可以从我们的数据或其他地方随机选择。</li><li id="310b" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated"><strong class="ir hi">【步骤3】—将每个数据点分配到最近的质心— </strong>然后我们计算数据集中每个点到所有K质心的距离。该指针被分配给与该质心距离最小的质心。这可以如下图所示:</li></ul><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/cf3d93da60aae98acd07a9f32522fde3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*kqtrqthBS5lxYxqaEB6AiA.png"/></div></div></figure><p id="8282" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对每个点重复这一过程，最终，每个点都被分配给某个或其他质心。这样，我们就得到K-簇。<br/>我们通常使用<a class="ae jx" rel="noopener" href="/p/1606989b7ee0#5191">欧几里德距离</a>来计算点和质心之间的距离。</p><ul class=""><li id="db09" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated"><strong class="ir hi">【步骤4】—计算新的质心— </strong>将每个点分配给一些其他质心后，我们通过取每个聚类中所有点的平均值来计算每个聚类的新质心。可以通过分别对所有数据点的x坐标和y坐标取平均值来计算新的质心。</li></ul><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es md"><img src="../Images/60e421ce001429e732386651bca0015d.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*DJGO2eKHkNJpj5ABZDgE6Q.png"/></div></figure><p id="24cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里的<strong class="ir hi"><em class="jn">‘m’</em></strong>是特定聚类中数据点的数量。</p><ul class=""><li id="0dbc" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated"><strong class="ir hi">【步骤5】—</strong>重复步骤3和4，直到达到收敛/停止条件</li></ul><h2 id="62ef" class="me kl hh bd km mf mg mh kq mi mj mk ku ja ml mm ky je mn mo lc ji mp mq lg mr bi translated">让我们想象一下</h2><p id="8157" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">1.<strong class="ir hi">【第一步】— </strong>假设我们选择k = 2 <br/> 2。<strong class="ir hi">【第二步】— </strong></p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/1f17f4588978c5a3113db927a151a853.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*fVrh6Nod9pVHUd7IcufYYA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Choosing 2 random centroids</figcaption></figure><p id="a353" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.<strong class="ir hi">【第三步】——</strong></p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/8e9e2e016f23085d3f4f8e9930739b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*vaCrXSm8oGzu1yMJqVouxA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Assigning points to closest centroid</figcaption></figure><p id="8e49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.<strong class="ir hi">【第四步】——</strong></p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/d270d0844841337f82d324e0cd20f805.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*RvFTVwENXDsk5-y4BNuqew.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Computing new centroids</figcaption></figure><p id="0a60" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">5.<strong class="ir hi">【第五步】——</strong></p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/4939f37aa791b06a96b87be527fc4a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*wnPEgQljkKs9nNxaobrNOA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Computing new centroids</figcaption></figure><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/40bdc37c9b091ded1201d3971f38109f.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*JS0oh8Z9O4E-n7j5JIOVzQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Convergence — perfect clusters</figcaption></figure><h1 id="091b" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated">4.收敛/停止条件</h1><p id="fc33" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">上述算法如果不收敛，将会无限重复。因此，我们需要上述算法的一些停止标准。<br/>有一些停止标准，遇到这些标准时算法应该停止。这些是:</p><ol class=""><li id="7808" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">分配到特定聚类中的数据点保持不变，我们运行算法，直到数据点没有被分配到任何新的聚类。<strong class="ir hi">这一过程非常缓慢，可能需要很长时间。</strong></li><li id="e337" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">质心保持不变——我们运行算法，直到计算出的新质心与先前的质心相同。<strong class="ir hi">这一过程非常缓慢，可能需要很多时间</strong></li><li id="327c" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">数据点到质心的距离是最小的，我们为数据点到质心的距离设置了一个阈值。当达到这个阈值时，算法将停止。<strong class="ir hi">这很快，但我们必须非常小心地选择距离阈值。</strong></li><li id="6cce" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">达到了固定的迭代次数—我们为迭代次数设置了一个阈值，当达到这个阈值时我们就停止。<strong class="ir hi">这很快，但是不明智的阈值设置会导致糟糕的集群形成。</strong></li></ol><p id="d9c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以在我们的算法中实现这些停止条件，以实现早期收敛和适当的聚类。</p><h1 id="b055" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated">5.质心初始化</h1><p id="7264" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">在K-Means中，我们随机初始化质心。这可能会有一些问题，并可能导致不良的集群形成。</p><ul class=""><li id="5a53" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated">如果质心是一个远点(离群点)，那么可能没有数据点被分配给这个质心。这可能会导致多个群集被指定给同一个质心。</li><li id="a656" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated">如果两个或多个质心被初始化为彼此非常接近，这可能会导致多个质心被指定给同一个簇。</li></ul><p id="a827" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这两个问题都可以在下图中看到:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/0914429cde20919e3c731b0e8e1fcc17.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*aMlajwGw3JKB2jRwIJjqrg.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Poor Clustering</figcaption></figure><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/20911cef2a03fb579024bd78d855baed.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*ANAWe3dOLLW4WyJ_gWN16Q.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Ideal Clustering</figcaption></figure><p id="26df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们需要一种更好的质心初始化方法。我们可以使用上述两种方法中的一种:</p><ol class=""><li id="f7b8" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi">重复K-Means多次，直到我们得到最佳聚类</strong></li><li id="67a9" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><strong class="ir hi">使用K-Means ++算法</strong></li></ol><p id="f44e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于显而易见的原因，前者在时间复杂度方面效率非常低。因此，我们使用K-Means++算法进行质心初始化。</p><h2 id="ad65" class="me kl hh bd km mf mg mh kq mi mj mk ku ja ml mm ky je mn mo lc ji mp mq lg mr bi translated">k-表示++的意思</h2><p id="6c4a" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">这只是一个初始化质心的算法，其余的过程与K-Means算法相同。</p><p id="baee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn">算法— </em> </strong></p><ol class=""><li id="33b6" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">随机选择第一个质心</li><li id="a8ef" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">现在计算每个点到最近质心的距离(算法开始时的第一个质心)</li><li id="c38c" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">现在给每个点分配概率值。概率值与该点到先前质心的距离成比例。这意味着离质心距离最大的点将具有被选为下一个质心的最高概率值。</li><li id="53c4" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated">重复这些步骤，直到我们有K个质心。</li></ol><p id="d855" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过这样的初始化，确保了每个质心尽可能远离另一个质心。因此，不超过一个质心可以被分配给同一聚类，并且不超过一个聚类被分配给同一质心。</p><p id="7140" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在此之后，恢复K-Means算法，然后将数据点分配给最近的质心，依此类推。</p><h1 id="c089" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated">6.选择最佳K</h1><p id="9a19" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">选择一个最佳的K值是非常重要的，因为这可能会导致结构优美的集群以及无组织的哑集群。</p><p id="0b8c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">选择K值的方法有很多，但最佳方法是通过一种叫做<strong class="ir hi">肘法的方法。</strong>我们的目标是找到K的值，对于该值，类内平方和误差(<strong class="ir hi"> WCSSE </strong>)最小。</p><p id="c912" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">WCSSE表示相同聚类内的数据点到质心的平方距离的总和。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es my"><img src="../Images/41a0ff0da9cf5818deeaef061e2f8731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*5T-pDBCL3PvtH1rs-EOI7g.png"/></div></figure><h2 id="12b1" class="me kl hh bd km mf mg mh kq mi mj mk ku ja ml mm ky je mn mo lc ji mp mq lg mr bi translated">肘法</h2><p id="aa3c" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated"><strong class="ir hi">肘方法</strong>，是一种我们可以通过查看每个K给出的结果来选择K值的技术</p><p id="b4d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算所有聚类的WCSSE，并绘制不同的k值。图中WCSSE急剧下降的点就是我们要寻找的点。这一点告诉我们，特定的K值，WCSSE急剧下降，几乎成为常数。因此，在这一点之后，增加K值不会使WCSSE降低太多，因此这一点是最佳K值。</p><p id="15cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">弯头方法的步骤如下:</p><ul class=""><li id="a2d8" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated">我们在一个范围内选择一个K(比如从1到10)</li><li id="e13c" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated">对于这个范围中的每个K值，我们找到所有聚类的WCSSE。</li><li id="e4bf" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated">然后我们绘制WCSSE对K，其中K在X轴上。</li><li id="6649" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated">WCSSE值急剧下降并形成肘状形状的K是我们选择的最佳K值。</li></ul><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/e3c0d813ff5ae9747ae3ab49ad75dde4.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*8BEx-AulMgXePwOeF_gSYw.png"/></div></figure><p id="9227" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从上图中我们可以看到，对于K值为5的情况，WCSSE急剧下降，之后几乎保持不变。因此，在5个集群之外，WCSSE下降不多。因此我们选择K= 5。</p><h1 id="497d" class="kk kl hh bd km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld ma lf lg lh bi translated"><strong class="ak"> 7。评估集群质量</strong></h1><p id="d5c1" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">为了使聚类有意义，我们必须评估每个聚类的质量。我所说的质量是指我们的聚类如何很好地解释我们的数据。为此，我们必须回到我们的<a class="ae jx" rel="noopener" href="/p/b58fa59334e5#a46d">集群属性</a>。符合所有属性的群集被认为是好群集。那么我们如何用数学方法来评估星团的性质呢？</p><p id="ff95" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><br/>有两种方式:<em class="jn"> 1。惯性<br/> 2。邓恩指数</em></p><h2 id="261c" class="me kl hh bd km mf mg mh kq mi mj mk ku ja ml mm ky je mn mo lc ji mp mq lg mr bi translated">1.惯性</h2><p id="e26b" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">惯性是指特定聚类的点和聚类质心之间的距离的总和。这也可以被认为是<strong class="ir hi">组内</strong>距离，因为我们正在计算组内点之间的距离。<br/>对于群集质心C1，我们可以将惯性定义为:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/dd82e9e97b75b94f115046acaf6c4f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*dzn_he6U9xc1lAUlGkNe9A.png"/></div></figure><p id="17a4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中<strong class="ir hi"> <em class="jn"> i </em> </strong>的范围从1到m(该聚类中的点数)</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/a2baf6e940bfb782c05e3fae6d716774.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*ApKWfgqAjHwRbGIll6rVZQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx">Intra-cluster distance</figcaption></figure><p id="189c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上图描绘了星团间的距离或<em class="jn">惯性。</em></p><p id="1d30" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于具有重要性的聚类，其点和质心之间的距离应该尽可能小。因此，该簇可以满足第一个性质。</p><blockquote class="nc nd ne"><p id="ae1a" class="ip iq jn ir b is it iu iv iw ix iy iz nf jb jc jd ng jf jg jh nh jj jk jl jm ha bi translated"><strong class="ir hi">因此，对于一个好的集群，惯性值应该尽可能的小。</strong></p></blockquote><h2 id="b15f" class="me kl hh bd km mf mg mh kq mi mj mk ku ja ml mm ky je mn mo lc ji mp mq lg mr bi translated">2.邓恩指数</h2><p id="3239" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">邓恩指数是衡量集群的第二个属性。它测量聚类之间的距离，这表明两个聚类之间的属性差异。Dunn Index通过计算簇间和簇内距离来实现。</p><p id="c10b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">簇间</strong>距离是指两个簇之间的距离。这种差异也取决于我们如何衡量它。</p><blockquote class="nc nd ne"><p id="b25d" class="ip iq jn ir b is it iu iv iw ix iy iz nf jb jc jd ng jf jg jh nh jj jk jl jm ha bi translated">—它可以是两个c的质心之差<br/> —它可以是离两个聚类最远的点之差<br/> —它可以是离两个聚类最近的点之差。</p></blockquote><p id="d5b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">独立于选择用于测量聚类间距离的标准，邓恩指数可以表示为:</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/7ba971db456dc9b7dce245458cd0932f.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*k6olsRyzphKRH-NjPRGM4A.png"/></div></div></figure><p id="9ba8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了使两个集群尽可能地不同，分子应该很大，分母应该很小。因此，</p><ul class=""><li id="f201" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm mb ju jv jw bi translated">任意两个聚类<strong class="ir hi"><em class="jn">【min(聚类间距离)】</em> </strong>之间的<strong class="ir hi">最小</strong>距离应该非常大，以便分子是大值。</li><li id="8711" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm mb ju jv jw bi translated">点与聚类的质心之间的最大距离<strong class="ir hi"/><strong class="ir hi"><em class="jn">【最大(聚类内距离)】</em> </strong>应该是一个非常小的数，以便分母是一个小值。</li></ul><p id="7a6d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了使聚类具有显著性，聚类间的距离必须尽可能大。因此该簇可以满足第二个性质。</p><blockquote class="nc nd ne"><p id="7e70" class="ip iq jn ir b is it iu iv iw ix iy iz nf jb jc jd ng jf jg jh nh jj jk jl jm ha bi translated">因此，对于一个好的聚类，邓恩指数必须尽可能大。</p></blockquote></div><div class="ab cl kd ke go kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ha hb hc hd he"><p id="a0be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这几乎是K-均值聚类的全部内容。代码也很微妙，不需要太多的努力就能理解。</p><blockquote class="nj"><p id="a99c" class="nk nl hh bd nm nn no np nq nr ns jm dx translated">最后，我想说的是，孔雀在看着你，所以你最好好好读书，嗯！</p><p id="4b96" class="nk nl hh bd nm nn no np nq nr ns jm dx">:)</p></blockquote><p id="58a1" class="pw-post-body-paragraph ip iq hh ir b is nt iu iv iw nu iy iz ja nv jc jd je nw jg jh ji nx jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn">更简单的算法是这样的— </em> </strong></p><ol class=""><li id="840b" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@priyanshsoni761/k-nearest-neighbors-knn-1606989b7ee0?source=user_profile---------3----------------------------"> KNN </a></li><li id="7b7d" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@priyanshsoni761/one-stop-for-logistic-regression-ee3d0d96b48f?source=user_profile---------0----------------------------">逻辑回归</a></li><li id="570e" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@priyanshsoni761/a-one-stop-for-support-vector-machine-2b9f26b3f247?source=user_profile---------2----------------------------">支持向量机</a></li><li id="717c" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@priyanshsoni761/one-stop-for-naive-bayes-70464fa7605c?source=user_profile---------6----------------------------">朴素贝叶斯</a></li><li id="1977" class="jo jp hh ir b is jy iw jz ja ka je kb ji kc jm jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@priyanshsoni761/classification-evaluation-metrics-7c3fe3b0119b?source=user_profile---------1----------------------------">评估指标—分类</a></li></ol><div class="ny nz ez fb oa ob"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">medium.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op in ob"/></div></div></a></div></div></div>    
</body>
</html>