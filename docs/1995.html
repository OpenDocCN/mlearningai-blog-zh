<html>
<head>
<title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一幅图像相当于16x16个字:大规模图像识别的变形金刚</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-51f3561a9f96?source=collection_archive---------0-----------------------#2022-02-20">https://medium.com/mlearning-ai/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-51f3561a9f96?source=collection_archive---------0-----------------------#2022-02-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d8c44e78df1a991584d9700163941fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fa65_7TmyNs4xLaW_fhRbA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">ViT architecture presented in the paper</figcaption></figure><p id="080a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这是谷歌研究的一篇论文。本文的主要观点是</p><blockquote class="jr js jt"><p id="6aab" class="it iu ju iv b iw ix iy iz ja jb jc jd jv jf jg jh jw jj jk jl jx jn jo jp jq ha bi translated">直接应用于图像补丁并在大型数据集上进行预训练的转换器在图像分类方面非常有效。</p></blockquote><p id="4be9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在本帖中，我们将详细讨论视觉变压器(ViT)架构以及论文中发表的结果。</p><p id="03bd" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">通过使用<a class="ae jy" href="https://github.com/souvik3333/medium_blogs/blob/main/transformers/ViT/ViT.ipynb" rel="noopener ugc nofollow" target="_blank">这台</a>笔记本来尝试这里使用的实现。点击<code class="du jz ka kb kc b">Open in Colab</code>直接在Colab上运行。</p><h1 id="d88d" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">ViT架构🏃</h1><ul class=""><li id="489a" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">ViT将图像分割成固定数量的面片，并使用它们来创建嵌入，并将它们通过标准的transformer编码器。</li><li id="1496" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">我们先讨论一下NLP变压器是如何工作的，然后再和ViT进行比较和讨论。</li></ul><h2 id="a705" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">NLP变压器编码器🎨</h2><figure class="mg mh mi mj fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/a02f7450904b6aa697f46becf091f549.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*q3Z092SdqVdBdZE--Xa2OQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Transformer encoder</figcaption></figure><ul class=""><li id="b6fc" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">我们把一个句子作为输入。但是，我们没有发送整个句子，而是使用一个<code class="du jz ka kb kc b">tokenizer</code>给每个单词一个<code class="du jz ka kb kc b">id</code>。现在，分词器实际上不必像按单词拆分那样简单地进行拆分，而是可以将单词拆分成多个部分并进行赋值。这个要看分词器怎么训练了。例如，一个简单的记号赋予器可以做如下工作</li></ul><figure class="mg mh mi mj fd ii"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="6324" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但是另一个人把句子拆成了下面这样:</p><figure class="mg mh mi mj fd ii"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="b088" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在我们有了一个将所有可能的id映射到向量表示的矩阵。因此，如果我们使用第二个记号赋予器，如果我们想要为<code class="du jz ka kb kc b">To</code>选择嵌入，我们将选择索引11处的向量表示。</p><figure class="mg mh mi mj fd ii"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="a3c8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这些嵌入可以在开始时随机初始化，我们在训练中学习。</p><h2 id="f4d7" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">位置嵌入</h2><ul class=""><li id="1c10" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">递归神经网络(RNNs)以连续的方式逐字分析句子。但是变压器架构不使用递归机制，而是支持多头自关注机制。这减少了变形金刚的训练时间，但是模型不知道单词的位置。</li><li id="092d" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">为了解决这个问题，我们向输入嵌入添加了一条额外的信息(位置编码)。</li><li id="14da" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">现在一个简单的方法就是给第一个单词赋值1，给第二个单词赋值2，依此类推。但在这种方法中，模型在推理过程中可能会得到一个比它在训练中看到的任何句子都长的句子。同样，对于一个较长的句子，会有较大的值需要添加，这会占用更多的内存。</li><li id="0d65" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">我们可以取一个范围，比如第一个作品加0，最后一个加1，在两者之间我们分割范围[0，1]并得到值。例如，对于一个3个单词的句子，我们可以对第一个单词取0，对第二个单词取0.5，对第三个单词取1；对于一个4个单词的句子，它将分别是0，0.33，0.66，1。这样的问题是位置差δ不是常数。在第一个例子中，它是0.5，但是在第二个例子中，它是0.33。</li><li id="c13f" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">使用的位置编码是一个d维向量。</li></ul><figure class="mg mh mi mj fd ii"><div class="bz dy l di"><div class="mn mo l"/></div></figure><ul class=""><li id="0b0a" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">这就是所有事物的结合</li></ul><figure class="mg mh mi mj fd ii"><div class="bz dy l di"><div class="mn mo l"/></div></figure><ul class=""><li id="92c7" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">接下来，我们将通过多头注意力模块传递这些向量。</li></ul><h2 id="db74" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">多头注意力🔥</h2><ul class=""><li id="e5a7" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">多头注意力有三个矩阵，分别是查询(Q)、键(K)、值(V)矩阵。它们中的每一个都具有与嵌入相同的维数。因此，在我们的例子中，所有3个矩阵都是512x512</li><li id="3320" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">对于每个令牌嵌入，我们将其与所有三个矩阵(Q，K，V)相乘。因此，对于每个令牌，我们将有3个长度为512的中间向量。</li><li id="5684" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">现在，如果我们有<code class="du jz ka kb kc b">n </code>个头，我们把每个向量分成<code class="du jz ka kb kc b">n</code>个部分。例如，如果我们有8个头，对于单词<code class="du jz ka kb kc b">Today</code>，我们将把所有3个中间向量分成维数为64的小向量。</li><li id="97ac" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">然后，每个头从所有中间向量中取出其对应的段。例如，第一个头将取得所有五个嵌入(对应于五个令牌)的所有三个中间向量(对应于查询、密钥、值乘法结果)的第一个分裂(维度64)。类似地，第二个头将取第二个段，依此类推。</li><li id="03a5" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">在每个标题中，我们点积查询和键矩阵相乘的向量。在下图中，我们在q1和所有关键矩阵相乘的向量(k{i}，i in [1，5])之间做点积。然后，我们将它乘以相应的值向量。最后，我们将它们相加以创建一个结果64维向量。这发生在q2、q3、q4、q5，最后，我们得到维数为64的5个向量。现在基本上每个结果向量都有所有其他向量的信息。</li></ul><figure class="mg mh mi mj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mp"><img src="../Images/43f19d1b3b883ba9e605843162c22e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4o4wPyvUIWU8Vkt-W4-Tyg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Attention logic</figcaption></figure><ul class=""><li id="f4d1" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">现在我们连接所有头部的结果向量。因此，我们将连接所有8个头的第一个结果向量，以创建512 dim第一个向量。对于所有其他4个向量也是如此。</li><li id="f853" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">最后我们有5个向量，每个都有512个维度。</li></ul><h2 id="3ed0" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">添加&amp;诺姆☯</h2><ul class=""><li id="aaa0" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">这些是正常的批处理规范化和剩余连接，如Resnet块。</li></ul><h2 id="d742" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">前馈🍀</h2><ul class=""><li id="1ffa" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">这些是简单的前馈神经网络，应用于每个注意力向量。</li></ul><p id="7b65" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这就是一个简单的变压器编码器的工作原理。接下来让我们看看ViT架构。同时我们也会看到如何实现。</p><h2 id="a280" class="lr ke hh bd kf ls lt lu kj lv lw lx kn je ly lz kr ji ma mb kv jm mc md kz me bi translated">ViT编码器架构</h2><p id="a908" class="pw-post-body-paragraph it iu hh iv b iw ld iy iz ja le jc jd je mq jg jh ji mr jk jl jm ms jo jp jq ha bi translated"><strong class="iv hi">嵌入补丁的☑️ </strong></p><ul class=""><li id="bf31" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">为了处理2D图像，图像被分成几个小块。我们将2D面片展平成1D矢量。</li><li id="d3a3" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">然后，我们将这些向量嵌入到模型维度空间中。在这种情况下，模型将每个向量转换为768维向量。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="dd32" class="lr ke hh kc b fi mx my l mz na">import torch<br/>import torch.nn as nn<br/>in_chans = 3 #RGB<br/>embed_dim = 768 # vector dimension in model space<br/>patch_size = 16 # each image patch size 16*16<br/>proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # this will create the patch in image<br/>img = torch.randn(1, 3, 224,224) # dummy image<br/>x = proj(img).flatten(2).transpose(1, 2) # BCHW -&gt; BNC<br/>print(x.shape)</span></pre><ul class=""><li id="d0d5" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">在上面的代码中，我们拍摄了大小为224*224的图像，并假设每个面片的大小为16x16。</li><li id="00a2" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">现在这将导致总共(224/16<em class="ju">* 224/16)= 14 *</em>14 = 196个向量。</li><li id="a31d" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">这些向量的大小都是16*16 = 256。但是，因为我们必须将其转换为模型维度，即768，所以我们在卷积中使用768作为输出通道。最后，我们将它展平为BNC，其中B=批次，N=生成的面片，C =模型空间中的向量维度。</li></ul><p id="1743" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">类嵌入🆕</strong></p><ul class=""><li id="0deb" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">ViT将一个可学习的嵌入附加到嵌入补丁序列中。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="fd26" class="lr ke hh kc b fi mx my l mz na">cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # create class embeddings without batch<br/>cls_token = cls_token.expand(x.shape[0], -1, -1) # add batch<br/>x = torch.cat((cls_token, x), dim=1) # append class token with linear proj embeddings<br/>x.shape # 196 -&gt; 197</span></pre><p id="46fe" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">位置嵌入☑️ </strong></p><ul class=""><li id="dd5e" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">我们创建一个维数为(num _ patches+1)<em class="ju">* embed _ dim(197 *</em>768)矩阵。这些值是在训练过程中学习到的。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="9f05" class="lr ke hh kc b fi mx my l mz na">num_patches = 14*14<br/>pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) # +1 for class token<br/>x = x + pos_embed # add position encoding<br/>x.shape</span></pre><p id="9e2d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">街区☑️ </strong></p><ul class=""><li id="8e6b" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">基于该模型，我们有<code class="du jz ka kb kc b">n</code>个块。</li><li id="94b4" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">每个区块都一样。每一层包括一个注意层和一个MLP层。</li></ul><p id="cf0b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">关注层☑️ </strong></p><ul class=""><li id="d4c2" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">与之前解释的NLP注意力相同。</li><li id="8191" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">让我们创建中间向量。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="6bbe" class="lr ke hh kc b fi mx my l mz na"># Transformation from source vector to query vector<br/>fc_q = nn.Linear(embed_dim, embed_dim)<br/># Transformation from source vector to key vector<br/>fc_k = nn.Linear(embed_dim, embed_dim)<br/># Transformation from source vector to value vector<br/>fc_v = nn.Linear(embed_dim, embed_dim)</span><span id="936b" class="lr ke hh kc b fi nb my l mz na">Q = fc_q(x)<br/>K = fc_k(x)<br/>V = fc_v(x)</span><span id="6632" class="lr ke hh kc b fi nb my l mz na">print(Q.shape, K.shape, V.shape)</span></pre><ul class=""><li id="743c" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">分割中间向量以在每个头中处理一个零件。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="dc4f" class="lr ke hh kc b fi mx my l mz na">num_heads = 8<br/>batch_size = 1<br/>Q = Q.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the Q matrix for 8 head<br/>K = K.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the K matrix for 8 head<br/>V = V.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the V matrix for 8 head<br/>print(Q.shape, K.shape, V.shape) # batch_size, num_head, num_patch+1, feature_vec dim per head</span></pre><ul class=""><li id="246f" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">注意力矩阵乘法。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="0ee2" class="lr ke hh kc b fi mx my l mz na">score = torch.matmul(Q, K.permute(0, 1, 3, 2)) # Q*k<br/>score = torch.softmax(score, dim=-1)<br/>score = torch.matmul(score, V) # normally we apply dropout layer before this<br/>score.shape # batch_size, num_head, num_patches+1, feature_vector_per_head (embed_dim/num_head)</span></pre><ul class=""><li id="eeb2" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">重塑结果</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="339f" class="lr ke hh kc b fi mx my l mz na">score = score.permute(0, 2, 1, 3).contiguous()<br/>score.shape # batch_size, num_patches+1, num_head, feature_vector_per_head (embed_dim/num_head)</span></pre><ul class=""><li id="c480" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">将向量合并回它们的原始形状</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="58fe" class="lr ke hh kc b fi mx my l mz na">score = score.view(batch_size, -1, embed_dim) # merge the vectors back to original shape<br/>score.shape # batch_size, num_patches+1, embed_dim</span></pre><p id="a3f7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> MLP负责人☑️ </strong></p><ul class=""><li id="688f" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">正常多层感知器。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="7247" class="lr ke hh kc b fi mx my l mz na">act_layer=nn.GELU # activation function<br/>in_features = embed_dim <br/>hidden_features = embed_dim * 4<br/>out_features = in_features<br/>fc1 = nn.Linear(in_features, hidden_features)<br/>act = act_layer()<br/>drop1 = nn.Dropout(0.5)<br/>fc2 = nn.Linear(hidden_features, out_features)<br/>drop2 = nn.Dropout(0.5)</span></pre><ul class=""><li id="ade8" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">从MLP图层中获取结果</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="760a" class="lr ke hh kc b fi mx my l mz na">x = fc1(score)<br/>x = act(x)<br/>x = drop1(x)<br/>x = fc2(x)<br/>x = drop2(x)<br/>x.shape</span></pre><ul class=""><li id="22a1" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">拿出<code class="du jz ka kb kc b">cls</code>令牌特性。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="81fd" class="lr ke hh kc b fi mx my l mz na">cls = x[:,0]</span></pre><p id="6ae7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi">选粉机机头🆕</strong></p><ul class=""><li id="55b4" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">创建一个简单的分类器头，并传递类令牌特征来获得预测。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="24af" class="lr ke hh kc b fi mx my l mz na">num_classes = 10 # assume 10 class classification<br/>head = nn.Linear(embed_dim, num_classes) <br/>pred = head(cls)<br/>pred</span></pre><h1 id="b837" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果发表在论文中📈</h1><blockquote class="jr js jt"><p id="4d58" class="it iu ju iv b iw ix iy iz ja jb jc jd jv jf jg jh jw jj jk jl jx jn jo jp jq ha bi translated">当在没有强正则化的中型数据集(如ImageNet)上训练时，这些模型产生的适度精度比可比大小的ResNets低几个百分点。</p><p id="8d93" class="it iu ju iv b iw ix iy iz ja jb jc jd jv jf jg jh jw jj jk jl jx jn jo jp jq ha bi translated">变压器缺乏CNN固有的一些归纳偏差，如翻译等变和局部性，因此在数据量不足的情况下训练时不能很好地概括。</p><p id="381a" class="it iu ju iv b iw ix iy iz ja jb jc jd jv jf jg jh jw jj jk jl jx jn jo jp jq ha bi translated">但是，如果模型是在更大的数据集(14M-300M图像)上训练的，情况就不同了。我们发现大规模训练胜过归纳偏差。</p></blockquote><ul class=""><li id="a3ff" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">作者提到，对于较小的预训练数据集(ImageNet ), ViT-Large模型的性能低于ViT-Base模型。对于大型数据集(JFT-300米)，ViT-大型模型效果很好。</li><li id="abad" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">在JFT-300M数据集上预训练的Vision Transformer模型在所有数据集上都优于基于ResNet的基线，而预训练所需的计算资源却少得多。</li><li id="e6d3" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated">下表显示了用JFT-300M数据集和ImageNet-21k数据集预处理的ViT的结果。这些列显示了用不同数据集预处理的几个模型。这些行是下游任务。</li></ul><figure class="mg mh mi mj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/2f153cd0efd8146a46558ae4b786e142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HaRA_RwNxD4i8uOAg-MfWg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Table 2 from the paper</figcaption></figure><h1 id="92fa" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">用PyTorch闪电和timm训练一个简单的ViT🎆</h1><ul class=""><li id="6b6b" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated">这里让我们使用<a class="ae jy" href="https://github.com/PyTorchLightning/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>和<a class="ae jy" href="https://github.com/rwightman/pytorch-image-models" rel="noopener ugc nofollow" target="_blank"> timm </a>训练一个简单的分类器。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="ef75" class="lr ke hh kc b fi mx my l mz na">import timm<br/>import torch<br/>import pytorch_lightning as pl<br/>import torchvision<br/>import torchvision.transforms as transforms<br/>from pytorch_lightning import Trainer, seed_everything<br/>from pytorch_lightning.callbacks import ModelCheckpoint<br/>import torchmetrics</span><span id="4c84" class="lr ke hh kc b fi nb my l mz na">seed_everything(42, workers=True)</span></pre><ul class=""><li id="0845" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">让我们创建一个简单的lightning模型类。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="cbf4" class="lr ke hh kc b fi mx my l mz na">class Model(pl.LightningModule):<br/>    """<br/>    Lightning model<br/>    """<br/>    def __init__(self, model_name, num_classes, lr = 0.001, max_iter=20):<br/>        super().__init__()<br/>        self.model = timm.create_model(model_name=model_name, pretrained=True, num_classes=num_classes)<br/>        self.metric = torchmetrics.Accuracy()<br/>        self.loss = torch.nn.CrossEntropyLoss()<br/>        self.lr = lr<br/>        self.max_iter = max_iter<br/>        <br/>    def forward(self, x):<br/>        return self.model(x)</span><span id="e1a3" class="lr ke hh kc b fi nb my l mz na">    def shared_step(self, batch, batch_idx):<br/>        x, y = batch<br/>        logits = self(x)<br/>        loss = self.loss(logits, y)<br/>        preds = torch.argmax(logits, dim=1)<br/>        self.metric(preds, y)<br/>        <br/>        return loss<br/>    <br/>    def training_step(self, batch, batch_idx):<br/>        loss = self.shared_step(batch, batch_idx)<br/>        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('train_acc', self.metric, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def validation_step(self, batch, batch_idx):<br/>        loss = self.shared_step(batch, batch_idx)<br/>        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('val_acc', self.metric, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def configure_optimizers(self):<br/>        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)<br/>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)<br/>        <br/>        return [optim], [scheduler]</span></pre><ul class=""><li id="6166" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">接下来，我们将定义转换并下载和加载CIFAR10数据集。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="4c41" class="lr ke hh kc b fi mx my l mz na">transform = transforms.Compose(<br/>    [transforms.Resize(224),<br/>     transforms.ToTensor(),<br/>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><span id="88bc" class="lr ke hh kc b fi nb my l mz na">batch_size = 128</span><span id="345f" class="lr ke hh kc b fi nb my l mz na">trainset = torchvision.datasets.CIFAR10(root='./data', train=True,<br/>                                        download=True, transform=transform)<br/>trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,<br/>                                          shuffle=True, num_workers=8)</span><span id="2dd9" class="lr ke hh kc b fi nb my l mz na">testset = torchvision.datasets.CIFAR10(root='./data', train=False,<br/>                                       download=True, transform=transform)<br/>testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,<br/>                                         shuffle=False, num_workers=8)</span><span id="53d0" class="lr ke hh kc b fi nb my l mz na">classes = ('plane', 'car', 'bird', 'cat',<br/>           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')</span></pre><ul class=""><li id="2293" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">现在我们将初始化<code class="du jz ka kb kc b">Model</code>类。这里，我们使用ViT的一个变体，它拍摄大小为224*224的图像，面片大小为16。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="dbbd" class="lr ke hh kc b fi mx my l mz na">model = Model(model_name="vit_tiny_patch16_224", num_classes=len(classes), lr = 0.001, max_iter=10)</span></pre><ul class=""><li id="24da" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">让我们创建一个检查点回调来保存最佳检查点。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="4d57" class="lr ke hh kc b fi mx my l mz na">checkpoint_callback = ModelCheckpoint(<br/>    monitor='val_loss',<br/>    dirpath='./checkpoints',<br/>    filename='vit_tpytorch_lightning6_224-cifar10-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'<br/>)</span></pre><ul class=""><li id="d8a4" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">快好了。让我们创建教练。</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="abb5" class="lr ke hh kc b fi mx my l mz na">trainer = Trainer(<br/>    deterministic=True, <br/>    logger=False, <br/>    callbacks=[checkpoint_callback], <br/>    gpus=[0], # change it based on gpu or cpu availability<br/>    max_epochs=10, <br/>    stochastic_weight_avg=True)</span></pre><ul class=""><li id="8db1" class="lb lc hh iv b iw ix ja jb je mk ji ml jm mm jq li lj lk ll bi translated">最后，让我们训练模型😃</li></ul><pre class="mg mh mi mj fd mt kc mu mv aw mw bi"><span id="9492" class="lr ke hh kc b fi mx my l mz na">trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=testloader)</span></pre><h1 id="5aad" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">相关资源</h1><ul class=""><li id="9f73" class="lb lc hh iv b iw ld ja le je lf ji lg jm lh jq li lj lk ll bi translated"><a class="ae jy" href="https://github.com/google-research/vision_transformer" rel="noopener ugc nofollow" target="_blank">官方代号</a></li><li id="0193" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated"><a class="ae jy" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">论文</a></li><li id="e41e" class="lb lc hh iv b iw lm ja ln je lo ji lp jm lq jq li lj lk ll bi translated"><a class="ae jy" href="https://github.com/souvik3333/medium_blogs/blob/main/transformers/ViT/ViT.ipynb" rel="noopener ugc nofollow" target="_blank">代码笔记本</a></li></ul><p id="6896" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">特别感谢<a class="nd ne ge" href="https://medium.com/u/bea753eb5d92?source=post_page-----51f3561a9f96--------------------------------" rel="noopener" target="_blank"> Prakash Jay </a>指导我完成这个项目。</p><div class="nf ng ez fb nh ni"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nj ab dw"><div class="nk ab nl cl cj nm"><h2 class="bd hi fi z dy nn ea eb no ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="np l"><h3 class="bd b fi z dy nn ea eb no ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nq l"><p class="bd b fp z dy nn ea eb no ed ef dx translated">medium.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw in ni"/></div></div></a></div></div></div>    
</body>
</html>