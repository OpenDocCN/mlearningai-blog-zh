<html>
<head>
<title>Fashion Item Classification and Recommendation System with NLP Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ—¶å°šå•†å“åˆ†ç±»å’Œæ¨èç³»ç»Ÿ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/fashion-item-classification-and-recommendation-system-with-nlp-techniques-c1cfd4eecc98?source=collection_archive---------0-----------------------#2021-02-06">https://medium.com/mlearning-ai/fashion-item-classification-and-recommendation-system-with-nlp-techniques-c1cfd4eecc98?source=collection_archive---------0-----------------------#2021-02-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/5b96b60516bd09fda25eb9ef34472e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5mNpFaDGIeYtkCYucRu5cA.jpeg"/></div></div></figure><p id="61b1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»å¤šä¸ªåœ¨çº¿é›¶å”®ç½‘ç«™æå–çš„æ•°æ®ï¼Œé€šè¿‡åˆ©ç”¨<strong class="ir hi">è‡ªç„¶è¯­è¨€å¤„ç†(NLP) </strong>æŠ€æœ¯æ¥æ„å»ºç‰©å“å±æ€§å·¥å…·å’Œè£…å¤‡æ¨èç³»ç»Ÿã€‚</p><p id="bae7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">æ•´ä¸ªé¡¹ç›®å°†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†:</p><ol class=""><li id="9dfe" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">äº§å“å±æ€§é¢„æµ‹</li><li id="37c2" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">æœè£…æ¨è</li></ol><p id="063b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">å®Œæ•´ä»£ç è¯·å‚è€ƒ<a class="ae kb" href="https://github.com/rachelsung/ThreadTogether-Fashion-Item-Classification-and-Recommendation-System" rel="noopener ugc nofollow" target="_blank">https://github . com/Rachel sung/thread together-Fashion-Item-class ification-and-Recommendation-System</a></p><h1 id="31d1" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">ç¬¬1éƒ¨åˆ†:äº§å“å±æ€§é¢„æµ‹</h1><h2 id="3408" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated"><strong class="ak"> A .æ•°æ®é¢„å¤„ç†</strong></h2><p id="4ffd" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">åŸå§‹æ•°æ®é›†åŒ…å«äº§å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ç‰Œã€äº§å“åç§°ã€æè¿°ã€å“ç‰Œç±»åˆ«å’Œäº§å“è¯¦ç»†ä¿¡æ¯ã€‚</p><p id="8dbf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¤„ç†åŸå§‹æ•°æ®é›†çš„æ­¥éª¤:</p><p id="60ec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">a.å°†æ‰€æœ‰åˆ—ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å¤„ç†ã€‚</p><p id="11fc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">b.åˆ é™¤åœç”¨è¯:</p><ul class=""><li id="8ce0" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lt jt ju jv bi translated">æ ‡è®°æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„å­—ç¬¦ä¸²å¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨</li><li id="f24c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lt jt ju jv bi translated">åˆ é™¤åœç”¨è¯</li><li id="62e1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lt jt ju jv bi translated">åˆ é™¤â€œæœªçŸ¥â€ï¼Œå› ä¸ºåœ¨åŸå§‹æ•°æ®é›†ä¸­ï¼Œæœ‰äº›äº§å“ä¿¡æ¯æ˜¯æœªçŸ¥çš„ã€‚</li></ul><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="f0ec" class="la kd hh lz b fi md me l mf mg"><strong class="lz hi">def</strong> remove_stopwords(input_data):<br/>    <em class="mh">'''Tokenize string.</em><br/><em class="mh">        Returns a list.'''</em><br/><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">pandas</strong> <strong class="lz hi">as</strong> <strong class="lz hi">pd</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">numpy</strong> <strong class="lz hi">as</strong> <strong class="lz hi">np</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">os</strong><br/>    <strong class="lz hi">from</strong> <strong class="lz hi">collections</strong> <strong class="lz hi">import</strong> Counter<br/>    <strong class="lz hi">import</strong> <strong class="lz hi">nltk</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">warnings</strong><br/>    warnings.filterwarnings("ignore")<br/>    <br/>    <em class="mh"># combine all columns - call combin_col() function</em><br/>    data2 = combine_col(input_data)<br/><br/>    <em class="mh"># remove_stopwords</em><br/>    <strong class="lz hi">from</strong> <strong class="lz hi">nltk.corpus</strong> <strong class="lz hi">import</strong> stopwords<br/>    <strong class="lz hi">import</strong> <strong class="lz hi">nltk</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">re</strong><br/><br/>    regex_word_tokenize = nltk.RegexpTokenizer(r"(\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)")<br/>    nltk_stopwords = list((set(stopwords.words('english'))))<br/><br/><br/>    nltk_stopwords.append('unknown')<br/><br/>    result2 = []<br/>    <strong class="lz hi">for</strong> line <strong class="lz hi">in</strong> data2['combined_data']:<br/>        filtered_words = []<br/>        <strong class="lz hi">if</strong> isinstance(line, str):<br/>            line = re.sub(r'\d+\+*[\- ]*[\-]*',' ',line)<br/>            <strong class="lz hi">for</strong> word <strong class="lz hi">in</strong> regex_word_tokenize.tokenize(line):<br/>                <strong class="lz hi">if</strong> word.isdigit() == <strong class="lz hi">False</strong>:<br/>                    <strong class="lz hi">if</strong> word.lower() <strong class="lz hi">not</strong> <strong class="lz hi">in</strong> nltk_stopwords:<br/>                        filtered_words.append(word.lower())<br/>            result2.append(" ".join(filtered_words))<br/>        <strong class="lz hi">else</strong>:<br/>            result2.append(np.nan)<br/>    data2['rm_sw'] = result2<br/>    <strong class="lz hi">return</strong> data2</span></pre><ul class=""><li id="ac23" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lt jt ju jv bi translated">è¯æ±‡è¯æ¡åŒ–</li></ul><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="5194" class="la kd hh lz b fi md me l mf mg"><strong class="lz hi">def</strong> lemmatize_word(input_data):<br/>    <strong class="lz hi">import</strong> <strong class="lz hi">pandas</strong> <strong class="lz hi">as</strong> <strong class="lz hi">pd</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">numpy</strong> <strong class="lz hi">as</strong> <strong class="lz hi">np</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">os</strong><br/>    <strong class="lz hi">from</strong> <strong class="lz hi">collections</strong> <strong class="lz hi">import</strong> Counter<br/>    <strong class="lz hi">import</strong> <strong class="lz hi">nltk</strong><br/>    <strong class="lz hi">import</strong> <strong class="lz hi">warnings</strong><br/>    warnings.filterwarnings("ignore")<br/>    <br/>    d1 = remove_stopwords(input_data)<br/><br/>    <strong class="lz hi">from</strong> <strong class="lz hi">nltk.stem</strong> <strong class="lz hi">import</strong> WordNetLemmatizer<br/>    <strong class="lz hi">from</strong> <strong class="lz hi">nltk.corpus</strong> <strong class="lz hi">import</strong> stopwords<br/>    nltk_stopwords = list((set(stopwords.words('english'))))<br/>    regex_word_tokenize = nltk.RegexpTokenizer(r"(\w+['-]?[a-zA-Z']*[a-z]|[0-9]+-*[0-9]*)")<br/>    lemmatizer = WordNetLemmatizer()<br/><br/>    result3 = []<br/>    <strong class="lz hi">for</strong> i <strong class="lz hi">in</strong> range(len(d1['rm_sw'])):<br/>        lemmatized = []<br/>        <strong class="lz hi">if</strong> isinstance(d1['rm_sw'].iloc[i],str):<br/>            <strong class="lz hi">for</strong> word <strong class="lz hi">in</strong> regex_word_tokenize.tokenize(d1['rm_sw'].iloc[i]):<br/>                lemmatized.append(lemmatizer.lemmatize(word))<br/>            result3.append(" ".join(lemmatized))<br/>        <strong class="lz hi">else</strong>:<br/>            result3.append(d1['rm_sw'].iloc[i])<br/>    d1['lemmatized'] = result3<br/>    d1['final'] = d1['lemmatized'] + " " + d1['brand']<br/>    d1['final_list'] = d1['final'].str.split()<br/>    <strong class="lz hi">return</strong> d1</span></pre><ul class=""><li id="86f8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lt jt ju jv bi translated">è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç å»é‡å¤:å¯¹äºæ‰€æœ‰å±æ€§åŒ–æ•°æ®(è®­ç»ƒæ•°æ®)ï¼Œæ¯ä¸ªå±æ€§éƒ½è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç å’Œå»é‡å¤ï¼Œè¿™æ˜¯é’ˆå¯¹è®­ç»ƒæ¨¡å‹çš„ã€‚</li></ul><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="aadd" class="la kd hh lz b fi md me l mf mg">LOL = [] <em class="mh">#LOL List of Lists, to store each of the 5 dataframes</em></span><span id="041f" class="la kd hh lz b fi mi me l mf mg"><strong class="lz hi">for</strong> cat <strong class="lz hi">in</strong> chosen_cats:<br/>    sub = fulldata[fulldata['attribute_name'] == cat] <em class="mh">#gather "sub" set of rows that contain a category (i.e. selecting all rows that have a 'style' in their attribute name)</em></span><span id="d321" class="la kd hh lz b fi mi me l mf mg">    x = pd.get_dummies(sub['attribute_value']) <em class="mh">#create one-hot encoding for a given subset of data </em></span><span id="4d35" class="la kd hh lz b fi mi me l mf mg">    merged = sub.merge(x,how='left',on=sub.index) <em class="mh">#merge the one-hot encoding with the category</em></span><span id="27a3" class="la kd hh lz b fi mi me l mf mg">    LOL.append(merged)</span><span id="c3bc" class="la kd hh lz b fi mi me l mf mg"><em class="mh">#gather the unique tags for each of the 5 attributes</em></span><span id="a78e" class="la kd hh lz b fi mi me l mf mg">styles = fulldata[fulldata['attribute_name'] == 'style']['attribute_value'].unique()</span><span id="8efd" class="la kd hh lz b fi mi me l mf mg">embels = fulldata[fulldata['attribute_name'] == 'embellishment']['attribute_value'].unique()</span><span id="7949" class="la kd hh lz b fi mi me l mf mg">occasi = fulldata[fulldata['attribute_name'] == 'occasion']['attribute_value'].unique()</span><span id="fab6" class="la kd hh lz b fi mi me l mf mg">catego = fulldata[fulldata['attribute_name'] == 'category']['attribute_value'].unique()</span><span id="094d" class="la kd hh lz b fi mi me l mf mg">drycle = fulldata[fulldata['attribute_name'] == 'dry_clean_only']['attribute_value'].unique()</span></pre><h2 id="bf21" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">B.å•è¯åµŒå…¥</h2><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="d9a7" class="la kd hh lz b fi md me l mf mg"><strong class="lz hi">from</strong> <strong class="lz hi">sklearn.feature_extraction.text</strong> <strong class="lz hi">import</strong> TfidfVectorizer<br/><br/>unique_prod_doc = output_data['final'].drop_duplicates(keep = 'first')<br/>corpus = list(unique_prod_doc.values)<br/>vectorizer = TfidfVectorizer()<br/>vectorizer.fit(corpus)<br/>training_tfidf_vectors = vectorizer.transform(list(output_data['final']))</span></pre><h2 id="4981" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">C.æ¨¡å‹ç»“æ„</h2><p id="b7ee" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">æˆ‘ä»¬æœ€åˆæ„å»ºäº†4ç§ä¸åŒçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬:</p><ul class=""><li id="3d30" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lt jt ju jv bi translated">é€’å½’ç¥ç»ç½‘ç»œ(RNN):ä½¿ç”¨æ‰‹å¥—åµŒå…¥çš„é€’å½’ç¥ç»ç½‘ç»œæ¨¡å‹å…·æœ‰85.3%çš„é¢„æµ‹å‡†ç¡®åº¦ã€‚</li><li id="62f7" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lt jt ju jv bi translated">ç‰©æµå›å½’:é¢„æµ‹å‡†ç¡®ç‡83.1%ã€‚</li><li id="a986" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lt jt ju jv bi translated">æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ:é¢„æµ‹å‡†ç¡®ç‡84.2%ã€‚</li><li id="31d5" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lt jt ju jv bi translated">ç‰©æµå’Œå†³ç­–æ ‘ç»„åˆ:é¢„æµ‹å‡†ç¡®ç‡ä¸º86.8%ã€‚</li></ul><p id="ac07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ä»¥ä¸‹æ˜¯ç‰©æµå’Œå†³ç­–æ ‘ç»„åˆçš„ä»£ç ï¼Œè¿™æ˜¯æœ€ç»ˆé€‰æ‹©çš„æ¨¡å‹ã€‚</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="db66" class="la kd hh lz b fi md me l mf mg"><em class="mh"># TFIDF</em><strong class="lz hi"><br/>from</strong> <strong class="lz hi">sklearn.feature_extraction.text</strong> <strong class="lz hi">import</strong> TfidfVectorizer<br/><br/>unique_prod_doc = output_data['final'].drop_duplicates(keep = 'first')<br/>corpus = list(unique_prod_doc.values)<br/>vectorizer = TfidfVectorizer()<br/>vectorizer.fit(corpus)<br/>training_tfidf_vectors = vectorizer.transform(list(output_data['final']))</span><span id="064d" class="la kd hh lz b fi mi me l mf mg"><em class="mh"># Build model</em> <br/><strong class="lz hi">from</strong> <strong class="lz hi">sklearn</strong> <strong class="lz hi">import</strong> preprocessing, linear_model <br/><strong class="lz hi">from</strong> <strong class="lz hi">sklearn.model_selection</strong> <strong class="lz hi">import</strong> train_test_split <br/><strong class="lz hi">from</strong> <strong class="lz hi">sklearn.linear_model</strong> <strong class="lz hi">import</strong> LogisticRegression <br/><strong class="lz hi">from</strong> <strong class="lz hi">sklearn.tree</strong> <strong class="lz hi">import</strong> DecisionTreeClassifier  <br/><strong class="lz hi">from</strong> <strong class="lz hi">sklearn.metrics</strong> <strong class="lz hi">import</strong> classification_report, confusion_matrix, roc_curve, roc_auc_score, auc, classification_report  </span><span id="67c1" class="la kd hh lz b fi mi me l mf mg">to_predict = lemmatize_word(data) </span><span id="82d5" class="la kd hh lz b fi mi me l mf mg">to_predict['vectorized_doc'] = list(vectorizer.transform(to_predict['final']).toarray()) </span><span id="6b29" class="la kd hh lz b fi mi me l mf mg">to_predict_df = to_predict</span></pre><h2 id="1b18" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">D.é¢„è¨€ï¼›é¢„æµ‹ï¼›é¢„å‘Š</h2><p id="8810" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹é¢„æµ‹æœªé¢„æµ‹æ•°æ®é›†çš„5ä¸ªå±æ€§(â€œ<em class="mh">å¹²æ´—</em>â€ã€â€œ<em class="mh">ç±»åˆ«</em>â€ã€â€œ<em class="mh">ä¿®é¥°</em>â€ã€â€œ<em class="mh">é£æ ¼</em>â€ã€â€œ<em class="mh">åœºåˆ</em>â€)ï¼Œå¹¶æä¾›excelæ–‡ä»¶ä½œä¸ºæˆ‘ä»¬çš„è¾“å‡ºã€‚</p><h1 id="73d0" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">ç¬¬2éƒ¨åˆ†:æœè£…æ¨è</h1><p id="de5e" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ¨èç³»ç»Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®æä¾›çš„è¾“å…¥(äº§å“idæˆ–äº§å“æè¿°)æä¾›æœè£…ç»„åˆã€‚è¾“å‡ºå°†æ˜¯ä¸è¾“å…¥ç›¸å…³çš„æ‰€æœ‰æ¨èæœè£…ã€‚</p><h2 id="8bc6" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">A.æ•°æ®é¢„å¤„ç†</h2><p id="2e6b" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">è¿™äº›æ–¹æ³•ä¸ç¬¬1éƒ¨åˆ†åŸºæœ¬ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†ä¸å†è®¨è®ºã€‚</p><h2 id="5bae" class="la kd hh bd ke lb lc ld ki le lf lg km ja lh li kq je lj lk ku ji ll lm ky ln bi translated">B.æ¨èæ¨¡å‹æ„å»º</h2><p id="7d6b" class="pw-post-body-paragraph ip iq hh ir b is lo iu iv iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm ha bi translated">æˆ‘ä»¬å‡è®¾æœ‰2ç§è¾“å…¥æ•°æ®ï¼Œä¸€ç§æ˜¯äº§å“idï¼Œå¦ä¸€ç§æ˜¯äº§å“æè¿°ã€‚å› æ­¤ï¼Œå¯¹äºæ¯ç§è¾“å…¥æ•°æ®ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥è·å¾—å»ºè®®ã€‚</p><p id="7778" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">a.è¾“å…¥:äº§å“Id</p><p id="1830" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">å¯¹äºè¿™ç§è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†<strong class="ir hi"> Fuzzy Wuzzy: </strong></p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="01f8" class="la kd hh lz b fi md me l mf mg">def recommend_id(test):<br/>    '''Searches user's inputted product id and returns the recommended outfit. '''<br/>    <br/>    # List of all product id<br/>    strOptions =list(set(df['product_id'].to_list()))<br/>    <br/>    # Str2match = user input<br/>    str2Match = test<br/>    <br/>    # Use fuzzywuzzy's process.extract() to get similarity ratio of the most similar product id to user input<br/>    Ratios = process.extract(str2Match,strOptions)<br/>    <br/>    # Most similar product id to the user input<br/>    highest = process.extractOne(str2Match,strOptions)<br/>    <br/>    # Product id of the most smilar<br/>    final_prod=highest[0]<br/>    <br/>    # Top few of outfit code of the most similar products to user input<br/>    outfit_code=df.loc[df['product_id']==final_prod]['outfit_id'].to_list()<br/>    <br/>    # Get the top one of the outfit code<br/>    outfit_code=outfit_code[0]<br/>    <br/>    # Return outfit<br/>    final_result=df[df["outfit_id"] == outfit_code]<br/>    <br/>    return final_result</span></pre><p id="de77" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">b.è¾“å…¥:äº§å“æè¿°</p><p id="d769" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">å¯¹äºè¿™ç§è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong class="ir hi"> TFIDF </strong></p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="71fd" class="la kd hh lz b fi md me l mf mg">def recommend_description(test):<br/>    '''Searches user's inputted product description and returns the recommended outfit. '''<br/>    <br/>    test = test.lower()<br/>    data = list(df.new_column)<br/>    d = []<br/>    for words in data:<br/>        words = str(words)<br/>        d.append(words)<br/>    <br/>    # creating test_model and dictionary<br/>    test_model = [[word for word in clean(words)] for words in d]<br/>    dictionary = corpora.Dictionary(test_model,prune_at=2000000)<br/>    <br/>    # constructing corpus<br/>    corpus_model= [dictionary.doc2bow(test) for test in test_model]<br/>    tfidf_model = models.TfidfModel(corpus_model)<br/>    <br/>    # constructing tfidf based on processed corpus<br/>    corpus_tfidf = tfidf_model[corpus_model]<br/>    <br/>    # creating the bag of words and calculating tfidf<br/>    test_bow = dictionary.doc2bow([word for word in word_tokenize(test)])<br/>    test_tfidf = tfidf_model[test_bow]<br/>    <br/>    # calculating similarities between test and original data<br/>    index = similarities.MatrixSimilarity(corpus_tfidf)<br/>    sims = pd.DataFrame(index[test_tfidf])<br/>    sims.columns = ["similaritie"]<br/>    sims["information"] = data<br/>    sims = sims[sims["similaritie"] &lt;= 0.98]<br/>    sims = sims.sort_values(by="similaritie", ascending=False).head(1)<br/>    <br/>    # get the product's id with the highest similarity<br/>    target_product = list(sims["information"])[0]<br/>    <br/>    # get the outfit id of the target product<br/>    outfitid = list(df[[v == target_product for v in df['new_column'].tolist()]].outfit_id)[0]<br/>    <br/>    # return all products with the same outfit id<br/>    target = df[df["outfit_id"] == outfitid]<br/><br/>    return target</span></pre><p id="8713" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">C.ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæœ€åä¸€æ­¥æ˜¯å°†æ¨¡å‹ç»„ç»‡æˆä¸€ä¸ªå‡½æ•°ã€‚</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="939f" class="la kd hh lz b fi md me l mf mg">def get_recommendation(input_str):<br/>    if test[0].isdigit(): <br/>        result = recommend_id(test)<br/>        result_short = result[['outfit_item_type','product_full_name','product_id']]<br/>    else:<br/>        result = recommend_description(test)<br/>        result_short = result[['outfit_item_type','product_full_name','product_id']]<br/>    for i in result_short.index:<br/>        print(f'\t{result_short.loc[i][0]} : {result_short.loc[i][1]} ({result_short.loc[i][2]})')<br/><br/>def more_rec_details(input_str):<br/>    if input_str.lower().startswith('y'):<br/>        if test[0].isdigit(): <br/>            result = recommend_id(test)<br/>        else:<br/>            result = recommend_description(test)<br/>        return result[['product_full_name','brand','outfit_item_type','product_id','details','description']]</span></pre><p id="c7ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="mh">è¯•è¯•å§ï¼Œé©¬ä¸Šè·å¾—ä½ çš„è£…å¤‡æ¨èï¼</em>ğŸ˜„</strong></p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="63d7" class="la kd hh lz b fi md me l mf mg">test = input("Enter your content (product_id or product descriptions/details): \n")<br/>get_recommendation(test)<br/><br/>more_details = input("\n\nDo you want more details on the outfit (y/n): \n")<br/>more_rec_details(more_details)</span></pre></div></div>    
</body>
</html>