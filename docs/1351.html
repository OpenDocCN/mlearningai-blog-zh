<html>
<head>
<title>“Horizontally Fused Training Array (HFTA): An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models” Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“水平融合训练阵列(HFTA):训练新型深度学习模型的有效硬件利用挤压器”摘要</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/horizontally-fused-training-array-hfta-an-effective-hardware-utilization-squeezer-for-training-773f16ec8e06?source=collection_archive---------2-----------------------#2021-11-28">https://medium.com/mlearning-ai/horizontally-fused-training-array-hfta-an-effective-hardware-utilization-squeezer-for-training-773f16ec8e06?source=collection_archive---------2-----------------------#2021-11-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4a0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注1 </strong> : SOTA代表<strong class="ig hi"> S </strong>泰特- <strong class="ig hi"> O </strong> f- <strong class="ig hi"> T </strong>何- <strong class="ig hi"> A </strong> rt</p><p id="7bf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注2 </strong>:本文是在提高硬件利用率的算法层面。关于模型融合算法的更多信息，应该阅读论文的附录。这个总结只是给出了背景扎实的思路。</p><h1 id="4d8a" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">介绍</h1><p id="551d" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">深度学习(<strong class="ig hi"> DL </strong>)已经无处不在。近年来，新车型的培训成本增长惊人。HFTA的作者分析了典型的DL培训工作所实现的硬件效率。他们的研究表明，当重复启动时，单加速器(<a class="ae kf" href="https://docs.fast.ai/dev/gpu.html" rel="noopener ugc nofollow" target="_blank">单GPU </a>)训练作业可以支配集群范围内的资源消耗(<strong class="ig hi">集群范围内总GPU时数</strong>的46.2%)。重复启动的目的通常是为了调整超参数。然而，这种优势导致了严重的硬件利用不足。他们的研究揭示:(1)这些模型通常具有相同形状的相同类型的算子，(2)这种算子的模型间水平融合在数学上等价于其他已经很好优化的算子。因此，他们提出了HFTA，这是一个新的DL框架扩展库，它将来自不同重复工作的模型水平融合到操作员，然后在一个共享的加速器上同时训练它们，以提高硬件利用率。在最新的加速器(GPU和TPUs)上对六个DL模型训练的评估表明了该机制的高效性。结果显示，与在单独的加速器上运行每个作业的标准实践相比，培训吞吐量提高了15.1倍。</p><h1 id="95f5" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">背景和动机</h1><h2 id="0254" class="kg jd hh bd je kh ki kj ji kk kl km jm ip kn ko jq it kp kq ju ix kr ks jy kt bi translated">趋势</h2><p id="856a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">值得一提的是，训练DL模型的计算量每<strong class="ig hi"> 3.4 </strong>个月翻一番，超过了摩尔定律<a class="ae kf" href="https://openai.com/blog/ai-and-compute/" rel="noopener ugc nofollow" target="_blank"> ref </a>。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/72fb9bc42026aa451dac666a09cab922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vKheZe-qmgN_bWN9nFK6hw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[<a class="ae kf" href="https://openai.com/blog/ai-and-compute/" rel="noopener ugc nofollow" target="_blank">ref</a>]</figcaption></figure><h2 id="8136" class="kg jd hh bd je kh ki kj ji kk kl km jm ip kn ko jq it kp kq ju ix kr ks jy kt bi translated">神经(模型)架构搜索(NAS)</h2><p id="655d" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">神经网络发展的一个重要方面是它们由超参数组成的结构。目前采用的架构是由人类专家通过反复试验的方法手工开发的。这种方法既耗时又容易出错。正因为如此，人们对自动化的欧洲建筑研究方法越来越感兴趣。这篇<a class="ae kf" href="https://jmlr.org/papers/v20/18-598.html" rel="noopener ugc nofollow" target="_blank">调查论文</a>概述了该研究领域的现有工作。</p><h2 id="00b8" class="kg jd hh bd je kh ki kj ji kk kl km jm ip kn ko jq it kp kq ju ix kr ks jy kt bi translated">收敛稳定性测试</h2><p id="78cf" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">这种方法用不同的随机种子对同一个模型进行多次训练，以验证最终的精度结果。</p><h2 id="8817" class="kg jd hh bd je kh ki kj ji kk kl km jm ip kn ko jq it kp kq ju ix kr ks jy kt bi translated">提高DL培训工作的硬件利用率并不容易！</h2><p id="0510" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">这一挑战是两个原因造成的。首先，DL研究人员和实践者缺乏系统和架构专业知识来优化他们的培训工作量。他们唯一天真的选择是增加小批量生产。然而，大的小批量可能会导致一些问题，例如增加达到精度的时间(<strong class="ig hi"> TTA </strong> ) [ <strong class="ig hi"> 2 </strong> ]，GANs中的训练不稳定性[ <strong class="ig hi"> 3，4 </strong> ]，推广差距[ <strong class="ig hi"> 5 </strong> ]，由于小批量缩放限制而导致的收益递减[ <strong class="ig hi"> 6 </strong> ]。第二，加速器向更高的计算能力发展，具有更专业的计算单元和更大的内存容量和带宽。</p><h2 id="5417" class="kg jd hh bd je kh ki kj ji kk kl km jm ip kn ko jq it kp kq ju ix kr ks jy kt bi translated">硬件共享</h2><p id="c0f4" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">适用于DL培训工作的基于硬件的共享解决方案有<a class="ae kf" href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">M</strong>ulti-<strong class="ig hi">P</strong>process<strong class="ig hi">S</strong>service(<strong class="ig hi">MPS</strong>)</a>和<strong class="ig hi">M</strong>ulti-<strong class="ig hi">I</strong>instance<strong class="ig hi">G</strong>PU(<strong class="ig hi">MIG</strong>)。</p><p id="83a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">多进程服务(<strong class="ig hi"> MPS </strong> ): MPS允许来自不同进程的CUDA内核通过一个叫做<a class="ae kf" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank"> Hyper-Q </a>的硬件特性在同一个GPU上并行运行。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lk"><img src="../Images/1d915125aee4b3fee50c105f583311c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3jf8Aqc5SQytPGKiMq5eA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">MPS</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ll"><img src="../Images/40921f7e1d25aefcabd485b15ae73b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8T4xpuIbGrWe7pr3kMPJA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Nvidia Hyper-Q Technology</figcaption></figure><p id="078f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">M<strong class="ig hi">M</strong>ulti-<strong class="ig hi">I</strong>instance<strong class="ig hi">G</strong>PU(<strong class="ig hi">MIG</strong>):这种能力在最新的A100 GPUs上可用，它们被包装在DGX A100机器中出售。它是由8个A100、一对64核AMD服务器芯片、1TB RAM和15TB NVME存储组成的机器。MIG功能将单个GPU划分为多个(最多7个实例)独立的GPU实例(GIs ),其中每个作业在单个GIs上运行。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lm"><img src="../Images/5ab7446ac943121eb87f527307bba41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Df4XU4SKZY7FJmjujcfDfg.png"/></div></div></figure><p id="9757" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">HFTA的作者提到了MPS和MIG机制的以下缺点:</p><ol class=""><li id="1140" class="ln lo hh ig b ih ii il im ip lp it lq ix lr jb ls lt lu lv bi translated">MPS和MIG都在来自不同训练作业的内核之间复制运行时开销，包括内核启动[ <a class="ae kf" href="http://www.istc-cc.cmu.edu/publications/papers/2013/dlustig_HPCA13.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> 7 </strong> </a> ]、<a class="ae kf" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html" rel="noopener ugc nofollow" target="_blank"> GEMM </a>设置和拆卸，以及<a class="ae kf" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html" rel="noopener ugc nofollow" target="_blank">内存格式转换</a></li><li id="6c48" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">MPS和MIG都需要将训练作业作为单独的进程运行，这导致DL框架堆栈保留的GPU内存开销加倍，并导致<a class="ae kf" href="https://github.com/pytorch/pytorch/issues/20532" rel="noopener ugc nofollow" target="_blank">更高的整体GPU内存占用量</a>。</li><li id="2bfb" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">MIG的划分粒度对于许多训练工作负载来说可能太粗，即使使用MIG的最细粒度。</li></ol><p id="6b88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Nvidia A100 (40GB) GPU由8个5GB内存片和7个计算片(或7个SMs)组成。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mb"><img src="../Images/200c8b6ebbaa2fd17e4f2768d722b695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*kKUbxN3wOuE5zxtK.png"/></div></figure><p id="8c28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下示例显示了如何将5GB内存片与1个计算片相结合来创建1g.5gb GPU实例(GI)。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mc"><img src="../Images/f897d98a42a174c19d35ed16facfe3ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/0*r8_yO6F_1NkTASjI.png"/></div></figure><h1 id="84d6" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">提议的机制:HFTA</h1><p id="190a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">研究人员[1]根据他们的关键观察，提出HTFA解决利用不足的挑战。</p><p id="7f8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(1)当重复启动时，跨这些作业使用的模型通常具有相同形状的相同类型的运算符。</p><p id="737c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(2)水平融合具有相同形状的相同类型的运算符通常会产生其他数学上等价的运算符，这些运算符已经存在于许多SOTA DL模型中，因此已经在不同加速器上的大多数DL框架栈中进行了优化。</p><p id="22ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了HFTA的基本概念。两个模型中的第一个算子都是形状相同的Conv2d许多Conv2d运算符的水平融合在数学上等同于一个分组Conv2d，该分组conv 2d已经在ResNeXt [ <a class="ae kf" href="https://arxiv.org/pdf/1611.05431.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> 8 </strong> </a> ]和MobileNets [ <a class="ae kf" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> 9 </strong> </a> ]模型中使用，并由Nvidia GPUs上的<a class="ae kf" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#grouped-convolutions" rel="noopener ugc nofollow" target="_blank"> cuDNN </a>和<a class="ae kf" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank"> TPUs上的</a><a class="ae kf" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm" rel="noopener ugc nofollow" target="_blank">ref</a>支持。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es md"><img src="../Images/2869f6a52495877b02b3befaae5b28bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v8yX2ZiKmxVt70buXTpLtQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[<strong class="bd je">1</strong>]</figcaption></figure><p id="5a41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上所述，许多培训滚刀可以融合成一个单一的工作。它不需要从头实现任何新的特定于设备的操作符，这既耗时又容易出错。此外，这种方法可以推广到DL框架已经支持的任何硬件后端。注意水平融合可以应用于单加速器和分布式训练。然而，对于DL研究人员和实践者来说，从零开始手动实现现有的训练工作负载到融合的工作负载是具有挑战性的。因此，作者开发了一个新的DL框架库HTFA。他们选择PyTorch是因为它的用户友好性和在ML社区中的受欢迎程度。只需修改几行代码，就可以使用开发的工具。下图显示了如何为AlexNet启用HFTA。模型定义保持不变，只有几行额外的代码来更新PyTorch的操作符类。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es me"><img src="../Images/8f2c45e38a38fb174274cabb8bbe1657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMKdMGqbFMt0PKj7MoMrdA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[<strong class="bd je">1</strong>]</figcaption></figure><p id="7580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图通过连接(1)沿通道维度的输入和(2)沿输出通道维度的权重(过滤器)和偏差，举例说明了卷积运算符的融合等效于其分组卷积对应项。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mf"><img src="../Images/9b41e5218a9923420654d7eed83c1db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqORVcIpejKBb9hXDdRaRg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">[<strong class="bd je">1</strong>]</figcaption></figure><h1 id="b0a9" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">结论</h1><p id="0322" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在这项工作中，作者从GPU集群使用分析中了解到，重复的单加速器训练作业(通常用于超参数调优)主导了集群范围的硬件资源使用，可能会严重利用不足。他们观察这些工作的具体特征，这些特征使得模型间的水平融合成为可能。因此，他们提出了HFTA库，该库通过在同一加速器上同时训练多个模型，将模型水平融合到操作员，从而显著提高硬件利用率。在六个极具影响力的DL模型上，HFTA实现了比在单独的加速器上运行每个作业高15.13倍的训练吞吐量，这是超参数调优框架采用的常见做法。</p><h1 id="3135" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">参考</h1><p id="5e74" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">【<strong class="ig hi"> 1 </strong>】王，尚等.<strong class="ig hi">水平融合训练阵列:训练新型深度学习模型的有效硬件利用挤压器</strong><em class="mg">机器学习与系统学报</em> 3 ( <strong class="ig hi"> 2021 </strong>)。</p><p id="5893" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【<strong class="ig hi">2</strong>】a . koli ousis，P. Watcharapichat，M. Weidlich，L. Mai，P. Costa，en P. Pietzuch，“<strong class="ig hi">Crossbow:Scaling Deep Learning with Small Batch size on Multi-GPU Servers</strong>”，<em class="mg"> Proc .VLDB捐赠。</em>，第12卷，第11期，bll 1399–1412，2019年7月<strong class="ig hi"/>。</p><p id="f29c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【<strong class="ig hi">3</strong>a . Brock，J. Donahue，en K. Simonyan，<strong class="ig hi">高保真自然图像合成的大规模GAN训练</strong>，<em class="mg"> CoRR </em>，第abs/1809.11096卷，<strong class="ig hi"> 2018 </strong>。</p><p id="4370" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[ <strong class="ig hi"> 4 </strong> ]关于GANs的公开问题，访问日期:2021年11月26日，<a class="ae kf" href="https://distill.pub/2019/gan-open-problems/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2019/gan-open-problems/</a>。</p><p id="4a4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[<strong class="ig hi">5</strong>n . s . Keskar，D. Mudigere，J. Nocedal，M. Smelyanskiy，en P. T. P. Tang，<strong class="ig hi">关于深度学习的大批量训练:泛化差距与尖锐极小值</strong>，<em class="mg"> CoRR </em>，第abs/1609.04836卷，<strong class="ig hi"> 2016 </strong>。</p><p id="bf84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【<strong class="ig hi">6</strong>】c . j . shall UE，J. Lee，J. M. Antognini，J. Sohl-Dickstein，R. Frostig，en G. E. Dahl，《测量数据并行性对神经网络训练的影响》，<em class="mg"> CoRR </em>，第abs/1811.03600卷，<strong class="ig hi"> 2018 </strong>。</p><p id="ddb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[ <strong class="ig hi"> 7 </strong> ] D. Lustig和M. Martonosi，“<strong class="ig hi">通过细粒度CPU-GPU同步减少GPU卸载延迟”，</strong>“<em class="mg">2013年IEEE第19届高性能计算机架构国际研讨会(</em><strong class="ig hi"><em class="mg">【HPCA</em></strong><em class="mg">)</em>，<strong class="ig hi"> 2013年</strong>，第354–365页，DOI: 10.1109</p><p id="4c0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【<strong class="ig hi">8</strong>s .谢，R. B. Girshick，P. Dollár，Z. Tu，en K. He，<strong class="ig hi">深度神经网络的聚合残差变换</strong>，<em class="mg"> CoRR </em>，vol abs/1611.05431，<strong class="ig hi"> 2016 </strong>。</p><p id="016d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【<strong class="ig hi">9</strong>a . g . Howard<em class="mg">等人</em>、<strong class="ig hi"> MobileNets:用于移动视觉应用的高效卷积神经网络</strong>、<em class="mg"> CoRR </em>、vol abs/1704.04861、<strong class="ig hi"> 2017 </strong>。</p><div class="mh mi ez fb mj mk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hi fi z dy mp ea eb mq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">medium.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my le mk"/></div></div></a></div></div></div>    
</body>
</html>