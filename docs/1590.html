<html>
<head>
<title>Quickly Master the Principal Components Analysis: the Data Dimensionality Reduction Technique You Must Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速掌握主成分分析:你必须知道的数据降维技术</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/quickly-master-the-principal-components-analysis-the-data-dimensionality-reduction-technique-you-28a0483e2b59?source=collection_archive---------8-----------------------#2022-01-08">https://medium.com/mlearning-ai/quickly-master-the-principal-components-analysis-the-data-dimensionality-reduction-technique-you-28a0483e2b59?source=collection_archive---------8-----------------------#2022-01-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="2eb4" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">为什么降维很重要？</p></blockquote><p id="3512" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">数据维度越高，模型拟合的难度越大。</strong>我们以线性回归的建模为例。当它包括两个预测值和一个目标时，很容易得出结论，模型拟合过程的“空间”是松散的<strong class="ik hi"> <em class="ij"> x </em>乘以<em class="ij"> y </em> </strong> <em class="ij">，</em> <strong class="ik hi"> <em class="ij"> </em> </strong>，它是一个<em class="ij"> </em>二次变量。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/696348fd78b705d6c92455934d158a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPXmCxjksSukgNhj9KIkTQ.jpeg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Fig 1. Two-dimensional space for the model-fitting process</figcaption></figure><p id="ce6e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">然后再增加一个预测器。现在，模型拟合过程的“空间”变成了松散的<strong class="ik hi"> <em class="ij"> x乘以y乘以z </em> </strong> <em class="ij">，</em>这是一个<em class="ij"> </em>立方变量。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jz"><img src="../Images/cbc0d8b96db184fd87aacf18d5d38b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3d9yM66_s_2aAEwVOGZ5A.jpeg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Fig 2. Three-dimensional space for the model-fitting process</figcaption></figure><p id="9912" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">综上所述，虽然这里的数据维度只增加了<strong class="ik hi"> 50% </strong>(即从2增加到3)，“空间”的容量经历了<strong class="ik hi">的指数增长</strong>(即从二次增加到三次)，这意味着模型拟合过程的难度大大增加。</p><blockquote class="ie if ig"><p id="5922" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">主成分分析(PCA)是如何进行降维的？</p></blockquote><p id="9c74" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">主成分分析将预测因子的总方差重新分配到“分量”中，使方差的分布更加集中。让我们考虑一下线性回归的情况。当它包括两个预测值和一个目标值时，应用主成分分析也将为我们提供相同数量的“分量”。使该分量不同于原始预测值的是，较少数量的分量将包括比原始预测值更多的方差。<strong class="ik hi">换句话说，给定两个模型拟合预测值，仅使用一个组件就可以达到相同的模型性能。</strong></p></div><div class="ab cl ka kb go kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="ha hb hc hd he"><h1 id="6779" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">通过一个超级简单的例子深入PCA</h1><p id="382d" class="pw-post-body-paragraph ih ii hh ik b il lf in io ip lg ir is jg lh iv iw jh li iz ja ji lj jd je jf ha bi translated">假设有如下所示的三个预测值可用于建模:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lk"><img src="../Images/13ccd9749c20b2a2ed51534f2b2167e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wHWkgTrnfvh3BNezJEmmZQ.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Fig 3. Example: three predictors and their variance</figcaption></figure><p id="33a9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">每个预测值的方差已经提供，方差之和为21.1088。然后，我们通过下面的python代码应用了PCA，我们获得了预期的三个组件。</p><pre class="jk jl jm jn fd ll lm ln lo aw lp bi"><span id="98b2" class="lq ki hh lm b fi lr ls l lt lu"># Applying the PCA to the example <br/>import numpy as np<br/>from sklearn.decomposition import PCA</span><span id="54ae" class="lq ki hh lm b fi lv ls l lt lu">predictors = np.array([[10.5, 11.2, 8.9],[7.3, 5.6, 3.2], [4.2, 8.1, 9.0],[10.4, 3.2, 7.6]])</span><span id="3818" class="lq ki hh lm b fi lv ls l lt lu"># create a 'pca' instance with 3 components<br/>pca = PCA(n_components=3)<br/>pca.fit(predictors)</span><span id="e404" class="lq ki hh lm b fi lv ls l lt lu"># show all 3 components<br/>pca.transform(predictors)</span></pre><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lw"><img src="../Images/3e53646bc6b25f62180d170502dd3cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8ToT2FhOKyIUbp-ADlN7g.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Fig 4. Components from the PCA</figcaption></figure><p id="cb1d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">正如我们在这里看到的，组件的总方差和预测值都与<strong class="ik hi">相同</strong>！令人惊讶的是，第一和第二分量比X1和X2的方差多<strong class="ik hi"> 14% </strong>。</p><p id="4535" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">因此，我们可以发现，尽管差异的总量是恒定的，但它已被集中到这些顶部组件中。现在，如果我们决定应用主成分分析的前两个成分进行建模，我们应该不会因为丢失三个原始预测值的大量信息而遭受损失。</p></div><div class="ab cl ka kb go kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="ha hb hc hd he"><h1 id="9ee6" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">参考</h1><p id="ae27" class="pw-post-body-paragraph ih ii hh ik b il lf in io ip lg ir is jg lh iv iw jh li iz ja ji lj jd je jf ha bi translated">【1】<em class="ij">sk learn。分解. PCA </em>。scikit。(未注明)。2022年1月8日检索，来自<a class="ae lx" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></p><div class="ly lz ez fb ma mb"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hi fi z dy mg ea eb mh ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">medium.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp jt mb"/></div></div></a></div></div></div>    
</body>
</html>