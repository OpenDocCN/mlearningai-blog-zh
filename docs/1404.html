<html>
<head>
<title>Sentiment Analysis of Movie Reviews with Google’s BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用谷歌的BERT对电影评论进行情感分析</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/sentiment-analysis-of-movie-reviews-with-googles-bert-c2b97f4217f?source=collection_archive---------1-----------------------#2021-12-10">https://medium.com/mlearning-ai/sentiment-analysis-of-movie-reviews-with-googles-bert-c2b97f4217f?source=collection_archive---------1-----------------------#2021-12-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="49e1" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">通过利用变压器编码器架构(BERT)而不是传统的LSTM来执行NLP任务</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/212a19832796070676d1bb1a0b02035e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E2fJXCCtLIWY0sJM"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@aditya1702?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aditya Vyas</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7540" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">谷歌目前正在使用BERT来优化对用户搜索查询的解释。BERT擅长的几个功能使这成为可能，包括:</p><p id="5302" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">基于序列对序列的语言生成任务，例如:</p><ul class=""><li id="c004" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">问题回答</li><li id="5f6c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">抽象概括</li><li id="5f4c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">句子预测</li><li id="cab3" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">会话响应生成</li></ul><p id="38b3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">BERT基于变压器编码器架构。重要的问题是，为什么需要变压器架构，它们如何优于传统的LSTM RNN的。</p><h2 id="bec8" class="kx ky hh bd kz la lb lc ld le lf lg lh jw li lj lk ka ll lm ln ke lo lp lq lr bi translated"><em class="ls">LSTM·RNN的缺点</em></h2><ul class=""><li id="87f8" class="kj kk hh jp b jq lt jt lu jw lv ka lw ke lx ki ko kp kq kr bi translated">LSTM的计算量很大，因此速度很慢。</li><li id="7cf5" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">它们不是真正的双向的，即两个方向的学习是独立发生的。因此，真实的上下文稍微有些丢失。</li><li id="9e92" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">由于数据是按顺序读取的，它们在倾向于并行处理的GPU上表现不佳。</li></ul><h2 id="359a" class="kx ky hh bd kz la lb lc ld le lf lg lh jw li lj lk ka ll lm ln ke lo lp lq lr bi translated">救世主来了——变形金刚</h2><p id="e56c" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">《变形金刚》在以下几个方面比LSTM做得更好</p><ul class=""><li id="1304" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">它们能够同时处理单词，从而减少计算量，促进并行处理，因此对GPU友好。</li><li id="fc98" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">他们倾向于更好地学习课文的上下文。</li></ul><blockquote class="mb"><p id="bc9c" class="mc md hh bd me mf mg mh mi mj mk ki dx translated">转换器包含两个模块——编码器和解码器模块</p></blockquote><p id="4d41" class="pw-post-body-paragraph jn jo hh jp b jq ml ii js jt mm il jv jw mn jy jz ka mo kc kd ke mp kg kh ki ha bi translated">现在你可以在这里详细了解编码器和解码器的功能</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mq mr l"/></div></figure><p id="a633" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下面我试着给你直觉。</p><p id="0ccc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi ms translated"><span class="l mt mu mv bm mw mx my mz na di"> E </span> ncoder同时接收输入，并将它们处理成单词嵌入。嵌入是具有单词真正含义的向量。</p><p id="a0cf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi ms translated"><span class="l mt mu mv bm mw mx my mz na di"> D </span> ecoder从编码器和之前学习的句子中获取输入，然后生成新单词。</p><p id="414e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当我们堆叠编码器时，我们得到BERT——变压器的双向编码器表示。</p><p id="bbf0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在本文中，我们尝试使用BERT来预测移动评论情绪。</p><h1 id="16c5" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">目录</h1><ol class=""><li id="bd79" class="kj kk hh jp b jq lt jt lu jw lv ka lw ke lx ki nm kp kq kr bi translated"><a class="ae jm" href="#57e1" rel="noopener ugc nofollow">关于数据集</a></li><li id="ccd4" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#ed75" rel="noopener ugc nofollow">数据导入</a></li><li id="857c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#e1e5" rel="noopener ugc nofollow">创建训练和验证集</a></li><li id="fc44" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#e816" rel="noopener ugc nofollow">选择Bert和预处理模块</a></li><li id="59a4" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#c5f4" rel="noopener ugc nofollow">将数据传递给预处理模块&amp; Bert </a></li><li id="11a4" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#f888" rel="noopener ugc nofollow">培训和评估</a></li><li id="ca8e" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#276b" rel="noopener ugc nofollow">保存并重新加载模型</a></li><li id="25fd" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#01b8" rel="noopener ugc nofollow">预测</a></li><li id="2de2" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#d6f3" rel="noopener ugc nofollow">总结</a></li><li id="543a" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki nm kp kq kr bi translated"><a class="ae jm" href="#c332" rel="noopener ugc nofollow">参考文献</a></li></ol><h1 id="57e1" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№1:关于数据集</h1><p id="5960" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">这个笔记本训练了一个情感分析模型，根据评论的文本将电影评论分为正面或负面。</p><p id="0e4c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">该数据集包含从IMDB电影数据库收集的超过50，000条电影评论。数据集可以从<a class="ae jm" href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" rel="noopener ugc nofollow" target="_blank">这里</a>下载</p><h1 id="ed75" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№2:数据导入</h1><p id="4747" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">为了处理文本信息，需要安装以下库</p><ul class=""><li id="40c0" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">张量流</li><li id="647c" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">模型—用于访问Bert</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="e314" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们使用Keras实用程序下载数据集。然后，我们访问训练数据集并删除不需要的文件。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><h1 id="e1e5" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№3:创建训练和验证集</h1><p id="7825" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">现在，我们定义训练集、验证集和测试集。现在，在使用Keras预处理<em class="no">_ text _ data _ from _ directory _</em>方法来定义训练集、验证集和测试集之前，有一些注意事项。我们定义训练集和验证集的方式非常标准，没有什么新奇的。</p><p id="9c34" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="no">警告</em> —评论应该在目标文件夹下。也就是说，如果有正面评价，它必须在<em class="no"> pos </em>文件夹下，同样负面评价必须在<em class="no"> neg </em>文件夹下</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="39eb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">访问一些样本及其目标值</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><h1 id="e816" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№4:选择Bert和预处理模块</h1><p id="2f3f" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">在Bert可用的不同变体中，您可以从下拉列表中选择所需的配置。</p><p id="5bf3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">由于我们可用的计算有限，建议使用处理最少参数的模型，因此我使用以-2结尾的模型。</p><p id="f2c4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">还需要基于所选择的Bert模型来选择预处理模型。下面的实用程序有助于自动选择正确的预处理模型。预处理模型的重要性将在后续单元中解释。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="04d7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">注意，在上面的代码块中，文本看起来很小很乱，但是如果你在google colab中打开它，看起来会更好一些，就像这样</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es np"><img src="../Images/6fd3619280453b66ca7177f9356f772a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJx8yTVKcJjHAeYFg7L19Q.png"/></div></div></figure><h1 id="c5f4" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№5:将数据传递给预处理模块&amp; Bert</h1><p id="33ac" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">下面我们将一个样本文本传递给预处理模型。该模型接受128个长度的输入，因此预处理最多完成128个字。预处理模型将文本转换成3个键</p><ul class=""><li id="233f" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">input_mask —这里每个单词显示为1，但您会注意到还有2个额外的1。例如，hello world中有4个1，而不是每个单词一个1。Bert的工作方式是在开头放一个特殊的记号，在结尾放一个分隔符记号。因此，我们看到每个输入句子有2个额外的标记值。</li><li id="c97d" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">输入类型标识—通常为零</li><li id="6dce" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">input_work_ids —这表示每个单词的标记值。不是第一个单词，即特殊标记将始终是101，分隔符标记最终将始终是102。</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="7745" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">使用伯特模型</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="68a5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Bert输出被传递到神经网络，并计算输出概率。BERT执行单词嵌入的任务，但在此之后，其余的活动由神经网络负责。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="1d16" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们使用二进制交叉熵来计算2类分类模型的损失</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><h1 id="f888" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№6:培训和评估</h1><p id="40d9" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">我们为3个时期训练模型，并定义一些超参数。让我们不要去讨论这些参数的细节，保持本教程的简单</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nq"><img src="../Images/5ddb236510024d7f9a1673d530722aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*_pvUNxAQePxRUmt_-mMiYw.png"/></div></figure><p id="86d6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">训练和验证损失似乎收敛得很好，因此我们的模型似乎没有过度拟合，这是一个好消息。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nr mr l"/></div></figure><h1 id="276b" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№7:保存并重新加载模型</h1><p id="a312" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">我们现在学习保存和重用保存的模型，这有助于我们节省所有的培训时间。为了保存模型，我们使用了<em class="no">。保存</em>方法和重新加载模型我们用<em class="no">。saved_model.load </em>方法</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><p id="2a74" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">您可以在下面看到，保存和重新加载的模型具有相同的预测，这证实了保存的模型正在按预期工作。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><h1 id="01b8" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№8:预测</h1><p id="ff22" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">我们在下面的样本上检查我们的预测</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nn mr l"/></div></figure><h1 id="d6f3" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">№9:摘要</h1><p id="a03f" class="pw-post-body-paragraph jn jo hh jp b jq lt ii js jt lu il jv jw ly jy jz ka lz kc kd ke ma kg kh ki ha bi translated">我们通过以下步骤使用Bert模型执行情感分类-</p><ul class=""><li id="8502" class="kj kk hh jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">将数据集导入我们的环境。</li><li id="913d" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">我们使用Keras实用函数<code class="du ns nt nu nv b">tf.keras.preprocessing.text_dataset_from_directory</code>来创建训练和验证集</li><li id="2224" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">现在我们选择BERT模型和相应的预处理模型。</li><li id="1ba6" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">我们解析了预处理模型的输入数据，并理解了生成的各种密钥。</li><li id="4d39" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">然后，我们将bert的输出传递给神经网络进行预测。</li><li id="c241" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">我们现在训练模型，并将超参数定义为默认值。</li><li id="77a6" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">我们还学习了保存和重新加载同一个模型</li><li id="c2a0" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">最后，我们使用训练好的模型进行预测。</li></ul><h1 id="c332" class="nb ky hh bd kz nc nd ne ld nf ng nh lh in ni io lk iq nj ir ln it nk iu lq nl bi translated">10号:参考文献</h1><ul class=""><li id="f531" class="kj kk hh jp b jq lt jt lu jw lv ka lw ke lx ki ko kp kq kr bi translated"><a class="ae jm" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/text/tutorials/classify _ text _ with _ Bert</a></li><li id="b3d6" class="kj kk hh jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">笔记本链接—<a class="ae jm" href="https://jovian.ai/hargurjeet/movie-reviews-using-bert" rel="noopener ugc nofollow" target="_blank">https://jovian.ai/hargurjeet/movie-reviews-using-bert</a></li></ul><p id="c43e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我希望你在阅读博客的时候过得愉快。如果你喜欢今天要学的东西，请随意给出👏。</p><p id="c167" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">请随时通过LinkedIn联系我</p><div class="nw nx ez fb ny nz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="oa ab dw"><div class="ob ab oc cl cj od"><h2 class="bd hi fi z dy oe ea eb of ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="og l"><h3 class="bd b fi z dy oe ea eb of ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oh l"><p class="bd b fp z dy oe ea eb of ed ef dx translated">medium.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on jg nz"/></div></div></a></div></div></div>    
</body>
</html>