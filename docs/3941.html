<html>
<head>
<title>Scalable ELT Data warehousing Pipeline Using Airflow, DBT, Postgres in a Docker Container</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Docker容器中使用气流、DBT、Postgres的可扩展ELT数据仓库管道</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/scalable-elt-data-warehousing-pipeline-using-airflow-dbt-postgres-in-a-docker-container-a0078362bc3?source=collection_archive---------4-----------------------#2022-11-13">https://medium.com/mlearning-ai/scalable-elt-data-warehousing-pipeline-using-airflow-dbt-postgres-in-a-docker-container-a0078362bc3?source=collection_archive---------4-----------------------#2022-11-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="ad9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">简介</strong></p><p id="9636" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">许多组织使用传感器和物联网设备从多个来源收集数据。为此构建系统的主要挑战是将一个工具与另一个工具集成。这份报告将展示如何创建一个有效的管道和我们将用于该项目的技术栈。</p><p id="d3bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">目标</strong></p><p id="2f81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个城市交通部门希望使用swarm UAVs(无人机)从该市的几个位置收集交通数据，并将收集的数据用于改善该市的交通流量和其他几个未公开的项目。该项目将负责创建一个可扩展的数据仓库，该数据仓库将托管通过分析swarm无人机拍摄的镜头提取的车辆轨迹数据。</p><p id="5d98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为此，我们将使用ELT管道。ELT是一个数据集成过程，用于将原始数据从源服务器传输到目标服务器上的数据系统(如数据仓库或数据湖),然后为下游使用准备信息。让我们简单看看ELT代表什么。</p><ul class=""><li id="9c7c" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">提取数据:它是从一个或多个源系统中识别和读取数据的过程。</li><li id="2c97" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">加载数据:加载是将提取的数据添加到目标数据库的过程。</li><li id="c09d" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">转换数据:这是将数据从其源格式转换为分析所需格式的过程</li></ul><p id="1a17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">数据</strong></p><p id="e0ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据通过<a class="ae jq" href="https://open-traffic.epfl.ch/index.php/downloads/#1599047632450-ebe509c8-1330" rel="noopener ugc nofollow" target="_blank"> pNEUMA网站</a>提供。pNEUMA是一个开放的大规模数据集，包含50万辆汽车的自然轨迹，由一群无人机在希腊雅典拥挤的市区进行的一项独一无二的实验收集而成。单个(区域、日期、时间)的每个文件大约是87MB的数据。</p><p id="3d98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">技术堆栈</strong></p><p id="a4f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是我们将在这个项目中使用的主要工具，我们将看到项目如何流动。</p><ul class=""><li id="aba4" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi"> Python </strong>:一种面向对象的编程语言，用于编写数据提取、加载和转换脚本。</li><li id="caf1" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi"> Docker </strong>:用于创建适当且有控制地使用技术堆栈的环境。</li><li id="594d" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi"> Airflow </strong> : Apache Airflow是一个开源的工作流管理平台，提供了以编程方式开发、监控和调度工作流的能力。气流管道用Python定义，然后转换成有向无环图(DAG)。</li><li id="8267" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi"> DBT </strong> : DBT是一个开发框架，它将模块化SQL与软件工程最佳实践相结合，使数据转换可靠而快速。它使具有数据分析师技能的人能够访问数据工程活动，使用简单的select语句转换仓库中的数据，有效地用代码创建整个转换过程。</li><li id="742e" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi"> Postgres </strong>:这是一个高级的企业级开源关系数据库，支持SQL(关系)和JSON(非关系)查询。我们用它来存储原始数据和转换后的数据。</li><li id="7941" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><strong class="ig hi"> Redash </strong>:是一个开源的web应用程序，用于探索、查询、可视化和共享来自我们数据源的数据。</li></ul><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/fc9cca8d773f0f1d13fa30e72ffd93a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NR6JX_KVJM5878BC"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx">The workflow pipelines</figcaption></figure><p id="ea6e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对数据进行探索性数据分析</strong></p><p id="8b97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">pNEUMA数据集以原始CSV格式提供。数据集的每一行代表一辆车的数据。第一行的前10列包括列名，而前4列包括关于轨迹的信息，如唯一的trackID、车辆类型、行驶的距离(米)和车辆的平均速度(公里/小时)</p><p id="a341" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使原始CSV数据可用，我用分隔符加载了它。</p><pre class="js jt ju jv fd kh ki kj bn kk kl bi"><span id="4a7a" class="km kn hh ki b be ko kp l kq kr">import pandas as pd<br/>df = pd.read_csv('./data/drone3.csv', sep='[;:]', index_col=False)</span></pre><ul class=""><li id="0c6c" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">数据是干净的，没有丢失的值。</li></ul><h2 id="fa01" class="ks kn hh bd kt ku kv kw kx ky kz la lb ip lc ld le it lf lg lh ix li lj lk ll bi translated">气流编排</h2><p id="0ee3" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">首先，我们需要建立开发环境。我创建了一个python虚拟环境。之后，我在airflow目录下创建了一个docker-compose.yaml文件。</p><p id="2cc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是docker-compose.yaml文件的样子。</p><pre class="js jt ju jv fd kh ki kj bn kk kl bi"><span id="21e8" class="km kn hh ki b be ko kp l kq kr">version: "3"<br/>x-airflow-common:<br/>  &amp;airflow-common #image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.4.1}<br/>  build: .<br/>  environment: &amp;airflow-common-env<br/>    AIRFLOW__CORE__EXECUTOR: CeleryExecutor<br/>    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow<br/>    # For backward compatibility, with Airflow &lt;2.3<br/>    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow<br/>    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow<br/>    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0<br/>    AIRFLOW__CORE__FERNET_KEY: ""<br/>    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"<br/>    # AIRFLOW__CORE__LOAD_EXAMPLES: "true"<br/>    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"<br/>    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}<br/>    depends_on: "postgres-dbt"<br/>  volumes:<br/>    - ./dags:/opt/airflow/dags<br/>    - ./logs:/opt/airflow/logs<br/>    - ./plugins:/opt/airflow/plugins<br/>    - ./data:/opt/airflow/data<br/>    - ../dbt:/opt/dbt<br/>  user: "${AIRFLOW_UID:-50000}:0"<br/>  depends_on: &amp;airflow-common-depends-on<br/>    redis:<br/>      condition: service_healthy<br/>    postgres:<br/>      condition: service_healthy<br/><br/>services:<br/>  postgres:<br/>    image: postgres:13<br/>    environment:<br/>      POSTGRES_USER: airflow<br/>      POSTGRES_PASSWORD: airflow<br/>      POSTGRES_DB: airflow<br/>    ports:<br/>      - "5434:5432"<br/>    volumes:<br/>      - postgres-db-volume:/var/lib/postgresql/data<br/>    healthcheck:<br/>      test: ["CMD", "pg_isready", "-U", "airflow"]<br/>      interval: 5s<br/>      retries: 5<br/>    restart: always<br/><br/>  redis:<br/>    image: redis:latest<br/>    expose:<br/>      - 6379<br/>    healthcheck:<br/>      test: ["CMD", "redis-cli", "ping"]<br/>      interval: 5s<br/>      timeout: 30s<br/>      retries: 50<br/>    restart: always<br/><br/>  airflow-webserver:<br/>    &lt;&lt;: *airflow-common<br/>    command: webserver<br/>    ports:<br/>      - 8080:8080<br/>    healthcheck:<br/>      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]<br/>      interval: 10s<br/>      timeout: 10s<br/>      retries: 5<br/>    restart: always<br/>    depends_on:<br/>      &lt;&lt;: *airflow-common-depends-on<br/>      airflow-init:<br/>        condition: service_completed_successfully<br/><br/>  airflow-scheduler:<br/>    &lt;&lt;: *airflow-common<br/>    command: scheduler<br/>    healthcheck:<br/>      test:<br/>        [<br/>          "CMD-SHELL",<br/>          'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"',<br/>        ]<br/>      interval: 10s<br/>      timeout: 10s<br/>      retries: 5<br/>    restart: always<br/>    depends_on:<br/>      &lt;&lt;: *airflow-common-depends-on<br/>      airflow-init:<br/>        condition: service_completed_successfully<br/><br/>  airflow-worker:<br/>    &lt;&lt;: *airflow-common<br/>    command: celery worker<br/>    healthcheck:<br/>      test:<br/>        - "CMD-SHELL"<br/>        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'<br/>      interval: 10s<br/>      timeout: 10s<br/>      retries: 5<br/>    environment:<br/>      &lt;&lt;: *airflow-common-env<br/>      # Required to handle warm shutdown of the celery workers properly<br/>      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation<br/>      DUMB_INIT_SETSID: "0"<br/>    restart: always<br/>    depends_on:<br/>      &lt;&lt;: *airflow-common-depends-on<br/>      airflow-init:<br/>        condition: service_completed_successfully<br/><br/>  airflow-triggerer:<br/>    &lt;&lt;: *airflow-common<br/>    command: triggerer<br/>    healthcheck:<br/>      test:<br/>        [<br/>          "CMD-SHELL",<br/>          'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"',<br/>        ]<br/>      interval: 10s<br/>      timeout: 10s<br/>      retries: 5<br/>    restart: always<br/>    depends_on:<br/>      &lt;&lt;: *airflow-common-depends-on<br/>      airflow-init:<br/>        condition: service_completed_successfully<br/><br/>  airflow-init:<br/>    &lt;&lt;: *airflow-common<br/>    entrypoint: /bin/bash<br/>    # yamllint disable rule:line-length<br/>    command:<br/>      - -c<br/>      - |<br/>        function ver() {<br/>          printf "%04d%04d%04d%04d" $${1//./ }<br/>        }<br/>        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO &amp;&amp; gosu airflow airflow version)<br/>        airflow_version_comparable=$$(ver $${airflow_version})<br/>        min_airflow_version=2.2.0<br/>        min_airflow_version_comparable=$$(ver $${min_airflow_version})<br/>        if (( airflow_version_comparable &lt; min_airflow_version_comparable )); then<br/>          echo<br/>          echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"<br/>          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"<br/>          echo<br/>          exit 1<br/>        fi<br/>        if [[ -z "${AIRFLOW_UID}" ]]; then<br/>          echo<br/>          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"<br/>          echo "If you are on Linux, you SHOULD follow the instructions below to set "<br/>          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."<br/>          echo "For other operating systems you can get rid of the warning with manually created .env file:"<br/>          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"<br/>          echo<br/>        fi<br/>        one_meg=1048576<br/>        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))<br/>        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)<br/>        disk_available=$$(df / | tail -1 | awk '{print $$4}')<br/>        warning_resources="false"<br/>        if (( mem_available &lt; 4000 )) ; then<br/>          echo<br/>          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"<br/>          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"<br/>          echo<br/>          warning_resources="true"<br/>        fi<br/>        if (( cpus_available &lt; 2 )); then<br/>          echo<br/>          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"<br/>          echo "At least 2 CPUs recommended. You have $${cpus_available}"<br/>          echo<br/>          warning_resources="true"<br/>        fi<br/>        if (( disk_available &lt; one_meg * 10 )); then<br/>          echo<br/>          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"<br/>          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"<br/>          echo<br/>          warning_resources="true"<br/>        fi<br/>        if [[ $${warning_resources} == "true" ]]; then<br/>          echo<br/>          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"<br/>          echo "Please follow the instructions to increase amount of resources available:"<br/>          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"<br/>          echo<br/>        fi<br/>        mkdir -p /sources/logs /sources/dags /sources/plugins<br/>        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}<br/>        exec /entrypoint airflow version<br/>    # yamllint enable rule:line-length<br/>    environment:<br/>      &lt;&lt;: *airflow-common-env<br/>      _AIRFLOW_DB_UPGRADE: "true"<br/>      _AIRFLOW_WWW_USER_CREATE: "true"<br/>      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}<br/>      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}<br/>      _PIP_ADDITIONAL_REQUIREMENTS: ""<br/>    user: "0:0"<br/>    volumes:<br/>      - .:/sources<br/><br/>  airflow-cli:<br/>    &lt;&lt;: *airflow-common<br/>    profiles:<br/>      - debug<br/>    environment:<br/>      &lt;&lt;: *airflow-common-env<br/>      CONNECTION_CHECK_MAX_COUNT: "0"<br/>    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252<br/>    command:<br/>      - bash<br/>      - -c<br/>      - airflow<br/><br/>volumes:<br/>  postgres-db-volume:</span></pre><p id="4634" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编写了一个Airflow Dag脚本，该脚本读取数据，创建一个表，并将提取的数据加载到创建的postgres表中，该脚本使用python操作符SQL Alchemy引擎将数据加载到docker映像上的Postgres数据库中。</p><pre class="js jt ju jv fd kh ki kj bn kk kl bi"><span id="3fb9" class="km kn hh ki b be ko kp l kq kr">import airflow<br/>import os,sys<br/>import pandas as pd<br/><br/>from sqlalchemy import create_engine<br/>from datetime import timedelta,datetime<br/><br/>from airflow import DAG<br/>from airflow.operators.python import PythonOperator<br/><br/>sys.path.append(os.path.abspath(".."))<br/><br/><br/>default_args = {<br/>    'owner':'nahom',<br/>    'retries':5,<br/>    'retry_delay':timedelta(minutes=2)<br/>}<br/><br/>def load_clean_data(path, table_name):<br/>    """task to load clean data to postgres database<br/><br/>    Args:<br/>        path (str): path to clean data <br/>        table_name (str): database table name<br/>    """<br/>    print("writing data..............")<br/>    engine = create_engine("postgresql+psycopg2://airflow:airflow@postgres/airflow", echo=True)<br/>    df = pd.read_csv(path, sep=",", index_col=False)<br/><br/>    df.to_sql(table_name, con=engine, if_exists='replace',index_label='id')<br/>    print("!!!!Done!!!!")<br/>    <br/>def clean_df(path):<br/>    """function to clean raw data <br/><br/>    Args:<br/>        path (str): path of the data to be cleaned<br/>    """<br/>    <br/>    df = pd.read_csv(path,sep='[;:]',index_col=False)<br/>    print('data loaded successfully!!!')<br/>    df.columns = df.columns.str.replace(' ','')<br/>    try:<br/>        df.to_csv('./data/cleanned_data.csv',index=False)<br/>    except Exception as e:<br/>        print(f'error: {e}')<br/><br/>with DAG(<br/>    dag_id='load_to_dwh_v2',<br/>    default_args=default_args,<br/>    description='This loads cleaned data to the postgres warehouse on railway',<br/>    start_date=airflow.utils.dates.days_ago(1),<br/>    schedule_interval='@once'<br/>)as dag:<br/>    task1 = PythonOperator(<br/>        task_id='load_clean_data',<br/>        python_callable=load_clean_data,<br/>        op_kwargs={<br/>            "path": "./data/cleanned_data.csv",<br/>            "table_name":"traffic_raw"<br/>        }<br/>    )<br/>    task_2 = PythonOperator(<br/>        task_id='extract_and_clean_data',<br/>        python_callable=clean_df,<br/>        op_kwargs={<br/>            "path":"./data/drone3.csv"}<br/>    )<br/>    task_2&gt;&gt;task1</span></pre><p id="93ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编写完脚本后，我们启动docker来运行dag并执行任务。从airflow目录运行以下命令:</p><pre class="js jt ju jv fd kh ki kj bn kk kl bi"><span id="9b27" class="km kn hh ki b be ko kp l kq kr">docker-compose up</span></pre><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es lr"><img src="../Images/2b418a020951e6d67c7e5acaf615117d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4c3zTtitWLR3TH4WPF-yYw.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx">Dag graph</figcaption></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ls"><img src="../Images/c80d7fc0211941a8ab758bed28e4b5ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-dkf1P0l2SubNf8HiPI5w.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx">partial table image from the docker postgres database</figcaption></figure><h2 id="b028" class="ks kn hh bd kt ku kv kw kx ky kz la lb ip lc ld le it lf lg lh ix li lj lk ll bi translated"><strong class="ak"> DBT转型</strong></h2><p id="f601" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">DBT在英语教学中做测试。它是一个数据工程工具，有助于构建可用于仓库数据转换的相互依赖的SQL模型。</p><p id="f5b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">dbt项目是一个目录，包含。sql和。yml文件。最少需要的文件是:</p><ul class=""><li id="3ac3" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">名为dbt_project.yml的项目文件:该文件包含dbt项目的配置。</li><li id="6032" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">型号(。sql文件):dbt中的模型只是一个简单的。包含单个select语句的sql文件。</li></ul><p id="524f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个dbt项目都需要一个dbt_project.yml文件——这是dbt知道一个目录是dbt项目的方式。它还包含告诉dbt如何操作您的项目的重要信息。</p><p id="1580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一些用于创建转换的dbt命令</p><ul class=""><li id="a2f3" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><em class="lt"> dbt init — </em>初始化我们项目中的dbt</li><li id="e753" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><em class="lt"> dbt调试— </em>调试dbt连接</li><li id="915a" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><em class="lt"> dbt运行— </em>执行编译后的SQL语句</li><li id="47c8" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><em class="lt"> dbt测试— </em>测试查询。</li><li id="3e45" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><em class="lt"> dbt文档生成</em> e和<em class="lt"> dbt文档服务— </em>服务于文档并将其部署在本地服务器上。</li></ul><p id="b8a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是一些dbt SQL语句，用于生成转换，如数据点计数、按车型列出的平均速度和按车型列出的平均行驶距离。它们存储在不同的。sql文件。但首先，我们需要创建Postgres数据库的实例。</p><pre class="js jt ju jv fd kh ki kj bn kk kl bi"><span id="bc7e" class="km kn hh ki b be ko kp l kq kr"># casting database feature and selecting from traffic_raw table<br/>{{ config(materialized='table') }}<br/>with traffic_model as (<br/>  select * from traffic_raw<br/>)<br/>select *<br/>from traffic_model<br/># average speed<br/>select type,AVG(avg_speed) as speed_avg from {{ ref('feature')}} group by type<br/># average distance traveled by vehicle type<br/>select type , AVG(traveled_d) as dist_avg<br/>from {{ ref('feature') }}<br/>group by type<br/># data point count grouped by vehicle type<br/>select type,COUNT(*) as data_count<br/>from {{ ref('feature')}}<br/>group by type</span></pre><p id="0499" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是dbt沿袭的外观:</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es lu"><img src="../Images/6af736ccda36fa6fb5653961f7e4f971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NEo_WTBYjUSFuXR7"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx">dbt linage graph</figcaption></figure><h2 id="5e47" class="ks kn hh bd kt ku kv kw kx ky kz la lb ip lc ld le it lf lg lh ix li lj lk ll bi translated"><strong class="ak">用于可视化仓库数据的红杉</strong></h2><p id="a6d0" class="pw-post-body-paragraph ie if hh ig b ih lm ij ik il ln in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">Redash是一个数据工程工具，用于清除数据库表和可视化结果。这些结果可以通过redash为您自动创建的仪表板查看。我们将使用红灰执行以下任务。</p><ul class=""><li id="bbb9" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">将redash与PostgreSQL仓储工具连接起来，以便引入数据和dbt转换模型。</li><li id="6e4c" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">编写查询以使用业务特定的筛选器分析数据。这个查询可以直接在资料来源上执行。</li><li id="6745" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">构建Dashboard，使用不同类型的图表创建强大的可视化效果，并为用户自定义提供更多控制。</li><li id="0a84" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">创建查询并保存它们以供以后重用，因为相同的查询可以作为仪表板的输入。</li><li id="a959" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">使用UI轻松安排查询。</li><li id="4cbe" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">针对数据中的任何差异创建警报。我们将通过电子邮件、slack或webhook提醒用户。</li></ul><p id="cf2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是执行上述任务后生成的Redash控制面板。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es lv"><img src="../Images/8b0136d0bcf439f9d1e423911c52b53d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQYaBbef7yTYH-NBOxC_uw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es lw"><img src="../Images/396385545a196335dcb6c2d74ee2ba2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5a4ERcvjgsY2iO31hR2XA.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx">Redash Dashboard on the dbt transformed data</figcaption></figure><p id="20ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考</strong></p><ul class=""><li id="7e48" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><a class="ae jq" href="https://www.kdnuggets.com/2021/07/dbt-data-transformation-tutorial.html" rel="noopener ugc nofollow" target="_blank">https://www . kdkings . com/2021/07/dbt-data-transformation-tutorial . html</a></li><li id="aa23" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae jq" rel="noopener" href="/jakartasmartcity/data-pipeline-using-apache-airflow-to-import-data-from-public-api-7ff719118ac8">https://media . com/Jakarta smartcity/data-pipeline-use-Apache-airflow-to-import-data-from-public-API-7ff 719118 AC 8</a></li><li id="6ede" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated"><a class="ae jq" href="https://redash.io/help/user-guide/getting-started" rel="noopener ugc nofollow" target="_blank">https://redash.io/help/user-guide/getting-started</a></li></ul><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如何成为Mlearning.ai的作者</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo kb ma"/></div></div></a></div></div></div>    
</body>
</html>