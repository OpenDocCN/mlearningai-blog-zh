# 女权主义能为 AI 伦理做些什么？

> 原文：<https://medium.com/mlearning-ai/what-can-feminism-do-for-ai-ethics-b7e401889441?source=collection_archive---------7----------------------->

尽管在工业界和学术界进行了广泛的讨论，但人工智能伦理领域在现实世界中的影响有限。女权主义可能正是理论和实践之间缺失的一环。

![](img/eba9e6c3ff052a2d93c1da4f89b33d44.png)

Photo by [Ece AK](https://www.pexels.com/@airwanderers?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/person-wearing-black-t-shirt-3130372/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)

人工智能或机器学习道德准则和承诺比比皆是，你可以在工业界和学术界找到大量的这些准则和承诺，但研究表明，它们在很大程度上是雄心勃勃的和抽象的。

人工智能伦理原则和实践之间有很大差距。

2020 年末，[爱丽丝·威特](https://www.rmit.edu.au/contact/staff-contacts/academic-staff/w/witt-dr-alice)博士和我发表了我们关于*女权主义数据护理伦理*框架如何帮助填补这一空白的建议。这篇论文发表在第一个星期一，并且可以公开阅读。

TLDR 是，除非所有利益相关者，特别是男性(与女性和其他边缘化人群一起工作)，在整个机器学习经济中承担更大的道德工作责任，否则人工智能道德不太可能以任何真正和有意义的方式实现。

**一个女权主义者的数据关怀伦理**

女权主义没有被普遍接受的定义，但从广义上来说，它描述了对妇女(包括那些被认为是单性化、变性和非二元性的妇女)的权利、机会、责任和结果平等的信念，以及对一种异质父权制现状的拒绝。

关怀伦理学是一种女权主义的伦理学方法。

与呼吁逻辑、理性和客观性来制定伦理规则的主流伦理理论不同，在关怀伦理框架内，还有一个角色是(你猜对了)*关怀*。

除了逻辑和理性，考虑个人的情况和他们在世界、人际关系、社会和环境中的地位也有作用。

在我们的论文中，我们为在机器学习经济中工作的人提出了一个特定的护理伦理模型，该模型来自护理伦理奖学金和其他女权主义文献的数据。

从根本上说，我们认为机器学习不能被视为一种客观的、自主的技术现象，而是体现在*中的*。

我们接受了唐娜·哈拉威的领导，并“从一个整体来看待机器学习，这个整体总是一个复杂的、矛盾的、结构化的整体，而不是从上面、从无处、从简单的角度来看待机器学习”。

机器学习来源于并依附于现实的人、生活和社会。

它也是由对数据、模型和应用做出决策的真实的人创造的——这些决策可能将个人和系统的偏见嵌入重要的社会技术系统。

到目前为止，所以*没有*争议，对吗？

大多数人都会同意，机器学习系统的输入不是中性的。

大多数人也会坦率地承认，机器学习系统会产生危害，尤其是对有色人种和女性。

机器学习支持的皮肤癌检测系统对肤色[较深](https://www.media.mit.edu/publications/full-gender-shades-thesis-17/)的人效果不太好。

谷歌的图像识别软件曾经将[黑人归类为大猩猩](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/)。

累犯风险软件已经使用[种族](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)来预测人们很可能在未来犯罪，而不管个人犯罪记录如何。

模型显示男性比女性更适合当领导，因为男性在数据中所占比例过高。

搜索引擎复制了有害的性别和种族刻板印象。

身体扫描系统对于不符合白人[异质标准](https://direct.mit.edu/books/book/4605/Design-JusticeCommunity-Led-Practices-to-Build-the)的身体效果不佳。

人们发现一个单词联想系统将“家庭主妇”与女性联系在一起，将“电脑程序员”与男性联系在一起。

你明白了。

当谈到偏见和伤害时，随着机器学习技术越来越多地被用于支持教育、银行、就业、住房、执法和社会保障领域的决策，赌注不断上升。

人工智能伦理文件的激增在一定程度上是对这些明显问题的回应。

但是，为解决和防止伤害而做出的过程和承诺迄今为止未能在机器学习经济中产生有意义的变化。目前的方法不起作用。

在我们的论文中，我们提出了一种新的人工智能伦理方法，一种*女权主义者* *关心机器学习的数据伦理*，具有 5 个关键原则:

1.  确保机器学习经济中的多样化代表和参与；
2.  批判性地评估定位；
3.  在机器学习管道的每个阶段以人类为中心；
4.  实施透明度和问责制措施；和
5.  公平分配责任，以实施女性数据护理伦理实践。

你可以在我们的文章中读到关于这个框架的所有内容，但这是最后一点需要立即关注的。

**在整个 Brotopia 分配道德工作的责任**

在实践中实现人工智能道德需要在机器学习经济的所有参与者之间公平分配道德工作:从微观工作者到首席执行官。

在人工智能伦理文件中，*责任*经常被提及，但它往往没有得到很好的定义，并且经常与抽象的问题有关，例如人工智能本身是否可以成为一个法律主体，从而对自动决策负责。

思考起来很有趣，但是那种抽象的思考不能代替将真正的责任分配给真正的决策者。这是当前人工智能伦理学方法的一个主要缺陷。

要改变现状，责任必须分配给真正的人，而且必须是广泛的。

所有参与机器学习经济的人都必须感到有能力并对道德行为负责。

当一个组织内实施道德原则的责任不广泛时，该组织的道德、技术和经济目标就不太可能趋同。

2021 年谷歌的 Timnit Gebru 的例子清楚地表明了这个问题。

Gebru 是公司内部一个人工智能伦理研究小组的高调领导人，谷歌高管要求她撤回她撰写的一篇警告新人工智能系统问题的研究论文。

在争议发生时，谷歌正在使用该系统作为其搜索引擎的一部分。

Gebru 最终离开了谷歌，双方都卷入了一场引人注目的公开纠纷，似乎没有调和道德和商业的必要性。

为了从伦理原则转向伦理实践，解决和防止伤害的责任必须在整个组织和整个机器学习经济中更加公平地分配。

在目前的条件下，数据科学领域被一个男性的[家长制阶层](https://www.amazon.com.au/Brotopia-Breaking-Boys-Silicon-Valley/dp/0735213534)所主导，这意味着*更多的男性需要承担更多的日常伦理工作。*

这可能首先需要机器学习经济的参与者改变意识。一种使他们能够成为自主的道德决策者，负责防止对个人、社会和环境的伤害。

> 我们需要支持公平分配人工智能伦理运作责任的组织文化。

一旦在机器学习经济中工作的真实的人被授权或激励成为负责任的决策者，他们就可以获得一系列技术、社会和文化解决方案来操作人工智能伦理。在这方面有很多很好的研究。

我们迫切需要的是从事这项技术工作的人改变文化，并在整个机器学习经济中建立护理能力。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)