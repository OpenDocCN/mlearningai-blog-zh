# ä¸€å¹…å›¾åƒç›¸å½“äº 16x16 ä¸ªå­—:å¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„å˜å½¢é‡‘åˆš

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-51f3561a9f96?source=collection_archive---------0----------------------->

![](img/d8c44e78df1a991584d9700163941fc0.png)

ViT architecture presented in the paper

è¿™æ˜¯è°·æ­Œç ”ç©¶çš„ä¸€ç¯‡è®ºæ–‡ã€‚æœ¬æ–‡çš„ä¸»è¦è§‚ç‚¹æ˜¯

> ç›´æ¥åº”ç”¨äºå›¾åƒè¡¥ä¸å¹¶åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„è½¬æ¢å™¨åœ¨å›¾åƒåˆ†ç±»æ–¹é¢éå¸¸æœ‰æ•ˆã€‚

åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºè§†è§‰å˜å‹å™¨(ViT)æ¶æ„ä»¥åŠè®ºæ–‡ä¸­å‘è¡¨çš„ç»“æœã€‚

é€šè¿‡ä½¿ç”¨[è¿™å°](https://github.com/souvik3333/medium_blogs/blob/main/transformers/ViT/ViT.ipynb)ç¬”è®°æœ¬æ¥å°è¯•è¿™é‡Œä½¿ç”¨çš„å®ç°ã€‚ç‚¹å‡»`Open in Colab`ç›´æ¥åœ¨ Colab ä¸Šè¿è¡Œã€‚

# ViT æ¶æ„ğŸƒ

*   ViT å°†å›¾åƒåˆ†å‰²æˆå›ºå®šæ•°é‡çš„é¢ç‰‡ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥åˆ›å»ºåµŒå…¥ï¼Œå¹¶å°†å®ƒä»¬é€šè¿‡æ ‡å‡†çš„ transformer ç¼–ç å™¨ã€‚
*   æˆ‘ä»¬å…ˆè®¨è®ºä¸€ä¸‹ NLP å˜å‹å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œç„¶åå†å’Œ ViT è¿›è¡Œæ¯”è¾ƒå’Œè®¨è®ºã€‚

## NLP å˜å‹å™¨ç¼–ç å™¨ğŸ¨

![](img/a02f7450904b6aa697f46becf091f549.png)

Transformer encoder

*   æˆ‘ä»¬æŠŠä¸€ä¸ªå¥å­ä½œä¸ºè¾“å…¥ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å‘é€æ•´ä¸ªå¥å­ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ª`tokenizer`ç»™æ¯ä¸ªå•è¯ä¸€ä¸ª`id`ã€‚ç°åœ¨ï¼Œåˆ†è¯å™¨å®é™…ä¸Šä¸å¿…åƒæŒ‰å•è¯æ‹†åˆ†é‚£æ ·ç®€å•åœ°è¿›è¡Œæ‹†åˆ†ï¼Œè€Œæ˜¯å¯ä»¥å°†å•è¯æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†å¹¶è¿›è¡Œèµ‹å€¼ã€‚è¿™ä¸ªè¦çœ‹åˆ†è¯å™¨æ€ä¹ˆè®­ç»ƒäº†ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„è®°å·èµ‹äºˆå™¨å¯ä»¥åšå¦‚ä¸‹å·¥ä½œ

ä½†æ˜¯å¦ä¸€ä¸ªäººæŠŠå¥å­æ‹†æˆäº†ä¸‹é¢è¿™æ ·:

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå°†æ‰€æœ‰å¯èƒ½çš„ id æ˜ å°„åˆ°å‘é‡è¡¨ç¤ºçš„çŸ©é˜µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ç¬¬äºŒä¸ªè®°å·èµ‹äºˆå™¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ä¸º`To`é€‰æ‹©åµŒå…¥ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ç´¢å¼• 11 å¤„çš„å‘é‡è¡¨ç¤ºã€‚

è¿™äº›åµŒå…¥å¯ä»¥åœ¨å¼€å§‹æ—¶éšæœºåˆå§‹åŒ–ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒä¸­å­¦ä¹ ã€‚

## ä½ç½®åµŒå…¥

*   é€’å½’ç¥ç»ç½‘ç»œ(RNNs)ä»¥è¿ç»­çš„æ–¹å¼é€å­—åˆ†æå¥å­ã€‚ä½†æ˜¯å˜å‹å™¨æ¶æ„ä¸ä½¿ç”¨é€’å½’æœºåˆ¶ï¼Œè€Œæ˜¯æ”¯æŒå¤šå¤´è‡ªå…³æ³¨æœºåˆ¶ã€‚è¿™å‡å°‘äº†å˜å½¢é‡‘åˆšçš„è®­ç»ƒæ—¶é—´ï¼Œä½†æ˜¯æ¨¡å‹ä¸çŸ¥é“å•è¯çš„ä½ç½®ã€‚
*   ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘è¾“å…¥åµŒå…¥æ·»åŠ äº†ä¸€æ¡é¢å¤–çš„ä¿¡æ¯(ä½ç½®ç¼–ç )ã€‚
*   ç°åœ¨ä¸€ä¸ªç®€å•çš„æ–¹æ³•å°±æ˜¯ç»™ç¬¬ä¸€ä¸ªå•è¯èµ‹å€¼ 1ï¼Œç»™ç¬¬äºŒä¸ªå•è¯èµ‹å€¼ 2ï¼Œä¾æ­¤ç±»æ¨ã€‚ä½†åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªæ¯”å®ƒåœ¨è®­ç»ƒä¸­çœ‹åˆ°çš„ä»»ä½•å¥å­éƒ½é•¿çš„å¥å­ã€‚åŒæ ·ï¼Œå¯¹äºä¸€ä¸ªè¾ƒé•¿çš„å¥å­ï¼Œä¼šæœ‰è¾ƒå¤§çš„å€¼éœ€è¦æ·»åŠ ï¼Œè¿™ä¼šå ç”¨æ›´å¤šçš„å†…å­˜ã€‚
*   æˆ‘ä»¬å¯ä»¥å–ä¸€ä¸ªèŒƒå›´ï¼Œæ¯”å¦‚ç¬¬ä¸€ä¸ªä½œå“åŠ  0ï¼Œæœ€åä¸€ä¸ªåŠ  1ï¼Œåœ¨ä¸¤è€…ä¹‹é—´æˆ‘ä»¬åˆ†å‰²èŒƒå›´[0ï¼Œ1]å¹¶å¾—åˆ°å€¼ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ª 3 ä¸ªå•è¯çš„å¥å­ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ç¬¬ä¸€ä¸ªå•è¯å– 0ï¼Œå¯¹ç¬¬äºŒä¸ªå•è¯å– 0.5ï¼Œå¯¹ç¬¬ä¸‰ä¸ªå•è¯å– 1ï¼›å¯¹äºä¸€ä¸ª 4 ä¸ªå•è¯çš„å¥å­ï¼Œå®ƒå°†åˆ†åˆ«æ˜¯ 0ï¼Œ0.33ï¼Œ0.66ï¼Œ1ã€‚è¿™æ ·çš„é—®é¢˜æ˜¯ä½ç½®å·®Î´ä¸æ˜¯å¸¸æ•°ã€‚åœ¨ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼Œå®ƒæ˜¯ 0.5ï¼Œä½†æ˜¯åœ¨ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼Œå®ƒæ˜¯ 0.33ã€‚
*   ä½¿ç”¨çš„ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ª d ç»´å‘é‡ã€‚

*   è¿™å°±æ˜¯æ‰€æœ‰äº‹ç‰©çš„ç»“åˆ

*   æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ä¼ é€’è¿™äº›å‘é‡ã€‚

## å¤šå¤´æ³¨æ„åŠ›ğŸ”¥

*   å¤šå¤´æ³¨æ„åŠ›æœ‰ä¸‰ä¸ªçŸ©é˜µï¼Œåˆ†åˆ«æ˜¯æŸ¥è¯¢(Q)ã€é”®(K)ã€å€¼(V)çŸ©é˜µã€‚å®ƒä»¬ä¸­çš„æ¯ä¸€ä¸ªéƒ½å…·æœ‰ä¸åµŒå…¥ç›¸åŒçš„ç»´æ•°ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ‰€æœ‰ 3 ä¸ªçŸ©é˜µéƒ½æ˜¯ 512x512
*   å¯¹äºæ¯ä¸ªä»¤ç‰ŒåµŒå…¥ï¼Œæˆ‘ä»¬å°†å…¶ä¸æ‰€æœ‰ä¸‰ä¸ªçŸ©é˜µ(Qï¼ŒKï¼ŒV)ç›¸ä¹˜ã€‚å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªä»¤ç‰Œï¼Œæˆ‘ä»¬å°†æœ‰ 3 ä¸ªé•¿åº¦ä¸º 512 çš„ä¸­é—´å‘é‡ã€‚
*   ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æœ‰`n` ä¸ªå¤´ï¼Œæˆ‘ä»¬æŠŠæ¯ä¸ªå‘é‡åˆ†æˆ`n`ä¸ªéƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ 8 ä¸ªå¤´ï¼Œå¯¹äºå•è¯`Today`ï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰ 3 ä¸ªä¸­é—´å‘é‡åˆ†æˆç»´æ•°ä¸º 64 çš„å°å‘é‡ã€‚
*   ç„¶åï¼Œæ¯ä¸ªå¤´ä»æ‰€æœ‰ä¸­é—´å‘é‡ä¸­å–å‡ºå…¶å¯¹åº”çš„æ®µã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªå¤´å°†å–å¾—æ‰€æœ‰äº”ä¸ªåµŒå…¥(å¯¹åº”äºäº”ä¸ªä»¤ç‰Œ)çš„æ‰€æœ‰ä¸‰ä¸ªä¸­é—´å‘é‡(å¯¹åº”äºæŸ¥è¯¢ã€å¯†é’¥ã€å€¼ä¹˜æ³•ç»“æœ)çš„ç¬¬ä¸€ä¸ªåˆ†è£‚(ç»´åº¦ 64)ã€‚ç±»ä¼¼åœ°ï¼Œç¬¬äºŒä¸ªå¤´å°†å–ç¬¬äºŒä¸ªæ®µï¼Œä¾æ­¤ç±»æ¨ã€‚
*   åœ¨æ¯ä¸ªæ ‡é¢˜ä¸­ï¼Œæˆ‘ä»¬ç‚¹ç§¯æŸ¥è¯¢å’Œé”®çŸ©é˜µç›¸ä¹˜çš„å‘é‡ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬åœ¨ q1 å’Œæ‰€æœ‰å…³é”®çŸ©é˜µç›¸ä¹˜çš„å‘é‡(k{i}ï¼Œi in [1ï¼Œ5])ä¹‹é—´åšç‚¹ç§¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å®ƒä¹˜ä»¥ç›¸åº”çš„å€¼å‘é‡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å®ƒä»¬ç›¸åŠ ä»¥åˆ›å»ºä¸€ä¸ªç»“æœ 64 ç»´å‘é‡ã€‚è¿™å‘ç”Ÿåœ¨ q2ã€q3ã€q4ã€q5ï¼Œæœ€åï¼Œæˆ‘ä»¬å¾—åˆ°ç»´æ•°ä¸º 64 çš„ 5 ä¸ªå‘é‡ã€‚ç°åœ¨åŸºæœ¬ä¸Šæ¯ä¸ªç»“æœå‘é‡éƒ½æœ‰æ‰€æœ‰å…¶ä»–å‘é‡çš„ä¿¡æ¯ã€‚

![](img/43f19d1b3b883ba9e605843162c22e49.png)

Attention logic

*   ç°åœ¨æˆ‘ä»¬è¿æ¥æ‰€æœ‰å¤´éƒ¨çš„ç»“æœå‘é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¿æ¥æ‰€æœ‰ 8 ä¸ªå¤´çš„ç¬¬ä¸€ä¸ªç»“æœå‘é‡ï¼Œä»¥åˆ›å»º 512 dim ç¬¬ä¸€ä¸ªå‘é‡ã€‚å¯¹äºæ‰€æœ‰å…¶ä»– 4 ä¸ªå‘é‡ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
*   æœ€åæˆ‘ä»¬æœ‰ 5 ä¸ªå‘é‡ï¼Œæ¯ä¸ªéƒ½æœ‰ 512 ä¸ªç»´åº¦ã€‚

## æ·»åŠ &è¯ºå§†â˜¯

*   è¿™äº›æ˜¯æ­£å¸¸çš„æ‰¹å¤„ç†è§„èŒƒåŒ–å’Œå‰©ä½™è¿æ¥ï¼Œå¦‚ Resnet å—ã€‚

## å‰é¦ˆğŸ€

*   è¿™äº›æ˜¯ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œåº”ç”¨äºæ¯ä¸ªæ³¨æ„åŠ›å‘é‡ã€‚

è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„å˜å‹å™¨ç¼–ç å™¨çš„å·¥ä½œåŸç†ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹ ViT æ¶æ„ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°å¦‚ä½•å®ç°ã€‚

## ViT ç¼–ç å™¨æ¶æ„

**åµŒå…¥è¡¥ä¸çš„â˜‘ï¸**

*   ä¸ºäº†å¤„ç† 2D å›¾åƒï¼Œå›¾åƒè¢«åˆ†æˆå‡ ä¸ªå°å—ã€‚æˆ‘ä»¬å°† 2D é¢ç‰‡å±•å¹³æˆ 1D çŸ¢é‡ã€‚
*   ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›å‘é‡åµŒå…¥åˆ°æ¨¡å‹ç»´åº¦ç©ºé—´ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†æ¯ä¸ªå‘é‡è½¬æ¢ä¸º 768 ç»´å‘é‡ã€‚

```
import torch
import torch.nn as nn
in_chans = 3 #RGB
embed_dim = 768 # vector dimension in model space
patch_size = 16 # each image patch size 16*16
proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) # this will create the patch in image
img = torch.randn(1, 3, 224,224) # dummy image
x = proj(img).flatten(2).transpose(1, 2) # BCHW -> BNC
print(x.shape)
```

*   åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æ‹æ‘„äº†å¤§å°ä¸º 224*224 çš„å›¾åƒï¼Œå¹¶å‡è®¾æ¯ä¸ªé¢ç‰‡çš„å¤§å°ä¸º 16x16ã€‚
*   ç°åœ¨è¿™å°†å¯¼è‡´æ€»å…±(224/16** 224/16)= 14 **14 = 196 ä¸ªå‘é‡ã€‚
*   è¿™äº›å‘é‡çš„å¤§å°éƒ½æ˜¯ 16*16 = 256ã€‚ä½†æ˜¯ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹ç»´åº¦ï¼Œå³ 768ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨å·ç§¯ä¸­ä½¿ç”¨ 768 ä½œä¸ºè¾“å‡ºé€šé“ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å®ƒå±•å¹³ä¸º BNCï¼Œå…¶ä¸­ B=æ‰¹æ¬¡ï¼ŒN=ç”Ÿæˆçš„é¢ç‰‡ï¼ŒC =æ¨¡å‹ç©ºé—´ä¸­çš„å‘é‡ç»´åº¦ã€‚

**ç±»åµŒå…¥ğŸ†•**

*   ViT å°†ä¸€ä¸ªå¯å­¦ä¹ çš„åµŒå…¥é™„åŠ åˆ°åµŒå…¥è¡¥ä¸åºåˆ—ä¸­ã€‚

```
cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # create class embeddings without batch
cls_token = cls_token.expand(x.shape[0], -1, -1) # add batch
x = torch.cat((cls_token, x), dim=1) # append class token with linear proj embeddings
x.shape # 196 -> 197
```

**ä½ç½®åµŒå…¥â˜‘ï¸**

*   æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç»´æ•°ä¸º(num _ patches+1)** embed _ dim(197 **768)çŸ©é˜µã€‚è¿™äº›å€¼æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°çš„ã€‚

```
num_patches = 14*14
pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) # +1 for class token
x = x + pos_embed # add position encoding
x.shape
```

**è¡—åŒºâ˜‘ï¸**

*   åŸºäºè¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬æœ‰`n`ä¸ªå—ã€‚
*   æ¯ä¸ªåŒºå—éƒ½ä¸€æ ·ã€‚æ¯ä¸€å±‚åŒ…æ‹¬ä¸€ä¸ªæ³¨æ„å±‚å’Œä¸€ä¸ª MLP å±‚ã€‚

**å…³æ³¨å±‚â˜‘ï¸**

*   ä¸ä¹‹å‰è§£é‡Šçš„ NLP æ³¨æ„åŠ›ç›¸åŒã€‚
*   è®©æˆ‘ä»¬åˆ›å»ºä¸­é—´å‘é‡ã€‚

```
# Transformation from source vector to query vector
fc_q = nn.Linear(embed_dim, embed_dim)
# Transformation from source vector to key vector
fc_k = nn.Linear(embed_dim, embed_dim)
# Transformation from source vector to value vector
fc_v = nn.Linear(embed_dim, embed_dim)Q = fc_q(x)
K = fc_k(x)
V = fc_v(x)print(Q.shape, K.shape, V.shape)
```

*   åˆ†å‰²ä¸­é—´å‘é‡ä»¥åœ¨æ¯ä¸ªå¤´ä¸­å¤„ç†ä¸€ä¸ªé›¶ä»¶ã€‚

```
num_heads = 8
batch_size = 1
Q = Q.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the Q matrix for 8 head
K = K.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the K matrix for 8 head
V = V.view(batch_size, -1, num_heads, embed_dim//num_heads).permute(0, 2, 1, 3) # split the V matrix for 8 head
print(Q.shape, K.shape, V.shape) # batch_size, num_head, num_patch+1, feature_vec dim per head
```

*   æ³¨æ„åŠ›çŸ©é˜µä¹˜æ³•ã€‚

```
score = torch.matmul(Q, K.permute(0, 1, 3, 2)) # Q*k
score = torch.softmax(score, dim=-1)
score = torch.matmul(score, V) # normally we apply dropout layer before this
score.shape # batch_size, num_head, num_patches+1, feature_vector_per_head (embed_dim/num_head)
```

*   é‡å¡‘ç»“æœ

```
score = score.permute(0, 2, 1, 3).contiguous()
score.shape # batch_size, num_patches+1, num_head, feature_vector_per_head (embed_dim/num_head)
```

*   å°†å‘é‡åˆå¹¶å›å®ƒä»¬çš„åŸå§‹å½¢çŠ¶

```
score = score.view(batch_size, -1, embed_dim) # merge the vectors back to original shape
score.shape # batch_size, num_patches+1, embed_dim
```

**MLP è´Ÿè´£äººâ˜‘ï¸**

*   æ­£å¸¸å¤šå±‚æ„ŸçŸ¥å™¨ã€‚

```
act_layer=nn.GELU # activation function
in_features = embed_dim 
hidden_features = embed_dim * 4
out_features = in_features
fc1 = nn.Linear(in_features, hidden_features)
act = act_layer()
drop1 = nn.Dropout(0.5)
fc2 = nn.Linear(hidden_features, out_features)
drop2 = nn.Dropout(0.5)
```

*   ä» MLP å›¾å±‚ä¸­è·å–ç»“æœ

```
x = fc1(score)
x = act(x)
x = drop1(x)
x = fc2(x)
x = drop2(x)
x.shape
```

*   æ‹¿å‡º`cls`ä»¤ç‰Œç‰¹æ€§ã€‚

```
cls = x[:,0]
```

**é€‰ç²‰æœºæœºå¤´ğŸ†•**

*   åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨å¤´ï¼Œå¹¶ä¼ é€’ç±»ä»¤ç‰Œç‰¹å¾æ¥è·å¾—é¢„æµ‹ã€‚

```
num_classes = 10 # assume 10 class classification
head = nn.Linear(embed_dim, num_classes) 
pred = head(cls)
pred
```

# ç»“æœå‘è¡¨åœ¨è®ºæ–‡ä¸­ğŸ“ˆ

> å½“åœ¨æ²¡æœ‰å¼ºæ­£åˆ™åŒ–çš„ä¸­å‹æ•°æ®é›†(å¦‚ ImageNet)ä¸Šè®­ç»ƒæ—¶ï¼Œè¿™äº›æ¨¡å‹äº§ç”Ÿçš„é€‚åº¦ç²¾åº¦æ¯”å¯æ¯”å¤§å°çš„ ResNets ä½å‡ ä¸ªç™¾åˆ†ç‚¹ã€‚
> 
> å˜å‹å™¨ç¼ºä¹ CNN å›ºæœ‰çš„ä¸€äº›å½’çº³åå·®ï¼Œå¦‚ç¿»è¯‘ç­‰å˜å’Œå±€éƒ¨æ€§ï¼Œå› æ­¤åœ¨æ•°æ®é‡ä¸è¶³çš„æƒ…å†µä¸‹è®­ç»ƒæ—¶ä¸èƒ½å¾ˆå¥½åœ°æ¦‚æ‹¬ã€‚
> 
> ä½†æ˜¯ï¼Œå¦‚æœæ¨¡å‹æ˜¯åœ¨æ›´å¤§çš„æ•°æ®é›†(14M-300M å›¾åƒ)ä¸Šè®­ç»ƒçš„ï¼Œæƒ…å†µå°±ä¸åŒäº†ã€‚æˆ‘ä»¬å‘ç°å¤§è§„æ¨¡è®­ç»ƒèƒœè¿‡å½’çº³åå·®ã€‚

*   ä½œè€…æåˆ°ï¼Œå¯¹äºè¾ƒå°çš„é¢„è®­ç»ƒæ•°æ®é›†(ImageNet ), ViT-Large æ¨¡å‹çš„æ€§èƒ½ä½äº ViT-Base æ¨¡å‹ã€‚å¯¹äºå¤§å‹æ•°æ®é›†(JFT-300 ç±³)ï¼ŒViT-å¤§å‹æ¨¡å‹æ•ˆæœå¾ˆå¥½ã€‚
*   åœ¨ JFT-300M æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ Vision Transformer æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½ä¼˜äºåŸºäº ResNet çš„åŸºçº¿ï¼Œè€Œé¢„è®­ç»ƒæ‰€éœ€çš„è®¡ç®—èµ„æºå´å°‘å¾—å¤šã€‚
*   ä¸‹è¡¨æ˜¾ç¤ºäº†ç”¨ JFT-300M æ•°æ®é›†å’Œ ImageNet-21k æ•°æ®é›†é¢„å¤„ç†çš„ ViT çš„ç»“æœã€‚è¿™äº›åˆ—æ˜¾ç¤ºäº†ç”¨ä¸åŒæ•°æ®é›†é¢„å¤„ç†çš„å‡ ä¸ªæ¨¡å‹ã€‚è¿™äº›è¡Œæ˜¯ä¸‹æ¸¸ä»»åŠ¡ã€‚

![](img/2f153cd0efd8146a46558ae4b786e142.png)

Table 2 from the paper

# ç”¨ PyTorch é—ªç”µå’Œ timm è®­ç»ƒä¸€ä¸ªç®€å•çš„ ViTğŸ†

*   è¿™é‡Œè®©æˆ‘ä»¬ä½¿ç”¨ [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) å’Œ [timm](https://github.com/rwightman/pytorch-image-models) è®­ç»ƒä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨ã€‚

```
import timm
import torch
import pytorch_lightning as pl
import torchvision
import torchvision.transforms as transforms
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint
import torchmetricsseed_everything(42, workers=True)
```

*   è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„ lightning æ¨¡å‹ç±»ã€‚

```
class Model(pl.LightningModule):
    """
    Lightning model
    """
    def __init__(self, model_name, num_classes, lr = 0.001, max_iter=20):
        super().__init__()
        self.model = timm.create_model(model_name=model_name, pretrained=True, num_classes=num_classes)
        self.metric = torchmetrics.Accuracy()
        self.loss = torch.nn.CrossEntropyLoss()
        self.lr = lr
        self.max_iter = max_iter

    def forward(self, x):
        return self.model(x) def shared_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss(logits, y)
        preds = torch.argmax(logits, dim=1)
        self.metric(preds, y)

        return loss

    def training_step(self, batch, batch_idx):
        loss = self.shared_step(batch, batch_idx)
        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('train_acc', self.metric, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        loss = self.shared_step(batch, batch_idx)
        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('val_acc', self.metric, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)

        return [optim], [scheduler]
```

*   æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰è½¬æ¢å¹¶ä¸‹è½½å’ŒåŠ è½½ CIFAR10 æ•°æ®é›†ã€‚

```
transform = transforms.Compose(
    [transforms.Resize(224),
     transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])batch_size = 128trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=8)testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=8)classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

*   ç°åœ¨æˆ‘ä»¬å°†åˆå§‹åŒ–`Model`ç±»ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ ViT çš„ä¸€ä¸ªå˜ä½“ï¼Œå®ƒæ‹æ‘„å¤§å°ä¸º 224*224 çš„å›¾åƒï¼Œé¢ç‰‡å¤§å°ä¸º 16ã€‚

```
model = Model(model_name="vit_tiny_patch16_224", num_classes=len(classes), lr = 0.001, max_iter=10)
```

*   è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ£€æŸ¥ç‚¹å›è°ƒæ¥ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹ã€‚

```
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./checkpoints',
    filename='vit_tpytorch_lightning6_224-cifar10-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'
)
```

*   å¿«å¥½äº†ã€‚è®©æˆ‘ä»¬åˆ›å»ºæ•™ç»ƒã€‚

```
trainer = Trainer(
    deterministic=True, 
    logger=False, 
    callbacks=[checkpoint_callback], 
    gpus=[0], # change it based on gpu or cpu availability
    max_epochs=10, 
    stochastic_weight_avg=True)
```

*   æœ€åï¼Œè®©æˆ‘ä»¬è®­ç»ƒæ¨¡å‹ğŸ˜ƒ

```
trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=testloader)
```

# ç›¸å…³èµ„æº

*   [å®˜æ–¹ä»£å·](https://github.com/google-research/vision_transformer)
*   [è®ºæ–‡](https://arxiv.org/abs/2010.11929)
*   [ä»£ç ç¬”è®°æœ¬](https://github.com/souvik3333/medium_blogs/blob/main/transformers/ViT/ViT.ipynb)

ç‰¹åˆ«æ„Ÿè°¢ [Prakash Jay](https://medium.com/u/bea753eb5d92?source=post_page-----51f3561a9f96--------------------------------) æŒ‡å¯¼æˆ‘å®Œæˆè¿™ä¸ªé¡¹ç›®ã€‚

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)