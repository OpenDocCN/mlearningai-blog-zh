<html>
<head>
<title>NLP-Day 19: You Better Pay Attention To Transformers (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP-第19天:你最好关注变形金刚(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/nlp-day-19-you-better-pay-attention-to-transformers-part-1-3b1784b2a7ee?source=collection_archive---------3-----------------------#2022-04-25">https://medium.com/mlearning-ai/nlp-day-19-you-better-pay-attention-to-transformers-part-1-3b1784b2a7ee?source=collection_archive---------3-----------------------#2022-04-25</a></blockquote><div><div class="dt ha hb hc hd he"/><div class="hf hg hh hi hj"><h2 id="665f" class="hk hl hm bd b fq hn ho hp hq hr hs dy ht translated" aria-label="kicker paragraph"># 30日</h2><div class=""/><div class=""><h2 id="35d8" class="pw-subtitle-paragraph is hv hm bd b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj dy translated">引入注意力的概念</h2></div><figure class="jl jm jn jo fe jp es et paragraph-image"><div class="es et jk"><img src="../Images/4ac50a663d2808c1aa9deda7c3ef7cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*-t7TiS1BeSvycr52MIZGLw.png"/></div><figcaption class="js jt eu es et ju jv bd b be z dy">Transformer-based architectures #30DaysOfNLP [Image by Author]</figcaption></figure><p id="a579" class="pw-post-body-paragraph jw jx hm jy b jz ka iw kb kc kd iz ke kf kg kh ki kj kk kl km kn ko kp kq kr hf bi translated"><a class="ae ks" rel="noopener" href="/mlearning-ai/nlp-day-18-machine-translation-with-sequence-to-sequence-part-2-dc32dc0e7e1b"> <strong class="jy hw">在上一集</strong> </a>中，我们实现了一个序列到序列的机器翻译模型。我们通过在编码器-解码器结构中创建一个网络来做到这一点，该网络利用两个LSTMs，将一个序列映射到另一个序列。</p></div></div>    
</body>
</html>