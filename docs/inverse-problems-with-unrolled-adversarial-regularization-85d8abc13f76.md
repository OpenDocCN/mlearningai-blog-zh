# å…·æœ‰å±•å¼€çš„å¯¹ç«‹æ­£åˆ™åŒ–çš„åé—®é¢˜

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/inverse-problems-with-unrolled-adversarial-regularization-85d8abc13f76?source=collection_archive---------3----------------------->

## ç®—æ³•å±•å¼€çš„ç«¯åˆ°ç«¯é‡å»ºç¬¦åˆè®¡ç®—æœºè§†è§‰çš„æ•°æ®é©±åŠ¨æ­£åˆ™åŒ–

![](img/3ba9f57f778241f43735efc09e2cc7b2.png)

Photo by [Tyler Casey](https://unsplash.com/@tylercaseyprod?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/blur?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

é€†é—®é¢˜æ™®éå­˜åœ¨äºæˆåƒåº”ç”¨ä¸­ï¼Œå…¶ä¸­äººä»¬è¯•å›¾ä»å…¶ä¸å®Œæ•´å’Œæœ‰å™ªå£°çš„æµ‹é‡ä¸­æ¢å¤æœªçŸ¥çš„æ¨¡å‹å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè¿™å¯ä»¥åº”ç”¨äºç…§ç‰‡å»å™ªï¼Œä»¥æå–æ›´é«˜è´¨é‡çš„ä¿¡æ¯ã€‚æœ¬å¸–ä¸­è®¨è®ºçš„æ–¹æ³•ï¼Œå±•å¼€å¯¹æŠ—æ­£åˆ™åŒ–(UAR)ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æ·±å…¥ UAR ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ç®—æ³•å±•å¼€ï¼Œè¿™æ˜¯ UAR çš„ä¸€ä¸ªåŸºæœ¬æ„ä»¶ã€‚

# ç®—æ³•å±•å¼€

ç®—æ³•å±•å¼€çš„åŠ¨æœºæ˜¯æ‰¾åˆ°ä¼ ç»Ÿ**è¿­ä»£ç®—æ³•**å’Œæ•°æ®é©±åŠ¨æ·±åº¦**ç¥ç»ç½‘ç»œ**ä¹‹é—´çš„è”ç³»ã€‚åŸºæœ¬æ€æƒ³æ˜¯å°†ç®—æ³•ä¸­çš„æ¯æ¬¡è¿­ä»£**å»ºæ¨¡ä¸ºç½‘ç»œä¸­çš„ä¸€å±‚**ï¼Œç”±æ­¤è¿­ä»£ç®—æ³•å¯ä»¥å»ºæ¨¡ä¸ºè¿æ¥åœ¨ä¸€èµ·çš„å¤šä¸ªå±‚ã€‚å› æ­¤ï¼Œç©¿è¿‡ç½‘ç»œç›¸å½“äºæ‰§è¡Œè¿­ä»£ç®—æ³•æœ‰é™æ¬¡ã€‚å› æ­¤ï¼Œç®—æ³•å‚æ•°å°†è‡ªç„¶åœ°ç”±ç½‘ç»œå‚æ•°è¡¨ç¤ºï¼Œå¹¶ä¸”è¢«è®­ç»ƒçš„ç½‘ç»œå¯ä»¥è¢«è§£é‡Šä¸ºå‚æ•°ä¼˜åŒ–çš„ç®—æ³•ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†å¤§å¤šæ•°å¸¸è§„ç¥ç»ç½‘ç»œä¸­ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚****

**![](img/241605d9b2489af8dfae73ffc605800c.png)**

**Figure 1: A high-level overview of algorithm unrolling: given an iterative algorithm (left), a corresponding deep network (right) can be generated by cascading its iterations h. The iteration step h (left) is executed a number of times, resulting in the network layers h1, h2, â€¦ (right). Each iteration h depends on a set of algorithm parameters, which are transferred into the corresponding set of network parameters. Instead of determining these parameters through cross-validation or analytical derivations, we learn them from training datasets through end-to-end training. In this way, the resulting network could achieve better performance than the original iterative algorithm. In addition, the network layers naturally inherit interpretability from the iteration procedure. The learnable parameters are colored in blue. (Cited from the original paper)**

## **å­¦ä¹ è¿­ä»£æ”¶ç¼©å’Œé˜ˆå€¼ç®—æ³•**

**å›¾ 2 æ˜¾ç¤ºäº†ç®—æ³•å±•å¼€æ€æƒ³çš„**ç¤ºä¾‹**åº”ç”¨ã€‚ä¼ ç»Ÿçš„ç¨€ç–ç¼–ç ç®—æ³•ï¼Œè¿­ä»£æ”¶ç¼©å’Œé˜ˆå€¼ç®—æ³•(ISTA)ï¼Œè¢«å±•å¼€å¹¶ç”±æ·±åº¦ç¥ç»ç½‘ç»œè¡¨ç¤ºã€‚æ³¨æ„ï¼Œå±•å¼€ç®—æ³•é€šè¿‡ä¸åœ¨è¿­ä»£(å³å±‚)ä¹‹é—´å…±äº«æƒé‡æ¥ä¿®æ”¹åŸå§‹æ–¹æ³•ï¼Œå› æ­¤å®ƒè¢«ç§°ä¸ºå­¦ä¹ çš„ ISTAï¼Œæˆ– LISTAã€‚**

**![](img/47b0a651e50826386485a7a6fd42de5c.png)**

**Figure 2: Illustration of LISTA: one iteration of ISTA executes a linear and then a non-linear operation and thus can be recast into a network layer; by stacking the layers together a deep network is formed. The network is subsequently trained using paired inputs and outputs by back-propagation to optimize the parameters. Âµ is a constant parameter that controls the step size of each iteration. The trained network, dubbed LISTA, is computationally more efficient compared with the original ISTA. The trainable parameters in the network are colored in blue. (Cited from the original paper)**

# **å±•å¼€çš„å¯¹æŠ—æ€§æ­£åˆ™åŒ–**

**UAR è€ƒè™‘çš„é€†é—®é¢˜å¯ä»¥ç”¨ç­‰å¼ 1 æ¥æ¦‚æ‹¬ï¼Œå…¶ä¸­æ­£å‘ç®—å­`A`åœ¨æ²¡æœ‰å™ªå£°çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿæµ‹é‡è¿‡ç¨‹ï¼Œè€Œ`e`è¡¨ç¤ºæµ‹é‡å™ªå£°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿå°†æµ‹é‡å€¼`y`è½¬æ¢æˆæ½œåœ¨ä¿¡æ¯`x`çš„ä¼°è®¡å™¨ã€‚è¿™é‡Œé¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜æ˜¯ï¼Œåœ¨ç ”ç©¶æ•°æ®é›†ä¸­ï¼Œ`y`å’Œ`x`é€šå¸¸ä¸ä¼šæˆå¯¹å‡ºç°**ã€‚****

**![](img/12eca6057b62a4f1bcff5d21c578c765.png)**

**Equation 1: Formulation of the inverse problem.**

**åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè®­ç»ƒæ ·æœ¬ä¸æ ‡ç­¾çš„ä¸åŒ¹é…ï¼Œè®¸å¤šåˆ¤åˆ«æ–¹æ³•éƒ½æ— æ³•åº”ç”¨ã€‚å› æ­¤ï¼Œç¬”è€…åœ¨ç”Ÿæˆæ¨¡å‹æ–‡çŒ®ä¸­å€Ÿé‰´äº†ç›´è§‰ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒUAR ç”±ä¸€ä¸ªä»æµ‹é‡å€¼`y`ä¸­é‡å»ºæ½œåœ¨ä¿¡æ¯`x`çš„**é‡å»º**ç½‘ç»œ(ç”Ÿæˆå™¨)å’Œä¸€ä¸ªåŒºåˆ†é‡å»ºå›¾åƒå’Œåœ°é¢äº‹å®çš„**æ­£åˆ™åŒ–**ç½‘ç»œ(é‰´åˆ«å™¨)ç»„æˆã€‚**

**![](img/3bfbdd3b25f1f28eebe62a14e6b9eaf1.png)**

**Equation 2: Objective function for the reconstruction network. It consists of a reconstruction term, where the reconstructed image is compared with the input, and a regularization term, estimated by the regularization network on the reconstruction.**

**é‡å»ºç½‘ç»œçš„è®­ç»ƒç›®æ ‡å¦‚ç­‰å¼ 2 æ‰€ç¤ºã€‚æœŸæœ›ç®—å­ä¸­çš„ç¬¬ä¸€é¡¹å¯¹åº”äºé‡å»ºæŸå¤±ï¼Œè®¡ç®—ä¸ºé‡å»ºå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„è·ç¦»ã€‚ç¬¬äºŒé¡¹æ˜¯ç»™å®šé‡å»ºå›¾åƒçš„æ­£åˆ™åŒ–æŸå¤±ã€‚ä¸ºäº†äº§ç”Ÿæ›´å¥½çš„é‡å»ºï¼Œè¿™ä¸¤é¡¹éƒ½éœ€è¦æœ€å°åŒ–ã€‚æ³¨æ„ï¼Œè¿™ä¸ªç›®æ ‡æ ¹æœ¬æ²¡æœ‰åˆ©ç”¨åœ°é¢çœŸå®ï¼Œæ‰€ä»¥å®ƒå¯ä»¥ä»…ç”¨è¾“å…¥å›¾åƒæ¥è®­ç»ƒã€‚**

**![](img/af845b4393d3dc615882800a9aadef85.png)**

**Equation 3: Objective function for the regularization network. The two terms have different signs because we would like the regularizer to be able to distinguish ground truth from reconstruction.**

**æ­£åˆ™åŒ–ç½‘ç»œè´Ÿè´£åŒºåˆ†é‡å»ºå’Œåœ°é¢å®å†µï¼Œå› æ­¤å¦‚ç­‰å¼ 3 æ‰€ç¤ºï¼Œå®ƒæœ€å¤§åŒ–é‡å»ºå›¾åƒçš„æŸå¤±å€¼ï¼ŒåŒæ—¶æœ€å°åŒ–åœ°é¢å®å†µçš„æŸå¤±å€¼ã€‚**

**å°½ç®¡è¯¥æ¡†æ¶åœ¨ç”¨äºé‡å»ºå’Œæ­£åˆ™åŒ–ç½‘ç»œçš„æ¶æ„ä¸­æ˜¯é€šç”¨çš„ï¼Œä½†åœ¨[3]ä¸­ä½¿ç”¨äº†å±•å¼€çš„ç”Ÿæˆå™¨å’Œæ­£å¸¸çš„æ·±åº¦ç½‘ç»œï¼Œå› æ­¤åœ¨åç§° UAR ä¸­æ˜¯â€œå±•å¼€çš„â€ã€‚æ­¤å¤–ï¼Œä½œè€…åœ¨[3]ä¸­ä¸º UAR æä¾›äº†ä¸¥è°¨çš„ç†è®ºç»“æœï¼Œæˆ‘ä»¬è®¤ä¸ºéå¸¸å€¼å¾—ä¸€è¯»ã€‚**

# **ç»“è®º**

**æˆ‘ä»¬å›é¡¾äº†å›¾åƒå¤„ç†ä¸­é€†é—®é¢˜çš„å±•å¼€å¯¹æŠ—æ­£åˆ™åŒ–ã€‚å®ƒä» GANs ä¸­æ±²å–ç›´è§‰ï¼Œä¼˜é›…åœ°è§£å†³äº†ä¸æˆå¯¹è®­ç»ƒæ ·æœ¬çš„é—®é¢˜(åœ¨æŸç§ç¨‹åº¦ä¸Šç¼ºå¤±æ•°æ®)ï¼Œå¹¶è¶…è¶Šäº†è¯¥é¢†åŸŸä¸­æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚**

**[](https://github.com/Subhadip-1/unrolling_meets_data_driven_regularization) [## GitHub-Subhadip-1/unrolling _ meets _ data _ driven _ regulation:åŒ…å« python è„šæœ¬ï¼Œç”¨äºâ€¦

### åŒ…å« python è„šæœ¬ï¼Œç”¨äºå­¦ä¹ è¿­ä»£å±•å¼€é‡å»ºä»¥åŠæ•°æ®é©±åŠ¨çš„â€¦

github.com](https://github.com/Subhadip-1/unrolling_meets_data_driven_regularization)** 

**[1]è’™åŠ ã€ç»´æ²™å°”ã€æè·ƒé¾™å’Œçº¦å°¼å¨œÂ·cÂ·åŸƒå°”è¾¾ã€‚"ç®—æ³•å±•å¼€:ç”¨äºä¿¡å·å’Œå›¾åƒå¤„ç†çš„å¯è§£é‡Šçš„ã€æœ‰æ•ˆçš„æ·±åº¦å­¦ä¹ ." *IEEE ä¿¡å·å¤„ç†æ‚å¿—*38.2(2021):18â€“44ã€‚**

**[2]æ ¼é›·æˆˆå°”ï¼Œå‡¯ç½—å°”å’Œæ‰¬Â·å‹’æ˜†ã€‚"å­¦ä¹ ç¨€ç–ç¼–ç çš„å¿«é€Ÿè¿‘ä¼¼."*ç¬¬ 27 å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†*ã€‚2010.**

**[3] Mukherjeeï¼ŒSubhadipï¼Œç­‰äººï¼Œâ€œç«¯åˆ°ç«¯é‡å»ºæ»¡è¶³é€†é—®é¢˜çš„æ•°æ®é©±åŠ¨æ­£åˆ™åŒ–ã€‚â€*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•* 34 (2021)ã€‚**

**[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) 

ğŸ”µ [**æˆä¸ºä½œå®¶**](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)**