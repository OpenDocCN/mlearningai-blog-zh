<html>
<head>
<title>Modelling Memory Retention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模拟记忆保持</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/modelling-memory-retention-3f68d1bbe131?source=collection_archive---------3-----------------------#2022-11-12">https://medium.com/mlearning-ai/modelling-memory-retention-3f68d1bbe131?source=collection_archive---------3-----------------------#2022-11-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="09e8" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">贝叶斯指数衰减</h2></div><p id="8818" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇博文中，我将引用Lee &amp; Wagemakers所著的《<a class="ae js" href="https://bayesmodels.com/" rel="noopener ugc nofollow" target="_blank">贝叶斯认知建模</a>》一书中的一个例子。这本书充满了认知心理学的例子，可以用贝叶斯方法来处理。由于这些例子大多是通过<strong class="iy hi"> WinBUGS </strong>完成的，我想看看我是否能使用<em class="jt"> brms </em>和<em class="jt"> stan </em>来重现它。所以，这就是这篇博文的内容。(我知道这本书也提供了stan代码，但我想看看我能用<em class="jt"> brms </em>走多远，它的功能比base <strong class="iy hi"> STAN </strong>或<strong class="iy hi"> WinBUGS </strong>要少)。</p><p id="8657" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我关注的章节是记忆保持，这与记忆测试有关。基本上，给受试者提供刺激来记忆，我们观察他们记住了多少。关键在于提供刺激和保持之间的间隔。</p><p id="af91" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们加载库，数据，并绘制我们所得到的。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="c9c4" class="kd ke hh jz b be kf kg l kh ki">rm(list=ls())<br/>library(dplyr)<br/>library(ggplot2)<br/>library(brms)<br/>library(bayesplot)<br/>library(tidybayes)</span></pre><pre class="kj jy jz ka bn kb kc bi"><span id="d969" class="kd ke hh jz b be kf kg l kh ki">t     &lt;- c(1, 2, 4, 7, 12, 21, 35, 59, 99, 200)<br/>nt    &lt;- length(t)<br/>slist &lt;- 1:4<br/>ns    &lt;- length(slist)<br/>k &lt;- matrix(c(18, 18, 16, 13, 9, 6, 4, 4, 4, NA,<br/>              17, 13,  9,  6, 4, 4, 4, 4, 4, NA,<br/>              14, 10,  6,  4, 4, 4, 4, 4, 4, NA,<br/>              NA, NA, NA, NA,NA,NA,NA,NA,NA, NA), nrow=ns, <br/>            ncol=nt, <br/>            byrow=T)<br/>dimnames(k)&lt;-list(slist,t)<br/>k&lt;-as.data.frame(k)<br/>d &lt;- k<br/>subject &lt;- rownames(d)<br/>rownames(d) &lt;- NULL<br/>df &lt;- cbind(subject,d)<br/>df_long&lt;-df%&gt;%tidyr::pivot_longer(cols=`1`:`200`,<br/>                            names_to = "interval",<br/>                            values_to = "retention")<br/>str(df_long)<br/>df_long$interval&lt;-as.numeric(df_long$interval)<br/>df_long$N&lt;-18<br/>df_long$subject&lt;-as.numeric(df_long$subject)<br/>df_long_compl&lt;-df_long%&gt;%tidyr::drop_na()</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div class="er es kk"><img src="../Images/9240cbe125ade7c3bb973040ad339608.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*5KuoQ2MyPN_FNMb7_j9sbA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">The data in long format.</figcaption></figure><p id="16d5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于每个受试者，我们都有一个间隔，一个保持数和刺激数。你可以清楚地看到，间隔的增加会导致更少的保留。我还假设会有一个主题特定的效果。因此，这将使模型在本质上具有层次性。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="eb2d" class="kd ke hh jz b be kf kg l kh ki">df_long%&gt;%<br/>  arrange(subject, interval)%&gt;%<br/>  ggplot(., <br/>       aes(x=interval, <br/>           y=retention, <br/>           group=factor(subject), <br/>           color=factor(subject)))+<br/>  geom_point()+<br/>  geom_line()+<br/>  theme_bw()+<br/>  facet_wrap(~factor(subject))+<br/>  labs(col="Subject")</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/80c608acd39346a8e72b88497a3a52b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4uhHAwEPmp6cj9-KJxnGA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">As you can see, the number of stimuli retained decreases very fast.</figcaption></figure><p id="d8ea" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在书中，提供了一个非线性的平均值公式，我将把它复制到这里。通过nls函数，我可以从参数的系数开始，看看它们的拟合程度如何。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="72ed" class="kd ke hh jz b be kf kg l kh ki">try&lt;-nls(retention ~ exp(-a1 * interval) + b1, <br/>         data = df_long,<br/>         algorithm="plinear",<br/>         start = list(a1 = 0.52, <br/>                      b1 = 0.56), <br/>         na.action=na.exclude, <br/>         control=nls.control(maxiter = 500))<br/>summary(try)<br/>df_long$pred&lt;-predict(try)<br/>ggplot(df_long)+<br/>  geom_point(aes(x=interval, <br/>                 y=retention))+<br/>  geom_line(aes(x=interval, <br/>                y=pred, <br/>                group=subject), col="red")+<br/>  theme_bw()<br/><br/></span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div class="er es kx"><img src="../Images/a2b6c77ae3c60050bc977f583c42ed6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*L_S71zdtwjC0oyniVryRig.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">The coefficients of the hyperparameters coming from <strong class="bd ky">nls</strong>.</figcaption></figure><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kz"><img src="../Images/2b209afe2900ef74e0c53b5d9776bd86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgEe82ucgnpiRVv3DLQltg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">And the <strong class="bd ky">nls </strong>fit on all the data together. Looks good, meaning that this formula well probably do the job.</figcaption></figure><p id="24ae" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们可以去看看我们的第一个贝叶斯模型。让我们从一个简单的线性回归开始，它有非常广泛的先验知识。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="106b" class="kd ke hh jz b be kf kg l kh ki">fit1&lt;-brms::brm(retention ~ interval + (1|subject), <br/>          data = df_long,<br/>          prior = c(<br/>            prior(normal(0, 100), class = Intercept),<br/>            prior(normal(0, 100), class = b, coef=interval),<br/>            prior(cauchy(0, 10),  class = sigma)), <br/>          chains=4, <br/>          cores=6)<br/>summary(fit1)<br/>plot(fit1)<br/>pp_check(fit1, ndraws = 500)<br/>pp_check(fit1, type = "error_hist", ndraws = 11)<br/>pp_check(fit1, type = "scatter_avg", ndraws = 100)<br/>pp_check(fit1, type = "stat_2d")<br/>pp_check(fit1, type = "loo_pit")</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es la"><img src="../Images/6ebc125ade9548e70b4cf728863bc422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*fqoyUAzmzHvr323bmrdppg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">Model fits well from a sampling point of view.</figcaption></figure><div class="ju jv jw jx fd ab cb"><figure class="lb kl lc ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/9bdadda562a46366bdcf8ad142b9a9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*U3TQkxi2sVyaSc94V-sPOw.png"/></div></figure><figure class="lb kl lh ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/9c43aabb812e216e0fd81deed0b174e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*gP70CEDgR-uu8fHB5zZQ-A.png"/></div></figure><figure class="lb kl lh ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/4ef42303a6729dfd28d6c9494bc9ab46.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*6MuxzT4lnGrnrsdj6xzqPA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx li di lj lk">Samples look good, but the posterior is not on par with the likelihood.</figcaption></figure></div><p id="3e98" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们看看后面的画，看看模型是如何表现的。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="cec5" class="kd ke hh jz b be kf kg l kh ki">df_long %&gt;%<br/>  add_epred_draws(fit1, <br/>                  ndraws=50, <br/>                  allow_new_levels = TRUE) %&gt;%<br/>  ggplot(aes(x = interval, <br/>             y = retention, <br/>             group=subject)) +<br/>  geom_line(aes(y = .epred, group = paste(subject, .draw)), alpha = 0.25) +<br/>  geom_point(data = df_long, color="black")+<br/>  facet_wrap(~subject)+<br/>  theme_bw()</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kz"><img src="../Images/606e23926f1e5905ea96c419d5777a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzTfQUlBbI4li9moNM_wCA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">As expected, the posterior draws are linear, which does not make for a very good model. In fact, as the interval increases.</figcaption></figure><p id="e90b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们回到最初的计划，并使用指定的公式对数据进行建模。这在<em class="jt"> brms </em>中很容易完成。我还会在这里添加层次部分。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="bd58" class="kd ke hh jz b be kf kg l kh ki">fit2&lt;-brms::brm(bf(retention ~ <br/>                     exp(-a1 * interval) + b1,<br/>                   a1 + b1~1 + (1|subject), nl=TRUE), <br/>                data = df_long,<br/>                prior = c(<br/>                  prior(beta(1, 1), nlpar = "a1"),<br/>                  prior(beta(1, 1), nlpar = "b1")), <br/>                chains=4, <br/>                cores=6, <br/>                warmup = 3000,<br/>                iter = 6000)<br/>summary(fit2)<br/>plot(fit2)<br/>pp_check(fit2, ndraws = 500)<br/>pp_check(fit2, type = "stat_2d")<br/>pp_check(fit2, type = "loo_pit")<br/><br/>ce&lt;-conditional_effects(fit2, surface = TRUE)<br/>pl_ce&lt;-plot(ce, rug=TRUE, theme=theme_bw())<br/><br/>df_long %&gt;%<br/>  add_epred_draws(fit2, <br/>                  ndraws=50, <br/>                  allow_new_levels = TRUE) %&gt;%<br/>  ggplot(aes(x = interval, <br/>             y = retention, <br/>             group=subject)) +<br/>  geom_line(aes(y = .epred, group = paste(subject, .draw)), alpha = 0.25) +<br/>  geom_point(data = df_long, color="black")+<br/>  facet_wrap(~subject)+<br/>  theme_bw()</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/8c5582e6c2fe73d1389b6ee05bfe80d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*LH2u2o4O2mf-XzfB4uzcuw.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Model looks good, but great. Rhat values are too big. Lets look at the chains.</figcaption></figure><div class="ju jv jw jx fd ab cb"><figure class="lb kl lm ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/cb25e9cca9a7f6012214ad6d22c21981.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*yDi4jsedqunwACK9Kv7zAQ.png"/></div></figure><figure class="lb kl ln ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/22738ac1c08d3f3a8e57c70e005a569c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*KkPPwse1gMYevfT_gS_U0g.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx lo di lp lk">Chains are not that good and te posterior densities do not seem to mimic the data. They do NOT have to, necessarily, but a serious deviation requires some solid reasoning.</figcaption></figure></div><div class="ab cb"><figure class="lb kl lq ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/14e363ef0d9af062b3f32d05d14b9f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*UcX0ux2Wfip5Nqjq5rhPQA.png"/></div></figure><figure class="lb kl lr ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/3aea61723498b970f72121465d7b8671.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*4aCELmJw0gCR_zuoLvI1PA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx ls di lt lk">The build-in conditional plot (left) and the posterior draws (right). The right plot shows that this model is clearly not what we should have.</figcaption></figure></div><p id="6bfa" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">之前的模型将保留视为正态分布的最佳变量。在书中，二项式是适用的，所以我们将做同样的事情。其余的我将保持不变，包括非常弱的无信息β(1，1)先验，它们在这个状态下是一致的。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="c161" class="kd ke hh jz b be kf kg l kh ki">fit3&lt;-brms::brm(bf(retention | trials(N) ~ <br/>                     exp(-a1 * interval) + b1,<br/>                   a1 + b1~1 + (1|subject), nl=TRUE), <br/>                data = df_long,<br/>                family=binomial(link = "logit"),<br/>                prior = c(<br/>                  prior(beta(1, 1), nlpar = "a1"),<br/>                  prior(beta(1, 1), nlpar = "b1")), <br/>                chains=4, <br/>                cores=6, <br/>                warmup = 3000,<br/>                iter = 6000)<br/>summary(fit3)<br/>df_long %&gt;%<br/>  add_epred_draws(fit3, <br/>                  ndraws=50, <br/>                  allow_new_levels = TRUE) %&gt;%<br/>  ggplot(aes(x = interval, <br/>             y = retention, <br/>             group=subject)) +<br/>  geom_line(aes(y = .epred, group = paste(subject, .draw)), alpha = 0.25) +<br/>  geom_point(data = df_long, color="black")+<br/>  facet_wrap(~subject)+<br/>  theme_bw()</span></pre><div class="ju jv jw jx fd ab cb"><figure class="lb kl lu ld le lf lg paragraph-image"><img src="../Images/bc475a7d4b069db099846af98e422bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wXcJKsS-hkRfdRzSO7jVXA.png"/></figure><figure class="lb kl lv ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/ea06aa61e041afbabf08a6e16039684a.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*kg2qmOJMputsIIGILWgnVQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx lw di lx lk">The Rhat values are still not that great, but the model looks better. Not perfect, but better than before.</figcaption></figure></div><p id="b8ca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们更深入地研究一下，调整一下先验知识，使之更具知识性。我真的没有任何具体的文章/论文可以使用，这将是建立信息先验的最佳方式，但我现在使用的统一先验真的不好。</p><p id="d531" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，对于<em class="jt"> b </em>参数，我将从正态分布中取样。<em class="jt"> sd </em>参数可以保持一致。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="adb6" class="kd ke hh jz b be kf kg l kh ki">get_prior(bf(retention | trials(N) ~ <br/>               exp(-a1 * interval) + b1,<br/>             a1 + b1~1 + (1|subject), nl=TRUE), <br/>          data = df_long,<br/>          family=binomial(link = "logit"))<br/>fit4&lt;-brms::brm(bf(retention | trials(N) ~ <br/>                     exp(-a1 * interval) + b1,<br/>                   a1 + b1 ~ 1 + (1|subject), nl=TRUE), <br/>                data = df_long,<br/>                family=binomial(link = "logit"),<br/>                prior = c(<br/>                  prior(normal(0, 1), class=b,  coef=Intercept, nlpar = "a1"),<br/>                  prior(normal(0, 1), class=b,  coef=Intercept, nlpar = "b1"),<br/>                  prior(beta(1, 1), class=sd, coef=Intercept, group=subject, nlpar = "a1"),<br/>                  prior(beta(1, 1), class=sd, coef=Intercept, group=subject, nlpar = "b1")), <br/>                chains=4, <br/>                cores=6, <br/>                warmup = 3000,<br/>                iter = 6000)<br/>summary(fit4)<br/><br/>pp_check(fit4, ndraws=500)<br/><br/>df_long %&gt;%<br/>  add_epred_draws(fit4, <br/>                  ndraws=50, <br/>                  allow_new_levels = TRUE) %&gt;%<br/>  ggplot(aes(x = interval, <br/>             y = retention, <br/>             group=subject)) +<br/>  geom_line(aes(y = .epred, group = paste(subject, .draw)), alpha = 0.25) +<br/>  geom_point(data = df_long, color="black")+<br/>  facet_wrap(~subject)+<br/>  theme_bw()</span></pre><div class="ju jv jw jx fd ab cb"><figure class="lb kl ly ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/f30df5da9f50c66e523ca7b6cd0148da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*mdjedLVB0uYRwVksfYVsIg.png"/></div></figure><figure class="lb kl lz ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/877f8a61cc27076ae166ccc443ee878e.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*2YcbBhcsGySyuuGsM1mPpw.png"/></div></figure><figure class="lb kl ma ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/10c6168296297fe49d4d6af877f3dae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*TJhKqGkBLHdf1E0ADm0QGQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx mb di mc lk">Rhat is a bit better, but posterior draws are not that great.</figcaption></figure></div><p id="f8e1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的图片显示，使用的先验知识没有帮助。此外，因为我们只有四个主题，先验有相当大的影响。我们做出的任何选择都会产生影响。</p><p id="54a6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">增加模型复杂性的另一种方法是在分级模型中增加一个额外的随机斜率组件。到目前为止，曲线只能通过<em class="jt">截距</em>由主体移动。</p><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es md"><img src="../Images/40730f6f3c02ff0bd93ddfbafa29d2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXhZ6hLuI5xiGjfy2WSqXg.png"/></div></div></figure><div class="ju jv jw jx fd ab cb"><figure class="lb kl me ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/5410528e107c28d29cc76957e9932a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*riMxIOjIK9MMpEN0SdBwhg.png"/></div></figure><figure class="lb kl mf ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/ab6a939c42e3b8dcb5f55e363b57d17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*vQDVDN7ORGiRNpO_wjOjOg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx mg di mh lk">The posterior draws are not stable.</figcaption></figure></div><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mi"><img src="../Images/84d13890d4a377d7b091d813d941cb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjrWzI8goSITh0K5bIYzrg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">Looks good in shape, but for sure not stable.</figcaption></figure><p id="6673" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">a1变量是决定衰变率的变量，而b1是常数。我认为之前的模型过于复杂，所以我将从<em class="jt"> b1 </em>中选取随机斜率。此外，我会将截距值设置回具有限制[0，1]的均匀先验，对<em class="jt"> a1 </em>的随机截距使用正态分布，对<em class="jt"> a1 </em>的随机斜率和<em class="jt"> b1 </em>的随机截距使用更宽的伽马分布。</p><pre class="ju jv jw jx fd jy jz ka bn kb kc bi"><span id="5e00" class="kd ke hh jz b be kf kg l kh ki">fit6&lt;-brms::brm(bf(retention | trials(N) ~ <br/>                     exp(-a1 * interval) + b1,<br/>                   a1  ~ 1 + (1 + interval | subject), <br/>                   b1  ~ 1 + (1 | subject), nl=TRUE), <br/>                data = df_long,<br/>                family=binomial(link = "logit"),<br/>                prior = c(<br/>                  prior(beta(1, 1), class=b,  nlpar = "a1", lb=0, ub=1),<br/>                  prior(beta(1, 1), class=b,  nlpar = "b1", lb=0, ub=1),<br/>                  prior(normal(0, 1),   class=sd, coef=Intercept, group=subject, nlpar = "a1"),<br/>                  prior(gamma(1, 1),   class=sd, coef=Intercept, group=subject, nlpar = "b1"), <br/>                  prior(gamma(1, 1),  class=sd, coef=interval, group=subject, nlpar = "a1")), <br/>                chains=4, <br/>                cores=6, <br/>                warmup = 3000,<br/>                iter = 6000)</span></pre><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/01cc165166fcc77942c7b2c44745ac63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVlbQE6Wb1Mb8UBnfUcUCA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">Looking much better.</figcaption></figure><div class="ju jv jw jx fd ab cb"><figure class="lb kl mk ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/3b617aec6e9d3c4b02c06a06ce78f369.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*OPg1Hq2bNrlLaRdrIbsCaw.png"/></div></figure><figure class="lb kl ml ld le lf lg paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/72d2014467a61e0fbb274bd8f67c678b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*8Laap0XUg7EtKiEVVfzJrg.png"/></div></figure></div><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kz"><img src="../Images/7f0b6d50a90c63746b11e0644353f133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgKmJnBWXYtPzCPLfFzcMw.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">Lookin much better.</figcaption></figure><figure class="ju jv jw jx fd kl er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kz"><img src="../Images/53341c97b83fac102d105149d155b2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zyk9F9mARIq1egh7HjfZPg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx">Much better as well, although in parts a bit unstable.</figcaption></figure><p id="b419" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最终，brms模型的功能比作者制作的STAN或WinBUGS模型要少，但它确实完成了工作。STAN的问题是它不能处理NA值，不像WinBUGS。</p><p id="4ace" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此外，我现在使用的先验信息很少，但不是基于科学证据。它们是根据我对模型的理解和经验选择的，但不应该是这样的。</p><p id="b86e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，我希望这篇博文展示了如何通过非线性贝叶斯方法对认知数据建模。</p><div class="mm mn ez fb mo mp"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd km mp"/></div></div></a></div></div></div>    
</body>
</html>