<html>
<head>
<title>Distributed Data Parallel with Slurm, Submitit &amp; PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与Slurm、Submitit &amp; PyTorch并行的分布式数据</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/distributed-data-parallel-with-slurm-submitit-pytorch-168c1004b2ca?source=collection_archive---------0-----------------------#2021-08-04">https://medium.com/mlearning-ai/distributed-data-parallel-with-slurm-submitit-pytorch-168c1004b2ca?source=collection_archive---------0-----------------------#2021-08-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5e6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PyTorch提供了多种方法来将您的训练分布到多个GPU上，无论GPU是在您的本地机器上、集群节点上，还是分布在多个节点上。作为一名人工智能研究人员，您可能有机会访问一个或多个GPU集群，并且您想知道如何修改您的单个GPU脚本以利用可用资源并加快您的训练。本文将介绍如何在具有多个GPU的本地机器上以及使用Slurm调度作业的GPU集群上使用分布式数据并行。</p><p id="85a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于您不必为每个新项目重新编写分布式培训的代码，我已经编写了一个<a class="ae jc" href="https://github.com/ramyamounir/Template" rel="noopener ugc nofollow" target="_blank">模板库</a>，其中包含所有已设置的DDP代码。要使用存储库，您所要做的就是用您的模型切换出架构，添加数据集并定义损失函数。无论您是向Slurm提交作业，还是在本地(或通过ssh远程)的几个GPU上运行代码，存储库都会自动处理所有分布式的训练代码。如果需要，代码也可以在单个GPU上运行。也就是说，我建议通读这篇文章，以了解幕后发生的事情以及每一行代码的目的。</p><h1 id="73cc" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据并行与模型并行</h1><p id="d72a" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">当考虑分布式训练时，您必须考虑您希望在GPU上分配什么。你有两个选择:<em class="kg">数据并行</em>和<em class="kg">模型并行。</em>如果您的模型适合一个GPU，但您希望通过增加批处理大小来加快训练速度，那么您希望通过使用数据并行方案将批处理分布在多个GPU上。但是，如果您的模型参数不适合单个GPU，那么您需要在不同的GPU上分配您的架构的不同部分，也称为模型并行。本文只讨论数据并行，假设您的模型适合单个GPU。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/7a617fb7b31745030a292d4891a2a8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVsAPmwuKqsF6CnkyVtfPA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Data Parallel vs. Model Parallel. (Image Source: <a class="ae jc" href="https://docs.chainer.org/en/stable/chainermn/model_parallel/overview.html" rel="noopener ugc nofollow" target="_blank">ChainerMN</a>)</figcaption></figure><h1 id="d5dc" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据并行与分布式数据并行</h1><p id="6e39" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">正如我之前提到的，PyTorch提供了许多工具来帮助您快速将单GPU训练脚本转换为多GPU脚本。由于DataParallel简单，大多数人都是从它开始尝试的。你可以很容易地用神经网络包装你的模型。DataParallel类，它将自动处理cuda _ visible _ devices(GPU)上的模型克隆，并在其上拆分(分批)数据。虽然DP类提供了一种简单的方法，但是您必须意识到它既不高效也不可伸缩(否则您不会在这里:P)。DP使用多线程和单个进程在GPU上分配训练，这不如为每个GPU使用单独的进程并在需要时传递结果(或参数梯度)高效。此外，因为DP使用单个进程，所以它不能跨多个机器/节点使用。所以，如果你需要在两个或更多的节点上训练，每个节点有4个GPU，你必须使用DDP。</p><h1 id="5cfe" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">分布式数据并行</h1><p id="349f" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">正如您可能已经猜到的，DDP不像DP那样容易设置，主要是因为GPU之间的通信需要初始化代码(可能在多个节点上)。尽管如此，这比你想象的要容易得多。我们先来定义一些术语。当使用GPU集群时，节点是具有可变数量的GPU(或者没有)、CPU、RAM等的单个机器。集群连接这些节点，并使用一些调度软件(如Slurm或SGE)在一个或多个节点上调度作业。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kx"><img src="../Images/eb8f4ea5b7fc3e3b3aec93c62f28b20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*EQiyWdaXQz4QhQjNHfu7pw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Example of a 3-nodes cluster</figcaption></figure><p id="2089" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当您的训练脚本利用DDP在单个或多个节点上运行时，它将产生多个进程；每个都将在不同的GPU上运行。每个进程都需要知道有多少其他进程在做同样的工作(<strong class="ig hi">世界大小</strong>)以及如何到达主进程。每个进程都需要有一个本地等级和一个全局等级。<strong class="ig hi">局部等级</strong>在其运行的节点内定义其顺序<em class="kg">，而<strong class="ig hi">全局等级</strong>在<em class="kg">所有进程</em>内定义其顺序。</em></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kx"><img src="../Images/3a0135e5bb684434afbd2a341d49b00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*yK98Fla9hVyiAsQF11n0Ng.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Example of communication arguments on 2-nodes cluster.</figcaption></figure><p id="de7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在生成多个进程并为每个进程提供主进程的world_size、local_rank、global_rank和address的副本后，您需要将数据集分割成world_size块，以便每个GPU获得数据集的不同部分。然后你可以用nn . parallel . distributed data parallel类包装你的模型，基本就完成了。在本文的其余部分，我们将更详细地介绍如何执行上述每个步骤，以及一些用于检查点、Tensorboard可视化、使用Submitit在Slurm上调度等等的帮助函数。</p><h1 id="4a37" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">在单个节点上初始化</h1><p id="75f0" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">让我们从尝试在同一个节点上生成多个流程开始。我们将需要torch.multiprocessing.spawn函数来生成args.world_size进程。为了保持事物的组织性和可定制性，我们可以使用argparse。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="a01c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">区分运行在主进程上的代码和运行在子进程上的代码是非常重要的。例如，在GPU上生成train函数之前，我们必须在主进程上定义CUDA_VISIBLE_DEVICES。我们还为所有GPU定义了dist_url，以便与主进程进行通信。因为我们在单个节点上本地运行，所以URL可以是localhost和一个随机端口。这个URL将提供给所有的GPU，这样任何进程都可以访问主进程。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="479f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在子进程(在GPU上)上运行的代码将有特定的初始化变量，比如局部秩。torch . distributed . init _ process _ group做所有繁重的工作；它初始化进程之间的通信，并等待，直到它确定它们可以互相交谈。对于参数的随机初始化，我们将所有GPU上的种子设置为相同。剩下的就是获取数据集加载器、模型、损失函数，并将它们传递给训练器函数。</p><p id="10f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以很容易地修改这段代码，以便在多个节点上运行这种训练，而不一定是在同一个集群上。您需要手动将args.dist_url中的“localhost”更改为Noda地址，并将args.world_size设置为您打算从所有节点使用的GPU总数。您还需要将节点秩传递给train函数，该函数可以添加到局部秩中以获得GPU的全局秩，如下所示。</p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="f34b" class="lf je hh lb b fi lg lh l li lj">args.rank = args.node_rank + gpu</span></pre><p id="f2be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就是这样；现在，您可以在不同的节点上运行该脚本的多个副本，并查看所有GPU上同时发生的训练。</p><h1 id="e7ca" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据加载器</h1><p id="a584" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">正如我们之前提到的，数据集需要分割成块，其中块的总数应该等于args.world_size。分布式Sampler类可以很容易地为我们做到这一点。您只需要定义您的数据集，并将其作为参数与其他参数(如当前进程的world_size和global_rank)一起传递给DistributedSampler类。输出将是一个sampler对象，您可以将其传递给DataLoader类。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><h1 id="2988" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">获取模型、损耗和训练师！</h1><p id="52b1" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">剩下的就像定义你的架构，把它包装在nn . parallel . distributed data parallel类中，定义你的损失函数，开始训练一样简单！:)</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><h1 id="782b" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">Slurm</h1><p id="7bad" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">Slurm是集群上使用的一个作业调度器，用于接受作业提交文件，并在请求的资源可用时对它们进行调度。通常的过程是用特定于Slurm的参数创建一个单独的脚本文件:</p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="0049" class="lf je hh lb b fi lg lh l li lj">#!/bin/bash<br/>#SBATCH -w "[node_name]"<br/>#SBATCH -p [partition]<br/>#SBATCH --mem=100GB<br/>srun python train.py</span></pre><p id="2118" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并使用sbatch“提交”，如下所示:</p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="e09e" class="lf je hh lb b fi lg lh l li lj">sbatch script.sh</span></pre><p id="3e7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然你可以按照上面的步骤让它做你想做的事情，但有一种更简单的方法，那就是利用一个名为“<a class="ae jc" href="https://github.com/facebookincubator/submitit" rel="noopener ugc nofollow" target="_blank"> Submitit </a>的库，它最近由脸书人工智能研究所(FAIR)开源。这个想法是使用Submitit为我们生成并提交作业脚本。我们可以很容易地定义有多少个节点以及每个节点上的GPU数量。我们甚至可以定义一个函数，在作业由于某种原因被抢占时重新提交。</p><h1 id="d131" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">提交它</h1><p id="e2a7" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">要使用Submitit生成作业并提交给Slurm，我们需要获得一个submitit。自动执行程序对象。我们可以使用submitit函数。AutoExecutor.update_parameters提供特定于Slurm的参数。Submitit将负责在GPU上生成不同的进程(即使在不同的节点上)。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="4a46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上面的代码所示，我们可以定义一个slurm_trainer类，并将该类的一个实例传递给executor.submit函数。这个提交函数将在executer参数中定义的多个GPU上产生slurm_trainer实例的__call__函数。slurm_trainer仍然调用train函数，该函数获取数据集、模型、损失函数，并开始训练。注意:您不再需要在train函数中定义args.gpu和args.rank，因为它们现在已经在第44和45行中定义了。提供的<a class="ae jc" href="https://github.com/ramyamounir/Template" rel="noopener ugc nofollow" target="_blank">模板</a>在一个脚本中结合了Slurm训练和本地训练。</p><h1 id="a57c" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">张量板</h1><p id="a7cb" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">模板中的utils目录为自动启动Tensorboard writer和server提供了一些帮助函数。当在远程服务器上运行脚本时，我建议启动一个Ngrok服务器，将端口6006转发到一个Ngrok域。Ngrok域可以从任何地方访问，甚至在你的智能手机上，以检查你的训练进度。</p><h1 id="7653" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">使用模板</h1><p id="36cc" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">该模板遵循模块化方法，其中代码的主要组件(架构、损耗、调度器、训练器等。)被组织到子目录中。</p><ul class=""><li id="9dce" class="lk ll hh ig b ih ii il im ip lm it ln ix lo jb lp lq lr ls bi translated"><a class="ae jc" href="https://github.com/ramyamounir/Template/blob/main/train.py" rel="noopener ugc nofollow" target="_blank"> train.py </a>脚本包含所有参数(由argparse解析)和节点/GPU初始化器(slurm或local)。它还包含用于导入数据集、模型、损失函数并将它们传递给训练器函数的代码。</li><li id="549d" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">lib/trainer/trainer.py脚本定义了训练过程的细节。</li><li id="3e97" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">lib/dataset/[args.dataset]。py导入数据并定义数据集函数。建议创建一个带有到数据集的软链接的数据目录，尤其是在多个数据集上进行测试时。</li><li id="d1a1" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">lib/core/ directory包含丢失、优化器、调度器函数的定义。</li><li id="bb00" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">lib/utils/目录包含按文件名组织的帮助函数。(即，用于分布式训练的帮助器函数被放置在lib/utils/distributed.py文件中)。</li></ul><p id="2d2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对于单节点，单GPU训练，试试:</strong></p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="d74f" class="lf je hh lb b fi lg lh l li lj">python train.py -gpus 0</span></pre><p id="6dec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对于单节点、多GPU训练，尝试:</strong></p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="615b" class="lf je hh lb b fi lg lh l li lj">python train.py -gpus 0,1,2</span></pre><p id="98ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对于SLURM上的单节点、多GPU训练，尝试:</strong></p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="4d36" class="lf je hh lb b fi lg lh l li lj">python train.py -slurm -slurm_nnodes 1 -slurm_ngpus 4<br/>-slurm_partition general</span></pre><p id="d0d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">对于SLURM上的多节点、多GPU训练，请尝试:</strong></p><pre class="ki kj kk kl fd la lb lc ld aw le bi"><span id="5bd1" class="lf je hh lb b fi lg lh l li lj">python train.py -slurm -slurm_nnodes 2 -slurm_ngpus 8 <br/>-slurm_partition general</span></pre><p id="c988" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">温馨提示:</strong></p><ul class=""><li id="8593" class="lk ll hh ig b ih ii il im ip lm it ln ix lo jb lp lq lr ls bi translated">要获得关于可用参数的更多信息，请运行:<code class="du ly lz ma lb b">python train.py -h</code></li><li id="8a78" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">要将Tensorboard服务器作为不同的线程自动启动，请添加参数:<code class="du ly lz ma lb b">-tb</code></li><li id="efe9" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">要覆盖模型日志文件并从头开始，添加参数:<code class="du ly lz ma lb b">-reset</code>，否则，它将使用最后的权重作为检查点，并继续写入相同的tensorboard日志文件-如果使用相同的模型名称。</li><li id="b542" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">要在SLURM上选择特定的节点名，可以使用参数:<code class="du ly lz ma lb b">-slurm_nodelist GPU17,GPU18</code>作为例子。</li><li id="78f3" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">如果在带张量核的GPU上运行，使用混合精度模型可以加快你的训练速度。添加参数<code class="du ly lz ma lb b">-fp16</code>进行试验。如果因为精度损失而使训练不稳定，就不要用:)</li><li id="eda9" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">该模板允许您通过传递不同的参数来轻松切换架构、数据集和训练器。例如，不同的架构可以添加到lib/arch/[arch-name]中。py目录，并将参数作为<code class="du ly lz ma lb b">-arch [arch-name]</code>或<code class="du ly lz ma lb b">-trainer [trainer-name]</code>或<code class="du ly lz ma lb b">-dataset [dataset-name]</code>传递</li><li id="3fb5" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">stdout和stderr将打印在共享目录中。我们只打印第一个GPU输出。确保根据您使用的集群更改lib/utils/distributed.py中的共享目录。</li></ul><h1 id="5058" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="a454" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在本文中，我们介绍了如何通过几个简单的步骤在多个GPU上使用DDP分发您的培训。DDP和DP的主要区别在于定义了通信参数，比如world_size、ranks和URL。我们还讨论了Slurm以及如何使用Submitit自动化脚本生成过程。基于Slurm的工作和本地培训的工作都合并在一个易于使用的模板下。如果您遇到问题，请在此处发表评论或在<a class="ae jc" href="https://github.com/ramyamounir/Template" rel="noopener ugc nofollow" target="_blank">模板</a>存储库中提出问题，让我知道。</p><p id="f856" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">快乐编码:)</p></div></div>    
</body>
</html>