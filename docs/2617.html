<html>
<head>
<title>The Well Known Regression — What Is It ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">众所周知的回归——它是什么？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-well-known-regression-what-is-it-6dbe5e0b2cd1?source=collection_archive---------4-----------------------#2022-05-23">https://medium.com/mlearning-ai/the-well-known-regression-what-is-it-6dbe5e0b2cd1?source=collection_archive---------4-----------------------#2022-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bb94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章从<strong class="ig hi">统计学</strong>的角度和<strong class="ig hi">机器学习视觉</strong>的角度解释了什么是<strong class="ig hi">回归</strong>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/08b3bf94f91c9e9150610d83a4718bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MEixm3ZUb095a-mG"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@katetrysh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kate Trysh</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ead2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回归问题是<strong class="ig hi">监督学习问题</strong>(用于训练模型的标签问题)，以两种类型的变量为特征。第一组是预测变量，标记为<em class="jt"> X </em>，第二组是因变量，标记为<em class="jt"> Y </em>。</p><p id="2118" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个简单的等式说明了一个回归问题:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ju"><img src="../Images/674650c3172b56a71ef89027fe920b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*cqHp4DDjl9KHw9Yyz5I2RA.png"/></div></figure><ul class=""><li id="d43c" class="jv jw hh ig b ih ii il im ip jx it jy ix jz jb ka kb kc kd bi translated">等式左边的<strong class="ig hi">部分</strong>代表因变量(<strong class="ig hi">输出</strong>)，模型试图<strong class="ig hi">预测</strong>这个变量，这就是为什么<em class="jt"> y </em>得到了一个帽子。</li><li id="31a8" class="jv jw hh ig b ih ke il kf ip kg it kh ix ki jb ka kb kc kd bi translated"><strong class="ig hi">右边部分</strong>是用于预测输出的<strong class="ig hi">输入</strong>。那样的话，你就有了用<em class="jt">x</em>s .<strong class="ig hi">θs</strong>表示的<em class="jt"> n </em> <strong class="ig hi">预测变量</strong>就是回归问题的<strong class="ig hi">参数</strong>。这些参数是试图预测输出的机器学习模型的主要焦点。找到每一个的正确值是能够从输入预测任何输出(具有最低误差幅度)的交易。<br/><em class="jt">θ0</em>表示所有输入为空的情况。每个输入都有其相关的参数。</li></ul><p id="ffb4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来了解一下这些在经典数据集中是如何表示的。</p><h1 id="d822" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">回归的数据模拟</h1><p id="eb04" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">由于这些问题需要大量的输入，矩阵符号被用来写出更好的数学概念。小写的<em class="jt"> x </em>(见前)表示数据集的列，可以记为X，它是所有输入变量的矩阵:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/b8ce4080d64b8c06225be13d36fdd6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*uQsQK4oN2jcSDBA2bUJhAw.png"/></div></figure><p id="5668" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">而小写的<em class="jt"> y </em>可以记为Y，输出的向量:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ln"><img src="../Images/84a18a08eba0663fcb840c9d2857157d.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*0j-fIj6RgH13kpJPfhGyoQ.png"/></div></figure><p id="9bae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从<strong class="ig hi">数据的角度来看</strong>，这些矩阵可以只是一个常规的数据表，形状为<em class="jt"> n </em>乘以<em class="jt"> n. </em> X是特征列，Y是因变量列(你要预测的列):</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lo"><img src="../Images/0843e688fdb237be6ba69da3b41c6aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwfSKbSfYE60JIOpIGcymA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">By the author</figcaption></figure></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><p id="41e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们区分两种回归:</p><h2 id="259f" class="lw kk hh bd kl lx ly lz kp ma mb mc kt ip md me kx it mf mg lb ix mh mi lf mj bi translated">单变量的</h2><p id="bd09" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">以唯一预测变量为特征的回归是单变量回归:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mk"><img src="../Images/db058196ceddfa08d6027be0527b2295.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*kzjn3EGdz8TaA6ECD582Bg.png"/></div></figure><p id="4a87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在机器学习中，模型将使用一个独特的特征(预测变量)来计算θ的最佳值。</p><h2 id="1839" class="lw kk hh bd kl lx ly lz kp ma mb mc kt ip md me kx it mf mg lb ix mh mi lf mj bi translated">多变量的</h2><p id="7441" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">当回归使用多个特征时，称为多元回归:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ju"><img src="../Images/674650c3172b56a71ef89027fe920b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*cqHp4DDjl9KHw9Yyz5I2RA.png"/></div></figure><p id="54a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可能的功能没有限制，但是模型性能将取决于好的功能，而不是数量。</p><p id="c6c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了基础知识，让我们深入研究不同的回归问题，最著名的一个叫做线性回归…</p></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><h1 id="b118" class="kj kk hh bd kl km ml ko kp kq mm ks kt ku mn kw kx ky mo la lb lc mp le lf lg bi translated">线性回归</h1><p id="53c1" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">线性回归问题是可以用直线<strong class="ig hi"/><strong class="ig hi">线</strong>建模的回归问题:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mq"><img src="../Images/59dea96f7d6ec76f1a8acae8a837ad3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvtoRABslNVeYYv2r9_F0Q.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">A simple linear regression prediction (by the author)</figcaption></figure><p id="0c21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">特征x1增加得越多，输出也增加得越多。它也可以是与随着x1增加而减小的线的负相关。</p><h1 id="1f99" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">多项式回归</h1><p id="7c66" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">当数据不是线性时，使用多项式回归，这意味着它不能用简单的线性回归来解决。然后，该想法将从数据集中现有的特征添加多项式特征，并通过<strong class="ig hi">揭示<strong class="ig hi">输入</strong>和<strong class="ig hi">输出</strong>之间的</strong> <strong class="ig hi">新的</strong> <strong class="ig hi">关系</strong>来创建更高的精度。一旦添加了这些新特征，模型就用经典的线性回归进行训练。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mr"><img src="../Images/39f80ea142a07edeef90b10e55658cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zuhEG40sFtDGacJxC6erg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">A polynomial regression prediction (by the author)</figcaption></figure><p id="f960" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的示例使用了二次多项式:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ms"><img src="../Images/8a3607a2337a884c273c9d74988fce20.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/1*k3nN8LN3r-QOk3_8rX33Jw.gif"/></div></figure><p id="ad99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们以<em class="jt"> x </em>为特征，<em class="jt"> x </em>幂2将被创建，以使模型更复杂，更精确。</p><p id="9fec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以无限程度地添加多项式要素，但是添加的复杂度越高，模型就越容易过度拟合，新数据的质量也越差。</p><h1 id="bdb0" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">正则线性回归</h1><p id="e5e0" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">正则化一个模型正在为它创建<strong class="ig hi">约束</strong>，更准确地说是为<strong class="ig hi">参数</strong>创建约束。一般来说，这里的想法是防止模型过度拟合(特别是如果函数是非线性的)，因为<strong class="ig hi">超参数</strong> <strong class="ig hi"> <em class="jt"> alpha </em> </strong>会降低特征参数的权重。因此，即使模型对一个特征增加了太多的重要性，由于方差的减少，它的参数将会更小，预测将会更准确(平均而言)。</p><p id="cb97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最常用的正则化方法有:</p><ul class=""><li id="16ef" class="jv jw hh ig b ih ii il im ip jx it jy ix jz jb ka kb kc kd bi translated">里脊回归</li><li id="d56f" class="jv jw hh ig b ih ke il kf ip kg it kh ix ki jb ka kb kc kd bi translated">套索回归</li><li id="abe4" class="jv jw hh ig b ih ke il kf ip kg it kh ix ki jb ka kb kc kd bi translated">弹性网</li></ul><p id="f7dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本上，这三者之间的区别在于它调整模型的方式。<strong class="ig hi">脊</strong>和<strong class="ig hi">套索</strong>都有办法做到这一点，这意味着一些不同，比如套索会把最不重要的特征放在一边。<br/> <strong class="ig hi">弹性</strong> <strong class="ig hi">网</strong>是<strong class="ig hi">脊</strong>和<strong class="ig hi">套索</strong>方法的组合，使用另一个超参数来指定每种方法的权重。</p><h1 id="ceda" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">逻辑回归(入侵者)</h1><p id="5521" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">那种类型的回归模型不是用于回归问题而是用于解决<strong class="ig hi">分类</strong> <strong class="ig hi">问题</strong>。事实上，逻辑这个术语来源于以S形为特征的逻辑<strong class="ig hi"/><strong class="ig hi">功能</strong>(<strong class="ig hi">S形</strong>T51】功能):</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mt"><img src="../Images/6e57bc35eef3907c16570e61f6dca04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dM9BOhaeiBWNXRBbRO9kpQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">The logistic function (by the author)</figcaption></figure><p id="4001" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">小写<em class="jt"> t </em>介于负无穷大和正无穷大之间，但是<strong class="ig hi">逻辑</strong> <strong class="ig hi">函数</strong> ( <em class="jt"> g(t) </em>)返回一个介于<strong class="ig hi"> 0和1 </strong>之间的值。这个逻辑函数是模型趋向于最小化的成本函数。根据阈值，<em class="jt"> g(t) </em>给出在正类中的概率。例如，如果阈值是0.5，那么对于一个观察返回0.7 的<strong class="ig hi">概率的逻辑函数将把这个观察分类为正(1)，因为0.7大于0.5(例如:如果一只狗是1，而0是一只猫，则模型调用一只狗)。它将总是返回0或1(类)的结果。</strong></p></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><p id="4e26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您阅读本文，我希望您现在了解什么是<strong class="ig hi">回归</strong>，它如何通过使用解释变量(<strong class="ig hi">特征</strong>)来预测因变量，从而模拟现实生活中的问题。最后，<strong class="ig hi">输入</strong>(特征)和<strong class="ig hi">输出</strong>(因变量)之间的关系可以表征模型将处理的回归问题的类型。</p><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl jm mx"/></div></div></a></div></div></div>    
</body>
</html>