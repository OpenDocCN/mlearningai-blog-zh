# 解释了政策上与政策外强化学习

> 原文：<https://medium.com/mlearning-ai/on-policy-v-off-policy-reinforcement-learning-explained-89054a6cc6?source=collection_archive---------1----------------------->

在大而复杂的深度强化学习领域，很容易迷失方向。

在这个领域中，您会遇到一些细微差别的术语和分类，它们很难相互解析。*什么是基于策略和基于价值的方法？无模型和基于模型的对比呢？这些东西是相互排斥的吗？*

重要的是，作为一个勇敢的探索者，你应该了解这个领域的不同部分，以及不同方法之间的区别。我是来帮你的！

![](img/d6aa4a07b13b0f84ca7f6de4109b908b.png)

当然，没有比区分**政策外和政策内方法**更好的起点了。这两个术语用于描述*如何*在一般意义上，代理在培训阶段学习和表现，以及代理学习/表现的两种主要方式。

但是首先，重要的是要问:**什么是策略？**

用最简单的术语来说，策略*包括代理应该为每个可能的状态*采取的建议动作。

你可以把保单想象成一个概率分布。对于每个状态，策略是我们在该状态下采取任何可能行动的概率。该策略可能确定在某个州，有 60%的机会采取行动 1，30%的机会改变采取行动 2，以及 10%的机会采取行动 3。只是，政策并不能决定概率。这是概率。

毫不夸张地说，策略*是*对于每个状态 s ∈ S 采取每个动作 a ∈ A 的概率。策略由 **π** 表示。

在任何给定时刻，我们的代理都遵循一个策略**π**k。随着我们的代理人接受培训，这一政策也在不断变化和完善。每次策略稍有变化(我们经过一个梯度步骤，神经网络的权重不同)，现在策略已经变成了**π**k+1。即使两种政策之间的唯一区别是现在更有可能被选择的单一行动，它们也是两种完全不同的政策。

一切都清楚了吗？很好。

下一个重要的澄清是**行为策略**和**更新策略**之间的定义和区别。

行为政策很简单。这正是我在以上段落中描述的政策。这是一个代理在任何给定的状态下如何*行动*(概率等)。).

更新策略是*代理在计算状态-动作对的值时想象它将如何动作*。为了更好地理解这一点，让我们首先考虑代理如何计算状态动作对的值(在特定状态下采取特定动作的值)。

一个状态-行为对的值就是*预期收益*。预期回报是代理人认为在所述状态下采取所述行动后的总体回报，并继续向前直到环境被重置。代理人在想"*好吧，如果我在这个状态采取这个行动，我可能会在状态 s 结束。如果我在状态* s *，我可能会采取行动* a *，然后我可能会得到奖励 r，并在状态 s*′*结束。*“代理人不断地遵循这个思路。

但这里有一个问题:代理人如何知道处于状态 s 时会采取什么行动？你看，预期收益是在假设代理人遵循特定政策的前提下计算的。该策略是*更新策略。*

当一个代理计算一个状态-动作对的值时，它在想“如果我采取这个动作，然后遵循更新策略 **π** 向前，累积的回报会是什么？”

有了这些解释，我们终于可以达到这篇文章的目的了。这里输入了*符合策略的*和*不符合策略的*方法之间的区别。

在基于策略的方法中，行为策略和更新策略是同一个**T21。在非政策方法中，它们是不同的。**

在基于策略的方法中，假设代理将遵循*当前行为策略*向前，计算状态-动作对的值。因此，该值仅根据此特定策略来定义。

在非策略方法中，情况并非如此。让我们使用 Q-Learning，一种非策略方法，来展示这将是什么样子。

在 Q-Learning 中，通常使用一种叫做*ε-贪婪的策略。在这种策略中，代理人的行为开始时完全是随机的，随着时间的推移，慢慢地开始变得更加贪婪和剥削。这将是行为政策。*

只是，当我们在 Q-Learning 中计算一个状态-动作对的值时，我们假设在每个状态下，我们选择的动作使它认为它可以获得的未来回报最大化。我们的更新政策是一个贪婪的政策。

在 Q-Learning 中，我们的更新策略假设我们在遵循一个贪婪的策略，即我们选择我们认为会给我们带来最大回报的行动。

事实上，我们是半随机地行动，而**并不是**一直贪婪。我们的行为方针不是完全贪婪。

正因为如此，我们的更新策略(一个*贪婪*策略)不同于我们的行为策略(一个*ε-贪婪策略)*使得 Q-Learning 成为一种非策略方法。

了解这些区别是很重要的，我相信你会继续探索 RL 领域，掌握这些新知识并准备好向前探索！

![](img/1b59406e5522eaa78bf542a807563418.png)

*顺便说一句，如果你仍然对这个话题感到不清楚，请在 twitter @jereminuer 上给我发个 DM，我很乐意解释更多！*

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)