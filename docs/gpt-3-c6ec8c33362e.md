# GPT-3

> 原文：<https://medium.com/mlearning-ai/gpt-3-c6ec8c33362e?source=collection_archive---------2----------------------->

![](img/2096346e45efe0ee9b1cb563692cb4f9.png)

GPT-3 是我们这个时代最强大的人工智能之一。它可以总结高级文本，回答问题，纠正不完整的句子，集思广益，甚至根据描述性文本编写代码！Debuild.co 是一个用 GPT-3 构建的工具，用于构建基于文本的网络应用。

从高层次来看，它的工作很简单。它接受一系列单词作为输入，并以一些单词作为响应。就是这样。什么是模糊？

自 2012 年以来，AI 一直擅长识别人脸。该功能已经出现在脸书和 Instagram 等许多应用程序中。奇怪的是，翻译和处理文本变得更加困难。

以前处理文本的技术使用*递归神经网络*，但是这种技术对长文本不太好。翻译单个单词或短句很容易，但当句子变长时，模型需要了解更多关于其周围的情况。

在下面的两句话中，*服务器*这个词的意思完全不同，人工智能必须明白第一句话是关于计算机的，第二句话是关于人的:

> 看起来我刚刚使服务器
> 崩溃了，我可以检查一下吗？

因此，将文本翻译到 90%很容易，但要达到 100%却非常困难，因为要做出完美的翻译，必须具备真实世界的知识。

以前的人工智能在处理文本方面表现不佳的一个原因是旧的算法不能很好地并行化，所以任务不能被分割到许多不同的核心上。

2017 年出现了创新*变形金刚*。变压器很容易并行化。这个模型可以用大量的数据来训练。

变形金刚的主要创新包括*位置编码*、*注意*和*自注意*。自我注意解决了上述翻译问题。

除了处理文本，变形金刚还被用来解决其他非常高级的问题，比如下围棋和 for *蛋白质折叠*。

生成式预训练变形金刚 3 (GPT-3)是一个*海量语言模型*，由旧金山人工智能研究实验室 OpenAI 于 2020 年创建。它由一个使用变压器的神经网络组成。GPT-3 包含 96 个按顺序连接的变压器。变压器的数量决定了神经网络的深度。

GPT-3 在来自不同数据集(包括维基百科和书籍)的大约 45TB 的文本数据上进行训练。对于一个 GPU 来说，训练模型需要 300 多年，训练神经网络需要 500 万美元。)同一个模型被用在许多不同的用例中(写代码，翻译文本……)。该模型由 1750 亿个参数组成。一个参数对应于人脑中的一个神经元。

在高层次上，GPT-3 唯一做的事情是:接受一些单词作为输入，然后给出单词的延续。

这听起来可能没什么了不起。但是什么是人类呢？我们漂浮在空间中，接收输入，产生输出，更新我们的“状态”(记忆)。

我问了 GPT-3 一个没有上下文的模糊问题:

> 他们之间的距离是多少？

…我得到了这个答案:

> 这两点相距 6 英里。

不知道为什么。可能两点通常相距 6 英里:)

但是如果我给出一些背景:

> 月球和地球哪个最大？地球是最大的。
> 
> 他们之间的距离是多少？

…那么答案会更好:

> 地球和月球之间的平均距离约为 384，400 公里(238，900 英里)。

所以神奇的是，人工智能不只是对命令做出反应，而是根据上下文做出反应。

实际上，被保存和处理的不是单词，而是*记号*。一个令牌大约有四个字符长。像*笛卡尔*这样更长的单词被分成三个记号*desc-艺术-es* 。像*这个*、*或者*、*这个*这样简单的词就变成了一个令牌而已。

一个限制是 GPT-3 只能接受 2048 个令牌作为输入(它的*上下文窗口*)，大约 1500 个单词，相当于大约三页 A4 大小的文本。仍然有很多东西需要跟踪。脑子里只能同时保留几个字:)

OpenAI 开发了不同名字的模型:*阿达*、*巴贝奇*、*居里*、*达芬奇*。达芬奇的质量最好，但速度最慢。 *Ada* 质量最差但速度最快。

GPT 3 号如何学习新的东西？它使用基本的机器学习原理:

*   我们连续输入四个单词，如*太阳是*
*   我们知道下一个词应该是*星*
*   人工智能猜测*月亮*
*   我们评估猜测有多好(比如说 75%)，并基于此更新模型(这是实际学习发生的地方)

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)