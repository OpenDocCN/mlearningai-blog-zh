<html>
<head>
<title>Demystifying Neural Nets with The Shapley Value</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Shapley值解密神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/demystifying-neural-nets-with-shapley-values-cca29c836089?source=collection_archive---------1-----------------------#2021-12-05">https://medium.com/mlearning-ai/demystifying-neural-nets-with-shapley-values-cca29c836089?source=collection_archive---------1-----------------------#2021-12-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="3510" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">用Shapley值和博弈论拆箱</h2></div><p id="d1a4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi js translated">尽管深度学习的历史很短，但它的可解释性正迅速获得动力。可解释的人工智能源于对神经网络决策的公平和正义的日益增长的需求，以及避免编码偏见。所谓的黑盒人工智能可以根据与现实世界产生共鸣的偏差，对一个实体进行假设和预测。许多技术应运而生，以减轻这种有害影响的人，尤其是社会少数群体。这些技术的核心功能是解释神经网络的决策过程及其行为。解释模型行为最广泛使用的工具之一叫做<a class="ae kb" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">特征重要性</strong> </a>。然而，事实证明这并不是反映模型真实行为的最可靠和最稳健的方法。原因如下:</p><h1 id="75ba" class="kc kd hh bd ke kf kg kh ki kj kk kl km in kn io ko iq kp ir kq it kr iu ks kt bi translated">特征重要性</h1><ul class=""><li id="d9f1" class="ku kv hh iy b iz kw jc kx jf ky jj kz jn la jr lb lc ld le bi translated">首先也是最重要的，让我们了解特性重要性是如何运作的。<strong class="iy hi">特征重要性测量置换特征值后模型预测误差的增加</strong>。这意味着一个特性的<em class="lf">重要性</em>是由模型对该特性的依赖定义的<strong class="iy hi">。如果模型误差在改变特征的值后增加，则意味着该特征在预测中起了很大的作用。相反，如果模型误差在洗牌后仍然停滞不前，这个特性可能不会对模型的决策产生太大影响。</strong></li><li id="da4d" class="ku kv hh iy b iz lg jc lh jf li jj lj jn lk jr lb lc ld le bi translated">但是，特征重要性仅在线性模型上正确运行。这是由于特征重要性的本质，它将主要特征效应和与其他特征的交互效应相加。因此，任何具有负结果的特征被加上正结果。就像把结果压成一大块。结果是它并没有真正反映出该特性的正面和负面影响。</li><li id="ed2f" class="ku kv hh iy b iz lg jc lh jf li jj lj jn lk jr lb lc ld le bi translated">此外，添加相关要素会使输出的解释更加复杂。假设减肥只有一个重要的特征‘<em class="lf">跑步机’</em>。现在我加了'<em class="lf"> stairmill </em>'，对减肥也有效。我们还假设跑步机和跑步机高度相关。现在，这两个特性在特性重要性图中都降到了中等水平，而不是保持在最高位置。这样的结果需要额外的时间和努力来解释结果。</li></ul><p id="e083" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Shaley值是克服这些缺点的很好的选择。那么到底什么是沙普利值呢？</p><h1 id="212b" class="kc kd hh bd ke kf kg kh ki kj kk kl km in kn io ko iq kp ir kq it kr iu ks kt bi translated">沙普利值</h1><p id="6c6e" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">沙普利价值源于博弈论。博弈论是对理性主体之间互动决策的研究。在游戏实例中，逻辑玩家<em class="lf">决策者</em>会根据其他玩家的行动做出战略决策，以赢得游戏或实现他们想要的目标。Shapley值在评估每个参与者对游戏结果的贡献时变得很方便。<strong class="iy hi">通过对所有可能的联盟中所有代理的边际贡献进行加权平均来计算该值</strong>。在机器学习中，玩家或代理对应于特征，该特征的重要性用Shapley值来计算。下面是获得Shapley值的等式。看看公式下面用人类语言写的描述将有助于解释公式。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/2632b6e6ca8f0ee0b8a59ef45e060796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLZ3UBCs56pe01lWou_fww.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx">Image by Author</figcaption></figure><p id="cf06" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">数学符号已经说得够多了，让我们用一个真实的例子来理解它的价值。有两个玩家和四个不同的游戏。第一局，双方球员都没有打比赛。在第二场比赛中，只有1号选手上场。第三局，只有2号玩家上场。上一场比赛，所有选手都参加了。预测列表示每种情况的预测值。下面的要点解释了如何计算Shapley值。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es me"><img src="../Images/3014abd541f5a3fc9e09a3c480bcb55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mZ4WI9kH4CNWd-0l90D-w.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mf"><img src="../Images/5bfb37f16e92e504df1bba80c73259d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ELEovOIE_f6Qq0tYDrTcQ.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx">Image by Author</figcaption></figure><p id="fd26" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与功能重要性不同，Shapley值考虑了性能的下降，功能重要性会将负面影响与正面交互影响和主要功能影响一起添加。此外，它在生成结果时更健壮。Shapley值的优点不止于此。它不仅适用于线性模型，也适用于神经网络！你可以用这个值来解释任何机器学习模型。</p><p id="51dc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以使用python中的<strong class="iy hi">SHAP</strong>(Shapley Additive exPlanations)库轻松实现这个值。SHAP的缺点是计算量大且速度慢。此外，需要注意的是，Shapley值不应被解释为因果关系。仅仅因为某个特征有助于预测并不总是意味着因果关系。</p><p id="2691" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于Tensorflow的版本不兼容问题，您可能会遇到一些错误。我将在这里解释如何解决这些问题，并运行一个简单的实验。实验的内容包括美式化妆和韩式化妆的图像分类，以及哪些特征有助于模型的预测。</p><p id="26cc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这不是一个广泛的实验，而是为了快速检查SHAP如何应用于神经网络。在这个实验中，我使用了一个在小数据集上训练的CNN模型。因此，通过扩大数据集的大小，结果可能会显著改善。您可以通过这个<a class="ae kb" href="https://github.com/Irene-kim/Demystify_NeuralNets/tree/main/Shaply" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">链接</strong> </a>访问详细的代码和数据集。</p><h2 id="061b" class="mg kd hh bd ke mh mi mj ki mk ml mm km jf mn mo ko jj mp mq kq jn mr ms ks mt bi translated">以下是在实现SHAP时解决错误的非常有用的提示。</h2><blockquote class="mu mv mw"><p id="b270" class="iw ix lf iy b iz ja ii jb jc jd il je mx jg jh ji my jk jl jm mz jo jp jq jr ha bi translated">资源耗尽:使用形状[，，，]分配张量时出现OOM</p></blockquote><p id="51f2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当您遇到这个问题时，请尝试以下两个步骤:</p><ol class=""><li id="11d7" class="ku kv hh iy b iz ja jc jd jf na jj nb jn nc jr nd lc ld le bi translated">将批量减少到16(可以更大或更小，但不会显示相应的错误)</li><li id="c9d1" class="ku kv hh iy b iz lg jc lh jf li jj lj jn lk jr nd lc ld le bi translated">将输入维度减少到100(可以更大或更小，但不会显示相应的错误)</li></ol><blockquote class="mu mv mw"><p id="a2c9" class="iw ix lf iy b iz ja ii jb jc jd il je mx jg jh ji my jk jl jm mz jo jp jq jr ha bi translated">如果您使用的是Tensorflow版本&gt; 2.4.0</p></blockquote><p id="eb1b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">重新启动运行时，并在代码顶部包含以下代码。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="0587" class="mg kd hh nf b fi nj nk l nl nm">import tensorflow as tf<br/>import tensorflow.compat.v1.keras.backend as K<br/>tf.compat.v1.disable_eager_execution()</span></pre><h2 id="ba37" class="mg kd hh bd ke mh mi mj ki mk ml mm km jf mn mo ko jj mp mq kq jn mr ms ks mt bi translated">实验</h2><p id="9eef" class="pw-post-body-paragraph iw ix hh iy b iz kw ii jb jc kx il je jf ll jh ji jj lm jl jm jn ln jp jq jr ha bi translated">我有两个包含训练集和测试集的文件夹。文件夹中的每张图片分为两类:“美国化妆”和“韩国化妆”。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="46c6" class="mg kd hh nf b fi nj nk l nl nm">base_dir = '/content/gdrive/MyDrive/research/images'<br/>train_dir = os.path.join(base_dir, 'train')<br/>test_dir = os.path.join(base_dir, 'test')</span><span id="ba1e" class="mg kd hh nf b fi nn nk l nl nm">class_name = os.listdir(train_dir)<br/>class_name_test = os.listdir(test_dir)</span><span id="f86a" class="mg kd hh nf b fi nn nk l nl nm">print(class_name)<br/>#['american_makeup', 'korean_makeup'] </span><span id="de98" class="mg kd hh nf b fi nn nk l nl nm">print(class_name_test)<br/>#['american_makeup', 'korean_makeup']</span></pre><p id="c845" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">到目前为止，每个类都用一个分类变量编写。我需要把它们编码成整型变量。这里我使用LabelEncoder将类名改为整数。完成整数编码后，下一步是执行一键编码。类名“American _ making”现在标记为[1，0],“Korean _ making”标记为[0，1]。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="fad6" class="mg kd hh nf b fi nj nk l nl nm">integer_encoded = LabelEncoder().fit_transform(class_name)<br/>integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)</span><span id="013e" class="mg kd hh nf b fi nn nk l nl nm">onehot_encoded = OneHotEncoder(sparse=False).fit_transform(integer_encoded)</span><span id="43c4" class="mg kd hh nf b fi nn nk l nl nm">print(onehot_encoded)<br/>#[[1. 0.]  [0. 1.]]</span><span id="ac44" class="mg kd hh nf b fi nn nk l nl nm">integer_encoded_test = LabelEncoder().fit_transform(class_name_test)<br/>integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)</span><span id="e7a5" class="mg kd hh nf b fi nn nk l nl nm">onehot_encoded_test = OneHotEncoder(sparse=False).fit_transform(integer_encoded_test)</span><span id="5eae" class="mg kd hh nf b fi nn nk l nl nm">print(onehot_encoded_test)<br/>#[[1. 0.]  [0. 1.]]</span></pre><p id="4585" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">调整训练数据集和测试数据集的大小和形状，并将其保存到新列表中。每个相应的标签存储在训练和测试标签列表中。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="d33d" class="mg kd hh nf b fi nj nk l nl nm">train_image = []<br/>train_label = []<br/>test_image = []<br/>test_label = []</span><span id="c940" class="mg kd hh nf b fi nn nk l nl nm"># for train dataset<br/>for i in range(len(class_name)):<br/>    path = os.path.join(train_dir, class_name[i])<br/>    img_list = os.listdir(path)<br/>    for j in img_list:<br/>        img = os.path.join(path, j)<br/>        img = cv2.imread(img, cv2.IMREAD_COLOR)<br/>        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>        img = cv2.resize(img, (100, 100), interpolation =<br/>              cv2.INTER_CUBIC)<br/>        img = img.reshape((100, 100, 3))<br/>        train_image.append(img)<br/>        train_label.append(onehot_encoded[i])</span><span id="49ce" class="mg kd hh nf b fi nn nk l nl nm"># for test dataset<br/>for i in range(len(class_name_test)):<br/>    path = os.path.join(train_dir, class_name_test[i])<br/>    img_list = os.listdir(path)<br/>    for j in img_list:<br/>        img = os.path.join(path, j)<br/>        img = cv2.imread(img, cv2.IMREAD_COLOR)<br/>        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>        img = cv2.resize(img, (100, 100), interpolation =<br/>              cv2.INTER_CUBIC)<br/>        img = img.reshape((100, 100, 3))<br/>        test_image.append(img)<br/>        test_label.append(onehot_encoded[i])</span></pre><p id="53be" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在应用SHAP之前，还需要完成几个步骤。首先，我们需要标准化数据。另一项重要任务是实际了解您的数据是如何存储的。在我的数据集中，美式化妆图片首先被排序，然后是韩式化妆图片。为了避免任何不希望的信息泄漏或学习，我把整个数据混洗了一遍。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="ffec" class="mg kd hh nf b fi nj nk l nl nm"># Normalize image data</span><span id="064e" class="mg kd hh nf b fi nn nk l nl nm">X_train = shuffle(train_image.reshape(283, 100, 100, 3).astype("float32") / 255, random_state = seed)</span><span id="4000" class="mg kd hh nf b fi nn nk l nl nm">X_test = shuffle(test_image.reshape(20, 100, 100, 3).astype("float32") / 255, random_state = seed)<br/></span><span id="be13" class="mg kd hh nf b fi nn nk l nl nm"># Shuffle data or use library to randomly split your data</span><span id="f837" class="mg kd hh nf b fi nn nk l nl nm">train_label = shuffle(train_label, random_state = seed)<br/>test_label = shuffle(test_label, random_state = seed)</span></pre><p id="fbde" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在你终于可以设计你的模型了！你的模型的架构完全取决于你想如何构建你的模型<strong class="iy hi">，因为SHAP是模型不可知的</strong>！</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="aa56" class="mg kd hh nf b fi nj nk l nl nm">def CNN():</span><span id="1627" class="mg kd hh nf b fi nn nk l nl nm">input_layer = keras.Input(shape=(100,100,3))</span><span id="0dce" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Conv2D(150, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=seed))(input_layer)</span><span id="dabd" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.BatchNormalization()(x)</span><span id="2170" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.MaxPooling2D((2, 2))(x)</span><span id="7098" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Conv2D(150, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=seed))(x)</span><span id="b02f" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.BatchNormalization()(x)</span><span id="c6df" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.MaxPooling2D((2, 2))(x)</span><span id="bf0d" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Conv2D(150, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=seed))(x)</span><span id="cded" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.BatchNormalization()(x)</span><span id="a955" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.MaxPooling2D((2, 2))(x)</span><span id="0e10" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Conv2D(150, (3,3), padding='same', activation = 'relu', kernel_initializer = keras.initializers.HeUniform(seed=seed))(x)</span><span id="08b7" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.BatchNormalization()(x)</span><span id="efdf" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.MaxPooling2D((2, 2))(x)</span><span id="9a1a" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Flatten()(x)</span><span id="6c3d" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Dense(128, activation='relu', kernel_initializer = keras.initializers.HeUniform(seed=seed))(x)</span><span id="d1df" class="mg kd hh nf b fi nn nk l nl nm">x = keras.layers.Dropout(0.5)(x)</span><span id="486b" class="mg kd hh nf b fi nn nk l nl nm">output_layer = keras.layers.Dense(2, activation='sigmoid')(x)</span><span id="cb2c" class="mg kd hh nf b fi nn nk l nl nm">model = keras.Model(inputs=input_layer, outputs=output_layer, name = 'CNN')</span><span id="eca8" class="mg kd hh nf b fi nn nk l nl nm">model.compile(loss='binary_crossentropy', optimizer= keras.optimizers.SGD(learning_rate=0.001),  metrics=['acc', 'AUC'])</span><span id="4d5c" class="mg kd hh nf b fi nn nk l nl nm">return model</span></pre><p id="02f2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">训练好模型后，在十幅美国名人图片和十幅韩国名人图片上进行了测试。让我们看看结果中的一个例子。</p><pre class="lp lq lr ls fd ne nf ng nh aw ni bi"><span id="8740" class="mg kd hh nf b fi nj nk l nl nm">idx = 6<br/>input_val = X_test[idx:idx+1]<br/>output_val = model.predict(input_val)<br/>real = test_label[idx:idx+1]</span><span id="a8dd" class="mg kd hh nf b fi nn nk l nl nm">print("Prediction : ", np.argmax(output_val))<br/>print("Ground Truth : ", np.argmax(real))</span><span id="660a" class="mg kd hh nf b fi nn nk l nl nm">plt.imshow(input_val.reshape(100, 100, 3),interpolation='nearest')<br/>plt.show()</span></pre><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es no"><img src="../Images/23f4b6d3be731eb3ee034ce3281d62f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*Rs8SQ5jQ7y4hCggdxLQ92Q.png"/></div></figure><p id="cb31" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">模型准确的对图像进行了分类！剩下的呢？二十张照片中，有三张是错的。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es np"><img src="../Images/d6ba3b333d7f23c30a047ddbba3e484b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*QtnpLIX3oDx7_YJ3vHvRvg.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx">The Complete Testset Result</figcaption></figure><p id="9434" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我想知道是什么影响了模型的分类。一般来说，美妆和韩妆有两种非常鲜明的风格。美国人的化妆倾向于突出拱形眉毛的形状，长长的假睫毛，以及非常烟熏的眼妆。另一方面，韩国化妆倾向于渴望非常自然的眉毛、浅色阴影、清晰的妆容和橘红色的嘴唇。我要看看我的猜测是否真的有助于模型的预测。</p><p id="e2f2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这些是美国化妆风格的图像。负SHAP值指的是负面影响，正SHAP值指的是对模型决策的正面影响。左边的图片是地面真实图像。中间的是0号标签，是美式彩妆，最右边的是1号标签，是韩式彩妆。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nq"><img src="../Images/b00af817153c9b1de02bc5b7ed0c5909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2C6D8mNwwXnHiHbxhJWxA.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nr"><img src="../Images/80f20224729ab996cee2eaaeb5a9bfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFr4czyBLd4tUa_7_ttZZA.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nq"><img src="../Images/4c86d53d15081446c8121b3696b54882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KETOdZfCPpE5Rsfl3_uMxQ.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ns"><img src="../Images/0fb8f387c8d5993f3df0266859bc87aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7elUivzif6ttICY2NBHLSA.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es nt"><img src="../Images/4dd1d948aa25909545e5873fa052810d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*WSX6SJEC6Hk0bmOMwxXmMg.png"/></div></figure><p id="ce53" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">令人惊讶的是，中间图像的红点集中在眼睛和眉毛周围，标签为0(美国化妆品)。韩式图像上眼睛和眉毛周围的蓝点表明这些部位在告诉我们这不是韩式化妆图像。现在我们来看看相反的结果。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nu"><img src="../Images/b9ddb9f0d07419b0961d6d85ca8bae97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-fwvQnB4oYej8Qxza-77cA.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nv"><img src="../Images/70fec7c520eaab5951b19459b8fa01be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tT4FmQ5RRwC3vVv8CzLoXw.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ca"><img src="../Images/39d4d471e6a7e18ef3f38c560ecb4d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ao0MARtxTEF3qLAbKa9Jg.png"/></div></div></figure><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es nt"><img src="../Images/7902dcc62885cd2a20ebad4cf94a6168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*dMYQaew9VfQm5oMc4r24aw.png"/></div></figure><p id="5714" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与美国的化妆形象相比，韩式化妆就不那么鲜明了。然而，我们可以猜测，当模型将图像分类为韩国化妆时，这主要是由于面部的整体皮肤或结构，因为红点遍布所有面部。</p><p id="1289" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">分类错误的案例呢？</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es nw"><img src="../Images/015b76d3ef3f76c8d1e4c8483eedc5bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*EGk81jpfPQIWnSCtb6TZUg.png"/></div></figure><p id="b8e2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我们可以看到标签0和1在图像的各处都有红色和蓝色的点。有趣的一点是，所有贴错标签的病例都不是白色的。这样的结果可以提供非常重要的线索来修复和处理您的模型。在现实生活中，这种有偏见的结果会带来灾难性的后果，并歧视某些人群。</p><p id="296d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">SHAP有多大用处？活着是多么美好的时光:-)</p><div class="nx ny ez fb nz oa"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ob ab dw"><div class="oc ab od cl cj oe"><h2 class="bd hi fi z dy of ea eb og ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="oh l"><h3 class="bd b fi z dy of ea eb og ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oi l"><p class="bd b fp z dy of ea eb og ed ef dx translated">medium.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ly oa"/></div></div></a></div></div></div>    
</body>
</html>