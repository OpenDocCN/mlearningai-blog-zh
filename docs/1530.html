<html>
<head>
<title>Regularization : What? Why? and How? (Part -1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化:什么？为什么？又是怎么做到的？(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea?source=collection_archive---------3-----------------------#2021-12-31">https://medium.com/mlearning-ai/regularization-what-why-and-how-part-1-ef6bdb6bafea?source=collection_archive---------3-----------------------#2021-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="e388" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">如果你是真正提出正规化的人，你会采取什么样的方法？在学习正则化的时候，你是否无法将我们通常看到的不同概念联系起来？也许是因为我们孤立地了解这个问题，或者是因为缺乏破解这个问题的通用工具。如果你是一个天生的发现者，喜欢阅读，浏览这篇文章/故事，看看你的点点滴滴。</h2></div><blockquote class="iw ix iy"><p id="6d92" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">这篇文章分为两部分，在第一部分，我们将重点放在作为一个工具建立正则化的先决条件和直觉，在第二部分，我们将学习正则化作为一个机器学习的主题。</p></blockquote><p id="e150" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">每个故事都有独白，那为什么不在这里呢？所以，让我们开始吧，</p><p id="f03f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">自从机器学习，特别是深度学习作为一个问题解决领域出现以来，很多时间都花在寻找解决问题的最佳方法上。但是大多数尝试使用深度神经网络作为单一功能来解决问题的研究工作都失败了；主要是因为他们试图做一些被称为无约束优化的事情(别担心，我们稍后会谈到这个术语)。</p><p id="f84f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><em class="jb">只是在纳入正规化和一些其他技术后，快速进步才开始发生。但是为什么我们总的来说失败了？正规化是如何解决一些问题的？</em></p><p id="006c" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">让我们通过<em class="jb">首先理解过度拟合来理解这一点。</em></p><p id="9b80" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们假设其重要性足以训练我们的模型的数据集是真实连续人口分布的<em class="jb">离散子集。</em></p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es jz"><img src="../Images/f53d772a4f4997889dae36ad3b81d48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XWgAeOxJox2waBeXPsbTjg.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">These black points are things which comprise our dataset, and our true distribution is our red curve (sine function)</figcaption></figure><p id="6e0e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">因此<em class="jb">我们不能确定；即使我们在这上面拟合一个完美的模型，我们也将完美地拟合真实的分布</em>，我们的主要任务是近似真实的分布！<em class="jb">这导致了所谓的过拟合</em>的情况。作为偏差(你的训练模型与最佳可能模型有多接近；平均)非常低，定义如下</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kl"><img src="../Images/e90d7d9b59ce3e35064245f02be8ce23.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*zXd3ANkNPLFboL4a4kOBow.png"/></div></figure><p id="5b29" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">其中<em class="jb">f’(x)是您的训练模型，f(x)是完美/最佳可能模型</em>(可以在给定数据集上训练的最佳模型，我们认为该数据集是真实分布的一部分；但是足够显著以代表真实分布)，而不是整个群体。</p><p id="faec" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">方差(与预测平均值相比，每样本预测值的变化量)很高，</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es km"><img src="../Images/783c36255a542bd4ff815c673c7796e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*ULoiAcqMuSYc39MRbLan2A.png"/></div></figure><p id="0254" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">那么，这种<em class="jb">高方差和低偏差</em>的含义是什么？</p><p id="413e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">你的<em class="jb">模型是复杂的</em>，但是这意味着什么呢？在一般的ML意义上，复杂性可以指两种情况，第一，如果有<em class="jb">太多的可学习参数</em>，第二，如果参数有<em class="jb">高自由度增长</em>。那就是它们的<em class="jb">系数是无界的。</em></p><p id="f799" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">说得更清楚些；让我们用一些类比和例子来再次理解它，</p><p id="912c" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">如果有大量的参数，每个参数可以对应于每个输入进行映射，那么我们将最终学习一个高度复杂的函数。它类似于用一个非常大的勺子喝汤，它会完成任务吗？是的，值得吗？不会。另一方面，如果有大量的重量积累，这将使模型对输入的微小变化非常敏感，因此模型将非常不稳定，这也不是所希望的情况。</p><p id="5fb5" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">因此，如果我们以某种方式降低模型的复杂性，那么我们将克服过度拟合？是的，奥卡姆剃刀也象征着同样的事情，一个更简单的模型总是更好，这就是正则化介入的地方。</p><p id="e29b" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><em class="jb">正则化有助于惩罚任何类型的复杂性，可能是由于大量参数或大量权重累积导致的复杂性。</em></p><p id="237d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们可以通过以下任一方法来执行正则化:或者下述两种情况，</p><p id="905f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb"> 1。约束参数数量</em> </strong></p><p id="d20f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">稀疏性在这方面起着非常重要的作用。理解这一点的一般想法是<em class="jb">如果网络稀疏，可学习参数的数量将会减少，因此复杂性将会降低</em>，防止模型过度拟合。</p><p id="5f08" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="jc hi"> <em class="jb"> 2。参数值</em>的约束大小</strong></p><p id="d8a7" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">将<em class="jb">惩罚置于参数值</em>之上，这通常是通过在成本函数中添加一个惩罚项来实现的，这具有正则化效果。即<em class="jb">如果参数值/权重变得太大，我们的成本函数会造成很大的损失</em>。我们将在后面的章节中更清楚地看到这一点。</p><p id="0c3d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">上述两点也可以相互结合使用。就像在L1正则化的情况下，考虑较大权重和稀疏性的惩罚项。我们有整整一节是关于L1和l2的，所以，请耐心听我说。</p><p id="3187" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在最后几段中，我们看到了一些新术语，如成本函数、惩罚项和约束/无约束优化。让我们看看他们如何融入大局。</p><p id="8a2c" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在前两段，我开始说这个问题是因为我们试图解决一个无约束的问题，然后我突然跳到过度拟合，似乎可疑，对不对？是的，有一点，现在让我们把它们连接起来，</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kn"><img src="../Images/955b0a31666a28116bfa093d7f9a5079.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*kG1qq24J8uMSr5wbvpVn3A.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">Unconstrained optimization</figcaption></figure><p id="693b" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在上面的图表中，右上角的螺旋形的东西是我们想要近似的分布。轴上有我们的参数，所以有一点是肯定的，<em class="jb">如果模型必须符合分布，它需要沿着w1和w2轴移动</em>。(拟合分布意味着近似统计分布，如平均值和标准偏差)</p><p id="06b1" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这是关键时刻，<em class="jb"> w1和w2可以变得任意大以适应分布，这将使我们的模型变得复杂，并将导致过度拟合</em>，这意味着我们<em class="jb">试图解决一个无约束问题</em>，或者简单地说这是一个无约束优化。</p><p id="6899" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">有人可能会说，如果我们将分布标准化或规范化，会不会防止参数变得疯狂，答案是否定的，假设你想写一个函数y = f(x)，其中x是[1，1]，预期y是4。f的参数是w1和w2。有多少种方法可以将它们结合起来，得到期望的y，使用一个线性方程组。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ko"><img src="../Images/1df511cec08f37e4bc7a2387147d1908.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*8anugyPETNoz1Hlm6bh4EQ.png"/></div></figure><p id="91db" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">w1和w2可以是字面上的任何值，在(-无穷大，无穷大)的范围内，这将满足等式。</p><blockquote class="iw ix iy"><p id="cecb" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh"> 4 = 4*1 + 0*1 </em></p><p id="5792" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh"> 4 = 100*1 + (-96*1) </em></p><p id="ad16" class="iz ja jb jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><em class="hh"> 4 = (-500*1) + 504*1 </em></p></blockquote><p id="1f5f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们还想给予<em class="jb">更多的关注，关注对我们的数据</em>更重要的参数/特性。因此，我们最好的办法是不允许权重超过某个点，或者简单地限制权重值，因为这将有助于我们限制权重以及一些特殊的事情，如在L1正则化的情况下促进稀疏性(我们将在后面的部分中讨论)。</p><p id="607e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">众所周知，较大的权重意味着较高的复杂度，因此在大多数情况下会导致过度拟合。我们在上面看到的<em class="jb">解决方案是对参数值</em>进行一些限制/约束。因此，我认为无约束优化和过度拟合之间的联系应该是明确的。但是，我们实际上如何玩这个优化的东西，它到底是什么？让我们在下一节讨论这个问题。</p></div><div class="ab cl kp kq go kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="ha hb hc hd he"><p id="0a4a" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><strong class="jc hi">什么是优化？</strong></p><p id="a638" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">在一般的机器学习意义上，它是<em class="jb">求解一个目标函数来执行最大或最小评估</em>。实际上，优化在使用上要深刻得多。</p><p id="7513" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">然后，我们有两个术语，我们经常听到“优化”作为后缀。凸/非凸优化和线性/非线性优化。</p><p id="e6ec" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">对于线性和非线性优化，我们手头的任务的名称是相当明显的。这实际上是求解一个线性或非线性的方程组。</p><p id="eac3" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">但是就凸和非凸而言，它们有点不同，这两个家伙可能有完整的博客，因此我们将轻轻地选择我们需要的东西，然后继续前进。</p><p id="dc51" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们需要知道的几点:</p><p id="6b39" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">1.如果成本函数梯度的Hessian/双导数/Jacobian都是正的，则函数是凸的，如果都是负的，则函数是凹的，如果两者都是，则函数是非凸的。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kw"><img src="../Images/af8d4e2ee4458bbd8662869b76d957af.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*-Ej7vL8nmq3qy3vZhQ1e0Q.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">convex vs non-convex function</figcaption></figure><p id="89ab" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">2.出于直观的目的，<em class="jb"> hessian定义了函数</em>的曲率，想象有一条U型曲线，像y = x，如果你在曲线上画任何一条连接任意两点的线，那么曲线之间的所有点都会位于曲线之下。因此曲率低于直线。现在想象画许多线，如果对所有线都适用，那么它应该是凸的。但是如果它在某个位置失败，那么它是非凸的。对你来说，这就是为什么在训练我们的模型时，与局部最小值相比，遇到鞍点的风险更大。</p><p id="5a0e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">3.我们知道凸函数和非凸函数是什么，那么加两个凸函数呢？我们得到了什么？答案是<em class="jb">两个凸函数总会出一个凸函数</em>，但是一个凸和一个非凸合在一起会出一个非凸。但是两个非凸曲面最终可能会形成一个凸曲面。这就是为什么我们通常看到类似于多个L1或L2项组合的东西，因为两者都是凸的，因此可以优化。</p><p id="974f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">4.但是为什么只有凸优化呢？很长一段时间都有人猜测，<em class="jb">把问题想得那么难</em>是关于线性和非线性，但后来得出结论，主要是<em class="jb">关于凸性和非凸性</em>。实际上，<em class="jb">凸问题是我们有某种确定性方法来解决</em>的唯一问题，也就是说，通过使用<em class="jb"> KKT条件</em>，因此我们希望将自己限制在仅凸的区域内。也许，将来我们也能想出一种方法来确定性地解决非凸问题，但现在让我们享受我们所拥有的，并对我们没有的感到好奇。</p><p id="4e07" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">太好了！我们现在知道什么是凸优化，为什么它很重要。我们已经准备好了自己发现正则化的工具。首先恭喜你。为此拍拍自己的肩膀。让我们开始发现规律的过程。</p><p id="07ae" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们提到我们想要对我们的参数值施加一些限制/约束。但是我们怎么做呢？问问你自己，我们想要一个函数，每当w1或w2大于我们的阈值时，它会给出一个很高的惩罚。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kx"><img src="../Images/e36286b15fefee651bda59fe178807d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*_0MwUqWNsfBHome4ie7T0w.png"/></div></figure><p id="2538" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这不是我们想要的吗？函数g(w)可以是任何东西，比如平方函数或绝对函数。</p><p id="0e06" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在让我们把这一项加到我们实际的目标函数上，它通常是一个MSE或MAE。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ky"><img src="../Images/40b5908e71c76dc3dcece91d3967d391.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*lYy0adBfvetrwRgEnzrRrA.png"/></div></figure><p id="a31e" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">但是P(W)很像阶跃函数，因为一旦我们踏出阈值范围，就会有突然的惩罚/损失。这将导致不健康的梯度流动。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kn"><img src="../Images/27aaa2b3af9a77f7ffd0939e71165d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*fCvyll_FnJLg1Vv3rZrtpA.jpeg"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">step function like P(W) wrt W</figcaption></figure><p id="10c8" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">想象一下，如果阈值为1，我们的P(W)为1.001，我们仍然会看到巨大的损失，这并不好，为了让事情变得更平稳一些，我们在P(W)和一个新项lambda之间建立了线性依赖关系。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kn"><img src="../Images/cc5b68feb099002827193648bf0ef932.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*6klB2L8dRb_i8uwlIBnZ1A.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">Smoother P(W) with lambda term wrt W</figcaption></figure><p id="890d" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们的新方程变成了，</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kz"><img src="../Images/6e556330217ffde541a1c5019f33e2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*721ZAhZsjp226l-TCsQT3w.png"/></div></figure><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es kn"><img src="../Images/0fd919fe512616b57a3e3bdecad91262.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*hCC9zywe7NIMk3NB6Gsc8w.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">general regularization</figcaption></figure><p id="9762" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">它让你想起什么了吗？是的，这是您在学习正则化时通常会看到的熟悉等式。我们谈到的这个λ是你所谓的“正则化参数”，据说它平衡了目标项和惩罚项，但它的作用远不止我们上面看到的那样。</p><p id="36a3" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">您已经看到我们可以在目标中添加一个惩罚项，但是如果我们添加多个惩罚项呢？一个通用的等式应该是这样的，</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es la"><img src="../Images/fb2a95c4e152ab898e683a5185c4f88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*RwXg5ypV5v3MeaIFtyqS_A.png"/></div></figure><p id="7bc9" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">这整个方程一起被称为<em class="jb">拉格朗日</em>。所以，你可以将<em class="jb">拉格朗日函数定义为你的主目标函数与所有惩罚项/正则项的组合。</em></p><p id="0350" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在这些<em class="jb">罚项可以是任何东西，可以是l2范数(结果转化为L2正则化)或L1范数(结果转化为L1正则化)或两者的组合(是的，你是对的，弹性网)</em>。不是很直观吗？</p><p id="3d5f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在，让我们来一点技术，我们想要最小化我们的主要目标函数，MSE，对吗？当我们使用惩罚项越过阈值时，我们希望给予较大的惩罚。因此，我们可以把这个等式看作，</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es lb"><img src="../Images/06e2a1b99c198f0b4bbb368d866deac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*rMxFabpVyEvaQ4R9-yV2eA.png"/></div></figure><p id="caa1" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">从博弈论的角度来看，你可以认为这是一个最小-最大游戏，优化MSE导致更大的W，另一方面优化P导致更小的W。<em class="jb">优化(最小化)这两个函数的唯一最佳点是在P的边界/阈值处。</em></p><p id="26c0" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">从机器学习的角度来看，你可以这样理解，<em class="jb">权重值会随着它们变得更大而占更大的惩罚</em>，一旦它们增长到足以触及我们的分布，它们就会停止增长，因为任何高于这个值的值，虽然会降低MSE，但也会占非常重的惩罚。因此，这个边界充当优化最佳点。</p><p id="754c" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">现在，我们几乎完成了，我们知道只要将这个正则项添加到我们的主目标函数中，就会产生一个新的目标函数，它的优化可以防止我们过度拟合(因为我们刚才讨论的最小-最大博弈)。</p><p id="64f0" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们有多种选择来定义P(W ),它可能是L1/L2/ L1 + L2或其他什么，但它应该是凸的。</p><p id="7ad1" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">我们现在有了完整的正则化方程，只剩下关于P的选择的一些见解。<em class="jb">由于我们要考虑权重的大小，因此最明显的选择是使用我们称之为规范的东西。</em></p></div><div class="ab cl kp kq go kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="ha hb hc hd he"><p id="d1f7" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated"><em class="jb">至此，我们结束了我们的第1部分，并了解了优化及其与复杂性的关联，我们看到了一个通用的数学但易于理解的制定组合目标函数的观点，在下一篇文章中，我们将从这里开始，并从机器学习的角度理解我们的P(W)。</em></p><p id="254f" class="pw-post-body-paragraph iz ja hh jc b jd je ii jf jg jh il ji jw jk jl jm jx jo jp jq jy js jt ju jv ha bi translated">第二部分链接:<a class="ae lc" rel="noopener" href="/@rsiddhant73/regularization-what-why-and-how-part-2-4a075ad68ad2">https://medium . com/@ rsiddhant 73/regulation-what-why-why-how-part-2-4a 075 ad 68 ad 2</a></p><div class="ld le ez fb lf lg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hi fi z dy ll ea eb lm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">medium.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu kf lg"/></div></div></a></div></div></div>    
</body>
</html>