<html>
<head>
<title>Serverless Prediction at Scale, Part 2: Custom Container Deployment on Vertex AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大规模无服务器预测，第2部分:Vertex AI上的定制容器部署</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290?source=collection_archive---------1-----------------------#2021-08-25">https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290?source=collection_archive---------1-----------------------#2021-08-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e8dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于Google云平台上定制模型的无服务器部署的2部分系列的第2部分。这一部分涵盖了Vertex AI上的定制容器部署。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/31a2de8d7af2c7b8fe5409bb01304645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyaQBZ_sjdQeXgnOjwgATA.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Image by Author</figcaption></figure><p id="5fc1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在之前的帖子[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中，我已经分享了一个使用AI平台上的自定义预测例程，将自定义构建的机器学习模型部署到Google云平台(GCP)的经验。定制预测例程部署是无服务器的，并且被证明是高度可扩展的。在这篇文章中，我将通过使用自定义容器方法[ <a class="ae js" href="https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]来分享GCP新顶点AI [ <a class="ae js" href="https://cloud.google.com/vertex-ai" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]的另一个经验，从而继续这一讨论。</p><h1 id="3c1a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">顶点人工智能</h1><p id="90c5" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">最近宣布的[ <a class="ae js" href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-vertex-ai-unified-platform-for-mlops" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]，Vertex AI是GCP上的一个统一机器学习平台，它提供了一套全面的工具和产品，用于在单一环境中构建和管理ML模型的生命周期。它整合了传统人工智能平台和AutoML (Table/Vision/NLP)的许多先前产品，并补充了几个新的流行的ML产品和服务，如标记任务、管道、特征存储、实验、模型注册等。</p><p id="e312" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从模型部署来看，Vertex AI目前支持定制模型的两种部署类型:</p><ul class=""><li id="e63e" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">预制集装箱</li><li id="efad" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated">自定义容器</li></ul><p id="0331" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">预构建的容器旨在用于从常用的ML框架(包括Scikit-learn、XGBoost和Tensorflow)构建的模型。预测时，预建容器直接从指定框架的已保存模型工件中调用<em class="lk"> predict() </em>方法。预构建容器不支持预测时的自定义服务代码，如预处理和后处理所需的自定义代码。然而，定制容器支持所有类型的ML框架和定制服务代码。它还支持部署在Vertex AI之外训练的自定义模型。这个选项的缺点是用户需要构建自己的定制docker容器来进行部署。</p><p id="73c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于我们在上一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中试验过的药物不良反应(ADR)模型，我们无法在Vertex AI上使用预先构建的容器。我们将使用定制容器方法。</p><h1 id="7e1e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">创建Docker容器映像</h1><p id="2223" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">部署过程的第一步是为要部署的定制容器创建一个docker映像。就模型工件而言，Vertex AI允许它们存储在云存储桶中，并在容器启动时加载到容器中。或者，模型人工产物也可以作为图像内容本身的一部分直接嵌入到docker图像中。在这个实验中，我们将使用嵌入选项。</p><p id="b066" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我在工作目录中创建了一个docker文件夹，其中包含构建docker容器映像所需的所有内容:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ll"><img src="../Images/1ddd863de82166590cc27299ff9de790.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*qA48HvcEdQTxARDX05_ytA.png"/></div></figure><ul class=""><li id="e2f6" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated"><em class="lk"> model_files </em>:一个子文件夹，包含与前一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中描述的相同的4个保存的模型工件文件</li><li id="04e2" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> gcp_adr_predictor.py </em>:与上一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中显示的相同的自定义服务预测器类</li><li id="0b3e" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> adr_serving_utils.py </em>:与上一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中讨论的相同的自定义实用程序助手函数</li><li id="5967" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> Dockerfile </em> : docker构建文件</li><li id="987e" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> requirement.txt </em>:所有必需的Python库依赖项</li><li id="6fe5" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> server.py </em>:带有端点路由定义的Flask服务器代码</li><li id="ec59" class="kw kx hh ig b ih lf il lg ip lh it li ix lj jb lb lc ld le bi translated"><em class="lk"> wsgi.py </em>:一个简单的Flask服务器运行程序</li></ul><p id="fc46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于模型预测器类，我重用了前一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中描述的相同的模型服务代码。docker构建文件的内容和定制容器所需的python库如下:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="9549" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我使用Gunicorn和Flask作为docker容器的HTTP web服务器。端口5050从容器中公开，为传入的HTTP请求提供服务。然后，我通过运行以下命令构建docker映像:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="978e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这在我的本地机器上用指定的图像标记名创建了一个定制的docker图像。{PROEJCT_ID}和{REGION}是我的GCP项目ID和区域名称的占位符。{REPOSITORY}和{IMAGE}是定制容器的工件注册库和docker映像的名称。</p><h1 id="8634" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">本地测试自定义容器</h1><p id="d1b6" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在构建docker映像之后，我通过启动定制容器在本地对其进行了测试，以确保定制容器能够按照预期为HTTP预测请求提供服务:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="3903" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我将容器的内部端口5050映射到本地机器的端口5050，用于服务HTTP请求。然后，我通过以下方式测试容器:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="f7b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">"<em class="lk"> sample_input.json </em>"是一个json文件，包含了前一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中使用的测试模型的输入数据示例:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="70ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，所有的模型输入数据字段都需要由数组中的顶级元素“<em class="lk">实例</em>包装。在自定义容器实现中，顶点AI需要这种数据结构。</p><p id="4417" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定制容器应该返回一个HTTP响应，其中的JSON内容与前一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中显示的预测值相同。这确认了docker映像构建正确，并且定制容器在本地环境中正常工作。</p><h1 id="a6d8" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">部署自定义容器到顶点人工智能</h1><p id="26d4" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在本地测试之后，定制容器现在可以部署到Vertex AI。首先，我在GCP上创建了一个工件注册库，并将docker映像推送到这个库中:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="2d27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我使用工件库中推送的docker映像在Vertex AI上导入了一个定制模型:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="6dd2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，在上面的命令中，容器端口被指定为5050，健康检查和预测路由也是基于它们在Flask web服务器代码中的定义来指定的。模型导入完成后，可以通过导航到Vertex AI控制台或运行以下命令进行确认:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="ea10" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我创建了一个端点，并将定制模型部署到该端点以提供服务:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="d187" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中{ENDPOINT_ID}是端点创建后由顶点AI分配的端点ID。在这里，我指定集群的每个节点使用标准的2-vCPU机器类型，最小节点数是1，最大节点数是100(在1到100之间自动调整)。到模型端点的所有流量都被路由到已部署模型的当前版本。Vertex AI允许在端点模型部署绑定中进行流量百分比分割。如果需要新型号版本的逐步发布，可以使用这种流量划分来实施蓝绿色部署策略。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lo"><img src="../Images/5892f48dbe5eab2579c8b6c787981739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByNVCBk_RC2_n_-Dae1Tpg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx"><strong class="bd jv">Vertex AI Endpoint and ADR Model Bindings</strong></figcaption></figure><h1 id="c16f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">在Vertex AI上测试自定义容器</h1><p id="3b86" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">部署定制容器后，我运行了以下测试来确保端点正常工作:</p><ul class=""><li id="ea57" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">a)使用gcloud SDK进行测试:</li></ul><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><ul class=""><li id="c0a9" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">b)使用HTTP请求进行测试:</li></ul><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="a424" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c)使用Python客户端进行测试:</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lm ln l"/></div></figure><h1 id="5d9c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型API的负载测试</h1><p id="20c3" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">就像上一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中讨论的自定义预测例程一样，自定义容器提供的预测服务端点也可以通过Apigee代理作为REST API公开。我通过模拟1和100个并发在线用户，对定制容器的模型API进行了非常相似的负载测试。两个负载测试的服务响应时间如下所示:</p><p id="42d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">负载测试1:单用户</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lp"><img src="../Images/f81967192b115de2388dc0aa05946ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pii9NfIEJkxhmAO2qeByTw.png"/></div></div></figure><p id="b631" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">负载测试2: 100个并发用户</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lq"><img src="../Images/6177f3fe92aed439ef7b221df987e60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3K60RbnhA041FPAyE_lKA.png"/></div></div></figure><p id="d4d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单用户测试用例的平均响应时间约为1.47秒，100个并发用户测试用例的平均响应时间约为2.23秒。这个测试结果与我们在之前的帖子[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中从自定义预测例程中观察到的结果非常一致，表明了在Vertex AI上部署的自定义模型的高可扩展性。</p><p id="17ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是我从Vertex AI上的端点监控控制台捕获的一些模型操作指标。这些指标对应于100个并发用户的第二个负载测试用例的连续3次运行(总共30分钟)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/988d46f49a21f9bea579bed2ab7d3cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*jMFyrdn8ci5nAtPkT_Lq4A.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ls"><img src="../Images/5c96259a858134798b54faff2683fa9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*p66DJ8mslZY_xJH2WAJKIg.png"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/7a2d32891825c7d6c392ec26c7e24cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Wz6fYf_wom0LF6cx2rnByA.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/0c6057d3f2e1db1dbf592582dcce82d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*zY2um24AY6_ZAoGQMyV4kA.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/5cd33bed8d5646e8068e2b45c3f97df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*s_2CJdBz7FOqc1-2Fm6v8Q.png"/></div></figure><h1 id="a398" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">摘要</h1><p id="94da" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在这篇文章中，我分享了另一个使用定制容器方法在Vertex AI上部署定制模型的经验。类似于上一篇文章[ <a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f"> 1 </a> ]中讨论的定制预测例程，Vertex AI上的定制容器部署被证明是非常灵活和高度可伸缩的。</p><p id="0e82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在撰写本文时，Vertex AI还不支持自定义预测例程。如果你更喜欢使用自定义预测例程，而不是创建自己的自定义docker容器，此时你将需要继续使用传统的AI平台。</p><h1 id="b68b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">承认</h1><p id="ba2f" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">我感谢谷歌客户工程师布伦丹·杜汉和内森·霍德森在这次实验中的支持。</p><h1 id="d195" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="ec77" class="kw kx hh ig b ih kr il ks ip lw it lx ix ly jb lz lc ld le bi translated"><a class="ae js" rel="noopener" href="/mlearning-ai/serverless-prediction-at-scale-custom-model-deployment-on-google-cloud-ai-platform-d2d0807a0b8f">https://medium . com/mlearning-ai/server less-prediction-at-scale-custom-model-deployment-on-Google-cloud-ai-platform-d2d 0807 a0b 8 f</a></li></ol><p id="d6b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<a class="ae js" href="https://cloud.google.com/vertex-ai#section-1" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/vertex-a</a>我</p><p id="9678" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.<a class="ae js" href="https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/vertex-ai/docs/predictions/use-custom-container</a></p><p id="9a28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.<a class="ae js" href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-vertex-ai-unified-platform-for-mlops" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/blog/products/ai-machine-learning/Google-cloud-launches-vertex-ai-unified-platform-for-mlops</a></p></div></div>    
</body>
</html>