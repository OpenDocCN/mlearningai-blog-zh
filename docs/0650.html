<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/feature-selection-using-filter-method-python-implementation-from-scratch-375d86389003?source=collection_archive---------1-----------------------#2021-06-05">https://medium.com/mlearning-ai/feature-selection-using-filter-method-python-implementation-from-scratch-375d86389003?source=collection_archive---------1-----------------------#2021-06-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="561b" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">如何在Python中从头开始实现要素选择</h2></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="ebb6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">使用过滤方法在Python中实现特征选择的初学者指南。简明扼要，涵盖所有过滤方法的指南|概念和代码的简单实现</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/acc231e84651cde5a713240944e7e832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*s2KwuNJGVQBLN2Oa.jpg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.bergerpaints.com%2Fcolour-magazine%2Faugust%2F2016%2Fworld-of-colours%2Fcolours-and-nature%2F&amp;psig=AOvVaw0e6oWxdBSfXKBLYLTgpMcJ&amp;ust=1623000415832000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCKD1gvCBgfECFQAAAAAdAAAAABBE" rel="noopener ugc nofollow" target="_blank">Image Reference</a></figcaption></figure><h2 id="e88e" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">什么是特征选择？</h2><p id="05f9" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">特征选择，也称为变量/预测器选择、属性选择或变量子集选择，是选择相关特征子集以用于机器学习模型构建的过程。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/8d4d82cd14901e84fb1e5c80910a3886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*itE5HLR57zB9qWqc.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Image: Towards Data Science</figcaption></figure><p id="6eed" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">机器学习的工作原理很简单——如果你把垃圾放进去，你只会让垃圾出来。这里的垃圾是指数据中的噪音。</p><p id="2a81" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">当特征的数量非常大时，这变得更加重要。你不需要使用每一个特性来创建一个算法。你可以通过只输入那些真正重要的特征来帮助你的算法。“有时候，越少越好！”</p><p id="7980" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">你不仅减少了培训时间和评估时间，而且你要担心的事情也更少了！</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kc"><img src="../Images/d699aa5b1de4ca6c0d6bf94b129ba32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IhnLGwhlEfZcu0MI.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Feature Selection Procedure</figcaption></figure><h2 id="b4f4" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">特征工程与特征选择:</h2><p id="11c0" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">与仅使用原始数据相比，特征工程使您能够构建更复杂的模型。它还允许您从任意数量的数据中构建可解释的模型。功能选择将帮助您将这些功能限制在可管理的数量内。</p><h2 id="7e2c" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">特征选择与特征提取:</h2><p id="8426" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">要素选择用于过滤数据集中不相关或冗余的要素。特征选择和特征提取之间的关键区别在于，特征选择保留原始特征的子集，而特征提取创建全新的特征。</p><h2 id="23c0" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">特征选择在机器学习中的重要性</h2><p id="e8d6" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">在大多数情况下，数据科学获奖者与其他人的区别在于两个方面:功能创建和功能选择。</p><p id="9830" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">换句话说，它归结为创建捕捉隐藏的业务洞察力的变量，然后做出正确的选择，为您的预测模型选择哪个变量</p><h2 id="ea8f" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">使用特征选择的主要原因是:</h2><ul class=""><li id="1d6f" class="kd ke hh in b io jx is jy hr kf hv kg hz kh jf ki kj kk kl bi translated">它使机器学习算法能够更快地训练。</li><li id="9b70" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">它降低了模型的复杂性，使其更容易解释。</li><li id="860c" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">如果选择了正确的子集，就可以提高模型的准确性。</li><li id="f1e1" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">它减少了过度拟合。接下来，我们将讨论各种方法和技术，您可以使用这些方法和技术来划分特征空间的子集，并帮助您的模型更好、更高效地运行。</li></ul><p id="bb57" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated"><strong class="in kr">进行特征选择的方式:</strong></p><p id="2c19" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">特征选择主要有三种方式:</p><ol class=""><li id="865f" class="kd ke hh in b io ip is it hr ks hv kt hz ku jf kv kj kk kl bi translated">过滤方法(我们将在这篇博客中看到)</li><li id="9417" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated">包装方法(向前、向后消除)</li><li id="0d02" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated">嵌入式方法(拉索-L1、里奇-L2回归)</li></ol><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kw"><img src="../Images/e8cde417c0ee956eb8bca12ec2637f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7XDAYHFWIy4JXy-WXhUZw.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Types of Methods for Feature Selection. Image by Author</figcaption></figure><h2 id="aa8d" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">特征选择的过滤方法</h2><p id="ccb8" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">过滤方法基于某个单变量度量对每个特征进行排名，然后选择排名最高的特征。</p><h2 id="2d95" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">过滤器选择使用以下选项选择独立特征:</h2><ul class=""><li id="d28e" class="kd ke hh in b io jx is jy hr kf hv kg hz kh jf ki kj kk kl bi translated">没有常量变量</li><li id="3ec7" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">无/少四元常量变量</li><li id="6f42" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">没有重复的行</li><li id="9f43" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">与目标变量高度相关</li><li id="983a" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">与另一个独立变量相关性低</li><li id="827a" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">自变量的较高信息增益或互信息。</li></ul><h2 id="cae8" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">过滤方法的优势</h2><ul class=""><li id="fef6" class="kd ke hh in b io jx is jy hr kf hv kg hz kh jf ki kj kk kl bi translated">过滤方法是模型不可知的(兼容的)</li><li id="5bd6" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">完全依赖数据集中的特征</li><li id="c797" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">计算速度非常快</li><li id="cd29" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">基于不同的统计方法</li></ul><h2 id="fd11" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">过滤方法的缺点</h2><p id="583a" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">过滤方法着眼于单个特征，以确定其相对重要性。一个特性本身可能没什么用，但当它与其他特性结合起来时，可能会产生重要的影响。过滤方法可能会遗漏这些特征。</p><p id="9a2a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">需要记住的一点是，筛选方法不会移除多重共线性。因此，在为数据训练模型之前，还必须处理要素的多重共线性。</p><p id="4756" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">让我们开始吧。你可以在这里找到我的完整代码和数据集:<a class="ae jw" href="https://github.com/shelvi31/Feature-Selection" rel="noopener ugc nofollow" target="_blank">https://github.com/shelvi31/Feature-Selection</a></p><h2 id="6a8c" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">导入数据集和库</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="a904" class="hf hg hh ky b fi lc ld l le lf">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/>data = pd.read_csv(r"Standard Customer Data.csv", nrows=40000)    #Taking only 40K rows from the start<br/>data.shape</span><span id="a9ea" class="hf hg hh ky b fi lg ld l le lf">(40000, 371)</span></pre><p id="b715" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">重要的是在这里提到，为了避免过度拟合，特征选择应该只应用于训练集。</p><h2 id="8b22" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">将数据分为训练集和测试集</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="dfbb" class="hf hg hh ky b fi lc ld l le lf">train_x, test_x, train_y, test_y= train_test_split(data.drop("TARGET",axis=1),data.TARGET,test_size=0.2,random_state=41)<br/><br/>#Here, X = data.drop("TARGET",axis=1), Y = data.Target</span></pre><h2 id="6c32" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">步骤1:移除恒定特征</h2><p id="c8cb" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">常量要素是数据集中所有输出仅包含一个值的要素类型。恒定特征不能提供有助于分类手头记录的信息。因此，建议从数据集中移除所有不变的要素。</p><h2 id="634c" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">使用VarianceThreshold移除常量要素</h2><p id="c09b" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">为了去除恒定特征，我们将使用VarianceThreshold函数。该函数需要一个阈值参数值。为参数传递零值将过滤所有方差为零的要素，即常数要素。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="4efc" class="hf hg hh ky b fi lc ld l le lf">from sklearn.feature_selection import VarianceThreshold<br/>constant_filter = VarianceThreshold(threshold=0)<br/><br/><strong class="ky kr">#Fit and transforming on train data</strong><br/>data_constant = constant_filter.fit_transform(train_x)<br/>print(data_constant.shape)<br/><br/><strong class="ky kr">#Extracting all constant columns using get support function of our filter</strong><br/>constant_columns = [column for column in train_x.columns<br/>                    if column not in train_x.columns[constant_filter.get_support()]]<br/><br/><strong class="ky kr">#No. of constant columns</strong><br/>print(len(constant_columns))<br/><br/><strong class="ky kr">#Constant columns names:</strong><br/>for column in constant_columns:<br/>    print(column)</span><span id="d445" class="hf hg hh ky b fi lg ld l le lf"><strong class="ky kr">Output:</strong></span><span id="3789" class="hf hg hh ky b fi lg ld l le lf">(32000, 320)<br/>50<br/>ind_var2_0<br/>ind_var2<br/>ind_var18_0.... coloumn names<br/></span></pre><h2 id="61ed" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">从数据库中删除上述常量列:</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="95bb" class="hf hg hh ky b fi lc ld l le lf">data_cons = data.drop(constant_columns,axis=1)<br/>data_cons.shape</span><span id="191f" class="hf hg hh ky b fi lg ld l le lf">(40000, 321)</span></pre><p id="8d36" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">早先长度是371。现在是320。要查看常量列的名称:</p><h2 id="ec66" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">步骤2:移除准常数特征</h2><p id="e561" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">准常数特征，顾名思义，就是几乎不变的特征。这样的特征对于做出预测不是很有用。关于准常数特征的方差的阈值应该是什么，没有规则。</p><p id="a71b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">但是，根据经验，移除输出观测值中相似值超过99%的准常数要素。</p><p id="f6de" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">因此，我们将传递0.01而不是0作为阈值参数的值，这意味着如果一列中的值的方差小于0.01，则删除该列。换句话说，删除大约99%的值相似的特征列。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="d4ea" class="hf hg hh ky b fi lc ld l le lf">qcons_filter = VarianceThreshold(threshold=0.01)<br/><br/><strong class="ky kr">#Fit and transforming on train data</strong><br/>data_qcons = qcons_filter.fit_transform(train_x)<br/>print(data_qcons.shape)<br/><br/><strong class="ky kr">#Extracting all Quasi constant columns using get support function of our filter</strong><br/>qcons_columns = [column for column in train_x.columns<br/>                    if column not in train_x.columns[qcons_filter.get_support()]]<br/><br/><strong class="ky kr">#No. of Quasi constant columns</strong><br/>print(len(qcons_columns))<br/><br/><strong class="ky kr">#Quasi Constant columns names:</strong><br/>for column in qcons_columns:<br/>    print(column)</span><span id="9242" class="hf hg hh ky b fi lg ld l le lf"><strong class="ky kr">Output:</strong></span><span id="02a3" class="hf hg hh ky b fi lg ld l le lf">(32000, 265)<br/>105<br/>ind_var1<br/>ind_var2_0<br/>ind_var2<br/>ind_var6_0... coloumn names <br/></span></pre><p id="c092" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们有105个准常数。之前方差为0时我们得到50。这无疑是一个更好的结果。要保持的门槛取决于我们。</p><h2 id="94fa" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">从我们的数据库中删除上面确定的准常数列:</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="4f8d" class="hf hg hh ky b fi lc ld l le lf">data_qcons = data.drop(qcons_columns,axis=1)<br/>data_qcons.shape</span><span id="68ef" class="hf hg hh ky b fi lg ld l le lf">(40000, 266)</span></pre><p id="a22c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们数据的剩余形状是，我们现在还剩266列！</p><h2 id="267f" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">步骤3删除重复的列</h2><p id="323d" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">重复要素是具有相似值的要素。重复特征不会给算法训练增加任何价值，相反，它们会增加训练时间的开销和不必要的延迟。因此，始终建议在训练之前移除数据集中的重复要素。</p><p id="35c0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">对于常量和准常量要素，我们没有可以移除重复要素的内置Python方法。但是，我们有一种方法可以帮助我们识别pandas数据帧中的重复行。通过移调来使用它</p><h2 id="6a15" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">转置我们的“准常数”修改数据集。</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="fcfc" class="hf hg hh ky b fi lc ld l le lf">data_qcons_t = data_qcons.T<br/>data_qcons_t.shape</span><span id="2a59" class="hf hg hh ky b fi lg ld l le lf">(266, 40000)</span></pre><p id="9da4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">现在，我们的列已经取代了行，我们可以在列中找到副本:</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="e553" class="hf hg hh ky b fi lc ld l le lf">print(data_qcons_t.duplicated().sum())</span><span id="876b" class="hf hg hh ky b fi lg ld l le lf">21</span></pre><p id="d2d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">因此，即使删除了准常数列，我们还有21个重复的列要删除。</p><p id="1315" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">最后，我们可以使用drop_duplicates()方法删除重复的行。如果首先将字符串值传递给drop_duplicates()方法的keep参数，则除了第一个副本之外，所有重复的行将被删除。然后，我们将转置回我们的新数据。</p><h2 id="7762" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">使用drop_duplicates()删除重复的方法</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="3ce2" class="hf hg hh ky b fi lc ld l le lf">data_cons_dup = data_qcons_t.drop_duplicates(keep='first').T<br/>data_cons_dup.shape</span><span id="4853" class="hf hg hh ky b fi lg ld l le lf">(40000, 245)</span></pre><p id="f38f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们得到了一个更好的训练集，现在有245列。</p><h2 id="e06d" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">步骤4:将特征与目标变量相关联</h2><p id="5ea8" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">除了重复要素之外，数据集还可以包含相关要素。识别与目标变量高度相关的输入特征。这里我们打印每个输入特征与目标变量的相关性。</p><p id="ff6a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">由于该数据库中的列相关性很低，我们将使用其他数据库进行计算。</p><h2 id="c33a" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">导入和处理数据库</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="508f" class="hf hg hh ky b fi lc ld l le lf">cardata = pd.read_csv("mpg.csv")<br/>cardata.dropna()<br/>cardata.shape<br/>cardata.columns</span><span id="01eb" class="hf hg hh ky b fi lg ld l le lf">Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',<br/>       'acceleration', 'model_year', 'origin', 'name'],<br/>      dtype='object')</span><span id="46d2" class="hf hg hh ky b fi lg ld l le lf"># cardata.info()</span></pre><h2 id="a7e4" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">处理分类变量:</h2><p id="e3f3" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">通过查看列的数据类型，可以看出有3个分类变量。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="7a3f" class="hf hg hh ky b fi lc ld l le lf">#Updating the horsepower feature to int<br/>cardata["horsepower"]= pd.to_numeric(cardata["horsepower"])</span></pre><p id="173f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们看到马力不再是一个分类变量，汽车名称是唯一的分类变量。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="b24f" class="hf hg hh ky b fi lc ld l le lf"># Label Encoding:<br/>    <br/>from sklearn.preprocessing import LabelEncoder<br/>labelencoder = LabelEncoder()<br/>X_en= cardata.iloc[:, 8].values<br/>X_en = labelencoder.fit_transform(X_en)</span><span id="534e" class="hf hg hh ky b fi lg ld l le lf">cardata = cardata.drop(["name","origin"],axis=1)<br/>cardata["name"] = X_en</span></pre><h2 id="9ee6" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">创建输入要素X和目标变量y</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="4b4b" class="hf hg hh ky b fi lc ld l le lf">y= cardata["mpg"]<br/>X = cardata.iloc[:,1:8]<br/>X.head(2)</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="ab fe cl lh"><img src="../Images/e8f86eca71c08ba20254da51ab30e23c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MuaLz640gxniuh_eOu8azQ.png"/></div></figure><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="0c47" class="hf hg hh ky b fi lc ld l le lf">y.head(2)</span><span id="53c0" class="hf hg hh ky b fi lg ld l le lf">0    18.0<br/>1    15.0<br/>Name: mpg, dtype: float64</span><span id="78e9" class="hf hg hh ky b fi lg ld l le lf">#Create a data set copy with all the input features after converting them to numeric including target variable<br/>full_data= X.copy()<br/>full_data["mpg"]= y<br/>print(full_data.head(2))</span></pre><h2 id="5cc1" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">寻找独立预测因子与目标变量的相关性:</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="6da9" class="hf hg hh ky b fi lc ld l le lf">imp = full_data.drop("mpg", axis=1).apply(lambda x: x.corr(full_data.mpg))<br/>print(imp)</span><span id="f633" class="hf hg hh ky b fi lg ld l le lf">indices = np.argsort(imp)<br/>print(indices)</span><span id="33aa" class="hf hg hh ky b fi lg ld l le lf">print(imp[indices])     #Sorted in ascending order</span></pre><h2 id="c0ec" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">可视化绘图</h2><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="2ce9" class="hf hg hh ky b fi lc ld l le lf">import matplotlib.pyplot as plt<br/><br/>names=['cylinders','displacement','horsepower','weight','acceleration','model year', 'name']<br/>plt.title('Miles Per Gallon')<br/><br/>#Plotting horizontal bar graph<br/>plt.barh(range(len(indices)), imp[indices], color='g', align='center')<br/>plt.yticks(range(len(indices)), [names[i] for i in indices])<br/>plt.xlabel('Relative Importance')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="ab fe cl lh"><img src="../Images/ed8aeee8a2c9f7c526ba9c1e02bae0dc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xSgUW0Z5a-ubsUxK2gnUmg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Output Image by Author</figcaption></figure><p id="5b2f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">上面显示的是每个特性与我们的目标变量(target)的相关性。我们希望保留仅与目标变量高度相关的特征。这意味着输入特征对预测目标变量有很大的影响。</p><p id="6c89" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们将阈值设置为绝对值0.4。只有当输入特征与目标变量的相关性大于0.4时，我们才保留输入特征</p><p id="7cef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">确定高度共线变量的相关阈值应该是0.50或接近0.50。将绝对值视为负相关和正相关。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="8b1a" class="hf hg hh ky b fi lc ld l le lf">for i in range(0, len(indices)):<br/>    if np.abs(imp[i])&gt;0.4:<br/>        print(names[i])</span><span id="f5d8" class="hf hg hh ky b fi lg ld l le lf">cylinders<br/>displacement<br/>horsepower<br/>weight<br/>acceleration<br/>model year</span></pre><p id="621b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">也就是说，根据我们对预测器与目标关系的观察，汽车名称可以从我们的数据集中删除。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="9815" class="hf hg hh ky b fi lc ld l le lf">X.drop("name",axis=1);</span></pre><h2 id="5aa8" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">第五步:与其他变量的相关性</h2><p id="8f5a" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">识别与其他独立变量相关性较低的输入特征。根据步骤1遍历所有过滤后的输入要素，并检查每个输入要素与所有其他输入要素的相关性。我们将保留与其他输入要素不高度相关的输入要素</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="dc55" class="hf hg hh ky b fi lc ld l le lf">for i in range(0,len(X.columns)):<br/>    for j in  range(0,len(X.columns)):<br/>        if i!=j:<br/>            corr_1=np.abs(X[X.columns[i]].corr(X[X.columns[j]]))<br/>            if corr_1 &lt;0.3:<br/>                print( X.columns[i] , " is not correlated  with ", X.columns[j])<br/>            elif corr_1&gt;0.75:<br/>                print( X.columns[i] , " is highly  correlated  with ", X.columns[j])</span><span id="37f6" class="hf hg hh ky b fi lg ld l le lf">cylinders  is highly  correlated  with  displacement<br/>cylinders  is highly  correlated  with  horsepower<br/>cylinders  is highly  correlated  with  weight... and so on </span></pre><p id="5eff" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">排量，马力，气缸，重量高度相关。我们将只保留其中的一个。基于以上结果，我们保留了气缸、加速度和年款，去掉了马力、排量和重量</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="0432" class="hf hg hh ky b fi lc ld l le lf">X= X.drop(["cylinders","weight","displacement"],axis=1);</span></pre><p id="8f72" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">在数据集有大量输入变量的情况下，我们总是可以使用皮尔逊或斯皮尔曼系数来计算相关变量。</p><h2 id="514c" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">步骤6:寻找相互信息或信息增益</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es li"><img src="../Images/da32421038ea5c53ce4e4d3392aaaec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nMad4pPf1bNk5R-I.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Image from Wikipedia. I(X; Y) represents Mutual information.</figcaption></figure><p id="af46" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">根据维基百科，在概率论和信息论中，两个随机变量的互信息(MI)是两个变量之间相互依赖的度量。更具体地说，它量化了通过观察一个随机变量获得的关于另一个随机变量的“信息量”。</p><p id="b24e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">根据Sklearn文档，两个随机变量之间的互信息(MI)是非负值，它衡量变量之间的相关性。当且仅当两个随机变量独立时，它等于零，更高的值意味着更高的依赖性。</p><p id="c794" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们将找到自变量相对于目标变量的信息增益或互信息。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="4de4" class="hf hg hh ky b fi lc ld l le lf">X.info()<br/>X.shape<br/>y.isnull()<br/>y.shape</span><span id="a983" class="hf hg hh ky b fi lg ld l le lf">(398,)</span><span id="e903" class="hf hg hh ky b fi lg ld l le lf">from sklearn.feature_selection import mutual_info_regression<br/>mi = mutual_info_regression(X, y);</span></pre><p id="0947" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">返回:每个特征和目标之间的估计互信息。</p><pre class="jh ji jj jk fd kx ky kz la aw lb bi"><span id="c709" class="hf hg hh ky b fi lc ld l le lf"># Plotting the mutual information<br/><br/>mi = pd.Series(mi)<br/>mi.index = X.columns<br/>mi.sort_values(ascending=False)<br/>mi.sort_values(ascending=False).plot.bar(figsize=(10, 4))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lj"><img src="../Images/12fd5ea326880b0b6c1d5657d8fa540e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7VlBYvcA_z6QUjLo.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Output Image by Author</figcaption></figure><p id="9e3f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我们现在有了预测每加仑英里数的特征重要性。每加仑行驶的英里数可以根据汽车的汽缸数量、汽车制造年份和加速度来预测。</p><p id="3b1e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">因此，用于特征选择的过滤方法是模型不可知的、简单的并且易于解释。</p><h1 id="0cdd" class="lk hg hh bd hi ll lm ln hm lo lp lq hq lr ls lt hu lu lv lw hy lx ly lz ic ma bi translated">过滤器和包装器方法之间的区别</h1><p id="ee08" class="pw-post-body-paragraph il im hh in b io jx iq ir is jy iu iv hr jz ix iy hv ka ja jb hz kb jd je jf ha bi translated">用于特征选择的过滤器和包装器方法之间的主要区别是:</p><ul class=""><li id="2138" class="kd ke hh in b io ip is it hr ks hv kt hz ku jf ki kj kk kl bi translated">过滤方法通过特征与因变量的相关性来衡量特征的<strong class="in kr">相关性，而<strong class="in kr">包装方法通过实际训练一个模型来衡量特征子集</strong>的有用性。</strong></li><li id="4490" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">与包装方法相比，过滤方法<strong class="in kr">要快得多</strong>，因为它们不涉及训练模型。另一方面，包装方法在计算上也非常昂贵。</li><li id="4af4" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">过滤方法使用统计方法来评估特征子集，而包装方法使用交叉验证。</li><li id="4766" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated"><strong class="in kr">过滤方法在许多情况下可能无法找到特性的最佳子集</strong>，但是包装方法总能提供特性的最佳子集。</li><li id="9786" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf ki kj kk kl bi translated">与使用过滤方法的特征子集相比，使用<strong class="in kr">包装方法的特征子集使模型更容易过度拟合</strong>。</li></ul><p id="03c3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">以下是我发现对其他要素选择方法有用的教程:<a class="ae jw" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/12/带示例的要素选择方法简介或如何选择正确的变量/T1</a></p><p id="bc19" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">我希望你觉得这个指南有用。不要忘记阅读其他要素选择方法，以便为购物篮添加更多数据科学工具。</p><p id="be43" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv hr iw ix iy hv iz ja jb hz jc jd je jf ha bi translated">干杯！</p><h1 id="30df" class="lk hg hh bd hi ll lm ln hm lo lp lq hq lr ls lt hu lu lv lw hy lx ly lz ic ma bi translated">参考:</h1><ol class=""><li id="9718" class="kd ke hh in b io jx is jy hr kf hv kg hz kh jf kv kj kk kl bi translated"><a class="ae jw" href="https://stackabuse.com/applying-filter-methods-in-python-for-feature-selection" rel="noopener ugc nofollow" target="_blank">https://stackabeuse . com/将python中的筛选方法应用于特征选择</a></li><li id="2667" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated"><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/12/功能选择方法简介-带示例或如何选择正确的变量/ </a></li><li id="c3ea" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated"><a class="ae jw" href="https://towardsdatascience.com/feature-selection-in-python-using-filter-method-7ae5cbc4ee05" rel="noopener" target="_blank">https://towards data science . com/python中的功能选择-使用筛选方法-7ae 5 CBC 4c ee05</a></li><li id="bb6c" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated"><a class="ae jw" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sklearn . feature _ selection。VarianceThreshold.html</a></li><li id="38f0" class="kd ke hh in b io km is kn hr ko hv kp hz kq jf kv kj kk kl bi translated"><a class="ae jw" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sklearn . feature _ selection . mutual _ info _ reversion . html</a></li></ol></div></div>    
</body>
</html>