<html>
<head>
<title>Taking a DALL-E type Text to Image model for a Spin</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">采用DALL-E型文本到图像模型进行旋转</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/taking-a-dall-e-type-text-to-image-model-for-a-spin-b6fb92b7c458?source=collection_archive---------1-----------------------#2021-04-18">https://medium.com/mlearning-ai/taking-a-dall-e-type-text-to-image-model-for-a-spin-b6fb92b7c458?source=collection_archive---------1-----------------------#2021-04-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/c8d01f579ece44ec357cd2e8864eeff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*vZZmCrTG3HrkuV-Yp_QutA.jpeg"/></div></figure><p id="8cdd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">深度学习领域最近的热潮是DALL-E论文和2021年1月初由<a class="ae jj" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>发表的结果。它可以将一个句子转换成一幅图像，目前是文本到图像建模的最新技术。由<a class="ae jj" href="https://github.com/lucidrains/DALLE-pytorch" rel="noopener ugc nofollow" target="_blank"> lucidrains </a>在Pytorch中实现/复制了一个更小的Dall-e模型架构。Dall-E name是皮克斯公司的Wall-E上的一个文字游戏，用“D”代替了著名画家萨尔瓦多·达利的“W”。我看了由<a class="ae jj" href="https://www.youtube.com/watch?v=j4xgkjWlfL4" rel="noopener ugc nofollow" target="_blank">扬尼克·基尔彻</a>制作的关于Dall-E模型技术细节的精彩视频。我决定试着在<a class="ae jj" href="https://github.com/lucidrains/DALLE-pytorch" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>版本的基础上训练一个小版本的Dall-E模型，在这里重新讲述我的经历。我一直觉得，当你修补DL模型的训练和推理部分时，你会学到很多东西。</p><p id="fdab" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">还有一点——open ai还没有开源发布完整的DALL-E模型，将来也可能不会。它最有可能通过付费的API调用来访问(就像今天的GPT-3)。他们只开源了VAE模型，并明确表示“用于从文本生成图像的转换器不属于这个代码版本。”DALL-E实际上是两个锁在一起的深度学习模型，我将简单介绍一下。我完全理解OpenAI的立场，因为过去的开源代码已经被大型技术公司大量利用，却没有相应地回馈开源社区。我们都记得Elasticsearch和AWS在这个问题上的争论。</p><p id="3a44" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这里，我不会讨论DALL-E模型的技术细节。这更多的是我在Pytorch 上玩DALL-E type <a class="ae jj" href="https://github.com/lucidrains/DALLE-pytorch" rel="noopener ugc nofollow" target="_blank">模型的经历。Dall-E有两个深度学习模型锁在一起。一种是将文本转换到潜像空间的变换器模型和将潜像空间转换到实际图像的变分编码器/解码器模型。离散VAE模型最初在数据集中的图像上被训练，以学习潜像向量空间。然后，预训练的离散VAE模型被用作变换器模型的一部分，以学习可以将文本转换成图像的变换器权重。这里需要注意的一点是，将文本翻译成图像的任务是不明确的:一个单独的标题通常对应于无数个看似合理的图像，因此图像不是唯一确定的。Dall-E模型通过对潜像空间向量进行某种采样，实际上为同一字幕生成了许多图片。在他们的官方DALL-E博客中，该模型实际上为一个标题生成了512幅不同的图像，然后他们使用一个</a><a class="ae jj" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">剪辑</a>模型为该标题挑选了前25幅图像，该模型为每幅图像分配文本匹配概率。OpenAI发布的<a class="ae jj" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> CLIP </a>模型和Dall-E模型一样重要。令人惊讶的是，CLIP可以在ImageNet数据上比Resnet-50模型做得更好，而无需实际在ImageNet数据上进行训练。</p><p id="137a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我使用了<a class="ae jj" href="https://github.com/lucidrains/DALLE-pytorch" rel="noopener ugc nofollow" target="_blank"> lucidrains </a>中的<a class="ae jj" href="https://github.com/lucidrains/DALLE-pytorch/blob/main/examples/rainbow_dalle.ipynb" rel="noopener ugc nofollow" target="_blank">示例</a>笔记本来试用Dall-E型号。此示例基于简单的几何图形(如正方形、矩形、圆形、六边形等)生成32x32的图像和文本。这些图像有不同的填充、大小和颜色。一个示例文本可以像“大填充阴影橙色菱形旋转两次”。图像尺寸比openAI使用的256 x 256图像小得多。我首先使用我的RTX-2080 GPU训练VQ-VAE模型。训练大概花了1个小时。之后，我使用预先训练的VQ-VAE模型来训练Dall-E变压器模型，这也花了大约1小时。使用具有指数速率调度器的1e-3的学习速率。我不得不使用64批次大小，因为任何更多的东西都不适合我的8GB GPU内存。大约100个时期后，损失变平。下面也显示了实际图像与预测图像的对比——有些图像缺少颜色，但形状正确。</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es jk"><img src="../Images/a5b1336f75858191c77a0d96b290b2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*on3P0P5jzcsoRQdvOTb_CQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Dall-E Type Model Training loss vs Epoch</figcaption></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es jt"><img src="../Images/62a088612ff112ddd699be34bcb7c3b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*L-kUTArF0W5uiHdPH2XuLQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Top Row is Actual Image for the Test Text and Bottom Row is predicted Image</figcaption></figure><p id="8074" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">来自Naver的研究工程师Kobiso已经在<a class="ae jj" href="http://www.vision.caltech.edu/visipedia/CUB-200.html" rel="noopener ugc nofollow" target="_blank"> CUB200 </a>数据集上训练了Dall-E类型的模型。他已经发布了预训练模型，我决定使用预训练模型进行推理，看看效果如何。预训练模型使用<a class="ae jj" href="https://www.deepspeed.ai/" rel="noopener ugc nofollow" target="_blank">deep speed</a>——一个微软深度学习优化库，使分布式训练变得简单、高效和有效——<em class="ju">10倍大的模型，10倍快的训练，最少的代码更改。</em>最初的Dall-E模型有120亿个参数。任何超过1.3 bil参数的模型都无法安装在单个GPU上(即使是32 GB内存的GPU)。因此，模型本身必须实现并行化——层内或层间——分成跨多个GPU的片段。这与数据并行化不同。DeepSpeed必须有助于模型并行化。我不得不安装deepspeed来测试他预先训练好的模型。然后我遇到了多个错误——首先是deepspeed要求Cuda小于11.0。这很容易处理。然后，第二个错误是“<em class="ju">由于硬件/软件问题导致不兼容，无法JIT加载sparse _ attn op</em>’。然后我在DeepSpeed库中查找了这个问题，他们说'<em class="ju">稀疏注意力内核是用Triton编写的，目前只在Tesla V100上工作；我们将很快升级到处理安培。但是，它与GeForce RTX不兼容。不知道为什么他们不能支持RTX的图灵架构。</em></p><p id="6823" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">由于Dall-E模型使用CLIP对图像进行排序，所以我决定尝试CLIP模型。我在这里找到了一个用于剪辑<a class="ae jj" href="https://github.com/svpino/clip-container" rel="noopener ugc nofollow" target="_blank">的docker容器</a>，它也与Sagemaker兼容。几个小时后，我明白了如何在Sagemaker端点上加载这个docker映像。CLIP类型的模型需要GPU进行快速推理——保持Sagemaker端点运行用于演示目的是非常昂贵的(即使最少的单个CPU实例的运行费用也是50美元/月)。AWS Lambda无服务器服务也不会有帮助，因为它无法处理GPU实例，并且受到docker图像大小的限制(10GB大小限制)。随后我试用了这款<a class="ae jj" href="https://colab.research.google.com/drive/1ZSDOa6W5NxNdIwT79zFCEE4UwQ3PZjoU?usp=sharing#scrollTo=00bHucsqrjsc" rel="noopener ugc nofollow" target="_blank"> Colab </a>笔记本进行剪辑测试。我可以看到使用CPU和GPU实例之间至少有3倍的时间差异。我将单独写更多关于剪辑的内容。</p></div></div>    
</body>
</html>