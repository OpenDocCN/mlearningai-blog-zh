<html>
<head>
<title>Tuning Neural Networks Part III</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调谐神经网络第三部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/tuning-neural-networks-part-iii-43dfd0c8600f?source=collection_archive---------2-----------------------#2021-11-23">https://medium.com/mlearning-ai/tuning-neural-networks-part-iii-43dfd0c8600f?source=collection_archive---------2-----------------------#2021-11-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="207a" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">哪些激活功能可以让您了解</h2></div><p id="ee14" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">本系列旨在通过考察整定参数如何影响神经网络的学习内容和方式，加深对神经网络的理解。内容假设您通过阅读本系列</em><em class="js"><a class="ae jt" rel="noopener" href="/@gallettilance/neural-networks-a-very-simple-derivation-from-logistic-regression-b2b972f29138"><em class="js">可以获得一些神经网络的先验知识。</em></a></em></p><p id="e2ba" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8">第一部分:数据标准化的重要性</a></p><p id="af29" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69">第二部分:初始化注意事项</a></p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><h1 id="09a5" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">学习了哪些类型的特征？</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/3460f7689ea7b5841bd6db706ea76ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*DUTetXGgB0Za8phglTQGYw.gif"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">LEFT: decision boundary in the learned feature space (using tanh) | RIGHT: decision boundary in the input space</figcaption></figure><p id="4789" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经网络的第一个隐藏层中的特征是通过激活输入的仿射函数来创建的，如上所述。下一个隐藏层中的要素是通过进一步拉伸、移动和变换在前一层提取的要素来创建的，以创建更复杂的要素。这可以在到达最后一层之前做任何次数，最后一层是根据所学的特征来建立Y模型。</p><p id="30cc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">注</strong>:不一定总是这样。例如，可以创建一个网络，通过变换上一层要素的加权乘积来创建要素。在卷积神经网络中，通过取上一层特征的最大值、最小值或平均值来创建一些特征。</p><p id="3e24" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">激活功能的选择以及隐藏层的数量和组成都对网络能够学习的功能有很大影响。在这里，我们将重点关注激活功能，并在第四部分检查网络架构。</p><p id="aa86" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">没有激活功能(或有身份激活功能)，网络只会学习线性特征，详见<a class="ae jt" rel="noopener" href="/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8">第一部分</a>。激活函数允许创建输入的非线性函数特征。</p><p id="c988" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们深入探讨激活函数允许您学习的内容之前，我们可能会问，我们希望从激活函数中获得哪些属性。</p><h1 id="ae46" class="kb kc hh bd kd ke lj kg kh ki lk kk kl in ll io kn iq lm ir kp it ln iu kr ks bi translated">激活函数应该具有哪些属性？</h1><p id="3b55" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">技术上，激活功能唯一的要求是<strong class="iy hi">可微性</strong>。否则，基于梯度的学习方法是不可能的。</p><p id="7723" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">激活函数有很多好的特性:</p><ol class=""><li id="2f01" class="lt lu hh iy b iz ja jc jd jf lv jj lw jn lx jr ly lz ma mb bi translated"><strong class="iy hi">激活阈值</strong>:在给定的输入范围外或超过一定值时，应明确该功能是否被激活。</li><li id="d102" class="lt lu hh iy b iz mc jc md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iy hi">以零为中心</strong>:我们可以使用偏置项控制和更新激活阈值，因此激活函数可以以零为中心。</li><li id="0b51" class="lt lu hh iy b iz mc jc md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iy hi">单调性</strong>:激活输入的正变化应使功能激活更多。</li><li id="d5b9" class="lt lu hh iy b iz mc jc md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iy hi">零中心输出</strong>:激活功能的输出应该是零中心的，如果它是用于隐藏层的话，这样以后的层可以得到一个中心输入。</li><li id="53c8" class="lt lu hh iy b iz mc jc md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iy hi">有界:</strong>如果激活是有界的，那么神经元可以取的值就只能有这么大。这有助于特征缩放。</li><li id="0197" class="lt lu hh iy b iz mc jc md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iy hi">有界导数:</strong>如果导数是无界的，则可能存在计算复杂性以及可能超过局部最小值的非常突然的大更新(称为爆炸梯度问题)。如果导数是有界的，但是边界太小，那么网络可能会遇到消失梯度问题(下面解释)。</li></ol></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="bdc9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">调整激活函数类似于试图寻找线性回归中独立变量的变换。在某种程度上，在神经网络中使用非标准激活函数几乎是一种反模式。</p><p id="d454" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">但是我们需要选择一个激活函数</strong>——无论是使用我们关于X和Y如何相关的直觉还是使用标准激活函数集。我强烈地感觉到，如果可能的话，人们应该根据他们对问题的直觉来使用激活函数。我将在这篇文章中尝试传达这一点。</p><p id="a80d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们先来看看“标准”激活功能的学习是什么样子的。</p><h1 id="32d8" class="kb kc hh bd kd ke lj kg kh ki lk kk kl in ll io kn iq lm ir kp it ln iu kr ks bi translated">标准激活功能</h1><h2 id="5185" class="mh kc hh bd kd mi mj mk kh ml mm mn kl jf mo mp kn jj mq mr kp jn ms mt kr mu bi translated">热卢</h2><p id="ba4e" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">我们将考察的第一个标准激活函数是整流线性单位(ReLU)函数，其中<strong class="iy hi"> ReLU(x) = max(0，x) </strong>如下所示:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/7caafcb5f0335dcc3198d2c19e1fec01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*EHQ-L6cmpJf0EOwf.png"/></div></figure><p id="f2a8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在单隐层神经网络中，<strong class="iy hi"> ReLU允许我们学习输入</strong>的分段线性函数，如果我们有足够多的段，它可能很好地逼近Y。在多隐藏层NN中，<strong class="iy hi"> ReLU </strong>学习前一层的分段函数。</p><p id="efc9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们回顾一下第一部分<a class="ae jt" rel="noopener" href="/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8">中的分类任务，目标是将蓝色和绿色分开。</a></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/3bf4b94f2f238f4eab0f80b14ce82a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*sgIpHmL_SSHJnvuG.png"/></div></figure><p id="c613" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">和以前一样，最后一层用<strong class="iy hi">s形管</strong>激活，以便利用逻辑回归的直觉。如果我们只使用2个隐藏神经元和一个用<strong class="iy hi"> ReLU </strong>激活的隐藏层:</p><pre class="ku kv kw kx fd mw mx my mz aw na bi"><span id="8f86" class="mh kc hh mx b fi nb nc l nd ne">model = keras.models.Sequential()<br/>model.add(layers.Dense(2, input_dim=2, activation='relu'))<br/>model.add(layers.Dense(1, activation='sigmoid'))<br/>model.compile(loss="binary_crossentropy")</span></pre><p id="a0e3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们得到以下特征和决策边界:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/d7c1c2f7dbd16a979a796bb5e5d2e642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5nK725uTBUeoIA0XjEyA_A.gif"/></div></div></figure><p id="373f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">观察学习到的特征(在左边)，看起来，大多数时候，当一个隐藏的神经元被激活时，另一个为0，反之亦然。如果两者都经常被激活，我们将在左侧图的中间看到更多的点。</p><p id="a7f1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">查看数据(在右边)，似乎我们至少需要5或6个片段来估计这个决策边界。使用3个隐藏的神经元应该给我们足够的线性片段来完成任务。</p><p id="164f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是学习到的特征空间和决策边界的样子:</p><div class="ku kv kw kx fd ab cb"><figure class="nf ky ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/1f728e1f5e0df73230d2538b22c018f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*GZqTs7NRFrbfH5G4Nqr_WA.jpeg"/></div></figure><figure class="nf ky nl nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/4e5cfb79ba4f345a5f5eb9a40af9f2a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*2ktjVL1P-3oSxU8KwGdmqA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx nm di nn no">LEFT: learned features | RIGHT: decision boundary using 3 hidden neurons</figcaption></figure></div><p id="6f69" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你以正确的方式观察特征空间，你可能能够看到超平面如何在该空间中分离数据。</p><p id="e3f6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果我们增加隐藏神经元的数量，我们可以得到更精确的决策边界的近似值。</p><div class="ku kv kw kx fd ab cb"><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/06bd35d28f93d8c9cb38f1df83d69dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_IptYeHBGSBSaPcQ7CeZDg.png"/></div></figure><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/3e3087aca3d0f69ceec03eaf74b58314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ykwxr9LmeclnSwje4owTIQ.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx nq di nr no">using 10 neurons (left) and 100 neurons (right)</figcaption></figure></div><p id="ea45" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可能凭直觉认为，随着隐藏神经元数量的增加，我们越来越接近真正的决策边界。但这受限于我们数据的稀疏性。如果我们使用100个神经元，我们学到的东西与我们用10个神经元学到的东西非常相似，因为没有接近决策边界的数据点迫使网络学习更多的东西。</p><p id="0ed6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，尽管在技术上我们可以很好地近似x[0] + x[1] — 1 = 0的决策边界(根据<a class="ae jt" href="https://cognitivemedium.com/magic_paper/assets/Hornik.pdf" rel="noopener ugc nofollow">通用近似定理</a>)，但前提是我们有正确数量的神经元，<strong class="iy hi">我们受到数据稀疏性的限制</strong>。</p><p id="5701" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随着我们增加隐藏神经元的数量，我们也面临着过度拟合的风险——我们的网络学习了一个过于特定于训练集的函数，当暴露于一般数据时表现不佳。</p><p id="d231" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，为了使用像ReLU这样的简单函数来估计复杂的决策边界，我们需要许多隐藏神经元。<strong class="iy hi">因此，在激活足够简单以使学习有效(<strong class="iy hi"> ReLU </strong>及其导数易于计算)和激活函数不需要太多神经元以进行学习(这将冒过拟合的风险)之间存在权衡</strong>。</p><h2 id="9349" class="mh kc hh bd kd mi mj mk kh ml mm mn kl jf mo mp kn jj mq mr kp jn ms mt kr mu bi translated">乙状结肠的</h2><p id="d4f3" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">另一个标准激活函数是<strong class="iy hi"> sigmoid: </strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/e646dd57e99f4eedd7c4589447a2c7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zRubj1E-e6JpJVGVabuXtw.png"/></div></figure><p id="8ed8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从图形上，我们可以看到<strong class="iy hi"> sigmoid </strong>变换它接收的仿射函数，使得任何超过6左右的都有效地为1，而低于-6左右的都有效地为0。应用<strong class="iy hi"> sigmoid </strong>激活而不是<strong class="iy hi"> ReLU </strong>我们得到具有2个神经元的单个隐藏层的以下特征和决策边界:</p><div class="ku kv kw kx fd ab cb"><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/e3cd70067084c9e5ad9496f3e198226c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6C924jfiKn2L-PKNmaAq5g.png"/></div></figure><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/670252655bda12df1491757110e1ced3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7b2--aAUhEAs_Q5WTwltsg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx nq di nr no">LEFT: learned features | RIGHT: decision boundary using 2 hidden neurons</figcaption></figure></div><p id="6deb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这与<strong class="iy hi"> ReLU </strong>所了解到的情况非常相似，神经元要么非常活跃，要么根本不活跃(上图中间没有很多点)。由于<strong class="iy hi"> sigmoid </strong>的值是平滑变换的，我们可以期望了解到一个不完全分段线性的决策边界。部件之间的连接/过渡将是弯曲的。</p><div class="ku kv kw kx fd ab cb"><figure class="nf ky ns nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/6034e898d46e8c19b682adb823ba113c.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*Vbg7QMj4rnHT9ky4dIyqOg.jpeg"/></div></figure><figure class="nf ky nt nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/e70d9abd1b79e925e6c4b50a2e765ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*Cks3ci2ozTN3zU4fGkiZqQ.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx nu di nv no">LEFT: learned features | RIGHT: decision boundary using 3 hidden neurons</figcaption></figure></div><p id="e1da" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<a class="ae jt" rel="noopener" href="/mlearning-ai/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69">第二部分</a>中，我们看到对深度神经网络使用<strong class="iy hi"> sigmoid </strong>和<strong class="iy hi"> ReLU </strong>导致了网络中数据分布的变化。这意味着更深层的数据被越来越多的数据激活，我们看到这些数据倾向于冻结神经元。在<strong class="iy hi"> sigmoid </strong>的情况下，发生冻结是因为学习到的特征对于整个数据来说将是常数~1。这意味着梯度的方向和大小在初始化时是固定的。方向是固定的，因为它完全取决于权重(因为激活是恒定的~1)。幅度是固定的，因为当<strong class="iy hi"> sigmoid </strong>的值接近1时，导数将非常接近0。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/ea69ae9266813dd9e2d58323ee262f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*lCPiBkDAq6XGVBmuz7-pag.png"/></div></figure><p id="3664" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在具有<strong class="iy hi">乙状窦</strong>激活的深层NNs中出现的另一个问题是消失梯度问题。由于梯度在反向传播时会在各层中相乘，所以只取0到1之间的值的梯度只会随着相乘而变小。在<strong class="iy hi"> sigmoid </strong>的情况下，它的导数只能取0到. 25之间的值。</p><p id="cdbd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可能会想，给定重量的梯度也取决于激活值，因此高激活可能会抵消这一点。你说得对！然而，在<strong class="iy hi">s形</strong>的情况下，数值被压缩在0和1之间，这只会进一步降低梯度。</p><p id="a2ed" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> ReLU </strong>没有这个问题，因为神经元被激活时导数一直是1。但是如果一个神经元学习到对于整个数据总是不活动的(即0)，那么它的相关权重将不会被更新。这些死亡的神经元是需要注意的。</p><p id="eaca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以sigmoid在做二元分类的时候更频繁的用在输出层(如上)。</p><h2 id="d4c8" class="mh kc hh bd kd mi mj mk kh ml mm mn kl jf mo mp kn jj mq mr kp jn ms mt kr mu bi translated">双曲正切</h2><p id="4470" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">另一个标准激活函数是双曲正切函数:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/b4fb21cbca8068b2cb2a255a25f45427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*2ENCrSJg67hWENHqP4s5IA.png"/></div></figure><p id="c03c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从图形上，我们可以看到<strong class="iy hi"> tanh </strong>以这样一种方式转换它接收的仿射函数，即任何超过4的都是有效的1，而低于-4的都是有效的-1。应用<strong class="iy hi"> tanh </strong>激活而不是<strong class="iy hi"> ReLU </strong>我们得到了具有2个神经元的单个隐藏层的以下特征和决策边界:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/25a1f6687584c35673c1ba53340e5ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jynT0RkGsZFqt3WSFcez4w.gif"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">LEFT: learned feature space (using tanh on 2 hidden neurons) | RIGHT: decision boundary in the input space</figcaption></figure><p id="61fe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这和<strong class="iy hi"> ReLU </strong>和<strong class="iy hi"> sigmoid </strong>学的东西很像。我喜欢把激活功能想象成拉伸和折叠输入空间。<strong class="iy hi">乙状结肠</strong>和<strong class="iy hi"> tanh </strong>轻轻折叠空间，同时<strong class="iy hi"> ReLU </strong>折叠空间，留下折痕。</p><p id="07bc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用3个隐藏神经元，我们学习以下特征和决策边界:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/3460f7689ea7b5841bd6db706ea76ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*DUTetXGgB0Za8phglTQGYw.gif"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">LEFT: decision boundary in the learned feature space (using tanh) | RIGHT: decision boundary in the input space</figcaption></figure><p id="0798" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">决策边界是(未变换的)分离超平面和(这里是2D)输入空间的交集。左侧的蓝色表面显示了输入空间是如何被扭曲的。如果我们取消扭曲空间(以获得右边的图)，那么橙色超平面就会被扭曲。在非扭曲操作中，蓝色的表面穿过橙色的表面，你可以看到在右边的确切位置。</p><p id="bcbf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> Tanh </strong>也遭受类似<strong class="iy hi"> sigmoid </strong>的消失梯度问题，因为它的导数被限制在区间[0，1]。</p><p id="323c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">虽然<strong class="iy hi"> tanh </strong>的输出以0为中心，但深度神经网络可能会学习权重，从而在数据通过网络时显著改变数据的分布，从而冻结更深层。但是初始化时的分布一般不会受到这种偏移的影响。</p><h1 id="e4bf" class="kb kc hh bd kd ke lj kg kh ki lk kk kl in ll io kn iq lm ir kp it ln iu kr ks bi translated">非标准激活</h1><h2 id="c044" class="mh kc hh bd kd mi mj mk kh ml mm mn kl jf mo mp kn jj mq mr kp jn ms mt kr mu bi translated">多项式激活</h2><p id="6fd0" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">由于我们可以绘制数据，我们可以看到决策边界可能是一个圆，它是输入平方的线性函数。</p><p id="d615" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以我们可以尝试使用𝞂 <strong class="iy hi"> (x) = x </strong>作为激活函数:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/3d5a028b4c709cb343b83006a17f9fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HveYpkhxbNvY_oe3jGz6PA.png"/></div></figure><p id="bc36" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在Keras中，您可以使用如下自定义激活功能:</p><pre class="ku kv kw kx fd mw mx my mz aw na bi"><span id="c330" class="mh kc hh mx b fi nb nc l nd ne">def custom_act(x):<br/>    return x**2</span><span id="1656" class="mh kc hh mx b fi nw nc l nd ne">model = keras.models.Sequential()<br/>model.add(keras.layers.Dense(2, input_dim=2, use_bias=False, activation=custom_act))<br/>model.add(keras.layers.Dense(1, activation='sigmoid'))<br/>model.compile(loss="binary_crossentropy")</span></pre><div class="ku kv kw kx fd ab cb"><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/6658de1b40a8b1427a22e195ae1da33a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NxZyribdkBI-Fd_wcUpKrw.png"/></div></figure><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/84e4d76431766817db66444ee2cc27f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*khl4HjiF3wtZ-8DKv3VGcg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx nq di nr no">LEFT: learned features | RIGHT: decision boundary using 2 hidden neurons</figcaption></figure></div><p id="23d7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用多项式激活函数的主要缺点是梯度是无限的。这意味着梯度的大小可能非常大(称为爆炸梯度问题)，特别是当它们在反向传播过程中通过网络向后倍增时，会产生溢出等计算复杂性和权重可能急剧更新的学习复杂性。</p><p id="a4ab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的另一个问题是，对于偶数多项式，激活仅是正的，因此我们将在深度神经网络中看到类似的分布变化。如上所述，使用<strong class="iy hi">符号(x) * x </strong>将有助于区分激活和未激活的神经元，并提供更大的学习灵活性。</p><h2 id="d608" class="mh kc hh bd kd mi mj mk kh ml mm mn kl jf mo mp kn jj mq mr kp jn ms mt kr mu bi translated">周期性激活</h2><p id="f336" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">从技术上讲，圆在极坐标域中是周期性的。对于神经网络来说，转换到极坐标并不简单。</p><p id="3d92" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，如果您对z = 1+<strong class="iy hi">cos</strong>(x)+<strong class="iy hi">cos</strong>(y)进行切片，您会得到类似于圆形的东西。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nx"><img src="../Images/a3bf3a50b05f971434122e173bfa06a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aStBVAOvq8Brh-mbifP4tg.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">generated by <a class="ae jt" href="https://www.geogebra.org/" rel="noopener ugc nofollow" target="_blank">geogebra.org/</a></figcaption></figure><p id="4cf4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，在2个隐藏神经元上使用类似于<strong class="iy hi"> sin </strong>或<strong class="iy hi"> cos </strong>的东西在理论上应该可行。实际上，网络很难学习这个函数而不经历停止学习的局部最小值。向隐藏层添加额外的神经元有助于解决这一问题:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/751e6d2737f2a636054969b8d06bd0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eg55njNpWbd0CXdGkbogag.gif"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">LEFT: learned features | RIGHT: decision boundary using 3 hidden neurons</figcaption></figure><p id="9fff" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">输出层的学习权重(它是这些余弦激活神经元的函数)是:[-3.8，1.4，-3.8]，偏差是1.4。这意味着第一神经元和第三神经元是冗余的，但是允许学习(近似)上面的z = 1+<strong class="iy hi">cos</strong>(x)+<strong class="iy hi">cos</strong>(y)。</p><p id="d97f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的问题是，由于周期性，这些圆圈将出现在整个2D平面上——显然这不是我们想要的。</p><p id="f6ba" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">周期性决策边界的一个更好的例子可能如下:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mv"><img src="../Images/e0933dcc3c9c90ab13de0e775da0e595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*XnLYJ1YdbTG3QT7sTZYAhA.png"/></div></figure><p id="7acf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正如我们在上面看到的，如果我们选择使用<strong class="iy hi"> ReLU </strong>，我们将需要相当多的片段来估计这个决策边界。</p><pre class="ku kv kw kx fd mw mx my mz aw na bi"><span id="6fbb" class="mh kc hh mx b fi nb nc l nd ne">model = keras.models.Sequential()<br/>model.add(layers.Dense(8, input_dim=2, activation='relu'))<br/>model.add(layers.Dense(1, activation='sigmoid'))<br/>model.compile(loss="binary_crossentropy")</span></pre><div class="ku kv kw kx fd ab cb"><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/ed29bae895d887cf696b491c7a391e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nj3WYozA_gVUjqG5l7_mIg.png"/></div></figure><figure class="nf ky np nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><img src="../Images/9a2c546050ffedffc3d51aaaef6745f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CHw7UinHg-aXDXsXhZ5mfA.png"/></div></figure></div><p id="7ce4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是，这并不是真正的学习分隔两个类的函数，正如我们在右边看到的。<strong class="iy hi"> ReLU </strong>(事实上，所有标准激活函数)无法学习这种重复模式，因为<strong class="iy hi">用于近似的片段数量是由网络架构中定义的神经元数量固定的</strong>。<strong class="iy hi"> </strong>为了学习右边的决策边界，我们需要增加隐藏神经元的数量，并重新训练我们的模型，以便获得更多的线性块进行逼近。</p><p id="8e96" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通常<strong class="iy hi">激活功能是为整个层</strong>定义的。这意味着创建一个可以学习类似Y = X + log(X)或上述Z = Cos(X) + Y的函数的神经网络，如果每个神经元都没有激活函数，这是不可能的。</p><p id="b8da" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里我们只对两个神经元中的一个引入余弦激活，因为决策边界是一个坐标是另一个的余弦:</p><pre class="ku kv kw kx fd mw mx my mz aw na bi"><span id="e7c3" class="mh kc hh mx b fi nb nc l nd ne">def custom_activation(x):<br/>    x_0 = math.cos(x[...,0])<br/>    x_1 = x[...,1]<br/>    return stack([x_0, x_1], axis = 1)</span><span id="5e4f" class="mh kc hh mx b fi nw nc l nd ne">model = keras.models.Sequential()<br/>model.add(layers.Dense(2, input_dim=2, use_bias=False, activation=custom_activation))<br/>model.add(layers.Dense(1, activation='sigmoid'))<br/>model.compile(loss="binary_crossentropy")</span></pre><p id="535a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其学习以下特征和决策边界:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/c1453264644fe0441456787d443867b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*vy0RmcABIGjTVUf52ZDLig.gif"/></div></div></figure><p id="1927" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可能还记得傅立叶级数仅仅是正弦和余弦的加权和。你可能想知道是否有神经网络可以通过学习函数的傅立叶级数的系数来学习函数。这个被称为傅立叶神经网络的正在进行的研究领域没有忽略周期性激活函数。</p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><h1 id="a901" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">确认</h1><p id="25f9" class="pw-post-body-paragraph iw ix hh iy b iz lo ii jb jc lp il je jf lq jh ji jj lr jl jm jn ls jp jq jr ha bi translated">感谢杨易进、卡梅隆·加里森、玛利亚·舍甫琴科、詹姆斯·昆斯特勒、林、克里斯蒂娜·徐、的贡献。</p><div class="ny nz ez fb oa ob"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">medium.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ld ob"/></div></div></a></div></div></div>    
</body>
</html>