# åŸºäºæ·±åº¦å­¦ä¹ çš„æ£®æ—ç«ç¾é¢„æµ‹å®éªŒ

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/the-experiment-of-forest-fires-prediction-using-deep-learning-d537e8c8e3a2?source=collection_archive---------0----------------------->

æ£®æ—ç«ç¾æ˜¯é‡è¦çš„ç¾å®³æ€§äº‹ä»¶ä¹‹ä¸€ï¼Œå¯¹ç¯å¢ƒã€åŸºç¡€è®¾æ–½å’Œäººç±»ç”Ÿæ´»éƒ½æœ‰å¾ˆå¤§å½±å“ã€‚é’ˆå¯¹æ£®æ—ç«ç¾é¢„è­¦æ¢æµ‹ç³»ç»Ÿçš„éœ€æ±‚ï¼Œå·²ç»é‡‡ç”¨äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬:åŸºäºç‰©ç†çš„æ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ã€æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

![](img/6deb5d24394d217b263e0de4ebcebb8f.png)

æœ¬æ–‡æ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ ï¼Œæ ¹æ®å‘ç°ç«ç¾çš„ç©ºé—´ã€æ—¶é—´å’Œå¤©æ°”å˜é‡ï¼Œè¿›è¡Œè¶…å‚æ•°è°ƒæ•´å®éªŒï¼Œä»¥é¢„æµ‹è‘¡è„ç‰™ä¸œåŒ—éƒ¨åœ°åŒºæ£®æ—ç«ç¾çš„çƒ§æ¯é¢ç§¯ã€‚

æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª UCI æœºå™¨å­¦ä¹ èµ„æºåº“çš„å…¬å…±æ•°æ®é›†:[http://archive.ics.uci.edu/ml/datasets/Forest+Fires](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)ã€‚è¿™ä¸€é¢„æµ‹å¯ç”¨äºè®¡ç®—æ´¾å¾€äº‹æ•…ç°åœºçš„éƒ¨é˜Ÿï¼Œå¹¶ç¡®å®šå±€åŠ¿çš„ç´§è¿«æ€§ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ–¹æ³•æ˜¯å…·æœ‰åˆ†ç±»é—®é¢˜çš„äººå·¥ç¥ç»ç½‘ç»œ/æ·±åº¦å­¦ä¹ æ¥é¢„æµ‹æ£®æ—ç«ç¾ã€‚

**äººå·¥ç¥ç»ç½‘ç»œæ¦‚è¿°:**
äººå·¥ç¥ç»ç½‘ç»œç”±å…·æœ‰è¾“å…¥å’Œè¾“å‡ºç»´åº¦çš„å±‚ç»„æˆã€‚åè€…ç”±**ç¥ç»å…ƒ**(ä¹Ÿç§°ä¸ºâ€œèŠ‚ç‚¹â€)çš„æ•°é‡å†³å®šï¼Œè®¡ç®—å•å…ƒé€šè¿‡**æ¿€æ´»å‡½æ•°**è¿æ¥åŠ æƒè¾“å…¥(å¸®åŠ©ç¥ç»å…ƒæ‰“å¼€/å…³é—­)ã€‚ä¸å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•ä¸€æ ·ï¼Œ**æƒé‡å’Œ**æƒé‡åœ¨è®­ç»ƒæœŸé—´è¢«éšæœºåˆå§‹åŒ–å’Œä¼˜åŒ–ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

![](img/c4981a819ee9b955dbebd66e60e4c7ef.png)

[Deep Learning with Python: Neural Networks (complete tutorial)](https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0)

ä»¥ä¸‹æ˜¯è¿›è¡Œå®éªŒçš„æ­¥éª¤:

# ç¬¬ 1 æ­¥:ç†è§£æ•°æ®é›†

åœ¨å¯¼å…¥æ•°æ®é›†ä¹‹å‰ï¼Œå¿…é¡»å¯¼å…¥æ‰€éœ€çš„åº“ã€‚

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('seaborn')
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScalerfrom sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import tensorflow as tensorflow
from keras.models import Sequential
from keras.layers import Dense, Dropout
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.utils.vis_utils import plot_model
```

æœ¬æ–‡ä½¿ç”¨çš„æ•°æ®é›†æ¥è‡ª UCI æœºå™¨å­¦ä¹ èµ„æºåº“:[http://archive.ics.uci.edu/ml/datasets/Forest+Fires](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)

è¦å¯¼å…¥æ•°æ®é›†ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤:

```
df = pd.read_csv('dataset.csv')
df.head(10)
```

![](img/6fefe49761467b6dfa2ab146dffc5c3f.png)

**å±æ€§ä¿¡æ¯:**

*   **X** :è’™ç‰¹è¾›éœå…¬å›­åœ°å›¾å†… X è½´ç©ºé—´åæ ‡:1-9
*   **Y** :è’™ç‰¹è¾›éœå…¬å›­åœ°å›¾å†…çš„ Y è½´ç©ºé—´åæ ‡:2-9
*   **æœˆ**:ä¸€å¹´ä¸­çš„æœˆä»½:â€œ1 æœˆâ€è‡³â€œ12 æœˆâ€
*   **æ—¥**:å‘¨ä¸€è‡³å‘¨æ—¥
*   **FFMC**:FWI ç³»ç»Ÿ FFMC(ç²¾ç»†ç‡ƒæ–™æ°´åˆ†ä»£ç )æŒ‡æ•°:18.7 ~ 96.20
*   **DMC** :æ¥è‡ª FWI ç³»ç»Ÿçš„ DMC(Duff weather Code)æŒ‡æ•°:1.1 è‡³ 291.3
*   **DC**:FWI ç³»ç»Ÿ DC(æ—±æƒ…ä»£ç )æŒ‡æ•°:7.9 ~ 860.6
*   **ISI**:FWI ç³»ç»Ÿ ISI(åˆå§‹åˆ©å·®æŒ‡æ•°)æŒ‡æ•°:0.0 ~ 56.10
*   **æ¸©åº¦**:æ‘„æ°æ¸©åº¦:2.2 è‡³ 33.30 åº¦
*   **ç›¸å¯¹æ¹¿åº¦**:ç›¸å¯¹æ¹¿åº¦ç™¾åˆ†æ¯”:15.0 è‡³ 100
*   **é£**:é£é€Ÿä»¥åƒç±³/å°æ—¶ä¸ºå•ä½:0.40 è‡³ 9.40
*   **é›¨**:å®¤å¤–é›¨ï¼Œå•ä½ä¸ºæ¯«ç±³/å¹³æ–¹ç±³:0.0 è‡³ 6.4
*   **é¢ç§¯**:æ£®æ—è¿‡ç«é¢ç§¯(å…¬é¡·):0.00-1090.84

# æ­¥éª¤ 2:æ•°æ®é¢„å¤„ç†

## 1)æ·»åŠ æ–°åˆ—= size_category

å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å°è¯•æ·»åŠ ä¸€ä¸ªæ–°åˆ—ï¼Œå³`size_category`æ¥å°†æ•°æ®åˆ†ä¸ºä¸¤ç±»:

*   å¦‚æœ`area` <çš„å€¼ä¸º 6 é‚£ä¹ˆ`size_category`å°†è¢«æ ‡è®°ä¸º 0(å°ç«)
*   å¦‚æœ`area`çš„å€¼â‰¥ 6ï¼Œé‚£ä¹ˆ`size_category`å°†è¢«æ ‡è®°ä¸º 1(å®½ç«)

```
df['size_category'] = np.where(df['area']>6, '1', '0')
df['size_category']= pd.to_numeric(df['size_category'])
df.tail(10)
```

![](img/4b138f4400dec889b13415d672dfce0a.png)

## 2)æ•°å¤©çš„æ•°æ®é¢„å¤„ç†

`day`çš„åˆ†å¸ƒçœ‹èµ·æ¥å¾ˆæ¼‚äº®ã€‚æˆ‘ä»¬å°†ä¸ç¼–ç  7 ä¸ªå˜é‡ï¼Œè€Œæ˜¯å°†å®ƒä»¬åˆ†æˆå‘¨æœ«(`True`)æˆ–éå‘¨æœ«(`False`)ã€‚å‡è®¾ç«ç¾ä¸­ç‡ƒçƒ§çš„é¢ç§¯ä¹Ÿä¸æ¶ˆé˜²å‘˜å¯¹ç«ç„°çš„ååº”æœ‰å…³ã€‚åœ¨å‘¨æœ«ï¼Œæ¶ˆé˜²é˜Ÿå‘˜çš„æ•°é‡æˆ–æ€»ä½“ååº”å¯èƒ½ä¸å·¥ä½œæ—¥ä¸åŒã€‚

![](img/8f8216e35a7806e8ae6b40a4e5429edf.png)

```
**# converting to is weekend**
df['day'] = ((df['day'] == 'sun') | (df['day'] == 'sat'))**# renaming column**
df = df.rename(columns = {'day' : 'is_weekend'})**# visualizing**
sns.countplot(df['is_weekend'])
plt.title('Count plot of weekend vs weekday')
```

![](img/3ecdc1a06ac28ed40cf2d4850e06b70e.png)

åæ–œä¸æ˜¯å¤ªå¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹è¿™ç§è½¬æ¢å¾ˆæ»¡æ„ã€‚

## 3)ç»“å¢åŒºåŸŸå’Œé›¨æ°´

![](img/a0be7cf77d7708f6b81a55d8e634d9ba.png)

`rain`å’Œ`area`çš„åˆ†å¸ƒè¿‡äºå€¾æ–œï¼Œæœ‰å¾ˆå¤§çš„å¼‚å¸¸å€¼ï¼Œå› æ­¤æˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œç¼©æ”¾ï¼Œä»¥ä½¿åˆ†å¸ƒå‡åŒ€ã€‚

```
**# natural logarithm scaling (+1 to prevent errors at 0)**
df.loc[:, ['rain', 'area']] = df.loc[:, ['rain', 'area']].apply(lambda x: np.log(x + 1), axis = 1)**# visualizing**
fig, ax = plt.subplots(2, figsize = (5, 8))
ax[0].hist(df['rain'])
ax[0].title.set_text('histogram of rain')
ax[1].hist(df['area'])
ax[1].title.set_text('histogram of area')
```

![](img/c27c1a91fc640fb3e932071e0f88cd2b.png)

`rain`çš„åˆ†å¸ƒä¸å¥½ï¼Œä½†`area`çš„åˆ†å¸ƒæœ‰å¾ˆå¤§æ”¹å–„ã€‚ç°åœ¨æˆ‘ä»¬ç¼©æ”¾æ•´ä¸ªæ•°æ®é›†ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è®¡åˆ’åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå› æ­¤æˆ‘ä»¬å°†ç¼©æ”¾è¯¥åŒºåŸŸï¼Œä½œä¸ºåº”å¯¹çˆ†ç‚¸æ¢¯åº¦çš„é¢„é˜²æªæ–½ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†å‰²æˆ**è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²**ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ç¼©æ”¾è®­ç»ƒé›†ï¼Œç„¶ååŸºäºè®­ç»ƒé›†ç¼©æ”¾æµ‹è¯•é›†ã€‚ç„¶åæˆ‘ä»¬å°†æ‰©å±•ä¸€åˆ‡ã€‚

## **4)åˆ—è½¦è¯•åˆ†è£‚**

æ•°æ®è¢«éšæœºåˆ†æˆè®­ç»ƒæ•°æ®(80 %)å’Œæµ‹è¯•æ•°æ®(20%)ã€‚

```
features = df.drop(['size_category'], axis = 1)
labels = df['size_category'].values.reshape(-1, 1)X_train, X_test, y_train, y_test = train_test_split(features,labels, test_size = 0.2, random_state = 42)
```

## 5)ç‰¹å¾ç¼©æ”¾:æ ‡å‡†ç¼©æ”¾å™¨

å¯¹æ•°æ®åº”ç”¨ç‰¹å¾ç¼©æ”¾:æ ‡å‡†ç¼©æ”¾å™¨

```
**# fitting scaler**
sc_features = StandardScaler()**# transforming features**
X_test = sc_features.fit_transform(X_test)
X_train = sc_features.transform(X_train)**# features**
X_test = pd.DataFrame(X_test, columns = features.columns)
X_train = pd.DataFrame(X_train, columns = features.columns)**# labels**
y_test = pd.DataFrame(y_test, columns = ['size_category'])
y_train = pd.DataFrame(y_train, columns = ['size_category'])X_train.head()
```

![](img/8fe863f15bceb800be49f26a2be0064b.png)

# æ­¥éª¤ 3:è¶…å‚æ•°/å®éªŒç»“æœ

# 1)å®éªŒ 1:åŸºç¡€æ¨¡å‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨æŸä¸ªåä¸º Sequential çš„ Keras ç±»æ¥åˆ›å»ºæˆ‘ä»¬çš„ ANN å¯¹è±¡ã€‚ä¸€æ—¦æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ç°åœ¨è¦åˆ›å»ºå±‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŸºç¡€æ¨¡å‹ç½‘ç»œï¼Œå®ƒå°†å…·æœ‰:

*   1 ä¸ªè¾“å…¥å±‚
*   2 ä¸ªéšè—å±‚
*   1 ä¸ªè„±è½å±‚
*   1 ä¸ªè¾“å‡ºå±‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªéšè—å±‚ï¼Œä½¿ç”¨çš„æ˜¯å±‚æ¨¡å—ä¸­çš„å¯†é›†ç±»ã€‚è¯¥ç±»æ¥å— 2 ä¸ªè¾“å…¥:

*   **å•ä½**:å°†å‡ºç°åœ¨ç›¸åº”å±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡
*   **æ¿€æ´»**:æŒ‡å®šä½¿ç”¨å“ªä¸ªæ¿€æ´»åŠŸèƒ½

æˆ‘ä»¬åˆ›å»ºä¸€ç³»åˆ—å±‚æ¥å®šä¹‰ç¥ç»ç½‘ç»œï¼Œå¹¶é€šè¿‡åˆå§‹åŒ–æƒé‡ã€å®šä¹‰æ¿€æ´»å‡½æ•°å’Œé€‰æ‹©æ¯ä¸ªéšè—å±‚çš„èŠ‚ç‚¹æ¥å®šä¹‰æ¯ä¸€å±‚ã€‚

```
model = Sequential()**# input layer + 1st hidden layer**
model.add(Dense(6, input_dim=13, activation='relu'))**# 2nd hidden layer**
model.add(Dense(6, activation='relu'))**# output layer**
model.add(Dense(6, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'relu'))model.summary()
```

![](img/0b79b51c19e53da879ec9a83599055fd.png)

ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„è¶…å‚æ•°ç¼–è¯‘æˆ‘ä»¬çš„ ANN:

![](img/9977525ffe3e77e45cfd081475e5e01c.png)

*   **çºªå…ƒ**:ç¥ç»ç½‘ç»œå°†è¢«è®­ç»ƒå¤šå°‘æ¬¡
*   **æ‰¹æ¬¡å¤§å°**:æ‰¹æ¬¡ä¸­åº”è¯¥æœ‰å¤šå°‘ä¸ªè§‚å¯Ÿå€¼ã€‚
*   **æ¿€æ´»å‡½æ•°**:æ¿€æ´»å‡½æ•°çš„ä¸»è¦ä½œç”¨æ˜¯å°†æ¥è‡ªèŠ‚ç‚¹çš„æ€»åŠ æƒè¾“å…¥è½¬æ¢æˆè¾“å‡ºå€¼ï¼Œä»¥é¦ˆé€åˆ°ä¸‹ä¸€ä¸ªéšè—å±‚æˆ–ä½œä¸ºè¾“å‡ºã€‚
*   **æŸå¤±å‡½æ•°**:æŸå¤±å‡½æ•°ç”¨äºç¡®å®šæˆ‘ä»¬ç®—æ³•çš„è¾“å‡ºä¸ç»™å®šç›®æ ‡å€¼ä¹‹é—´çš„è¯¯å·®ã€‚
*   **å­¦ä¹ ç‡:**å­¦ä¹ ç‡æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå®ƒæ§åˆ¶æ¯æ¬¡æ›´æ–°æ¨¡å‹æƒé‡æ—¶ï¼Œæ ¹æ®ä¼°è®¡è¯¯å·®æ”¹å˜æ¨¡å‹çš„ç¨‹åº¦ã€‚
*   **ä¼˜åŒ–å™¨**:ä¼˜åŒ–å™¨æ˜¯ç”¨äºæœ€å°åŒ–è¯¯å·®å‡½æ•°(æŸå¤±å‡½æ•°)æˆ–æœ€å¤§åŒ–ç”Ÿäº§æ•ˆç‡çš„ç®—æ³•æˆ–æ–¹æ³•ã€‚

ä¸ºäº†æ£€æŸ¥è¿™äº›æ–¹æ³•çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç²¾åº¦åº¦é‡ã€‚

```
**# Compile Model**
model.compile(optimizer = 'adam', metrics=['accuracy'], loss ='binary_crossentropy')**# Train Model** history = model.fit(X_train, y_train, validation_data = (X_test, y_test), batch_size = 10, epochs = 100)
```

![](img/25e1466c347c158f9c1925f592f65fef.png)

```
_, train_acc = model.evaluate(X_train, y_train, verbose=0)
_, valid_acc = model.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))
```

![](img/e043b3669923eeb1908f787a4b3869c4.png)

åŸºäºä½¿ç”¨åŸºç¡€æ¨¡å‹çš„è¶…å‚æ•°çš„å®éªŒ 1 çš„ç»“æœï¼Œè®­ç»ƒæ•°æ®çš„å‡†ç¡®åº¦åˆ†æ•°æ˜¯ 96%ï¼Œæœ‰æ•ˆæˆ–æµ‹è¯•æ•°æ®çš„å‡†ç¡®åº¦åˆ†æ•°æ˜¯ 92%ã€‚

```
plt.figure(figsize=[8,5])
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Valid')
plt.legend()
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Accuracy', fontsize=16)
plt.title('Accuracy Curves Epoch 100, Batch Size 10', fontsize=16)
plt.show()
```

![](img/0934b090617149d9215a4b8ca70fe815.png)

åŸºäºå‡†ç¡®åº¦å›¾çš„è¾“å‡ºï¼Œæ¨¡å‹å¼€å§‹æ˜¾ç¤ºåœ¨æ—¶æœŸ 60 åˆ° 100 çš„ç¨³å®šæ€§ã€‚

# 2)å®éªŒäºŒ:æ‰¹é‡:4ï¼Œ6ï¼Œ10ï¼Œ16ï¼Œ32ï¼Œ64ï¼Œ128ï¼Œ260

å¯¹äºå®éªŒ 2ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¦‚ä¸‹è¶…å‚æ•°ç»†èŠ‚è¿›è¡Œäººå·¥ç¥ç»ç½‘ç»œå»ºæ¨¡:

![](img/06ddbb526be68f25e28d09ac8d822a9a.png)

```
**# Fit a model and plot learning curve** def fit_model(X_train, y_train, X_test, y_test, n_batch):**# Define Model** model = Sequential()
model.add(Dense(6, input_dim=13, activation='relu'))
model.add(Dense(6, activation='relu'))
model.add(Dense(6, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'relu'))**# Compile Model**
model.compile(optimizer = 'adam',
metrics=['accuracy'],
loss = 'binary_crossentropy')**# Fit Model**
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0, batch_size=n_batch)**# Plot Learning Curves** plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.title('batch='+str(n_batch))
plt.legend()**# Create learning curves for different batch sizes** batch_sizes = [4, 6, 10, 16, 32, 64, 128, 260]plt.figure(figsize=(10,15))
for i in range(len(batch_sizes)):**# Determine the Plot Number** plot_no = 420 + (i+1)
plt.subplot(plot_no)**# Fit model and plot learning curves for a batch size** fit_model(X_train, y_train, X_test, y_test, batch_sizes[i])**# Show learning curves**
plt.show()
```

![](img/5ccf061fde216fb4910f9c97e94aac76.png)

æ ¹æ®ä¸Šé¢çš„ç²¾åº¦å›¾ï¼Œè¶³ä»¥æ˜¾ç¤ºç¨³å®šæ€§çš„å‹å·æ˜¯ **batch = 6 çš„å‹å·ã€‚**

# 3)å®éªŒ 3:æ‰¹é‡= 6ï¼Œæ—¶æœŸ= 20ï¼Œ50ï¼Œ100ï¼Œ120ï¼Œ150ï¼Œ200ï¼Œ300ï¼Œ400

å¯¹äºå®éªŒ 3ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¦‚ä¸‹è¶…å‚æ•°ç»†èŠ‚è¿›è¡Œäººå·¥ç¥ç»ç½‘ç»œå»ºæ¨¡:

![](img/aeaefa26fd68b4f329b92f5415d74c09.png)

```
**# fit a model and plot learning curve** def fit_model(trainX, trainy, validX, validy, n_epoch):**# define model** model = Sequential()
model.add(Dense(6, input_dim=13, activation='relu'))
model.add(Dense(6, activation='relu'))
model.add(Dense(6, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'relu'))**# compile model** model.compile(optimizer ='adam', metrics=['accuracy'], loss = 'binary_crossentropy')**# fit model** history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=n_epoch, verbose=0, batch_size=6)**# plot learning curves** plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.title('epoch='+str(n_epoch))
plt.legend()**# Create learning curves for different batch sizes** epochs = [20, 50, 100, 120, 150, 200, 300, 400]plt.figure(figsize=(10,15))
for i in range(len(batch_sizes)):**# Determine the Plot Number**
plot_no = 420 + (i+1)
plt.subplot(plot_no)**# Fit model and plot learning curves for a batch size**
fit_model(X_train, y_train, X_test, y_test, epochs[i])**# Show learning curves**
plt.show()
```

![](img/0bc972c847134962c45abadafc6f7fe3.png)

æ ¹æ®ä¸Šé¢çš„ç²¾åº¦å›¾ï¼Œè¶³ä»¥æ˜¾ç¤ºç¨³å®šæ€§çš„æ¨¡å‹æ˜¯ epoch = 200ã€300 å’Œ 400 **çš„æ¨¡å‹ã€‚**

# 4)å®éªŒå››

## æ‰¹é‡= 6ï¼Œ**æå‰åœæ­¢(è€å¿ƒï¼Œæ¨¡å‹æ£€æŸ¥ç‚¹)**

å¯¹äºå®éªŒ 4ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¦‚ä¸‹è¶…å‚æ•°ç»†èŠ‚è¿›è¡Œäººå·¥ç¥ç»ç½‘ç»œå»ºæ¨¡:

![](img/55c4f28670880f84750e146bed2d4327.png)

```
def init_model():**# define model** model = Sequential()
model.add(Dense(6, input_dim=13, activation='relu'))
model.add(Dense(6, activation='relu'))model.add(Dense(6, activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'relu'))
model.compile(optimizer ='adam',
metrics=['accuracy'],
loss = 'binary_crossentropy')return model**# init model**
model = init_model()**# simple early stopping** es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=150)**# model checkpoint** mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)**# fitting model** history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=250, verbose=0, batch_size=6, callbacks=[es, mc])
```

![](img/079d0bab2ea8edc5e87500ea12d8c3b5.png)

```
**# plot training history**
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='valid')
plt.legend()
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.title('Loss Curves', fontsize=16)
plt.show()
```

![](img/c2272735e1cff5acfdafc3cd8d0deb7b.png)

```
plt.figure(figsize=[8,5])
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Valid')
plt.legend()
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Accuracy', fontsize=16)
plt.title('Accuracy Curves', fontsize=16)
plt.show()
```

![](img/8c04cf2511558248a4ffb4337e3787a6.png)

```
_, train_acc = model.evaluate(X_train, y_train, verbose=0)
_, valid_acc = model.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))
```

![](img/47beff2d1cf12a46abc743fb7e30d15d.png)

åŸºäºå®éªŒ 4 çš„ç»“æœï¼Œä½¿ç”¨æœ‰è€å¿ƒçš„æ—©æœŸåœæ­¢å’Œæ¨¡å‹æ£€æŸ¥ç‚¹æ–¹æ³•ï¼Œè®­ç»ƒæ•°æ®çš„å‡†ç¡®åº¦åˆ†æ•°æ˜¯ 97%ï¼Œæœ‰æ•ˆæˆ–æµ‹è¯•æ•°æ®çš„å‡†ç¡®åº¦åˆ†æ•°æ˜¯ 99%ã€‚

# ç»“è®ºå’Œè®¨è®º

æ§åˆ¶æ£®æ—ç«ç¾çš„å…³é”®æˆåŠŸä¹‹ä¸€æ˜¯æ—©æœŸå‘ç°ç«ç¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¶…å‚æ•°è°ƒæ•´å®éªŒï¼Œä»¥é¢„æµ‹è‘¡è„ç‰™ä¸œåŒ—éƒ¨åœ°åŒºçš„æ£®æ—ç«ç¾ç‡ƒçƒ§é¢ç§¯ï¼ŒåŸºäºä½¿ç”¨æ·±åº¦å­¦ä¹ å‘ç°ç«ç¾çš„ç©ºé—´ã€æ—¶é—´å’Œå¤©æ°”å˜é‡ã€‚

ä¸ºäº†æ‰¾åˆ°å¦ä¸€ç§æœ€ä½³æ–¹æ³•ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æ•°æ®é¢„å¤„ç†çš„å…¶ä»–é€‰é¡¹ï¼Œå¹¶å°è¯•ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœº(SVM)ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—åˆ†ç±»å™¨ã€æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç­‰ã€‚

# å‚è€ƒèµ„æ–™:

[http://archive.ics.uci.edu/ml/datasets/Forest+Fires](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)

[](https://github.com/psohn/Only-You-Can-Prevent-Forest-Fires/blob/master/1.0_prs_preprocessing_eda.ipynb) [## åªæœ‰ä½ èƒ½é¢„é˜²æ£®æ—ç«ç¾/1.0 _ PRS _ preprocessing _ EDA . ipynb at masterâ€¦

### è‘¡è„ç‰™æ£®æ—ç«ç¾å›å½’æ¨¡å‹é¢„æµ‹ç«ç¾æŸå¤±â€¦

github.com](https://github.com/psohn/Only-You-Can-Prevent-Forest-Fires/blob/master/1.0_prs_preprocessing_eda.ipynb) [](https://github.com/psohn/Only-You-Can-Prevent-Forest-Fires/blob/master/3.0_prs_artificial_neural_network.ipynb) [## åªæœ‰ä½ èƒ½é¢„é˜²æ£®æ—ç«ç¾/3.0 _ PRS _ artificial _ neural _ network . ipynb at masterâ€¦

### è‘¡è„ç‰™æ£®æ—ç«ç¾å›å½’æ¨¡å‹é¢„æµ‹ç«ç¾æŸå¤±â€¦

github.com](https://github.com/psohn/Only-You-Can-Prevent-Forest-Fires/blob/master/3.0_prs_artificial_neural_network.ipynb) [](https://www.kaggle.com/psvishnu/forestfire-impact-prediction-stats-and-ml) [## æ£®æ—ç«ç¾å½±å“é¢„æµ‹(ç»Ÿè®¡å’Œ ml)

### ä½¿ç”¨ Kaggle ç¬”è®°æœ¬æ¢ç´¢å’Œè¿è¡Œæœºå™¨å­¦ä¹ ä»£ç |ä½¿ç”¨æ¥è‡ªæ£®æ—ç«ç¾æ•°æ®é›†çš„æ•°æ®

www.kaggle.com](https://www.kaggle.com/psvishnu/forestfire-impact-prediction-stats-and-ml) [](https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [## Python æ·±åº¦å­¦ä¹ :ç¥ç»ç½‘ç»œ(å®Œæ•´æ•™ç¨‹)

### ç”¨ TensorFlow å»ºç«‹ã€ç»˜åˆ¶å’Œè§£é‡Šäººå·¥ç¥ç»ç½‘ç»œ

towardsdatascience.com](https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0) [](https://www.analyticsvidhya.com/blog/2021/10/implementing-artificial-neural-networkclassification-in-python-from-scratch/) [## ä»é›¶å¼€å§‹ç”¨ Python å®ç°äººå·¥ç¥ç»ç½‘ç»œ

### ç¥ç»ç½‘ç»œã€‚21 ä¸–çºªè“¬å‹ƒå‘å±•çš„æŠ€æœ¯çªç ´ä¹‹ä¸€ã€‚ä½ æœ‰å…´è¶£åˆ›é€ â€¦

www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2021/10/implementing-artificial-neural-networkclassification-in-python-from-scratch/) [](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) 

ğŸŸ åœ¨ MLearning.ai æˆä¸º [**ä½œå®¶**](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)