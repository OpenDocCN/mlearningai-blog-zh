<html>
<head>
<title>“TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems” Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“TensorFlow:异构分布式系统上的大规模机器学习”摘要</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems-1c16cd1efacb?source=collection_archive---------1-----------------------#2021-11-25">https://medium.com/mlearning-ai/tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems-1c16cd1efacb?source=collection_archive---------1-----------------------#2021-11-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="e1e0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="2480" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> TensorFlow </strong> ( <strong class="je hi"> TF </strong>)是一个<strong class="je hi">接口 </strong>和<strong class="je hi"> <em class="ka">实现</em> </strong>，用于<em class="ka">表达</em>和<em class="ka">执行</em>机器学习(<strong class="je hi"> ML </strong>)算法。一个在TensorFlow内表达的计算，几乎没有变化或者没有变化，就可以在广泛的异构系统上执行(<strong class="je hi">具有不同处理器的系统</strong>)。TensorFlow可用于表达各种算法，包括深度学习(DL)模型的训练和推理。已经使用的领域:<strong class="je hi">语音识别</strong>、<strong class="je hi">计算机视觉</strong>、<strong class="je hi">机器人学</strong>、<strong class="je hi">信息检索</strong>、<strong class="je hi">自然语言处理</strong> ( <strong class="je hi"> NLP </strong>)、<strong class="je hi">地理信息提取</strong>、<strong class="je hi">计算药物发现</strong>。本文总结了TensorFlow [ <strong class="je hi"> 1 </strong> ]的第一份白皮书，描述了TensorFlow接口以及在<strong class="je hi"> Google </strong>上构建的该接口的实现。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/0f4032edc595f5fce56e1b680482c1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrvMKrWMhi3HomoiTLPsfw.png"/></div></div></figure><p id="5b89" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated"><strong class="je hi">谷歌大脑</strong>项目始于<strong class="je hi"> 2011 </strong>探索超大规模深度神经网络的用途(用于研究和谷歌的产品)。作为早期工作的一部分，难以置信接口(<strong class="je hi">第一代系统</strong>)是为<em class="ka">可扩展的分布式训练和推理</em>而构建的。“不相信”很管用，它被这个团队、谷歌的其他团队、其他字母表公司(例如谷歌搜索、谷歌翻译等)使用。基于怀疑的经验和对DNNs的训练和推理的期望系统属性和要求的更完整理解，开发了TensorFlow ( <strong class="je hi">第二代</strong>)用于大规模机器学习模型的实现和部署。</p><p id="d87a" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">TensorFlow进行计算(使用<strong class="je hi">一个类似数据流的模型</strong>进行描述，<strong class="je hi"> <em class="ka">将它们映射到各种不同的硬件平台</em> </strong>(在Android和iOS等移动设备平台上进行推理<strong class="je hi"> | </strong>使用包含一个或多个GPU卡的单台机器的中等规模的训练和推理系统<strong class="je hi"> | </strong>在数百台具有数千个GPU的专用机器上运行的大规模训练系统)。拥有一个跨越如此广泛平台的单一系统极大地简化了ML系统的实际应用，避免了维护负担和抽象漏洞。我们的目标是拥有一个灵活的系统，能够快速地在学术界进行实验，具有高性能，并且对于ML模型的生产培训和部署来说是健壮的。</p><h1 id="7d2b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">编程模型和基本概念</h1><p id="eba4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一个<strong class="je hi">张量流计算</strong>由<strong class="je hi">一个有向图</strong>描述。该图表示数据流计算，具有扩展，用于允许某些类型的节点维护和更新持久状态，以及用于在图内分支和循环控制结构，如Naiad [ <strong class="je hi"> 2 </strong> ]，这是用于执行数据并行和循环数据流程序的分布式系统<br/><em class="ka"/>。用户使用一种受支持的前端语言(如C++或Python)构建计算图。</p><p id="4407" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">下图显示了如何使用Python构建TF图的示例。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ks"><img src="../Images/3d7d3e01298d50b65a7b80d2842dc02b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UpYOEuEZlZQtV2MdKGr7tw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="cd6f" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">下图显示了生成的计算图形。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kx"><img src="../Images/d5a919a82908b448ee44fbe030f628e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVLCvhMgKxMA6LF4fmCDyw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="13a7" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">在TensorFlow计算图中，每个节点有零个(<em class="ka">如输入节点有零个输入</em>或更多个输入)和零个(<em class="ka">如输出节点或标签有零个输出</em>或更多个输出。每个节点<strong class="je hi"> <em class="ka">代表一个操作</em> </strong>的实例化。沿着正常边流动的值(从输入到输出)是<strong class="je hi">张量</strong> ( <strong class="je hi">数组</strong>)，其中底层元素类型(如整数、浮点)在图构造时被指定或推断。控制依赖边(沿着这样的边没有数据流)指示控制依赖的源节点必须在控制依赖的目的节点开始执行之前完成执行。TensorFlow有时会插入控制依赖项，以实现控制峰值内存使用等目的。</p><h2 id="5660" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">操作和内核</h2><p id="d122" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一个<strong class="je hi">操作</strong>是一个<strong class="je hi">抽象计算</strong>，像“矩阵乘法”或“加法”，带有一个名字和一些属性。所有属性都必须在构建图时提供或推断，以实例化要操作的节点。属性的一个常见用途是在不同的张量元素类型上进行多态操作(比如将两个浮点数和整数的张量相加)。</p><p id="e73f" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">一个<strong class="je hi">内核</strong>是一个特定类型设备操作的具体实现<strong class="je hi">，例如CPU或GPU。张量流二进制定义了通过注册机制可用的操作和内核的集合，并且该集合可以通过链接附加的操作和/或内核定义/注册来扩展。下表显示了核心TensorFlow库中内置的一些操作。</strong></p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lm"><img src="../Images/ebf76b350af7ffa9b93cc323bed723f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j36VqftBPfV1cvUxpUMiRg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="359d" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">会议</h2><p id="c646" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">用户程序通过创建会话与TensorFlow系统进行交互。为了创建计算图，会话接口支持扩展方法，以使用附加节点和边来扩充由会话管理的当前图，<strong class="je hi"> <em class="ka">注意，当会话被创建时，初始计算图是空的</em> </strong>。session接口支持的另一个主要操作是<strong class="je hi"> Run </strong>，它接受一组需要计算的输出名称，以及一组可选的张量，以代替节点的某些输出。</p><h2 id="c13e" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">变量</h2><p id="3299" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在大多数计算中，一个图形被执行多次。大多数张量在图的单次执行后无法存活。然而，变量是一种特殊的操作，它返回一个持久可变张量的句柄，该张量在图的执行中继续存在。这些持久可变张量的句柄可以传递给一些特殊的操作，这些操作会改变所引用的张量。对于TensorFlow的ML应用，模型的参数通常存储在变量中的张量中，并作为模型的训练图的<strong class="je hi">运行</strong>的一部分进行更新。</p><h1 id="97e7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">履行</h1><p id="3a34" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">TensorFlow系统中的主要组件是<strong class="je hi">客户端</strong>、<strong class="je hi">主机</strong>和<strong class="je hi">一个或多个工作进程</strong>。客户端使用会话接口与主服务器通信。每个工作进程负责仲裁对一个或多个计算设备(如CPU内核或GPU卡)的访问，以便按照主进程的指示在这些设备上执行图节点。TensorFlow有两种实现:<strong class="je hi">本地</strong>和<strong class="je hi">分布式</strong>。当客户机、主机和工作机都在单个操作系统进程的上下文中的单个机器上运行时，使用本地实现。另一方面，分布式实现扩展了本地实现，支持这样一种环境，在这种环境中，客户机、主机和工作机都可以在不同的机器上处于不同的进程中。在分布式环境中，不同的任务是由集群调度系统管理的作业的容器。下图显示了TensorFlow的本地和分布式实现版本。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ln"><img src="../Images/850d8c7f9598240c45d8d4371ea2a190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InQYy05L1PYI6Wf7r_0HCw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="6f70" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">设备</h2><p id="2745" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">设备是TensorFlow的计算核心。每个工人负责一台或多台设备。每个设备都有类型和名称。设备名称由标识设备类型的部分和设备在worker中的索引组成。<strong class="je hi"> <em class="ka">在分布式设置中，工人的工作和任务的标识也被考虑</em> </strong>。CPU和GPU有设备接口的实现，其他设备类型的新设备实现可以通过注册机制 提供<strong class="je hi"> <em class="ka">。每个设备对象负责管理设备内存的分配和取消分配，并负责安排TensorFlow实现中更高级别所请求的任何内核的执行。</em></strong></p><h2 id="e6b9" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">张量</h2><p id="59b0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">张量是一个类型化的多维数组。TensorFlow支持多种类型，包括从8位到64位的有符号、无符号整数、IEEE float和double类型、复数类型和字符串类型。适当大小的后备存储器由特定于张量所在设备的<strong class="je hi"> <em class="ka">分配器</em> </strong>管理。张量后备存储缓冲区是引用计数的，当没有引用剩余时被释放。</p><h2 id="6f52" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">单设备执行</h2><p id="b1ab" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是最简单的执行场景，其中单个工作进程在单个设备上工作。数据流图的节点以尊重节点间依赖性的顺序执行。当一个节点的依赖项计数变为零时，它就有资格执行，然后被添加到就绪队列中。</p><h2 id="66d1" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">多设备执行</h2><p id="5a07" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这种模式下，有两个复杂的问题:决定将图中每个节点的计算放在哪个设备上，然后管理这些放置决定所隐含的跨设备边界的数据通信。</p><p id="1449" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated"><strong class="je hi">节点放置</strong>。放置算法的一个输入是成本模型，该成本模型包含对每个图的输入和输出张量的大小(以字节为单位)的估计，以及当呈现每个节点的输入张量时对每个节点所需的计算时间的估计。放置算法从计算图的源开始，并随着其进展模拟系统中每个设备上的活动。对于在计算数据流图的遍历中到达的每个节点，考虑可行设备的集合(当设备不提供实现特定操作的内核时，该设备是不可行的)。对于具有多个可行设备的节点，放置算法使用贪婪启发式算法，该算法检查将节点放置在每个可能的设备上对节点的完成时间的影响。试探法根据成本模型考虑在该类设备上操作的估计或测量的执行时间，并且还包括将输入从其他设备传输到所考虑的设备所引入的通信成本。选择节点操作将最快完成的设备，然后布局继续为图中准备好模拟执行的其他节点做出布局决策。</p><p id="8495" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated"><strong class="je hi">跨设备通信</strong>。一旦计算出节点位置，该图就被分割成一组子图，每个器件一个子图。从x到y的任何跨设备边被移除，并被从x到x的子图中的新发送节点的边和从y的子图中的对应接收节点到y的边所替代。下图说明了这个想法。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lo"><img src="../Images/3d7cc29ab030ad346c9ad2ecae588b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*3IxoFhQJri--wJFjfHTQ5A.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="f606" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">在运行时，发送和接收节点的实现相互协调，在设备间传输数据，隔离发送和接收实现中的所有通信，这简化了运行时的其余部分。发送/接收节点强制特定设备上特定张量的所有用户使用单个接收节点，而不是特定设备上每个下游用户一个接收节点。它确保所需张量的数据仅在源设备和目的设备之间传输一次，并且张量的存储器仅分配一次，而不是多次。</p><p id="cc0a" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">此外，这些发送/接收节点允许将不同设备上的图的各个节点的调度分散到工作器中:发送和接收节点在不同的工作器和设备之间传递必要的同步，并且主节点仅需要在每次图执行时向具有图的任何节点的每个工作器发出单个运行请求，而不是参与每个节点或每个跨设备通信的调度。这使得系统更具可扩展性，并且与调度由主节点强制完成相比，允许更细粒度的节点执行。</p><h2 id="ba04" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">分布式执行</h2><p id="239c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">它非常类似于多设备执行。在器件放置之后，为每个器件创建一个子图。跨工作进程通信的发送/接收节点对使用远程通信机制，如<strong class="je hi"> TCP </strong>或<strong class="je hi"> RDMA </strong>来跨机器边界移动数据。</p><p id="2f63" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">对于容错，考虑发送和接收节点对之间的通信。此外，从主进程到每个工作进程的定期健康检查。当检测到故障时，整个图形执行被中止，并从头开始。</p><h1 id="1291" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">扩展ˌ扩张</h1><h2 id="a8d3" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">梯度计算</h2><p id="8a8f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于梯度下降在许多优化算法中使用，并且是一种常见需求，TensorFlow内置了对自动梯度计算的支持。有人提到，它使优化变得复杂，特别是对于内存使用。</p><h2 id="f571" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">部分执行</h2><p id="b8ae" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">可以根据用户的需求仅执行整个执行图的子图。为了支持它，一旦客户机在会话中建立了计算图，Run方法允许它们执行整个图的任意子图，并沿着图中的任意边注入任意数据，以及检索沿着图中的任意边流动的数据。这是通过向将要执行的子图添加feed和fetch节点来完成的，如下图所示。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lp"><img src="../Images/e13d6b375dff203dec57d0d1677f8919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fP_7L7SKeRFtjD-ti8ZRIA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="aba4" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">设备限制</h2><p id="90e9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">TensorFlow客户端可以通过为节点提供关于它可以在哪些设备上执行的部分约束来控制节点在设备上的放置。例如，仅在GPU上，任何设备，与名为variable13的节点共存。在这些约束的范围内，放置算法负责选择节点到设备的分配，该分配提供快速执行并且还满足设备施加的各种约束，例如限制设备上执行其图节点子集所需的存储器总量。支持这样的约束要求改变放置算法。首先，计算每个节点的可行设备集，然后在并置约束图上使用union-find来计算必须放在一起的图组件。对于每个这样的组件，计算可行设备集的交集。计算出的每个节点的可行器件集很容易适合放置算法的模拟器。</p><h2 id="08a9" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">控制流</h2><p id="cdd0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">没有任何显式控制流的数据流图很有表现力。然而，作者[ <strong class="je hi"> 1 </strong> ]声称，在某些情况下，支持条件和循环可以导致ML算法的更简洁和有效的表示。TensorFlow包括一小组原始的控制流操作符，它被推广用于处理循环数据流图。<strong class="je hi">开关</strong>和<strong class="je hi">合并</strong>操作允许基于布尔张量<strong class="je hi">的值跳过整个子图的执行。<strong class="je hi"> Enter </strong>、<strong class="je hi"> Leave </strong>和<strong class="je hi"> NextIteration </strong>运算符允许表达迭代。诸如if-conditionals和while-loops之类的高级编程语言结构可以用前面提到的控制流、操作符编译成数据流图。</strong></p><p id="75fe" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">TensorFlow迭代实现了类似于MIT标记令牌机[ <strong class="je hi"> 3 </strong> ]的标记和帧的概念。循环的每一次迭代都是唯一标识的，其执行状态由一个帧表示。只要输入可用，它就可以进入迭代；因此，可以同时执行多次迭代。TensorFlow使用分布式协调机制来执行带有控制流的图形。一般来说，一个环路可以包含分配给许多不同设备的节点。因此，管理循环的状态成为分布式终止检测的问题。TensorFlow的解决方案是基于<a class="ae lq" href="https://en.wikipedia.org/wiki/Graph_rewriting" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">图重写</strong>。</a></p><h2 id="6096" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">输入操作</h2><p id="e9f8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">除了通过feed节点向计算提供数据之外，另一种用于训练大规模机器学习模型的常见方法是在图中具有特殊的输入操作节点。它们配置有一组文件名，每次执行时都会产生一个张量，其中包含来自该组文件中存储的数据的一个或多个示例。这些输入操作的优点是将数据直接从底层存储系统读取到机器的内存中，该机器将对数据执行后续处理。在客户端进程与工作进程分离的配置中，如果数据被馈送，将需要额外的网络跳跃。额外的一跳，因为数据应该从存储系统到客户端，然后从客户端到工作机。但是，对于操作节点，当使用输入节点时，数据直接从存储系统传输到工作节点。</p><h2 id="a048" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">行列</h2><p id="1a12" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在TF中使用队列来实现图形不同部分的异步执行，并通过入队和出队操作移交数据。入队操作可以阻塞，直到队列中有空间可用，出队操作可以阻塞，直到队列中有所需最小数量的元素可用。</p><p id="a54f" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">队列的一个用途是当ML模型的计算部分仍在处理前一批数据时，允许从磁盘预取输入数据。</p><p id="132b" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">除了正常的FIFO队列之外，TF中还添加了洗牌队列，它在一个大的内存缓冲区中随机洗牌。这种混洗功能对于想要随机化处理示例的顺序的机器学习算法非常有用。</p><h2 id="a0f2" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">容器</h2><p id="425d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">容器是一种用于管理长期可变状态的机制。变量的后备存储位于容器中。默认容器一直存在，直到进程终止。容器可以通过完全清除其内容来重置。通过使用容器，跨与不同会话相关联的不相交计算图共享状态是可行的。</p><h1 id="b40a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">最佳化</h1><h2 id="727f" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">公共子表达式消除</h2><p id="ea3b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在编译器理论中，CSE是一种编译器优化技术，它搜索相同表达式的实例，这些实例都评估为相同的值，并分析是否值得用保存计算值的单个变量来替换它们[ <a class="ae lq" href="https://en.wikipedia.org/wiki/Common_subexpression_elimination" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> ref </strong> </a> ]。TensorFlow采用了类似于Click [ <strong class="je hi"> 4 </strong> ]的CSE，因为计算图的构建通常由客户端代码中许多不同的抽象层完成，计算图很容易以相同计算的冗余副本结束。</p><h2 id="7e83" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">控制数据通信和内存使用</h2><p id="35d8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">仔细调度TensorFlow操作可以提高数据传输和内存使用的性能。TensorFlow对于读取远程值的<strong class="je hi">接收</strong>节点的调度非常小心。如果不采取预防措施，接收节点将会比需要的时间更早地启动，当执行开始时，所有节点会同时启动。TensorFlow通过执行尽可能早/尽可能晚(<strong class="je hi"> ASAP/ALAP </strong>)计算，分析图的关键路径，以估计何时开始接收节点。然后，插入控制边以延迟接收节点的开始，直到需要它们的结果之前。</p><h2 id="593f" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">异步内核</h2><p id="a137" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">TensorFlow支持非阻塞内核(<em class="ka">记住:内核是特定设备(如CPU或GPU </em>)操作的特定实现，接口略有不同。这种优化适用于拥有许多活动线程在内存使用或其他资源方面相对昂贵的环境，并允许避免在等待I/O或其他事件发生时无限期地占用执行线程。异步内核的例子包括接收内核、入队和出队内核，如果队列空间不可用或者没有数据可供读取，这两个内核可能分别需要阻塞。</p><h2 id="490e" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">内核实现的优化库</h2><p id="ddc5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">TensorFlow使用可用的高度优化的数值库来实现一些操作的内核，如BLAS、用于执行高效矩阵乘法的cuBLAS以及用于卷积内核的Cuda-convnet、cuDNN。许多TensorFlow内核实现都是上述优化库的包装器。值得一提的是TensorFlow广泛使用开源的特征线性代数库作为其内核实现(工作团队扩展了任意维度张量运算的特征)。</p><h2 id="ffe9" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">有损压缩</h2><p id="2d3b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于训练神经网络可以容忍噪声和降低的算术精度，TensorFlow在设备之间发送数据时使用更高精度的有损压缩。</p><h1 id="a029" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">地位和经验</h1><p id="d3f3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这一小节中，作者提到了他们从怀疑(第一代)系统迁移到TensorFlow(第二代)系统的经历。</p><h1 id="3e43" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">常见的编程习惯用法</h1><p id="d7cd" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本节讨论TensorFlow中用于加速神经网络训练的技术。假设使用随机梯度下降(<strong class="je hi"> SGD </strong>)和相对适度的小批量<strong class="je hi"> 100 </strong>到<strong class="je hi"> 1000 </strong>示例来训练模型。</p><h2 id="1c26" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">数据并行训练</h2><p id="e1df" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该技术在小批量元件之间并行计算小批量的梯度。例如，如果小批量的大小是1000，在10个设备上有模型的10个副本，每个副本处理100个元素，然后组合梯度并同步应用对参数的更新。由于并行化和更多的设备，总体执行时间减少了(由于并行化，性能更高)。在这种情况下，TensorFlow图形有许多执行大量模型计算的图形部分的副本，一个客户端线程驱动这个大型图形的整个训练循环，如下图所示。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lr"><img src="../Images/092f09ed26089c080a9d99216682301c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zK8SFRgk8vdtutDt7tr6iA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="dbd5" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">在异步类型的数据并行中，每个复制品异步地将参数更新应用于模型参数。在此配置中，每个图形副本都有一个客户端线程，如下图所示。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ls"><img src="../Images/d9ae83dde7d5911d0cf41c6326bd917a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OByFuguaBfhImLQH3XW2Gw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="74f6" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">模型并行训练</h2><p id="f647" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">模型并行训练在TensorFlow框架内是可行的。下图显示了一个循环的深度LSTM模型的示例，该模型用于跨三个不同设备并行化的序列学习序列。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lt"><img src="../Images/8d165efe085c7f5f0a61c763f30426cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*XII6K2wG8_iTGBZ5E1vkYg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="afc3" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">模型计算流水线的并发步骤</h2><p id="f701" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">通过运行少量并发步骤在设备内流水线化模型计算是TensorFlow中采用的另一种提高利用率的机制。它类似于异步数据并行，只是并行发生在单个设备中。下图说明了这个想法。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lu"><img src="../Images/77eb65d83dac02fbde07c2130ae82a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*lxRWc4T4aM7b6kUV8SPJeA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h1 id="917c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">与核心TensorFlow图形执行引擎一起开发的工具</h1><h2 id="a346" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">TensorBoard:图形结构和汇总统计的可视化</h2><p id="f336" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于计算图中有大量的节点，使用简单的可视化技术经常会产生混乱不堪的图。为了帮助用户看到图形的底层组织，TensorBoard中的算法将节点折叠成高级块，突出显示具有相同结构的组。整个可视化是交互式的，用户可以放大查看更多细节。下图显示了深度卷积图像模型图形的可视化效果。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lv"><img src="../Images/f7bc14915da557062bbbc92f9957bbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ew4KEVAkQlPxdElcZ44qPA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><p id="a78a" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">另外，TensorFlow提供了汇总数据的可视化。下图显示了一个示例。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lw"><img src="../Images/1e465340f62e7b17ba54354cf2c0d171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d11mXYg1eL-ONAfY5c_aWQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">[<strong class="bd ig">1</strong>]</figcaption></figure><h2 id="1631" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">性能跟踪</h2><p id="278d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">EGG tool(非开源)是一个内部工具，用于收集和可视化关于TensorFlow图执行的精确排序和性能特征的非常细粒度的信息。它可以用于单个和分布式场景，并提供有用的信息，以便更好地理解TensorFlow程序的计算和通信模式中的瓶颈。从机器上同时收集的轨迹在一个可视化服务器中进行组合，该服务器用于提取指定时间范围内的事件，并在适当的详细级别上进行总结，以解决用户界面问题。由于通信、同步或与DMA相关的停顿而导致的任何重大延迟都会被识别出来，并在可视化中使用箭头突出显示。放大显示给用户更多的信息。下图是这种EEG可视化工具的示例。</p><div class="kc kd ke kf fd ab cb"><figure class="lx kg ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><img src="../Images/c4afb1eb649c8735028e6589b8f79d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*sUJ-yk-MGQiLnCGUU3s8qw.png"/></div></figure><figure class="lx kg md lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><img src="../Images/aa63631834fe198585f25c7477bc9dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*z2r2r8gPM0ecZeJuWk-5iA.png"/></div></figure><figure class="lx kg me lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><img src="../Images/9f76ecdc825d5e7bfd7bae9f743c38d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*DtOzlRwIUfMV4IOzEyp-UA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx mf di mg mh">[1]</figcaption></figure></div><p id="ce48" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated"><strong class="je hi">注</strong>:为了更好地理解张量流的作用，必须阅读白皮书中引用的相关工作。得律阿德斯，烟雾，CIEL，Naiad，Spark是作者声称TensorFlow从他们那里收集好东西的系统。</p><h1 id="df14" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">下一次阅读</h1><ul class=""><li id="a557" class="mi mj hh je b jf jg jj jk jn mk jr ml jv mm jz mn mo mp mq bi translated">米（meter的缩写））阿巴迪<em class="ka">等</em>，“<strong class="je hi"> TensorFlow:大规模机器学习的系统</strong>”，载于第十二届<strong class="je hi"><em class="ka">USENIX</em></strong><em class="ka">操作系统设计与实现会议</em><strong class="je hi">美国佐治亚州萨凡纳<strong class="je hi"> 2016 </strong>，bll 265–283。</strong></li></ul><h1 id="5738" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><p id="c84a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">【<strong class="je hi">1</strong>m . Abadi<em class="ka">等</em>，<strong class="je hi"> TensorFlow:异构分布式系统上的大规模机器学习</strong>。<strong class="je hi"> 2015 </strong>。</p><p id="4b41" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">[ <strong class="je hi"> 2 </strong> ] D. G. Murray，F. McSherry，R. Isaacs，M. Isard，P. Barham，en M. Abadi，<strong class="je hi"> Naiad:一个及时的数据流系统</strong>，<em class="ka">第二十四届ACM操作系统原理研讨会会议录</em>，宾夕法尼亚州法明顿，<strong class="je hi"> 2013 </strong>，bll 439–455。</p><p id="1319" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">[ <strong class="je hi"> 3 </strong> ] Arvind和R. S. Nikhil，<strong class="je hi">在麻省理工学院标记令牌数据流架构上执行程序，</strong>，<em class="ka"> IEEE计算机汇刊</em>，第39卷，第3期，第300-318页，1990年3月<strong class="je hi"/>，DOI: 10.1109/12.48862。</p><p id="eab0" class="pw-post-body-paragraph jc jd hh je b jf kn jh ji jj ko jl jm jn kp jp jq jr kq jt ju jv kr jx jy jz ha bi translated">[<strong class="je hi">4</strong>c . Click，“<strong class="je hi">全局代码运动/全局值编号</strong>”，在<em class="ka">美国加州拉荷亚ACM SIGPLAN 1995编程语言设计和实现会议</em>上，<strong class="je hi"> 1995 </strong>，bll 246–257。</p><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hi fi z dy mz ea eb na ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kl mu"/></div></div></a></div></div></div>    
</body>
</html>