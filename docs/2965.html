<html>
<head>
<title>Dimensionality Reduction — Fight Your Model Complexity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维—降低模型复杂性</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/dimensionality-reduction-fight-your-model-complexity-b80143b60b48?source=collection_archive---------2-----------------------#2022-07-03">https://medium.com/mlearning-ai/dimensionality-reduction-fight-your-model-complexity-b80143b60b48?source=collection_archive---------2-----------------------#2022-07-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3392" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将深入研究<strong class="ig hi">维度</strong> <strong class="ig hi">缩减</strong>来理解它的工作方式，以及为什么当你必须训练你的模型时，它可以成为一个救命稻草。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/ea81c514784ac9786f55007742e5144c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WDwlbaNN6bAHO03L"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@paramir?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ehud Neuhaus</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="81f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">维度</strong> <strong class="ig hi">约简</strong>常与 <strong class="ig hi">维度</strong>的<strong class="ig hi">诅咒</strong> <strong class="ig hi">概念一起引入。</strong></p><p id="d4ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">维数灾难可以表示为这样一种情况，其中 <strong class="ig hi">的<strong class="ig hi">号</strong> <strong class="ig hi">特征</strong>(表格数据的列)为<strong class="ig hi">高</strong>，而数据集中的样本没有那么多。</strong></p><p id="465c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们有一个由2个要素和12个观测值组成的数据集:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jt"><img src="../Images/8ead0060c2c72156ddf122400c5110a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DI8vBtz5udDkXiwG0iMkKQ.jpeg"/></div></div></figure><p id="26c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们添加一个特征，数据点之间的距离将增加，并改变模型的学习方式:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ju"><img src="../Images/4bea68fb78c9f24db038c89450b5320c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuHx4gC-nNh0h2zIiuCH6A.jpeg"/></div></div></figure><p id="f918" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi">高</strong> <strong class="ig hi">次元</strong> <strong class="ig hi">空间</strong>中，点彼此相距甚远。因此，对于具有许多要素的数据集来说，训练数据点将被广泛地分开。因此，维数灾难的出现意味着，如果要训练的数据太少，拥有许多特征将导致<strong class="ig hi">较差的</strong> <strong class="ig hi">学习</strong>，并且模型将对新的观察结果做出糟糕的预测。</p><blockquote class="jv jw jx"><p id="e8b1" class="ie if jy ig b ih ii ij ik il im in io jz iq ir is ka iu iv iw kb iy iz ja jb ha bi translated">在训练过程中增加特征的数量会导致数据结构更加复杂，因此，由于模型无法将观察结果相互关联，可能会出现<strong class="ig hi">过度拟合</strong>。</p></blockquote><p id="4ad1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个想法是通过允许机器从数据之间的最佳关系中学习来解决这个问题。事实上，我们所知道的机器学习是，在模型中初始化的算法将试图理解数据在它们之间是如何表示的，以便在面对新数据时能够正确地瞄准。<br/>这也适用于其他类型的数据，如图像、文本或音频，模型将学习从特征中找出区别。然而，在上面呈现的非结构化数据(图像、文本、音频……)的上下文中，所述特征成倍增加(一个单词被表示为与其自身相关，但也与其他等等相关)，这导致了无法想象的维度。</p><p id="fb44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">有什么方法可以降低数据集的维度，并面对这种负担？</strong></p><p id="34dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有不同的降维方法:一种是可以与<strong class="ig hi">投影</strong>相关联的<strong class="ig hi">线性</strong> <strong class="ig hi">方法</strong>，另一种是克服了第一种方法的缺点的<strong class="ig hi">非线性</strong> <strong class="ig hi">方法</strong>，<strong class="ig hi">流形</strong> <strong class="ig hi">学习</strong>(实际上也包括投影)。</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><h1 id="7d5a" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">对抗维数灾难的两种方法</h1><p id="4fe2" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">正如我们之前看到的，处理数据集中的维度就是处理它包含的要素数量。一个解决方案是减少这个数字的<strong class="ig hi">。然而，主要的障碍是拥有较少的特征，但是保持它们和目标之间的关系。我们想要的是我们的数据集的一种表示，它可以适合一个小维度的空间，以便模型学习得更好(更快)。但是必须维护数据的结构和每个点之间的关系。</strong></p><h2 id="025f" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">推断</h2><p id="2242" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">第一种方法基本上是通过<strong class="ig hi">将</strong> <strong class="ig hi">数据</strong> <strong class="ig hi">点</strong>从上级维度空间投影到下级维度空间来降低维度，同时保留特征之间的关系。</p><p id="e9e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们将投影应用于3D数据集，它将允许数据集被表示成二维子空间，如下所示:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ma"><img src="../Images/2ee66ea2f41c50b10adcf4069276a1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRJVE3G1LoIqMcCslel4EA.jpeg"/></div></div></figure><p id="82f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个例子的灵感来自奥雷连·盖伦的书。</p><p id="347f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以注意到的一个特殊性是，新的表示(在2D)仅使用2个特征进行投影，而3D表示使用了3个特征。然而，在此之前，能够保持特征/变量之间的关系是合乎逻辑的，投影创建包含数据集的"<strong class="ig hi">本质</strong>"的新特征。<strong class="ig hi">主</strong> <strong class="ig hi">分量</strong> <strong class="ig hi">分析</strong> ( <strong class="ig hi"> PCA </strong>)是最广为人知的投影技术，我们将在下一部分重点介绍。</p><p id="3e3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于投影的一件重要事情是，它在处理数据的线性结构时表现良好，但在处理非线性结构时有困难。它将创建一个新的表示，不会真正保留数据点之间的差异。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/fd2be827767636bcb4d584ae7a7d14be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jx4-xRFxReCsYbysvnB3UA.jpeg"/></div></div></figure><p id="3a85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所注意到的，投影使得机器学习变得困难。让我们看看如何达到第二个约简，并处理非线性结构。</p><h2 id="8053" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">流形学习</h2><p id="9d40" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">这种方法实际上包括投影，因为数学中的流形是:</p><blockquote class="mc"><p id="addb" class="md me hh bd mf mg mh mi mj mk ml jb dx translated">在每个点附近局部类似于<a class="ae js" href="https://en.wikipedia.org/wiki/Euclidean_space" rel="noopener ugc nofollow" target="_blank">欧几里得空间</a>的<a class="ae js" href="https://en.wikipedia.org/wiki/Topological_space" rel="noopener ugc nofollow" target="_blank">拓扑空间</a>(<a class="ae js" href="https://en.wikipedia.org/wiki/Manifold" rel="noopener ugc nofollow" target="_blank">维基百科</a>)。</p></blockquote><p id="3892" class="pw-post-body-paragraph ie if hh ig b ih mm ij ik il mn in io ip mo ir is it mp iv iw ix mq iz ja jb ha bi translated">流形允许将任何数据结构表示到低维空间中，通过最终将高维表示重组为唯一的局部表示来保持相同的结构。</p><p id="9fd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们考虑投影，它不会真正重新组合，而只是将3D结构表示成2D结构。</p><p id="3553" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个<strong class="ig hi">2D</strong>T8】流形是一个可以折叠成一个大空间的二维:比如一个<strong class="ig hi">球体</strong>或者一个<strong class="ig hi">圆环体</strong>。</p><p id="ce9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">流形学习站在<strong class="ig hi">两个</strong> <strong class="ig hi">假设</strong>:</p><ul class=""><li id="0e73" class="mr ms hh ig b ih ii il im ip mt it mu ix mv jb mw mx my mz bi translated">“流形假设是真实世界的高维数据存在于嵌入在高维空间中的低维流形上”(来自<a class="ae js" href="https://bjlkeng.github.io/posts/manifolds/" rel="noopener ugc nofollow" target="_blank">流形的陈述:温和的介绍</a>)。这意味着由于流形的存在，我们可以用高维数据来描述真实世界。</li><li id="e6b8" class="mr ms hh ig b ih na il nb ip nc it nd ix ne jb mw mx my mz bi translated">第二种假设倾向于解释，如果在低维流形空间中完成，则<strong class="ig hi">回归</strong>或<strong class="ig hi">分类</strong>问题<strong class="ig hi">问题</strong>将会更容易<strong class="ig hi"/><strong class="ig hi">到</strong> <strong class="ig hi">求解</strong>(这并不意味着模型会执行得更好，但是，它可以学习得更快或者只是允许他学习)。</li></ul><p id="df81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经知道了降维的两种主要方法，让我们看看使用两种算法的技术:<strong class="ig hi"> PCA </strong>和<strong class="ig hi"> LLE </strong>。</p></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><h1 id="6753" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">降维的主要技术</h1><p id="d9bf" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">我们将深入研究获取数据集结构并降低这些结构维度的技术。</p><h2 id="0435" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">主成分分析</h2><p id="7a57" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">这种技术是最常见的，也可以用于无监督学习问题。</p><p id="8871" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> PCA </strong>使用投影方法来降低数据的维度。它找到数据集结构中的主要成分，并保留最有用的成分。</p><p id="cf33" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">主分量是<strong class="ig hi">单位向量</strong>(特别能指定平面上非nul向量方向的向量) :</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nf"><img src="../Images/54ed8e1e9816b35cbf0907e4da7eb96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ocb3X8P7oN9OaEoqQqrP-w.jpeg"/></div></div></figure><p id="bfa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些组件跟踪数据结构的表示，因为每个组件都对数据的变化有贡献(保留信息)。向量c1和c2是数据集的两个第一主分量。</p><p id="2e61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"> <strong class="ig hi">的<strong class="ig hi">比率</strong> <strong class="ig hi">解释</strong> <strong class="ig hi">方差</strong>是PCA中的重要度量之一，因为通过查看每个组件的比率，您可以选择要保留的组件数量作为数据集的新表示。例如，第一个分量(最佳分量)可以解释数据中80%的方差，第二个分量可以解释10% …如果您决定保留90%的方差，那么您将保留这两个分量，您的数据集现在将由2个要素组成，因此是2个维度。这就是基于投影的降维。</strong></p><p id="884a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于这些分量是单位向量，数据集的原始特征将不再用于学习，新特征将是主分量1、主分量2…</p><p id="9a14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">主成分分析在<strong class="ig hi"> Scikit-Learn </strong>中可用，代码如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ng"><img src="../Images/d6e200b042673ffc555e45100144da00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ftjpenr_OPaucRIcxB7MeA.png"/></div></div></figure><p id="604a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA的<strong class="ig hi">主要问题</strong>是它主要对<strong class="ig hi">线性</strong> <strong class="ig hi">数据</strong> <strong class="ig hi">结构</strong>起作用<strong class="ig hi">的事实，这就是为什么存在其他技术的原因。</strong></p><h2 id="da97" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated">局部线性嵌入</h2><p id="c2c8" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated"><strong class="ig hi"> LLE </strong>是一种非线性降维技术，它评估高维空间中每个数据点的邻域，并寻找保持这些最近邻域关系的较低邻域。<br/>sci kit-Learn定义提供了一个很好的比较:LLE“<em class="jy">可以被认为是一系列局部主成分分析，它们被全局比较以找到最佳非线性嵌入</em>”。</p><p id="cd19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这项技术分几个步骤进行:</p><ul class=""><li id="4ff4" class="mr ms hh ig b ih ii il im ip mt it mu ix mv jb mw mx my mz bi translated">找到每个数据点的<strong class="ig hi">局部线性关系</strong> / <strong class="ig hi"> k </strong> - <strong class="ig hi">最近的</strong> <strong class="ig hi">邻居</strong>，并关联使数据点与其邻居之间的距离最小的权重。由于这一点，该算法知道所有的本地社区。</li><li id="fdc4" class="mr ms hh ig b ih na il nb ip nc it nd ix ne jb mw mx my mz bi translated">一旦它理解了数据，它将寻找在<em class="jy"> d </em>维空间中所有这些局部线性关系的<strong class="ig hi">新的</strong>T35】表示。为此，它在新空间中搜索每个点的位置，该位置考虑第一步中找到的最小化的关系和距离。</li></ul><p id="797e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为PCA，<strong class="ig hi"> LLE </strong>在Scikit-Learn中可用:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ng"><img src="../Images/31de7283f6be6102d5e2a616c921ef7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSa19EjGdfwxPfte3VfNBg.png"/></div></div></figure><h1 id="3ada" class="kj kk hh bd kl km nh ko kp kq ni ks kt ku nj kw kx ky nk la lb lc nl le lf lg bi translated"><strong class="ak">结论</strong></h1><p id="a337" class="pw-post-body-paragraph ie if hh ig b ih lh ij ik il li in io ip lj ir is it lk iv iw ix ll iz ja jb ha bi translated">在一个数据集越来越大的世界里，维数约简 <strong class="ig hi">非常重要和有用。任何非结构化和复杂的数据都需要经过降维，以便模型进行学习。<br/>存在不同的方法，如<strong class="ig hi">投影</strong>或<strong class="ig hi">流形</strong> <strong class="ig hi">学习</strong>，每种方法都有可直接应用于数据的算法。例如<strong class="ig hi"> PCA </strong>或<strong class="ig hi"> LLE </strong>，但其他一些也同样有效:<strong class="ig hi"> Isomap </strong>，<strong class="ig hi"> t-SNE </strong> <strong class="ig hi">方法</strong>，<strong class="ig hi"> LDA </strong>(线性判别分析)…</strong></p><p id="4f12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢并更好地了解降维是如何工作的！</p><h2 id="f667" class="lm kk hh bd kl ln lo lp kp lq lr ls kt ip lt lu kx it lv lw lb ix lx ly lf lz bi translated"><strong class="ak">资源:</strong></h2><div class="nm nn ez fb no np"><a href="https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">Python -机器学习掌握中的主成分降维分析</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">减少预测模型的输入变量的数量被称为降维。更少的输入…</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od jm np"/></div></div></a></div><div class="nm nn ez fb no np"><a href="https://scikit-learn.org/stable/modules/manifold.html" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">2.2.流形学习</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">寻找最基本的必需品简单的最基本的必需品忘记你的忧虑和冲突我是说最基本的…</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">scikit-learn.org</p></div></div><div class="ny l"><div class="oe l oa ob oc ny od jm np"/></div></div></a></div><div class="nm nn ez fb no np"><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">通过最近的一系列突破，深度学习推动了整个机器学习领域。现在，甚至…</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">www.oreilly.com</p></div></div><div class="ny l"><div class="of l oa ob oc ny od jm np"/></div></div></a></div><div class="nm nn ez fb no np"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hi fi z dy nu ea eb nv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">medium.com</p></div></div><div class="ny l"><div class="og l oa ob oc ny od jm np"/></div></div></a></div></div></div>    
</body>
</html>