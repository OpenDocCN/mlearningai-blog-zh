<html>
<head>
<title>BLOOM 176B — how to run a real LARGE language model in your own cloud?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BLOOM 176B —如何在自己的云中运行真正的大型语言模型？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/bloom-176b-how-to-run-a-real-large-language-model-in-your-own-cloud-e5f6bdfb3bb1?source=collection_archive---------0-----------------------#2022-12-27">https://medium.com/mlearning-ai/bloom-176b-how-to-run-a-real-large-language-model-in-your-own-cloud-e5f6bdfb3bb1?source=collection_archive---------0-----------------------#2022-12-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d19cb8c8910aa0a269d42a1c6a467c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWOzKkTbwjnRplxYJLAH5g.png"/></div></div></figure><h1 id="f0e9" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">我们中的许多人以SaaS的方式使用GPT-3或其他LLM，由他们的供应商托管。但是<strong class="ak">在自己的云中运行GPT-3大小的模型是什么感觉？</strong></h1><p id="d7af" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi">设置起来并不琐碎，但是运行自己的模型</strong>超级刺激。让我告诉你如何开始，你可以期待什么样的结果。</p><p id="f079" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><a class="ae kq" href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi">BLOOM</strong></a>—big science大型开放科学开放存取多语种语言模型是由<strong class="jp hi"> 1000+研究人员</strong><a class="ae kq" href="https://bigscience.huggingface.co/" rel="noopener ugc nofollow" target="_blank">创建的一个基于<strong class="jp hi"> transformer的语言模型</strong>。对其进行了关于</a><a class="ae kq" href="https://arxiv.org/abs/2211.05100" rel="noopener ugc nofollow" target="_blank">1.6 TB预处理多语言文本</a>的培训。这是免费的——任何想尝试的人都可以试试。参数中最大的BLOOM模型的大小是176B，大约是有史以来最成功的语言模型openAI的GPT-3模型的大小。还有一些更小的型号，如7b、3b、1b7等。</p><p id="44bd" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">当你可以像SaaS像自来水一样使用商业模式时，你为什么要建立自己的模式</strong>？在许多原因中，主要论点是完全的<strong class="jp hi">数据主权</strong>——预训练数据和用户输入的数据完全在你的控制之下——而不是在人工智能公司的控制之下。BLOOM还没有真正的托管SaaS解决方案。</p><p id="1b4b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">好的，那你是怎么做的呢？</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e9e0f3324c28521e85db6630f0fecebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lmbIvqsq9F794Cqe-1M4A.png"/></div></div></figure><h1 id="1783" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated"><strong class="ak">型号尺寸</strong></h1><p id="cb88" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们的Bloom模型需要大约360 GB的RAM来运行——这是一个传统云托管无法通过双击按钮来实现的要求，而且这也非常昂贵。</p><p id="ad77" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">幸运的是，微软已经提供了一个具有INT8权重(来自原始FLOAT16权重)的<a class="ae kq" href="https://huggingface.co/microsoft/bloom-deepspeed-inference-int8" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">下采样变体</strong> </a> <strong class="jp hi">，它运行在<strong class="jp hi"> DeepSpeed推断</strong>引擎上，并使用张量平行排列。DeepSpeed-Inference引入了几个特性来有效地服务于基于transformer的PyTorch模型。它支持模型并行性(MP ),以适应原本不适合GPU内存的大型模型。</strong></p><p id="77eb" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">这里有更多关于最小化和加速模型的信息。</p><p id="a314" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在微软回购协议中，张量被分成8个部分。因此，一方面，绝对模型尺寸减小，另一方面，较小的模型被拆分和并行化，因此可以分布在<strong class="jp hi"> 8个GPU</strong>上。</p><h1 id="23e7" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">托管设置</h1><p id="22ec" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们为模型选择的主机是AWS，因为它为能够初始化模型的深度学习容器提供了SageMaker设置。这样做的说明可以在这里找到:<br/> <a class="ae kq" href="https://aws.amazon.com/de/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/" rel="noopener ugc nofollow" target="_blank">用大型模型推理深度学习容器和DeepSpeed </a>在亚马逊SageMaker上部署BLOOM-176B和OPT-30B。</p><p id="e947" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">请只考虑Bloom 176b部分，OPT部分与我们的目的无关。</p><p id="366b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在AWS上，你必须<strong class="jp hi">找到正确的数据中心</strong>来设置模型。所需的能力不是随处可得的。我们去了东海岸，去了弗吉尼亚州，乘坐us-east-1.amazonaws.com的T9号列车。您必须通过支持获得您需要的实例，您无法通过自我配置来实现。<strong class="jp hi">你需要8个Nvidia A100，每个40 GB内存。</strong></p><p id="ad1a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">我们的devops工程师Thomas </strong>找到了创建环境的方法，他最终完成了创建，在此，我要为他的成功向他深深鞠躬。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f01e5420fe94cd120b9694e1abe51ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G22Ylb3hGOYghrBUqPLgow.png"/></div></div></figure><h1 id="7dab" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">启动模型</h1><p id="7bdb" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">托管模型可以从Huggingface上的<a class="ae kq" href="https://huggingface.co/microsoft/bloom-deepspeed-inference-int8/tree/main" rel="noopener ugc nofollow" target="_blank">微软存储库</a>加载到同一数据中心的S3中——这就是我们所做的，以便让模型接近运行时环境——或者您可以在公共S3环境中使用AWS提供的模型。<strong class="jp hi">型号大小为180 GB。</strong></p><p id="1ebc" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在Github 上的<a class="ae kq" href="https://aws.amazon.com/de/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/" rel="noopener ugc nofollow" target="_blank">说明</a>和<a class="ae kq" href="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/nlp/realtime/llm/bloom_176b/djl_deepspeed_deploy.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter笔记本中，您将找到在Sagemaker上创建模型和设置端点的各个步骤，这些步骤对于获得低延迟运行的模型是必要的。</a></p><p id="1c4d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">搞定。BLOOM 176B模型现在正在运行。</strong></p><p id="06ce" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在我们的设置中，运行时每小时花费大约32美元。因此，如果模型不需要在生产系统中一直运行，那么为测试回合启动模型并再次关闭它以释放资源是有意义的。使用该脚本，您可以在大约18分钟内启动它，关闭和释放资源需要几秒钟。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9d4ab7d79396a5c23db998b28e4b084b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAHLhzRt1IkjH7DmDxANWg.png"/></div></div></figure><h1 id="1603" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用模型</h1><p id="fef5" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我们在Sagemaker端点之上的接口中放置了一个定制的API网关和lambda函数，允许用户使用API密钥进行外部连接——这使得使用和调用更加容易。点击此处查看介绍。</p><p id="138b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">对Bloom模型的调用原则上与对其他完成模型的调用完全相同:您抛出一个文本和参数，如temperature、max_new_tokens等。并得到一个文本响应。</p><pre class="kr ks kt ku fd kv kw kx bn ky kz bi"><span id="8030" class="la iq hh kw b be lb lc l ld le">smr_client.invoke_endpoint(<br/>    EndpointName=endpoint_name,<br/>    Body=json.dumps(<br/>        {<br/>            "input": "The BLOOM large language model is a",<br/>            "gen_kwargs": {<br/>                "min_length": 5,<br/>                "max_new_tokens": 100,<br/>                "temperature": 0.8,<br/>                "num_beams": 5,<br/>                "no_repeat_ngram_size": 2,<br/>            },<br/>        }<br/>    ),<br/>    ContentType="application/json",<br/>)["Body"].read().decode("utf8")</span></pre><p id="5cda" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们首先用一个较小的BLOOM模型1B7(带有SageMaker JumpStart)测试了模型接口，然后在一切正常后，用大型的176B模型进行了测试。</p><p id="1904" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">嗯，结果是什么？</strong></p><p id="4105" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我们让模型做两个hello-world和一个goodbye-world:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lf"><img src="../Images/c8d2bd49e0ee693274482b1464e16900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FU6hffsogiiKfn041E8ojg.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/296f81e593771445878b3bd10a766618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ9eUwVJrhnMgknkcBBWvg.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lh"><img src="../Images/40de67bae8efb3c4172c1aa91d6c711f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UjBd-RY2jrXbnKUyiaePw.png"/></div></div></figure><p id="e05a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">成功了！</strong></p><p id="e8ce" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">随着模型的美好，但不是非常具体的再见世界，我想关闭今年。我将在后续文章中与BLOOM-model分享一些能力测试的结果。</p><p id="65b5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">非常感谢，甘兹，甘兹，感谢托马斯·博格曼</p><p id="dca9" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">获取有关设置和文章的帮助。</p><p id="05f3" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">用OpenAI的DALL-E 2生成的所有bloomy图像。</p><p id="9d51" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">请随时问我关于BLOOM设置的问题。</p><div class="li lj ez fb lk ll"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lm ab dw"><div class="ln ab lo cl cj lp"><h2 class="bd hi fi z dy lq ea eb lr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ls l"><h3 class="bd b fi z dy lq ea eb lr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lt l"><p class="bd b fp z dy lq ea eb lr ed ef dx translated">medium.com</p></div></div><div class="lu l"><div class="lv l lw lx ly lu lz in ll"/></div></div></a></div></div></div>    
</body>
</html>