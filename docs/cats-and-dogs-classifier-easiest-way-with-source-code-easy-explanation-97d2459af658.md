# çŒ«ç‹—åˆ†ç±»å™¨â€”â€”æœ€ç®€å•çš„æ–¹æ³•â€”â€”å¸¦æºä»£ç â€”â€”ç®€å•æ˜“æ‡‚

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/cats-and-dogs-classifier-easiest-way-with-source-code-easy-explanation-97d2459af658?source=collection_archive---------0----------------------->

åœ¨ä»Šå¤©çš„åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ„å»ºä¸€ä¸ªçŒ«ç‹—åˆ†ç±»å™¨ã€‚è¿™å°†æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„åšå®¢ï¼Œæ‰€ä»¥æ²¡æœ‰ä»»ä½•è¿›ä¸€æ­¥çš„åˆ°æœŸã€‚

**åœ¨è¿™é‡Œé˜…è¯»å¸¦æºä»£ç çš„æ•´ç¯‡æ–‡ç« â€”**[https://machine learning projects . net/cats-and-dogs-classifier/](https://machinelearningprojects.net/cats-and-dogs-classifier/)

![](img/a33f869201a692068f009a221877ddc6.png)

augmented images

# è®©æˆ‘ä»¬å¼€å§‹å§â€¦

## æ­¥éª¤ 1-å¯¼å…¥çŒ«ç‹—åˆ†ç±»å™¨æ‰€éœ€çš„åº“ã€‚

```
import numpy as np
import pandas as pd 
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
import cv2
```

## æ­¥éª¤ 2 â€”åˆå§‹åŒ–ä¸€äº›å¸¸æ•°ã€‚

```
IMAGE_WIDTH=128
IMAGE_HEIGHT=128
IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNELS=3
```

## æ­¥éª¤ 3-åŠ è½½çŒ«ç‹—åˆ†ç±»å™¨çš„è¾“å…¥æ•°æ®ã€‚

```
filenames = os.listdir("train")
categories = []
for filename in filenames:
    category = filename.split('.')[0]
    if category == 'dog':
        categories.append(1)
    else:
        categories.append(0)

df = pd.DataFrame({
    'filename': filenames,
    'category': categories
})
```

*   æˆ‘ä»¬åœ¨ç«è½¦æ–‡ä»¶å¤¹é‡Œæœ‰æˆ‘ä»¬æ‰€æœ‰çš„å›¾åƒã€‚æ‰€ä»¥ os.listdir('train ')ä¼šç»™å‡ºæ‰€æœ‰å›¾åƒåç§°çš„åˆ—è¡¨ã€‚
*   ç„¶åæˆ‘ä»¬éå†æ‰€æœ‰å›¾åƒï¼Œä»å›¾åƒåç§°ä¸­æå–å®ƒä»¬çš„ç±»åˆ«(ç‹—æˆ–çŒ«)ã€‚
*   æœ€åï¼Œæˆ‘ä»¬æ­£åœ¨åˆ›å»ºæ•°æ®çš„æ•°æ®æ¡†æ¶ã€‚

## æ­¥éª¤ 4 â€”æ£€æŸ¥æ•°æ®çš„å¤´éƒ¨ã€‚

```
df.head()
```

![](img/c120fc425969d5c286f2650fb333d1b9.png)

## æ­¥éª¤ 5 â€”æ£€æŸ¥æ•°æ®çš„å°¾éƒ¨ã€‚

```
df.tail()
```

![](img/ab6379e5c8d57b3844e956829daa9b0b.png)

## æ­¥éª¤ 6 â€”å¯è§†åŒ–ç±»åˆ«åˆ—ã€‚

```
df[â€˜categoryâ€™].value_counts().plot.bar()
```

![](img/cf2d110154437bba87d2cc042046458f.png)

## æ­¥éª¤ 7-ä¸ºçŒ«ç‹—åˆ†ç±»å™¨å»ºç«‹æ¨¡å‹ã€‚

```
model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes

model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

model.summary()
```

*   åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªéå¸¸å®¹æ˜“ä½¿ç”¨çš„é¡ºåº Keras æ¨¡å‹ã€‚åªéœ€ä½¿ç”¨ model.add()å¹¶æ ¹æ®æ‚¨çš„ç”¨ä¾‹æ–¹ä¾¿åœ°ç»§ç»­æ·»åŠ å±‚ã€‚
*   è¿™é‡Œæˆ‘ä»¬åŸºæœ¬ä¸Šç”¨äº† 3 å¥—[**ã€Conv2Dã€‘**](https://keras.io/api/layers/convolution_layers/convolution2d/)**â€”â€”**[**batch normalization**](https://keras.io/api/layers/normalization_layers/batch_normalization/)**â€”â€”**[**max pooling**](https://keras.io/api/layers/pooling_layers/max_pooling2d/)**â€”â€”**[**Dropout**](https://keras.io/api/layers/regularization_layers/dropout/)å›¾å±‚ã€‚
*   ä¹‹åï¼Œæˆ‘ä»¬å±•å¹³è¿™äº›å±‚çš„ç»“æœï¼Œå¹¶å°†è¿™äº›ç»“æœä¼ é€’ç»™ [**å¯†é›†**](https://keras.io/api/layers/core_layers/dense/) å±‚æˆ–å®Œå…¨è¿æ¥çš„å±‚ã€‚
*   æœ€åçš„ [**å¯†é›†**](https://keras.io/api/layers/core_layers/dense/) å±‚å°†æ€»æ˜¯åŒ…å« n ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­ n æ˜¯æ•°æ®é›†ä¸­çš„ç±»çš„æ•°é‡ã€‚
*   æ„å»ºæ¨¡å‹çš„æœ€åä¸€æ­¥æ˜¯ model.compile()ï¼Œå®ƒç”¨äºå°†æˆ‘ä»¬ä¸Šé¢æ‰€åšçš„ä¸€åˆ‡æ”¾åœ¨ä¸€èµ·ã€‚
*   æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ [**åˆ†ç±»äº¤å‰ç†µ**](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class) å› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œæœ‰ 2 ä¸ªåˆ†ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† [**rmsprop**](https://keras.io/api/optimizers/rmsprop/) ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ adamï¼Œä½† rmsprop åœ¨è¿™ç§æƒ…å†µä¸‹æä¾›äº†æ›´å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å°†è¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡æ˜¯**å‡†ç¡®æ€§**ã€‚

![](img/8b9e09bbb2db4004fa062d8a0e903587.png)

## æ­¥éª¤ 8-åˆå§‹åŒ–çŒ«ç‹—åˆ†ç±»å™¨æ¨¡å‹çš„å›è°ƒã€‚

```
earlystop = EarlyStopping(patience=10)

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', 
                                            patience=2, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)

callbacks = [earlystop, learning_rate_reduction]
```

*   ç®€å•åˆå§‹åŒ– [**è¿™é‡Œæå‰åœæ­¢**](https://keras.io/api/callbacks/early_stopping/) å’Œ[**ReduceLROnPlateau**](https://keras.io/api/callbacks/reduce_lr_on_plateau/)ã€‚
*   æå‰åœæ­¢å½“ val_accuracy åœæ­¢å¢åŠ æˆ– val_loss åœæ­¢å‡å°‘æ—¶ï¼Œæå‰åœæ­¢è®­ç»ƒã€‚
*   å½“æˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°æŸå¤±å‡½æ•°çš„æå°å€¼é™„è¿‘æ—¶ï¼ŒReduceLROnPlateau é™ä½å­¦ä¹ ç‡ã€‚

## æ­¥éª¤ 9-ç”¨çŒ«ä»£æ›¿ 0ï¼Œç”¨ç‹—ä»£æ›¿ 1ã€‚

```
df[â€œcategoryâ€] = df[â€œcategoryâ€].replace({0: â€˜catâ€™, 1: â€˜dogâ€™})
```

## æ­¥éª¤ 10-å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯ã€‚

```
train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)

train_df = train_df.reset_index(drop=True)
validate_df = validate_df.reset_index(drop=True)
```

## æ­¥éª¤ 11-åˆ—è½¦æ•°æ®ä¸­ä¸¤ä¸ªç±»åˆ«çš„è®¡æ•°ã€‚

```
train_df[â€˜categoryâ€™].value_counts().plot.bar()
```

*   0 å’Œ 1 åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å„æœ‰ 10000 å¹…å›¾åƒã€‚

![](img/72abfd2fadca422ae26f2515c4abf53f.png)

## ç¬¬ 12 æ­¥â€”éªŒè¯æ•°æ®ä¸­ä¸¤ä¸ªç±»åˆ«çš„è®¡æ•°ã€‚

```
validate_df[â€˜categoryâ€™].value_counts().plot.bar()
```

*   0 å’Œ 1 åœ¨éªŒè¯æ•°æ®é›†ä¸­å„æœ‰ 2500 ä¸ªå›¾åƒã€‚

![](img/684db9b50d750b04b9981f704136549d.png)

## æ­¥éª¤ 13â€”â€”è·å¾—ä¸€äº›å½¢çŠ¶ã€‚

```
total_train = train_df.shape[0]
total_validate = validate_df.shape[0]
batch_size=15
```

*   ç¬¬ä¸€è¡Œç»™å‡ºäº†è®­ç»ƒæ•°æ®é›†ä¸­çš„å›¾åƒæ•°é‡ã€‚
*   ç¬¬äºŒè¡Œç»™å‡ºäº†éªŒè¯æ•°æ®é›†ä¸­å›¾åƒçš„æ•°é‡ã€‚

## æ­¥éª¤ 14-æ‰©å……çŒ«ç‹—åˆ†ç±»å™¨çš„è®­ç»ƒæ•°æ®ã€‚

```
train_datagen = ImageDataGenerator(
    rotation_range=15,
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    "train/", 
    x_col='filename',
    y_col='category',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)
```

*   ä½¿ç”¨ [**å›¾åƒæ•°æ®ç”Ÿæˆå™¨**](https://keras.io/api/preprocessing/image/) æ‰©å……åˆ—è½¦æ•°æ®ã€‚

![](img/4438b967562b3755046b5d8a9d2e7634.png)

## æ­¥éª¤ 15â€”â€”æ‰©å……éªŒè¯æ•°æ®ã€‚

```
validation_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = validation_datagen.flow_from_dataframe(
    validate_df, 
    "train/", 
    x_col='filename',
    y_col='category',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)
```

*   ä½¿ç”¨ [**å›¾åƒæ•°æ®ç”Ÿæˆå™¨**](https://keras.io/api/preprocessing/image/) æ‰©å……éªŒè¯æ•°æ®ã€‚

![](img/be47385c36f6b56fda2d245dc1fbd965.png)

## æ­¥éª¤ 16-åœ¨éšæœºç¤ºä¾‹å›¾åƒä¸Šå¯è§†åŒ–å¢å¼ºã€‚

```
example_df = train_df.sample(n=1).reset_index(drop=True)

example_generator = train_datagen.flow_from_dataframe(
    example_df, 
    "train/", 
    x_col='filename',
    y_col='category',
    target_size=IMAGE_SIZE,
    class_mode='categorical'
)

plt.figure(figsize=(12, 12))
for i in range(0, 15):
    plt.subplot(5, 3, i+1)
    for X_batch, Y_batch in example_generator:
        image = X_batch[0]
        plt.imshow(image)
        break
plt.tight_layout()
plt.show()
```

![](img/a33f869201a692068f009a221877ddc6.png)

augmented images

## æ­¥éª¤ 17-è®­ç»ƒå’Œä¿å­˜æˆ‘ä»¬çš„çŒ«ç‹—åˆ†ç±»å™¨æ¨¡å‹ã€‚

```
epochs = 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)
model.save("model.h5")
```

## ç¬¬ 18 æ­¥â€”â€”å¯è§†åŒ–åŸ¹è®­è¿‡ç¨‹ã€‚

```
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
ax1.plot(history.history['loss'], color='b', label="Training loss")
ax1.plot(history.history['val_loss'], color='r', label="validation loss")
ax1.set_xticks(np.arange(1, epochs, 1))
ax1.set_yticks(np.arange(0, 1, 0.1))

ax2.plot(history.history['accuracy'], color='b', label="Training accuracy")
ax2.plot(history.history['val_accuracy'], color='r',label="Validation accuracy")
ax2.set_xticks(np.arange(1, epochs, 1))

legend = plt.legend(loc='best', shadow=True)
plt.tight_layout()
plt.show()
```

![](img/06309d6a5ec496ccb1b9bbda27a614cf.png)

## æ­¥éª¤ 19-çŒ«ç‹—åˆ†ç±»å™¨çš„å®æ—¶é¢„æµ‹ã€‚

```
for i in range(10):
    all_test_images = os.listdir('test')
    random_image = random.choice(all_test_images)
    img = cv2.imread(f'test/{random_image}')
    img = cv2.resize(img,(IMAGE_HEIGHT,IMAGE_WIDTH))

    org = img.copy()
    img = img.reshape(1,128,128,3)

    pred = model.predict(img)
    print(['cat','dog'][int(pred[0][0])])
    cv2.imshow('Live predictions',org)
    cv2.waitKey(0)
cv2.destroyAllWindows()
```

*   ä»æµ‹è¯•æ–‡ä»¶å¤¹ä¸­éšæœºå–å‡º 10 å¼ å›¾ç‰‡ï¼Œå¯¹å…¶è¿›è¡Œé¢„æµ‹ã€‚

å¦‚æœå¯¹çŒ«ç‹—åˆ†ç±»å™¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·é€šè¿‡ç”µå­é‚®ä»¶æˆ– LinkedIn è”ç³»æˆ‘ã€‚

***æ¢ç´¢æ›´å¤šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€NLPã€Flask é¡¹ç›®è®¿é—®æˆ‘çš„åšå®¢â€”*** [***æœºå™¨å­¦ä¹ é¡¹ç›®***](https://machinelearningprojects.net/)

**å¦‚éœ€è¿›ä¸€æ­¥çš„ä»£ç è§£é‡Šå’Œæºä»£ç ï¼Œè¯·è®¿é—®æ­¤å¤„**â€”[https://machine learning projects . net/cats-and-dogs-classifier/](https://machinelearningprojects.net/cats-and-dogs-classifier/)

*è¿™å°±æ˜¯æˆ‘å†™ç»™è¿™ä¸ªåšå®¢çš„å…¨éƒ¨å†…å®¹ï¼Œæ„Ÿè°¢ä½ çš„é˜…è¯»ï¼Œæˆ‘å¸Œæœ›ä½ åœ¨é˜…è¯»å®Œè¿™ç¯‡æ–‡ç« åï¼Œèƒ½æœ‰æ‰€æ”¶è·ï¼Œç›´åˆ°ä¸‹ä¸€æ¬¡ğŸ‘‹â€¦*

***é˜…è¯»æˆ‘ä¹‹å‰çš„å¸–å­:*** [***ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨é™ç»´***](https://machinelearningprojects.net/dimensionality-reduction-using-autoencoders/)

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)