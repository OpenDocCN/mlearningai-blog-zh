# 蜂巢教程 1

> 原文：<https://medium.com/mlearning-ai/hive-tutorial-1-1c4192389c02?source=collection_archive---------2----------------------->

## **1。为什么我们需要一个蜂巢？**

在深入研究蜂巢之前，我们应该知道我们为什么需要它。所以，这一切都始于 90 年代初，当时脸书刚刚起步，用户数量慢慢增加。随着时间的推移，他们拥有了 10 亿用户，他们必须处理的数据也随之增加，每天大约有 1000 的数据、10 万次查询和 500 张上传的照片。因此，这是一个巨大的数据量，我们需要处理它。
所以，每个人首先想到使用的是“RDBMS ”,但是 RDBMS 不能处理如此大量的数据，也没有足够的能力处理这些数据。因此，我们可以使用的第二个工具是“Hadoop ”,它能够处理大量的数据，但在 Hadoop 上处理所有查询并不容易，因为我们需要了解 MAP-REDUCE，而且在 HADOOP 中，处理查询需要花费大量时间。但是所有的软件开发人员都知道“SQL ”,所以脸书所做的是将 HADOOP
和 SQL 结合起来。他们提出了一个解决方案，利用 HADOOP 的数据处理能力和 SQL 接口，这是 HIVE 的起源。
因此，Hive 的创建是为了让拥有强大 SQL 技能的分析师能够对脸书存储在 HDFS 的大量数据进行查询。
它是在脸书开发的，这样有 SQL 经验的人就能够在不实际学习 MapReduce 或新编程语言的情况下查询数据集。

## **2。什么是 HIVE？**

Apache Hive 是一个分布式容错数据仓库系统，支持大规模分析。数据仓库提供了一个中央信息存储库，可以轻松地对其进行分析，从而做出基于数据的明智决策。Hive 建立在 Apache Hadoop 之上，这是一个开源框架，用于高效存储
和处理大型数据集。因此，Hive 与 Hadoop 紧密集成，旨在快速处理数 Pb 的数据。

## **3。什么是数据仓库？**

数据仓库是一种数据管理系统，旨在实现和支持分析。数据仓库仅用于执行查询和分析，通常包含大量历史数据。
数据仓库集中并整合了来自多个来源的大量数据。其分析能力使组织能够从数据中获得有价值的业务见解，从而改进决策。

## **4。Hive 和 Hadoop 的区别？**

Hive 是一个处理大数据的框架 Hive 是一个基于 SQL 的工具，构建于 Hadoop 之上，用于处理数据。Hadoop 适用于所有类型的数据，无论是结构化、非结构化还是半结构化数据。Hive 只能处理/查询结构化数据。Hadoop 只能理解 Map Reduce。它编译语言有两个主要任务:映射器和缩减器。我们可以使用 Python 或 Java 来定义任务。Hive 支持用于交互和数据建模的类似 SQL 的查询语言，即 HiveQL。

Hadoop 具有较低的抽象级别，而 Hive 是较高的抽象级别。
因此，Hadoop 效率较高，Hive 效率较低，因为 Map-Reduce 是 Hadoop 不可或缺的一部分，Hive 的查询首先转换为 Map Reduce，然后由 Hadoop 处理以查询数据。
Hadoop 中需要定义更多行代码。在 Hive 中执行一个查询需要更少的代码行，这就是 Hadoop 需要更多开发时间而 Hive 需要更少开发时间的原因。

# **5。蜂巢的主要特征**

1.  Hive 提供了类似 SQL 的声明性语言，称为 HiveQL，用于表达查询。
2.  各种用于处理日期、字符串等的内置函数。
3.  数据的简单 ETL(提取、转换和加载)。
4.  Hive engine 将这些查询编译成要在 Hadoop 上执行的 Map-Reduce 作业。
5.  在 Hive 中，首先创建表和数据库，然后将数据加载到这些表中。
6.  Hive 是一个数据仓库，专门用于管理和查询存储在表中的结构化数据。
7.  在处理结构化数据时，Map 减少了像 UDF 这样的可用性特性，但 Hive framework 做到了。
8.  Hive 支持分区和桶概念，以便在客户端执行查询时轻松检索数据。
9.  Hive 的一个新的重要组件，即用于存储模式信息的元存储。
10.  Hive 支持四种文件格式:文本文件、序列文件、ORC 和 RCFILE(记录列文件)。

## 6.蜂巢建筑

一个蜂巢由 3 部分组成:
*1。Hive 客户端
2。蜂巢服务
3。Hive 存储与计算*
**Hive 客户端:**
Hive 允许用各种语言编写应用，包括 Java、Python、C++。它支持不同类型的客户端，例如:-
***节俭服务器-***
它是一个跨语言服务提供商平台，请求所有那些支持节俭的
编程语言。脸书的大多数服务都写得很节俭。
***JDBC 驱动-***
用于在 hive 和 Java 应用之间建立连接。JDBC 驱动程序存在于类*org . Apache . Hadoop . hive . JDBC . Hive Driver .*
***ODBC 驱动程序-***
它允许支持 ODBC 协议的应用程序连接到 Hive。

***Hive 服务:***
***以下是 Hive 提供的服务:-***
***Hive CLI***—Hive CLI(命令行界面)是一个 shell，我们可以在其中执行 Hive 查询和命令。
***【Hive Web 用户界面***—Hive Web 用户界面只是 Hive CLI 的一个替代方案。它为执行 Hive 查询和命令提供了一个基于 web 的 GUI。
***Hive 元存储*** —它是一个中央存储库，存储仓库中各种表和分区的所有结构信息。
***Hive 服务器*** —它被称为 Apache 节俭服务器。它接受来自不同客户端的请求，并将其提供给配置单元驱动程序。

***Hive 驱动程序—*** 它接收来自不同来源的查询，如 web UI、CLI、Thrift 和 JDBC/ODBC 驱动程序。它将查询传递给编译器。
***Hive 编译器*** —编译器的目的是解析查询，并对不同的查询块和表达式执行语义分析。它将 HiveQL 语句转换为 MapReduce 作业。
***Hive 执行引擎—*** 优化器生成 map-reduce 任务和 HDFS 任务的 DAG 形式的逻辑计划。最后，执行引擎按照任务的依赖性顺序执行传入的任务。
***Hive 存储和计算—*** 元存储、文件系统和作业客户端等 Hive 服务依次与 Hive 存储通信并执行以下操作。在 Hive 中创建的表的元数据信息存储在 Hive“元存储数据库”中。所有 Hive 实现都需要一个元存储服务，它在其中存储元数据。它是使用关系数据库中的表实现的。表中加载的查询结果和数据将存储在 HDFS 的 Hadoop 集群中。

## **7*。配置单元*中的作业执行**

Hive 中的数据流表现为以下模式:
1。配置单元的接口，如命令行或 Web 用户界面，将查询传递给
驱动程序执行。
2。驱动程序正在与编译器交互以获取计划。(此处计划是指查询
执行)流程及其相关元数据信息采集
3。编译器为要执行的作业创建计划。编译器正在与 Meta
存储进行通信，以获取元数据请求
4。元存储将元数据信息发送回编译器
5。编译器使用建议的计划与驱动程序通信，以执行查询
6。驱动程序向执行引擎发送执行计划

7.执行引擎(EE)作为 Hive 和 Hadoop 之间的桥梁来处理查询。对于 DFS 操作。EE 应该首先联系名称节点，然后再联系数据节点来获取存储在表中的值。EE 将从数据节点获取所需的记录。表
的实际数据只存在于数据节点中。而从名称节点，它只获取查询的元数据信息。
它从与上述查询相关的数据节点收集实际数据
执行引擎(EE)与配置单元中的元存储进行双向通信，以执行 DDL(数据定义语言)操作。这里完成 DDL 操作，如创建、删除和修改表和数据库。元存储将只存储有关数据库名、表名和列名的信息。它将获取与提到的查询相关的数据。执行引擎(EE)依次与 Hadoop 守护进程(如名称节点、数据节点和作业跟踪器)通信，以在 Hadoop 文件系统上执行查询

8.从驱动程序
9 获取结果。将结果发送到执行引擎。一旦将结果从数据节点提取到 EE，它将通过执行引擎将结果发送回与 Hadoop 文件系统及其守护程序保持联系的驱动程序和 UI 或 CLI
Hive。

## **8。蜂巢的不同模式**

根据 Hadoop 中数据节点的大小，Hive 可以在两种模式下运行。
***1。本地模式
2。Map-reduce 模式***
***何时使用本地模式:***
【如果 Hadoop 安装在伪模式下，并且有一个数据节点，我们在此模式下使用 Hive。如果数据大小较小，仅限于单个本地机器，我们可以使用此模式处理本地机器中的较小数据集
***何时使用 Map reduce 模式:***
如果 Hadoop 有多个数据节点，并且数据分布在不同的节点上，我们在此模式下使用 Hive。它将在大量数据集上执行，查询将以并行方式执行，通过该模型可以实现对大型数据集的更好性能处理。