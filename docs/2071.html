<html>
<head>
<title>Machine Learning Optimization Algorithms 1: Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习优化算法1:梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/machine-learning-optimization-algorithms-1-gradient-descent-258dfb5987e1?source=collection_archive---------3-----------------------#2022-03-03">https://medium.com/mlearning-ai/machine-learning-optimization-algorithms-1-gradient-descent-258dfb5987e1?source=collection_archive---------3-----------------------#2022-03-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="aad0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://www.linkedin.com/in/sathya-krishnan-suresh-914763217/" rel="noopener ugc nofollow" target="_blank">萨提亚·克里希南·苏雷什</a>，<a class="ae jc" rel="noopener" href="/@pshunmugapriya">顺穆加布里亚P </a></p><p id="1b0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">简介:</strong> <br/>优化算法是机器学习算法从数字中学习的过程中不可或缺的一部分。有几种优化算法，但最常用的是梯度下降及其变体。在这篇文章中，让我们看看梯度下降(GD)，随机梯度下降(SGD)，小批量梯度下降(MBGD)。</p><p id="d32f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">梯度下降:</strong> <br/>假设你站在一座山顶上(几乎所有人都用这个例子来解释梯度下降，没有更好的例子了)，你想到达一个山谷或地面。你将试图通过感觉脚下地面的坡度来到达目标，你将开始朝着坡度向下的方向朝目标走去。这正是梯度下降的工作原理。</p><p id="8963" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">许多ML算法，如线性回归、逻辑回归等，都使用GD或它的变体。但我们必须明白的重要一点是，优化算法和成本函数是协同工作的。你甚至可以说，成本函数是基于所使用的优化算法或其他方式来选择的。所以为了让GD工作，我们需要一个形状为凸的代价函数。想象一下上面的例子。从山顶到山谷的路径形成一个凸函数。在数学术语中，该谷被称为全局最小值，并且GD算法将达到全局最小值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/6fa42c1aa79fa2eb79eb7d2c9b67533f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*hC_Sp7SCmtkqBM8_DsqOcg.png"/></div></figure><p id="e808" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看GD是如何工作的。GD在每次迭代中计算成本函数的导数。然后将它乘以学习率，学习率是我们希望GD算法在试图达到全局最小值时所采取的步长。因为我们想去梯度的相反方向，所以我们把梯度的负值加到权重上并更新它们。这就是GD在单次迭代中所做的。通过重复迭代，GD将能够通过达到全局最小值来计算正确的权重。</p><p id="90ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率和迭代次数是我们应该小心的两个超参数。学习率是一个重要的超参数。如果选择正确，可以在短时间内达到全局最小值，否则GD将会超调，它将永远无法达到全局最小值。迭代的次数是我们允许模型看到我们的数据的次数。它与学习速度有相互依赖的关系。如果学习率太小，这意味着GD算法仅朝着全局最小值迈出很小的步伐，因此，为了使模型达到全局最小值，历元必须更高。必须尝试学习速率和时期的不同组合，以找到最佳模型。</p><p id="4053" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们来看看使用梯度下降的线性回归的实现。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/af6b7a5540630ca7254af3c17f79bb70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y8JF6-7-IlCJJKd-CsC0Gg.png"/></div></div></figure><p id="2607" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">fit方法中的“for loop”构成了该类的主要部分，因为该部分是查找要素权重的部分。那三条线实现了梯度下降。</p><p id="0cbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">即使保证GD将达到全局最小值，也将花费大量时间，因为在每次迭代期间，计算每个实例的梯度。GD算法也可能需要很长时间来摆脱局部极小值。</p><p id="d642" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机梯度下降:</strong> <br/>顾名思义随机梯度下降(SGD)在梯度下降算法中引入了随机性。这是通过一次只选择一个随机实例进行训练，而不是选择整个训练数据来实现的。SGD的成本函数不会平滑地降低，因为模型每次都是在随机实例上训练的。对于一个实例，成本函数可能减小，而对于下一个实例，成本函数可能增大。因此，当涉及SGD时，成本函数仅平均降低，而在GD中平稳降低。</p><p id="c1a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SGD的主要缺点是它永远无法达到全局最小值。它将达到一个接近全球最小值的点。这是因为它的随机性。通过一次只使用一个实例，减少了到达接近全局最小值的点所需的时间。这种随机性也有助于避免局部极小值。SGD用的很多，因为它支持在线学习。</p><p id="a77e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来看一个用SGD构建的线性回归模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/ab6e192cd7912402c037b132e216d0db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*JQCYxVNR8wKRwbN6ykNu3w.png"/></div></figure><p id="163e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的代码中，我使用了一个固定的学习速率，而专业使用的SGD算法使用的学习速率随着历元数的增加而降低。这样做是为了减少模型的随机性，因为它变得接近全局最小值。</p><p id="db35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">小批量梯度下降:</strong> <br/>小批量梯度下降是梯度下降和随机梯度下降的混合。在小批量GD中，我们随机选择一组特定大小的实例，称为批量大小，我们在这些实例上训练我们的模型进行一次迭代。在接下来的迭代中，选择相同大小的不同实例集，并在这些实例集上训练我们的模型。迷你批量GD两全其美。与SGD相比，它的成本函数平滑地降低，它能够达到非常接近全局最小值的点，最终它比GD更快。小批量GD也在在线学习系统中大量使用。</p><p id="2a25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面给出了使用小批量GD实现线性回归的代码。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jr"><img src="../Images/40ce7d2372f98381f455d4533c99e594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1zop20_lWh40YezSmdCkA.png"/></div></div></figure><p id="e63e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">算法比较:</strong> <br/>我们将看看每个算法如何与<a class="ae jc" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>实现“线性回归”一起执行。</p><p id="b5da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来看看我们的玩具数据集。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/f78332919953d20970f85043597094fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*DFZNYfkS_bng3xi8rbE-Cg.png"/></div></figure><p id="74eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些是学习率为0.01和100个时期的算法获得的权重值。你可以看到小批量GD表现最好，它的值非常接近玩具数据集中的值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/1ada70d3bf7da4f5ca84ac4efc3e0f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*hTDLBuN7OVvw2k0zm1KIIg.png"/></div></figure><p id="3384" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们来看看这些算法发展出来的直线。你可以清楚地看到Mini-Batch GD和scikit-learn的LinearRegression比GD和SGD表现得更好。最终代码可在<a class="ae jc" href="https://github.com/SathyaKrishnan1211/Medium-Article-Code" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ju"><img src="../Images/911e1c35b5ee3a30ccb54370183945fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dvl_VDI4yPPm_Y3xYgep0A.png"/></div></div></figure><p id="7ef8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong> <br/>机器学习中还有很多优化算法，我会试着写一下。希望你喜欢这篇文章。如果你喜欢这篇文章，请订阅并留下评论。</p><div class="jv jw ez fb jx jy"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="jz ab dw"><div class="ka ab kb cl cj kc"><h2 class="bd hi fi z dy kd ea eb ke ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kf l"><h3 class="bd b fi z dy kd ea eb ke ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kg l"><p class="bd b fp z dy kd ea eb ke ed ef dx translated">medium.com</p></div></div><div class="kh l"><div class="ki l kj kk kl kh km jj jy"/></div></div></a></div></div></div>    
</body>
</html>