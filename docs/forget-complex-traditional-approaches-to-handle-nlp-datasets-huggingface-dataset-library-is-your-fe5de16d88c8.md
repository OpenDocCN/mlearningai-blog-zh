# å¿˜è®°å¤æ‚çš„ä¼ ç»Ÿæ–¹æ³•æ¥å¤„ç† NLP æ•°æ®é›†ï¼ŒHuggingFace æ•°æ®é›†åº“æ˜¯æ‚¨çš„æ•‘æ˜Ÿï¼ç¬¬äºŒéƒ¨åˆ†

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-fe5de16d88c8?source=collection_archive---------2----------------------->

# ä½œè€…

**çº³å·´ä¼¦Â·å·´é²é˜¿**

[Git](https://github.com/nabarunbaruaAIML)/[LinkedIn](https://www.linkedin.com/in/nabarun-barua-aiml-engineer/)/[towards data science](/@nabarun.barua)

é˜¿å°”ç¼Â·åº“å§†å·´å…‹æ‹‰

[Git](https://github.com/arjunKumbakkara)/[LinkedIn](https://www.linkedin.com/in/arjunkumbakkara/)/[towards data science](/@arjunkumbakkara)

è¿™æ˜¯å‰ä¸€éƒ¨åˆ†çš„å»¶ç»­ï¼Œå¦‚æœä½ æƒ³çœ‹æˆ‘ä»¬çš„[æ—©äº›æ—¶å€™å…¬å¸ƒçš„æ–‡ä»¶](/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f)ï¼Œè¿™é‡Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æ›´é«˜çº§çš„ä¸œè¥¿ã€‚

åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨:

*   åˆå¹¶æ•°æ®é›†
*   ç¼“å­˜æ•°æ®é›†
*   å’Œäº‘å­˜å‚¨
*   å¦‚ä½•åˆ›å»ºæ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ

# åˆå¹¶æ•°æ®é›†

æœ‰äº›æƒ…å†µä¸‹ï¼Œæ•°æ®ç§‘å­¦å®¶å¯èƒ½éœ€è¦å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆå¹¶æ•°æ®é›†:

## **ä¸²è”**

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœè½´ä¸ºé›¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆå¹¶å…·æœ‰ç›¸åŒåˆ—æ•°å’Œå…±äº«ç›¸åŒåˆ—ç±»å‹çš„ä¸åŒæ•°æ®é›†ã€‚

å¦‚æœè½´æ˜¯ä¸€ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿æ¥ä¸¤ä¸ªæ•°æ®é›†ï¼Œå¦‚æœæ•°æ®é›†ä¸­çš„è¡Œæ•°ç›¸åŒã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­(å–è‡ª huggingface.co çš„ä¾‹å­)

```
from datasets import concatenate_datasets, load_dataset  
bookcorpus = load_dataset("bookcorpus", split="train")     
wiki = load_dataset("wikipedia", "20200501.en", split="train")     wiki = wiki.remove_columns("title")  # only keep the text      assert bookcorpus.features.type == wiki.features.type     bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

## äº¤é”™æ•°æ®é›†

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å°†å‡ ä¸ªæ•°æ®é›†åˆå¹¶åœ¨ä¸€èµ·ï¼Œä»æ¯ä¸ªæ•°æ®é›†ä¸­é€‰æ‹©å¦ä¸€ä¸ªä¾‹å­æ¥åˆ›å»ºæ–°æ•°æ®é›†ã€‚è¿™å°±æ˜¯æ‰€è°“çš„äº¤é”™ã€‚

è¿™å¯ç”¨äºå¸¸è§„æ•°æ®é›†å’Œæµæ•°æ®é›†ä¸­

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æµæ•°æ®é›†(åŒæ ·å¯ä»¥é’ˆå¯¹å¸¸è§„æ•°æ®é›†)è¿›è¡Œæ“ä½œï¼Œå¹¶åœ¨å¯é€‰çš„ä¾‹å­ä¸­ç»™å‡ºæ¦‚ç‡ã€‚å¦‚æœç»™å®šäº†æ¦‚ç‡ï¼Œåˆ™æœ€ç»ˆæ•°æ®é›†æ˜¯åŸºäºç›¸åŒçš„æ¦‚ç‡å½¢æˆçš„ã€‚(huggingface.co çš„ä¾‹å­)

```
from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
```

æœ€ç»ˆæ•°æ®é›†çš„å¤§çº¦ 80%ç”± en_dataset æ„æˆï¼Œ20%ç”± fr_dataset æ„æˆã€‚

# ç¼“å­˜æ•°æ®é›†

ä¸‹è½½æ•°æ®é›†æ—¶ï¼Œå¤„ç†è„šæœ¬å’Œæ•°æ®å­˜å‚¨åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šã€‚ç¼“å­˜å…è®¸ğŸ¤—æ•°æ®é›†ï¼Œä»¥é¿å…æ¯æ¬¡ä½¿ç”¨æ—¶é‡æ–°ä¸‹è½½æˆ–å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚

## ç¼“å­˜ç›®å½•

æˆ‘ä»¬å¯ä»¥æ”¹å˜å½“å‰ç›®å½•çš„é»˜è®¤ç¼“å­˜ç›®å½•ï¼Œå³`~/.cache/huggingface/datasets`ã€‚åªéœ€è®¾ç½®ç¯å¢ƒå˜é‡ã€‚

```
$ export HF_DATASETS_CACHE=â€/path/to/another/directoryâ€
```

ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ä¸åŒçš„å…³é”®å­—ä¸­ä¼ é€’å‚æ•° *cache_dir* æ¥åšåŒæ ·çš„äº‹æƒ…ï¼Œå¦‚ load_datasetã€load_metric &ç­‰ã€‚

ä¾‹å­

```
from datasets import concatenate_datasets, load_datasetbookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20200501.en", split="train",cache_dir="/path/to/another/directory")
```

## ä¸‹è½½æ¨¡å¼

ä¸€æ—¦æ•°æ®é›†è¢«ä¸‹è½½ï¼Œå®ƒå°±ä¼šè¢«ç¼“å­˜ï¼Œå› æ­¤å½“æˆ‘ä»¬åŠ è½½æ•°æ®é›†æ—¶ï¼Œå®ƒä¸æ˜¯ä»æºä¸‹è½½ï¼Œè€Œæ˜¯ä»ç¼“å­˜åŠ è½½ã€‚

ç°åœ¨ï¼Œå¦‚æœæ•°æ®é›†ä¸­æœ‰ä»»ä½•å˜åŒ–ï¼Œå¹¶ä¸”å¦‚æœæˆ‘ä»¬æƒ³è¦ä»æºåŠ è½½æ•°æ®é›†æœªæ”¹å˜çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä½¿ç”¨å‚æ•°`download_mode`ã€‚

ä¾‹å­

```
from datasets import concatenate_datasets, load_datasetbookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20200501.en", split="train",download_mode='force_redownload')
```

## æ¸…ç†ç¼“å­˜æ–‡ä»¶

å¦‚æœéœ€è¦æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®Œæˆ

```
# Below function just clears the cache of Dataset
ds.cleanup_cache_files()
```

## å¯ç”¨æˆ–ç¦ç”¨ç¼“å­˜

å¯èƒ½ä¼šå‡ºç°æˆ‘ä»¬ä¸æƒ³ç¼“å­˜çš„æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æˆ–å…¨å±€ç¦ç”¨ç¼“å­˜ã€‚

**æœ¬åœ°:**å¦‚æœæˆ‘ä»¬åœ¨æœ¬åœ°ä½¿ç”¨ä¸€ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œå®ƒå°†è‡ªåŠ¨é‡æ–°åŠ è½½æ‚¨ä¹‹å‰åº”ç”¨åˆ°æ•°æ®é›†çš„ä»»ä½•è½¬æ¢ã€‚æˆ‘ä»¬å¯ä»¥åœ¨`datasets.Dataset.map()`ä¸­ä½¿ç”¨å‚æ•°`load_from_cache=False`ç¦ç”¨ç¼“å­˜ã€‚

ç¤ºä¾‹:

```
updated_dataset = small_dataset.map(tokenizer_function, load_from_cache=False)
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼ŒğŸ¤—æ•°æ®é›†å°†å¯¹æ•´ä¸ªæ•°æ®é›†å†æ¬¡æ‰§è¡Œå‡½æ•°`tokenizer_function`,è€Œä¸æ˜¯ä»å…ˆå‰çš„çŠ¶æ€åŠ è½½æ•°æ®é›†ã€‚

**å…¨å±€:**å¦‚æœæˆ‘ä»¬æƒ³è¦å…¨å±€ç¦ç”¨ç¼“å­˜ï¼Œé‚£ä¹ˆéœ€è¦ä½¿ç”¨`datasets.set_caching_enabled()`ä¸ºå…¨å±€ç¦ç”¨ç¼“å­˜è®¾ç½®ä»¥ä¸‹å‚æ•°:

```
from datasets import set_caching_enabled
set_caching_enabled(False)
```

å½“æ‚¨ç¦ç”¨ç¼“å­˜æ—¶ï¼ŒğŸ¤—å¯¹æ•°æ®é›†åº”ç”¨å˜æ¢æ—¶ï¼Œæ•°æ®é›†å°†ä¸å†é‡æ–°åŠ è½½ç¼“å­˜æ–‡ä»¶ã€‚æ‚¨åœ¨æ•°æ®é›†ä¸Šåº”ç”¨ä»»ä½•è½¬æ¢éƒ½éœ€è¦é‡æ–°åº”ç”¨ã€‚

# äº‘å­˜å‚¨

Huggingface æ•°æ®é›†å¯ä»¥å­˜å‚¨åˆ°æµè¡Œçš„äº‘å­˜å‚¨ä¸­ã€‚Hugginface æ•°æ®é›†å…·æœ‰å†…ç½®åŠŸèƒ½æ¥æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚

å®ƒæ”¯æŒçš„äº‘åˆ—è¡¨ä»¥åŠéœ€è¦å®‰è£…çš„æ–‡ä»¶ç³»ç»Ÿï¼Œä»¥ä¾¿åœ¨åŠ è½½æ•°æ®é›†æ—¶ç›´æ¥ä½¿ç”¨(è¡¨æ ¼å–è‡ª Huggingface.co):

![](img/24439a3e7debbaf55a60917eceb0c007.png)

Cloud Table

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å°è¯•å±•ç¤ºå¦‚ä½•ä½¿ç”¨ s3fs å°†æ•°æ®é›†åŠ è½½å’Œä¿å­˜åˆ° S3 å­˜å‚¨æ¡¶ã€‚å¯¹äºå…¶ä»–äº‘ï¼Œè¯·å‚è§æ–‡æ¡£ã€‚å°½ç®¡å¯ä»¥ç±»ä¼¼åœ°ä½¿ç”¨å…¶ä»–äº‘æ–‡ä»¶ç³»ç»Ÿå®ç°ã€‚

é¦–å…ˆå®‰è£…æ•°æ®é›†çš„ S3 ä¾èµ–é¡¹ã€‚

```
pip install datasets[s3]
```

**åŠ è½½æ•°æ®é›†:**ç°åœ¨ï¼Œé€šè¿‡è¾“å…¥æ‚¨çš„ aws_access_key_id å’Œ aws_secret_access_keyï¼Œä»ç§æœ‰ S3 å­˜å‚¨æ¡¶è®¿é—®æ•°æ®é›†

```
import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)# load encoded_dataset to from s3 bucket
dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)
```

**ä¿å­˜æ•°æ®é›†:**å¤„ç†å®Œæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`datasets.Dataset.save_to_disk()`å°†å…¶ä¿å­˜åˆ° S3:

```
import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)# saves encoded_dataset to your s3 bucket
encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)
```

# å¦‚ä½•åˆ›å»ºæ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ

æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ˜¯ç”¨äºé—®ç­”ç³»ç»Ÿç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ•°æ®é›†çš„ä¸€ä¸ªå…·ä½“ä¾‹å­ã€‚æœ¬æ–‡æ¡£å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸ºæ‚¨çš„æ•°æ®é›†æ„å»ºç´¢å¼•æœç´¢ï¼Œä»è€Œå…è®¸æ‚¨ä»æ•°æ®é›†ä¸­æœç´¢é¡¹ç›®ã€‚

## è„¸ä¹¦äººå·¥æ™ºèƒ½ç›¸ä¼¼æ€§æœç´¢

æ•°æ®é›†æœ‰ä¸€ç§æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºåµŒå…¥åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œè¿™é‡Œé¦–å…ˆå°†é•¿æ®µè½è½¬æ¢æˆå•ä¸ª a åµŒå…¥ï¼Œè¯¥åµŒå…¥ç¨åå¯ä»¥ç”¨ä½œæ£€ç´¢ç³»ç»Ÿã€‚åœ¨å°†æ®µè½è½¬åŒ–ä¸ºåµŒå…¥ä¹‹åï¼Œæˆ‘ä»¬å°†é—®é¢˜è½¬åŒ–ä¸ºåµŒå…¥ã€‚ç°åœ¨æœ‰äº†æ•°æ®é›†çš„ FAISS ç´¢å¼•æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒè¿™ä¸¤ä¸ªåµŒå…¥ï¼Œä»æ•°æ®é›†ä¸­å¾—åˆ°æœ€ç›¸ä¼¼çš„æ®µè½ã€‚

![](img/ac0b59e6e5302d5aeff8310f12e76733.png)

Credits: [https://huggingface.co](https://huggingface.co/)

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¿™æ ·çš„æ¨¡å‹å‘½åä¸º DPR(å¯†é›†æ®µè½æ£€ç´¢)ã€‚å–è‡ª Huggingface æ•°æ®é›†æ–‡æ¡£çš„ç¤ºä¾‹ã€‚è¯·éšæ„ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å‹ï¼Œå¦‚å¥å­å˜å½¢é‡‘åˆšç­‰ã€‚

**ç¬¬ä¸€æ­¥:**åŠ è½½ä¸Šä¸‹æ–‡ç¼–ç å™¨æ¨¡å‹&åˆ†è¯å™¨ã€‚

```
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
import torch
torch.set_grad_enabled(False)
ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
```

**æ­¥éª¤ 2:** åŠ è½½æ•°æ®é›†å¹¶è·å¾—åµŒå…¥

æ­£å¦‚æˆ‘ä»¬å‰é¢æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ¯ä¸ªæ¡ç›®è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ã€‚

```
from datasets import load_dataset
ds = load_dataset('crime_and_punish', split='train[:100]')
ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example["line"], return_tensors="pt"))[0][0].numpy()})
```

**ç¬¬ä¸‰æ­¥:**å‘ FAISS æœç´¢ç´¢å¼•æ·»åŠ åµŒå…¥ã€‚

ç°åœ¨ä½¿ç”¨ FAISS è¿›è¡Œæœ‰æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`datasets.Dataset.add_faiss_index()`åˆ›å»ºç´¢å¼•

```
ds_with_embeddings.add_faiss_index(column='embeddings')
```

**æ­¥éª¤ 4:** ä½¿ç”¨ FAISS æœç´¢ç´¢å¼•è¿›è¡Œæœç´¢æŸ¥è¯¢

ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åµŒå…¥ç´¢å¼•æŸ¥è¯¢æ‚¨çš„æ•°æ®é›†ã€‚åŠ è½½ DPR é—®é¢˜ç¼–ç å™¨ï¼Œç”¨`datasets.Dataset.get_nearest_examples()`æœç´¢é—®é¢˜

```
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")question = "Is it serious ?"
question_embedding = q_encoder(**q_tokenizer(question, return_tensors="pt"))[0][0].numpy()
scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)
retrieved_examples["line"][0]
```

**ç¬¬äº”æ­¥:**ä¿å­˜ FAISS æŒ‡æ ‡

æˆ‘ä»¬å¯ä»¥ç”¨`datasets.Dataset.save_faiss_index()`æŠŠç´¢å¼•ä¿å­˜åœ¨ç£ç›˜ä¸Š

```
ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')
```

**æ­¥éª¤ 6:** åŠ è½½ FAISS æŒ‡æ•°

ç°åœ¨å¯ä»¥ä»ç£ç›˜åŠ è½½ç´¢å¼•äº†`datasets.Dataset.load_faiss_index()`

```
ds = load_dataset('crime_and_punish', split='train[:100]')
ds.load_faiss_index('embeddings', 'my_index.faiss')
```

## å¼¹æ€§æœç´¢

ä¸ FAISS ä¸åŒï¼ŒElasticSearch åŸºäºæœç´¢ä¸Šä¸‹æ–‡ä¸­çš„ç²¾ç¡®åŒ¹é…æ¥æ£€ç´¢æ–‡æ¡£ã€‚

æ›´å¤šä¿¡æ¯è¯·å‚è§[å®‰è£…&é…ç½®æŒ‡å—](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html)

å¼¹æ€§æœç´¢ä¸ FAISS æœç´¢éå¸¸ç›¸ä¼¼ã€‚ç¤ºä¾‹å–è‡ªæ ‡å‡† Huggingface æ•°æ®é›†æ–‡æ¡£ã€‚

**æ­¥éª¤ 1:** åŠ è½½æ•°æ®é›†

```
from datasets import load_dataset
squad = load_dataset('squad', split='validation')
```

**æ­¥éª¤ 2:** å‘æ•°æ®é›†æ·»åŠ å¼¹æ€§æœç´¢

```
squad.add_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
```

**ç¬¬ä¸‰æ­¥:**æ‰§è¡ŒæŸ¥è¯¢æœç´¢

```
query = "machine"
scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
retrieved_examples["title"][0]
```

**æ­¥éª¤ 4:** å¦‚æœæä¾›äº†å§“åï¼Œåˆ™åŠ è½½å¼¹æ€§æœç´¢ç´¢å¼•

```
from datasets import load_dataset
squad = load_dataset('squad', split='validation')
squad.load_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")
query = "machine"
scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)
```

æ­£å¦‚æ‰€æ‰¿è¯ºçš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ‰©å±•äº†ä¹‹å‰çš„[ä½¿ç”¨ HugginFace æ•°æ®é›†åº“çš„ä¸€ç«™å¼æŒ‡å—](/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f)

å¦‚æœä½ å–œæ¬¢è¿™ä¸ªåšå®¢ï¼Œè¯·è¡¨è¾¾ä½ çš„çˆ±ï¼Œç»™æˆ‘ä»¬ä¸€ä¸ªå¤§æ‹‡æŒ‡ï¼Œç»™æˆ‘ä»¬åŠ æ˜Ÿï¼Œå¦‚æœä¸å–œæ¬¢ï¼Œè¯·åœ¨è¯„è®ºåŒºç»™æˆ‘ä»¬ä¸€ä¸ªåé¦ˆã€‚å¸Œæœ›ä½ ä¼šåœ¨å›¾ä¹¦é¦†ç©å¾—å¼€å¿ƒï¼

ä¸ºäº†åˆä½œï¼Œå¸®åŠ©å’Œä¸€èµ·å­¦ä¹ -

åŠ å…¥æˆ‘ä»¬çš„ä¸å’Œè°æœåŠ¡å™¨:[https://discord.gg/Z7Kx96CYGJ](https://discord.gg/Z7Kx96CYGJ)

ä¸€è·¯å¹³å®‰ï¼

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) 

ğŸ”µ [**æˆä¸ºä½œå®¶**](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)