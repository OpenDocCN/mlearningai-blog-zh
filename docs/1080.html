<html>
<head>
<title>Inter-Class Clustering of Text Data Using Dimensionality Reduction and BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于降维和BERT的文本数据类间聚类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/inter-class-clustering-of-text-data-using-dimensionality-reduction-and-bert-390a5f9954b8?source=collection_archive---------0-----------------------#2021-09-27">https://medium.com/mlearning-ai/inter-class-clustering-of-text-data-using-dimensionality-reduction-and-bert-390a5f9954b8?source=collection_archive---------0-----------------------#2021-09-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/17528f08e6afa8f9c2c2687a224b5a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKC_Vd2iV4Ze6iO_itiAow.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd it">Source:</strong> <em class="iu">A Bibliometric Analysis of the Landscape of Cancer Rehabilitation Research (</em>Stout, N. L. et al., 2018<em class="iu">)</em></figcaption></figure><p id="74da" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">主题建模是一种有吸引力的自然语言处理工具，用于发现文本集合的模式、方面和特征。在生物文本挖掘或医学和临床信息学领域，它可用于了解全球疫情期间公众的看法。</p><p id="7294" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">这些年来，我对主题建模进行了一点点修补，但经常发现自己对与方法相关的常见问题感到沮丧。为了得到满意的结果，通常需要花相当多的时间对数据进行预处理。例如停用词的移除、词汇表的清理和标准化，甚至是移除封闭类的词性词。最重要的是，大多数主题建模方法受益于大型数据集，输出通常是每个主题的单个单词的集合，这些单词并不总是容易解释。某些方法仅适用于较长的文本，并且通常需要不同的超参数，例如主题的数量。</p><p id="622b" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">主题建模也可以看作是一种聚类的形式，寻找意义相似的词，并将它们放在一个主题中。这种形式的无监督学习，结合语言模型的日益流行，已经带来了更新的方法，如<a class="ae jt" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank"> Top2Vec </a> ⁴.本文概述了我自己对该方法的实现，虽然它没有所有令人惊叹的功能作为灵感，但我确实提出了一个小的改变，以最小化超参数的选择。</p><h1 id="a11f" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">任务和应用</h1><p id="cd6f" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">让我们想象一个场景，其中我们有一个文本数据集合，属于几个不同的类/标签。虽然知道样本属于哪个类，但我们仍然希望发现每个类中的聚类。</p><p id="01c6" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">这不仅有助于查找数据类内部的差异，也有助于查找数据类之间的差异。让我们想象一个医疗场景，每个类别属于一种疾病的不同严重程度，收集的数据是患者的经验丰富的文本投诉。换句话说，所有课程的主题都是一样的，我们想要发现它们之间微妙而有意义的差异。这在尝试描述或发现某个主题的新事物(如疾病)时特别有用。</p><p id="ff6a" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">或者我们收集了不同产品类别的顾客评论。为每个产品(类别)形成集群有助于发现哪些产品做得好，哪些产品需要更多关注，以及营销需要如何调整。</p><p id="3316" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">只要有一点创意，用例就有很多，而且应用广泛，而实际的实现并不复杂！</p><p id="58f1" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">所以让我们开始吧。当然，所有用于此的代码和文件都可以在下面的<a class="ae jt" href="https://github.com/boorism/medium-clustering.git" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p><h1 id="462b" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">拟议管道</h1><p id="396a" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">所提出的方法的总体思想紧密遵循用于主题建模和语义搜索的<a class="ae jt" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank"> Top2Vec </a>算法，我们将遵循的流水线可以在图1中看到。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kw"><img src="../Images/7e2e5d32ecd9bd2629d98478e79d89fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBQweNWuWDw0zBfFNEfolA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Figure 1: The proposed clustering pipeline</figcaption></figure><p id="dcdf" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">对于每个单词、句子或段落(取决于我们的数据)，我们使用<a class="ae jt" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>库计算单词嵌入。这个模块的好处在于它提供了对大量语言模型的访问。</p><p id="72eb" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">典型地，结果嵌入是高维的，在BERT语言模型的情况下超过760。在送入UMAP维数约简方法之前，用PCA将这些数据约简到某个期望的方差。</p><p id="72e6" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">UMAP(就像t-SNE一样)因严重依赖used⁵.的超参数集而臭名昭著结合这些参数的非平凡直观性，到较低维度的适当映射通常需要反复试验。此外，对于更加统一的数据集，决定“正确”的投影可能过于主观，并且会在data⁶.中引入不存在的聚类</p><p id="0223" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">而不是试图比较数百个情节(我自己也这么做过，没那么好玩..)，我们迭代了许多不同的超参数组合。在每个组合中，我们使用HDBSCAN对减少的嵌入进行聚类。对于每个聚类，我们提取最常见的类别标签和相关联的句子，并将它们存储在字典中。该字典跟踪每次运行中找到该特定聚类的次数。</p><p id="8a34" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">所提出的方法的假设是,“最强”的聚类是在参数组合中出现最频繁的聚类，并且受UMAP投影的微小变化的影响较小。</p><h1 id="d8f1" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">t-SNE VS UMAP</h1><p id="c7c7" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">快速说明一下为什么我认为UMAP比SNE更合适。最初在设计这个方法的时候，我选择了t-SNE作为我的降维方法，主要是因为这是一个我经常遇到的技术。</p><p id="13e5" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">虽然SNE霸王龙确实产生了令人满意的结果，但它总能找到比UMAP更少的集群。在寻找解释时，我看到了这篇<a class="ae jt" href="https://pair-code.github.io/understanding-umap/" rel="noopener ugc nofollow" target="_blank">精彩的文章</a>，它用简单易懂的术语比较了这两种方法。虽然这两种方法都深受对方的启发，但看起来最大的不同是在局部和全局结构上的平衡(图2)。这两种方法都能找到有意义的聚类，但是UMAP倾向于产生更清晰分离的聚类，同时仍然保持较高的全局结构(属于相似类别的聚类倾向于共存)。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/6c1f371bc131b8bb052adc40bad77b10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F21oYACRDCwa_r8UBuerQg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Figure 2: UMAP &amp; t-SNE cluster formation comparison</figcaption></figure><h1 id="c2db" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">数据</h1><p id="ba74" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">对于我们的聚类，我们将利用奇妙的Kaggle及其众多公开可用的数据集。当寻找一个数据集使用时，我想要的东西会对方法有一点挑战，同时仍然有一些实际用途。</p><p id="4b56" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">我选定了<a class="ae jt" href="https://www.kaggle.com/shivanandmn/multilabel-classification-dataset?select=train.csv" rel="noopener ugc nofollow" target="_blank">下面的研究论文摘要和标题的集合</a>，跨越了六个不同的主题。这里的标签指的是不同的科学，然而它们中的大多数在本质上都是跨学科的，所以我们期望标题之间有一些重叠。此外，所使用的语言很可能是非常具体的行话和非常研究导向。看看这些科普领域之间的主要话题和研究趋势会是很有趣的。</p><p id="45a3" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">当加载数据时，我们马上注意到标签的编码需要改变，最好是从0开始的唯一整数。现在，我们还更改了列名，将我们想要集群的列(标题)设置为“text”。似乎有些文章属于一个以上的类别，这些样本也被删除。</p><p id="c4b9" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">检查最长标题和摘要的长度是一个好主意，因为语言模型对标记的数量有限制。我们将对数据执行的唯一预处理是删除换行符(" \n ")。</p><pre class="kx ky kz la fd lc ld le lf aw lg bi"><span id="b537" class="lh jv hh ld b fi li lj l lk ll"><strong class="ld hi">#Preparing dataset 2: Research paper abstracts and topic types<br/>#Loading in the abstracts dataset<br/></strong>abstracts = pd.read_csv("datasets/science.csv")</span><span id="e85a" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Check data encoding</strong><br/>print(abstracts.head())</span><span id="e9db" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Need to change the encodings. A way can be to first concat the binary indicators into a list/string of numbers which then is converted to the label<br/></strong>abstracts['labels'] = abstracts[abstracts.columns[3:]].apply(lambda x: "".join(x.astype(str)), axis = 1)</span><span id="1211" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Check class balance</strong><br/>print(abstracts.labels.value_counts())</span><span id="4649" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Seems like there are cases where entries belong to multiple classes, remove those and just keep single members. Combine with mapping dict instead of making new list. A bit ugly, change later.. 0 = Physics, 1 = CS, 2 = Maths, 3 = Stats, 4 = Quantitative Biology, 5 = Quantitative Finance<br/></strong>labels_mapping = {"010000" : 0, "100000" : 1, "001000" : 2, "000100" : 3, "000010" : 4, "000001" : 5}</span><span id="ee7b" class="lh jv hh ld b fi lm lj l lk ll">abstracts = abstracts[abstracts.labels.isin(list(labels_mapping.keys()))]<br/>abstracts.reset_index(drop=True, inplace= True)<br/>abstracts.labels = abstracts.labels.apply(lambda x: labels_mapping[x])</span><span id="c835" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Rename</strong><br/>abstracts.rename(columns={"ABSTRACT": "text1"}, inplace= True)<br/>abstracts.rename(columns={"TITLE": "text"}, inplace= True)</span><span id="c3d3" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Only preprocessing can be removing the new-line character (\n)</strong><br/>abstracts.text = abstracts.text.apply(lambda x: x.replace("\n", " "))<br/>abstracts.text1 = abstracts.text1.apply(lambda x: x.replace("\n", " "))</span><span id="8520" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Lets check again the longest title entries:</strong><br/>lengths_title = abstracts.text.str.len()<br/>argmax = np.where(lengths_title == lengths_title.max())[0]</span><span id="f9ea" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Check length of longest title</strong><br/>print(abstracts.text.iloc[argmax].to_numpy().ravel().tolist())<br/>print(len(abstracts.text.iloc[argmax].to_numpy().ravel().tolist()[0].split()))</span><span id="dd5b" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Check length of longest abstract</strong><br/>lengths_abstract = abstracts.text1.str.len()<br/>argmax = np.where(lengths_abstract == lengths_abstract.max())[0]</span><span id="db4f" class="lh jv hh ld b fi lm lj l lk ll">print(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist())<br/>print(len(abstracts.text1.iloc[argmax].to_numpy().ravel().tolist()[0].split()))</span><span id="2779" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Finally lets keep only the columns we are interested in further</strong><br/>abstracts = abstracts[['text', 'text1', 'labels']]</span></pre><h1 id="84e8" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">获取嵌入</h1><p id="a659" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">既然数据是我们满意的形式，我们可以将文本转换成相应的嵌入。如前所述，我们使用了<a class="ae jt" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">语句转换器</a>模块，并选择了“<em class="ln"> all-mpnet-base-v2 </em>语言模型。该库也支持使用其他模型，这取决于您的任务或所使用的语言。这个想法是创建两层(模型)，一层用于获得单词嵌入，另一层将它们汇集在一起形成句子嵌入。出于我们的目的，我们从每个班级中抽取500个样本(如果可能的话)。</p><p id="a52b" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">由于这个过程需要一些时间，我们还定义了两个助手函数，用于将对象作为“pickles”保存和加载。以这种方式保存对象的好处是原始变量类型被保留下来，以后可以在不转换它的情况下进行处理。</p><pre class="kx ky kz la fd lc ld le lf aw lg bi"><span id="3b9c" class="lh jv hh ld b fi li lj l lk ll"><strong class="ld hi">#Helper functions for saving/loading pickle objects</strong></span><span id="1c93" class="lh jv hh ld b fi lm lj l lk ll">def save_obj(obj, name ):<br/>   with open('datasets/'+ name + '.pkl', 'wb') as f:<br/>      pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)</span><span id="89a5" class="lh jv hh ld b fi lm lj l lk ll">def load_obj(name ):<br/>   with open('datasets/' + name + '.pkl', 'rb') as f:<br/>      return pickle.load(f)</span></pre><p id="bdae" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">获取嵌入的函数:</p><pre class="kx ky kz la fd lc ld le lf aw lg bi"><span id="c1b6" class="lh jv hh ld b fi li lj l lk ll"><strong class="ld hi">#getting sentence embeddings for all the separated questions</strong><br/>def get_sentence_embeddings(dataset):</span><span id="85a9" class="lh jv hh ld b fi lm lj l lk ll">   word_embedding_model = models.Transformer('sentence-transformers/all-mpnet-base-v2', max_seq_length=384)<br/>   pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())</span><span id="ef97" class="lh jv hh ld b fi lm lj l lk ll">   model = SentenceTransformer(modules=[word_embedding_model, pooling_model])</span><span id="49a3" class="lh jv hh ld b fi lm lj l lk ll">   <strong class="ld hi">#Store the embeddings in a list</strong><br/>   sentence_embeddings = []</span><span id="117d" class="lh jv hh ld b fi lm lj l lk ll">  <strong class="ld hi"> #Go through the data and get the sentence embeddings</strong><br/>   for index in tqdm(range(len(dataset))):<br/>      sentence = dataset.text[index]<br/>      embedding = model.encode(sentence)<br/>      sentence_embeddings.append(embedding)</span><span id="9598" class="lh jv hh ld b fi lm lj l lk ll">   dataset['embeddings'] = sentence_embeddings</span><span id="bf75" class="lh jv hh ld b fi lm lj l lk ll">   return dataset</span></pre><p id="b154" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">取500个样本，嵌入并保存以备后用…</p><pre class="kx ky kz la fd lc ld le lf aw lg bi"><span id="9e83" class="lh jv hh ld b fi li lj l lk ll"><strong class="ld hi">#Getting embeddings as well for the abstract titles</strong><br/>abstracts_subset = abstracts.groupby("labels").head(500)<br/>abstracts_subset.reset_index(drop = True, inplace = True)</span><span id="2aa1" class="lh jv hh ld b fi lm lj l lk ll">abstract_embeddings = get_sentence_embeddings(abstracts_subset)<br/>save_obj(abstract_embeddings, "abstract_embeddings")</span></pre><p id="86bb" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">最后，在运行UMAP和HDBSCAN之前，我们使用PCA来降低嵌入的维数并提高性能。在解释了75%的差异后，我们已经降到了133个维度。</p><pre class="kx ky kz la fd lc ld le lf aw lg bi"><span id="0f27" class="lh jv hh ld b fi li lj l lk ll"><strong class="ld hi">#Reduce the data using PCA</strong><br/>pca = PCA(.75).fit(abstract_embeddings.embeddings.to_list())</span><span id="235d" class="lh jv hh ld b fi lm lj l lk ll">embeddings_pca_transformed = pca.transform(abstract_embeddings.embeddings.to_list())</span><span id="3560" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Define new column for new pca reduced embeddings</strong><br/>abstract_embeddings['pca_embeddings'] = ""</span><span id="b898" class="lh jv hh ld b fi lm lj l lk ll"><strong class="ld hi">#Add new embeddings to the dataframe</strong><br/>for index in range(len(abstract_embeddings)):</span><span id="1ef8" class="lh jv hh ld b fi lm lj l lk ll">   abstract_embeddings.pca_embeddings[index] = embeddings_pca_transformed[index]</span><span id="9003" class="lh jv hh ld b fi lm lj l lk ll">print(len(abstract_embeddings.pca_embeddings[0]))</span></pre><h1 id="c894" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">UMAP和HDBSCAN聚类</h1><p id="054c" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">现在我们到了管道中最有趣的部分。我不会在这里粘贴完整的函数，因为它们可能会占用太多的空间(听起来我需要编写更简洁的代码)，但会解释一般的过程和步骤。确保您检查了<a class="ae jt" href="https://github.com/boorism/medium-clustering/blob/main/clustering.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>中的全部代码，以便您可以将它们应用到您自己的数据中！</p><p id="38ed" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">PCA简化嵌入现在被馈送到“<strong class="ix hi"> umap_reduce() </strong>”函数，该函数目前具有一些最重要的umap超参数的预定义列表。使用每组超参数，数据被减少并映射到较低的维度，然后用于聚类("<strong class="ix hi"> hdb_clustering() </strong>")。</p><p id="437e" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">HDBSCAN的好处以及使用它的动机之一是只需要一个超参数，即最小集群大小。在这种情况下，在运行实际聚类之前，我们通过尝试min_size的范围并找到聚类概率低于0.05的点最少的情况来计算最佳聚类大小("<strong class="ix hi"> get_optimum_min_size() </strong>")。</p><p id="de58" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">对于每个HDBSCAN，我们还打印一个聚类图(图3 ),以确保没有发生任何奇怪的事情，在返回带有聚类标签和概率的数据之前，我们过滤掉标签为-1(没有分配聚类)和概率低于0.8的点。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/3880906f9f03e44fd2ae52570663b1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*NKps2-ICCmeXJNTfkX0EHg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Figure 3: An example plot of the clustering, with the UMAP parameters in the title</figcaption></figure><p id="4526" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">接下来，我们通过找到最常见的标签和相关联的文本样本来分析每个聚类("<strong class="ix hi"> analyze_clusters() </strong>")，这些标签和相关联的文本样本存储在临时字典中。然后迭代临时字典，发现的聚类连同它们在UMAP超参数迭代中的频率一起存储在另一个更“全局”的字典中。</p><p id="ecf4" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">这个过程相当复杂，可能有点复杂(这是我将来要考虑的事情)，但它是可行的，最终我们得到一个字典列表的对象，列表中的字典索引对应于类标签号(因此需要从0开始标记数据)..).这整个方法确实需要一些时间，在我那台有这么多超级参数的破旧MacBook上大约需要一个小时。</p><h1 id="a14a" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">最终输出</strong></h1><p id="572a" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">我们列表中的字典很好，但是其中的聚类不是根据它们的频率排序的。我们仍然希望遵循这样的假设，即在多次UMAP迭代中更频繁发现的聚类也更多地出现在数据中。</p><p id="15f8" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">所以最后一步是对集群进行排序，并保存到一个CSV文件中供我们查看("<strong class="ix hi"> order_clusters() </strong>")。在这样做的同时，我们还删除了只有一个成员和单一发现频率的集群。</p><p id="efb2" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">很高兴看到这个方法确实有效。我们不仅没有对我们的数据进行预处理、超参数选择或探索性分析，我们还给它提供了相对少量的数据点用于无监督学习标准。此外，该方法能够处理不同长度的文本，而不仅仅是输出难以解释的单词集合。下面的表1和表2显示了每个标签的一些更常见的(有点“精选的”)清晰形成的簇的例子。</p><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/9e25be9b276d0f54d8c6613f0ccba8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ukhp0ekoOLzbx8VV1tbIA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Table 1: Cluster examples for first three labels</figcaption></figure><figure class="kx ky kz la fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/d7eb1f24c5c531b3dc6e6338718089db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_k2GOVTYBHjTiysjHAGsuQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Table 2: Cluster examples for last three labels</figcaption></figure><h1 id="5650" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论和未来工作</h1><p id="5eb8" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">这里展示的工作希望向你展示语言模型和维度缩减技术可以实现的可能性和很酷的事情。我相信这里介绍的东西有广泛的适用性，可以让公司对不断增长的数据收集有有用的见解。</p><p id="f190" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">与典型的主题建模技术相比，所提出的方法的一些优点是:</p><ul class=""><li id="611f" class="lq lr hh ix b iy iz jc jd jg ls jk lt jo lu js lv lw lx ly bi translated">不需要预处理:如停用词或特定的位置标签删除</li><li id="7ed1" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">不需要超参数调整或优化</li><li id="b9fd" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">处理不同程度的句子</li><li id="953b" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">输出更容易解释，而不是单个单词的集合</li><li id="5bfc" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">可以处理小型数据集</li></ul><p id="eee7" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">当然也有一些缺点，这种方法可以在未来的迭代中得到很大的改进！</p><ul class=""><li id="ecb6" class="lq lr hh ix b iy iz jc jd jg ls jk lt jo lu js lv lw lx ly bi translated">不是最快的男孩(但也不是其他一些方法，如SeaNMF⁷从经验)</li><li id="1a86" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">应该合并的一些已发现的集群之间存在大量重叠</li><li id="d5ca" class="lq lr hh ix b iy lz jc ma jg mb jk mc jo md js lv lw lx ly bi translated">缺少有用的附加功能</li></ul><h1 id="2a7a" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结束语</h1><p id="c803" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">我希望你喜欢我的第一篇关于Medium的文章，并设法在你自己的项目中找到这个管道的一些用途！虽然这里介绍的方法比较不同的类，但是同样的思想也适用于单个类。也可以随意试验不同的或更多的UMAP超参数，因为这可以提高性能，并在数据中找到更多隐藏的主题。</p><p id="8322" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">如果您有任何意见、反馈或问题，也请随时联系我们。我非常乐于讨论和学习新的东西！</p><h1 id="ec86" class="ju jv hh bd it jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><em class="iu">参考文献</em></h1><p id="2d2b" class="pw-post-body-paragraph iv iw hh ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js ha bi translated">[1]b . Marinov，Spenader，j .，&amp; Caselli，T. (2020年12月)。早期疫情荷兰新冠肺炎推特社区的话题和情感发展。</p><p id="beec" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[2]宋M，金SY (2013)通过挖掘全文集合检测生物信息学的知识结构。</p><p id="13a9" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[3]黄Z，董伟，纪立，甘C，陆X等(2014)利用概率主题模型从事件日志中发现临床路径模式</p><p id="0546" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[4]安杰洛夫特区(2020年)。Top2vec:主题的分布式表示。</p><p id="3ac7" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[5]胡，q .，&amp;格林，C. S. (2018)。参数调整是通过单细胞RNA转录组的深度变分自动编码器降维的关键部分。</p><p id="c2e4" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[6]“K-means对t-SNE的输出进行聚类”。<em class="ln">交叉验证</em>。检索到2018–04–16。</p><p id="5b54" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">[7]施，t .，康，k .，周，j .，，雷迪，C. K. (2018年4月)。基于非负矩阵分解的短文本主题建模</p></div></div>    
</body>
</html>