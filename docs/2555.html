<html>
<head>
<title>Introduction to Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习简介</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/deep-learning-and-its-concepts-30aa243eb14?source=collection_archive---------9-----------------------#2022-05-18">https://medium.com/mlearning-ai/deep-learning-and-its-concepts-30aa243eb14?source=collection_archive---------9-----------------------#2022-05-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="5b0c" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">计算机视觉、深度学习面试问题、神经网络、机器学习、Pytorch、Tensorflow</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/102c26b159f449ddc653dc6b2808b621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4PWtN5incuN4I-w9OZFjkg.jpeg"/></div></div></figure><p id="7f59" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">深度学习是人工神经网络的一个更容易接近的名字。深度学习中的“深”是指网络的深度。人工神经网络可以非常浅。</p><p id="7c34" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">神经网络受到大脑皮层结构的启发。最基本的是感知器，生物神经元的数学表示。就像大脑皮层一样，可以有几层相互连接的感知器。</p><p id="fc53" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">第一层是输入层。这一层中的每个节点接受一个输入，然后将其输出作为输入传递给下一层中的每个节点。同一层中的节点之间通常没有连接，最后一层产生输出。</p><p id="d3d1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们称中间部分为隐藏层。这些神经元与外界没有联系(例如输入或输出)，只被前一层的节点激活。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ke"><img src="../Images/27b9c00e052f3a2760075475c18f702a.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*S74phZBWpV_Hkwn9lOjjnw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Neural Network Diagram</figcaption></figure><h2 id="9a2a" class="kj kk hh bd kl km kn ko kp kq kr ks kt jr ku kv kw jv kx ky kz jz la lb lc ld bi translated"><strong class="ak">为什么深度学习比机器学习好？</strong></h2><p id="c72e" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">传统的机器学习算法解决了我们的许多案例，但在处理高维数据时，它们并不有用，因为在高维数据中我们有大量的输入和输出。例如，在手写识别的情况下，我们有大量的输入，其中我们将有与不同类型的手写相关联的不同类型的输入</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lj"><img src="../Images/57b13d3d985d54242c8d569fb5ed61b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XuDbGCoXefMwLPHAz9AzQg.png"/></div></div></figure><p id="67fe" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">第二个主要挑战是告诉计算机它应该寻找哪些特征，这些特征将在预测结果中发挥重要作用，同时实现更高的准确性。</p><h2 id="6233" class="kj kk hh bd kl km kn ko kp kq kr ks kt jr ku kv kw jv kx ky kz jz la lb lc ld bi translated"><strong class="ak">利用深度学习可以解决什么样的问题？</strong></h2><p id="c4c9" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">深度学习用于以模仿人类解决问题的方式来解决问题。</p><p id="d401" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们可以使用深度学习来解决图像识别、物体检测和自然语言处理等问题——翻译、造句、文本到语音、语音到文本对动作语义的理解</p></div><div class="ab cl lk ll go lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ha hb hc hd he"><h1 id="6adc" class="lr kk hh bd kl ls lt lu kp lv lw lx kt in ly io kw iq lz ir kz it ma iu lc mb bi translated">深度学习概念</h1><h2 id="d856" class="kj kk hh bd kl km kn ko kp kq kr ks kt jr ku kv kw jv kx ky kz jz la lb lc ld bi translated">在本文中，我将涵盖以下六个概念:</h2><blockquote class="mc md me"><p id="4bd3" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">1.损失函数与成本函数</p><p id="379b" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">2.输入层、隐藏层和输出层</p><p id="a797" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">3.反向传播</p><p id="23c4" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">4.链式法则</p><p id="a00d" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">5.激活功能</p><p id="0d36" class="ji jj mf jk b jl jm ii jn jo jp il jq mg js jt ju mh jw jx jy mi ka kb kc kd ha bi translated">6.权重初始化。</p></blockquote><h1 id="6cbc" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 1。损失函数vs成本函数:</strong></h1><p id="9398" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">损失函数计算单个训练示例的误差。</p><p id="f500" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">成本函数计算整个训练示例的误差。</p><h1 id="892f" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 2 </strong>。<strong class="ak">输入层、隐藏层和输出层</strong></h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mo"><img src="../Images/a23faed85bf34fa0ec7b34a8fafa1711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*QVPkn8wqTfjEp4mSFSb4oA.png"/></div></figure><p id="5d63" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">第一层是输入层。这一层中的每个节点接受一个输入，然后将其输出作为输入传递给下一层中的每个节点。同一层中的节点之间通常没有连接，最后一层产生输出。</p><p id="7b9f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们称中间部分为隐藏层。这些神经元与外界没有联系(例如输入或输出)，只被前一层的节点激活。</p><h1 id="7fa2" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 3 </strong>。<strong class="ak">反向传播</strong></h1><p id="24f1" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">反向传播是神经网络训练的本质，也是基于前一时期获得的错误率微调神经网络权重的方法。权重的适当调整使我们能够降低错误率，并通过提高其泛化能力来使模型可靠。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mp"><img src="../Images/bda195939468accd0d4f1eb256d76f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJO2OhcuuIEnbjkv6OBiJQ.jpeg"/></div></div></figure><p id="27ec" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">反向传播是“误差反向传播”的简称这是训练人工神经网络的标准方法。这有助于计算损失函数相对于网络中所有权重的梯度。推导反向传播涉及向量函数的链式法则的许多巧妙应用。</p><h1 id="df42" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 4。链式法则</strong></h1><p id="f5e0" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">链式法则是一种计算函数导数的方法，该函数的变量本身就是其他变量的函数。如果c是标量𝑧的标量值函数，而𝑧本身是另一个标量变量𝑤的标量值函数，那么链规则规定</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mq"><img src="../Images/110f2ed7897f67ec2d452490c772547c.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*LyIirAq0rP-jXQH_sqiejw.png"/></div></figure><p id="60c9" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">对于不止一个变量的标量值函数，链式法则实质上变成了加法法则。换句话说，如果𝐶C是𝑁N变量𝑧1,…,𝑧𝑁z1,…,zN的标量值函数，每个变量都是某个变量𝑤w的函数，则链式法则表示</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/12bcec93316fdce82431073efa51c50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*HUvMPNCf5Mn0Al8Dgpn3zg.png"/></div></figure><h1 id="7b36" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 5。激活功能</strong></h1><p id="21db" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">激活函数有助于确定神经网络的输出。这些类型的函数附加到网络中的每个神经元，并根据每个神经元的输入是否与模型的预测相关来确定是否应该激活它。</p><p id="043d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">激活函数还有助于将每个神经元的输出标准化到1和0之间或-1和1之间的范围。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ms"><img src="../Images/01921040d29f981e0d9cc9fd0ab65a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*mAZyng7Y1aAKyrG2Iw2DXg.jpeg"/></div></figure><p id="3089" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在神经网络中，输入被输入到输入层的神经元中。每个神经元都有一个权重，输入数乘以权重就得到神经元的输出，输出传递到下一层。</p><p id="457c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">激活函数是馈送当前神经元的输入和去往下一层的输出之间的数学“门”。它可以像阶跃函数一样简单，根据规则或阈值打开或关闭神经元输出。</p><h2 id="28bc" class="kj kk hh bd kl km kn ko kp kq kr ks kt jr ku kv kw jv kx ky kz jz la lb lc ld bi translated"><strong class="ak">常用激活功能:</strong></h2><p id="4383" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated"><strong class="jk hi">a . s形函数</strong></p><p id="4fd0" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">函数公式和图表如下</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mt"><img src="../Images/d06081eddd1976e2fdfb199ed67c588c.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*Z0Xz1dHo_ueW5g5HoVj1mw.png"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mu"><img src="../Images/0acae4166704377b8f6ffce2f3af0004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*FNB-q5DI42gxru2Gjq2f1A.png"/></div></figure><p id="0e33" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">Sigmoid函数是深度学习开始时最常用的激活函数。这是一个容易推导的平滑函数。</p><p id="b4a1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在sigmoid函数中，我们可以看到它的输出在开区间(0，1)。我们可以想到概率，但严格意义上，不要把它当成概率。乙状结肠函数再次流行起来。它可以被认为是神经元的放电频率。中间斜率比较大的地方，是神经元的敏感区。在坡度非常平缓的一侧，是神经元的抑制区。</p><p id="7a6c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> B .双曲正切函数</strong></p><p id="3055" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">双曲正切函数公式和曲线如下</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mv"><img src="../Images/4aca7e2e4ab7d6fae7dc39cc083fe9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*L7ovptS44eS1IM93eIsKvA.png"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mw"><img src="../Images/6e9496b25a45851ba92c0341ee3243b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*yRkAfvFVFxQq9qh7Gmo7uQ.png"/></div></figure><p id="e0d4" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">双曲正切函数。双曲正切函数和sigmoid函数的曲线相对相似。我们来对比一下。首先，输入大或小时，输出几乎平滑，梯度小，不利于权重更新。区别在于输出间隔。</p><p id="c5df" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">tanh的输出区间为1)，整个函数以0为中心，比sigmoid要好。</p><p id="c8e3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在一般的二进制分类问题中，双曲正切函数用于隐藏层，sigmoid函数用于输出层。但这些都不是一成不变的，具体要用的激活函数，要根据具体问题具体分析，还是要看调试。</p><p id="21bb" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> C </strong>。<strong class="jk hi"> ReLU功能</strong></p><p id="77b5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">ReLU函数公式和曲线如下</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mx"><img src="../Images/6b4e06eeb11f2022686292f1cae1a8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*9MartTYQ4Bdy7XPlsk8rLw.png"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es my"><img src="../Images/96d63cca6847e927c98fb07e3dd40b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8lAt8t4LzjZfNLqaGD9mw.png"/></div></div></figure><p id="15ae" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">ReLU函数实际上是取最大值的函数。注意这不是完全区间可导的，但是我们可以取一个次梯度，如上图所示。ReLU虽然简单，但却是近年来的重要成果。</p><p id="3772" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">ReLU(整流线性单位)功能是目前比较流行的激活功能。</p><h1 id="ead1" class="lr kk hh bd kl ls mj lu kp lv mk lx kt in ml io kw iq mm ir kz it mn iu lc mb bi translated"><strong class="ak"> 6。权重初始化</strong></h1><p id="865e" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">重量初始化有两种主要类型:-零初始化和随机初始化。</p><p id="859e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">零初始化</strong>:在此过程中，偏差和权重被初始化为0。如果权重被设置为0，则权重矩阵中关于损失函数的所有导数变得相等。因此，在随后的迭代中，权重都不会改变。将偏移设置为0会抵消它可能产生的任何影响。</p><p id="7ffa" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">由于零初始化，所有隐藏单元变得对称。一般来说，零初始化对于分类来说不是非常有用或准确，因此在需要任何分类任务时必须避免。</p><p id="d544" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">随机初始化</strong>:与0初始化相比，这包括为权重设置随机值。唯一的缺点是设置非常高的值将增加学习时间，因为sigmoid激活函数映射接近1。同样，如果设置的值较低，随着激活函数映射接近0，学习时间会增加。</p><p id="d9f5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">因此，设置过高或过低的值通常会导致渐变爆炸或消失的问题。</p><p id="56e6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">类似“He初始化”和“Xavier初始化”的新型权重初始化也已经出现。这些都是基于特定的方程，这里没有提到，因为它们非常复杂。</p><p id="1fb7" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="mf">更多内容请看</em><a class="ae mz" href="https://plainenglish.io/" rel="noopener ugc nofollow" target="_blank"><strong class="jk hi"><em class="mf">plain English . io</em></strong></a><em class="mf">。报名参加我们的</em> <a class="ae mz" href="http://newsletter.plainenglish.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="jk hi"> <em class="mf">免费周报</em> </strong> </a> <em class="mf">。关注我们关于</em><a class="ae mz" href="https://twitter.com/inPlainEngHQ" rel="noopener ugc nofollow" target="_blank"><strong class="jk hi"><em class="mf">Twitter</em></strong></a><em class="mf">和</em><a class="ae mz" href="https://www.linkedin.com/company/inplainenglish/" rel="noopener ugc nofollow" target="_blank"><strong class="jk hi"><em class="mf">LinkedIn</em></strong></a><em class="mf">。查看我们的</em> <a class="ae mz" href="https://discord.gg/GtDtUAvyhW" rel="noopener ugc nofollow" target="_blank"> <strong class="jk hi"> <em class="mf">社区不和谐</em> </strong> </a> <em class="mf">加入我们的</em> <a class="ae mz" href="https://inplainenglish.pallet.com/talent/welcome" rel="noopener ugc nofollow" target="_blank"> <strong class="jk hi"> <em class="mf">人才集体</em> </strong> </a> <em class="mf">。</em></p><div class="na nb ez fb nc nd"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hi fi z dy ni ea eb nj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jg nd"/></div></div></a></div></div></div>    
</body>
</html>