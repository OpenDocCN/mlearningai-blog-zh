<html>
<head>
<title>Re-tell a Paper: “Deep Learning in Multimodal Remote Sensing Data Fusion: a Comprehensive Review”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">转述一篇论文:《多模态遥感数据融合中的深度学习:综述》</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/re-tell-a-paper-deep-learning-in-multimodal-remote-sensing-data-fusion-a-comprehensive-review-e54d8bd860e3?source=collection_archive---------3-----------------------#2022-07-10">https://medium.com/mlearning-ai/re-tell-a-paper-deep-learning-in-multimodal-remote-sensing-data-fusion-a-comprehensive-review-e54d8bd860e3?source=collection_archive---------3-----------------------#2022-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="3724" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">笔记，深度学习，遥感，高级方法。</h2><div class=""/><div class=""><h2 id="132a" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">我读过的论文的注释</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/4818d197988c9dec4af2d8825d72ef8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sIkuZu1M7H56yQHv"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Photo by <a class="ae jv" href="https://unsplash.com/@usgs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">USGS</a> on <a class="ae jv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8896" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">大家好，</p><p id="133f" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">本周我想分享一些我从李等人2022 的论文中摘录的46页笔记。该论文讨论了用于多模态遥感研究的深度学习方法的概述。</p><p id="6832" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">本文的综述可分为四个部分:</p><ol class=""><li id="78e2" class="ks kt hh jy b jz ka kc kd kf ku kj kv kn kw kr kx ky kz la bi translated">通过深度学习进行遥感数据多模态分析的趋势，</li><li id="dca7" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr kx ky kz la bi translated">遥感数据融合的深度学习方法，</li><li id="27fa" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr kx ky kz la bi translated">收集的资源列表，</li><li id="50c8" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr kx ky kz la bi translated">挑战和未来方向。</li></ol><h1 id="e925" class="lg lh hh bd li lj lk ll lm ln lo lp lq iw lr ix ls iz lt ja lu jc lv jd lw lx bi translated">介绍</h1><p id="4a94" class="pw-post-body-paragraph jw jx hh jy b jz ly ir kb kc lz iu ke kf ma kh ki kj mb kl km kn mc kp kq kr ha bi translated">从遥感数据中提取信息有许多方法。其中之一是我们可以一起使用深度学习和不同类型的输入数据。深度学习在图像处理方面已经取得了令人印象深刻的进展，并在今天被遥感研究人员大量采用。事实上，对于遥感研究人员来说，一种类型的图像可能不足以有效地解释一种现象。我们通常试图将一种类型的图像与其他不同类型的图像融合在一起。这是因为图像融合可以互补其特性，可以产生更稳健的分析，例如，<a class="ae jv" href="http://dx.doi.org/10.3390/rs13050851" rel="noopener ugc nofollow" target="_blank">傲霜等，2021 </a>融合卫星影像来评估土地利用变化的影响。</p><p id="886e" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">此外，“情态”术语缺乏明确的定义。参考文件给出了一些细节。作者将情态定义为<strong class="jy hr">内部属性</strong>和<strong class="jy hr">外部属性</strong>对描述场景的贡献。内部属性是指传感器的技术规格，如成像机制，以及空间、光谱、辐射和时间域的分辨率。而外部属性指的是实际采集条件，例如采集时间、观察角度等。一些典型的遥感模式包括全色、多光谱、超光谱、激光雷达、合成孔径雷达、红外、夜灯、卫星视频数据和地理空间大数据(GBD)。</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><h1 id="9762" class="lg lh hh bd li lj mk ll lm ln ml lp lq iw mm ix ls iz mn ja lu jc mo jd lw lx bi translated">1.基于深度学习的遥感数据多模态分析趋势</h1><p id="ad11" class="pw-post-body-paragraph jw jx hh jy b jz ly ir kb kc lz iu ke kf ma kh ki kj mb kl km kn mc kp kq kr ha bi translated">趋势分析基于科学网和cite-space ( <a class="ae jv" href="https://doi.org/10.1002/asi.20317" rel="noopener ugc nofollow" target="_blank">陈，2006 </a>)，关键词为“遥感”、“深度学习”、“融合”。它检索了2015年至2022年发表的598篇论文。</p><p id="029b" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">趋势分析是通过检查每年发表的论文数量、它们的国家和期刊分布以及流行关键词来进行的。下面的图1、图2和图3显示了上述趋势分析。根据分析，关于这一主题的研究增长是有希望的，因为发表的论文数量正在显著增加。中国被认为做出了重大贡献，其次是美国和德国。大多数论文发表在遥感杂志上，然后是TGRS和JSTAR。CNN成为最常见的进行多模态分析的深度学习方法之一。此外，利用多模态遥感数据融合的常见应用是分类、去云和目标检测。同时，多光谱、超光谱、激光雷达和合成孔径雷达是最常用的数据。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mp"><img src="../Images/5cbc8c4a099c7056f73e04706de2eb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CN4w7n_UdkuGJH0prpdeRw.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 1. The number of published papers annually. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 6.</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mq"><img src="../Images/7eff4443f8619052149a5c451f46265b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6AzpIfD6ZM8QQL7NG25Vgw.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 2. Countries and journals distribution. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 6.</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mr"><img src="../Images/23023f1b37d5ed185f167a25f8dad36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XF65WMiCiv-X67tYOYgV4A.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 3. Popular keywords. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 7.</figcaption></figure></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><h1 id="559d" class="lg lh hh bd li lj mk ll lm ln ml lp lq iw mm ix ls iz mn ja lu jc mo jd lw lx bi translated">2.遥感数据融合的深度学习方法</h1><p id="e7b2" class="pw-post-body-paragraph jw jx hh jy b jz ly ir kb kc lz iu ke kf ma kh ki kj mb kl km kn mc kp kq kr ha bi translated">本节的讨论分为两大类，即同质融合和异质融合。<strong class="jy hr">同质融合</strong>与融合相似的数据类型或传感器有关，例如，空间光谱融合(泛锐化、超光谱泛锐化和超光谱-多光谱)和时空融合。而<strong class="jy hr">异源融合</strong>涉及不同数据类型的融合，如高光谱-光学、合成孔径雷达-光学、遥感-GBD。这两个子领域的文献列表如图4所示。所有引文都可以在<a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">引用的论文</a>中找到。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ms"><img src="../Images/b796f0c281e15a76055d54521747d630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4POlKZ4OdmptKuSs5PBr8A.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 4. Literatur taxonomi. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 8.</figcaption></figure></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><h1 id="bec7" class="lg lh hh bd li lj mk ll lm ln ml lp lq iw mm ix ls iz mn ja lu jc mo jd lw lx bi translated">3.收集的资源列表</h1><p id="0f52" class="pw-post-body-paragraph jw jx hh jy b jz ly ir kb kc lz iu ke kf ma kh ki kj mb kl km kn mc kp kq kr ha bi translated">鉴于分析多模态卫星数据的众多基于DL的模型，作者提供了相关来源，包括初学者教程(图5)、开源数据集(图6)和开源代码(<a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank"> Li et al .，2022 </a> page 20)。所有引文均可在<a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">参考论文</a>中找到。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mt"><img src="../Images/977f4204bad8447409dde8ca1cb7edae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wk2sFnedRdD-uZfVKufsmw.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 5. List of tutorials for beginners. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 18.</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mu"><img src="../Images/5ee105ee3df0d499b9ecd8cb4413e726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGzGKwYW3ebteC59OG5uHA.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Figure 6. List of multimodal remote sensing dataset. Source: <a class="ae jv" href="https://arxiv.org/abs/2205.01380" rel="noopener ugc nofollow" target="_blank">Li et al., 2022</a> page 19.</figcaption></figure></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><h1 id="8e76" class="lg lh hh bd li lj mk ll lm ln ml lp lq iw mm ix ls iz mn ja lu jc mo jd lw lx bi translated">4.挑战和未来方向</h1><p id="4c03" class="pw-post-body-paragraph jw jx hh jy b jz ly ir kb kc lz iu ke kf ma kh ki kj mb kl km kn mc kp kq kr ha bi translated">对于这一部分，我认为可以在另一篇文章中做更详细的说明。然而，我指出了挑战的要点和参考文件中正在考虑的未来方向。</p><ul class=""><li id="9ef7" class="ks kt hh jy b jz ka kc kd kf ku kj kv kn kw kr mv ky kz la bi translated">从注册良好到未注册，</li><li id="d753" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr mv ky kz la bi translated">从面向图像到面向应用的质量评估，</li><li id="fb7a" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr mv ky kz la bi translated">从双模态到多模态，</li><li id="df12" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr mv ky kz la bi translated">从多模态学习到跨模态学习，</li><li id="84a4" class="ks kt hh jy b jz lb kc lc kf ld kj le kn lf kr mv ky kz la bi translated">从黑盒到可解释的深度学习。</li></ul></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="721f" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">本文对多模态遥感数据深度学习的研究进展进行了综述。我们可以得到该领域研究的总体趋势，已应用的不同方法的文献分类，数据和代码来源，以及对未来的一些建议方向。</p></div><div class="ab cl md me go mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ha hb hc hd he"><p id="6c6e" class="pw-post-body-paragraph jw jx hh jy b jz ka ir kb kc kd iu ke kf kg kh ki kj kk kl km kn ko kp kq kr ha bi translated">感谢阅读。</p><div class="mw mx ez fb my mz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hr fi z dy ne ea eb nf ed ef hq bi translated">Mlearning.ai提交建议</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">medium.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn jp mz"/></div></div></a></div></div></div>    
</body>
</html>