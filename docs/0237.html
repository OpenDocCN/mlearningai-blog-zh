<html>
<head>
<title>The Mathematics behind Linear Regression.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归背后的数学。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-mathematics-behind-linear-regression-fb4db1ebd7b5?source=collection_archive---------2-----------------------#2021-03-08">https://medium.com/mlearning-ai/the-mathematics-behind-linear-regression-fb4db1ebd7b5?source=collection_archive---------2-----------------------#2021-03-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="7b38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我将用最简单的方式解释与线性回归相关的各种数学概念。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/45971c24c6e83f863df125e6c7f8d654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*abND0EuChwXYOB5tRx2r7g.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Typical Linear Regression plot</figcaption></figure><p id="4959" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归是一种机器学习算法，属于监督学习方法，其中历史数据被标记并用于根据预测值/自变量确定输出/因变量的值。顾名思义，因变量和自变量之间的关系被假定为<strong class="ig hi">线性。</strong></p><p id="832a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于预测变量的数量，有两种类型的线性回归算法:</p><p id="8ffa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">简单线性回归:</strong>只用一个预测变量来预测因变量的值。直线的方程:y = c + mX。在哪里</p><p id="ed72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y:因变量</p><p id="d6dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x:预测变量</p><p id="93bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">m:定义X和y之间关系的直线的斜率，也称为X的系数</p><p id="5bf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c:拦截</p><p id="0f35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">多元线性回归:</strong>用多个预测变量来预测因变量的值。</p><p id="2c2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直线的方程:y = c + m1x1+ m2x2 + m3x3 … + mixi(很多预测变量x1，x2 … xi)。</p><p id="0173" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(m1、m2 … mi是各自的系数)</p><p id="1534" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想了解一个线性回归模型是如何从零开始建立的，请参考我下面的文章:</p><p id="717f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jo" rel="noopener" href="/mlearning-ai/linear-regression-simple-explanation-with-example-fba51b2c181d">https://medium . com/mlearning-ai/linear-regression-simple-explain-with-example-FBA 51 b 2c 181d</a></p><p id="cf37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归的目的是为给定的X和y变量找到<strong class="ig hi">最佳拟合线</strong>，这样我们就可以在上述方程中获得c和m的最佳值。现在要做到这一点，我们需要正确理解上面的图表。</p><p id="9d3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于每一个X值，比如Xk，根据我们的数据，有一个y值，比如y_true。我们的回归线也给了我们一个y值，比如y_pred。为了便于理解，我们能画出的最好的直观的线是通过所有y值的平均值的线，比如y = y_avg。</p><p id="5b49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以现在我们有了y_true，y_pred和y_avg。让我们看看他们是如何联系的。</p><p id="383c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y_true - y_pred给出了与Xk相关的误差项，也称为残差。基于y_true值，这些误差项可以是正的或负的。所以我们取这些剩余项的平方，以避免负号。所有这些残差的总和称为残差平方和(RSS)。这就形成了我们的成本函数。我们需要最小化这个成本函数，以便获得线性回归线的斜率(m)和截距(c)的最优值，我们将在本文中进一步了解。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jp"><img src="../Images/83362633a62543e1f9f581ccfa7c4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*w1AxpGa-tmelli748bK6NA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Residual Sum of Squares equation</figcaption></figure><p id="0493" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与平均值相比，解释平方和给出了回归线给出的值中存在多少变化的度量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jp"><img src="../Images/7366fe8a633dcb85074d46eae670dadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*QEIvW8frBEeZ6bWCd_CpfQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Explained Sum of Squares equation</figcaption></figure><p id="b6f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总平方和给出了与平均值相比，观察值中存在多少变化的度量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jp"><img src="../Images/78f747f78b17d8e49bd0cab541a01f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*sjPqjja7vae6kbV535kdYg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Total Sum of Squares equation</figcaption></figure><p id="df74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上面的等式和图中，我们可以验证:</p><p id="fef7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jq"> TSS = ESS + RSS </em> </strong></p><p id="da6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">决定系数(R平方)</strong></p><p id="b8b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于图中所示的回归线，决定系数是衡量因变量中有多少方差是由自变量解释的。简而言之，R平方告诉我们，对于给定的数据，我们的模型拟合得有多好。</p><p id="925f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">R平方的值在0到1之间。接近1的值通常意味着模型是很好的拟合，或者更具体地说，自变量解释了因变量中的大量方差。</p><p id="906f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以直观上我们可以写成RSS = ESS/TSS。</p><p id="39bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，R平方= (TSS-RSS)/TSS。</p><p id="4fc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，<strong class="ig hi"> <em class="jq"> R平方= 1- (RSS/TSS)。</em>T9】</strong></p><p id="4cd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于线性回归来说，这是一个重要的等式。</p><h1 id="c3c3" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">调整后的R平方</strong></h1><p id="8469" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">在建立多元线性回归模型时，当我们向模型中添加变量时，R平方值继续增加。但是如果增加的变量并不重要呢？这可能会不必要地使模型变得复杂，并增加过度拟合的机会。为了解决这个问题，有一种方法叫做调整后的R平方。</p><p id="c97f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果添加的变量不重要，调整的R平方会惩罚模型。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ku"><img src="../Images/7b820acac018a7c95640bb323133848c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*58mpG5dsqZ9Car-OXf-vwA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Equation for Adjusted R-squared</figcaption></figure><p id="14c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中N是数据中样本点的数量，P是预测变量的数量。</p><p id="ebab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这也是w.r.t .线性回归非常重要的概念。</p><p id="4de5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个好的线性回归模型总是具有彼此接近的R平方值和调整后的R平方值。但是，调整后的R平方值始终小于R平方值。</p><h1 id="6b30" class="jr js hh bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">梯度下降</strong></h1><p id="193c" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">为了得到最佳拟合线，我们必须最小化潜在的成本函数，在我们的例子中是RSS，如上所述。迭代优化和封闭形式优化是用于最小化所讨论的潜在成本函数的两种流行算法。</p><p id="fb3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在闭于解中，我们只需找到函数的导数，并使它等于0，我们就得到最小值。但是当数据是多维的时，这就变得复杂了。</p><p id="f122" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然当数据集较小时，封闭形式的解决方案是优选的，但是梯度下降对于较大的数据是有用的，并且它也是不太复杂和便宜的选择。梯度下降迭代运行，直到达到成本函数的最小值。为了了解更多，让我们考虑下图。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kv"><img src="../Images/e6ca4727608ed09f0f67f63ae40fdf2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*eEUiFlhUs93Um1Mn1Mxhsg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Gradient Descent method</figcaption></figure><p id="f16b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">J(m)是我们的成本函数，我们可以看到，我们希望达到函数收敛的点(达到最小值)。换句话说，我们想要达到一个点，在这个点上我们的函数值是最小的。所以我们分步进行，也称为迭代。</p><p id="8075" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率是我们向最小化方向移动的速度。学习率越大，我们错过最小值点的机会就越大。因此，保持较小的学习速率是明智的。这可能会使过程变得缓慢，但这是值得的。</p><p id="f54b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> m1 = m0 -(学习率)。(dJ/dm) </strong></p><p id="f322" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中:m1是迭代给出的下一个点</p><p id="c1e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">m0:当前迭代的起点</p><p id="3cff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们知道我们的成本函数是RSS = (y_pred-y_avg)</p><p id="7263" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用‘c+MX’等于y_pred。我们得到一个函数，其中有两个未知数<strong class="ig hi"> m </strong>和<strong class="ig hi"> c </strong>。</p><p id="bce4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们取这些函数的偏导数，一次w.r.t到<strong class="ig hi"> m </strong>一次w.r.t <strong class="ig hi"> c </strong>并使它们等于0。我们得到两个方程和两个未知数。求解这两个方程，我们得到了m值和c值。</p><p id="2f65" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在梯度下降中，当我们的函数到达它收敛的点时，RSS变得最小，我们得到斜率(<strong class="ig hi"> m </strong>)和截距(c)的最佳值。</p><p id="dc88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们可以说回归线<strong class="ig hi"> y = mx+c </strong>是最佳拟合线。</p><p id="bfd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong>涉及到的数学很多，在学习线性回归的同时，但我已经尽量用简化的方式解释了线性回归背后的主要数学概念。希望大家喜欢读这篇文章。</p><p id="302a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢你。</p><p id="bd2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">【https://www.linkedin.com/in/pathakpuja/】</strong>可以在LinkedIn上联系我:  <a class="ae jo" href="https://www.linkedin.com/in/pathakpuja/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="jq"/></strong></a></p><p id="7f51" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jq">请访问我的GitHub简介获取python代码:</em></strong><a class="ae jo" href="https://github.com/pujappathak" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jq">https://github.com/pujappathak</em></strong></a></p><p id="3960" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jq">如果你喜欢我的文章，请随时评论并给出你的反馈。</em> </strong></p></div></div>    
</body>
</html>