<html>
<head>
<title>Knowledge Distillation — Make your neural networks smaller</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">知识蒸馏——让你的神经网络变小</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/knowledge-distillation-make-your-neural-networks-smaller-398485f811c6?source=collection_archive---------2-----------------------#2022-07-01">https://medium.com/mlearning-ai/knowledge-distillation-make-your-neural-networks-smaller-398485f811c6?source=collection_archive---------2-----------------------#2022-07-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4e8c9656d0e5916936ebdc09b3921807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nUO0SL8KYVFRueABxwZDg.png"/></div></div></figure><p id="06d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">部署一个拥有数百万参数的庞大模型并不容易，如果我们可以将这些知识转移到一个更小的模型中，并使用它进行推理，会怎么样呢？在这篇文章的结尾，你会明白这是怎么做到的。</p><h1 id="9876" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="a4a3" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">知识蒸馏意味着以最小的信息损失将大模型的知识转移到小模型。它也可以指将多个模型的知识(集成)转移到单个模型中。</p><h2 id="94f0" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">动机</h2><p id="f78a" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在大多数情况下，我们使用相同的模型进行训练和推理。使用大量的计算能力来训练模型是可以的，但是为了部署的目的，我们应该能够用更少的资源更快地存储模型和执行预测。</p><h2 id="5cdb" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">知识</h2><p id="0580" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">这里的知识指的是输入图像到输出向量的映射，这正是我们想要传递的。输出概率不仅包含关于哪个类别是正确类别的信息，还包含不正确类别的<strong class="ir hi">相对概率。</strong></p><p id="a086" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如:-</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es le"><img src="../Images/81562ecdd2a3c229535dafd34c18c48d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc_fpvKH0KG9kowojEpyiQ.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx">The example mentioned in Hinton’s paper demonstrates what information is contained in the output</figcaption></figure><p id="89a4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">宝马被误认为垃圾车的概率很低，但同样的图像被误认为胡萝卜的概率更低，这是理解模型如何概括的有价值的信息。</p><h1 id="bc2c" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">方法</h1><p id="d4e1" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">我们有两种模型——教师模型(大而笨重)和学生模型(小而简单)。老师也可以是多个模特的合奏。教师将在数据集上单独接受培训，经过培训的模型将用于<strong class="ir hi">监督</strong>学生网络的学习过程。</p><p id="abcb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">培训学生模型主要包括4个步骤</p><p id="e2e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">1.通过教师模型<br/> 2向前传递一个数据样本。通过学生模型<br/> 3向前传递相同的数据样本。计算损耗，即两个输出之间的距离<br/> 4。使用反向传播将这种损失降至最低</p><p id="d1ab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">设<strong class="ir hi"> P(xᵢ) </strong>和<strong class="ir hi"> Q(xᵢ) </strong>分别代表数据样本xᵢ.的教师和学生网络的输出概率z和Y是网络的无标度输出逻辑。</p><p id="0ade" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">P = [p₁，p₂，…，pₖ]其中k是类的数量，同样适用于q</p><h2 id="1c9f" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">温度</h2><p id="276b" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">模型通常通过使用softmax层产生输出概率，softmax层将logits Z转换为概率q。对于蒸馏，我们使用soft max的不同变体，其参数称为<strong class="ir hi">温度</strong>。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/0c84b3541ac46a855951f68f44c24f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GvqfyIn0X6zUuh7_n3FwOA.png"/></div></div></figure><p id="ac1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用更高的温度会导致概率的软化。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/1ba55472ae69944c3e25363e38f83542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Taoxv3eo-o3wqXCsQv2_TA.png"/></div></div></figure><p id="1ea8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，如果[0.01，0.98，0.01]是硬概率，提高温度可以给我们[0.1，0.8，0.1]，这是一个更软的版本，进一步提高温度可以给我们类似于[0.2，0.6，0.2]。</p><p id="6f61" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">但为什么是温度呢？</strong></p><p id="ca50" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将2误认为3的概率可以是0.000001，而将7误认为2的概率可以是0.000000001。注意，相差1000倍。但是该信息可能不会使用交叉熵来传递，因为两个值都非常接近于0。然而，软化概率将使我们能够<strong class="ir hi">保存这个信息</strong>并相应地传输它。</p><p id="34bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">温度的使用也对学生模型具有正则化效果，并且该模型能够更好地概括。</p><h2 id="89b1" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">损失函数</h2><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/20ba611851ba3b655f6724d688ba29f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-IsmOOM1jgxswqI-j6u4Q.png"/></div></div></figure><p id="f1fa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第一部分是<strong class="ir hi">库尔贝克-莱布勒散度</strong>。应用温度(T &gt; 1)后，计算教师和学生模型的输出。</p><p id="8ad5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">损失的第二部分就是<strong class="ir hi">交叉熵损失</strong>。在计算时，我们使用学生模型的标签和输出，而不应用温度(或者说T=1)。</p><p id="1443" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">软目标产生的梯度幅度为1/T。为了确保两种损失的相对贡献在温度变化时大致保持不变，蒸馏损失乘以T。</p><p id="63c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里的α是一个<strong class="ir hi">超参数</strong>，用于平衡两个损耗，在本文中，α取0.5，给予两个损耗相等的权重。</p><h2 id="b335" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">关于KL散度的小知识</h2><p id="262e" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">KL散度可以看作是两个概率分布相似或不同程度的度量。在数学上它非常类似于交叉熵，事实上它是一种更普遍的形式。</p><p id="e644" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数学表达式参见此<a class="ae lq" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">链接</a>。<a class="ae lq" href="https://www.youtube.com/watch?v=SxGYPqCgJWM" rel="noopener ugc nofollow" target="_blank">这个</a>视频在题目上给人很大的直觉。</p><h2 id="7d78" class="kq jo hh bd jp kr ks kt jt ku kv kw jx ja kx ky kb je kz la kf ji lb lc kj ld bi translated">多模型蒸馏</h2><p id="1af1" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">当使用集合进行预测时，所需的计算能力是单个模型的数倍。知识发现可以用来将知识从多个模型转移到单个模型。可以通过取单个模型输出的<strong class="ir hi">平均值来计算概率(在计算概率之前或之后)，然后用于上述损失函数。</strong></p><h1 id="917f" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="e081" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">KD是一种很棒的现成的网络压缩技术。许多新的知识转移方法已经出现，它仍然是一个不断发展的领域。压缩网络可以帮助解决许多问题，并使我们能够在移动和边缘设备上部署模型。</p><h1 id="a925" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h1><p id="0a73" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">—<a class="ae lq" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">辛顿、杰弗里、奥里奥尔·维尼亚尔斯和杰夫·迪恩。"从神经网络中提取知识."<em class="lr"> arXiv预印本arXiv:1503.02531</em>2.7(2015)</a><br/>–<a class="ae lq" href="https://arxiv.org/pdf/2006.05525.pdf" rel="noopener ugc nofollow" target="_blank">苟、建平等，《知识升华:一项调查》<em class="lr">国际计算机视觉杂志</em>129.6(2021):1789–1819</a></p><div class="ls lt ez fb lu lv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lw ab dw"><div class="lx ab ly cl cj lz"><h2 class="bd hi fi z dy ma ea eb mb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mc l"><h3 class="bd b fi z dy ma ea eb mb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="md l"><p class="bd b fp z dy ma ea eb mb ed ef dx translated">medium.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj in lv"/></div></div></a></div></div></div>    
</body>
</html>