<html>
<head>
<title>Review — PAN: Pyramid Attention Network for Semantic Segmentation (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述——潘:语义切分的金字塔注意网络(Semantic Segmentation)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a?source=collection_archive---------0-----------------------#2021-04-18">https://medium.com/mlearning-ai/review-pan-pyramid-attention-network-for-semantic-segmentation-semantic-segmentation-8d94101ba24a?source=collection_archive---------0-----------------------#2021-04-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="90fa" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用FPA和GAU模块，性能优于<a class="ae iw" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>、<a class="ae iw" href="https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------" rel="noopener" target="_blank"> CRF-RNN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------" rel="noopener" target="_blank"> DeconvNet </a>、<a class="ae iw" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>、<a class="ae iw" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>、<a class="ae iw" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank">refinent</a>、<a class="ae iw" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca"> DUC </a>和<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a></h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/4b6f48fc7f275020cdd9bc0c2d421f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*JFgh57TcCmvM_qEUVBh34w.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Visualization results on VOC dataset</strong></figcaption></figure><p id="c751" class="pw-post-body-paragraph jk jl hh jm b jn jo ii jp jq jr il js jt ju jv jw jx jy jz ka kb kc kd ke kf ha bi kg translated"><span class="l kh ki kj bm kk kl km kn ko di">在</span>这个故事里，<strong class="jm hi">由北京理工大学、(Face ++ Inc .)和北京大学的</strong>、(潘)所做的用于语义分割的<strong class="jm hi">金字塔注意力网络被评论。在本文中:</strong></p><ul class=""><li id="90d6" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated"><strong class="jm hi">引入特征金字塔注意(FPA) </strong>模块，对高层输出进行空间金字塔注意结构，结合全局池学习更好的特征表示</li><li id="2e31" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">全局注意力上采样(GAU) </strong>在每个解码器层上引入模块，以提供全局上下文作为低级特征的指导来选择类别本地化细节。</li></ul><p id="1ded" class="pw-post-body-paragraph jk jl hh jm b jn jo ii jp jq jr il js jt ju jv jw jx jy jz ka kb kc kd ke kf ha bi translated">这是一篇发表在<strong class="jm hi"> 2018 BMVC </strong>的论文，被<strong class="jm hi">引用超过200次</strong>。(<a class="ld le ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----8d94101ba24a--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @ Medium)</p></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="c823" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated">概述</h1><ol class=""><li id="355c" class="kp kq hh jm b jn md jq me jt mf jx mg kb mh kf mi kv kw kx bi translated"><strong class="jm hi">潘:网络架构</strong></li><li id="77fd" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf mi kv kw kx bi translated"><strong class="jm hi">特征金字塔注意力(FPA)模块</strong></li><li id="5e6f" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf mi kv kw kx bi translated"><strong class="jm hi">全球关注样本(GAU)模块</strong></li><li id="4fd3" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf mi kv kw kx bi translated"><strong class="jm hi">消融研究</strong></li><li id="6200" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf mi kv kw kx bi translated"><strong class="jm hi">实验结果</strong></li></ol></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="b4de" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated">1.潘:网络架构</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mj"><img src="../Images/6c3d87ef960a15b59769080a21e850fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_WEElD7UeCGpf2R504Rtg.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">PAN: Network Architecture</strong></figcaption></figure><ul class=""><li id="6d8c" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">ImageNet预处理过的<a class="ae iw" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101，带扩张卷积(最初来自<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLab </a>，或<a class="ae iw" href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------" rel="noopener" target="_blank"> DilatedNet </a>)，用作基线。</li><li id="f1a2" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">对res5b块</strong>应用了速率为2的膨胀卷积，因此<a class="ae iw" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>输出的特征图大小为输入图像的<strong class="jm hi"> 1/16，类似于<a class="ae iw" rel="noopener" href="/@sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90"> DeepLabv3+ </a>。</strong></li><li id="bf6c" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">将原<a class="ae iw" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101 <strong class="jm hi">中的7×7卷积层</strong>替换为类似<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和<a class="ae iw" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca"> DUC </a>的三个3×3卷积层</strong>。</li><li id="5232" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi"> FPA </strong>模块用于<strong class="jm hi">从<a class="ae iw" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>的输出中采集密集像素级注意力信息</strong>。结合<strong class="jm hi">全局上下文</strong>，最终的逻辑由<strong class="jm hi"> GAU </strong>模块跟随生成最终的预测图。</li></ul></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="14d6" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated"><strong class="ak"> 2。特征金字塔注意力(FPA)模块</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/f0c7a0413e4529cb45b7166e737afacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*JhWWLQsxkJZoyuR-oIxAPA.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Feature Pyramid Attention (FPA)</strong></figcaption></figure><ul class=""><li id="85eb" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">FPA通过实施类似特征金字塔网络的U形结构(<a class="ae iw" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>)来融合三种不同金字塔尺度下的特征。</li><li id="211a" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">为了更好地从<strong class="jm hi">不同的金字塔尺度</strong>中提取上下文，我们在金字塔结构中分别使用<strong class="jm hi"> 3×3、5×5、7×7卷积</strong>。</li><li id="7032" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">由于高级特征图的分辨率较小，使用大的核大小并不会带来太大的计算负担。</li><li id="de74" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">然后金字塔结构逐步整合不同尺度的信息，这样<strong class="jm hi">可以更精确地整合上下文特征的相邻尺度。</strong></li><li id="5be2" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">此外，来自CNN的源特征在经过一个<strong class="jm hi"> 1×1卷积</strong>后，与金字塔注意力特征逐像素相乘。</li><li id="a49b" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">全局平均池分支</strong>，起源于<a class="ae iw" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank"> SENet </a>，也引入添加输出特性。</li></ul><blockquote class="mp mq mr"><p id="7b7d" class="jk jl ms jm b jn jo ii jp jq jr il js mt ju jv jw mu jy jz ka mv kc kd ke kf ha bi translated">特征金字塔注意(FPA)模块可以<strong class="jm hi">融合不同尺度的上下文信息</strong>和<strong class="jm hi">同时对高级特征地图产生更好的像素级注意</strong>。</p></blockquote></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="01f5" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated"><strong class="ak"> 3。全球关注样本(GAU)模块</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/e8059fcfc3da29d84875c815855e9f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*hxW0710PijUYHe-BCimWoQ.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Global Attention Upsample (GAU)</strong></figcaption></figure><ul class=""><li id="32ab" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">GAU执行全球平均池，以<strong class="jm hi">提供全球上下文作为低级功能的指导，从而选择类别本地化细节。</strong></li><li id="94d3" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">具体来说，在低层特征上执行<strong class="jm hi"> 3×3卷积</strong>，以减少来自CNN的特征图的通道。</li><li id="36e9" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">从高层特征生成的全局上下文是通过<strong class="jm hi"> 1×1卷积</strong>与<a class="ae iw" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <strong class="jm hi">批量归一化</strong> </a>和<strong class="jm hi"> ReLU </strong>非线性，然后<strong class="jm hi">乘以低层特征。</strong></li><li id="9b8f" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">最后，<strong class="jm hi">高层特征与加权的低层特征相加</strong>，并逐渐上采样。</li></ul><blockquote class="mp mq mr"><p id="7d6b" class="jk jl ms jm b jn jo ii jp jq jr il js mt ju jv jw mu jy jz ka mv kc kd ke kf ha bi translated">GAU更有效地部署不同比例的特征地图，并使用高级特征以简单的方式向低级特征地图提供指导信息。</p></blockquote></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="b195" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated"><strong class="ak"> 4。</strong>消融研究</h1><h2 id="7a9b" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">4.1.FPA模块</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/e9af0023ea5c5fb40bdedb4af4aca174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*Pyi5ryDd8SjWVqHQCZZc5Q.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Detailed performance of Feature Pyramid Attention with different settings on VOC 2012</strong></figcaption></figure><ul class=""><li id="7946" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated"><strong class="jm hi"> C333 </strong>:表示所有卷积的核大小为3×3。</li><li id="6d2c" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi"> C357 </strong>:表示卷积的核大小分别为3×3、5×5、7×7。</li><li id="2c8d" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">最大</strong>和<strong class="jm hi">平均</strong>:表示最大池化和平均池化操作。</li><li id="7d39" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi"> GP </strong>:全球统筹分行。</li><li id="e7fc" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">对于使用具有<strong class="jm hi"> 3×3 </strong>内核大小的所有卷积，<strong class="jm hi">【AVE】</strong>设置与【MAX】设置相比，将性能从77.13%提高到<strong class="jm hi"> 77.54% </strong>。</li><li id="ad01" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">与<a class="ae iw" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank"> SENet </a>警示模块相比，“C333”和“AVE”设置将性能提高了近1.8%。</li><li id="d9fd" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">用大核卷积‘C357’</strong>代替3×3的核大小，性能从77.54%提高到<strong class="jm hi"> 78.19%。</strong></li><li id="9e6e" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hi">通过进一步添加全局池分支</strong>，最终，最佳设置在平均IoU和像素Acc方面产生结果<strong class="jm hi"> 78.37% </strong> /95.03。(%).</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/78138af758c7332d32e5cb87beea279a.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*a1kVUywjrUeIQ3bXXN0Vzw.png"/></div></figure><ul class=""><li id="8a0c" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">与<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv3 </a>相比，FPA模块分别比<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv3 </a>高出约1.5%和1.2%。</li></ul><h2 id="367d" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">4.2.GAU模块</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/05ed17f7738022f2d1135a19bc92f4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*S5Ck-eHEp5CpfMwbpxhPyg.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Detailed performance with different settings of decoder module on VOC 2012</strong></figcaption></figure><ul class=""><li id="0202" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">在没有全球背景注意分支的情况下，GAU仅将绩效从72.60%提高到73.56%。</li><li id="9300" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">利用全局池操作提取全局上下文注意信息，使性能从73.56%提高到77.84%。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/bd1609eb87f38b824d9e43f9c640e8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*1-3VkWsDYvGNBIgQ6SMv2w.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Comparison with other state-of-art decoder module</strong></figcaption></figure><ul class=""><li id="8bde" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">GAU模块超过RRB 1.2%。</li><li id="990f" class="kp kq hh jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">值得注意的是，全局卷积网络(<a class="ae iw" href="https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------" rel="noopener" target="_blank"> GCN </a>)使用额外的COCO数据集结合VOC数据集进行训练，获得了77.50%，而<strong class="jm hi"> GAU模块在没有COCO数据集进行训练的情况下可以达到77.84%。</strong></li></ul></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h1 id="8f5e" class="lm ln hh bd jj lo lp lq lr ls lt lu lv in lw io lx iq ly ir lz it ma iu mb mc bi translated">5.实验结果</h1><h2 id="189c" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">5.1.帕斯卡VOC 2012</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/c23669c92133f6142dd43c6e01c7f605.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*al73oIFUX8pu1kA7EKjkyg.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Performance on VOC 2012 val set</strong></figcaption></figure><ul class=""><li id="28ee" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">利用多尺度输入(尺度= {0.5，0.75，1.0，1.25，1.5，1.75})以及在评估中左右翻转图像，实现了81.19%的mIOU。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es nq"><img src="../Images/d7af466ce5a21a9600ac2e7c891597b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Is6ZisZPPU3UX5kGOxTEwg.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Per-class results on PASCAL VOC 2012 test set.</strong></figcaption></figure><ul class=""><li id="8c4c" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">最后，<strong class="jm hi">提出的PAN在没有MS-COCO和Dense-CRF后处理(<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv1 </a>)的情况下实现了84.0% </strong>的性能，优于<a class="ae iw" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>、<a class="ae iw" href="https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------" rel="noopener" target="_blank"> CRF-RNN </a>、<a class="ae iw" href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------" rel="noopener" target="_blank"> DeconvNet </a>、<a class="ae iw" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、分段、<a class="ae iw" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和EncNet。</li></ul><h2 id="09d4" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">5.2.城市景观</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es nr"><img src="../Images/7bead42e6edc14e136f36cb5c885e3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*74VGD-x5r7rs05NfkL1PzA.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx"><strong class="bd jj">Performance on Cityscapes testing set without coarse annotation dataset</strong></figcaption></figure><ul class=""><li id="cd42" class="kp kq hh jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">同样，潘的表现也优于的方法，比如的<a class="ae iw" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc">、</a>、<a class="ae iw" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2的</a>、<a class="ae iw" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank"> RefineNet的</a>、<a class="ae iw" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca">、</a>PSPNet的。</li></ul></div><div class="ab cl lf lg go lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ha hb hc hd he"><h2 id="569f" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">参考</h2><p id="625e" class="pw-post-body-paragraph jk jl hh jm b jn md ii jp jq me il js jt ns jv jw jx nt jz ka kb nu kd ke kf ha bi translated">【2018】【潘】<br/> <a class="ae iw" href="https://arxiv.org/abs/1805.10180" rel="noopener ugc nofollow" target="_blank">金字塔注意力网络进行语义切分</a></p><h2 id="dd70" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated">语义分割</h2><p id="f172" class="pw-post-body-paragraph jk jl hh jm b jn md ii jp jq me il js jt ns jv jw jx nt jz ka kb nu kd ke kf ha bi translated">)(我)(们)(都)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(就)(是)(这)(些)(事)(,)(我)(们)(还)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(就)(没)(想)(到)(这)(些)(事)(了)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(有)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(,)(她)(们)(们)(们)(还)(有)(什)(么)(好)(的)(事)(。</p><h2 id="0caf" class="mx ln hh bd jj my mz na lr nb nc nd lv jt ne nf lx jx ng nh lz kb ni nj mb nk bi translated"><a class="ae iw" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>