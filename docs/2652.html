<html>
<head>
<title>A Gentle Introduction to Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树的简明介绍</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-gentle-introduction-to-decision-trees-3f85fb0c451e?source=collection_archive---------8-----------------------#2022-05-26">https://medium.com/mlearning-ai/a-gentle-introduction-to-decision-trees-3f85fb0c451e?source=collection_archive---------8-----------------------#2022-05-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/c373790436d346ea0aa1b2e975ac911f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F_Fon_CjIDLgQBmP"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@markkoenig?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mark König</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="b7ef" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">什么是决策树？</h1><p id="294d" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">决策树是一种分类算法，使用基于树的方法来分类。</p><p id="b352" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">决策树是倒过来画的，它的根在顶部。最顶端的节点代表根节点，然后继续到条件/ <strong class="ju hi">内部节点</strong>，基于此，树分裂成分支/ <strong class="ju hi">边缘</strong>。不再分裂的树枝末端，代表决定/ <strong class="ju hi">叶子。</strong></p><p id="c930" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">假设我们有一个数据集如下——其中x1和x2是因变量，y是目标变量。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/0f3c65f23c2d971ec101b4461f5ee6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*fIvgEsrSOQIOhO9z-6fZow.png"/></div></figure><p id="63e1" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">上述数据集的决策树将如下图所示。对于每个阶段，测试一个条件，属于该条件的数据行被拆分，直到我们到达死胡同。</p><p id="d558" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">死胡同可能是当你已经达到最大深度或者当你已经达到最大叶子数的时候。</p><p id="73ed" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">树的开始应该具有最高的杂质。什么是杂质？让我们在下一节讨论。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/5af311a2cfc4d69ab0aac0d4aae6e2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_K56LU3Ht7t_pt9Zic3Qw.png"/></div></div></figure><h1 id="1c8d" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">杂质函数</h1><p id="2811" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">创建决策树的方法包含一些杂质的概念。当决定在一个节点上测试哪个条件时，我们考虑问题后其子节点中的杂质。我们希望它尽可能的低(或者高纯度)。</p><p id="a744" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">例如，考虑下图。最右边的子集是具有最高杂质的子集，这意味着测试时的条件将导致两类分裂的相等或良好混合，而不像最左边的子集，分裂将立即到达终端或叶节点。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/7766aae8ff79d6d3cb8308b608ce7907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2EBd4x4c7Lfm4T_r1h7BmA.png"/></div></div></figure><h1 id="3331" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">熵和基尼杂质</h1><h2 id="04e2" class="lc iv hh bd iw ld le lf ja lg lh li je kd lj lk ji kh ll lm jm kl ln lo jq lp bi translated">有两种广泛使用的杂质函数-</h2><h2 id="769e" class="lc iv hh bd iw ld le lf ja lg lh li je kd lj lk ji kh ll lm jm kl ln lo jq lp bi translated">基尼杂质</h2><p id="a051" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">这衡量了我们错误分类数据点的概率。</p><p id="5881" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">让<strong class="ju hi"> </strong>假设我们有C个总类别，p(i)p(i)p(i)是选择具有类别iii的数据点的概率，那么基尼系数杂质计算如下</p><pre class="kw kx ky kz fd lq lr ls lt aw lu bi"><span id="7d24" class="lc iv hh lr b fi lv lw l lx ly">G=∑i=1Cp(i)∗(1−p(i))</span></pre><p id="b501" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">基尼系数为0的杂质是最低和最好的杂质</strong>。只有一切都是同一个阶层，才能实现。</p><h2 id="e99e" class="lc iv hh bd iw ld le lf ja lg lh li je kd lj lk ji kh ll lm jm kl ln lo jq lp bi translated">熵</h2><p id="71b1" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">熵可以定义为亚分裂纯度的量度。熵总是在0到1之间。任何分裂的熵都可以用这个公式来计算</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/5b8b0857d1b880c6cd1dd325fbc6dd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*zhpcnWJ6rNhUbh4HHl0gYg.png"/></div></figure><p id="a233" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">如果我们比较这两种方法，那么就计算能力而言，基尼系数比熵更有效。</p><h1 id="7145" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">信息增益</h1><p id="4a82" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">父节点的杂质和子节点杂质的加权和之差被定义为信息增益。<br/>子节点的杂质越低，信息增益越大</p><h1 id="8a5b" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">既然我们理解了杂质和信息增益，让我们形式化CART算法</h1><p id="06ba" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">CART代表分类和回归树。它构建了一个二叉树(是-否分裂)。</p><ol class=""><li id="17fa" class="ma mb hh ju b jv kq jz kr kd mc kh md kl me kp mf mg mh mi bi translated">循环遍历每个特征并分割它所取的每个值:<br/>将数据集分割成两部分(左和右):<br/>跟踪使杂质(信息增益)平均减少最大化的分割:</li></ol><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/d034d9c191d3eac5d1b2c54d13e3a343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*w51CsWvYDQ4OsZu6R_S-sw.png"/></div></figure><p id="fb4c" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">2.递归地将左右数据集传递给子节点。</p><p id="c6e9" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">3.如果满足一些停止标准，什么也不做:只识别返回的值<br/>最佳猜测:平均或最常见的类</p><div class="mk ml ez fb mm mn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb in mn"/></div></div></a></div></div></div>    
</body>
</html>