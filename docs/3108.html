<html>
<head>
<title>Evolution of Transformers — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器的发展——第一部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/evolution-of-transformers-part-1-faac3f19d780?source=collection_archive---------2-----------------------#2022-07-21">https://medium.com/mlearning-ai/evolution-of-transformers-part-1-faac3f19d780?source=collection_archive---------2-----------------------#2022-07-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/54ead3b4ccf0afb7999e458a0305621a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GPpb0-SyUPJcSqb0"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Samule Sun</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0b46" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="4d1f" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">从1956年人工智能这个术语首次被创造出来开始，到今天，它已经被应用于各个领域，经历了一段漫长的旅程。深度学习正在慢慢超越机器学习，成为大多数涉及人工智能的任务的首选方法，引领这一演变的模型是变形金刚。</p><p id="b416" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">这将是一个由3篇文章组成的系列，将介绍自2017年第一台变压器开发以来，许多研究人员在过去5年中开发的一些开创性的变压器模型。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/fe708a8690d35eb394e8643e8d1ea4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjK51ir02N5EldyVjdacNA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Evolution of Transformers. Courtesy: <a class="ae it" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">HuggingFace</a></figcaption></figure><p id="55a2" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">我们将简要回顾变压器和4种变压器架构，它们是在本部分变压器之后介绍的。</p><p id="baab" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">注意:在我们开始之前，让我们看看哪些任务由哪些模型执行。</p><ol class=""><li id="0688" class="la lb hh ju b jv kq jz kr kd lc kh ld kl le kp lf lg lh li bi translated">仅编码器模型:它们可以用于需要理解输入的任务，比如情感分析。</li><li id="6b1a" class="la lb hh ju b jv lj jz lk kd ll kh lm kl ln kp lf lg lh li bi translated">纯解码器模式:它们可以用于需要生成文本的任务，比如写故事。</li><li id="ced9" class="la lb hh ju b jv lj jz lk kd ll kh lm kl ln kp lf lg lh li bi translated">编码器-解码器模型或序列到序列模型:它们用于必须从输入中生成文本的任务，例如摘要。</li></ol><h1 id="1f42" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">谷歌的变形金刚</h1><p id="c220" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">上映日期:2017年6月12日</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/3c89a856b09cf4d21b52844b4811df72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F3ze0JiQNPsLTN8tDFKfaQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Transformers Architecture</a>. Courtesy: <a class="ae it" href="https://stats.stackexchange.com/questions/512242/why-does-transformer-has-such-a-complex-architecture" rel="noopener ugc nofollow" target="_blank">StackExchange</a></figcaption></figure><p id="67cc" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">变形金刚是一种革命性的模型，由谷歌研究人员在2017年在论文“<a class="ae it" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”中推出。变形金刚主要用于NLP任务。它们在以下方面不同于递归神经网络:</p><ol class=""><li id="7561" class="la lb hh ju b jv kq jz kr kd lc kh ld kl le kp lf lg lh li bi translated">rnn接收顺序数据。例如，如果我的输入是一个句子，RNNs将一次取一个单词作为输入。这不是变压器的情况，因为它们是非顺序的。他们可以把句子的所有单词作为输入。</li><li id="3cd7" class="la lb hh ju b jv lj jz lk kd ll kh lm kl ln kp lf lg lh li bi translated">让变形金刚更特别的是注意力机制。正是由于这种注意机制，变形金刚才能够理解上下文并获取过去的信息。rnn只能在一定程度上访问过去的信息，大多数情况下只能访问以前的状态。信息会在下一个状态中丢失。</li><li id="9c53" class="la lb hh ju b jv lj jz lk kd ll kh lm kl ln kp lf lg lh li bi translated">位置嵌入:转换器用它来存储单词在句子中的位置信息。</li></ol><p id="c95a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">这些就是《变形金刚》迅速流行起来的原因。在所有的NLP任务中，变形金刚都超过了RNNs。这就是为什么理解这种架构及其在过去5年中的演变非常重要。</p><h1 id="bfba" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">开放的GPT</h1><p id="e028" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">上映日期:2018年6月11日</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/d2f17bace80d2a080ee38053fd4add3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kat5xG7EkOYZOJeiEfw40w.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">GPT</a> Architecture. Courtesy: <a class="ae it" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank">OpenAI</a></figcaption></figure><p id="e063" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">伙计，GPT最近几年也有自己的发展。没有比GPT更好的模型来开始谈论变形金刚了。GPT主张生殖预训练。它引入了无监督学习的概念，作为预训练和监督学习，用于微调，现在许多变压器广泛使用。</p><p id="4661" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">它在由7000本未出版书籍组成的书籍语料库数据集上进行训练。GPT的架构由12个解码器堆叠在一起，这意味着它是一个只有解码器的模型。GPT-1包含1.17亿个参数，与今天开发的变形金刚相比是个小数目！它是单向的，因为它由解码器组成。解码器屏蔽当前标记右侧的标记。它一次生成一个令牌，并将该输出作为下一个时间步长的输入。GPT只是变形金刚时代的开始。前方的路更加迷人！</p><h1 id="aac7" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">由谷歌伯特</h1><p id="f652" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">上映日期:2018年10月11日</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/605cce75bc20d153fd903db1d0838e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*P3oLe64xEDQlTESt1EYqOQ.jpeg"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT</a> Architecture. Courtesy: <a class="ae it" href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" rel="noopener ugc nofollow" target="_blank">Stanford</a></figcaption></figure><p id="53eb" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">BERT代表来自变压器的双向编码器表示。顾名思义，BERT是一个双向模型。注意机制能够关注当前令牌的两个方向，左和右。这是因为BERT是由12个编码器堆叠在一起制成的，这意味着它是一个只有编码器的模型。编码器可以将完整的句子作为输入，因此可以引用句子中的任何单词来执行任务。</p><p id="63d2" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">BERT由1.1亿个参数组成。像GPT一样，它被训练完成一项特定的任务，并可以针对其他任务进行微调。它以一种特殊的方式被预先训练过。简而言之，用来预先训练伯特的任务就是填空。</p><h1 id="28e6" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">OpenAI的GPT-2</h1><p id="0ee6" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">上映日期:2019年2月14日</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lr"><img src="../Images/934e08891ecbdb940daefff0a395dab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J95Hjhkh-I5bnaNRmdcYEg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">GPT-2</a> Example. Courtesy: <a class="ae it" href="https://bair.berkeley.edu/blog/2020/12/20/lmmem/" rel="noopener ugc nofollow" target="_blank">BAIR</a></figcaption></figure><p id="566d" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">GPT-2，顾名思义，是GPT的下一个版本。像GPT一样，GPT-2 Large由48个解码器堆叠在一起，使它成为一个只有解码器的模型。它由15亿个参数组成。它被训练的任务是根据前面的单词预测句子中的下一个单词。它是在800万个网页的数据集上训练的！</p><h1 id="8463" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">脸书的罗伯塔</h1><p id="92dd" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">上映日期:2019年7月26日</p><p id="db08" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><a class="ae it" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa代表稳健优化的BERT预训练方法</a>。罗伯塔和伯特的建筑很相似。现在你可能想知道，它和伯特有什么不同？嗯，脸书的研究人员优化了BERT的超参数，以在<a class="ae it" href="https://cims.nyu.edu/~sbowman/multinli/" rel="noopener ugc nofollow" target="_blank"> MNLI </a>、<a class="ae it" href="https://paperswithcode.com/dataset/qnli" rel="noopener ugc nofollow" target="_blank"> QNLI </a>、<a class="ae it" href="https://paperswithcode.com/dataset/rte" rel="noopener ugc nofollow" target="_blank"> RTE </a>、S <a class="ae it" href="https://paperswithcode.com/dataset/sts-benchmark" rel="noopener ugc nofollow" target="_blank"> TS-B </a>、<a class="ae it" href="https://paperswithcode.com/dataset/glue" rel="noopener ugc nofollow" target="_blank"> GLUE </a>和<a class="ae it" href="https://www.cs.cmu.edu/~glai1/data/race/" rel="noopener ugc nofollow" target="_blank"> RACE </a>任务中取得最先进的结果。</p><p id="9afc" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">它删除了BERT的下一句话预训练目标，用更大的小批量和学习率训练它，还改变了掩蔽模式。它的训练数据也比BERT多。它在现有的未标注的NLP数据集以及CC-News上进行训练，CC-News是一个从公共新闻文章中提取的新集合。</p><h1 id="4181" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">结论</h1><p id="bcd3" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">正如我们所看到的，许多研究人员在Transformer发布后的最初方法是将编码器和解码器架构分开，并将它们堆叠起来，以创建一个全新的架构，能够比原始Transformer模型更好地执行某些NLP任务。我们谈到的另一个重要方面是用于预训练的无监督学习方法，然后是用于微调模型的监督学习。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ls"><img src="../Images/a835786b993a577b365dbabdfd591d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hjbkUmFAnzDRK-jh"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@etiennegirardet?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Etienne Girardet</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1ea8" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在下一部分，我们将看到2019-2021年该领域的一些关键发展。下一部分的链接将在本文发布后立即更新。再见，敬请期待！</p></div><div class="ab cl lt lu go lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ha hb hc hd he"><h1 id="dec9" class="iu iv hh bd iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn me jp jq jr bi translated">参考</h1><p id="b122" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">[1]祖海卜·阿赫塔尔，<a class="ae it" href="https://iq.opengenus.org/introduction-to-gpt-models/#:~:text=First%20was%20with%20117%20million,parameters%20(GPT%2D2)." rel="noopener ugc nofollow" target="_blank">GPT车型介绍</a></p><p id="fc36" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">[2] Jacob Devlin，Chang Ming-Wei，<a class="ae it" href="http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">开源BERT:自然语言处理的最先进的预培训</a></p><p id="bfd1" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">[3]亚历克·拉德福德，杰弗里·吴，雷文·柴尔德，大卫·栾，达里奥·阿莫代，伊利亚·苏茨基弗，<a class="ae it" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习器</a></p><p id="456f" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">[4]刘，米莱奥特，纳曼戈亚尔，杜，曼达尔乔希，陈，奥梅尔列维，，卢克塞特勒莫耶，韦塞林斯托扬诺夫，<a class="ae it" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔:一种稳健优化的伯特预训练方法</a></p><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw in mi"/></div></div></a></div></div></div>    
</body>
</html>