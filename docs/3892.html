<html>
<head>
<title>Cross Validation with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用代码示例进行交叉验证</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/cross-validation-with-code-examples-eaabc440f61d?source=collection_archive---------3-----------------------#2022-11-04">https://medium.com/mlearning-ai/cross-validation-with-code-examples-eaabc440f61d?source=collection_archive---------3-----------------------#2022-11-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4d3b3bbbaec0b062022a32deb92d854b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yz3-BSDxe4wO3pli"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/es/@agforl24?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tai Bui</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="9525" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">总结:</h2><ol class=""><li id="9406" class="js jt hh ju b jv jw jx jy jf jz jj ka jn kb kc kd ke kf kg bi translated">什么是交叉验证？</li><li id="0101" class="js jt hh ju b jv kh jx ki jf kj jj kk jn kl kc kd ke kf kg bi translated">为什么我们要使用交叉验证？</li><li id="0ba4" class="js jt hh ju b jv kh jx ki jf kj jj kk jn kl kc kd ke kf kg bi translated">交叉验证的常见类型有哪些？</li><li id="f3fc" class="js jt hh ju b jv kh jx ki jf kj jj kk jn kl kc kd ke kf kg bi translated">如何应用交叉验证(带代码)？</li><li id="e41b" class="js jt hh ju b jv kh jx ki jf kj jj kk jn kl kc kd ke kf kg bi translated">交叉验证中的过度拟合和欠拟合</li><li id="56dd" class="js jt hh ju b jv kh jx ki jf kj jj kk jn kl kc kd ke kf kg bi translated">我们应该注意什么？</li></ol></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h2 id="48b0" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">什么是交叉验证？</h2><p id="e017" class="pw-post-body-paragraph kt ku hh ju b jv jw kv kw jx jy kx ky jf kz la lb jj lc ld le jn lf lg lh kc ha bi translated">交叉验证是一种用于评估机器学习模型性能的评估技术。它使用多个训练测试分割来评估单个模型，并返回多个准确度分数。该过程类似于重采样过程，使用原始数据集的不同子集来训练和评估同一模型，以便我们可以从一系列得分中获得一个平均准确度得分，以确定我们训练的模型是否是一个好的预测器。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h2 id="b929" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">为什么我们要使用交叉验证？</h2><p id="bfe2" class="pw-post-body-paragraph kt ku hh ju b jv jw kv kw jx jy kx ky jf kz la lb jj lc ld le jn lf lg lh kc ha bi translated">首先，交叉验证给了我们一个<strong class="ju hi">更稳定可靠的模型估计</strong>。估计的准确度分数将根据在训练和测试集中结束的数据样本而变化。因此，通过交叉验证，我们可以从多个训练测试拆分返回的一系列分数中获得模型的平均准确度分数，而不是依赖于单个特定的训练集来获得最终的准确度分数。</p><p id="96b3" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">第二，交叉验证可以<strong class="ju hi">显示模型敏感度</strong>。有了交叉验证返回的一系列准确度分数，我们可以对模型性能做最坏情况或最好情况的设想。具体来说，我们可以绘制准确度分数的分布图，以查看我们的模型在新数据集上表现不佳或非常好的可能性。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h2 id="b907" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">交叉验证的常见类型有哪些？</h2><p id="178b" class="pw-post-body-paragraph kt ku hh ju b jv jw kv kw jx jy kx ky jf kz la lb jj lc ld le jn lf lg lh kc ha bi translated"><strong class="ju hi"> K重交叉验证。</strong></p><p id="2f1f" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">以K = 5为例。将原始数据集随机分成大小相等的5份，并重复该过程5次。对于每一次，一个折叠作为测试集，另外四个折叠作为训练集训练模型得到相应的准确率得分。有了所有这些分数，我们可以获得一个平均交叉验证准确度分数。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/a3158be00b0a118e5024cca2619f4a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkufYLIo0nU2iwQP8IG2Xg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image Source: SIADS542 by Kevyn Collins-Thompson</figcaption></figure><p id="c079" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">由于在模型训练和测试期间使用了所有数据样本，K-fold交叉验证返回的模型偏差较小。</p><p id="7662" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">然而，如果原始数据样本是按照某种顺序或根据类别标签排序的，那么我们可能会以不平衡的训练子集和无代表性的测试集来结束对模型的训练。因此，得到的准确度分数将是不准确的。</p><p id="3b70" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">代码示例</strong></p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="4cc6" class="iu iv hh lt b fi lx ly l lz ma"># import libs<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score</span><span id="c9f4" class="iu iv hh lt b fi mb ly l lz ma"># get cancer data<br/>cancer = load_breast_cancer()<br/>X_cancer = cancer['data']<br/>y_cancer = cancer['target']</span><span id="7129" class="iu iv hh lt b fi mb ly l lz ma"># normalized the data<br/>scaler = MinMaxScaler()<br/>X_cancer_scaled = scaler.fit_transform(X_cancer)</span><span id="c35b" class="iu iv hh lt b fi mb ly l lz ma"># apply classifier<br/>clf = LogisticRegression()</span><span id="5bb0" class="iu iv hh lt b fi mb ly l lz ma"># get cv scores<br/>cv_scores = cross_val_score(clf, X_cancer_scaled, y_cancer, cv = 5)</span><span id="8421" class="iu iv hh lt b fi mb ly l lz ma">print('Cross validation scores (5 folds): {}'.format(cv_scores))<br/>print('The average cross validation score (5 folds): {}'.format(np.mean(cv_scores)))</span><span id="a330" class="iu iv hh lt b fi mb ly l lz ma">## final result ##<br/>## Cross validation scores (5 folds): [0.95614035 0.96491228 0.97368421 0.95614035 0.96460177]<br/>## The average cross validation score (5 folds): 0.9630957925787922</span></pre><p id="b483" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">分层K折交叉验证。</strong></p><p id="89dd" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">为了解决潜在的不平衡问题，我们可以使用分层交叉验证。基本上，数据样本被重新排列，以确保每个子集中类的比例尽可能接近整个数据集中类的实际比例。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/c0cd08d39e50d1bd191689902760d365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RelCENxw7r-51P9T4tDWXg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image Source: SIADS542 by Kevyn Collins-Thompson</figcaption></figure><p id="ec0e" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">这样，每个子集都很好地代表了整个数据集。</p><p id="3942" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">代码示例</strong></p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="3c59" class="iu iv hh lt b fi lx ly l lz ma"># import libs<br/>from sklearn.datasets import load_iris<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import StratifiedKFold</span><span id="2126" class="iu iv hh lt b fi mb ly l lz ma"># get iris data<br/>iris = load_iris()<br/>X_iris = iris['data']<br/>y_iris = iris['target']</span><span id="c740" class="iu iv hh lt b fi mb ly l lz ma"># stratified 3-fold splits<br/>skf = StratifiedKFold(n_splits=3)<br/>skf.get_n_splits(X_iris, y_iris)   # 3 iterations</span><span id="2cc3" class="iu iv hh lt b fi mb ly l lz ma"># apply classifier<br/>clf = LogisticRegression()</span><span id="aac7" class="iu iv hh lt b fi mb ly l lz ma"># get stratified cv scores<br/>cv_skf_scores = cross_val_score(clf, X_iris, y_iris, cv = skf)<br/>print('Cross validation scores (3 folds): {}'.format(cv_skf_scores))<br/>print('The average cross validation score (3 folds): {}'.format(np.mean(cv_skf_scores)))</span><span id="66c2" class="iu iv hh lt b fi mb ly l lz ma">## final result ##<br/>## Cross validation scores (3 folds): [0.98 0.96 0.98]<br/>## The average cross validation score (3 folds): 0.9733333333333333</span></pre><p id="4d24" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">需要注意的是，这里我们只是为了演示。事实上，我们自己并不需要进行分层拆分。<code class="du md me mf lt b">cross_val_score</code>中的<code class="du md me mf lt b">cv</code>参数将自行识别输入估算器。如果<code class="du md me mf lt b">y</code>是二进制或多类，则自动使用<code class="du md me mf lt b">StratifiedKFold</code>。也就是说，我们可以直接用<code class="du md me mf lt b">cv = 3 </code>代替<code class="du md me mf lt b">cv = skf </code>得到同样的结果。</p><p id="d9ce" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">留一交叉验证。</strong></p><p id="bafd" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">当K等于数据集中数据样本的总数(n)时，留一交叉验证是K重交叉验证。顾名思义，只留下一个数据样本作为测试集，其余所有数据样本作为训练集。迭代K = n次后，可以得到平均交叉验证准确率得分。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/d99a1d14b69a80647962987497b69bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jchp8OiWV2yqUZrtkQGVYw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image Source: SIADS542 by Kevyn Collins-Thompson</figcaption></figure><p id="670c" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">如果我们使用自定义的P个样本而不是一个样本作为测试集，那么这种交叉验证称为<strong class="ju hi">留P-out交叉验证</strong>。</p><p id="3faf" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">代码示例</strong></p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="ea05" class="iu iv hh lt b fi lx ly l lz ma"># import libs<br/>from sklearn.datasets import load_iris<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import LeaveOneOut<br/>from sklearn.model_selection import LeavePOut</span><span id="889d" class="iu iv hh lt b fi mb ly l lz ma"># get iris data<br/>iris = load_iris()<br/>X_iris = iris['data']<br/>y_iris = iris['target']</span><span id="2222" class="iu iv hh lt b fi mb ly l lz ma"># leave one out splits<br/>loo = LeaveOneOut()<br/>loo.get_n_splits(X_iris)   # 150 iterations</span><span id="ac72" class="iu iv hh lt b fi mb ly l lz ma"># leave P out splits<br/>lpo = LeavePOut(2)<br/>lpo.get_n_splits(X_iris)   # 11175 iterations</span><span id="139a" class="iu iv hh lt b fi mb ly l lz ma"># apply classifier<br/>clf = LogisticRegression()</span><span id="6332" class="iu iv hh lt b fi mb ly l lz ma"># get leave-one-out cv scores<br/>cv_loo_scores = cross_val_score(clf, X_iris, y_iris, cv = loo)<br/>print('Cross validation scores: {}'.format(cv_loo_scores))<br/>print('The average cross validation score: {}'.format(np.mean(cv_loo_scores)))</span><span id="9e8d" class="iu iv hh lt b fi mb ly l lz ma">## leave-one-out result ##<br/>Cross validation scores: [1. ... 1. 1.]<br/>The average cross validation score : 0.9666666666666667</span><span id="0b89" class="iu iv hh lt b fi mb ly l lz ma"># get leave-p-out cv scores<br/>long time to run!<br/>cv_lpo_scores = cross_val_score(clf, X_iris, y_iris, cv = lpo)<br/>print('Cross validation scores: {}'.format(cv_lpo_scores))<br/>print('The average cross validation score: {}'.format(np.mean(cv_lpo_scores)))</span><span id="ffb7" class="iu iv hh lt b fi mb ly l lz ma">## leave-p-out result ##<br/>## Cross validation scores : [1. 1. 1. ... 1. 1. 1.]<br/>## The average cross validation score: 0.9652796420581655</span></pre><p id="ca17" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">注意，留一法和留p法都是详尽的交叉验证技术。当我们有一个小的数据集时，最好使用它们，否则，运行起来会非常昂贵。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h2 id="d982" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">绘制验证曲线，查看过度拟合和欠拟合</h2><p id="ace0" class="pw-post-body-paragraph kt ku hh ju b jv jw kv kw jx jy kx ky jf kz la lb jj lc ld le jn lf lg lh kc ha bi translated">下面，我们使用<code class="du md me mf lt b">validation_curve()</code>获得SVM模型在<em class="mh">乳腺癌</em>数据集上的训练和交叉验证分数，我们之前使用该数据集来查看SVM模型欠拟合或过拟合的相应伽马值。</p><figure class="lo lp lq lr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mi"><img src="../Images/90c3db587688ecf68036c25fffbd56de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XYC52pgUFnp9uXPUE-JrKQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by the author</figcaption></figure><p id="6a16" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">我们可以看到，当伽玛值低于10的负7次方时，模型欠拟合，当伽玛值高于10的负4次方时，模型过拟合。一个好的gamma值应该介于两者之间，因为训练和交叉验证分数都很高(≥0.9)。</p><p id="d687" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><strong class="ju hi">代码示例</strong></p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="a023" class="iu iv hh lt b fi lx ly l lz ma"><em class="mh">Reference: </em><a class="ae it" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py" rel="noopener ugc nofollow" target="_blank"><em class="mh">Scikit-learn.org</em></a></span><span id="0815" class="iu iv hh lt b fi mb ly l lz ma">from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import validation_curve<br/>from sklearn.svm import SVC<br/>import matplotlib.pyplot as plt</span><span id="0f1b" class="iu iv hh lt b fi mb ly l lz ma"># get cancer data<br/>cancer = load_breast_cancer()<br/>X_cancer = cancer['data']<br/>y_cancer = cancer['target']</span><span id="2d0d" class="iu iv hh lt b fi mb ly l lz ma"># set gamma parameter values<br/>param_range = np.logspace(-10, -2, 6)</span><span id="50a2" class="iu iv hh lt b fi mb ly l lz ma"># get training and test scores<br/>train_score, test_score = validation_curve(SVC(random_state=0), X_cancer, y_cancer,param_name = 'gamma',param_range = param_range, cv = 5)</span><span id="4b28" class="iu iv hh lt b fi mb ly l lz ma"># get means and stds of training and test scores <br/>train_score_mean = np.mean(train_score, axis = 1)<br/>test_score_mean = np.mean(test_score, axis = 1)<br/>train_score_std = np.std(train_score, axis = 1)<br/>test_score_std = np.std(test_score, axis = 1)</span><span id="0393" class="iu iv hh lt b fi mb ly l lz ma"># make validation curve plot<br/>plt.figure(figsize = (8,6))</span><span id="d3df" class="iu iv hh lt b fi mb ly l lz ma">plt.title("Validation Curve with SVM on Breast Cancer Dataset")<br/>plt.xlabel(r"gamma $\gamma$")<br/>plt.ylabel("Accuracy Score")<br/>plt.ylim(0.0, 1.1)</span><span id="3f5a" class="iu iv hh lt b fi mb ly l lz ma">plt.semilogx(<br/>    param_range, train_score_mean, label="Training score", color="blue", lw=2<br/>)<br/>plt.fill_between(<br/>    param_range,<br/>    train_score_mean - train_score_std,<br/>    train_score_mean + train_score_std,<br/>    alpha=0.2,<br/>    color="blue",<br/>    lw=2,<br/>)<br/>plt.semilogx(<br/>    param_range, test_score_mean, label="Cross-validation score", color="green", lw=2<br/>)</span><span id="d6b0" class="iu iv hh lt b fi mb ly l lz ma">plt.fill_between(<br/>    param_range,<br/>    test_score_mean - test_score_std,<br/>    test_score_mean + test_score_std,<br/>    alpha=0.2,<br/>    color="green",<br/>    lw=2,<br/>)<br/>plt.legend(loc="best")<br/>plt.show()</span></pre></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><h2 id="1334" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">我们应该注意什么？</h2><p id="4dfc" class="pw-post-body-paragraph kt ku hh ju b jv jw kv kw jx jy kx ky jf kz la lb jj lc ld le jn lf lg lh kc ha bi translated">交叉验证可能<strong class="ju hi">计算量很大</strong>，尤其是当我们有一个大数据集并设置了一个大折叠值时。这是因为算法不能并行计算折叠结果，所以需要K倍的时间来获得所有分数。</p><p id="7a51" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated">此外，交叉验证<strong class="ju hi">用于模型评估，而非模型调整</strong>。我们应该使用网格搜索，而不是使用交叉验证来调整模型参数。</p></div><div class="ab cl km kn go ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ha hb hc hd he"><p id="1360" class="pw-post-body-paragraph kt ku hh ju b jv li kv kw jx lj kx ky jf lk la lb jj ll ld le jn lm lg lh kc ha bi translated"><em class="mh">感谢阅读！如果你喜欢这篇文章，请为我鼓掌</em>👏更多内容请关注我！ ☀️🌸😺</p><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na in mm"/></div></div></a></div></div></div>    
</body>
</html>