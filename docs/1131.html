<html>
<head>
<title>A Guide to Using U-Nets for Image Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用U-网进行图像分割的指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-guide-to-using-u-nets-for-image-segmentation-4799410c8aef?source=collection_archive---------1-----------------------#2021-10-08">https://medium.com/mlearning-ai/a-guide-to-using-u-nets-for-image-segmentation-4799410c8aef?source=collection_archive---------1-----------------------#2021-10-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="14f0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">图像分割是在每像素水平上定位图像中的对象或边界的强大技术。在这篇博客中，我们快速浏览一下U-net是如何工作的，并探索它们是如何建立在标准CNN上的。</h1><p id="dfeb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">基于深度学习(DL)的图像处理现在被用于各种行业。特别是，像图像分类(即识别图像代表什么)和对象检测(即识别图像中的对象及其位置)这样的方法正被应用于从计算机视觉应用(例如，自动驾驶)到识别安全视频中的人的用例中。</p><p id="2063" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">另一个强大的方法是<em class="kf">语义图像分割</em>，也称为<em class="kf">像素级分类</em>或<em class="kf">密集预测</em>，它允许我们在每像素级别上对属于同一对象类的图像部分进行分类。例如，上图显示了如何通过分析全色图像，使用分割来突出显示属于摩托车及其驾驶者的像素。并且<em class="kf">实例分割</em>通过识别属于同一类(例如，摩托车1、摩托车2)的对象的所有实例来更进一步。</p><p id="46f2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">语义图像分割可以通过使用一个<em class="kf"> U-Net </em>，一种特殊类型的卷积神经网络(CNN)来实现。U-Net增加了一个<em class="kf">扩展路径</em>来生成属于在源图像中发现的特征或对象的像素的分类。换句话说，它将输出扩展到某个图像大小，并在网络中形成U的后半部分。U-Net使我们能够超越常规的图像分类和物体检测，对这些物体的像素进行精确的形状分类。</p><p id="4632" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">让我们仔细看看这是如何工作的。</p><p id="1f70" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">CNN综述</strong></p><p id="a837" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了理解U-Nets，让我们简单回顾一下他们的底层基础<em class="kf">标准</em> CNN是如何工作的。</p><p id="ff24" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们强调<em class="kf">标准</em>是因为有许多变体，包括<a class="ae kg" href="https://en.wikipedia.org/wiki/LeNet" rel="noopener ugc nofollow" target="_blank"> LeNet </a>、<a class="ae kg" href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>和其他CNN，它们都基于相同的一般原理，即以图像为输入，使用<em class="kf">滤波器</em>(也称<em class="kf">核</em>)对其进行<em class="kf">卷积</em>以提取一个或多个<em class="kf">特征图</em>，这一过程被称为<em class="kf">特征提取</em>。然后通过<em class="kf">汇集</em>对这些特征图进行下采样，并传递到下一层进行进一步的卷积和汇集。在每个后续卷积/汇集步骤中生成的要素图可提取更高级别的要素。最终的下采样(混合)特征图然后被展平，并用作完全连接的神经网络的输入，用于分类，如图1所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/868df63d2089f3a050f18223a8ec7287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/0*cqoVasXOAG7xo4vi"/></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 1: A simple CNN with one convolution/pooling layer that can classify an image. (</em><a class="ae kg" href="https://www.researchgate.net/figure/Schematic-diagram-of-a-basic-convolutional-neural-network-CNN-architecture-26_fig1_336805909" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="a519" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">根据您构建网络的方式，您可以修改要素提取阶段的工作方式，并根据应用程序的需要添加任意数量的图层。例如，下图2所示的<a class="ae kg" href="https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks" rel="noopener ugc nofollow" target="_blank"> R-CNN </a>通过对图像的区域进行分类来进行物体检测:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/51dc699c03066c6bda7cd985b5731d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7G7fMwTAi9ug4Xvg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 2: Example of an R-CNN that can classify objects in an image. (</em><a class="ae kg" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" rel="noopener" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="99f5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">以我们想象的骑在摩托车上的骑手为例。分析该图像的R-CNN的第一个特征图可能由表示图像中发现的基本形状(例如，曲线、直线段等)的像素组组成。).下一层可以提取组成这些对象的某些特征的形状组(例如，头盔、轮子等)。)，等等。</p><p id="7058" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">最终输出包括分类，显示在图像区域中发现了什么对象，如图3所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kz"><img src="../Images/881bdd3c7bb431a90fea9d42fc091866.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/0*r9JvRS4jlpz0sQOO"/></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 3: Example of object detection using a R-CNN that classifies objects found in certain regions of the image.</em> (<a class="ae kg" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a>)</figcaption></figure><p id="63e4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">中枢神经系统已成为图像处理任务的热门，因为它们可以处理空间和时间方差，同时使用汇集减少(下采样)模型中的中间图像，以提高处理速度。事实上，中枢神经系统对图像处理非常有用，因此您会在我们的<a class="ae kg" href="https://blog.perceptilabs.com/top-five-ways-that-machine-learning-is-being-used-for-image-processing-and-computer-vision/" rel="noopener ugc nofollow" target="_blank">机器学习用于图像处理和计算机视觉的五大方式</a>博客中多次提到它们。</p><p id="0795" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果您想进一步加深您对有线电视新闻网工作原理的理解，请查看这个<a class="ae kg" href="https://www.cs.ryerson.ca/~aharley/vis/conv/" rel="noopener ugc nofollow" target="_blank">非常酷的交互式3D有线电视新闻网可视化工具</a>:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es la"><img src="../Images/84171250586f1d91d02068ba3ab99e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SjRtMEG0MzLaoDBi"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 4: Screenshot from an interactive online 3D CNN tool showing how a CNN classifies an image containing a numbe</em>r. (<a class="ae kg" href="https://www.cs.ryerson.ca/~aharley/vis/conv/" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a>)</figcaption></figure><p id="9762" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">关于构建特征图的其他信息，请务必查看我们在博客中对卷积层的描述:<a class="ae kg" href="https://blog.perceptilabs.com/four-common-types-of-neural-network-layers-and-when-to-use-them/" rel="noopener ugc nofollow" target="_blank">四种常见类型的神经网络层(以及何时使用)</a>。</p><p id="4096" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">扩展到U网</strong></p><p id="dc8a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">到目前为止描述的CNN架构由一个<em class="kf">收缩路径</em>(也称为<em class="kf">编码器</em>)组成，它使用连续下采样的数据迭代地构建特征地图。</p><p id="15f5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">U-Net通过添加相应的<em class="kf">扩展路径</em>(也称为<em class="kf">解码器</em>)来增强标准CNN架构，目的是产生全分辨率语义预测。换句话说，生成突出在图像中发现的特定特征和对象的分割图像。这些高光是使用特定的颜色值在每个像素级别上完成的，并且输出图像通常至少与源图像的大小(尺寸)相同。</p><p id="cf6b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">以上面的摩托车图片为例。使用U-Net进行图像分割，我们可以在比对象检测更细粒度的级别上对对象进行分类，对象检测仅限于边界框。如下面的图5所示，输出(右)基于每个像素对源图像(左)中的对象进行分类:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lb"><img src="../Images/4f65c8113060eb3df61df712a8fd00d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_n7eDhbX1mpq1FsS"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 5: Example of a segmentation output (right) generated from a source image (left) to classify objects on a per-pixel level.</em> (<a class="ae kg" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a>)</figcaption></figure><p id="7241" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">U-Net的扩展路径通过使用<em class="kf">转置卷积</em>对收缩路径的各种特征图进行上采样来工作，该过程有时被称为<em class="kf">去卷积</em>。扩展路径也与收缩路径有一些连接，称为<em class="kf">跳过连接</em>。使用跳过连接和正常连接，扩展路径将要素信息(在收缩路径期间增加)与空间信息(在收缩路径期间减少)结合在一起。</p><p id="af0a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">DL实践者经常将收缩和扩张路径以及它们的跳跃连接描绘成U形，如图6所示，因此得名<em class="kf"> U-Net </em>:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lc"><img src="../Images/b5c47d339b315aec37b6a1a8749436a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2NkO05d7UHNlbrh9"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 6: Example of a U-Net model diagram showing the correspondence between the layers of the expansive and contracting path. (</em><a class="ae kg" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a>¹<em class="kt">)</em></figcaption></figure><p id="5489" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在为<a class="ae kg" href="https://blog.perceptilabs.com/when-to-use-transfer-learning-in-image-processing/" rel="noopener ugc nofollow" target="_blank">转移学习</a>构建U-Net时，选择模型的<em class="kf">主干</em>变得很重要。这实质上意味着选择基本的CNN(例如，VGG、ResNet等。)作为U-Net的基础。该选择定义了收缩路径和相应扩展路径中的层的配置。</p><p id="c91f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">感知实验室让U网变得简单</strong></p><p id="2fbc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">PerceptiLabs可以很容易地建立一个U-Net，并用不同的参数和主干进行实验。只需选择一个U-Net组件并将其放到您的模型中，将其连接到您的输入和目标，然后配置其参数。U-Net组件抽象掉了所有的收缩层和扩展层，这样您就可以关注它是如何转换输入的，如下面的图7所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ld"><img src="../Images/c89fec021dd60d47bcbdd2f93aa658b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WGFN_ZbyKA_aVBsa"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 7: PerceptiLabs’ U-Net Component gives you everything you need to experiment with U-Nets. (</em><a class="ae kg" href="http://www.perceptilabs.com" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="6be6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">您可以通过从组件的<strong class="je hi">主干</strong>设置中选择不同的主干来轻松尝试。其他设置，如<strong class="je hi">激活</strong>、<strong class="je hi">输出激活</strong>、<strong class="je hi">汇集</strong>和<strong class="je hi">取消汇集</strong>方法，也可以以类似的方式进行试验。从那以后，只需在Perceptilabs的<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/references/ui-overview/statistics-view" rel="noopener ugc nofollow" target="_blank">统计视图</a>中查看训练和验证结果，就像您试验不同的值一样，如图8所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ld"><img src="../Images/d16a2ed0bf07cbe732380bd3e1f1a228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*08lGUH_u2Tk4e7LD"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 8: PerceptiLabs’ Statistics View while training a U-Net. (</em><a class="ae kg" href="http://www.perceptilabs.com" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="119e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">统计视图显示实时指标，包括覆盖在基础事实上的预测分割(左上)和Union上的<a class="ae kg" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">交集</a> (IoU)(中右)，用于跨时期的验证和训练。</p><p id="7499" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">IoU是评估模型准确性的一个很好的方法。它通过比较输出中的对象与地面真实中的对象的重叠程度，超越了像素精度(由于背景比对象级别的像素多，像素精度可能会不平衡)。您还可以在PerceptiLabs的<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/references/ui-overview/test-view" rel="noopener ugc nofollow" target="_blank">测试视图</a>中查看这个模型的测试数据，如图9所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ld"><img src="../Images/88b47580fb520851c1f1ca48099cd00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f30cX5-oxxr5-7dp"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 9: PerceptiLabs’ Test View. (</em><a class="ae kg" href="http://www.perceptilabs.com" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="804a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">或者，您可以在PerceptiLabs中从头开始构建U网。图10显示了如何使用一系列<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/references/components/deep-learning#convolution" rel="noopener ugc nofollow" target="_blank">卷积</a>组件创建收缩路径，同时使用<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/references/components/operations#merge" rel="noopener ugc nofollow" target="_blank">合并</a>和卷积组件的组合构建扩展路径:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es le"><img src="../Images/c1ab930f51cb0347704709267ab3b43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*avrq0CkLGm9RwCzb"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx"><em class="kt">Figure 10: A U-Net manually-created in PerceptiLabs. (</em><a class="ae kg" href="http://www.perceptilabs.com" rel="noopener ugc nofollow" target="_blank"><em class="kt">Source</em></a><em class="kt">)</em></figcaption></figure><p id="ae21" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">虽然创建这种方法需要更多的工作，但它确实为每一层如何转换数据提供了额外的可视化。</p><p id="a065" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">结论</strong></p><p id="a372" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">U-Nets是一种用于有效图像分割的强大类型的CNN。它们最初是为生物医学分割而开发的，但后来在其他垂直领域发挥了作用，包括军事、机器人、物联网、城市规划和其他需要详细分析图像的领域。</p><p id="a0fd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">PerceptiLabs的U-Net组件封装了构建和训练U-Net所需的一切，同时让您可以选择从头构建它们。除了可以组织模型的<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/references/ui-overview/model-hub" rel="noopener ugc nofollow" target="_blank">模型中心</a>和可以实时查看训练结果的统计和训练视图，U-Nets的试验从未如此简单快捷。</p><p id="5543" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">准备好试用U-Nets了吗？今天就运行<a class="ae kg" href="https://docs.perceptilabs.com/perceptilabs/getting-started/quickstart-guide" rel="noopener ugc nofollow" target="_blank">感知实验室</a>并尝试一下吧！</p><p id="a85a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如需更多信息，我们建议查看以下优秀文章:</p><ul class=""><li id="adc7" class="lf lg hh je b jf ka jj kb jn lh jr li jv lj jz lk ll lm ln bi translated"><a class="ae kg" href="https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5" rel="noopener" target="_blank"> UNet —逐行解释</a></li><li id="5947" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz lk ll lm ln bi translated"><a class="ae kg" href="https://www.jeremyjordan.me/semantic-segmentation/" rel="noopener ugc nofollow" target="_blank">语义图像分割概述</a></li><li id="07a3" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz lk ll lm ln bi translated"><a class="ae kg" href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47" rel="noopener" target="_blank">用UNET理解语义分割</a></li></ul><p id="f30a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Ronneberge等人。</p><p id="7d8c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kg" href="https://en.wikipedia.org/wiki/U-Net" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/U-Net</a></p></div></div>    
</body>
</html>