<html>
<head>
<title>The Johnson-Lindenstrauss Lemma in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的Johnson-Lindenstrauss引理</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-johnson-lindenstrauss-lemma-ef10698d0dc6?source=collection_archive---------4-----------------------#2022-06-13">https://medium.com/mlearning-ai/the-johnson-lindenstrauss-lemma-ef10698d0dc6?source=collection_archive---------4-----------------------#2022-06-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/db0af2820e8252c4818b3b23689bdece.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/1*FGC_qP7wBbWNuEWctZkvQA.gif"/></div></figure><div class=""/><div class=""><h2 id="151f" class="pw-subtitle-paragraph il hn ho bd b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc dx translated">如何使用Python利用JL引理？</h2></div><p id="55c3" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">本文讨论了降维领域的一个惊人成果。毫无疑问，这是关于欧几里德域中维数约减的最有力和令人满意的陈述之一。它是以数学家威廉·b·约翰逊(1944—)和乔拉姆·林登·施特劳斯(1936—2012)的名字命名的，他们在1984年证明了这一点。</p><h2 id="47e2" class="jz ka ho bd kb kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt bi translated">内容</h2><ul class=""><li id="f351" class="ku kv ho jf b jg kw jj kx jm ky jq kz ju la jy lb lc ld le bi translated">JL引理暗示了什么？</li><li id="6369" class="ku kv ho jf b jg lf jj lg jm lh jq li ju lj jy lb lc ld le bi translated">JL引理的形式定义。</li><li id="5196" class="ku kv ho jf b jg lf jj lg jm lh jq li ju lj jy lb lc ld le bi translated">Python中的工作示例。</li></ul><h2 id="d0d3" class="jz ka ho bd kb kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt bi translated">JL引理暗示了什么？</h2><blockquote class="lk ll lm"><p id="fabd" class="jd je ln jf b jg jh ip ji jj jk is jl lo jn jo jp lp jr js jt lq jv jw jx jy ha bi translated">直觉上，JL引理是这样说的:<strong class="jf hp">如果你选择一个随机子空间并投影到它上面，点之间的成比例的成对距离将<em class="ho">可能</em>被保留</strong>。</p></blockquote><p id="787a" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">不管你有什么样的点集，这都是正确的。想知道为什么，考虑一下我画的这个玩具例子。(注意，选择这个例子是因为我们可以可视化三维，而不是因为JL引理的数学在这里有用！仅仅是为了直觉<em class="ln"/>。)</p><div class="lr ls lt lu fd ab cb"><figure class="lv hj lw lx ly lz ma paragraph-image"><img src="../Images/8afef0ceeb1eafa240befd4c4e6d7591.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*BZinUaM_ApVM-NvY7zEGNA.png"/></figure><figure class="lv hj lw lx ly lz ma paragraph-image"><img src="../Images/e6f2e26665345bc084c315650e2a7483.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*RThsPnZtS_3BR5Ic2FTOpA.png"/><figcaption class="mb mc et er es md me bd b be z dx mf di mg mh">One the left we have an optimal PCA projection, and on the right we have a random projection. Both are projected from dimension 3 to dimension 2.</figcaption></figure></div><p id="4aa3" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">你会注意到，在右边的例子中，有些飞机似乎比其他飞机“更好”。你也许可以把右边的点集投影到任何平面上，几乎可以肯定它也是一样糟糕的。然而，左边的数据似乎靠近一个平面，因此“靠近”数据的平面似乎“不太坏”</p><p id="1a9c" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">因此，一方面，JL引理表明成对距离不可能被扭曲。另一方面，几何学告诉我们一些投影比其他的“更好”。这种不匹配揭示了随机投影的一个有趣的事实:</p><ul class=""><li id="1b09" class="ku kv ho jf b jg jh jj jk jm mi jq mj ju mk jy lb lc ld le bi translated"><strong class="jf hp">成对距离并不能揭示降维的全部内容。</strong>JL引理无法解释为什么右边数据集中的一些投影比其他投影差。它告诉我们的是，成比例的成对距离没有过度扭曲。</li><li id="180b" class="ku kv ho jf b jg lf jj lg jm lh jq li ju lj jy lb lc ld le bi translated"><strong class="jf hp">不过，还是挺有用的。</strong>例如，如果您正在运行近似最近邻或其他方法，您可以选择一个随机投影，在保持大部分正确的同时显著降低维度。</li></ul><p id="3c5c" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">因此，在某些方面，JL引理似乎是正确的，因为成对距离对降维并不像我们希望的那样重要。尽管如此，有趣的是，它们对随机投影如此抗拒，这值得我们去探究为什么。</p></div><div class="ab cl ml mm go mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ha hb hc hd he"><h2 id="d573" class="jz ka ho bd kb kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt bi translated">JL引理的形式定义</h2><figure class="lr ls lt lu fd hj er es paragraph-image"><div class="er es ms"><img src="../Images/b2bdb8a97c31715ce84e6de3f46a570c.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/1*y1xC3_qN7d8QVHZCg9Oryw.gif"/></div></figure><figure class="lr ls lt lu fd hj er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es mt"><img src="../Images/f666a2f6538f1e2b81c06f9e55038c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrEgonQBbWTE7MA2zWiXkg.png"/></div></div></figure><p id="26c3" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">基本上，这表明大约。对于一些<em class="ln"> k </em> &gt;对数(<em class="ln"> n </em> )/ <em class="ln"> β </em>，其中<em class="ln"> n </em>是点数，<em class="ln"> β </em>是误差容限，很有可能地图<em class="ln"> f </em>不会将任意两点之间的成对距离改变超过<em class="ln"> </em> (1 <em class="ln"> β </em>的一个因子)。</p><p id="c82c" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">关于JL引理，有几件事要记住，它可能会帮助你确定它何时适用于你的问题。</p><ul class=""><li id="91e5" class="ku kv ho jf b jg jh jj jk jm mi jq mj ju mk jy lb lc ld le bi translated">它声称将高空间降低到“中等”空间。对于极小的空间，如1维空间，这种方法不太适用。</li><li id="0728" class="ku kv ho jf b jg lf jj lg jm lh jq li ju lj jy lb lc ld le bi translated">JL引理指出，我们对<em class="ln"> k </em>(记住，我们映射到一个更低的维度<em class="ln"> k </em>)的选择应该仅仅基于点的数量<em class="ln"> n </em>和我们的误差容限<em class="ln"> β </em>。</li></ul><h2 id="990f" class="jz ka ho bd kb kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt bi translated">Python中的工作示例</h2><p id="9497" class="pw-post-body-paragraph jd je ho jf b jg kw ip ji jj kx is jl jm my jo jp jq mz js jt ju na jw jx jy ha bi translated">查看我的Colab笔记本，获取完整代码。T11】</p><figure class="lr ls lt lu fd hj er es paragraph-image"><div class="er es nc"><img src="../Images/e46169219c06de7270771d2f8dfa3097.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*lmGZ2TuAbhFKI5SrxuyEtw.gif"/></div></figure><p id="bc5f" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">首先，我们采样5个随机数据点，其huuuuuuuuuuge维度为<em class="ln"> 10000 </em>。</p><pre class="lr ls lt lu fd nd ne nf ng aw nh bi"><span id="dfa6" class="jz ka ho ne b fi ni nj l nk nl">import numpy as np<br/>from sklearn import random_projection</span><span id="c3c5" class="jz ka ho ne b fi nm nj l nk nl"># init the number of data points n and sample the random data points<br/>n = 5<br/>X_large = np.random.rand(n, 10000)<br/>X_large.shape</span></pre><p id="fff6" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">接下来，我们计算JL引理对我们的数据集成立所需的最小维数。</p><pre class="lr ls lt lu fd nd ne nf ng aw nh bi"><span id="7882" class="jz ka ho ne b fi ni nj l nk nl">from sklearn.random_projection import johnson_lindenstrauss_min_dim</span><span id="ff69" class="jz ka ho ne b fi nm nj l nk nl"># calculate mininimum number of random projection required by JL<br/>min_dim = johnson_lindenstrauss_min_dim(n_samples=n, eps=0.1)</span><span id="a37b" class="jz ka ho ne b fi nm nj l nk nl">##### min_dim = 1379</span></pre><p id="5436" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">现在我们可以执行随机投影。</p><pre class="lr ls lt lu fd nd ne nf ng aw nh bi"><span id="1383" class="jz ka ho ne b fi ni nj l nk nl"># perform random projection<br/>transformer = random_projection.GaussianRandomProjection(n_components=min_dim)<br/>X_small = transformer.fit_transform(X_large)<br/>X_small.shape</span></pre><p id="f175" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">最后，我们计算高维数据集和低维数据集的每个点之间的欧几里德距离。</p><pre class="lr ls lt lu fd nd ne nf ng aw nh bi"><span id="ca46" class="jz ka ho ne b fi ni nj l nk nl">from sklearn.metrics.pairwise import euclidean_distances</span><span id="601d" class="jz ka ho ne b fi nm nj l nk nl"># euclidean distance between points in original large dimensions<br/>euclidean_distances(X_large, X_large)</span><span id="518d" class="jz ka ho ne b fi nm nj l nk nl">##### That gives the following point-pairwise distances<br/>[[ 0.        , 40.62866384, 40.98203556, 40.68316198, 40.66748849],        [40.62866384,  0.        , 41.23296197, 40.42156429, 40.62927887],        [40.98203556, 41.23296197,  0.        , 41.29828264, 41.06729139],        [40.68316198, 40.42156429, 41.29828264,  0.        , 40.99215274],        [40.66748849, 40.62927887, 41.06729139, 40.99215274,  0.        ]]</span><span id="f106" class="jz ka ho ne b fi nm nj l nk nl"># euclidean distance between points in reduced small dimensions<br/>euclidean_distances(X_small, X_small)</span><span id="07b7" class="jz ka ho ne b fi nm nj l nk nl">###### That gives the following point-pairwise distances<br/>[[ 0.        , 40.22947225, 40.76984244, 41.23327281, 40.23943891],        [40.22947225,  0.        , 40.53902276, 41.5846761 , 40.20033872],        [40.76984244, 40.53902276,  0.        , 41.55588154, 42.22873298],        [41.23327281, 41.5846761 , 41.55588154,  0.        , 42.20924386],        [40.23943891, 40.20033872, 42.22873298, 42.20924386,  0.        ]]</span></pre><p id="94f9" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">正如我们所见，欧几里得距离保留得相当好！</p></div><div class="ab cl ml mm go mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ha hb hc hd he"><p id="6ee2" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jf hp">关注我:<br/> </strong> <a class="ae nb" href="https://twitter.com/r3d_robot" rel="noopener ugc nofollow" target="_blank">推特:@ r3d _ robot</a><br/><a class="ae nb" href="https://www.youtube.com/channel/UC-47UN9znQBo3ItNj8Ghspw/featured" rel="noopener ugc nofollow" target="_blank">Youtube:r3d _ robot</a></p><figure class="lr ls lt lu fd hj er es paragraph-image"><div class="er es nn"><img src="../Images/94e3beac15b99bfdaa1f8305bf732a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/1*B8PTx0CAi96MihD_vfDpZQ.gif"/></div></figure><div class="hg hh ez fb hi no"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hp fi z dy nt ea eb nu ed ef hn bi translated">Mlearning.ai提交建议</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc hk no"/></div></div></a></div></div></div>    
</body>
</html>