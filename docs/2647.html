<html>
<head>
<title>4 Value function methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4价值函数方法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/4-value-function-methods-17f2c898a00d?source=collection_archive---------3-----------------------#2022-05-26">https://medium.com/mlearning-ai/4-value-function-methods-17f2c898a00d?source=collection_archive---------3-----------------------#2022-05-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d68f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我总结Sergey Levine教授主持的CS285讲座系列文章的第四篇，所有的荣誉都归于他。所有图片均取自他的讲座。<a class="ae jc" href="https://samuelebolotta.medium.com/1-an-introduction-to-deep-reinforcement-learning-c5ab792af013" rel="noopener">我写的这篇文章</a>是对深度强化学习的介绍。<a class="ae jc" href="https://samuelebolotta.medium.com/3-actor-critic-algorithms-779f14465b74" rel="noopener">演员-评论家算法</a>建立在我们在<a class="ae jc" href="https://samuelebolotta.medium.com/2-deep-reinforcement-learning-policy-gradients-5a416a99700a" rel="noopener">这篇文章</a>中讨论的政策梯度框架之上。最重要的是，它们还增加了学习价值函数和Q函数。</p><h2 id="af5f" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">可以完全省略政策梯度吗？</strong></h2><p id="401e" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">如果我们只是学习一个价值函数，然后尝试使用这个价值函数来计算如何行动呢？为什么这应该是可能的直觉是，价值函数告诉我们哪些状态比其他状态更好，所以如果我们简单地选择进入更好状态的动作，也许我们不再需要显式的策略神经网络。</p><p id="fb7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是让这种直觉更正式一点的方法。优点是:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/37e8dde9ac3f91255e59e00b8eabaef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*Pn8TY4wKj04ftvBwTn6efg.png"/></div></figure><p id="ec32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是我们的Q值和我们的值之间的差，直观上，优势表明这个动作比根据策略的平均动作好多少。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kl"><img src="../Images/b777bbb5b01c22d3e72ddb7f51f816e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*6sTz7Reda0j6PSalg7UI_g.png"/></div></figure><p id="b250" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么arg max是我们可以采取的最佳行动，如果我们在此后遵循该策略的话。这意味着argmax将至少与我们从当前策略中抽取的动作一样好。我们知道它至少一样好，因为它实际上是最好的。有趣的是，不管政策实际上是什么，这都是真的。这个argmax应该立即向我们建议，不管我们以前有哪种策略，即使它是一个非常糟糕的随机策略，我们也应该能够通过根据优势的arg max选择行动来改进它。</p><p id="4276" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也许我们可以忘记显式表示策略，我们可以只使用这个argmax。这是基于价值的方法的基础。我们将隐式地构造新的策略，因此在每次迭代中，我们可以构造一个新的策略，如果动作a是优势的argmax，则为其分配概率1，否则为0。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es km"><img src="../Images/b2dcbb10945b9a182c1e60f82d176dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*KWTc577bJWkrRruk2iWAWQ.png"/></div></figure><p id="8907" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们不再对显式策略进行梯度上升，而是将隐式策略构造为arg max，这样就不会发生实际的学习。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kn"><img src="../Images/78639cc5490cf924e2b99a4c53336b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*2HXd3xKD1EH_42A6dvj52Q.png"/></div></figure><p id="cbf9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这被称为策略迭代。它被称为策略迭代，因为我们在第一步评估策略和第二步更新策略之间迭代。第二步非常简单，但是最大的难题是如何完成第一步。对于给定的先前策略，如何评估特定状态-动作元组的优势？</p><p id="deba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像之前一样，我们可以将优势表示为奖励加上gamma乘以下一时间步的期望值减去当前时间步的值。我们来评估一下价值。</p><h2 id="390a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">动态编程</strong></h2><p id="9352" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">为了评估策略迭代的这些优势，评估价值的一种方法是使用动态编程。现在让我们假设我们知道转移概率，让我们假设状态和动作都是离散的和小的。这不是我们通常在无模型RL中操作的设置，但我们现在假设这是我们的设置，以便我们可以导出简单的动态编程算法，然后在稍后将其转换为无模型算法。</p><p id="6754" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以想象，我们基本上可以枚举我们的整个状态和动作空间——我们可以用一个表格来表示它。例如，在这个gridworld中，有16个状态，每个状态有4个动作。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ko"><img src="../Images/5031700dd7da856d2975a70ca007cf62.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*m0XRVhDqi_YZgN7w-sIjIQ.png"/></div></figure><p id="bdfb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你的转移概率T用一个16乘16乘4的张量来表示。所以，当我们说我们在做表格强化学习或者表格动态编程的时候，我们真正指的是这样一个设置。</p><p id="1603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们可以根据这些显式的已知概率，写下上一篇文章中看到的value函数的引导更新。如果我们想要更新状态的值，我们可以将它设置为从奖励策略中采样的动作的期望值加上gamma乘以从下一个状态的值的转移概率中采样的下一个状态的期望值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kp"><img src="../Images/bd38d70de2c75e0714be8847ebb5a013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*MHRbKbz1wpLcSZabagifag.png"/></div></figure><p id="b34e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您有一个表格形式的MDP，这意味着您有一个小的离散状态空间，并且您知道转移概率，则每个期望值可以通过对该随机变量的所有值求和，然后将括号内的值乘以其概率来计算。当然，我们需要知道下一个状态的值，所以我们只需要使用当前对这个值的估计。一旦我们以这种方式计算了一个价值函数，那么我们就可以通过将概率1分配给作为优势的argmax的动作来构造一个更好的策略pi素数。这也意味着我们的策略将是确定性的，因此关于这个策略的期望值将很容易计算。我们可以通过移除关于策略的期望并直接插入唯一具有非零概率的动作来简化我们的引导更新:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kq"><img src="../Images/603fba22df9024659b88323b3a7f6a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*apV8-LLF5awUZ6VhbnlQbg.png"/></div></figure><p id="b16d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们可以将这个过程插入到我们的策略迭代算法中，特别是在步骤1中，当我们评估状态的值时。你可以证明重复这个递归最终收敛到一个不动点并且这个不动点就是真值函数。</p><h2 id="ef9c" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">甚至更简单的动态编程</strong></h2><p id="791c" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">您可能注意到了策略评估的工作方式:值在每次迭代中一致地传播，但是很慢。但是，即使我们在单次迭代后截断策略评估，我们仍然可以通过在策略评估的单次状态空间扫描后采用Q函数估计的贪婪策略来改进初始策略。这个算法是RL中的另一个基本算法:它被称为值迭代(VI)。VI可以被认为是“贪婪地修改政策”，因为我们尽可能快地贪婪地计算贪婪的政策。VI不会等到我们对策略有了一个准确的估计之后才进行改进，而是在一次状态扫描之后就终止了策略评估阶段。</p><p id="5686" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从数学上讲，这个想法就是优势的argmax等于取Q函数的argmax。因为如果你去掉状态的值，你可以这样做，因为argmax是关于动作的，那么你只得到Q函数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kr"><img src="../Images/3366dac8b12e6ed1a30252c6db5f7d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*UHeYivRkGP8QcWJHqe3i2g.png"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ks"><img src="../Images/2ee144d62670f7bdd257b500b3a3458f.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*x2lCFvN9yn9glILNTRmiCg.png"/></div></figure><p id="069e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以直接跳过恢复索引并获取值的步骤，而不是从优势的argmax中获取索引，然后将其插入Q函数以获取最高值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kt"><img src="../Images/b8d5436eabadc2f8e94cd463130b0226.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*ISHY-bmchh-XMfBBmTQtwg.png"/></div></figure><p id="e3c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一步中，我们将Q值设置为奖励加上下一时间步的价值函数的期望值，然后在第二步中，我们将价值函数设置为Q函数表中动作的最大值。这里，跳过了显式策略计算，我们实际上从来没有显式地表示过策略，但是您可以将其视为在步骤2中隐式显示，因为将值设置为Q值表中操作的最大值类似于获取argmax，然后将argmax的索引插入表中以恢复值。但是因为取argmax然后把它插入到表中与只取max是一样的，我们基本上可以简化这个步骤，得到这个过程</p><h2 id="d966" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">拟合值迭代和Q迭代</strong></h2><p id="5528" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">到目前为止，我们讨论了如何学习以表格形式表示的值函数，所以没有神经网络。现在让我们来谈谈如何引入神经网络和函数逼近</p><p id="7bc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们如何表示价值？到目前为止，我们讨论了如何将它表示为一个大表。然而，让我们假设你在玩一个基于图像的视频游戏:在这个视频游戏中，如果你有一个200乘200像素的图像，可能的状态数是255，这是每个像素可以接受的值的数量，因为有三个颜色通道，所以它的三次方是200的200次方。维护一个包含这么多条目的表是不可能的:这比宇宙中的原子数量还要多。这就是众所周知的维数灾难:一个简单的事实是，如果你有一个多维的状态空间，你在表格强化学习中需要的条目数量是维数的指数。</p><p id="6ed1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我们会用一个函数逼近器。我们将有一个从状态映射到标量值的神经网络净值函数。我们可以通过对目标值进行最小二乘回归来拟合我们的神经网络净值函数，如果我们使用上一节中的值迭代过程，那么我们的目标值就是Q函数作用的最大值。</p><p id="e531" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么我们的拟合值迭代算法将如下所示:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ku"><img src="../Images/08c2184de4c2c763bac5888441b87384.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*FxyygkZ2y3sVYlghTmllFA.png"/></div></figure><p id="c680" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一步，通过为每个采样状态下的每个可能的动作构造Q函数来计算你的目标值(我们仍然假设我们有一个离散的动作空间，所以我们可以精确地执行这个枚举)；对于每一个动作，我们把它的回报加上gamma乘以下一个状态的期望值。做最大值，得到我们的目标值，然后在第二步回归到这些目标值。</p><p id="0851" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个我们可以使用的合理算法，但它仍然需要我们知道跃迁动力学:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/605a4203386e38dfd3d35d9c028b603b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOPyVVe4CcAZxuAv_fA63w.png"/></div></div></figure><p id="66c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有两种方式需要转变动力学的知识。首先，它需要能够计算预期值，也许更重要的是，它需要我们能够从同一个状态尝试多个不同的操作，如果我们只能在环境中运行策略，而不是多次传送到一个状态并从同一个状态尝试多个操作，我们通常无法做到这一点</p><p id="514b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们回到政策迭代。在策略迭代中，我们交替评估Q函数，然后将策略设置为贪婪argmax策略。策略迭代中的第一步涉及策略评估，这需要重复应用这个价值函数递归，正如我们之前看到的。那么，如果我们不是应用价值函数递归来学习价值函数，而是以类似的方式直接构造Q函数递归，会怎么样呢？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es la"><img src="../Images/81f84f3da96e53fd29f054a459efbf7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*ONVXObo6diraYhO64hWT7A.png"/></div></figure><p id="60ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我想在一个特定的状态-动作元组中构造Q函数，我可以写完全相同的递归，除了现在因为Q函数是一个状态和动作的函数，我不需要在给定状态和策略的情况下计算下一个状态，我只需要在给定状态-动作元组的情况下计算下一个状态，我正在训练我的Q函数。乍一看，这似乎是一个非常微妙的区别，但这是一个非常重要的区别，因为现在随着我的策略改变，我需要对下一个状态进行采样的操作实际上并没有改变，这意味着如果我有一堆样本(状态、操作、下一个状态)，我可以使用这些样本来拟合我的Q函数，而不管我有什么策略。策略出现的唯一地方是作为下一个状态的Q函数的参数，在期望值内。事实证明，这个看起来非常简单的变化允许我们执行策略迭代式的算法，而不需要实际了解转换动态，只需要对一些元组进行采样，我们可以通过运行任何我们想要的策略来获得这些元组。如果我们在策略迭代的第一步这样做，我们将不再需要知道转移概率。</p><p id="f880" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们能再玩一次“麦克斯”的把戏吗？</p><p id="f4d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们似乎后退了一步，因为在我们推导策略迭代之前，我们简化了它，得到了值迭代，我们得到值迭代的方法是使用这个最大值技巧。在值迭代中，我们看到，当我们构建策略时，我们采用argmax，但我们只是采用argmax操作的值，因此评估arg max的值就像采用max一样，因此我们可以放弃第二步，直接执行值迭代。</p><p id="a90c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以用Q函数做同样的max技巧吗？这是我们之前做的:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lb"><img src="../Images/2c59854151809a5ad31e627ae23ca60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qolb-lzkmLUqFnzsQ_-Sw.png"/></div></div></figure><p id="243e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们构造拟合Q迭代算法的方式非常类似于拟合值迭代。我们将我们的目标值构造为奖励加上gamma乘以下一个状态的价值函数的期望值。然后，在第二步中，我们简单地将Q函数回归到这些目标值上:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es la"><img src="../Images/df6c6e58d0608e40e5723fb7d06ab88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*uCl_FQaCveHG2axWP1Ty_A.png"/></div></figure><p id="4c22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，诀窍是我们必须在不知道转移概率的情况下评估第一步。我们要做两件事:首先，我们要用Q函数中动作的最大值替换下一个状态的值:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lc"><img src="../Images/0e0b31147d6835e63f14a37a840597b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Lqno6oiWS5j2umG4uGKlkg.png"/></div></figure><p id="eb66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是因为我们只是在逼近Q函数，而不是价值函数。</p><p id="64f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二，我们不是对所有可能的下一个状态进行完全预期，而是使用生成样本时得到的采样状态。现在，我们运行该算法所需要的只是样本(状态、动作、下一个状态)，这些样本可以通过推出我们的策略来构建。这不需要模拟不同的操作，只需要您上次运行策略时实际采样的操作。</p><p id="a78e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这甚至适用于不符合策略的样本，因此该算法不做任何假设，即动作实际上是从最新的策略中采样的，动作可能是从任何地方采样的。您可以存储迄今为止收集的所有数据，它不需要来自您的最新政策。</p><p id="bcd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不幸的是，这个过程对于非线性函数逼近没有任何收敛保证；然而，如果你使用表格表示，它实际上是保证收敛的。</p><p id="78e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将这些片段放在一起，这是拟合的Q迭代算法:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ld"><img src="../Images/b1669339de621b4f98c10098a33d0f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abOKrLEfZ7oyFgg4qAvChg.png"/></div></div></figure><p id="c439" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该算法适用于各种不同的策略。您必须选择的参数之一是您要收集的这种转换的数量。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es le"><img src="../Images/36dd5eb49e44eedae194cc4e2f96a0ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WoxSjKoT98kvPn80-K0tSQ.png"/></div></div></figure><p id="23c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二步，对于你采样的每个转换，计算一个目标值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lb"><img src="../Images/4ee54be9ab14ac557ad78988c7b71763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nefhS_9NqpjK7uviRn4YUg.png"/></div></div></figure><p id="4b0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第三步，训练一个新的Q函数，这意味着通过最小化Q的值与对应的目标值之间的差来找到一个新的参数向量。您必须选择的一个参数是在执行优化时您将进行的梯度步骤的数量。做一次第三步实际上并不能得到最好的Q函数；在出去收集更多的数据之前，你可以将第二步和第三步交替进行几次。</p><h2 id="cf9e" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">从Q迭代到Q学习</h2><p id="f1af" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">让我们再多谈一点，拟合Q迭代作为一种非策略算法意味着什么。提醒一下:不符合策略意味着您不需要来自最新策略的样本来继续运行您的RL算法。通常，这意味着您可以对同一组样本采取许多梯度步骤，或者重用来自以前迭代的样本，因此您不必丢弃旧样本，并且可以继续使用它们，这实际上为您提供了更多的训练数据。</p><p id="a5a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直观地说，fitted-Q迭代允许我们使用非策略数据的主要原因是，使用策略的一个地方是利用Q函数，而不是通过模拟器步进。所以随着我们政策的改变，真正改变的是这个最大值:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lf"><img src="../Images/f31efa45de0a166387c09006b37841ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*6_eUDTG76_fXplAFpBDUNw.png"/></div></figure><p id="2a1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，我们获得这个最大值的方法是获取argmax，然后将其插回到Q值中，以获得策略的实际值。因此，在max内部，您可以对其进行解包，并假设它是下一个状态的Q函数和Q的arg max。arg max基本上是我们的策略，因此这是策略显示的唯一位置，并且非常方便地显示为Q函数的参数，这意味着随着我们的策略发生变化，我们不需要生成新的部署，您几乎可以将此视为一种模型。Q函数可以让你模拟，如果你采取不同的行动，你会得到什么样的值，当然，如果你想最大程度地改善你的行为，你会采取最好的行动。这个最大值近似于我们在下一个状态的贪婪策略的值。</p><p id="a5da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以从结构上考虑拟合Q迭代的一种方式是，你有一个不同跃迁的大桶，你要做的是备份每个跃迁的值，每个备份都会提高你的Q值，但你真的不太关心具体是哪些跃迁，只要它们能很好地覆盖所有可能跃迁的空间。所以你可以想象你有这个跃迁的数据集，你只是在这个数据集上不停地工作，运行拟合的Q迭代，每次循环时改进你的Q函数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lg"><img src="../Images/2a995f27b3984ed422962aaf7322ec7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*T-0c_dJRU6OE3S84Sl6a_w.png"/></div></figure><h2 id="cbc5" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">什么是拟合Q迭代优化？</strong></h2><p id="584b" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">取最大值的步骤改进了您的策略，因此在表格中，这实际上是您的策略改进。您的第三步是最小化拟合误差，因此如果您有一个表格更新，您只需将这些目标直接写入您的表中</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lh"><img src="../Images/3b32fbce0c0b1b6380dbeba0f6aac715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OmooJ-ZElK-q4KUC_3ycGg.png"/></div></div></figure><p id="d056" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，因为你有一个神经网络，你必须执行一些优化，以尽量减少对这些y的误差。您可以将拟合Q迭代视为优化一个误差，该误差是贝尔曼误差——Q函数和目标之间的差异。Tat是最接近实际优化目标的一种，但当然，误差本身并不真正反映您策略的好坏，它只是您能够复制目标值的精确度。如果误差为零，那么你知道这是一个最优Q函数，但如果误差不为零，那么你真的不能说这个策略的性能。</p><p id="19f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们来讨论几个特例。</p><h2 id="794f" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">在线Q学习算法</strong></h2><p id="55e8" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">您可以用对应于在线算法的那些超参数的特定选择来实例化先前算法的特例。在在线算法中，你正好采取一个动作并观察一个转变；然后在第二步中，你为你刚刚进行的过渡计算一个目标值，非常类似于你如何在在线演员-评论家中为你刚刚进行的一个过渡计算优势值；然后在第三步，你对你的Q值和你刚刚计算的目标值之间的误差进行梯度下降。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es li"><img src="../Images/35b4c4cba2b074b725453d0f5e1d3655.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*k49aGbnGWZ4vEb7tYvxCrQ.png"/></div></figure><p id="8b63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我这里的等式看起来有点复杂，但我基本上只是将概率链规则应用于第三步argmin中的目标。这些括号中的误差有时被称为时间差误差。</p><h2 id="0875" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">用Q-learning探索</strong></h2><p id="73c0" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">在学习过程中，使用贪婪策略可能不是一个好主意。原因是我们的argmax策略是确定性的，如果我们的初始Q函数很差，它就不是随机的，而是任意的。然后，它将在每次进入特定状态时，提交我们的argmax策略来采取相同的操作，如果该操作不是一个非常好的操作，我们可能会永久地采取那个糟糕的操作，并且我们可能永远不会发现更好的操作存在。因此，在实践中，当我们运行fitted Q迭代时，非常需要修改我们在第一步中使用的策略，不仅是argmax策略，而且要注入一些额外的随机性以产生更好的探索。我们在实践中做出了许多选择来促进这一点。</p><p id="2182" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个常见的选择叫做ε贪婪；这是一个最简单的探索规则，我们可以用在离散的行动中，它简单地说，概率为1减去ε，你会采取贪婪的行动，然后概率为ε，你会随机采取其他行动中的一个。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lj"><img src="../Images/ac7d2cca513a9ec8bf40e29f3b9d6109.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*_FgAIncJZ1ySH-nvaA7QLg.png"/></div></figure><p id="294c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们选择ε是某个小数字，这意味着大多数时候我们采取我们认为最好的行动，这通常是一个好主意——但我们总是有很小但非零的概率采取其他行动，这将确保如果我们的Q函数不好，最终我们会随机做一些更好的事情。一个非常常见的实际选择是在训练过程中改变ε的值，这很有意义，因为你预计Q函数最初会很差，此时你可能希望使用更大的ε。然后随着学习的进展，你的Q函数变得更好，然后你可以减少ε</p><p id="073b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以使用的另一个探索规则是玻尔兹曼探索，来选择你的行动，与你的Q值的一些正向转换成比例。一个特别流行的正变换是取幂。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lk"><img src="../Images/637fb7ba6305856f61e18dad30bd45e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*OU1KT9AurA1Hi3ovjkp_Iw.png"/></div></div></figure><p id="53b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以如果你采取的行动与Q值的指数成正比，将会发生的是，最佳行动将是最频繁的，几乎与最佳行动一样好的行动也将被频繁采取，因为它们有相似的高概率。但是如果某个行为的Q值极低，那么它几乎永远不会被采取。在某些情况下，这种探索规则可以优于ε贪婪，因为1)对于ε贪婪，恰好是最大值的动作获得高得多的概率，并且如果有两个动作几乎一样好，则第二好的动作具有低得多的概率，而对于这种求幂规则，如果你真的有两个同样好的动作， 如果你有一个非常糟糕的行为，并且你已经知道这只是一个非常糟糕的行为，你可能不想浪费时间去探索它，而epsilon greedy不会利用这一点。</p><h2 id="7e44" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">总结</strong></h2><p id="4a22" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我们已经讨论了基于值的方法，这些方法不明确地学习策略，而只是学习一个值函数或Q函数。我们已经讨论了如果您有一个值函数，您如何使用argmax恢复策略，以及我们如何设计这种拟合的Q迭代方法，这种方法不需要了解转换动态，因此它是一种真正的无模型方法，我们可以以各种方式将其实例化为批处理模式、非策略方法或在线Q学习方法。</p><h2 id="a2f3" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">理论上的价值函数</strong></h2><p id="3aa8" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">现在让我们深入一点理论来解释我之前说的基于值的神经网络方法通常不会收敛到最优解是什么意思。首先，让我们从之前讨论过的值迭代算法开始。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ll"><img src="../Images/b91c20aec43a9333326c3b916c09e42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*1FyxRPms4XURZNKxnOYfEA.png"/></div></div></figure><p id="44b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个非常简单的算法，对我们来说更容易思考，但我们稍后会回到Q迭代方法。提醒大家，在价值迭代中，我们可以认为它有两个步骤。第一步，构建你的Q值表，作为奖励加上gamma乘以下一个状态的期望值；第二步，将你的值函数设置为表中各行的最大值。所以你可以把它想象成构造一个值表，然后迭代这个过程。我们可以问的问题是:这个算法收敛吗？如果它收敛了，它会收敛到什么？我们可以从这个分析开始的方法之一是，我们可以定义一个贝尔曼算子:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lm"><img src="../Images/b2f3458746c63fb50a5533288bc8464e.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*ibvxl_a26xZdNj0nvfPH6A.png"/></div></div></figure><p id="6c81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当应用于值函数时，该运算符执行以下操作:</p><ul class=""><li id="68b2" class="ln lo hh ig b ih ii il im ip lp it lq ix lr jb ls lt lu lv bi translated">首先，它采用V并应用运算符T下标a，这是一个维数为SxS的矩阵，其中该矩阵中的每个条目都是给定当前状态和动作的下一个状态的概率，并且根据该最大值选择动作。这基本上就是计算期望值，这是一个线性算子。</li><li id="59c8" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">我们把它乘以伽玛</li><li id="6fb7" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">我们添加了向量r下标a，这是一个奖励向量，对于每个状态，你为相应的动作选择奖励，然后在此之外，你对该动作执行最大值。重要的是，这个最大值是每个元素的，所以对于每个状态，我们取一个最大值</li></ul><p id="f4c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种编写贝尔曼备份的有趣方式基本上只是抓住了值迭代算法，它包括对向量V重复应用运算符B</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mb"><img src="../Images/349044a179775d04027987a3cccf2b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWnCezusiVGX0DDU1vXNxw.png"/></div></div></figure><p id="2215" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以证明的一个有趣的性质是v星(最优策略的价值函数)是b的不动点。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mc"><img src="../Images/d85fbc8080ce1dc1f43a5bf90ea7ea2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*OjrTtbb0n-U2wFgvdILykg.png"/></div></figure><p id="1f56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们找到满足这个等式的价值函数，我们就有了最优价值函数，如果我们使用arg max策略，我们将得到最优策略，即最大化总回报的策略。这意味着V星等于B乘以V星。这很好:如果我们找到B的一个不动点，那么我们就有了最优值函数。此外，也有可能证明v星一直存在。这个不动点总是存在的，它总是唯一的，并且总是对应于最优策略。所以我们剩下的唯一问题是:对V重复应用B，真的能找到这个固定点吗？换句话说，不动点迭代算法收敛吗？如果它收敛，它将收敛到最优策略，并且它有唯一的解。我不会详细讨论这个证明，但是我们论证值迭代收敛背后的高层次草图是通过论证它是一个收缩。我们可以证明值迭代达到v星是因为B是收缩。收缩是什么意思？意思是如果你有任意两个向量V和v bar，那么对V和v bar都应用B会让那些向量更靠近，意思是BV减去B * V bar的范数小于或者等于V减去V bar的范数。事实上，这是某种系数的收缩。那个系数恰好是γ，所以不仅B*V减B * V杠的范数小于或等于V减V杠的范数，而且也小于或等于V减V杠的范数乘以γ。所以，你会收缩，你会收缩一个不小的量，这意味着，当你对它们施加B时，V和V杠会越来越近</p><h2 id="c475" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">非表格值函数学习</strong></h2><p id="7ed0" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">正则值迭代可以非常简洁地写成重复应用这一步:V到BV。</p><p id="2597" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们去拟合值迭代算法。拟合值迭代算法有另一个操作；它有一个第二步，您可以根据参数执行arg min。你可以认为监督学习的一种方式是，你有一些可以表示的价值函数。该集合是一个连续的集合，由所有可能的具有特定架构的神经网络组成，但具有不同的权重值。所以我们将集合表示为集合ω。在监督学习中，我们有时称之为假设集或假设空间。监督学习包括在你的假设空间中找到一个优化你的目标的元素，我们的目标是状态值和我们的目标值之间的平方差。现在，我们的目标值是多少？我们的目标价值基本是BV。你可以把整个拟合值迭代算法想象成反复寻找一个新的值函数v prime，它是v prime和BV的平方差的集合omega内部的自变量，其中BV是你之前的值函数</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es md"><img src="../Images/c2aecf55578292700b4ad363401edc46.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*DDZw-k47jeiuA0VLUrSDXA.png"/></div></figure><p id="a6f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个过程本身也是一种收缩。当你执行这种监督学习时，你可以把它看作是L2规范中的一个投影。你有你的旧v，你有你的一组可能的神经网络，用这条线代表:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es me"><img src="../Images/52f06a36b79d5f75e48734c747968316.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*-ZJipr0MKd26fQS2QN2eIg.png"/></div></figure><p id="9aa9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ω基本上就是那条线上的所有点。当我们构造BV时，我们可能会离开这条线，所以点BV不在集合ω中。当我们执行监督学习时，当我们执行拟合值迭代的第二步时，我们真正做的是在集合ω中找到一个尽可能接近BV的点。“尽可能接近”意味着它将是一个直角，所以我们将向下投影到集合ω上，这将是一个直角投影。那会给我们带来v prime。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ll"><img src="../Images/00d6e2a8e31265e78e33efc9d8c36dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*cN99yScjTFNWf_fJAl_4Jg.png"/></div></figure><p id="434e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以将此定义为新运算符。我们可以称这个算符π为“投影”,我们会说πV就是这个目标的集合ω内的argmin，这就是L2范数。</p><p id="3f12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b是关于无穷范数的收缩。π是相对于L2范数的收缩。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mf"><img src="../Images/4d4fa7dc0a9d6e60c79e7851a096b33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1GWzt0R1Iw_P6KOKyywmkg.png"/></div></div></figure><p id="50a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">π是一个收缩的原因是，如果你在欧几里得空间中有任何两个点，并将它们投影到一条线上，它们只能彼此靠近，永远不能更远。不幸的是，πB不是任何形式的收缩。</p><p id="7bf6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想象蓝点是你的起点，黄星是最佳价值函数:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mg"><img src="../Images/20dddbb789db4946311aa59970777d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*f0D_Bt-t41lco7OuYaxnZQ.png"/></div></figure><p id="96c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你迈出一步，那么你的正则值迭代会逐渐离星星越来越近:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mh"><img src="../Images/e7d50f2702b01b4a0402e308d1f2af32.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*aO7x95xI_MM4efbMhEIshA.png"/></div></figure><p id="9e0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你有一个投影值迭代算法，一个拟合值迭代算法，那么你要把你的值函数限制在这条线的每一步，而你的贝尔曼备份会让你在无穷范数方面更接近恒星:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mi"><img src="../Images/00de5b7e93c12139d8459806824becfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*OFZ8y18lrMBchtYDofC2iQ.png"/></div></figure><p id="313f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后你的投影会把你移回到线上:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mj"><img src="../Images/bd5d379f5075f24414d4eb5c022513ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*dm9lAjPQ9No9RPrgNZHtow.png"/></div></figure><p id="aaa3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然这两个操作都是收缩，注意V撇现在实际上比V离恒星更远。你会遇到这样的情况，每一步都会让你离V星越来越远。</p><p id="4480" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从所有这些中得出的令人悲伤的结论是:</p><ul class=""><li id="00ac" class="ln lo hh ig b ih ii il im ip lp it lq ix lr jb ls lt lu lv bi translated">在表格情况下，值迭代确实收敛</li><li id="9312" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">拟合值迭代一般不收敛，在实践中也经常不收敛</li><li id="82b1" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">拟合Q迭代是一样的:πB不是任何形式的收缩。</li></ul><p id="e032" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是，有些人可能认为该算法中的步骤3可能只是梯度下降，它收敛于:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mk"><img src="../Images/d4b55d31c00dc47f8e2d31e53a1a901e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*QsAM7ofAS-dDjmuUGeJzDg.png"/></div></figure><p id="9fb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，Q-learning不是梯度下降。它不是在一个明确定义的目标上采取梯度步骤，这是因为Q-learning的目标值本身取决于Q值——但你没有考虑通过这些目标值的梯度。所以你实际使用的梯度不是一个定义明确的函数的真正梯度，这就是为什么它可能不收敛。现在可能值得一提的是，你可以把这个算法变成一个梯度下降算法，通过实际计算这些目标值的梯度。更大的问题是，由此产生的算法被称为残差算法，它具有非常差的数值特性，并且不能很好地工作。</p><h2 id="184b" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">一个可悲的推论</strong></h2><p id="fdb6" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">出于同样的原因，我们之前讨论的actor-critic算法也不能保证收敛于函数逼近。在这里，我们也做一个贝尔曼备份，当我们使用引导更新，我们做一个投影，当我们更新我们的价值函数。这些的串联不是收敛运算符，因此拟合的引导策略评估也不会收敛。</p><h2 id="4696" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">总结</strong></h2><p id="64d3" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我们讨论了一些值迭代理论，我们讨论了用于备份的算子，用于投影的算子；我们讨论了备份是如何收缩的，以及我们的表格值迭代如何收敛。我们讨论了函数逼近的一些收敛性质，其中投影也是一种压缩，但因为它是不同范数下的压缩，所以投影后的备份实际上不是压缩，因此拟合值迭代一般不会收敛。以及它对Q学习的启示。我们将在下一篇文章中发现，在实践中，我们实际上可以使所有这些算法工作得非常好，但它们的理论性质留给我们许多需要改进的地方。</p><p id="646e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ml">随时给我留言或者:</em></p><ol class=""><li id="9ec5" class="ln lo hh ig b ih ii il im ip lp it lq ix lr jb mm lt lu lv bi translated">通过<strong class="ig hi"> </strong> <a class="ae jc" href="https://www.linkedin.com/in/samuele-bolotta-841b16160/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae jc" href="https://twitter.com/SamBolotta" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我</li><li id="bf1f" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb mm lt lu lv bi translated">在<a class="ae jc" rel="noopener" href="/@samuelebolotta">媒体</a>上跟随我</li></ol><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hi fi z dy mv ea eb mw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne kj mq"/></div></div></a></div></div></div>    
</body>
</html>