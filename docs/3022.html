<html>
<head>
<title>Generating images with DDPMs: A PyTorch Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用DDPMs生成图像:PyTorch实现</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1?source=collection_archive---------0-----------------------#2022-07-10">https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1?source=collection_archive---------0-----------------------#2022-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="0b2f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="6260" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">去噪扩散概率模型(<strong class="je hi"> DDPM </strong>)是深度生成模型，由于其令人印象深刻的性能，最近受到了很多关注。像OpenAI的<a class="ae ka" href="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> DALL-E 2 </strong> </a> <strong class="je hi"> </strong>和Google的<a class="ae ka" href="https://arxiv.org/pdf/2205.11487.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> Imagen </strong> </a>发电机等全新型号都是基于DDPMs。他们以文本作为生成器的条件，这样就有可能在给定任意文本字符串的情况下生成照片般逼真的图像。</p><p id="66ac" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">比如输入“<em class="kg">一只背着背包的柴犬骑自行车的照片。这是戴着太阳镜和沙滩帽的"</em>到新的<strong class="je hi"> Imagen </strong>模型和"<em class="kg">一个被描绘成星云爆炸的柯基犬的头</em>"到<strong class="je hi"> DALL-E 2 </strong>模型产生了以下图像:</p><div class="kh ki kj kk fd ab cb"><figure class="kl km kn ko kp kq kr paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><img src="../Images/2faa932f84b47b12fa210b0e2df115bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*v2XjA_PXm9MEPoNLU5hn9Q.png"/></div></figure><figure class="kl km ky ko kp kq kr paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><img src="../Images/6fb2cf27124dece78541064dcf36cceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*vMY1bHjyObh2FS1KjvDggQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx ld di le lf">Image generated by Imagen (left) and DALL-E 2 (right)</figcaption></figure></div><p id="a927" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这些模型简直令人瞠目结舌，但要理解它们是如何工作的，就需要理解Ho et的原著。艾尔。<em class="kg">《去噪扩散概率模型》。</em></p><p id="5927" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">在这篇简短的文章中，我将着重于从头开始(在PyTorch中)创建一个简单版本的DDPM。特别是，我将重新执行何的<a class="ae ka" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank">原文。等人</a>。我们将使用传统的、不需要大量资源的MNIST和时尚MNIST数据集，并尝试凭空生成图像。先说一点理论。</p><h1 id="cfd0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">去噪扩散概率模型</h1><p id="0ee9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">去噪扩散概率模型(DDPMs)最早出现在<a class="ae ka" href="https://arxiv.org/pdf/2006.11239.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>中。</p><p id="fbcb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这个想法很简单:给定一组图像，我们一步一步地添加一点噪声。每走一步，图像就变得越来越不清晰，直到只剩下噪声。这被称为“前进过程”。然后，我们学习一个机器学习模型，可以撤销这样的每一个步骤，我们称之为“逆向过程”。如果我们可以成功地学习一个反向过程，我们就有了一个可以从纯随机噪声中生成图像的模型。</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lg"><img src="../Images/1e53f22540188bd2d6c84fe54d7015fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ul2nLb10sMMXgy_DbBd6Tw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">The main idea of DDPM: Map images x0 to more and more noisy images with probability distribution q. Then, learn the inverse function p parametrized by parameters theta. The image is taken from “Denoising DIffusion Probabilistic Models” by Ho et. al.</figcaption></figure><p id="4815" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">正向过程中的一个步骤在于通过从多元高斯分布中采样来使输入图像噪声更大(步骤t中的x ),该多元高斯分布的平均值是先前图像的缩小版本(步骤t-1中的x ),并且该协方差矩阵是对角的和固定的。换句话说，我们通过添加一些正态分布的值来独立地扰动图像中的每个像素。</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lh"><img src="../Images/01a48e3d10121f4f9b42dab9d0681a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*P8UMKuD87otlQdsFRjhE4w.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Forward process: we sample from a normal distribution which mean is a scaled version of the current image and which covariance matrix simply has all equal variance terms beta t.</figcaption></figure><p id="edf5" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">对于每一步，都有一个不同的系数β，它告诉我们在这一步中图像失真的程度。beta越高，图像中添加的噪声越多。我们可以自由选择系数β，但是我们应该尽量不要一次添加太多噪声，并且整个正向过程应该是“平滑”的。在何等人的原著中。艾尔。β被放置在从0.0001到0.02的线性空间中。</p><p id="cf5c" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">高斯分布的一个很好的特性是，我们可以通过向平均向量添加一个由标准偏差缩放的正态分布噪声向量来对其进行采样。这导致:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es li"><img src="../Images/808efe5c6575896bcd74992e13ea64d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*muGXZf32XX11WSO-"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Forward process but sampling is done by just adding the mean and scaling a normally distributed noise (epsilon) by the standard deviation.</figcaption></figure><p id="072a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们现在知道如何在正向过程中获取下一个样本，只需缩放现有样本并添加一些缩放噪声。如果我们现在认为这个公式是递归的，我们可以写出:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es li"><img src="../Images/70ef88722613ccaa5e0e5bb32306ffcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNbA7DNxA5XOfsuXzXHCkw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">The formula of the forward process is recursive, so we can start expanding it.</figcaption></figure><p id="0b2b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">如果我们继续这样做并做一些简化，我们可以一路返回并获得一个公式，用于从原始无噪声图像x0开始在步骤t获得噪声样本:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lj"><img src="../Images/208072704367c51d6dcdd020742a0774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*GXEpYr-eXJsRu5Z-KzazlQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">The equation for the forward process that allows to directly get a desired noisy level starting from the original non-noisy image.</figcaption></figure><p id="d66a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">太好了。现在不管我们的正向过程会有多少步，我们总会有办法直接从原始图像中直接得到第t步的含噪图像。</p><p id="4905" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">对于后向过程，我们知道我们的模型也应该作为高斯分布工作，所以我们只需要模型来预测给定噪声图像和时间步长的分布平均值和标准偏差。实际上，在关于DDPMs的第一篇论文中，协方差矩阵保持固定，因此我们只想预测高斯的平均值(给定噪声图像和我们当前所处的时间步长):</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lk"><img src="../Images/a1d395b435464cf6650271a86a45afbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*GTnXXTDiiYeHRBks7gIfNg.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Backward process: we try to go back to a less noisy image (x at timestep t-1) using a gaussian distribution which mean is predicted by a model</figcaption></figure><p id="ce27" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">现在，事实证明，要预测的最佳平均值只是我们已经熟悉的项的函数:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es ll"><img src="../Images/3f499d7b293e99b80b6d996540879f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*nho2mApDKXoN_3BTZCSuxA.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">The optimal mean value to be predicted to reverse the noising process. Given the more noisy image at step t, we can make it less noisy by subtracting a scale of the added noise and applying a scaling afterwards.</figcaption></figure><p id="1057" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">因此，我们可以进一步简化我们的模型，仅用噪声图像和时间步长的函数来预测噪声ε。</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lm"><img src="../Images/eb113b584b17bf5997168950f3c2ee1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*0AeMMe7cbXMHkfOY2T6LkQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Our model just predicts the noise that was added, and we use this to recover a less noisy image using the information for the particular time step.</figcaption></figure><p id="6def" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们的损失函数将是添加的真实噪声和模型预测的噪声之间的均方误差(MSE)的缩放版本</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ln"><img src="../Images/c5cdeb2a9fb14c552dd71bccedd489f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fDTOexzKw_C4kyIRM0bPQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Final loss function. We minimize the MSE between the noises actually added to the images and the one predicted by the model. We do so for all images in our dataset and all time steps.</figcaption></figure><p id="6f15" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">一旦模型被训练(算法1)，我们可以使用去噪模型来采样新图像(算法2)。</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lo"><img src="../Images/efae3f473e2cc02f68e11e988e3be57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgG9N8H2FvNWcv7oXwxpTA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Training and sampling algorithms. Once the model is trained, we can use it to generate brand new samples starting from gaussian noise.</figcaption></figure><h1 id="16b0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">让我们开始编码吧</h1><p id="daf1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">既然我们对扩散模型的工作原理有了大致的了解，是时候实现我们自己的东西了。你可以自己在这个<a class="ae ka" href="https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>中或者用这个<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>运行下面的代码。</p><p id="8fc7" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">和往常一样，进口只是我们的第一步。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="f7f3" class="lu if hh lq b be lv lw l lx ly"># Import of libraries<br/>import random<br/>import imageio<br/>import numpy as np<br/>from argparse import ArgumentParser<br/><br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/><br/>import einops<br/>import torch<br/>import torch.nn as nn<br/>from torch.optim import Adam<br/>from torch.utils.data import DataLoader<br/><br/>from torchvision.transforms import Compose, ToTensor, Lambda<br/>from torchvision.datasets.mnist import MNIST, FashionMNIST<br/><br/># Setting reproducibility<br/>SEED = 0<br/>random.seed(SEED)<br/>np.random.seed(SEED)<br/>torch.manual_seed(SEED)<br/><br/># Definitions<br/>STORE_PATH_MNIST = f"ddpm_model_mnist.pt"<br/>STORE_PATH_FASHION = f"ddpm_model_fashion.pt"</span></pre><p id="83a6" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">接下来，我们为实验定义几个参数。具体来说，我们决定是否要运行训练循环，是否要使用时尚-MNIST数据集和一些训练超参数</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="8048" class="lu if hh lq b be lv lw l lx ly">no_train = False<br/>fashion = True<br/>batch_size = 128<br/>n_epochs = 20<br/>lr = 0.001<br/>store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"</span></pre><p id="4015" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">接下来，我们真的想展示图像。我们对训练图像和模型生成的图像都感兴趣。我们编写一个效用函数，给定一些图像，将显示一个正方形(或尽可能接近)的子图形网格:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="7b38" class="lu if hh lq b be lv lw l lx ly">def show_images(images, title=""):<br/>    """Shows the provided images as sub-pictures in a square"""<br/><br/>    # Converting images to CPU numpy arrays<br/>    if type(images) is torch.Tensor:<br/>        images = images.detach().cpu().numpy()<br/><br/>    # Defining number of rows and columns<br/>    fig = plt.figure(figsize=(8, 8))<br/>    rows = int(len(images) ** (1 / 2))<br/>    cols = round(len(images) / rows)<br/><br/>    # Populating figure with sub-plots<br/>    idx = 0<br/>    for r in range(rows):<br/>        for c in range(cols):<br/>            fig.add_subplot(rows, cols, idx + 1)<br/><br/>            if idx &lt; len(images):<br/>                plt.imshow(images[idx][0], cmap="gray")<br/>                idx += 1<br/>    fig.suptitle(title, fontsize=30)<br/><br/>    # Showing the figure<br/>    plt.show()</span></pre><p id="20dc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">为了测试这个效用函数，我们加载数据集并显示第一批。<strong class="je hi">重要提示:</strong>图像必须在[-1，1]范围内归一化，因为我们的网络必须预测正态分布的噪声值:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="51e6" class="lu if hh lq b be lv lw l lx ly"># Shows the first batch of images<br/>def show_first_batch(loader):<br/>    for batch in loader:<br/>        show_images(batch[0], "Images in the first batch")<br/>        break</span></pre><pre class="lz lp lq lr bn ls lt bi"><span id="6357" class="lu if hh lq b be lv lw l lx ly"># Loading the data (converting each image into a tensor and normalizing between [-1, 1])<br/>transform = Compose([<br/>    ToTensor(),<br/>    Lambda(lambda x: (x - 0.5) * 2)]<br/>)<br/>ds_fn = FashionMNIST if fashion else MNIST<br/>dataset = ds_fn("./datasets", download=True, train=True, transform=transform)<br/>loader = DataLoader(dataset, batch_size, shuffle=True)</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es ma"><img src="../Images/ffdeb6cefa004560606aad3e13386e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*3j7Y0l_IaqmWhmWrLInqhg.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Images in our first batch. If you kept the same randomizing seed, you should get the exact same batch.</figcaption></figure><p id="5826" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">太好了！现在我们有了这个很好的效用函数，我们稍后也将把它用于我们的模型生成的图像。在我们真正开始处理DDPM模型之前，我们将从colab获得一个GPU设备(通常是非colab-pro用户的<em class="kg">特斯拉T4 </em>):</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mb"><img src="../Images/9e118c4e31cce0260dfe745b5c422fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trEQpvjm5RvsXjvIE6S6Kg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Getting a device and, if it is a GPU, printing its name</figcaption></figure><h1 id="b1f7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">DDPM模式</h1><p id="83ab" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">既然我们已经解决了琐碎的事情，现在是时候研究DDPM了。我们将创建一个<em class="kg"> MyDDPM </em> PyTorch模块，它将负责存储betas和alphas值并应用转发过程。相反，对于后向过程，<em class="kg"> MyDDPM </em>模块将简单地依赖于用于构建DDPM的网络:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="4597" class="lu if hh lq b be lv lw l lx ly"># DDPM class<br/>class MyDDPM(nn.Module):<br/>    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 28, 28)):<br/>        super(MyDDPM, self).__init__()<br/>        self.n_steps = n_steps<br/>        self.device = device<br/>        self.image_chw = image_chw<br/>        self.network = network.to(device)<br/>        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(<br/>            device)  # Number of steps is typically in the order of thousands<br/>        self.alphas = 1 - self.betas<br/>        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)<br/><br/>    def forward(self, x0, t, eta=None):<br/>        # Make input image more noisy (we can directly skip to the desired step)<br/>        n, c, h, w = x0.shape<br/>        a_bar = self.alpha_bars[t]<br/><br/>        if eta is None:<br/>            eta = torch.randn(n, c, h, w).to(self.device)<br/><br/>        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta<br/>        return noisy<br/><br/>    def backward(self, x, t):<br/>        # Run each image through the network for each timestep t in the vector t.<br/>        # The network returns its estimation of the noise that was added.<br/>        return self.network(x, t)</span></pre><p id="d198" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">注意，正向过程独立于用于去噪的网络，所以从技术上来说，我们已经可以看到它的效果。同时，我们还可以创建一个应用<strong class="je hi">算法2 </strong>(采样过程)生成新图像的效用函数。我们通过两个DDPM的特定效用函数来实现:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="7c46" class="lu if hh lq b be lv lw l lx ly">def show_forward(ddpm, loader, device):<br/>    # Showing the forward process<br/>    for batch in loader:<br/>        imgs = batch[0]<br/><br/>        show_images(imgs, "Original images")<br/><br/>        for percent in [0.25, 0.5, 0.75, 1]:<br/>            show_images(<br/>                ddpm(imgs.to(device),<br/>                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),<br/>                f"DDPM Noisy images {int(percent * 100)}%"<br/>            )<br/>        break</span></pre><p id="b51d" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">为了生成图像，我们从随机噪声开始，让T从T回到0。在每一步，我们将噪声估计为<strong class="je hi"> eta_theta </strong>并应用去噪函数。最后，像朗之万动力学一样，增加了额外的噪声。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="c8c2" class="lu if hh lq b be lv lw l lx ly">def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name="sampling.gif", c=1, h=28, w=28):<br/>    """Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples"""<br/>    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)<br/>    frames = []<br/><br/>    with torch.no_grad():<br/>        if device is None:<br/>            device = ddpm.device<br/><br/>        # Starting from random noise<br/>        x = torch.randn(n_samples, c, h, w).to(device)<br/><br/>        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):<br/>            # Estimating noise to be removed<br/>            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()<br/>            eta_theta = ddpm.backward(x, time_tensor)<br/><br/>            alpha_t = ddpm.alphas[t]<br/>            alpha_t_bar = ddpm.alpha_bars[t]<br/><br/>            # Partially denoising the image<br/>            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)<br/><br/>            if t &gt; 0:<br/>                z = torch.randn(n_samples, c, h, w).to(device)<br/><br/>                # Option 1: sigma_t squared = beta_t<br/>                beta_t = ddpm.betas[t]<br/>                sigma_t = beta_t.sqrt()<br/><br/>                # Option 2: sigma_t squared = beta_tilda_t<br/>                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t &gt; 0 else ddpm.alphas[0]<br/>                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t<br/>                # sigma_t = beta_tilda_t.sqrt()<br/><br/>                # Adding some more noise like in Langevin Dynamics fashion<br/>                x = x + sigma_t * z<br/><br/>            # Adding frames to the GIF<br/>            if idx in frame_idxs or t == 0:<br/>                # Putting digits in range [0, 255]<br/>                normalized = x.clone()<br/>                for i in range(len(normalized)):<br/>                    normalized[i] -= torch.min(normalized[i])<br/>                    normalized[i] *= 255 / torch.max(normalized[i])<br/><br/>                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame<br/>                frame = einops.rearrange(normalized, "(b1 b2) c h w -&gt; (b1 h) (b2 w) c", b1=int(n_samples ** 0.5))<br/>                frame = frame.cpu().numpy().astype(np.uint8)<br/><br/>                # Rendering frame<br/>                frames.append(frame)<br/><br/>    # Storing the gif<br/>    with imageio.get_writer(gif_name, mode="I") as writer:<br/>        for idx, frame in enumerate(frames):<br/>            writer.append_data(frame)<br/>            if idx == len(frames) - 1:<br/>                for _ in range(frames_per_gif // 3):<br/>                    writer.append_data(frames[-1])<br/>    return x</span></pre><p id="44c3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">所有与DDPM有关的事情现在都摆在桌面上。我们只需要定义一个模型，在给定图像和当前时间步长的情况下，该模型将实际完成预测图像中噪声的工作。为此，我们将创建一个定制的U-Net模型。不言而喻，您可以自由选择使用任何其他模型。</p><h1 id="ee60" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">优信网</h1><p id="b252" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们通过创建一个保持空间维度不变的块来开始创建我们的U-Net。这个块将用于我们U-Net的每个级别。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="4cdd" class="lu if hh lq b be lv lw l lx ly">class MyBlock(nn.Module):<br/>    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):<br/>        super(MyBlock, self).__init__()<br/>        self.ln = nn.LayerNorm(shape)<br/>        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)<br/>        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)<br/>        self.activation = nn.SiLU() if activation is None else activation<br/>        self.normalize = normalize<br/><br/>    def forward(self, x):<br/>        out = self.ln(x) if self.normalize else x<br/>        out = self.conv1(out)<br/>        out = self.activation(out)<br/>        out = self.conv2(out)<br/>        out = self.activation(out)<br/>        return out</span></pre><p id="0a7e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">DDPMs中的棘手之处在于，我们的图像到图像模型必须以当前时间步长为条件。为了在实践中做到这一点，我们使用正弦嵌入和一层MLPs。由此产生的张量将通过U-Net的每一级按信道添加到网络的输入中。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="1979" class="lu if hh lq b be lv lw l lx ly">def sinusoidal_embedding(n, d):<br/>    # Returns the standard positional embedding<br/>    embedding = torch.zeros(n, d)<br/>    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])<br/>    wk = wk.reshape((1, d))<br/>    t = torch.arange(n).reshape((n, 1))<br/>    embedding[:,::2] = torch.sin(t * wk[:,::2])<br/>    embedding[:,1::2] = torch.cos(t * wk[:,::2])<br/><br/>    return embedding</span></pre><p id="8e11" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们创建了一个小的效用函数，该函数创建了一个用于绘制位置嵌入地图的一层MLP。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="efdf" class="lu if hh lq b be lv lw l lx ly">def _make_te(self, dim_in, dim_out):<br/>  return nn.Sequential(<br/>    nn.Linear(dim_in, dim_out),<br/>    nn.SiLU(),<br/>    nn.Linear(dim_out, dim_out)<br/>  )</span></pre><p id="f055" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">现在我们知道了如何处理时间信息，我们可以创建一个自定义的U-Net网络。我们将有3个下采样部分，一个网络中间的瓶颈，和3个带有通常的U-Net剩余连接(连接)的上采样步骤。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="8229" class="lu if hh lq b be lv lw l lx ly">class MyUNet(nn.Module):<br/>    def __init__(self, n_steps=1000, time_emb_dim=100):<br/>        super(MyUNet, self).__init__()<br/><br/>        # Sinusoidal embedding<br/>        self.time_embed = nn.Embedding(n_steps, time_emb_dim)<br/>        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)<br/>        self.time_embed.requires_grad_(False)<br/><br/>        # First half<br/>        self.te1 = self._make_te(time_emb_dim, 1)<br/>        self.b1 = nn.Sequential(<br/>            MyBlock((1, 28, 28), 1, 10),<br/>            MyBlock((10, 28, 28), 10, 10),<br/>            MyBlock((10, 28, 28), 10, 10)<br/>        )<br/>        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)<br/><br/>        self.te2 = self._make_te(time_emb_dim, 10)<br/>        self.b2 = nn.Sequential(<br/>            MyBlock((10, 14, 14), 10, 20),<br/>            MyBlock((20, 14, 14), 20, 20),<br/>            MyBlock((20, 14, 14), 20, 20)<br/>        )<br/>        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)<br/><br/>        self.te3 = self._make_te(time_emb_dim, 20)<br/>        self.b3 = nn.Sequential(<br/>            MyBlock((20, 7, 7), 20, 40),<br/>            MyBlock((40, 7, 7), 40, 40),<br/>            MyBlock((40, 7, 7), 40, 40)<br/>        )<br/>        self.down3 = nn.Sequential(<br/>            nn.Conv2d(40, 40, 2, 1),<br/>            nn.SiLU(),<br/>            nn.Conv2d(40, 40, 4, 2, 1)<br/>        )<br/><br/>        # Bottleneck<br/>        self.te_mid = self._make_te(time_emb_dim, 40)<br/>        self.b_mid = nn.Sequential(<br/>            MyBlock((40, 3, 3), 40, 20),<br/>            MyBlock((20, 3, 3), 20, 20),<br/>            MyBlock((20, 3, 3), 20, 40)<br/>        )<br/><br/>        # Second half<br/>        self.up1 = nn.Sequential(<br/>            nn.ConvTranspose2d(40, 40, 4, 2, 1),<br/>            nn.SiLU(),<br/>            nn.ConvTranspose2d(40, 40, 2, 1)<br/>        )<br/><br/>        self.te4 = self._make_te(time_emb_dim, 80)<br/>        self.b4 = nn.Sequential(<br/>            MyBlock((80, 7, 7), 80, 40),<br/>            MyBlock((40, 7, 7), 40, 20),<br/>            MyBlock((20, 7, 7), 20, 20)<br/>        )<br/><br/>        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)<br/>        self.te5 = self._make_te(time_emb_dim, 40)<br/>        self.b5 = nn.Sequential(<br/>            MyBlock((40, 14, 14), 40, 20),<br/>            MyBlock((20, 14, 14), 20, 10),<br/>            MyBlock((10, 14, 14), 10, 10)<br/>        )<br/><br/>        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)<br/>        self.te_out = self._make_te(time_emb_dim, 20)<br/>        self.b_out = nn.Sequential(<br/>            MyBlock((20, 28, 28), 20, 10),<br/>            MyBlock((10, 28, 28), 10, 10),<br/>            MyBlock((10, 28, 28), 10, 10, normalize=False)<br/>        )<br/><br/>        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)<br/><br/>    def forward(self, x, t):<br/>        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)<br/>        t = self.time_embed(t)<br/>        n = len(x)<br/>        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)<br/>        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)<br/>        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)<br/><br/>        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)<br/><br/>        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)<br/>        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)<br/><br/>        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)<br/>        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)<br/><br/>        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)<br/>        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)<br/><br/>        out = self.conv_out(out)<br/><br/>        return out<br/><br/>    def _make_te(self, dim_in, dim_out):<br/>        return nn.Sequential(<br/>            nn.Linear(dim_in, dim_out),<br/>            nn.SiLU(),<br/>            nn.Linear(dim_out, dim_out)<br/>        )</span></pre><p id="fdcb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">现在我们已经定义了去噪网络，我们可以实例化一个DDPM模型，并进行一些可视化处理。</p><h1 id="6b6c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">一些可视化</h1><p id="217d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们使用自定义的U-Net实例化DDPM模型，如下所示。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="257b" class="lu if hh lq b be lv lw l lx ly"># Defining model<br/>n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors<br/>ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)</span></pre><p id="f6ff" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">让我们看看转发过程是什么样子的:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="0ee2" class="lu if hh lq b be lv lw l lx ly"># Optionally, show the diffusion (forward) process<br/>show_forward(ddpm, loader, device)</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mc"><img src="../Images/cd6d480144910cca2a88c57905bf7bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lNCAyYvDTnUmVCS4MIY-g.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Result of running the forward process. Images get noisier and noisier with each step until just noise is left.</figcaption></figure><p id="63cf" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们还没有训练模型，但我们已经可以使用允许我们生成新图像的函数，看看会发生什么:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es md"><img src="../Images/6ad6d13b8e85b82a48a293eead9367bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*g3Wr3q4SZFyEAuaJ7hq67A.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Generating new images with a non-trained model. Noise is produced.</figcaption></figure><p id="06ab" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">毫不奇怪，当我们这样做时，什么也没发生。但是，当模型完成训练后，我们将再次使用相同的方法。</p><h1 id="fec0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">训练循环</h1><p id="fe18" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们现在实现算法1来学习将知道如何对图像去噪的模型。这对应于我们的训练循环。</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="a0ac" class="lu if hh lq b be lv lw l lx ly">def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path="ddpm_model.pt"):<br/>    mse = nn.MSELoss()<br/>    best_loss = float("inf")<br/>    n_steps = ddpm.n_steps<br/><br/>    for epoch in tqdm(range(n_epochs), desc=f"Training progress", colour="#00ff00"):<br/>        epoch_loss = 0.0<br/>        for step, batch in enumerate(tqdm(loader, leave=False, desc=f"Epoch {epoch + 1}/{n_epochs}", colour="#005500")):<br/>            # Loading data<br/>            x0 = batch[0].to(device)<br/>            n = len(x0)<br/><br/>            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars<br/>            eta = torch.randn_like(x0).to(device)<br/>            t = torch.randint(0, n_steps, (n,)).to(device)<br/><br/>            # Computing the noisy image based on x0 and the time-step (forward process)<br/>            noisy_imgs = ddpm(x0, t, eta)<br/><br/>            # Getting model estimation of noise based on the images and the time-step<br/>            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))<br/><br/>            # Optimizing the MSE between the noise plugged and the predicted noise<br/>            loss = mse(eta_theta, eta)<br/>            optim.zero_grad()<br/>            loss.backward()<br/>            optim.step()<br/><br/>            epoch_loss += loss.item() * len(x0) / len(loader.dataset)<br/><br/>        # Display images generated at this epoch<br/>        if display:<br/>            show_images(generate_new_images(ddpm, device=device), f"Images generated at epoch {epoch + 1}")<br/><br/>        log_string = f"Loss at epoch {epoch + 1}: {epoch_loss:.3f}"<br/><br/>        # Storing the model<br/>        if best_loss &gt; epoch_loss:<br/>            best_loss = epoch_loss<br/>            torch.save(ddpm.state_dict(), store_path)<br/>            log_string += " --&gt; Best model ever (stored)"<br/><br/>        print(log_string)</span></pre><p id="de14" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">正如你所看到的，在我们的训练循环中，我们只是简单地对一些图像进行采样，并对每个图像进行随机的时间步长。然后，我们用正向过程使它们有噪声，并对这些有噪声的图像运行反向过程。实际添加的噪声和模型预测的噪声之间的MSE被优化。</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es me"><img src="../Images/31fc8a66a966e85f4b9bb38d129845b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*giuoyh5vEA4kNNbqij9ZTQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Training takes roughly 24 seconds per epoch. With 20 epochs, it takes roughly 8 minutes to train a DDPM model.</figcaption></figure><p id="5161" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">默认情况下，我将训练周期设置为20，因为每个周期需要24秒(训练总共大约需要8分钟)。请注意，有可能通过更多的纪元、更好的U-Net和其他技巧获得更好的性能。在这篇文章中，为了简单起见，我省略了这些。</p><h1 id="df23" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">测试模型</h1><p id="d3a1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">既然工作已经完成，我们就可以享受成果了。我们根据MSE损失函数加载训练期间获得的最佳模型，将其设置为评估模式，并使用它来生成新样本</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="b4b5" class="lu if hh lq b be lv lw l lx ly"># Loading the trained model<br/>best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)<br/>best_model.load_state_dict(torch.load(store_path, map_location=device))<br/>best_model.eval()<br/>print("Model loaded")</span></pre><pre class="lz lp lq lr bn ls lt bi"><span id="0d58" class="lu if hh lq b be lv lw l lx ly">print("Generating new images")<br/>generated = generate_new_images(<br/>        best_model,<br/>        n_samples=100,<br/>        device=device,<br/>        gif_name="fashion.gif" if fashion else "mnist.gif"<br/>    )<br/>show_images(generated, "Final result")</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es mf"><img src="../Images/9159ecb2c6324b7ce48b9cc6720d4113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*uLr2G8QGlCtNBA92Ukmgag.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Final result on the MNIST dataset</figcaption></figure><p id="c017" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">蛋糕上的樱桃是我们的生成函数自动创建扩散过程的漂亮gif的事实。我们使用以下命令在Colab中可视化gif:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es mg"><img src="../Images/6f8fe8cf57ba763fb696be8e17bd409f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*or9fTUuSybnRvlYTIhStVw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Showing the generated gif image</figcaption></figure><div class="kh ki kj kk fd ab cb"><figure class="kl km mh ko kp kq kr paragraph-image"><img src="../Images/89ee312cd28d38435fcb520d2640b3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/1*byEJ4YdbrSjDHyRowXL8-w.gif"/></figure><figure class="kl km mh ko kp kq kr paragraph-image"><img src="../Images/bd2cc02a6962eec2ac69fb3124cd3651.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/1*d3o6yuMPthgNFTP208HoZQ.gif"/><figcaption class="kz la et er es lb lc bd b be z dx mi di mj lf">Obtained GIFs for Fashion-MNIST and MNIST datasets.</figcaption></figure></div><p id="ff01" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们完了。我们终于让我们的DDPM模型工作了！</p><h1 id="171b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">进一步的改进</h1><p id="01ad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">已经做了进一步的改进，以允许<a class="ae ka" href="https://arxiv.org/pdf/2006.09011.pdf" rel="noopener ugc nofollow" target="_blank">生成更高分辨率的图像</a>，<a class="ae ka" href="https://arxiv.org/pdf/2010.02502.pdf" rel="noopener ugc nofollow" target="_blank">加速采样</a>或获得<a class="ae ka" href="https://arxiv.org/abs/2102.09672" rel="noopener ugc nofollow" target="_blank">更好的样本质量和可能性</a>。Imagen和DALL-E 2型号基于原始DDPMs的改进版本。</p><h1 id="95cf" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">更多参考</h1><p id="05a8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">关于DDPMs的更多参考，我强烈推荐阅读Lilian Weng和Niels Rogge的<a class="ae ka" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="noopener ugc nofollow" target="_blank">杰出文章</a>和Kashif Rasul的惊人<a class="ae ka" href="https://huggingface.co/blog/annotated-diffusion" rel="noopener ugc nofollow" target="_blank">拥抱脸博客</a>。Colab笔记本的最后还提到了其他作者。</p><h1 id="c8ea" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="5fcc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">扩散模型是学习迭代去噪图像的生成模型。从一些噪声开始，然后可以要求模型对样本去噪声，直到获得一些真实的图像。</p><p id="e5b6" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我们在PyTorch中从头创建了一个DDPM，并让它学习去噪MNIST /时尚MNIST的图像。经过训练后，这个模型最终能够从随机噪声中生成新的图像。很神奇，对吧？</p><p id="5361" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">带有所示实现的Colab笔记本可以在<a class="ae ka" href="https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing" rel="noopener ugc nofollow" target="_blank">这个链接</a>免费访问，而<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>包含。py文件。如果你觉得这个故事有用，可以考虑为它鼓掌<strong class="je hi">👏</strong>。如果你觉得有什么不清楚的地方，不要犹豫，直接联系我！我很高兴与你讨论它。</p><div class="mk ml ez fb mm mn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb kw mn"/></div></div></a></div></div></div>    
</body>
</html>