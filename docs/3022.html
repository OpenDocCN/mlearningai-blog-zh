<html>
<head>
<title>Generating images with DDPMs: A PyTorch Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨DDPMsç”Ÿæˆå›¾åƒ:PyTorchå®ç°</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1?source=collection_archive---------0-----------------------#2022-07-10">https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1?source=collection_archive---------0-----------------------#2022-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="0b2f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ä»‹ç»</h1><p id="6260" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(<strong class="je hi"> DDPM </strong>)æ˜¯æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œç”±äºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œæœ€è¿‘å—åˆ°äº†å¾ˆå¤šå…³æ³¨ã€‚åƒOpenAIçš„<a class="ae ka" href="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> DALL-E 2 </strong> </a> <strong class="je hi"> </strong>å’ŒGoogleçš„<a class="ae ka" href="https://arxiv.org/pdf/2205.11487.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> Imagen </strong> </a>å‘ç”µæœºç­‰å…¨æ–°å‹å·éƒ½æ˜¯åŸºäºDDPMsã€‚ä»–ä»¬ä»¥æ–‡æœ¬ä½œä¸ºç”Ÿæˆå™¨çš„æ¡ä»¶ï¼Œè¿™æ ·å°±æœ‰å¯èƒ½åœ¨ç»™å®šä»»æ„æ–‡æœ¬å­—ç¬¦ä¸²çš„æƒ…å†µä¸‹ç”Ÿæˆç…§ç‰‡èˆ¬é€¼çœŸçš„å›¾åƒã€‚</p><p id="66ac" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ¯”å¦‚è¾“å…¥â€œ<em class="kg">ä¸€åªèƒŒç€èƒŒåŒ…çš„æŸ´çŠ¬éª‘è‡ªè¡Œè½¦çš„ç…§ç‰‡ã€‚è¿™æ˜¯æˆ´ç€å¤ªé˜³é•œå’Œæ²™æ»©å¸½çš„"</em>åˆ°æ–°çš„<strong class="je hi"> Imagen </strong>æ¨¡å‹å’Œ"<em class="kg">ä¸€ä¸ªè¢«æç»˜æˆæ˜Ÿäº‘çˆ†ç‚¸çš„æŸ¯åŸºçŠ¬çš„å¤´</em>"åˆ°<strong class="je hi"> DALL-E 2 </strong>æ¨¡å‹äº§ç”Ÿäº†ä»¥ä¸‹å›¾åƒ:</p><div class="kh ki kj kk fd ab cb"><figure class="kl km kn ko kp kq kr paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><img src="../Images/2faa932f84b47b12fa210b0e2df115bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*v2XjA_PXm9MEPoNLU5hn9Q.png"/></div></figure><figure class="kl km ky ko kp kq kr paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><img src="../Images/6fb2cf27124dece78541064dcf36cceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*vMY1bHjyObh2FS1KjvDggQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx ld di le lf">Image generated by Imagen (left) and DALL-E 2 (right)</figcaption></figure></div><p id="a927" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¿™äº›æ¨¡å‹ç®€ç›´ä»¤äººç ç›®ç»“èˆŒï¼Œä½†è¦ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå°±éœ€è¦ç†è§£Ho etçš„åŸè‘—ã€‚è‰¾å°”ã€‚<em class="kg">ã€Šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‹ã€‚</em></p><p id="5927" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">åœ¨è¿™ç¯‡ç®€çŸ­çš„æ–‡ç« ä¸­ï¼Œæˆ‘å°†ç€é‡äºä»å¤´å¼€å§‹(åœ¨PyTorchä¸­)åˆ›å»ºä¸€ä¸ªç®€å•ç‰ˆæœ¬çš„DDPMã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†é‡æ–°æ‰§è¡Œä½•çš„<a class="ae ka" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank">åŸæ–‡ã€‚ç­‰äºº</a>ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¼ ç»Ÿçš„ã€ä¸éœ€è¦å¤§é‡èµ„æºçš„MNISTå’Œæ—¶å°šMNISTæ•°æ®é›†ï¼Œå¹¶å°è¯•å‡­ç©ºç”Ÿæˆå›¾åƒã€‚å…ˆè¯´ä¸€ç‚¹ç†è®ºã€‚</p><h1 id="cfd0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹</h1><p id="0ee9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPMs)æœ€æ—©å‡ºç°åœ¨<a class="ae ka" href="https://arxiv.org/pdf/2006.11239.pdf" rel="noopener ugc nofollow" target="_blank">è¿™ç¯‡è®ºæ–‡</a>ä¸­ã€‚</p><p id="fbcb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è¿™ä¸ªæƒ³æ³•å¾ˆç®€å•:ç»™å®šä¸€ç»„å›¾åƒï¼Œæˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ·»åŠ ä¸€ç‚¹å™ªå£°ã€‚æ¯èµ°ä¸€æ­¥ï¼Œå›¾åƒå°±å˜å¾—è¶Šæ¥è¶Šä¸æ¸…æ™°ï¼Œç›´åˆ°åªå‰©ä¸‹å™ªå£°ã€‚è¿™è¢«ç§°ä¸ºâ€œå‰è¿›è¿‡ç¨‹â€ã€‚ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥æ’¤é”€è¿™æ ·çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œé€†å‘è¿‡ç¨‹â€ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥æˆåŠŸåœ°å­¦ä¹ ä¸€ä¸ªåå‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªå¯ä»¥ä»çº¯éšæœºå™ªå£°ä¸­ç”Ÿæˆå›¾åƒçš„æ¨¡å‹ã€‚</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lg"><img src="../Images/1e53f22540188bd2d6c84fe54d7015fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ul2nLb10sMMXgy_DbBd6Tw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">The main idea of DDPM: Map images x0 to more and more noisy images with probability distribution q. Then, learn the inverse function p parametrized by parameters theta. The image is taken from â€œDenoising DIffusion Probabilistic Modelsâ€ by Ho et. al.</figcaption></figure><p id="4815" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ­£å‘è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªæ­¥éª¤åœ¨äºé€šè¿‡ä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·æ¥ä½¿è¾“å…¥å›¾åƒå™ªå£°æ›´å¤§(æ­¥éª¤tä¸­çš„x ),è¯¥å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¹³å‡å€¼æ˜¯å…ˆå‰å›¾åƒçš„ç¼©å°ç‰ˆæœ¬(æ­¥éª¤t-1ä¸­çš„x ),å¹¶ä¸”è¯¥åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çš„å’Œå›ºå®šçš„ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ ä¸€äº›æ­£æ€åˆ†å¸ƒçš„å€¼æ¥ç‹¬ç«‹åœ°æ‰°åŠ¨å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ ã€‚</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lh"><img src="../Images/01a48e3d10121f4f9b42dab9d0681a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*P8UMKuD87otlQdsFRjhE4w.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Forward process: we sample from a normal distribution which mean is a scaled version of the current image and which covariance matrix simply has all equal variance terms beta t.</figcaption></figure><p id="edf5" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¯¹äºæ¯ä¸€æ­¥ï¼Œéƒ½æœ‰ä¸€ä¸ªä¸åŒçš„ç³»æ•°Î²ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬åœ¨è¿™ä¸€æ­¥ä¸­å›¾åƒå¤±çœŸçš„ç¨‹åº¦ã€‚betaè¶Šé«˜ï¼Œå›¾åƒä¸­æ·»åŠ çš„å™ªå£°è¶Šå¤šã€‚æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©ç³»æ•°Î²ï¼Œä½†æ˜¯æˆ‘ä»¬åº”è¯¥å°½é‡ä¸è¦ä¸€æ¬¡æ·»åŠ å¤ªå¤šå™ªå£°ï¼Œå¹¶ä¸”æ•´ä¸ªæ­£å‘è¿‡ç¨‹åº”è¯¥æ˜¯â€œå¹³æ»‘â€çš„ã€‚åœ¨ä½•ç­‰äººçš„åŸè‘—ä¸­ã€‚è‰¾å°”ã€‚Î²è¢«æ”¾ç½®åœ¨ä»0.0001åˆ°0.02çš„çº¿æ€§ç©ºé—´ä¸­ã€‚</p><p id="cf5c" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">é«˜æ–¯åˆ†å¸ƒçš„ä¸€ä¸ªå¾ˆå¥½çš„ç‰¹æ€§æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘å¹³å‡å‘é‡æ·»åŠ ä¸€ä¸ªç”±æ ‡å‡†åå·®ç¼©æ”¾çš„æ­£æ€åˆ†å¸ƒå™ªå£°å‘é‡æ¥å¯¹å…¶è¿›è¡Œé‡‡æ ·ã€‚è¿™å¯¼è‡´:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es li"><img src="../Images/808efe5c6575896bcd74992e13ea64d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*muGXZf32XX11WSO-"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Forward process but sampling is done by just adding the mean and scaling a normally distributed noise (epsilon) by the standard deviation.</figcaption></figure><p id="072a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬ç°åœ¨çŸ¥é“å¦‚ä½•åœ¨æ­£å‘è¿‡ç¨‹ä¸­è·å–ä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Œåªéœ€ç¼©æ”¾ç°æœ‰æ ·æœ¬å¹¶æ·»åŠ ä¸€äº›ç¼©æ”¾å™ªå£°ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨è®¤ä¸ºè¿™ä¸ªå…¬å¼æ˜¯é€’å½’çš„ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡º:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es li"><img src="../Images/70ef88722613ccaa5e0e5bb32306ffcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNbA7DNxA5XOfsuXzXHCkw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">The formula of the forward process is recursive, so we can start expanding it.</figcaption></figure><p id="0b2b" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¦‚æœæˆ‘ä»¬ç»§ç»­è¿™æ ·åšå¹¶åšä¸€äº›ç®€åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€è·¯è¿”å›å¹¶è·å¾—ä¸€ä¸ªå…¬å¼ï¼Œç”¨äºä»åŸå§‹æ— å™ªå£°å›¾åƒx0å¼€å§‹åœ¨æ­¥éª¤tè·å¾—å™ªå£°æ ·æœ¬:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lj"><img src="../Images/208072704367c51d6dcdd020742a0774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*GXEpYr-eXJsRu5Z-KzazlQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">The equation for the forward process that allows to directly get a desired noisy level starting from the original non-noisy image.</figcaption></figure><p id="d66a" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¤ªå¥½äº†ã€‚ç°åœ¨ä¸ç®¡æˆ‘ä»¬çš„æ­£å‘è¿‡ç¨‹ä¼šæœ‰å¤šå°‘æ­¥ï¼Œæˆ‘ä»¬æ€»ä¼šæœ‰åŠæ³•ç›´æ¥ä»åŸå§‹å›¾åƒä¸­ç›´æ¥å¾—åˆ°ç¬¬tæ­¥çš„å«å™ªå›¾åƒã€‚</p><p id="4905" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¯¹äºåå‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿåº”è¯¥ä½œä¸ºé«˜æ–¯åˆ†å¸ƒå·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¨¡å‹æ¥é¢„æµ‹ç»™å®šå™ªå£°å›¾åƒå’Œæ—¶é—´æ­¥é•¿çš„åˆ†å¸ƒå¹³å‡å€¼å’Œæ ‡å‡†åå·®ã€‚å®é™…ä¸Šï¼Œåœ¨å…³äºDDPMsçš„ç¬¬ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œåæ–¹å·®çŸ©é˜µä¿æŒå›ºå®šï¼Œå› æ­¤æˆ‘ä»¬åªæƒ³é¢„æµ‹é«˜æ–¯çš„å¹³å‡å€¼(ç»™å®šå™ªå£°å›¾åƒå’Œæˆ‘ä»¬å½“å‰æ‰€å¤„çš„æ—¶é—´æ­¥é•¿):</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lk"><img src="../Images/a1d395b435464cf6650271a86a45afbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*GTnXXTDiiYeHRBks7gIfNg.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Backward process: we try to go back to a less noisy image (x at timestep t-1) using a gaussian distribution which mean is predicted by a model</figcaption></figure><p id="ce27" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç°åœ¨ï¼Œäº‹å®è¯æ˜ï¼Œè¦é¢„æµ‹çš„æœ€ä½³å¹³å‡å€¼åªæ˜¯æˆ‘ä»¬å·²ç»ç†Ÿæ‚‰çš„é¡¹çš„å‡½æ•°:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es ll"><img src="../Images/3f499d7b293e99b80b6d996540879f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*nho2mApDKXoN_3BTZCSuxA.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">The optimal mean value to be predicted to reverse the noising process. Given the more noisy image at step t, we can make it less noisy by subtracting a scale of the added noise and applying a scaling afterwards.</figcaption></figure><p id="1057" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»…ç”¨å™ªå£°å›¾åƒå’Œæ—¶é—´æ­¥é•¿çš„å‡½æ•°æ¥é¢„æµ‹å™ªå£°Îµã€‚</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es lm"><img src="../Images/eb113b584b17bf5997168950f3c2ee1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*0AeMMe7cbXMHkfOY2T6LkQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Our model just predicts the noise that was added, and we use this to recover a less noisy image using the information for the particular time step.</figcaption></figure><p id="6def" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬çš„æŸå¤±å‡½æ•°å°†æ˜¯æ·»åŠ çš„çœŸå®å™ªå£°å’Œæ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„å‡æ–¹è¯¯å·®(MSE)çš„ç¼©æ”¾ç‰ˆæœ¬</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ln"><img src="../Images/c5cdeb2a9fb14c552dd71bccedd489f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fDTOexzKw_C4kyIRM0bPQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Final loss function. We minimize the MSE between the noises actually added to the images and the one predicted by the model. We do so for all images in our dataset and all time steps.</figcaption></figure><p id="6f15" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ä¸€æ—¦æ¨¡å‹è¢«è®­ç»ƒ(ç®—æ³•1)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å»å™ªæ¨¡å‹æ¥é‡‡æ ·æ–°å›¾åƒ(ç®—æ³•2)ã€‚</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lo"><img src="../Images/efae3f473e2cc02f68e11e988e3be57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pgG9N8H2FvNWcv7oXwxpTA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Training and sampling algorithms. Once the model is trained, we can use it to generate brand new samples starting from gaussian noise.</figcaption></figure><h1 id="16b0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§</h1><p id="daf1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ—¢ç„¶æˆ‘ä»¬å¯¹æ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†æœ‰äº†å¤§è‡´çš„äº†è§£ï¼Œæ˜¯æ—¶å€™å®ç°æˆ‘ä»¬è‡ªå·±çš„ä¸œè¥¿äº†ã€‚ä½ å¯ä»¥è‡ªå·±åœ¨è¿™ä¸ª<a class="ae ka" href="https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colabç¬”è®°æœ¬</a>ä¸­æˆ–è€…ç”¨è¿™ä¸ª<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm" rel="noopener ugc nofollow" target="_blank"> GitHubåº“</a>è¿è¡Œä¸‹é¢çš„ä»£ç ã€‚</p><p id="8fc7" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å’Œå¾€å¸¸ä¸€æ ·ï¼Œè¿›å£åªæ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="f7f3" class="lu if hh lq b be lv lw l lx ly"># Import of libraries<br/>import random<br/>import imageio<br/>import numpy as np<br/>from argparse import ArgumentParser<br/><br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/><br/>import einops<br/>import torch<br/>import torch.nn as nn<br/>from torch.optim import Adam<br/>from torch.utils.data import DataLoader<br/><br/>from torchvision.transforms import Compose, ToTensor, Lambda<br/>from torchvision.datasets.mnist import MNIST, FashionMNIST<br/><br/># Setting reproducibility<br/>SEED = 0<br/>random.seed(SEED)<br/>np.random.seed(SEED)<br/>torch.manual_seed(SEED)<br/><br/># Definitions<br/>STORE_PATH_MNIST = f"ddpm_model_mnist.pt"<br/>STORE_PATH_FASHION = f"ddpm_model_fashion.pt"</span></pre><p id="83a6" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºå®éªŒå®šä¹‰å‡ ä¸ªå‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å†³å®šæ˜¯å¦è¦è¿è¡Œè®­ç»ƒå¾ªç¯ï¼Œæ˜¯å¦è¦ä½¿ç”¨æ—¶å°š-MNISTæ•°æ®é›†å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="8048" class="lu if hh lq b be lv lw l lx ly">no_train = False<br/>fashion = True<br/>batch_size = 128<br/>n_epochs = 20<br/>lr = 0.001<br/>store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"</span></pre><p id="4015" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœŸçš„æƒ³å±•ç¤ºå›¾åƒã€‚æˆ‘ä»¬å¯¹è®­ç»ƒå›¾åƒå’Œæ¨¡å‹ç”Ÿæˆçš„å›¾åƒéƒ½æ„Ÿå…´è¶£ã€‚æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªæ•ˆç”¨å‡½æ•°ï¼Œç»™å®šä¸€äº›å›¾åƒï¼Œå°†æ˜¾ç¤ºä¸€ä¸ªæ­£æ–¹å½¢(æˆ–å°½å¯èƒ½æ¥è¿‘)çš„å­å›¾å½¢ç½‘æ ¼:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="7b38" class="lu if hh lq b be lv lw l lx ly">def show_images(images, title=""):<br/>    """Shows the provided images as sub-pictures in a square"""<br/><br/>    # Converting images to CPU numpy arrays<br/>    if type(images) is torch.Tensor:<br/>        images = images.detach().cpu().numpy()<br/><br/>    # Defining number of rows and columns<br/>    fig = plt.figure(figsize=(8, 8))<br/>    rows = int(len(images) ** (1 / 2))<br/>    cols = round(len(images) / rows)<br/><br/>    # Populating figure with sub-plots<br/>    idx = 0<br/>    for r in range(rows):<br/>        for c in range(cols):<br/>            fig.add_subplot(rows, cols, idx + 1)<br/><br/>            if idx &lt; len(images):<br/>                plt.imshow(images[idx][0], cmap="gray")<br/>                idx += 1<br/>    fig.suptitle(title, fontsize=30)<br/><br/>    # Showing the figure<br/>    plt.show()</span></pre><p id="20dc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ä¸ºäº†æµ‹è¯•è¿™ä¸ªæ•ˆç”¨å‡½æ•°ï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®é›†å¹¶æ˜¾ç¤ºç¬¬ä¸€æ‰¹ã€‚<strong class="je hi">é‡è¦æç¤º:</strong>å›¾åƒå¿…é¡»åœ¨[-1ï¼Œ1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç½‘ç»œå¿…é¡»é¢„æµ‹æ­£æ€åˆ†å¸ƒçš„å™ªå£°å€¼:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="51e6" class="lu if hh lq b be lv lw l lx ly"># Shows the first batch of images<br/>def show_first_batch(loader):<br/>    for batch in loader:<br/>        show_images(batch[0], "Images in the first batch")<br/>        break</span></pre><pre class="lz lp lq lr bn ls lt bi"><span id="6357" class="lu if hh lq b be lv lw l lx ly"># Loading the data (converting each image into a tensor and normalizing between [-1, 1])<br/>transform = Compose([<br/>    ToTensor(),<br/>    Lambda(lambda x: (x - 0.5) * 2)]<br/>)<br/>ds_fn = FashionMNIST if fashion else MNIST<br/>dataset = ds_fn("./datasets", download=True, train=True, transform=transform)<br/>loader = DataLoader(dataset, batch_size, shuffle=True)</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es ma"><img src="../Images/ffdeb6cefa004560606aad3e13386e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*3j7Y0l_IaqmWhmWrLInqhg.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Images in our first batch. If you kept the same randomizing seed, you should get the exact same batch.</figcaption></figure><p id="5826" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¤ªå¥½äº†ï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªå¾ˆå¥½çš„æ•ˆç”¨å‡½æ•°ï¼Œæˆ‘ä»¬ç¨åä¹Ÿå°†æŠŠå®ƒç”¨äºæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚åœ¨æˆ‘ä»¬çœŸæ­£å¼€å§‹å¤„ç†DDPMæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä»colabè·å¾—ä¸€ä¸ªGPUè®¾å¤‡(é€šå¸¸æ˜¯écolab-proç”¨æˆ·çš„<em class="kg">ç‰¹æ–¯æ‹‰T4 </em>):</p><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mb"><img src="../Images/9e118c4e31cce0260dfe745b5c422fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trEQpvjm5RvsXjvIE6S6Kg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Getting a device and, if it is a GPU, printing its name</figcaption></figure><h1 id="b1f7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">DDPMæ¨¡å¼</h1><p id="83ab" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ—¢ç„¶æˆ‘ä»¬å·²ç»è§£å†³äº†çç¢çš„äº‹æƒ…ï¼Œç°åœ¨æ˜¯æ—¶å€™ç ”ç©¶DDPMäº†ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª<em class="kg"> MyDDPM </em> PyTorchæ¨¡å—ï¼Œå®ƒå°†è´Ÿè´£å­˜å‚¨betaså’Œalphaså€¼å¹¶åº”ç”¨è½¬å‘è¿‡ç¨‹ã€‚ç›¸åï¼Œå¯¹äºåå‘è¿‡ç¨‹ï¼Œ<em class="kg"> MyDDPM </em>æ¨¡å—å°†ç®€å•åœ°ä¾èµ–äºç”¨äºæ„å»ºDDPMçš„ç½‘ç»œ:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="4597" class="lu if hh lq b be lv lw l lx ly"># DDPM class<br/>class MyDDPM(nn.Module):<br/>    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 28, 28)):<br/>        super(MyDDPM, self).__init__()<br/>        self.n_steps = n_steps<br/>        self.device = device<br/>        self.image_chw = image_chw<br/>        self.network = network.to(device)<br/>        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(<br/>            device)  # Number of steps is typically in the order of thousands<br/>        self.alphas = 1 - self.betas<br/>        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)<br/><br/>    def forward(self, x0, t, eta=None):<br/>        # Make input image more noisy (we can directly skip to the desired step)<br/>        n, c, h, w = x0.shape<br/>        a_bar = self.alpha_bars[t]<br/><br/>        if eta is None:<br/>            eta = torch.randn(n, c, h, w).to(self.device)<br/><br/>        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta<br/>        return noisy<br/><br/>    def backward(self, x, t):<br/>        # Run each image through the network for each timestep t in the vector t.<br/>        # The network returns its estimation of the noise that was added.<br/>        return self.network(x, t)</span></pre><p id="d198" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ³¨æ„ï¼Œæ­£å‘è¿‡ç¨‹ç‹¬ç«‹äºç”¨äºå»å™ªçš„ç½‘ç»œï¼Œæ‰€ä»¥ä»æŠ€æœ¯ä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°å®ƒçš„æ•ˆæœã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªåº”ç”¨<strong class="je hi">ç®—æ³•2 </strong>(é‡‡æ ·è¿‡ç¨‹)ç”Ÿæˆæ–°å›¾åƒçš„æ•ˆç”¨å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªDDPMçš„ç‰¹å®šæ•ˆç”¨å‡½æ•°æ¥å®ç°:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="7c46" class="lu if hh lq b be lv lw l lx ly">def show_forward(ddpm, loader, device):<br/>    # Showing the forward process<br/>    for batch in loader:<br/>        imgs = batch[0]<br/><br/>        show_images(imgs, "Original images")<br/><br/>        for percent in [0.25, 0.5, 0.75, 1]:<br/>            show_images(<br/>                ddpm(imgs.to(device),<br/>                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),<br/>                f"DDPM Noisy images {int(percent * 100)}%"<br/>            )<br/>        break</span></pre><p id="b51d" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ä¸ºäº†ç”Ÿæˆå›¾åƒï¼Œæˆ‘ä»¬ä»éšæœºå™ªå£°å¼€å§‹ï¼Œè®©Tä»Tå›åˆ°0ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å™ªå£°ä¼°è®¡ä¸º<strong class="je hi"> eta_theta </strong>å¹¶åº”ç”¨å»å™ªå‡½æ•°ã€‚æœ€åï¼Œåƒæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ä¸€æ ·ï¼Œå¢åŠ äº†é¢å¤–çš„å™ªå£°ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="c8c2" class="lu if hh lq b be lv lw l lx ly">def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name="sampling.gif", c=1, h=28, w=28):<br/>    """Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples"""<br/>    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)<br/>    frames = []<br/><br/>    with torch.no_grad():<br/>        if device is None:<br/>            device = ddpm.device<br/><br/>        # Starting from random noise<br/>        x = torch.randn(n_samples, c, h, w).to(device)<br/><br/>        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):<br/>            # Estimating noise to be removed<br/>            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()<br/>            eta_theta = ddpm.backward(x, time_tensor)<br/><br/>            alpha_t = ddpm.alphas[t]<br/>            alpha_t_bar = ddpm.alpha_bars[t]<br/><br/>            # Partially denoising the image<br/>            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)<br/><br/>            if t &gt; 0:<br/>                z = torch.randn(n_samples, c, h, w).to(device)<br/><br/>                # Option 1: sigma_t squared = beta_t<br/>                beta_t = ddpm.betas[t]<br/>                sigma_t = beta_t.sqrt()<br/><br/>                # Option 2: sigma_t squared = beta_tilda_t<br/>                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t &gt; 0 else ddpm.alphas[0]<br/>                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t<br/>                # sigma_t = beta_tilda_t.sqrt()<br/><br/>                # Adding some more noise like in Langevin Dynamics fashion<br/>                x = x + sigma_t * z<br/><br/>            # Adding frames to the GIF<br/>            if idx in frame_idxs or t == 0:<br/>                # Putting digits in range [0, 255]<br/>                normalized = x.clone()<br/>                for i in range(len(normalized)):<br/>                    normalized[i] -= torch.min(normalized[i])<br/>                    normalized[i] *= 255 / torch.max(normalized[i])<br/><br/>                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame<br/>                frame = einops.rearrange(normalized, "(b1 b2) c h w -&gt; (b1 h) (b2 w) c", b1=int(n_samples ** 0.5))<br/>                frame = frame.cpu().numpy().astype(np.uint8)<br/><br/>                # Rendering frame<br/>                frames.append(frame)<br/><br/>    # Storing the gif<br/>    with imageio.get_writer(gif_name, mode="I") as writer:<br/>        for idx, frame in enumerate(frames):<br/>            writer.append_data(frame)<br/>            if idx == len(frames) - 1:<br/>                for _ in range(frames_per_gif // 3):<br/>                    writer.append_data(frames[-1])<br/>    return x</span></pre><p id="44c3" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ‰€æœ‰ä¸DDPMæœ‰å…³çš„äº‹æƒ…ç°åœ¨éƒ½æ‘†åœ¨æ¡Œé¢ä¸Šã€‚æˆ‘ä»¬åªéœ€è¦å®šä¹‰ä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šå›¾åƒå’Œå½“å‰æ—¶é—´æ­¥é•¿çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å°†å®é™…å®Œæˆé¢„æµ‹å›¾åƒä¸­å™ªå£°çš„å·¥ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå®šåˆ¶çš„U-Netæ¨¡å‹ã€‚ä¸è¨€è€Œå–»ï¼Œæ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å‹ã€‚</p><h1 id="ee60" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ä¼˜ä¿¡ç½‘</h1><p id="b252" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªä¿æŒç©ºé—´ç»´åº¦ä¸å˜çš„å—æ¥å¼€å§‹åˆ›å»ºæˆ‘ä»¬çš„U-Netã€‚è¿™ä¸ªå—å°†ç”¨äºæˆ‘ä»¬U-Netçš„æ¯ä¸ªçº§åˆ«ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="4cdd" class="lu if hh lq b be lv lw l lx ly">class MyBlock(nn.Module):<br/>    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):<br/>        super(MyBlock, self).__init__()<br/>        self.ln = nn.LayerNorm(shape)<br/>        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)<br/>        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)<br/>        self.activation = nn.SiLU() if activation is None else activation<br/>        self.normalize = normalize<br/><br/>    def forward(self, x):<br/>        out = self.ln(x) if self.normalize else x<br/>        out = self.conv1(out)<br/>        out = self.activation(out)<br/>        out = self.conv2(out)<br/>        out = self.activation(out)<br/>        return out</span></pre><p id="0a7e" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">DDPMsä¸­çš„æ£˜æ‰‹ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬çš„å›¾åƒåˆ°å›¾åƒæ¨¡å‹å¿…é¡»ä»¥å½“å‰æ—¶é—´æ­¥é•¿ä¸ºæ¡ä»¶ã€‚ä¸ºäº†åœ¨å®è·µä¸­åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­£å¼¦åµŒå…¥å’Œä¸€å±‚MLPsã€‚ç”±æ­¤äº§ç”Ÿçš„å¼ é‡å°†é€šè¿‡U-Netçš„æ¯ä¸€çº§æŒ‰ä¿¡é“æ·»åŠ åˆ°ç½‘ç»œçš„è¾“å…¥ä¸­ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="1979" class="lu if hh lq b be lv lw l lx ly">def sinusoidal_embedding(n, d):<br/>    # Returns the standard positional embedding<br/>    embedding = torch.zeros(n, d)<br/>    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])<br/>    wk = wk.reshape((1, d))<br/>    t = torch.arange(n).reshape((n, 1))<br/>    embedding[:,::2] = torch.sin(t * wk[:,::2])<br/>    embedding[:,1::2] = torch.cos(t * wk[:,::2])<br/><br/>    return embedding</span></pre><p id="8e11" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå°çš„æ•ˆç”¨å‡½æ•°ï¼Œè¯¥å‡½æ•°åˆ›å»ºäº†ä¸€ä¸ªç”¨äºç»˜åˆ¶ä½ç½®åµŒå…¥åœ°å›¾çš„ä¸€å±‚MLPã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="efdf" class="lu if hh lq b be lv lw l lx ly">def _make_te(self, dim_in, dim_out):<br/>  return nn.Sequential(<br/>    nn.Linear(dim_in, dim_out),<br/>    nn.SiLU(),<br/>    nn.Linear(dim_out, dim_out)<br/>  )</span></pre><p id="f055" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†å¦‚ä½•å¤„ç†æ—¶é—´ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„U-Netç½‘ç»œã€‚æˆ‘ä»¬å°†æœ‰3ä¸ªä¸‹é‡‡æ ·éƒ¨åˆ†ï¼Œä¸€ä¸ªç½‘ç»œä¸­é—´çš„ç“¶é¢ˆï¼Œå’Œ3ä¸ªå¸¦æœ‰é€šå¸¸çš„U-Netå‰©ä½™è¿æ¥(è¿æ¥)çš„ä¸Šé‡‡æ ·æ­¥éª¤ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="8229" class="lu if hh lq b be lv lw l lx ly">class MyUNet(nn.Module):<br/>    def __init__(self, n_steps=1000, time_emb_dim=100):<br/>        super(MyUNet, self).__init__()<br/><br/>        # Sinusoidal embedding<br/>        self.time_embed = nn.Embedding(n_steps, time_emb_dim)<br/>        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)<br/>        self.time_embed.requires_grad_(False)<br/><br/>        # First half<br/>        self.te1 = self._make_te(time_emb_dim, 1)<br/>        self.b1 = nn.Sequential(<br/>            MyBlock((1, 28, 28), 1, 10),<br/>            MyBlock((10, 28, 28), 10, 10),<br/>            MyBlock((10, 28, 28), 10, 10)<br/>        )<br/>        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)<br/><br/>        self.te2 = self._make_te(time_emb_dim, 10)<br/>        self.b2 = nn.Sequential(<br/>            MyBlock((10, 14, 14), 10, 20),<br/>            MyBlock((20, 14, 14), 20, 20),<br/>            MyBlock((20, 14, 14), 20, 20)<br/>        )<br/>        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)<br/><br/>        self.te3 = self._make_te(time_emb_dim, 20)<br/>        self.b3 = nn.Sequential(<br/>            MyBlock((20, 7, 7), 20, 40),<br/>            MyBlock((40, 7, 7), 40, 40),<br/>            MyBlock((40, 7, 7), 40, 40)<br/>        )<br/>        self.down3 = nn.Sequential(<br/>            nn.Conv2d(40, 40, 2, 1),<br/>            nn.SiLU(),<br/>            nn.Conv2d(40, 40, 4, 2, 1)<br/>        )<br/><br/>        # Bottleneck<br/>        self.te_mid = self._make_te(time_emb_dim, 40)<br/>        self.b_mid = nn.Sequential(<br/>            MyBlock((40, 3, 3), 40, 20),<br/>            MyBlock((20, 3, 3), 20, 20),<br/>            MyBlock((20, 3, 3), 20, 40)<br/>        )<br/><br/>        # Second half<br/>        self.up1 = nn.Sequential(<br/>            nn.ConvTranspose2d(40, 40, 4, 2, 1),<br/>            nn.SiLU(),<br/>            nn.ConvTranspose2d(40, 40, 2, 1)<br/>        )<br/><br/>        self.te4 = self._make_te(time_emb_dim, 80)<br/>        self.b4 = nn.Sequential(<br/>            MyBlock((80, 7, 7), 80, 40),<br/>            MyBlock((40, 7, 7), 40, 20),<br/>            MyBlock((20, 7, 7), 20, 20)<br/>        )<br/><br/>        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)<br/>        self.te5 = self._make_te(time_emb_dim, 40)<br/>        self.b5 = nn.Sequential(<br/>            MyBlock((40, 14, 14), 40, 20),<br/>            MyBlock((20, 14, 14), 20, 10),<br/>            MyBlock((10, 14, 14), 10, 10)<br/>        )<br/><br/>        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)<br/>        self.te_out = self._make_te(time_emb_dim, 20)<br/>        self.b_out = nn.Sequential(<br/>            MyBlock((20, 28, 28), 20, 10),<br/>            MyBlock((10, 28, 28), 10, 10),<br/>            MyBlock((10, 28, 28), 10, 10, normalize=False)<br/>        )<br/><br/>        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)<br/><br/>    def forward(self, x, t):<br/>        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)<br/>        t = self.time_embed(t)<br/>        n = len(x)<br/>        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)<br/>        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)<br/>        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)<br/><br/>        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)<br/><br/>        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)<br/>        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)<br/><br/>        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)<br/>        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)<br/><br/>        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)<br/>        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)<br/><br/>        out = self.conv_out(out)<br/><br/>        return out<br/><br/>    def _make_te(self, dim_in, dim_out):<br/>        return nn.Sequential(<br/>            nn.Linear(dim_in, dim_out),<br/>            nn.SiLU(),<br/>            nn.Linear(dim_out, dim_out)<br/>        )</span></pre><p id="fdcb" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†å»å™ªç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–ä¸€ä¸ªDDPMæ¨¡å‹ï¼Œå¹¶è¿›è¡Œä¸€äº›å¯è§†åŒ–å¤„ç†ã€‚</p><h1 id="6b6c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ä¸€äº›å¯è§†åŒ–</h1><p id="217d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„U-Netå®ä¾‹åŒ–DDPMæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="257b" class="lu if hh lq b be lv lw l lx ly"># Defining model<br/>n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors<br/>ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)</span></pre><p id="f6ff" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è®©æˆ‘ä»¬çœ‹çœ‹è½¬å‘è¿‡ç¨‹æ˜¯ä»€ä¹ˆæ ·å­çš„:</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="0ee2" class="lu if hh lq b be lv lw l lx ly"># Optionally, show the diffusion (forward) process<br/>show_forward(ddpm, loader, device)</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mc"><img src="../Images/cd6d480144910cca2a88c57905bf7bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lNCAyYvDTnUmVCS4MIY-g.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Result of running the forward process. Images get noisier and noisier with each step until just noise is left.</figcaption></figure><p id="63cf" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œä½†æˆ‘ä»¬å·²ç»å¯ä»¥ä½¿ç”¨å…è®¸æˆ‘ä»¬ç”Ÿæˆæ–°å›¾åƒçš„å‡½æ•°ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es md"><img src="../Images/6ad6d13b8e85b82a48a293eead9367bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*g3Wr3q4SZFyEAuaJ7hq67A.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Generating new images with a non-trained model. Noise is produced.</figcaption></figure><p id="06ab" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ¯«ä¸å¥‡æ€ªï¼Œå½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œä»€ä¹ˆä¹Ÿæ²¡å‘ç”Ÿã€‚ä½†æ˜¯ï¼Œå½“æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ã€‚</p><h1 id="fec0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">è®­ç»ƒå¾ªç¯</h1><p id="fe18" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æˆ‘ä»¬ç°åœ¨å®ç°ç®—æ³•1æ¥å­¦ä¹ å°†çŸ¥é“å¦‚ä½•å¯¹å›¾åƒå»å™ªçš„æ¨¡å‹ã€‚è¿™å¯¹åº”äºæˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ã€‚</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="a0ac" class="lu if hh lq b be lv lw l lx ly">def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path="ddpm_model.pt"):<br/>    mse = nn.MSELoss()<br/>    best_loss = float("inf")<br/>    n_steps = ddpm.n_steps<br/><br/>    for epoch in tqdm(range(n_epochs), desc=f"Training progress", colour="#00ff00"):<br/>        epoch_loss = 0.0<br/>        for step, batch in enumerate(tqdm(loader, leave=False, desc=f"Epoch {epoch + 1}/{n_epochs}", colour="#005500")):<br/>            # Loading data<br/>            x0 = batch[0].to(device)<br/>            n = len(x0)<br/><br/>            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars<br/>            eta = torch.randn_like(x0).to(device)<br/>            t = torch.randint(0, n_steps, (n,)).to(device)<br/><br/>            # Computing the noisy image based on x0 and the time-step (forward process)<br/>            noisy_imgs = ddpm(x0, t, eta)<br/><br/>            # Getting model estimation of noise based on the images and the time-step<br/>            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))<br/><br/>            # Optimizing the MSE between the noise plugged and the predicted noise<br/>            loss = mse(eta_theta, eta)<br/>            optim.zero_grad()<br/>            loss.backward()<br/>            optim.step()<br/><br/>            epoch_loss += loss.item() * len(x0) / len(loader.dataset)<br/><br/>        # Display images generated at this epoch<br/>        if display:<br/>            show_images(generate_new_images(ddpm, device=device), f"Images generated at epoch {epoch + 1}")<br/><br/>        log_string = f"Loss at epoch {epoch + 1}: {epoch_loss:.3f}"<br/><br/>        # Storing the model<br/>        if best_loss &gt; epoch_loss:<br/>            best_loss = epoch_loss<br/>            torch.save(ddpm.state_dict(), store_path)<br/>            log_string += " --&gt; Best model ever (stored)"<br/><br/>        print(log_string)</span></pre><p id="de14" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œåœ¨æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°å¯¹ä¸€äº›å›¾åƒè¿›è¡Œé‡‡æ ·ï¼Œå¹¶å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œéšæœºçš„æ—¶é—´æ­¥é•¿ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”¨æ­£å‘è¿‡ç¨‹ä½¿å®ƒä»¬æœ‰å™ªå£°ï¼Œå¹¶å¯¹è¿™äº›æœ‰å™ªå£°çš„å›¾åƒè¿è¡Œåå‘è¿‡ç¨‹ã€‚å®é™…æ·»åŠ çš„å™ªå£°å’Œæ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„MSEè¢«ä¼˜åŒ–ã€‚</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es me"><img src="../Images/31fc8a66a966e85f4b9bb38d129845b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*giuoyh5vEA4kNNbqij9ZTQ.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Training takes roughly 24 seconds per epoch. With 20 epochs, it takes roughly 8 minutes to train a DDPM model.</figcaption></figure><p id="5161" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘å°†è®­ç»ƒå‘¨æœŸè®¾ç½®ä¸º20ï¼Œå› ä¸ºæ¯ä¸ªå‘¨æœŸéœ€è¦24ç§’(è®­ç»ƒæ€»å…±å¤§çº¦éœ€è¦8åˆ†é’Ÿ)ã€‚è¯·æ³¨æ„ï¼Œæœ‰å¯èƒ½é€šè¿‡æ›´å¤šçš„çºªå…ƒã€æ›´å¥½çš„U-Netå’Œå…¶ä»–æŠ€å·§è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘çœç•¥äº†è¿™äº›ã€‚</p><h1 id="df23" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">æµ‹è¯•æ¨¡å‹</h1><p id="d3a1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ—¢ç„¶å·¥ä½œå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥äº«å—æˆæœäº†ã€‚æˆ‘ä»¬æ ¹æ®MSEæŸå¤±å‡½æ•°åŠ è½½è®­ç»ƒæœŸé—´è·å¾—çš„æœ€ä½³æ¨¡å‹ï¼Œå°†å…¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç”Ÿæˆæ–°æ ·æœ¬</p><pre class="kh ki kj kk fd lp lq lr bn ls lt bi"><span id="b4b5" class="lu if hh lq b be lv lw l lx ly"># Loading the trained model<br/>best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)<br/>best_model.load_state_dict(torch.load(store_path, map_location=device))<br/>best_model.eval()<br/>print("Model loaded")</span></pre><pre class="lz lp lq lr bn ls lt bi"><span id="0d58" class="lu if hh lq b be lv lw l lx ly">print("Generating new images")<br/>generated = generate_new_images(<br/>        best_model,<br/>        n_samples=100,<br/>        device=device,<br/>        gif_name="fashion.gif" if fashion else "mnist.gif"<br/>    )<br/>show_images(generated, "Final result")</span></pre><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es mf"><img src="../Images/9159ecb2c6324b7ce48b9cc6720d4113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*uLr2G8QGlCtNBA92Ukmgag.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Final result on the MNIST dataset</figcaption></figure><p id="c017" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">è›‹ç³•ä¸Šçš„æ¨±æ¡ƒæ˜¯æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°è‡ªåŠ¨åˆ›å»ºæ‰©æ•£è¿‡ç¨‹çš„æ¼‚äº®gifçš„äº‹å®ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨Colabä¸­å¯è§†åŒ–gif:</p><figure class="kh ki kj kk fd km er es paragraph-image"><div class="er es mg"><img src="../Images/6f8fe8cf57ba763fb696be8e17bd409f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*or9fTUuSybnRvlYTIhStVw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx">Showing the generated gif image</figcaption></figure><div class="kh ki kj kk fd ab cb"><figure class="kl km mh ko kp kq kr paragraph-image"><img src="../Images/89ee312cd28d38435fcb520d2640b3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/1*byEJ4YdbrSjDHyRowXL8-w.gif"/></figure><figure class="kl km mh ko kp kq kr paragraph-image"><img src="../Images/bd2cc02a6962eec2ac69fb3124cd3651.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/1*d3o6yuMPthgNFTP208HoZQ.gif"/><figcaption class="kz la et er es lb lc bd b be z dx mi di mj lf">Obtained GIFs for Fashion-MNIST and MNIST datasets.</figcaption></figure></div><p id="ff01" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬å®Œäº†ã€‚æˆ‘ä»¬ç»ˆäºè®©æˆ‘ä»¬çš„DDPMæ¨¡å‹å·¥ä½œäº†ï¼</p><h1 id="171b" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">è¿›ä¸€æ­¥çš„æ”¹è¿›</h1><p id="01ad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å·²ç»åšäº†è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œä»¥å…è®¸<a class="ae ka" href="https://arxiv.org/pdf/2006.09011.pdf" rel="noopener ugc nofollow" target="_blank">ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒ</a>ï¼Œ<a class="ae ka" href="https://arxiv.org/pdf/2010.02502.pdf" rel="noopener ugc nofollow" target="_blank">åŠ é€Ÿé‡‡æ ·</a>æˆ–è·å¾—<a class="ae ka" href="https://arxiv.org/abs/2102.09672" rel="noopener ugc nofollow" target="_blank">æ›´å¥½çš„æ ·æœ¬è´¨é‡å’Œå¯èƒ½æ€§</a>ã€‚Imagenå’ŒDALL-E 2å‹å·åŸºäºåŸå§‹DDPMsçš„æ”¹è¿›ç‰ˆæœ¬ã€‚</p><h1 id="95cf" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">æ›´å¤šå‚è€ƒ</h1><p id="05a8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å…³äºDDPMsçš„æ›´å¤šå‚è€ƒï¼Œæˆ‘å¼ºçƒˆæ¨èé˜…è¯»Lilian Wengå’ŒNiels Roggeçš„<a class="ae ka" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="noopener ugc nofollow" target="_blank">æ°å‡ºæ–‡ç« </a>å’ŒKashif Rasulçš„æƒŠäºº<a class="ae ka" href="https://huggingface.co/blog/annotated-diffusion" rel="noopener ugc nofollow" target="_blank">æ‹¥æŠ±è„¸åšå®¢</a>ã€‚Colabç¬”è®°æœ¬çš„æœ€åè¿˜æåˆ°äº†å…¶ä»–ä½œè€…ã€‚</p><h1 id="c8ea" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ç»“è®º</h1><p id="5fcc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ‰©æ•£æ¨¡å‹æ˜¯å­¦ä¹ è¿­ä»£å»å™ªå›¾åƒçš„ç”Ÿæˆæ¨¡å‹ã€‚ä»ä¸€äº›å™ªå£°å¼€å§‹ï¼Œç„¶åå¯ä»¥è¦æ±‚æ¨¡å‹å¯¹æ ·æœ¬å»å™ªå£°ï¼Œç›´åˆ°è·å¾—ä¸€äº›çœŸå®çš„å›¾åƒã€‚</p><p id="e5b6" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">æˆ‘ä»¬åœ¨PyTorchä¸­ä»å¤´åˆ›å»ºäº†ä¸€ä¸ªDDPMï¼Œå¹¶è®©å®ƒå­¦ä¹ å»å™ªMNIST /æ—¶å°šMNISTçš„å›¾åƒã€‚ç»è¿‡è®­ç»ƒåï¼Œè¿™ä¸ªæ¨¡å‹æœ€ç»ˆèƒ½å¤Ÿä»éšæœºå™ªå£°ä¸­ç”Ÿæˆæ–°çš„å›¾åƒã€‚å¾ˆç¥å¥‡ï¼Œå¯¹å§ï¼Ÿ</p><p id="5361" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">å¸¦æœ‰æ‰€ç¤ºå®ç°çš„Colabç¬”è®°æœ¬å¯ä»¥åœ¨<a class="ae ka" href="https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing" rel="noopener ugc nofollow" target="_blank">è¿™ä¸ªé“¾æ¥</a>å…è´¹è®¿é—®ï¼Œè€Œ<a class="ae ka" href="https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm" rel="noopener ugc nofollow" target="_blank"> GitHubåº“</a>åŒ…å«ã€‚pyæ–‡ä»¶ã€‚å¦‚æœä½ è§‰å¾—è¿™ä¸ªæ•…äº‹æœ‰ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä¸ºå®ƒé¼“æŒ<strong class="je hi">ğŸ‘</strong>ã€‚å¦‚æœä½ è§‰å¾—æœ‰ä»€ä¹ˆä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œä¸è¦çŠ¹è±«ï¼Œç›´æ¥è”ç³»æˆ‘ï¼æˆ‘å¾ˆé«˜å…´ä¸ä½ è®¨è®ºå®ƒã€‚</p><div class="mk ml ez fb mm mn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hi fi z dy ms ea eb mt ed ef hg bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb kw mn"/></div></div></a></div></div></div>    
</body>
</html>