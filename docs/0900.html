<html>
<head>
<title>Text to Image Synthesis Using Multimodal (VQGAN + CLIP) Architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用多模态(VQGAN + CLIP)架构的文本到图像合成</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/text-to-image-synthesis-using-multimodal-vqgan-clip-architectures-fab2d243f9dd?source=collection_archive---------2-----------------------#2021-08-17">https://medium.com/mlearning-ai/text-to-image-synthesis-using-multimodal-vqgan-clip-architectures-fab2d243f9dd?source=collection_archive---------2-----------------------#2021-08-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/95209f3f03920d5c5b6c9edbea8acb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*qalBx3e2XDufNjqiuAS5jQ.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">After synthesis of text input “A man fighting with a bull”</figcaption></figure><p id="e817" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">怎么了，伙计们！！</p><p id="7b30" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我在媒体上的第一个故事。</p><p id="c276" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">今天，我将使用一个名为<strong class="ir hi"> clip </strong>的多模态深度学习架构来做一个GAN项目，该项目将文本与视觉元素相链接。我将把它与一个生成模型结合起来，一个转换类型的架构，这样我就可以接受文本提示并生成视觉和图像，甚至可以根据文本提示制作视频序列。</p><p id="71ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以和我一起做。让我们开始吧:</p><p id="0cf2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我将与两个尖端的生成架构合作，它们将允许我们进行多模态生成。连接不同模态的能力，在这种情况下，是文本和图像。文本和视觉元素结合了两种架构:<strong class="ir hi"> CLIP </strong>架构(OpenAI)和驯服变形金刚</p><h1 id="2c74" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">夹</strong>架构:</h1><p id="b445" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">从自然语言监督和<strong class="ir hi">剪辑</strong>中学习可转移的视觉模型意味着<strong class="ir hi">C</strong>ontrastive<strong class="ir hi">L</strong>language<strong class="ir hi">I</strong>mage<strong class="ir hi">P</strong>再训练。</p><p id="435f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">经训练的模型预测哪种文本编码以及哪种文本编码对应于哪种视觉编码的哪种编码。</p><p id="4a80" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">【<em class="kq">来源:</em> <a class="ae kr" href="https://github.com/openai/CLIP" rel="noopener ugc nofollow" target="_blank"> <em class="kq">回形针GitHub、</em> </a> <em class="kq"> </em> <a class="ae kr" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank"> <em class="kq">回形针</em> </a> <em class="kq">、</em> <a class="ae kr" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> <em class="kq">博客</em> </a> <em class="kq"> </em></p><h1 id="98f1" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">驯服变形金刚:</strong></h1><p id="439d" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">一种生成式架构，可以创建和发明文本，以便在文本提示后显示越来越多的文本。这是一种不同类型的变压器，使用一种称为VQGAN的架构。它将卷积架构的元素与GAN类型的元素相结合。它使用密码本，处理补丁。</p><p id="a047" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建文本元素序列并不十分困难，但是图像具有非常大的像素数量，这导致在变形金刚中创建长序列。</p><p id="9db9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是我们可以通过使用补丁来解决这个问题。16×16像素片，这样我们就降低了问题的维度。</p><p id="211e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Codebook训练网络来学习矩阵，而不是直接处理像素。它学习存储在码本中的图像部分的各种表示，然后当它通过解码器生成时，它通过获取码本的部分来工作。(它有不同的部分，卷积，解码器等。)</p><h1 id="a04a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">优化过程:</strong></h1><p id="6851" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">获取<strong class="ir hi">文本短语</strong>并通过CLIP架构对其进行编码。并获得512个数字的编码(架构的编码，对该文本的剪辑架构的理解)。对图像做同样的事情，但不是按原样发送图像，而是放大、旋转、移动或创建裁剪图像(20，30，40，50，…)。).在这种情况下，我将创建30种不同的作物。这样做是为了通过给它多个版本来帮助架构更好地理解图像。因此，我将发送30组图像剪辑(带有特定的旋转、平移)并对这些剪辑的理解进行编码。30组编码或512个值。</p><p id="97bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我将使用函数<strong class="ir hi">余弦相似度</strong>来比较这些编码，这是一个数学函数，用于计算数学向量的相似度。这将有助于计算损耗值，网络的性能。</p><p id="ea07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我设法使文本的编码尽可能与图像的裁剪编码相似，这将意味着图像的内容与文本的内容相匹配。</p><p id="d830" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">包含提示:</strong></p><p id="9f7a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们想要的结果:</p><ul class=""><li id="4a2f" class="ks kt hh ir b is it iw ix ja ku je kv ji kw jm kx ky kz la bi translated">珠穆朗玛峰上的一只猴子</li><li id="dcfa" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated">20个人在天空中滑翔伞</li><li id="e261" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated">两个人在踢足球</li></ul><p id="5ffd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">排除提示:</strong></p><p id="e1b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们不希望的结果是:</p><ul class=""><li id="da17" class="ks kt hh ir b is it iw ix ja ku je kv ji kw jm kx ky kz la bi translated">不希望图像中有蓝色。</li><li id="395a" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated">不希望图像中有令人困惑的东西。</li></ul><p id="3e01" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">额外提示:</strong></p><p id="cc25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们希望应用于所有<strong class="ir hi">的内容包括</strong>提示:</p><p id="7ddd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如。</p><p id="3f60" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">包括提示:</p><ul class=""><li id="6157" class="ks kt hh ir b is it iw ix ja ku je kv ji kw jm kx ky kz la bi translated">一个穿着粉红色夹克的小伙子。</li><li id="3b9a" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated">下棋的男孩</li><li id="490b" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated">一只绿腿的大象</li></ul><p id="4189" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">额外提示:</p><ul class=""><li id="ee05" class="ks kt hh ir b is it iw ix ja ku je kv ji kw jm kx ky kz la bi translated">水彩纸纹理</li></ul><p id="bd21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后水彩纸纹理将适用于所有的包括提示。</p><p id="e9f1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将它们发送到clip进行编码，并最终在优化过程中计算损失。对包含文本的编码给予一定的权重，并排除那些要惩罚的文本，使得应该被排除的文本应该增加价值，而应该被包含的文本应该减少损失价值。</p><p id="536d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，可以从特定的图像/图片而不是噪声输入开始生成，并在文本提示指定的方向上修改、编辑该图像。</p><p id="1ac3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，在生成新的图像之后，我将向你们展示在由该过程生成的潜在参数之间创建插值的能力，然后制作一个视频，显示从一个创作到其他创作的插值，然后在屏幕上显示该视频。</p><h1 id="b591" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">编码和执行:</strong></h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f583" class="lp jo hh ll b fi lq lr l ls lt">!git clone <a class="ae kr" href="https://github.com/openai/CLIP.git" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/CLIP.git</a><br/>!git clone <a class="ae kr" href="https://github.com/CompVis/taming-transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/CompVis/taming-transformers</a></span></pre><p id="5c2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/04d247be2a485d1b6352d76321754360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mSITTKNecCdqAFzv"/></div></div></figure><p id="a998" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">安装一些库:</strong></p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="c229" class="lp jo hh ll b fi lq lr l ls lt">!pip install — no-deps ftfy regex tqdm<br/>!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8 <br/>!pip uninstall torchtext — yes<br/>!pip install einops</span></pre><p id="c99b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lz"><img src="../Images/80d5b09b456d72c70d8d36b60ece89e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Hp_RcrtUneqdK10Y"/></div></div></figure><p id="c2d0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">导入库:</strong></p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="e10a" class="lp jo hh ll b fi lq lr l ls lt"># import libraries<br/>import numpy as np<br/>import torch, os, imageio, pdb, math<br/>import torchvision<br/>import torchvision.transforms as T<br/>import torchvision.transforms.functional as TF </span><span id="1bdd" class="lp jo hh ll b fi ma lr l ls lt">import PIL<br/>import matplotlib.pyplot as plt</span><span id="74ec" class="lp jo hh ll b fi ma lr l ls lt">import yaml <br/>from omegaconf import OmegaConf</span><span id="cf59" class="lp jo hh ll b fi ma lr l ls lt">from CLIP import clip</span><span id="f1bf" class="lp jo hh ll b fi ma lr l ls lt">#import warnings<br/>#warnings.filterwarnings(‘ignore’)</span></pre><p id="c034" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">助手功能:</strong></p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="ddb5" class="lp jo hh ll b fi lq lr l ls lt">def ShowFromTensor(tensor):<br/>  img = tensor.clone()<br/>  img = img.mul(255).byte()<br/>  img = img.cpu().numpy().transpose((1,2,0))</span><span id="885e" class="lp jo hh ll b fi ma lr l ls lt">plt.figure(figsize=(10,7))<br/>  plt.axis('off')<br/>  plt.imshow(img)<br/>  plt.show()</span><span id="b4ac" class="lp jo hh ll b fi ma lr l ls lt">def NormData(data):<br/>  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result</span><span id="15e8" class="lp jo hh ll b fi ma lr l ls lt">### Parameters <br/>learning_rate = .5<br/>batch_size = 1<br/>wd = .1 ## weight decay is regularization parameter, help to limit the size of weight and improve generalization capabilities of the architecture<br/>noise_factor = .22 ## .1, .14 ...</span><span id="a75e" class="lp jo hh ll b fi ma lr l ls lt">total_iter=100 ## use more no. of iterations for more polished result <br/>im_shape = [450, 450, 3] # height, width, channel<br/>size1, size2, channels = im_shape</span></pre><p id="1aac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">现在创建一个剪辑模型:</strong></p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="bdda" class="lp jo hh ll b fi lq lr l ls lt"># Create CLIP model<br/>clipmodel, _ = clip.load('ViT-B/32', jit=False)<br/>clipmodel.eval()<br/>print(clip.available_models())</span><span id="6f03" class="lp jo hh ll b fi ma lr l ls lt">print("Clip model visual input resolution: ", clipmodel.visual.input_resolution)</span><span id="f864" class="lp jo hh ll b fi ma lr l ls lt">device=torch.device("cuda:0")<br/>torch.cuda.empty_cache()</span></pre><p id="e878" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mb"><img src="../Images/61ee43ef56ae042140bd27b42d12b156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9Ix0na60cwDekyf0"/></div></div></figure><p id="fea4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">剪辑下载一个预先训练好的模型，所以我们不需要训练剪辑，我们可以继续使用它在推理，eval模式直接编码文本和图像。</p><p id="7f65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kq"> ['RN50 '，' RN101 '，' RN50x4 '，' RN50x16 '，' ViT-B/32 '，' ViT-B/16'] </em>是CLIP可以在内部使用的架构类型。(例如，RN50—50层resNet架构，ViT-B/32 —可视化变压器)</p><p id="f56f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">和剪辑模型视觉输入分辨率224像素。这意味着，当我们在CLIP中编码图像时，我们需要将它们设置为224像素。</p><h1 id="2420" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">驯服变压器型号:</strong></h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="ef61" class="lp jo hh ll b fi lq lr l ls lt">%cd taming-transformers/</span><span id="a751" class="lp jo hh ll b fi ma lr l ls lt">!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints<br/>!mkdir -p models/vqgan_imagenet_f16_16384/configs</span><span id="1c32" class="lp jo hh ll b fi ma lr l ls lt">if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:<br/>   !wget '<a class="ae kr" href="https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'" rel="noopener ugc nofollow" target="_blank">https://heibox.uni-heidelberg.de/f/867b05fc8c481768640/?dl=1'</a> -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt' <br/>   !wget '<a class="ae kr" href="https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'" rel="noopener ugc nofollow" target="_blank">https://heibox.uni-heidelberg.de/f/274fb24ed3831bfa753/?dl=1'</a> -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'</span></pre><p id="fb86" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，<em class="kq"> vqgan_imagenet_f16_16384 </em>意味着vqgan图像网是用来自图像元数据集f-16的图像来训练的，因为该文件是使用每个图像的下采样因子f16来命名的。16384是码本维数。</p><p id="43f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在创建两个检查点和配置文件夹(如果它们还不存在)。</p><p id="0716" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这两条链路到目前为止都在尼泊尔工作，但是如果它们在将来不能工作，应该会有一些替代方案。</p><p id="7564" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过运行这个，它下载海德堡的预训练VQGAN模型、最后的检查点和配置信息，这些信息将在<em class="kq"> model.yaml </em>文件中。</p><p id="4eb1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mc"><img src="../Images/7882de5bf5821602d72e2f72a2a79fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*71_KLK7bmdWOMJcY"/></div></div></figure><p id="6065" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在下载检查点后，让我们实例化驯服变压器VQGAN架构。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="94e2" class="lp jo hh ll b fi lq lr l ls lt">from taming.models.vqgan import VQModel</span><span id="dd2c" class="lp jo hh ll b fi ma lr l ls lt">def LoadConfig(config_path, display=False):<br/>   config_data = OmegaConf.load(config_path)<br/>   if display:<br/>     print(yaml.dump(OmegaConf.to_container(config_data)))<br/>   return config_data</span><span id="bca5" class="lp jo hh ll b fi ma lr l ls lt">def LoadVQGAN(config, chk_path=None):<br/>  model = VQModel(**config.model.params)<br/>  if chk_path is not None:<br/>    state_dict = torch.load(chk_path, map_location="cpu")["state_dict"]<br/>    missing, unexpected = model.load_state_dict(state_dict, strict=False)<br/>  return model.eval()</span><span id="6408" class="lp jo hh ll b fi ma lr l ls lt">def Generator(x):<br/>  x = taming_model.post_quant_conv(x)<br/>  x = taming_model.decoder(x)<br/>  return x</span><span id="8a01" class="lp jo hh ll b fi ma lr l ls lt">taming_config=LoadConfig("./models/vqgan_imagenet_f16_16384/configs/model.yaml", display=True)<br/>taming_model=LoadVQGAN(taming_config,chk_path="./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt").to(device)</span></pre><p id="d786" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，使用库OmegaConf创建一个用于加载配置的函数，并转储到YAML文件以查看配置。</p><p id="41b5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其次，创建另一个函数来加载VQGAN模型。将先前导入的模型与配置文件一起使用。包含模型所有参数的Load state_dict字典。并使用路径加载检查点，将位置映射到“CPU”。现在将字典加载到模型中。</p><p id="b1be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，我们将直接使用一个模型，无需训练，因此将其置于评估或eval()模式。</p><p id="eb47" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第三个也是最后一个函数是生成器。接受输入并从中生成图像。为此，输入通过驯服模型传递，输出传递给解码器。</p><p id="6c48" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后在模型上加载配置。</p><p id="ab08" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(<strong class="ir hi"> <em class="kq">模型</em> </strong>文件夹和我们之前做的一样在<strong class="ir hi"> <em class="kq">驯服-变形金刚</em> </strong>文件夹内，所以路径相对于那个文件夹的内容)</p><p id="2b2d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es md"><img src="../Images/2d9bd0505ad17652b44077e395e3d9b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xHv5Da4yGAAIqWkH"/></div></div></figure><p id="b349" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">声明我们要优化的值(潜在空间、参数):</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="1eeb" class="lp jo hh ll b fi lq lr l ls lt">class Parameters(torch.nn.Module):<br/>  def __init__(self):<br/>    super(Parameters, self).__init__()<br/>    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda() # 1x256x28.125x28.125 (450/16, 450/16)<br/>    self.data = torch.nn.Parameter(torch.sin(self.data))</span><span id="7eb3" class="lp jo hh ll b fi ma lr l ls lt">def forward(self):<br/>    return self.data</span><span id="62aa" class="lp jo hh ll b fi ma lr l ls lt">def init_params():<br/>  params=Parameters().cuda()<br/>  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr':learning_rate}], weight_decay=wd)<br/>  return params, optimizer</span></pre><p id="4d3f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">用来自正态分布的随机数初始化数据，参数为批次大小、通道和大小，其中大小根据需要除以16(16 x 16个补丁)。对于随机值为1的张量，它将是1x 256 x 28.125 x 28.125 .(450/16 = 28.125，450/16 = 28.125)</p><p id="f3a0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，乘以0.5是因为个人实验发现，在从正态分布获取数据之前，乘以0.5是好的。</p><p id="b2c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，在递归神经网络中，它们有基于架构本身的元素定位的明确指示，但在转换架构中，你只需一次推送所有数据。这意味着使用周期性数学函数将定位信息嵌入数据中。我们用的是数学正弦。</p><p id="bbdd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建一个转发函数，返回刚刚调用的数据。</p><p id="5b53" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，创建一个助手函数。在这个调用中，函数(参数)，随叫随到刷新和重置那些参数，然后声明优化器。(Adam optimizer用于此。)</p><p id="d82f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">剪辑架构中文本提示的编码:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="3b5d" class="lp jo hh ll b fi lq lr l ls lt"># Let’s Encode prompts<br/>normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))</span><span id="9347" class="lp jo hh ll b fi ma lr l ls lt">def encodeText(text):<br/>  t=clip.tokenize(text).cuda()<br/>  t=clipmodel.encode_text(t).detach().clone()<br/>  return t</span><span id="04b0" class="lp jo hh ll b fi ma lr l ls lt">def createEncodings(include, exclude, extras):<br/>  include_enc=[]<br/>  for text in include:<br/>    include_enc.append(encodeText(text))<br/>  exclude_enc=encodeText(exclude) if exclude != '' else 0<br/>  extras_enc=encodeText(extras) if extras !='' else 0</span><span id="e0c3" class="lp jo hh ll b fi ma lr l ls lt">return include_enc, exclude_enc, extras_enc</span><span id="1920" class="lp jo hh ll b fi ma lr l ls lt">augTransform = torch.nn.Sequential(<br/>    torchvision.transforms.RandomHorizontalFlip(),<br/>    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)  <br/>).cuda()</span><span id="0c78" class="lp jo hh ll b fi ma lr l ls lt">Params, optimizer = init_params()</span><span id="b90e" class="lp jo hh ll b fi ma lr l ls lt">with torch.no_grad():<br/>  print(Params().shape)<br/>  img= NormData(Generator(Params()).cpu()) # 1 x 3 x 450 x 450 [450 x 450]<br/>  print("img dimensions: ",img.shape)<br/>  ShowFromTensor(img[0])</span></pre><p id="b71a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，在这个架构中需要标准化，因此定义一个变量，并使用已经为我们准备好的平均值、方差值。</p><p id="2aef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建一个接收文本的函数，然后调用剪辑模型来标记文本。通过模型的编码函数传递结果，并从梯度计算中分离结果，并将克隆结果，以便我们可以在变量上使用自己的内存空间。</p><p id="ad14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在定义一个通用函数，它将应用于排除、包含和附加的提示。</p><p id="58ff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当发送图像进行编码时，我们不会发送任何图像。我们还发送了一组农作物的图像。声明一个用于图像增强变换的变量。在做了一些随机旋转之后，然后一些平移和剩余的用零填充。</p><p id="0be3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">[ <strong class="ir hi">注:</strong> <em class="kq">增强首先应用于生成器生成的单个图像，然后从该输出创建不同的作物。</em> ]</p><p id="164b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">初始化参数和优化器，并做一个简单的测试，以生成一个图像通过一个变压器发电机与随机的初始参数。调用生成器函数，传递我们之前声明的参数，并将结果返回给CPU。它将在第一维返回1，然后是三个通道，448，448。<strong class="ir hi"> ( <em class="kq"> 1 x 3 x 448 x 448 [450，450]但是由于四舍五入，我们得到448。</em> ) </strong></p><p id="0fab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es me"><img src="../Images/3a3ab160d4fbb53084149afec3fd0945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CA4Pb_EpcO1P6A6A"/></div></div></figure><p id="a860" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建作物:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="7327" class="lp jo hh ll b fi lq lr l ls lt">def createCrops(img, num_crops=32): <br/>  p=size1//2<br/>  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 672 x 672 (adding 112*2 on all sides to 448x448)</span><span id="7e41" class="lp jo hh ll b fi ma lr l ls lt">  img = augTransform(img) #RandomHorizontalFlip and RandomAffine</span><span id="c764" class="lp jo hh ll b fi ma lr l ls lt">  crop_set = []<br/>    for ch in range(num_crops):<br/>      gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)<br/>      offsetx = torch.randint(0, int(size1*2-gap1),())<br/>      offsety = torch.randint(0, int(size1*2-gap1),())</span><span id="d036" class="lp jo hh ll b fi ma lr l ls lt">      crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]</span><span id="ad32" class="lp jo hh ll b fi ma lr l ls lt">      crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)<br/>      crop_set.append(crop)</span><span id="8248" class="lp jo hh ll b fi ma lr l ls lt">  img_crops=torch.cat(crop_set,0) ## 30 x 3 x 224 x 224</span><span id="b732" class="lp jo hh ll b fi ma lr l ls lt">  randnormal = torch.randn_like(img_crops, requires_grad=False)<br/>  num_rands=0<br/>  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda() #32<br/>  <br/>  for ns in range(num_rands):<br/>    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()</span><span id="d5f1" class="lp jo hh ll b fi ma lr l ls lt">  img_crops = img_crops + noise_factor*randstotal*randnormal</span><span id="da27" class="lp jo hh ll b fi ma lr l ls lt">  return img_crops</span></pre><p id="7e7a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建一个名为<strong class="ir hi"> createCrops </strong>的函数。让我们为作物数量值设置默认值32。在图像周围添加一些填充，以便我们可以旋转，平移和保存图像信息。</p><p id="7f28" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们将填充设置为高度除以2或3。然后将各边的填充设置为(p，p，p，p)。所以维数会是1 x 3 x(448+112+112)x(448+112+112)= 1 x 3 x 672 x 672。</p><p id="5f38" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在应用先前声明的增强转换(augTransforms)。设置最初为空的作物。定义一对变量或像素数或数字，用于偏移和裁剪量，这里定义了间隙1和间隙2。</p><p id="53dc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">裁剪图像，保持前两个尺寸不变，更改后两个尺寸，用间隙1扩展。</p><p id="2f0f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将裁剪尺寸调整为224x224，并累积到我们之前定义的裁剪字典中。分辨率将为30 x 3 x 224 x 224。</p><p id="544d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，给作物添加一些噪声。具有噪声因子和来自相同维度的正态分布的一些随机值。</p><p id="e909" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">达芬奇Sfumato </strong>纹理:-</p><p id="0bc9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面代码中的行实际上是为了sfumato效果。(最后你就知道了。)</p><p id="2529" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">rand normal = torch . randn _ like(img _ crops，requires _ grad = False)<br/>num _ rands = 0<br/>randstotal = torch . rand((img _ crops . shape[0]，1，1，1))。cuda()<em class="kq"># 32</em><br/><br/>for ns in range(num _ rands):<br/>randstotal * = torch . rand((img _ crops . shape[0]，1，1，1))。cuda()<br/>img _ crops = img _ crops+noise _ factor * randstotal * rand normal</p><p id="df9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">【<strong class="ir hi"> <em class="kq">注:</em> </strong> <em class="kq">达芬奇</em> <a class="ae kr" href="https://en.wikipedia.org/wiki/Sfumato#:~:text=Sfumato%20(Italian%3A%20%5Bsfu%CB%88ma%CB%90to%5D,out%2Dof%2Dfocus%20plane." rel="noopener ugc nofollow" target="_blank"> <em class="kq"> Sfumato </em> </a>】</p><p id="9dba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们创建一个函数来查看任何状态下生成的图像:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="2700" class="lp jo hh ll b fi lq lr l ls lt">def showMe(Params, show_crop):<br/>  with torch.no_grad():<br/>    generated = Generator(Params())</span><span id="8178" class="lp jo hh ll b fi ma lr l ls lt">    if (show_crop):<br/>      print(“Augmented cropped example”)<br/>      aug_gen = generated.float() # 1 x 3 x 224 x 400 <br/>      aug_gen = createCrops(aug_gen, num_crops=1)<br/>      aug_gen_norm = NormData(aug_gen[0])<br/>      ShowFromTensor(aug_gen_norm)</span><span id="0352" class="lp jo hh ll b fi ma lr l ls lt">    print(“Generation”)<br/>    latest_gen=NormData(generated.cpu()) # 1 x 3 x 224 x 400<br/>    ShowFromTensor(latest_gen[0])</span><span id="f19e" class="lp jo hh ll b fi ma lr l ls lt">  return (latest_gen[0])</span></pre><p id="cb15" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里先不涉及梯度。传递我们用于优化生成器的参数，以在参数的当前状态下生成示例图像。(1 x 3 x 450 x 450)</p><p id="4872" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在调用createCrops函数来创建作物，但只能创建一种作物。将结果标准化并显示。</p><p id="c900" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后对于生成，最新的生成将正常化我们已经生成的内容。最后，展示图像。</p><p id="42dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过调整参数优化模式:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="e5c5" class="lp jo hh ll b fi lq lr l ls lt"># Optimization process</span><span id="6727" class="lp jo hh ll b fi ma lr l ls lt">def optimizeResult(Params, prompt):<br/>  alpha=1 ## the importance of the include encodings<br/>  beta=.5 ## the importance of the exclude encodings</span><span id="cc3a" class="lp jo hh ll b fi ma lr l ls lt">## image encoding<br/>  out = Generator(Params())<br/>  out = NormData(out)<br/>  out = createCrops(out)<br/>  out = normalize(out) # 32 x 3 x 448 x 448<br/>  image_enc=clipmodel.encode_image(out) ## 32 x 512</span><span id="0f0f" class="lp jo hh ll b fi ma lr l ls lt">## text encoding  w1 and w2<br/>  final_enc = w1*prompt+w1*extras_enc # prompt and extras_enc:1x512<br/>  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True) #1x512<br/>  final_text_exclude_enc = exclude_enc</span><span id="e8ca" class="lp jo hh ll b fi ma lr l ls lt">## calculate the loss<br/>  main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1) # 32<br/>  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1) # 32</span><span id="dbe4" class="lp jo hh ll b fi ma lr l ls lt">  final_loss = -alpha*main_loss + beta*penalize_loss</span><span id="451d" class="lp jo hh ll b fi ma lr l ls lt">  return final_loss</span><span id="eb28" class="lp jo hh ll b fi ma lr l ls lt">def Optimize(Params, optimizer, prompt):<br/>  loss = optimizeResult(Params, prompt).mean()<br/>  optimizer.zero_grad()<br/>  loss.backward()<br/>  optimizer.step()<br/>  return loss</span></pre><p id="4075" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们创建一个名为<strong class="ir hi"> Optimize </strong>的函数，它将接收参数、优化器和文本提示的当前状态。计算当前的损失调用函数<strong class="ir hi"> optimzeResult </strong>，我们要声明。之后，计算每种作物的平均损失，并计算其平均值。将优化器设置为梯度值零，进行反向传播，并使用参数值调整-更新步骤。这是一个优化函数。</p><p id="992a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，定义<strong class="ir hi"> optimizeResult </strong>函数，该函数将找到驱动<strong class="ir hi"> optimize </strong>函数的丢失值和最小值。它将接收参数，并提示。分别用值1和0.5声明一对变量alpha和beta。它们将分别代表包含和排除编码的意义或重要性。(这里，由于alpha &gt; beta，在编码过程中，include比exclude更重要)</p><p id="3f36" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">之后，根据参数生成新的图像，并对结果进行归一化。</p><p id="2a80" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">【<em class="kq">注:</em> </strong> <em class="kq">为了计算损失，文本提示的编码</em> <strong class="ir hi"> <em class="kq">和</em> </strong> <em class="kq">应该比较从潜在空间参数的当前状态生成的图像裁剪的编码。因此，我们需要通过生成器传递参数来生成新的图像，以便对其进行编码。</em><strong class="ir hi"/></p><p id="0482" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">和创建裁剪，用额外的标准化来标准化它们(使图像与裁剪结构相匹配)。然后通过调用剪辑模型来创建图像编码。(32个作物由500个so组成，编码维数为512x512)</p><p id="c27f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">W1、W2是包含和附加文本编码的权重。将编码值除以它们的归一化值，使它们处于正确的范围内。</p><p id="feb8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在使用余弦相似性数学函数计算损失。主要损失(包括)，惩罚损失(不包括)。(32种作物中的每一种将分别与包含文本和排除文本的单一编码进行比较)</p><p id="8e8e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么总损失将是两个损失的总和:</p><p id="1cb6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">主要损失乘以alpha(包括因子)，惩罚损失乘以beta(不包括因子)。</p><p id="b5f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在是训练循环编码的时候了:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f6cc" class="lp jo hh ll b fi lq lr l ls lt">### training loop<br/>def training_loop(Params, optimizer, show_crop=False):<br/>  res_img=[]<br/>  res_z=[]</span><span id="d661" class="lp jo hh ll b fi ma lr l ls lt">for prompt in include_enc:<br/>    iteration=0<br/>    Params, optimizer = init_params() # 1 x 256 x 14 x 25 (225/16, 400/16)</span><span id="0f92" class="lp jo hh ll b fi ma lr l ls lt">for it in range(total_iter):<br/>      loss = Optimize(Params, optimizer, prompt)</span><span id="9ba8" class="lp jo hh ll b fi ma lr l ls lt">if iteration&gt;=80 and iteration%show_step == 0:<br/>        new_img = showMe(Params, show_crop)<br/>        res_img.append(new_img)<br/>        res_z.append(Params()) # 1 x 256 x 14 x 25<br/>        print("loss:", loss.item(), "\nno. of iteration:",iteration)</span><span id="17c9" class="lp jo hh ll b fi ma lr l ls lt">iteration+=1<br/>    torch.cuda.empty_cache()<br/>  return res_img, res_z</span></pre><p id="c88d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建一个带参数的函数，优化器。初始化每个编码，迭代设置为零。每次调用时初始化参数。</p><p id="49ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算损失调用先前定义的优化函数。并显示您想要的图像。这里，我将在最终迭代之前显示图像1迭代。所以，调用showMe函数并声明提示，增加迭代次数。最后可以删除GPU的缓存。</p><p id="e104" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">表演时间到了:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="528e" class="lp jo hh ll b fi lq lr l ls lt">torch.cuda.empty_cache()<br/>include=[‘A primeminister giving a speech in public’]<br/>exclude=’watermark, cropped, confusing, blurry’<br/>extras = “watercolor paper”<br/>w1=1<br/>w2=1<br/>noise_factor= .28<br/>total_iter=1000<br/>show_step=100 # set this to see the result every 100 iterations<br/>include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras) <br/>res_img, res_z=training_loop(Params, optimizer, show_crop=True)</span></pre><p id="a1e4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><p id="70f6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">100次迭代后，裁剪和非裁剪图像结果:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/c989af904a8194cda0f4304749ba1024.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*rB7E8Iq6PQvuxMMCvIvkyw.png"/></div></figure><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/3629605e0e0d08328c02ec144eddd070.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*mJ-AVFZ-fzcxTTTXwbWQ1g.png"/></div></figure><p id="b3fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">诸如此类…</p><p id="c0dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果从开始刷新并重新运行，它将在同一步骤中显示不同的图像。这是因为我们使用的是随机过程，而不是确定性过程。</p><p id="cbae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">检查生成的图像尺寸:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="53f8" class="lp jo hh ll b fi lq lr l ls lt">print(len(res_img), len(res_z))<br/>print(res_img[0].shape, res_z[0].shape)<br/>print(res_z[0].max(), res_z[0].min())</span></pre><p id="050d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mh"><img src="../Images/2b1df698e1e303a087f004d6fe2a4f22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0EDxeceGuYymMWPL"/></div></div></figure><p id="b5e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看另一个例子:</p><p id="1a45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">包括:</strong>《与公牛搏斗的人》</p><p id="8628" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">排除:</strong>“水印、模糊、裁剪、混淆、剪切、不连贯”</p><p id="fbe0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">号外:</strong></p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/2aec9dcc0f6f7ab4a928c7cd1bc6dfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/0*ofCDv0tesl4GebFR"/></div></figure><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/dfd5194db79260b568ee6053b82f59c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/0*MVVfncmQakBwgcSP"/></div></figure><p id="dfc7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">太棒了。！</p><p id="dbde" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">【最好的一代不一定是最后一代……也可能是上一代】</strong></p><p id="a4c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，出现多个提示和结果:</p><p id="1fc8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">包括:</strong> [《与公牛搏斗的人》、《睡在公园里的狗》、《在路上打人的人》]</p><p id="c589" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">排除:</strong>“水印、模糊、裁剪、混淆、剪切、不连贯”</p><p id="c920" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">号外:</strong>" "</p><p id="4d8e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">谢谢大家！</p></div></div>    
</body>
</html>