<html>
<head>
<title>A BERT Flavor to Sample and Savor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">值得品尝的伯特风味</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-bert-flavor-to-sample-and-savor-8787b11a63a3?source=collection_archive---------3-----------------------#2022-01-10">https://medium.com/mlearning-ai/a-bert-flavor-to-sample-and-savor-8787b11a63a3?source=collection_archive---------3-----------------------#2022-01-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/730576da8be9e525fac2c82a320fcf53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgOp7mRMi0JvmtfVydEjDQ.jpeg"/></div></div></figure><p id="0f0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">幸运的是，我们可以从BERT架构中派生出各种模型来满足我们的内存和延迟需求。事实证明，模型容量(参数的数量)取决于三个变量，即 <strong class="ir hi">层</strong>的<strong class="ir hi">数量、<strong class="ir hi">隐藏嵌入大小</strong>，以及<strong class="ir hi">注意力头</strong>的数量。</strong></p><p id="6de7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这篇文章聚焦于这三个参数。</p><h2 id="1938" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">编码器层数</h2><p id="906d" class="pw-post-body-paragraph ip iq hh ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">BERT模型由几个相似的transformer编码器层组成，相互堆叠。第N-1层的输出是第N层的输入。BERT Base有12层，BERT Large有24层。</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kn"><img src="../Images/eb2f6e020465eee01f3a636b501cd9e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPwqw14NH4W33m59bHN1BA.png"/></div></div></figure><h2 id="6185" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated"><strong class="ak">隐藏嵌入尺寸</strong></h2><p id="de7e" class="pw-post-body-paragraph ip iq hh ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">隐藏嵌入大小是每一层的输出的大小(在BERT基的情况下是768，在BERT大的情况下是1024)。</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/b961743f3ca57c2b751f8f2a1423190a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6bkyi5Hcz2VLxDVl5a7rA.png"/></div></div></figure><h2 id="3a94" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">注意力头的数量</h2><p id="d8fc" class="pw-post-body-paragraph ip iq hh ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">正如在卷积神经网络中，我们在输出端使用几个过滤器来产生几个不同的特征图，多头注意力也有类似的目的。键、查询和值(K，Q，V)被多次(<strong class="ir hi"> A </strong>次)用不同的、已学习的线性投影投影到不同的空间。BERT Base使用A=12的注意力头，BERT Large使用A=16。(不要与L=12个编码器层合并)</p><h2 id="e721" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">有名的</h2><p id="d7a4" class="pw-post-body-paragraph ip iq hh ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated">内部前馈层始终是隐藏嵌入大小的四倍(因此它是一个隐式变量)。</p><h2 id="2055" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">总结</h2><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="f44e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">学生模型结构紧凑，适用于资源受限的环境(比如移动和边缘环境)</p><figure class="ko kp kq kr fd ii"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h2 id="7d0e" class="jn jo hh bd jp jq jr js jt ju jv jw jx ja jy jz ka je kb kc kd ji ke kf kg kh bi translated">参考</h2><p id="5b9e" class="pw-post-body-paragraph ip iq hh ir b is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji km jk jl jm ha bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1908.08962.pdf" rel="noopener ugc nofollow" target="_blank">博览群书的学生学得更好:论预培训紧凑模型的重要性</a></p><p id="8f0b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kv" href="https://huggingface.co/transformers/v3.3.1/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">抱脸预训练模特</a></p><p id="ee11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1909.10351.pdf" rel="noopener ugc nofollow" target="_blank"> TinyBERT:提取用于自然语言理解的BERT</a></p><p id="4bbc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kv" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">比以往任何时候都更好地理解搜索</a></p><p id="7ec5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kv" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder" rel="noopener ugc nofollow" target="_blank">尼斯伯特图(图一)</a></p><div class="kw kx ez fb ky kz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hi fi z dy le ea eb lf ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">medium.com</p></div></div><div class="li l"><div class="lj l lk ll lm li ln in kz"/></div></div></a></div></div></div>    
</body>
</html>