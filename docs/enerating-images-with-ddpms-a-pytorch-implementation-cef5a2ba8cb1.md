# ç”¨ DDPMs ç”Ÿæˆå›¾åƒ:PyTorch å®ç°

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1?source=collection_archive---------0----------------------->

# ä»‹ç»

å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹( **DDPM** )æ˜¯æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œç”±äºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œæœ€è¿‘å—åˆ°äº†å¾ˆå¤šå…³æ³¨ã€‚åƒ OpenAI çš„ [**DALL-E 2**](https://cdn.openai.com/papers/dall-e-2.pdf) å’Œ Google çš„ [**Imagen**](https://arxiv.org/pdf/2205.11487.pdf) å‘ç”µæœºç­‰å…¨æ–°å‹å·éƒ½æ˜¯åŸºäº DDPMsã€‚ä»–ä»¬ä»¥æ–‡æœ¬ä½œä¸ºç”Ÿæˆå™¨çš„æ¡ä»¶ï¼Œè¿™æ ·å°±æœ‰å¯èƒ½åœ¨ç»™å®šä»»æ„æ–‡æœ¬å­—ç¬¦ä¸²çš„æƒ…å†µä¸‹ç”Ÿæˆç…§ç‰‡èˆ¬é€¼çœŸçš„å›¾åƒã€‚

æ¯”å¦‚è¾“å…¥â€œ*ä¸€åªèƒŒç€èƒŒåŒ…çš„æŸ´çŠ¬éª‘è‡ªè¡Œè½¦çš„ç…§ç‰‡ã€‚è¿™æ˜¯æˆ´ç€å¤ªé˜³é•œå’Œæ²™æ»©å¸½çš„"*åˆ°æ–°çš„ **Imagen** æ¨¡å‹å’Œ"*ä¸€ä¸ªè¢«æç»˜æˆæ˜Ÿäº‘çˆ†ç‚¸çš„æŸ¯åŸºçŠ¬çš„å¤´*"åˆ° **DALL-E 2** æ¨¡å‹äº§ç”Ÿäº†ä»¥ä¸‹å›¾åƒ:

![](img/2faa932f84b47b12fa210b0e2df115bf.png)![](img/6fb2cf27124dece78541064dcf36cceb.png)

Image generated by Imagen (left) and DALL-E 2 (right)

è¿™äº›æ¨¡å‹ç®€ç›´ä»¤äººç ç›®ç»“èˆŒï¼Œä½†è¦ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå°±éœ€è¦ç†è§£ Ho et çš„åŸè‘—ã€‚è‰¾å°”ã€‚*ã€Šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‹ã€‚*

åœ¨è¿™ç¯‡ç®€çŸ­çš„æ–‡ç« ä¸­ï¼Œæˆ‘å°†ç€é‡äºä»å¤´å¼€å§‹(åœ¨ PyTorch ä¸­)åˆ›å»ºä¸€ä¸ªç®€å•ç‰ˆæœ¬çš„ DDPMã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘å°†é‡æ–°æ‰§è¡Œä½•çš„[åŸæ–‡ã€‚ç­‰äºº](https://arxiv.org/abs/2006.11239)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¼ ç»Ÿçš„ã€ä¸éœ€è¦å¤§é‡èµ„æºçš„ MNIST å’Œæ—¶å°š MNIST æ•°æ®é›†ï¼Œå¹¶å°è¯•å‡­ç©ºç”Ÿæˆå›¾åƒã€‚å…ˆè¯´ä¸€ç‚¹ç†è®ºã€‚

# å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹

å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPMs)æœ€æ—©å‡ºç°åœ¨[è¿™ç¯‡è®ºæ–‡](https://arxiv.org/pdf/2006.11239.pdf)ä¸­ã€‚

è¿™ä¸ªæƒ³æ³•å¾ˆç®€å•:ç»™å®šä¸€ç»„å›¾åƒï¼Œæˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°æ·»åŠ ä¸€ç‚¹å™ªå£°ã€‚æ¯èµ°ä¸€æ­¥ï¼Œå›¾åƒå°±å˜å¾—è¶Šæ¥è¶Šä¸æ¸…æ™°ï¼Œç›´åˆ°åªå‰©ä¸‹å™ªå£°ã€‚è¿™è¢«ç§°ä¸ºâ€œå‰è¿›è¿‡ç¨‹â€ã€‚ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥æ’¤é”€è¿™æ ·çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œé€†å‘è¿‡ç¨‹â€ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥æˆåŠŸåœ°å­¦ä¹ ä¸€ä¸ªåå‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªå¯ä»¥ä»çº¯éšæœºå™ªå£°ä¸­ç”Ÿæˆå›¾åƒçš„æ¨¡å‹ã€‚

![](img/1e53f22540188bd2d6c84fe54d7015fe.png)

The main idea of DDPM: Map images x0 to more and more noisy images with probability distribution q. Then, learn the inverse function p parametrized by parameters theta. The image is taken from â€œDenoising DIffusion Probabilistic Modelsâ€ by Ho et. al.

æ­£å‘è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªæ­¥éª¤åœ¨äºé€šè¿‡ä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·æ¥ä½¿è¾“å…¥å›¾åƒå™ªå£°æ›´å¤§(æ­¥éª¤ t ä¸­çš„ x ),è¯¥å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¹³å‡å€¼æ˜¯å…ˆå‰å›¾åƒçš„ç¼©å°ç‰ˆæœ¬(æ­¥éª¤ t-1 ä¸­çš„ x ),å¹¶ä¸”è¯¥åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çš„å’Œå›ºå®šçš„ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ ä¸€äº›æ­£æ€åˆ†å¸ƒçš„å€¼æ¥ç‹¬ç«‹åœ°æ‰°åŠ¨å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ ã€‚

![](img/01a48e3d10121f4f9b42dab9d0681a69.png)

Forward process: we sample from a normal distribution which mean is a scaled version of the current image and which covariance matrix simply has all equal variance terms beta t.

å¯¹äºæ¯ä¸€æ­¥ï¼Œéƒ½æœ‰ä¸€ä¸ªä¸åŒçš„ç³»æ•°Î²ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬åœ¨è¿™ä¸€æ­¥ä¸­å›¾åƒå¤±çœŸçš„ç¨‹åº¦ã€‚beta è¶Šé«˜ï¼Œå›¾åƒä¸­æ·»åŠ çš„å™ªå£°è¶Šå¤šã€‚æˆ‘ä»¬å¯ä»¥è‡ªç”±é€‰æ‹©ç³»æ•°Î²ï¼Œä½†æ˜¯æˆ‘ä»¬åº”è¯¥å°½é‡ä¸è¦ä¸€æ¬¡æ·»åŠ å¤ªå¤šå™ªå£°ï¼Œå¹¶ä¸”æ•´ä¸ªæ­£å‘è¿‡ç¨‹åº”è¯¥æ˜¯â€œå¹³æ»‘â€çš„ã€‚åœ¨ä½•ç­‰äººçš„åŸè‘—ä¸­ã€‚è‰¾å°”ã€‚Î²è¢«æ”¾ç½®åœ¨ä» 0.0001 åˆ° 0.02 çš„çº¿æ€§ç©ºé—´ä¸­ã€‚

é«˜æ–¯åˆ†å¸ƒçš„ä¸€ä¸ªå¾ˆå¥½çš„ç‰¹æ€§æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘å¹³å‡å‘é‡æ·»åŠ ä¸€ä¸ªç”±æ ‡å‡†åå·®ç¼©æ”¾çš„æ­£æ€åˆ†å¸ƒå™ªå£°å‘é‡æ¥å¯¹å…¶è¿›è¡Œé‡‡æ ·ã€‚è¿™å¯¼è‡´:

![](img/808efe5c6575896bcd74992e13ea64d7.png)

Forward process but sampling is done by just adding the mean and scaling a normally distributed noise (epsilon) by the standard deviation.

æˆ‘ä»¬ç°åœ¨çŸ¥é“å¦‚ä½•åœ¨æ­£å‘è¿‡ç¨‹ä¸­è·å–ä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Œåªéœ€ç¼©æ”¾ç°æœ‰æ ·æœ¬å¹¶æ·»åŠ ä¸€äº›ç¼©æ”¾å™ªå£°ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨è®¤ä¸ºè¿™ä¸ªå…¬å¼æ˜¯é€’å½’çš„ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡º:

![](img/70ef88722613ccaa5e0e5bb32306ffcf.png)

The formula of the forward process is recursive, so we can start expanding it.

å¦‚æœæˆ‘ä»¬ç»§ç»­è¿™æ ·åšå¹¶åšä¸€äº›ç®€åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€è·¯è¿”å›å¹¶è·å¾—ä¸€ä¸ªå…¬å¼ï¼Œç”¨äºä»åŸå§‹æ— å™ªå£°å›¾åƒ x0 å¼€å§‹åœ¨æ­¥éª¤ t è·å¾—å™ªå£°æ ·æœ¬:

![](img/208072704367c51d6dcdd020742a0774.png)

The equation for the forward process that allows to directly get a desired noisy level starting from the original non-noisy image.

å¤ªå¥½äº†ã€‚ç°åœ¨ä¸ç®¡æˆ‘ä»¬çš„æ­£å‘è¿‡ç¨‹ä¼šæœ‰å¤šå°‘æ­¥ï¼Œæˆ‘ä»¬æ€»ä¼šæœ‰åŠæ³•ç›´æ¥ä»åŸå§‹å›¾åƒä¸­ç›´æ¥å¾—åˆ°ç¬¬ t æ­¥çš„å«å™ªå›¾åƒã€‚

å¯¹äºåå‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿåº”è¯¥ä½œä¸ºé«˜æ–¯åˆ†å¸ƒå·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¨¡å‹æ¥é¢„æµ‹ç»™å®šå™ªå£°å›¾åƒå’Œæ—¶é—´æ­¥é•¿çš„åˆ†å¸ƒå¹³å‡å€¼å’Œæ ‡å‡†åå·®ã€‚å®é™…ä¸Šï¼Œåœ¨å…³äº DDPMs çš„ç¬¬ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œåæ–¹å·®çŸ©é˜µä¿æŒå›ºå®šï¼Œå› æ­¤æˆ‘ä»¬åªæƒ³é¢„æµ‹é«˜æ–¯çš„å¹³å‡å€¼(ç»™å®šå™ªå£°å›¾åƒå’Œæˆ‘ä»¬å½“å‰æ‰€å¤„çš„æ—¶é—´æ­¥é•¿):

![](img/a1d395b435464cf6650271a86a45afbc.png)

Backward process: we try to go back to a less noisy image (x at timestep t-1) using a gaussian distribution which mean is predicted by a model

ç°åœ¨ï¼Œäº‹å®è¯æ˜ï¼Œè¦é¢„æµ‹çš„æœ€ä½³å¹³å‡å€¼åªæ˜¯æˆ‘ä»¬å·²ç»ç†Ÿæ‚‰çš„é¡¹çš„å‡½æ•°:

![](img/3f499d7b293e99b80b6d996540879f79.png)

The optimal mean value to be predicted to reverse the noising process. Given the more noisy image at step t, we can make it less noisy by subtracting a scale of the added noise and applying a scaling afterwards.

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»…ç”¨å™ªå£°å›¾åƒå’Œæ—¶é—´æ­¥é•¿çš„å‡½æ•°æ¥é¢„æµ‹å™ªå£°Îµã€‚

![](img/eb113b584b17bf5997168950f3c2ee1e.png)

Our model just predicts the noise that was added, and we use this to recover a less noisy image using the information for the particular time step.

æˆ‘ä»¬çš„æŸå¤±å‡½æ•°å°†æ˜¯æ·»åŠ çš„çœŸå®å™ªå£°å’Œæ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„å‡æ–¹è¯¯å·®(MSE)çš„ç¼©æ”¾ç‰ˆæœ¬

![](img/c5cdeb2a9fb14c552dd71bccedd489f9.png)

Final loss function. We minimize the MSE between the noises actually added to the images and the one predicted by the model. We do so for all images in our dataset and all time steps.

ä¸€æ—¦æ¨¡å‹è¢«è®­ç»ƒ(ç®—æ³• 1)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å»å™ªæ¨¡å‹æ¥é‡‡æ ·æ–°å›¾åƒ(ç®—æ³• 2)ã€‚

![](img/efae3f473e2cc02f68e11e988e3be57c.png)

Training and sampling algorithms. Once the model is trained, we can use it to generate brand new samples starting from gaussian noise.

# è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§

æ—¢ç„¶æˆ‘ä»¬å¯¹æ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†æœ‰äº†å¤§è‡´çš„äº†è§£ï¼Œæ˜¯æ—¶å€™å®ç°æˆ‘ä»¬è‡ªå·±çš„ä¸œè¥¿äº†ã€‚ä½ å¯ä»¥è‡ªå·±åœ¨è¿™ä¸ª [Google Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing)ä¸­æˆ–è€…ç”¨è¿™ä¸ª [GitHub åº“](https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm)è¿è¡Œä¸‹é¢çš„ä»£ç ã€‚

å’Œå¾€å¸¸ä¸€æ ·ï¼Œè¿›å£åªæ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥ã€‚

```
# Import of libraries
import random
import imageio
import numpy as np
from argparse import ArgumentParser

from tqdm.auto import tqdm
import matplotlib.pyplot as plt

import einops
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import DataLoader

from torchvision.transforms import Compose, ToTensor, Lambda
from torchvision.datasets.mnist import MNIST, FashionMNIST

# Setting reproducibility
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Definitions
STORE_PATH_MNIST = f"ddpm_model_mnist.pt"
STORE_PATH_FASHION = f"ddpm_model_fashion.pt"
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºå®éªŒå®šä¹‰å‡ ä¸ªå‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å†³å®šæ˜¯å¦è¦è¿è¡Œè®­ç»ƒå¾ªç¯ï¼Œæ˜¯å¦è¦ä½¿ç”¨æ—¶å°š-MNIST æ•°æ®é›†å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°

```
no_train = False
fashion = True
batch_size = 128
n_epochs = 20
lr = 0.001
store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœŸçš„æƒ³å±•ç¤ºå›¾åƒã€‚æˆ‘ä»¬å¯¹è®­ç»ƒå›¾åƒå’Œæ¨¡å‹ç”Ÿæˆçš„å›¾åƒéƒ½æ„Ÿå…´è¶£ã€‚æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªæ•ˆç”¨å‡½æ•°ï¼Œç»™å®šä¸€äº›å›¾åƒï¼Œå°†æ˜¾ç¤ºä¸€ä¸ªæ­£æ–¹å½¢(æˆ–å°½å¯èƒ½æ¥è¿‘)çš„å­å›¾å½¢ç½‘æ ¼:

```
def show_images(images, title=""):
    """Shows the provided images as sub-pictures in a square"""

    # Converting images to CPU numpy arrays
    if type(images) is torch.Tensor:
        images = images.detach().cpu().numpy()

    # Defining number of rows and columns
    fig = plt.figure(figsize=(8, 8))
    rows = int(len(images) ** (1 / 2))
    cols = round(len(images) / rows)

    # Populating figure with sub-plots
    idx = 0
    for r in range(rows):
        for c in range(cols):
            fig.add_subplot(rows, cols, idx + 1)

            if idx < len(images):
                plt.imshow(images[idx][0], cmap="gray")
                idx += 1
    fig.suptitle(title, fontsize=30)

    # Showing the figure
    plt.show()
```

ä¸ºäº†æµ‹è¯•è¿™ä¸ªæ•ˆç”¨å‡½æ•°ï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®é›†å¹¶æ˜¾ç¤ºç¬¬ä¸€æ‰¹ã€‚**é‡è¦æç¤º:**å›¾åƒå¿…é¡»åœ¨[-1ï¼Œ1]èŒƒå›´å†…å½’ä¸€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç½‘ç»œå¿…é¡»é¢„æµ‹æ­£æ€åˆ†å¸ƒçš„å™ªå£°å€¼:

```
# Shows the first batch of images
def show_first_batch(loader):
    for batch in loader:
        show_images(batch[0], "Images in the first batch")
        break
```

```
# Loading the data (converting each image into a tensor and normalizing between [-1, 1])
transform = Compose([
    ToTensor(),
    Lambda(lambda x: (x - 0.5) * 2)]
)
ds_fn = FashionMNIST if fashion else MNIST
dataset = ds_fn("./datasets", download=True, train=True, transform=transform)
loader = DataLoader(dataset, batch_size, shuffle=True)
```

![](img/ffdeb6cefa004560606aad3e13386e98.png)

Images in our first batch. If you kept the same randomizing seed, you should get the exact same batch.

å¤ªå¥½äº†ï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªå¾ˆå¥½çš„æ•ˆç”¨å‡½æ•°ï¼Œæˆ‘ä»¬ç¨åä¹Ÿå°†æŠŠå®ƒç”¨äºæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚åœ¨æˆ‘ä»¬çœŸæ­£å¼€å§‹å¤„ç† DDPM æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä» colab è·å¾—ä¸€ä¸ª GPU è®¾å¤‡(é€šå¸¸æ˜¯é colab-pro ç”¨æˆ·çš„*ç‰¹æ–¯æ‹‰ T4* ):

![](img/9e118c4e31cce0260dfe745b5c422fd4.png)

Getting a device and, if it is a GPU, printing its name

# DDPM æ¨¡å¼

æ—¢ç„¶æˆ‘ä»¬å·²ç»è§£å†³äº†çç¢çš„äº‹æƒ…ï¼Œç°åœ¨æ˜¯æ—¶å€™ç ”ç©¶ DDPM äº†ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª *MyDDPM* PyTorch æ¨¡å—ï¼Œå®ƒå°†è´Ÿè´£å­˜å‚¨ betas å’Œ alphas å€¼å¹¶åº”ç”¨è½¬å‘è¿‡ç¨‹ã€‚ç›¸åï¼Œå¯¹äºåå‘è¿‡ç¨‹ï¼Œ *MyDDPM* æ¨¡å—å°†ç®€å•åœ°ä¾èµ–äºç”¨äºæ„å»º DDPM çš„ç½‘ç»œ:

```
# DDPM class
class MyDDPM(nn.Module):
    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 28, 28)):
        super(MyDDPM, self).__init__()
        self.n_steps = n_steps
        self.device = device
        self.image_chw = image_chw
        self.network = network.to(device)
        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(
            device)  # Number of steps is typically in the order of thousands
        self.alphas = 1 - self.betas
        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)

    def forward(self, x0, t, eta=None):
        # Make input image more noisy (we can directly skip to the desired step)
        n, c, h, w = x0.shape
        a_bar = self.alpha_bars[t]

        if eta is None:
            eta = torch.randn(n, c, h, w).to(self.device)

        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta
        return noisy

    def backward(self, x, t):
        # Run each image through the network for each timestep t in the vector t.
        # The network returns its estimation of the noise that was added.
        return self.network(x, t)
```

æ³¨æ„ï¼Œæ­£å‘è¿‡ç¨‹ç‹¬ç«‹äºç”¨äºå»å™ªçš„ç½‘ç»œï¼Œæ‰€ä»¥ä»æŠ€æœ¯ä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°å®ƒçš„æ•ˆæœã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªåº”ç”¨**ç®—æ³• 2** (é‡‡æ ·è¿‡ç¨‹)ç”Ÿæˆæ–°å›¾åƒçš„æ•ˆç”¨å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ª DDPM çš„ç‰¹å®šæ•ˆç”¨å‡½æ•°æ¥å®ç°:

```
def show_forward(ddpm, loader, device):
    # Showing the forward process
    for batch in loader:
        imgs = batch[0]

        show_images(imgs, "Original images")

        for percent in [0.25, 0.5, 0.75, 1]:
            show_images(
                ddpm(imgs.to(device),
                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),
                f"DDPM Noisy images {int(percent * 100)}%"
            )
        break
```

ä¸ºäº†ç”Ÿæˆå›¾åƒï¼Œæˆ‘ä»¬ä»éšæœºå™ªå£°å¼€å§‹ï¼Œè®© T ä» T å›åˆ° 0ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å™ªå£°ä¼°è®¡ä¸º **eta_theta** å¹¶åº”ç”¨å»å™ªå‡½æ•°ã€‚æœ€åï¼Œåƒæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ä¸€æ ·ï¼Œå¢åŠ äº†é¢å¤–çš„å™ªå£°ã€‚

```
def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name="sampling.gif", c=1, h=28, w=28):
    """Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples"""
    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)
    frames = []

    with torch.no_grad():
        if device is None:
            device = ddpm.device

        # Starting from random noise
        x = torch.randn(n_samples, c, h, w).to(device)

        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):
            # Estimating noise to be removed
            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()
            eta_theta = ddpm.backward(x, time_tensor)

            alpha_t = ddpm.alphas[t]
            alpha_t_bar = ddpm.alpha_bars[t]

            # Partially denoising the image
            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)

            if t > 0:
                z = torch.randn(n_samples, c, h, w).to(device)

                # Option 1: sigma_t squared = beta_t
                beta_t = ddpm.betas[t]
                sigma_t = beta_t.sqrt()

                # Option 2: sigma_t squared = beta_tilda_t
                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]
                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t
                # sigma_t = beta_tilda_t.sqrt()

                # Adding some more noise like in Langevin Dynamics fashion
                x = x + sigma_t * z

            # Adding frames to the GIF
            if idx in frame_idxs or t == 0:
                # Putting digits in range [0, 255]
                normalized = x.clone()
                for i in range(len(normalized)):
                    normalized[i] -= torch.min(normalized[i])
                    normalized[i] *= 255 / torch.max(normalized[i])

                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame
                frame = einops.rearrange(normalized, "(b1 b2) c h w -> (b1 h) (b2 w) c", b1=int(n_samples ** 0.5))
                frame = frame.cpu().numpy().astype(np.uint8)

                # Rendering frame
                frames.append(frame)

    # Storing the gif
    with imageio.get_writer(gif_name, mode="I") as writer:
        for idx, frame in enumerate(frames):
            writer.append_data(frame)
            if idx == len(frames) - 1:
                for _ in range(frames_per_gif // 3):
                    writer.append_data(frames[-1])
    return x
```

æ‰€æœ‰ä¸ DDPM æœ‰å…³çš„äº‹æƒ…ç°åœ¨éƒ½æ‘†åœ¨æ¡Œé¢ä¸Šã€‚æˆ‘ä»¬åªéœ€è¦å®šä¹‰ä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šå›¾åƒå’Œå½“å‰æ—¶é—´æ­¥é•¿çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å°†å®é™…å®Œæˆé¢„æµ‹å›¾åƒä¸­å™ªå£°çš„å·¥ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå®šåˆ¶çš„ U-Net æ¨¡å‹ã€‚ä¸è¨€è€Œå–»ï¼Œæ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å‹ã€‚

# ä¼˜ä¿¡ç½‘

æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªä¿æŒç©ºé—´ç»´åº¦ä¸å˜çš„å—æ¥å¼€å§‹åˆ›å»ºæˆ‘ä»¬çš„ U-Netã€‚è¿™ä¸ªå—å°†ç”¨äºæˆ‘ä»¬ U-Net çš„æ¯ä¸ªçº§åˆ«ã€‚

```
class MyBlock(nn.Module):
    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):
        super(MyBlock, self).__init__()
        self.ln = nn.LayerNorm(shape)
        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)
        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)
        self.activation = nn.SiLU() if activation is None else activation
        self.normalize = normalize

    def forward(self, x):
        out = self.ln(x) if self.normalize else x
        out = self.conv1(out)
        out = self.activation(out)
        out = self.conv2(out)
        out = self.activation(out)
        return out
```

DDPMs ä¸­çš„æ£˜æ‰‹ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬çš„å›¾åƒåˆ°å›¾åƒæ¨¡å‹å¿…é¡»ä»¥å½“å‰æ—¶é—´æ­¥é•¿ä¸ºæ¡ä»¶ã€‚ä¸ºäº†åœ¨å®è·µä¸­åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­£å¼¦åµŒå…¥å’Œä¸€å±‚ MLPsã€‚ç”±æ­¤äº§ç”Ÿçš„å¼ é‡å°†é€šè¿‡ U-Net çš„æ¯ä¸€çº§æŒ‰ä¿¡é“æ·»åŠ åˆ°ç½‘ç»œçš„è¾“å…¥ä¸­ã€‚

```
def sinusoidal_embedding(n, d):
    # Returns the standard positional embedding
    embedding = torch.zeros(n, d)
    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)])
    wk = wk.reshape((1, d))
    t = torch.arange(n).reshape((n, 1))
    embedding[:,::2] = torch.sin(t * wk[:,::2])
    embedding[:,1::2] = torch.cos(t * wk[:,::2])

    return embedding
```

æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå°çš„æ•ˆç”¨å‡½æ•°ï¼Œè¯¥å‡½æ•°åˆ›å»ºäº†ä¸€ä¸ªç”¨äºç»˜åˆ¶ä½ç½®åµŒå…¥åœ°å›¾çš„ä¸€å±‚ MLPã€‚

```
def _make_te(self, dim_in, dim_out):
  return nn.Sequential(
    nn.Linear(dim_in, dim_out),
    nn.SiLU(),
    nn.Linear(dim_out, dim_out)
  )
```

ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†å¦‚ä½•å¤„ç†æ—¶é—´ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ U-Net ç½‘ç»œã€‚æˆ‘ä»¬å°†æœ‰ 3 ä¸ªä¸‹é‡‡æ ·éƒ¨åˆ†ï¼Œä¸€ä¸ªç½‘ç»œä¸­é—´çš„ç“¶é¢ˆï¼Œå’Œ 3 ä¸ªå¸¦æœ‰é€šå¸¸çš„ U-Net å‰©ä½™è¿æ¥(è¿æ¥)çš„ä¸Šé‡‡æ ·æ­¥éª¤ã€‚

```
class MyUNet(nn.Module):
    def __init__(self, n_steps=1000, time_emb_dim=100):
        super(MyUNet, self).__init__()

        # Sinusoidal embedding
        self.time_embed = nn.Embedding(n_steps, time_emb_dim)
        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)
        self.time_embed.requires_grad_(False)

        # First half
        self.te1 = self._make_te(time_emb_dim, 1)
        self.b1 = nn.Sequential(
            MyBlock((1, 28, 28), 1, 10),
            MyBlock((10, 28, 28), 10, 10),
            MyBlock((10, 28, 28), 10, 10)
        )
        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)

        self.te2 = self._make_te(time_emb_dim, 10)
        self.b2 = nn.Sequential(
            MyBlock((10, 14, 14), 10, 20),
            MyBlock((20, 14, 14), 20, 20),
            MyBlock((20, 14, 14), 20, 20)
        )
        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)

        self.te3 = self._make_te(time_emb_dim, 20)
        self.b3 = nn.Sequential(
            MyBlock((20, 7, 7), 20, 40),
            MyBlock((40, 7, 7), 40, 40),
            MyBlock((40, 7, 7), 40, 40)
        )
        self.down3 = nn.Sequential(
            nn.Conv2d(40, 40, 2, 1),
            nn.SiLU(),
            nn.Conv2d(40, 40, 4, 2, 1)
        )

        # Bottleneck
        self.te_mid = self._make_te(time_emb_dim, 40)
        self.b_mid = nn.Sequential(
            MyBlock((40, 3, 3), 40, 20),
            MyBlock((20, 3, 3), 20, 20),
            MyBlock((20, 3, 3), 20, 40)
        )

        # Second half
        self.up1 = nn.Sequential(
            nn.ConvTranspose2d(40, 40, 4, 2, 1),
            nn.SiLU(),
            nn.ConvTranspose2d(40, 40, 2, 1)
        )

        self.te4 = self._make_te(time_emb_dim, 80)
        self.b4 = nn.Sequential(
            MyBlock((80, 7, 7), 80, 40),
            MyBlock((40, 7, 7), 40, 20),
            MyBlock((20, 7, 7), 20, 20)
        )

        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)
        self.te5 = self._make_te(time_emb_dim, 40)
        self.b5 = nn.Sequential(
            MyBlock((40, 14, 14), 40, 20),
            MyBlock((20, 14, 14), 20, 10),
            MyBlock((10, 14, 14), 10, 10)
        )

        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)
        self.te_out = self._make_te(time_emb_dim, 20)
        self.b_out = nn.Sequential(
            MyBlock((20, 28, 28), 20, 10),
            MyBlock((10, 28, 28), 10, 10),
            MyBlock((10, 28, 28), 10, 10, normalize=False)
        )

        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)

    def forward(self, x, t):
        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)
        t = self.time_embed(t)
        n = len(x)
        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)
        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)
        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)

        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)

        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)
        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)

        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)
        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)

        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)
        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)

        out = self.conv_out(out)

        return out

    def _make_te(self, dim_in, dim_out):
        return nn.Sequential(
            nn.Linear(dim_in, dim_out),
            nn.SiLU(),
            nn.Linear(dim_out, dim_out)
        )
```

ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†å»å™ªç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–ä¸€ä¸ª DDPM æ¨¡å‹ï¼Œå¹¶è¿›è¡Œä¸€äº›å¯è§†åŒ–å¤„ç†ã€‚

# ä¸€äº›å¯è§†åŒ–

æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„ U-Net å®ä¾‹åŒ– DDPM æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```
# Defining model
n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors
ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)
```

è®©æˆ‘ä»¬çœ‹çœ‹è½¬å‘è¿‡ç¨‹æ˜¯ä»€ä¹ˆæ ·å­çš„:

```
# Optionally, show the diffusion (forward) process
show_forward(ddpm, loader, device)
```

![](img/cd6d480144910cca2a88c57905bf7bce.png)

Result of running the forward process. Images get noisier and noisier with each step until just noise is left.

æˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œä½†æˆ‘ä»¬å·²ç»å¯ä»¥ä½¿ç”¨å…è®¸æˆ‘ä»¬ç”Ÿæˆæ–°å›¾åƒçš„å‡½æ•°ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ:

![](img/6ad6d13b8e85b82a48a293eead9367bb.png)

Generating new images with a non-trained model. Noise is produced.

æ¯«ä¸å¥‡æ€ªï¼Œå½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œä»€ä¹ˆä¹Ÿæ²¡å‘ç”Ÿã€‚ä½†æ˜¯ï¼Œå½“æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ã€‚

# è®­ç»ƒå¾ªç¯

æˆ‘ä»¬ç°åœ¨å®ç°ç®—æ³• 1 æ¥å­¦ä¹ å°†çŸ¥é“å¦‚ä½•å¯¹å›¾åƒå»å™ªçš„æ¨¡å‹ã€‚è¿™å¯¹åº”äºæˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ã€‚

```
def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path="ddpm_model.pt"):
    mse = nn.MSELoss()
    best_loss = float("inf")
    n_steps = ddpm.n_steps

    for epoch in tqdm(range(n_epochs), desc=f"Training progress", colour="#00ff00"):
        epoch_loss = 0.0
        for step, batch in enumerate(tqdm(loader, leave=False, desc=f"Epoch {epoch + 1}/{n_epochs}", colour="#005500")):
            # Loading data
            x0 = batch[0].to(device)
            n = len(x0)

            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars
            eta = torch.randn_like(x0).to(device)
            t = torch.randint(0, n_steps, (n,)).to(device)

            # Computing the noisy image based on x0 and the time-step (forward process)
            noisy_imgs = ddpm(x0, t, eta)

            # Getting model estimation of noise based on the images and the time-step
            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))

            # Optimizing the MSE between the noise plugged and the predicted noise
            loss = mse(eta_theta, eta)
            optim.zero_grad()
            loss.backward()
            optim.step()

            epoch_loss += loss.item() * len(x0) / len(loader.dataset)

        # Display images generated at this epoch
        if display:
            show_images(generate_new_images(ddpm, device=device), f"Images generated at epoch {epoch + 1}")

        log_string = f"Loss at epoch {epoch + 1}: {epoch_loss:.3f}"

        # Storing the model
        if best_loss > epoch_loss:
            best_loss = epoch_loss
            torch.save(ddpm.state_dict(), store_path)
            log_string += " --> Best model ever (stored)"

        print(log_string)
```

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œåœ¨æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°å¯¹ä¸€äº›å›¾åƒè¿›è¡Œé‡‡æ ·ï¼Œå¹¶å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œéšæœºçš„æ—¶é—´æ­¥é•¿ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”¨æ­£å‘è¿‡ç¨‹ä½¿å®ƒä»¬æœ‰å™ªå£°ï¼Œå¹¶å¯¹è¿™äº›æœ‰å™ªå£°çš„å›¾åƒè¿è¡Œåå‘è¿‡ç¨‹ã€‚å®é™…æ·»åŠ çš„å™ªå£°å’Œæ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¹‹é—´çš„ MSE è¢«ä¼˜åŒ–ã€‚

![](img/31fc8a66a966e85f4b9bb38d129845b0.png)

Training takes roughly 24 seconds per epoch. With 20 epochs, it takes roughly 8 minutes to train a DDPM model.

é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘å°†è®­ç»ƒå‘¨æœŸè®¾ç½®ä¸º 20ï¼Œå› ä¸ºæ¯ä¸ªå‘¨æœŸéœ€è¦ 24 ç§’(è®­ç»ƒæ€»å…±å¤§çº¦éœ€è¦ 8 åˆ†é’Ÿ)ã€‚è¯·æ³¨æ„ï¼Œæœ‰å¯èƒ½é€šè¿‡æ›´å¤šçš„çºªå…ƒã€æ›´å¥½çš„ U-Net å’Œå…¶ä»–æŠ€å·§è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘çœç•¥äº†è¿™äº›ã€‚

# æµ‹è¯•æ¨¡å‹

æ—¢ç„¶å·¥ä½œå·²ç»å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥äº«å—æˆæœäº†ã€‚æˆ‘ä»¬æ ¹æ® MSE æŸå¤±å‡½æ•°åŠ è½½è®­ç»ƒæœŸé—´è·å¾—çš„æœ€ä½³æ¨¡å‹ï¼Œå°†å…¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç”Ÿæˆæ–°æ ·æœ¬

```
# Loading the trained model
best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)
best_model.load_state_dict(torch.load(store_path, map_location=device))
best_model.eval()
print("Model loaded")
```

```
print("Generating new images")
generated = generate_new_images(
        best_model,
        n_samples=100,
        device=device,
        gif_name="fashion.gif" if fashion else "mnist.gif"
    )
show_images(generated, "Final result")
```

![](img/9159ecb2c6324b7ce48b9cc6720d4113.png)

Final result on the MNIST dataset

è›‹ç³•ä¸Šçš„æ¨±æ¡ƒæ˜¯æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°è‡ªåŠ¨åˆ›å»ºæ‰©æ•£è¿‡ç¨‹çš„æ¼‚äº® gif çš„äº‹å®ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨ Colab ä¸­å¯è§†åŒ– gif:

![](img/6f8fe8cf57ba763fb696be8e17bd409f.png)

Showing the generated gif image

![](img/89ee312cd28d38435fcb520d2640b3fb.png)![](img/bd2cc02a6962eec2ac69fb3124cd3651.png)

Obtained GIFs for Fashion-MNIST and MNIST datasets.

æˆ‘ä»¬å®Œäº†ã€‚æˆ‘ä»¬ç»ˆäºè®©æˆ‘ä»¬çš„ DDPM æ¨¡å‹å·¥ä½œäº†ï¼

# è¿›ä¸€æ­¥çš„æ”¹è¿›

å·²ç»åšäº†è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œä»¥å…è®¸[ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒ](https://arxiv.org/pdf/2006.09011.pdf)ï¼Œ[åŠ é€Ÿé‡‡æ ·](https://arxiv.org/pdf/2010.02502.pdf)æˆ–è·å¾—[æ›´å¥½çš„æ ·æœ¬è´¨é‡å’Œå¯èƒ½æ€§](https://arxiv.org/abs/2102.09672)ã€‚Imagen å’Œ DALL-E 2 å‹å·åŸºäºåŸå§‹ DDPMs çš„æ”¹è¿›ç‰ˆæœ¬ã€‚

# æ›´å¤šå‚è€ƒ

å…³äº DDPMs çš„æ›´å¤šå‚è€ƒï¼Œæˆ‘å¼ºçƒˆæ¨èé˜…è¯» Lilian Weng å’Œ Niels Rogge çš„[æ°å‡ºæ–‡ç« ](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)å’Œ Kashif Rasul çš„æƒŠäºº[æ‹¥æŠ±è„¸åšå®¢](https://huggingface.co/blog/annotated-diffusion)ã€‚Colab ç¬”è®°æœ¬çš„æœ€åè¿˜æåˆ°äº†å…¶ä»–ä½œè€…ã€‚

# ç»“è®º

æ‰©æ•£æ¨¡å‹æ˜¯å­¦ä¹ è¿­ä»£å»å™ªå›¾åƒçš„ç”Ÿæˆæ¨¡å‹ã€‚ä»ä¸€äº›å™ªå£°å¼€å§‹ï¼Œç„¶åå¯ä»¥è¦æ±‚æ¨¡å‹å¯¹æ ·æœ¬å»å™ªå£°ï¼Œç›´åˆ°è·å¾—ä¸€äº›çœŸå®çš„å›¾åƒã€‚

æˆ‘ä»¬åœ¨ PyTorch ä¸­ä»å¤´åˆ›å»ºäº†ä¸€ä¸ª DDPMï¼Œå¹¶è®©å®ƒå­¦ä¹ å»å™ª MNIST /æ—¶å°š MNIST çš„å›¾åƒã€‚ç»è¿‡è®­ç»ƒåï¼Œè¿™ä¸ªæ¨¡å‹æœ€ç»ˆèƒ½å¤Ÿä»éšæœºå™ªå£°ä¸­ç”Ÿæˆæ–°çš„å›¾åƒã€‚å¾ˆç¥å¥‡ï¼Œå¯¹å§ï¼Ÿ

å¸¦æœ‰æ‰€ç¤ºå®ç°çš„ Colab ç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™ä¸ªé“¾æ¥](https://colab.research.google.com/drive/1AZ2_BAwXrU8InE_qAE9cFZ0lsIO5a_xp?usp=sharing)å…è´¹è®¿é—®ï¼Œè€Œ [GitHub åº“](https://github.com/BrianPulfer/PapersReimplementations/tree/master/ddpm)åŒ…å«ã€‚py æ–‡ä»¶ã€‚å¦‚æœä½ è§‰å¾—è¿™ä¸ªæ•…äº‹æœ‰ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä¸ºå®ƒé¼“æŒ**ğŸ‘**ã€‚å¦‚æœä½ è§‰å¾—æœ‰ä»€ä¹ˆä¸æ¸…æ¥šçš„åœ°æ–¹ï¼Œä¸è¦çŠ¹è±«ï¼Œç›´æ¥è”ç³»æˆ‘ï¼æˆ‘å¾ˆé«˜å…´ä¸ä½ è®¨è®ºå®ƒã€‚

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)