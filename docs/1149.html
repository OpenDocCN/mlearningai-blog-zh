<html>
<head>
<title>Review of Machine Learning Models with Heart Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于心脏数据的机器学习模型综述</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/3-machine-learning-models-with-heart-data-8e0671d4e500?source=collection_archive---------1-----------------------#2021-10-12">https://medium.com/mlearning-ai/3-machine-learning-models-with-heart-data-8e0671d4e500?source=collection_archive---------1-----------------------#2021-10-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="bbe1" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">使用<code class="du iw ix iy iz b">heart</code>数据集，构建和评估3个机器学习模型来预测心脏病的存在</h2></div><p id="2e3f" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">在这篇文章中，我将解释如何读取数据并建立3个机器学习模型来检测病人的病理。这些将是二元分类模型。我们将评估每个模型，并确定哪一个是最好的。这些模型是决策树、随机森林和朴素贝叶斯。此外，我将通过摆弄数据来比较这些模型。在文章的最后，我将尝试解释如何提高模型的精确度。这位是<a class="ae jw" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank">心脏病UCI </a>从<a class="ae jw" href="https://www.kaggle.com" rel="noopener ugc nofollow" target="_blank">Kaggle</a>【1】。数据的许可是<a class="ae jw" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank"> Reddit API条款</a>。</p><p id="fb59" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">为了更好地可视化和理解代码，点击<a class="ae jw" href="https://github.com/esmasert/Medical-Image/blob/main/HeartData.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="cf4e" class="jx jy hh bd jz ka kb kc kd ke kf kg kh in ki io kj iq kk ir kl it km iu kn ko bi translated">入门指南</h1><p id="d81b" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">我们需要下载数据。在这本笔记本中，我们将使用开源的心脏病数据集，在这里发布<a class="ae jw" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="88ab" class="lc jy hh iz b fi ld le l lf lg">!pip install opendatasets<br/>import opendatasets as od<br/>od.download("https://www.kaggle.com/ronitf/heart-disease-uci")</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lh"><img src="../Images/8619812ef8f73afbf949b0e2807f380d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7g9NunkYPO61qxaJ8x0OA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><h2 id="0b63" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">数据准备</h2><p id="03c1" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">此数据集包含以下列:</p><p id="a7ef" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">连续数值列</strong></p><ul class=""><li id="52a1" class="mg mh hh jc b jd je jg jh jj mi jn mj jr mk jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">age</code>:以年为单位的年龄</li><li id="c0cc" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">trestbps</code>:静息血压</li><li id="56d4" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">chol</code>:血清胆固醇，单位为毫克/分升</li><li id="154a" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">thalach</code>:达到最大心率</li><li id="21fd" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">oldpeak</code>:运动相对于休息诱发的ST段压低</li></ul><p id="b096" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">分类列</strong></p><ul class=""><li id="da97" class="mg mh hh jc b jd je jg jh jj mi jn mj jr mk jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">sex</code> : 1 =男性；0 =女性</li><li id="accc" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">cp</code>:胸痛类型(4个数值)</li><li id="f468" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">fbs</code>:空腹血糖&gt; 120 mg/dl。1 =真；0 =假</li><li id="5f6e" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">restecg</code>:静息心电图结果(数值0，1，2)</li><li id="a5c6" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">exang</code>:运动诱发心绞痛，1 =是；0 =否</li><li id="bc52" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">slope</code>:运动ST段峰值的斜率(3个值)</li><li id="eac3" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">ca</code>:荧光染色的主要血管数(0-3)</li><li id="5a29" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">thal</code>:地中海贫血，1 =正常；2 =修复缺陷；3 =可逆转缺陷</li><li id="db78" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated"><code class="du iw ix iy iz b">target</code>:存在心脏病。0 =正，1 =负</li></ul><p id="1425" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">该数据集包含一个列<code class="du iw ix iy iz b">target</code>，它是一个二进制标志，表示存在心脏病。这是我们希望预测的目标列。</p><p id="a698" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">要查看表中数据的前五个和后五个索引:</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="dd77" class="lc jy hh iz b fi ld le l lf lg"><em class="mu"># Reading Data:</em><br/>df=pd.read_csv("/Users/esmasert/Desktop/CODES/Jupyter/DataAssessment/heart-disease-uci/heart.csv")<br/>df.head()  <em class="mu"># Showing the First Five Rows:</em></span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mv"><img src="../Images/39bfedc1ea69150814741b45834885d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TI8-fX36vLgvF5WOta4HOQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="350f" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">要查看列名和数据信息:</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="279e" class="lc jy hh iz b fi ld le l lf lg">df.columns <em class="mu"># Column Names<br/></em>df.info() <em class="mu"># Data Information</em></span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mw"><img src="../Images/92e7c5c6c62081132b1883053cc4d6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMsAA8BkDCUO_U-5U6LDIg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="fc11" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们可以通过遍历所有列来查看行中是否有丢失的值。如果有，百分比是多少？</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="ffd5" class="lc jy hh iz b fi ld le l lf lg"><strong class="iz hi">for</strong> clmn <strong class="iz hi">in</strong> df.columns:<br/>    any_missing = df[clmn].isnull().sum()<br/>    print(f'<strong class="iz hi">{</strong>clmn<strong class="iz hi">}</strong> - <strong class="iz hi">{</strong>any_missing <strong class="iz hi">:</strong>.1%<strong class="iz hi">}</strong>')</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mx"><img src="../Images/eff8d754cf60d5d33c66a67e1a5a132d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QoPqaitmjtDDHbJm5wbb4A.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="5ba2" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">太好了！没有缺失值。</p><p id="e685" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">现在我们需要检查数据，看看是否有重复的行。看到这个:</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="6836" class="lc jy hh iz b fi ld le l lf lg">df.duplicated().sum() <em class="mu"># Counting the duplicated rows</em></span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/68ba3a5b73f43ffd8501a17bbf4ca829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6P15cp6DiuiN3l2JQSsRAg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="51fa" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">数据中有重复的行。现在，我们必须处理它，以防止任何负面影响的训练部分。因为，冗余会对数据分析产生负面影响，因为它们是不需要的值。为了克服这一点:</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mz"><img src="../Images/a830146e018245c7ab17316d694ef597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gncw33ahUiHAIAtTRyJpZg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="86b3" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">为了更好的可视化，我们可以调换列和索引。</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es na"><img src="../Images/cb13397609ef6bba443d62833be465b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRThqaNUFZ5Gcoul-CY3Xg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="4ec4" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">现在为了更好地理解数据，我们可以列出每个特性的唯一值。</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nb"><img src="../Images/fd8b5b11c97ca6ad1d213c17ff2900ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtZcijeUjeb4D3pPWKz7CQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="b0b6" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们都完成了数字可视化，现在让我们使用图表！</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="ade6" class="lc jy hh iz b fi ld le l lf lg">print(f'Number of people identified as sex 0 are <strong class="iz hi">{</strong>df.sex.value_counts()[0]<strong class="iz hi">}</strong> and Number of people identified as sex 1 are <strong class="iz hi">{</strong>df.sex.value_counts()[1]<strong class="iz hi">}</strong>')<br/>plt.figure(figsize=(7,7))<br/>p = sns.set_theme(style="darkgrid")<br/>p = sns.countplot(data=df, x="sex", palette='Set3')</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nc"><img src="../Images/b92049d6a58bb617d6c93ef76aab5e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rV9-EaEV7oWm9VyKcefvsQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="c0a0" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">也试试这段代码！结果会让你大吃一惊:)</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="3d5d" class="lc jy hh iz b fi ld le l lf lg">g = sns.PairGrid(df, hue="target")<br/>g.map_diag(sns.histplot)<br/>g.map_offdiag(sns.scatterplot)<br/>g.add_legend()</span></pre><p id="08e0" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">现在我们来看看相同年龄和性别的人有多少。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="0cb3" class="lc jy hh iz b fi ld le l lf lg"><strong class="iz hi">import</strong> <strong class="iz hi">seaborn</strong> <strong class="iz hi">as</strong> <strong class="iz hi">sns</strong><br/><strong class="iz hi">import</strong> <strong class="iz hi">matplotlib.pyplot</strong> <strong class="iz hi">as</strong> <strong class="iz hi">plt</strong><br/> <br/>plt.figure(figsize=(12,6))<br/> <br/><em class="mu"># count plot on two categorical variable</em><br/>sns.countplot(x ='age', hue = "sex", data = df)<br/> <br/><em class="mu"># Show the plot</em><br/>plt.show()</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nd"><img src="../Images/d45d65da57d831f5541b0a98f96d4890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aa0qTzd2A3GM8anSVAcvSQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nc"><img src="../Images/6ccff01234ce7f94cf0bcb6029d9be9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsHobPde3D60aPOajKgdyw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="0c4a" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">正如我们所看到的，我们从男性数据中获得了更多的信息。(1 =男性；0 =女性)</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ne"><img src="../Images/7c4ad7246be1c78348eeae2ae4ba6906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R4HAwRxbXiJ7o9fqLlWeAQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="4da5" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">从图表中我们可以看出，男性患心脏病的比例更高。(目标；0 =正，1 =负)(性；1 =男性；0 =女性)</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nf"><img src="../Images/20b371cb31ccf24ba0ff0eef349770a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbjPEfd26U98eiB_xy7jXQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="f1e8" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">在55岁到63岁之间，患心脏病的风险要大得多。(目标；心脏病的存在。0 =正，1 =负)</p><p id="d59c" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">更多图表:</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ng"><img src="../Images/27b693f761306e9a8f851577bedaac10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QmvDov7_jvwWeinBit70Sg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nh"><img src="../Images/3ec7db3473476eb3522bae11b945db4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8Rn4TCQiyr4PoVFZpFobA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ni"><img src="../Images/91ed927b5e347738469777e4027d1e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggVaF2RmQnh5S8oShUJvow.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><h1 id="0cee" class="jx jy hh bd jz ka kb kc kd ke kf kg kh in ki io kj iq kk ir kl it km iu kn ko bi translated">构建模型</h1><h2 id="4825" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">准备数据</h2><p id="5f99" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated"><strong class="jc hi">功能选择</strong></p><p id="0454" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">首先，我们需要把给定的列分成两种类型的变量；自变量(特征变量)和因变量(目标变量)。</p><p id="e208" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">拆分数据</strong></p><p id="a4b6" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">然后，我们将数据集分为训练集和测试集，将我们的数据分成80%作为训练集，20%作为测试集。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="db7d" class="lc jy hh iz b fi ld le l lf lg"><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.model_selection</strong> <strong class="iz hi">import</strong> train_test_split <em class="mu"># to split the data</em><br/><br/>X_train, X_test, y_train, y_test = train_test_split(df.drop('target', 1), df['target'], <br/>                                                    test_size = .2, random_state=10) <em class="mu">#split the data</em></span></pre><h2 id="4313" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">决策树分类</h2><p id="38db" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">现在，让我们使用Scikit-learn创建一个决策树模型。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="7bd5" class="lc jy hh iz b fi ld le l lf lg"><strong class="iz hi">import</strong> <strong class="iz hi">pandas</strong> <strong class="iz hi">as</strong> <strong class="iz hi">pd</strong><br/><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.tree</strong> <strong class="iz hi">import</strong> DecisionTreeClassifier <em class="mu"># importing Decision Tree Classifier</em><br/><strong class="iz hi">from</strong> <strong class="iz hi">sklearn</strong> <strong class="iz hi">import</strong> metrics <em class="mu"># metrics module for accuracy calculation</em></span><span id="6704" class="lc jy hh iz b fi nj le l lf lg"><em class="mu"># Create Decision Tree Classifer object</em><br/>clfDT = DecisionTreeClassifier()<br/><br/><em class="mu"># Train Decision Tree Classifer</em><br/>clfDT = clfDT.fit(X_train,y_train)<br/><br/><em class="mu">#Predict the response for test dataset</em><br/>y_pred = clfDT.predict(X_test)</span></pre><p id="2535" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">评估模型</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nk"><img src="../Images/c03baadfda3ccb28a7a7a7097a35c30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zbqAzXOpberrz6sdpFPQlQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="a582" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">并绘制决策树</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nl"><img src="../Images/a8401ae43b93ac9484b6bb068fe6149f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvKPRlT7aBOPRgbHd-2m7Q.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="7941" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">混淆矩阵</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="4420" class="lc jy hh iz b fi ld le l lf lg"><em class="mu"># Print Accuracy</em><br/>print("Accuracy:",metrics.accuracy_score(y_test, y_pred))<br/>print('<strong class="iz hi">\n</strong>')<br/><br/><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.metrics</strong> <strong class="iz hi">import</strong> plot_confusion_matrix<br/><br/>fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,7))<br/><br/>titles_options = [("Confusion matrix for Decision Tree, without normalization", <strong class="iz hi">None</strong>, axes.flatten()[0]),<br/>                  ("Normalized confusion matrix for Decision Tree", 'true', axes.flatten()[1])]<br/><br/><strong class="iz hi">for</strong> title, normalize, ax <strong class="iz hi">in</strong> titles_options:<br/>    <br/>    disp = plot_confusion_matrix(clfDT, X_test, y_test, cmap=plt.cm.Blues, ax=ax, normalize = normalize)<br/>    <br/>    disp.ax_.set_title(title)<br/>    <br/>    plt.rcParams['axes.grid'] = <strong class="iz hi">False</strong><br/><br/>    print(title)<br/>    print(disp.confusion_matrix)<br/><br/>plt.show()</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nf"><img src="../Images/eca474b75b0b0a58595c87e127a56541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bS9F7ivH1rJUsAJXuFiOag.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="1230" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">让我们将数据标准化:</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="547f" class="lc jy hh iz b fi ld le l lf lg">b = df.target.values<br/>a_data = df.drop(['target'], axis = 1)<br/><br/><em class="mu"># Normalize</em><br/>a = (a_data - np.min(a_data)) / (np.max(a_data) - np.min(a_data)).values</span><span id="b4db" class="lc jy hh iz b fi nj le l lf lg">a_train, a_test, b_train, b_test = train_test_split(a, b, test_size = 0.2, random_state=0)</span><span id="a6a1" class="lc jy hh iz b fi nj le l lf lg"><em class="mu"># Create Decision Tree classifer object</em><br/>clfNorm = DecisionTreeClassifier()<br/><br/><em class="mu"># Train Decision Tree Classifer</em><br/>clfNorm = clfNorm.fit(a_train,b_train)<br/><br/><em class="mu">#Predict the response for test dataset</em><br/>b_pred = clfNorm.predict(a_test)</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nm"><img src="../Images/5fc06c0c79e0cc8de17a4e24aff3e236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*by_FZrLTdztBiY2_aQifCQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="a281" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">太好了！正如您在这里看到的，我们可以通过在训练前标准化数据来提高准确性。</p><h2 id="269d" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">随机森林分类</h2><p id="aa8e" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">随机森林算法是一种基于随机决策树的集成平均算法。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="63b7" class="lc jy hh iz b fi ld le l lf lg"><em class="mu">#Import Random Forest Model</em><br/><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.ensemble</strong> <strong class="iz hi">import</strong> RandomForestClassifier</span><span id="ab37" class="lc jy hh iz b fi nj le l lf lg"><em class="mu">#Create a Gaussian Classifier</em><br/>clfRF=RandomForestClassifier(n_estimators=100)<br/><br/><em class="mu">#Train the model using the training sets y_pred=clf.predict(X_test)</em><br/>clfRF.fit(X_train,y_train)<br/><br/>y_pred=clfRF.predict(X_test)</span></pre><p id="3bc3" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">评估模型:</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nn"><img src="../Images/3312f90123d833eddf5c21d2abf1531b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*icTPSFQ4ell3sMaaaDTPow.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="87ea" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">这个模型的混淆矩阵。</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es no"><img src="../Images/06072f76548e8b7901634b81b79d3e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Pn_MrINL7Jg8AuqeRAIeQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="a3ea" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">如果我们用标准化的数据训练模型，</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="3f8a" class="lc jy hh iz b fi ld le l lf lg"><em class="mu">#Create a Gaussian Classifier</em><br/>clfRFNorm=RandomForestClassifier(n_estimators=100)<br/><br/><em class="mu">#Train the model using the training sets y_pred=clf.predict(X_test)</em><br/>clfRFNorm.fit(a_train,b_train)<br/><br/>b_predNorm=clfRFNorm.predict(a_test)</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es np"><img src="../Images/f91ac4c6a48c8f9069077b7269aa2bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCB4lmcveTKEmm5swvygXQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="5eb0" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">太好了！正如您在这里看到的，我们通过将训练前的数据从0.770归一化到0.868来提高精确度。</p><p id="96b0" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">寻找重要特征</strong></p><p id="fb94" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们使用特征重要性变量来查看特征重要性分数。然后我们将使用seaborn库可视化这些分数。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="abb7" class="lc jy hh iz b fi ld le l lf lg">feature_imp = pd.Series(clfRF.feature_importances_, index=X_train.columns.values.tolist()).sort_values(ascending=<strong class="iz hi">False</strong>)<br/>feature_imp</span><span id="2c1c" class="lc jy hh iz b fi nj le l lf lg"><strong class="iz hi">from</strong> <strong class="iz hi">matplotlib</strong> <strong class="iz hi">import</strong> pyplot<br/><br/><em class="mu"># Creating a bar plot</em><br/>fig, ax = pyplot.subplots(figsize=(14, 7))<br/>sns.barplot(x=feature_imp, y=feature_imp.index)<br/><br/><em class="mu"># Add labels to your graph</em><br/>plt.xlabel('Feature Importance Score',fontsize=15)<br/>plt.ylabel('Features',fontsize=15)<br/>plt.title("Visualizing Important Features",fontsize=18)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nq"><img src="../Images/ac1e300ce780eb030ce101833f3239a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLTybAZF4GvFwPDijb53CQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="a4e0" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">在所选特征上生成模型</strong></p><p id="12ff" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">这里，我们可以删除最后4个特征(fbs、restecg、sex、exang ),因为根据其他特征，它们的重要性非常低，并选择其余的剩余特征。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="d6db" class="lc jy hh iz b fi ld le l lf lg"><em class="mu"># Split dataset into features and labels</em><br/>RmX= df[['age','cp','trestbps','chol','thalach','oldpeak','slope','ca','thal']]  <em class="mu"># Removed feature "sepal length"</em><br/>Rmy= df['target']                         <br/><br/><em class="mu"># Split dataset into training set and test set</em><br/>RmX_train, RmX_test, Rmy_train, Rmy_test = train_test_split(RmX, Rmy, test_size=0.20, random_state=5) <em class="mu"># 80% training and 20% test</em></span><span id="7131" class="lc jy hh iz b fi nj le l lf lg"><em class="mu">#Create a Gaussian Classifier</em><br/>Rmclf=RandomForestClassifier(n_estimators=100)<br/><br/><em class="mu">#Train the model using the training sets y_pred=clf.predict(X_test)</em><br/>Rmclf.fit(RmX_train,Rmy_train)<br/><br/><em class="mu"># prediction on test set</em><br/>Rmy_pred=Rmclf.predict(RmX_test)</span></pre><p id="0cf5" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">评估模型:</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nr"><img src="../Images/4ac653d7882260b6f14fb35de48d84b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dtY3yOqZbhFPO2jJv_6nQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="3558" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">太棒了。！准确度从0.770提高到0.836。</p><p id="970d" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">您可以看到，在删除最不重要的特征后，精确度提高了。这是因为我们去除了误导性的数据和噪音。较少的特征也减少了训练时间。因为我们删除了影响较小的特性，所以我们已经去掉了不必要的特性。这就是标准化数据可能不适用于这种方法原因。</p><h2 id="a724" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">朴素贝叶斯分类</h2><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="375c" class="lc jy hh iz b fi ld le l lf lg"><em class="mu">#Import Gaussian Naive Bayes model</em><br/><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.naive_bayes</strong> <strong class="iz hi">import</strong> GaussianNB</span><span id="7eed" class="lc jy hh iz b fi nj le l lf lg"><em class="mu">#Create a Gaussian Classifier</em><br/>NBclf = GaussianNB()<br/><br/><em class="mu"># Train the model using the training sets</em><br/>NBclf.fit(X_train,y_train)<br/><br/><em class="mu">#Predict Output</em><br/>NBy_pred = NBclf.predict(X_test) <em class="mu"># 0:Overcast, 2:Mild</em></span></pre><p id="3bc5" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">评估模型</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ns"><img src="../Images/8f9623f9cf55b33724bd6c21bf884bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYH6z69qbYn_fWdnBoHJkA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="67d9" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">混淆矩阵:</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nt"><img src="../Images/174028c40fab0ae95e3d6657da0f0d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hr34FOxJBUyPAM39Y-yY2A.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="d3ca" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">在接近尾声时，我创建了一个表格，显示了每个步骤，以了解模型精度是如何受到影响的。</p><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nu"><img src="../Images/a97e1cd3f6034d9fb25d543a7bfd39b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoWQHPsMuMYQ3-6J5x_Bpg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><h1 id="def2" class="jx jy hh bd jz ka kb kc kd ke kf kg kh in ki io kj iq kk ir kl it km iu kn ko bi translated">结论</h1><h2 id="f1a9" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">比较模型</h2><p id="39a7" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">我们可以通过查看图表来比较这些模型，</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="54ca" class="lc jy hh iz b fi ld le l lf lg">colors = ['red','plum','slateblue','lavender','mediumaquamarine','silver','khaki','yellowgreen','lightskyblue']<br/>plt.figure(figsize=(14,7))<br/>plt.title("Accuracies of different models")<br/>plt.xlabel("Algorithms")<br/>plt.ylabel("Accuracy %")<br/>plt.bar(results_df['Model'],results_df['Testing Accuracy %'], color = colors)<br/>plt.xticks(rotation='vertical')<br/><br/>plt.show()</span></pre><figure class="ku kv kw kx fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nv"><img src="../Images/2840790d3159e254ab999d9fb0b95724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CT_HuFe-F3utvk5arG0s-g.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Image by Author</figcaption></figure><p id="b16c" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">正如我们在这里看到的，实现最高准确性的最有效的算法是朴素贝叶斯和心脏病UCI数据集的归一化数据。即使朴素贝叶斯算法的训练精度低于其他算法，有趣的是测试精度结果却是最高的。</p><p id="8e80" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">同样，我从具有FIF的朴素贝叶斯分类器和标准化数据结果中获得了较小的准确性。此外，由于我没有从带有标准化数据和FIF的随机森林中获得更高的结果，为了防止复杂的查找，我从表中排除了带有FIF和标准化数据结果的朴素贝叶斯分类器。</p><h2 id="03ea" class="lc jy hh bd jz lt lu lv kd lw lx ly kh jj lz ma kj jn mb mc kl jr md me kn mf bi translated">如何提高精确度</h2><p id="6d28" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated"><strong class="jc hi">归一化数据</strong></p><p id="ba25" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">数据规范化本质上是一种过程，在这种过程中，数据被重新组织，以便用户可以正确地利用它进行进一步的查询和分析。规范化的目标是将数据集中数值列的值更改为一个通用的比例，而不会扭曲值范围的差异。</p><p id="bc6b" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">正如你所看到的，几乎在所有的方法中，归一化方法对精度都有很大的影响。因此，使用这种方法在大多数时候是有益的。</p><p id="c8a4" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">组装</strong></p><p id="1729" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">为了提高模型的精度，我们可以使用集成技术。集成方法的目标是将几个基本估计量的预测与给定的学习算法结合起来，以提高单个估计量的可推广性/稳健性。</p><p id="1b67" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">有两种类型的集合方法:</p><ul class=""><li id="9633" class="mg mh hh jc b jd je jg jh jj mi jn mj jr mk jv ml mm mn mo bi translated">平均方法:驱动原理是独立地建立几个估计量，然后平均它们的预测。平均而言，组合估计量通常比任何单基估计量都好，因为它的方差减小了。</li><li id="17d0" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">Boosting方法:基本估计量是按顺序建立的，人们试图减少组合估计量的偏差。其动机是将几个弱模型组合起来，产生一个强大的集合。</li></ul><p id="eb6d" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">我们已经使用了一种整体平均方法，随机森林。因此，我们将继续讨论其他问题。但是，为了证明集成方法的积极效果，我们可以只检查随机森林算法的结果。</p><p id="d656" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">找到重要的特征，去掉最不重要的特征</strong></p><p id="46e5" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">特征重要性指的是一类用于为预测模型的输入特征分配分数的技术，该预测模型在进行预测时指示每个特征的相对重要性。</p><p id="89f7" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">分数非常有用，可用于预测建模问题中的各种情况，例如:</p><ul class=""><li id="121b" class="mg mh hh jc b jd je jg jh jj mi jn mj jr mk jv ml mm mn mo bi translated">更好地理解数据。</li><li id="4f80" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">更好地理解模型。</li><li id="47d0" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">减少输入特征的数量。</li><li id="11f7" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">减少训练时间，更少的数据点降低了算法复杂性，算法训练更快。</li><li id="0a0e" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">减少过度拟合，减少冗余数据意味着减少基于噪声做出决策的机会。</li><li id="3d01" class="mg mh hh jc b jd mp jg mq jj mr jn ms jr mt jv ml mm mn mo bi translated">提高准确性，减少误导性数据意味着建模准确性提高。</li></ul><p id="366b" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">超参数调谐</strong></p><p id="b360" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">为了查看超参数调整效果，我们将调整随机森林模型的超参数。我们将调整的超参数包括max_features和n_estimators。</p><pre class="ku kv kw kx fd ky iz kz la aw lb bi"><span id="aef1" class="lc jy hh iz b fi ld le l lf lg"><strong class="iz hi">from</strong> <strong class="iz hi">sklearn.model_selection</strong> <strong class="iz hi">import</strong> GridSearchCV<br/><br/>max_features_range = np.arange(1,6,1)<br/>n_estimators_range = np.arange(10,210,10)<br/>param_grid = dict(max_features=max_features_range, n_estimators=n_estimators_range)<br/><br/>RanFrstCls = RandomForestClassifier()<br/><br/>gridSCV = GridSearchCV(estimator=RanFrstCls, param_grid=param_grid, cv=5)</span><span id="300d" class="lc jy hh iz b fi nj le l lf lg">gridSCV.fit(X_train, y_train)</span></pre><p id="c679" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">scikit-learn的GridSearchCV()函数用于执行超参数调整。特别地，GridSearchCV()函数可以执行分类器的典型功能，例如拟合、评分和预测，以及predict_proba、decision_function、transform和inverse_transform。</p><p id="3e39" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">调整随机森林算法的超参数后，准确率从80.327869提高到85.9439。</p><p id="f44c" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">这是我们尝试过的所有随机森林分类方法中精度最高的！</p><h1 id="b9b8" class="jx jy hh bd jz ka kb kc kd ke kf kg kh in ki io kj iq kk ir kl it km iu kn ko bi translated">结果</h1><p id="73b7" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">当我们用心脏病UCI数据集训练3个算法时，我们获得了如此多不同的准确性。</p><p id="a695" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">只要我们搜索，目前为止最好的准确率是使用归一化数据的朴素贝叶斯分类算法(90.1634)</p><p id="9543" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">第二高的精确度是具有调整的超参数的随机森林算法(85.9354)</p><p id="420f" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">第三高的精度是集成方法中的梯度树提升算法。(85.2459)</p><p id="4a04" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">最后，第四高的准确性是随机森林分类器与发现重要特征的方法。(83.6065)</p><p id="1678" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">在这篇文章中，我尝试了；寻找重要的特征、集合、超参数调整和标准化数据，以了解它们如何影响精度。在训练或建模之前，我们有更多的方法可以改变数据。</p><p id="379e" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">从以上信息中，我们可以得出结论，我们尝试的所有算法都通过额外的算法和方法实现了更高的精度。所有方法和算法的效率都随着数据和建立的分类器而变化。</p><p id="59a5" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated"><strong class="jc hi">非常感谢你的阅读！</strong></p></div><div class="ab cl nw nx go ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ha hb hc hd he"><h1 id="62db" class="jx jy hh bd jz ka od kc kd ke oe kg kh in of io kj iq og ir kl it oh iu kn ko bi translated">参考</h1><p id="2562" class="pw-post-body-paragraph ja jb hh jc b jd kp ii jf jg kq il ji jj kr jl jm jn ks jp jq jr kt jt ju jv ha bi translated">https://www.kaggle.com/ronitf/heart-disease-uci</p><p id="f315" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">[2]<a class="ae jw" href="https://scikit-learn.org/stable/modules/ensemble.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/ensemble.html</a></p><p id="8980" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">[3]<a class="ae jw" rel="noopener" href="/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029">https://medium . com/@ urvashilluniya/why-data-normalization-is-required-for-machine-learning-models-681 b65a 05029</a></p><p id="e74a" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">[4]<a class="ae jw" href="https://machinelearningmastery.com/calculate-feature-importance-with-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/calculate-feature-importance-with-python/</a></p><p id="d3a7" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">[5]https://github.com/dataprofessor/code/blob/master/python<a class="ae jw" href="https://github.com/dataprofessor/code/blob/master/python" rel="noopener ugc nofollow" target="_blank"/></p><p id="a219" class="pw-post-body-paragraph ja jb hh jc b jd je ii jf jg jh il ji jj jk jl jm jn jo jp jq jr js jt ju jv ha bi translated">[6]<a class="ae jw" href="https://www.datacamp.com/community/tutorials/" rel="noopener ugc nofollow" target="_blank">https://www.datacamp.com/community/tutorials/</a></p></div></div>    
</body>
</html>