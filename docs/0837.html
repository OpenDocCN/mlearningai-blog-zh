<html>
<head>
<title>A must-have training trick for VAE(variational autoencoder)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VAE的必备训练技巧(变分自动编码器)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-must-have-training-trick-for-vae-variational-autoencoder-d28ff53b0023?source=collection_archive---------0-----------------------#2021-08-01">https://medium.com/mlearning-ai/a-must-have-training-trick-for-vae-variational-autoencoder-d28ff53b0023?source=collection_archive---------0-----------------------#2021-08-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/f18fba096c5fa69a3b31e52c9cb14404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*DBHIexU4GMXBw2-D"/></div><figcaption class="il im et er es in io bd b be z dx"><a class="ae ip" href="http://www.quickmeme.com/meme/35m30c" rel="noopener ugc nofollow" target="_blank">http://www.quickmeme.com/meme/35m30c</a></figcaption></figure><p id="a869" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这个诀窍被称为<a class="ae ip" href="https://aclanthology.org/N19-1021.pdf" rel="noopener ugc nofollow" target="_blank">循环退火时间表</a>，正如杜克大学和微软研究院在雷蒙德的一篇论文中所描述的。</p><p id="c1a5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">VAE是自然语言处理任务中常见的一种强大的深度生成模型。虽然VAE的概念不是本文的重点，但对VAE的简要介绍有助于理解这个技巧。在训练VAE模型时，我们使用训练数据本身作为标签，并将数据抑制到低维空间中。在VAE我们有两个部分:编码器和解码器。编码器将数据编码到低维空间中，而解码器从潜在表示中重构原始数据。通过这样做，我们迫使模型以紧凑的方式表达我们的训练数据，并在潜在空间中将相似的数据分组在一起。空间中相邻点之间的相似性取决于目标函数的设计。在自然语言处理领域，一个好的表示意味着很多，因为一个单词或句子通常是在非常高的维度上描述的，因此建模具有挑战性。有了<strong class="is hi">意味深长的</strong>和<strong class="is hi">紧凑的</strong>表象，我们就能在下游任务中表现得更好。例如，在药物发现中，研究人员必须在10⁶⁰的化学空间中搜索新分子的大小，但现在他们可以缩小搜索空间，并在所需的区域中导航。</p><p id="c3ca" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">VAE的所有训练技巧都服务于一个目的:以较小的信息损失更好地表示原始数据。</p><p id="697f" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在目标函数中有两个分量:重建损失和kull back-lei bler发散项的损失(KL损失)。前者表明VAE能多好地从潜在空间重建输入序列，而后者测量两个数据分布彼此有多相似。在实际应用中，我们通常计算潜在分布和标准正态分布之间的KL散度。正如我们在下面的函数中看到的，系数β控制着KL散度在总损失中的权重。</p><figure class="jp jq jr js fd ii er es paragraph-image"><div class="er es jo"><img src="../Images/2d8d99b0904f31f40ef970a56b0ad0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*QtJakQL7gWokYMI9ZuGjJg.png"/></div><figcaption class="il im et er es in io bd b be z dx"><a class="ae ip" href="https://www.jeremyjordan.me/variational-autoencoders/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/variational-autoencoders/</a></figcaption></figure><p id="776e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这里我们有一个臭名昭著的KL消失问题:KL项在训练期间变得非常小(接近于零)。当解码器以自回归方式工作时，就会发生这种情况。而KL消失问题将导致数据分布符合标准正态分布的不太有趣(如果不是无意义的话)的表示。</p><p id="a578" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">缓解KL-消失问题的一种方法是对KL项应用退火程序。</p><p id="0b4b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">传统的方法是单调退火。通过从小系数β开始，我们迫使模型集中于重建输入序列，而不是最小化KL损失。随着Beta的增加，模型逐渐强调数据分布的形状，最终Beta达到1或预先指定的值。</p><figure class="jp jq jr js fd ii er es paragraph-image"><div class="er es jt"><img src="../Images/fc1cab4de92ace3bfdbcb265da487ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*bThxwnhB8Pakd-K4ZxgMdw.png"/></div><figcaption class="il im et er es in io bd b be z dx"><a class="ae ip" href="https://arxiv.org/pdf/1511.06349.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1511.06349.pdf</a></figcaption></figure><p id="2c1d" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">杜克大学和微软研究院的研究人员提出了循环退火计划，其中传统的退火重复多次。这个时间表有助于建立一个更好地组织的潜在空间，并在计算中增加微不足道的额外成本。论文中的官方演示如下所示，</p><figure class="jp jq jr js fd ii er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/3c7040d1496c19a53d6ba190c1bea584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYGczD_zPGMIxH9c-FIwKQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><a class="ae ip" href="https://aclanthology.org/attachments/N19-1021.Supplementary.pdf" rel="noopener ugc nofollow" target="_blank">https://aclanthology.org/attachments/N19-1021.Supplementary.pdf</a></figcaption></figure><p id="1b88" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">正如我们所见，有三种方法可以增加KL项的权重:线性，Sigmoid和余弦。根据经验，我发现他们在训练中大致相同，下游任务的表现提高了两位数的百分比。官方GitHub回购的论文是<a class="ae ip" href="https://github.com/haofuml/cyclical_annealing/blob/master/plot/plot_schedules.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="a821" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">参考:</p><p id="cbf8" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">循环退火程序:一种减轻KL消失的简单方法</p><p id="54d9" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">https://aclanthology.org/N19-1021.pdf<a class="ae ip" href="https://aclanthology.org/N19-1021.pdf" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="fdb0" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">无关的想法</h1><p id="4fa0" class="pw-post-body-paragraph iq ir hh is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn ha bi translated">试着把ELBO想成一个双目标优化。然后，我们可以将技巧应用于任何多目标优化问题(特别是当损失函数的一个组件导致模型绕过所有其他损失组件时)。</p></div></div>    
</body>
</html>