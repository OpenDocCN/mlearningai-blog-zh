# PPOâ€”â€”æœ€æ–°å¼ºåŒ–å­¦ä¹ çš„ç›´è§‚æŒ‡å—

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/ppo-intuitive-guide-to-state-of-the-art-reinforcement-learning-410a41cb675b?source=collection_archive---------0----------------------->

# ä»‹ç»

(è¿™ä¸ªæ•…äº‹ä¹Ÿå¯ä»¥ä½œä¸ºä¸€ä¸ª [Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing))ã€‚

**P** è¿‘ä¼¼ **P** ç­–ç•¥ **O** ä¼˜åŒ–( **PPO** )è‡ªä»åœ¨è®ºæ–‡ [**è¿‘ä¼¼ç­–ç•¥ä¼˜åŒ–ç®—æ³•**](https://arxiv.org/abs/1707.06347)(Schulman et al .è‰¾å°”ã€‚, 2017).è¿™ç§ä¼˜é›…çš„ç®—æ³•å¯ä»¥å¹¶ä¸”å·²ç»è¢«ç”¨äºå„ç§ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå®ƒä¹Ÿè¢«ç”¨äº ChatGPT çš„è®­ç»ƒï¼Œè¿™æ˜¯ç›®å‰æœ€çƒ­é—¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

PPO ä¸ä»…åœ¨ RL ç¤¾åŒºä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œè€Œä¸”å®ƒè¿˜æ˜¯é€šè¿‡æ·±åº¦å­¦ä¹ (DL)æ¨¡å‹è§£å†³ RL é—®é¢˜çš„ä¼˜ç§€å…¥é—¨ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ç»™å‡ºäº†å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„å¿«é€Ÿæ¦‚è¿°ï¼Œè§£å†³ RL é—®é¢˜çš„ç®—æ³•çš„åˆ†ç±»ï¼Œä»¥åŠåœ¨[è®ºæ–‡](https://arxiv.org/abs/1707.06347)ä¸­æå‡ºçš„ PPO ç®—æ³•çš„è¯„è®ºã€‚æœ€ååˆ†äº«ä¸€ä¸‹[æˆ‘è‡ªå·±åœ¨ PyTorch ä¸­å¯¹ PPO ç®—æ³•çš„å®ç°](https://github.com/BrianPulfer/PapersReimplementations)ï¼Œå¯¹å¾—åˆ°çš„ç»“æœè¿›è¡Œè¯„è®ºï¼Œå¹¶ä»¥ç»“è®ºç»“æŸã€‚

# å¼ºåŒ–å­¦ä¹ 

![](img/230d7f9136896e3de729fe2339f5dae0.png)

ChatGPTâ€™s answer to the prompt: â€œGive an overview on the field of Reinforcement Learningâ€. While I asked help to ChatGPT for the introduction to the field of RL which was used to train ChatGPT itself (quite meta), I promise that everything in this article apart from this picture is written by me.

é¦–å…ˆå‘æ¥è¿‘ RL çš„äººå±•ç¤ºçš„ç»å…¸å›¾ç‰‡å¦‚ä¸‹:

![](img/a2cbef0b3f99d799b6a4ce04ff2d9f62.png)

Reinforcement Learning framework. Image from [neptune.ai](https://neptune.ai/blog/reinforcement-learning-agents-training-debug)

åœ¨æ¯ä¸ªæ—¶é—´æˆ³ï¼Œç¯å¢ƒå‘ä»£ç†æä¾›å¥–åŠ±å’Œå¯¹å½“å‰çŠ¶æ€çš„è§‚å¯Ÿã€‚ç»™å®šè¿™äº›ä¿¡æ¯ï¼Œä»£ç†åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ï¼Œå¹¶ä»¥æ–°çš„å¥–åŠ±å’ŒçŠ¶æ€ç­‰åšå‡ºå“åº”ã€‚è¿™ä¸ªéå¸¸é€šç”¨çš„æ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸã€‚

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›é€ ä¸€ä¸ªèƒ½æœ€å¤§åŒ–æ‰€è·å›æŠ¥çš„ä»£ç†äººã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é€šå¸¸å¯¹æŠ˜æ‰£å¥–åŠ±çš„æ€»å’Œæœ€å¤§åŒ–æ„Ÿå…´è¶£

![](img/a9f1eb8818dc7dd5e9b9f841e3c92b83.png)

å…¶ä¸­ï¼ŒÎ³æ˜¯æŠ˜æ‰£å› å­ï¼Œé€šå¸¸åœ¨[0.95ï¼Œ0.99]èŒƒå›´å†…ï¼Œr_t æ˜¯æ—¶é—´æˆ³ t çš„å¥–åŠ±ã€‚

# ç®—æ³•

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è§£å†³ RL é—®é¢˜å‘¢ï¼Ÿæœ‰å¤šç§ç®—æ³•ï¼Œä½†å®ƒä»¬å¯ä»¥(é’ˆå¯¹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æˆ– MDP)åˆ†ä¸ºä¸¤ç±»:**åŸºäºæ¨¡å‹çš„**(åˆ›å»ºç¯å¢ƒçš„æ¨¡å‹)å’Œ**æ— æ¨¡å‹çš„**(åªéœ€äº†è§£ç»™å®šä¸€ä¸ªçŠ¶æ€è¦åšä»€ä¹ˆ)ã€‚

![](img/0374f13917271b72108dc4ec1be7066d.png)

Taxonomy of Reinforcement Learning algorithms (from [OpenAI spinning up](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html))

**åŸºäºæ¨¡å‹çš„**ç®—æ³•ä½¿ç”¨ä¸€ä¸ªç¯å¢ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥é¢„æµ‹æœªæ¥çš„çŠ¶æ€å’Œå¥–åŠ±ã€‚è¯¥æ¨¡å‹è¦ä¹ˆæ˜¯ç»™å®šçš„(ä¾‹å¦‚æ£‹ç›˜)ï¼Œè¦ä¹ˆæ˜¯å­¦ä¹ çš„ã€‚

**æ— æ¨¡å‹**ç®—æ³•å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œç›´æ¥å­¦ä¹ å¦‚ä½•é’ˆå¯¹è®­ç»ƒä¸­é‡åˆ°çš„çŠ¶æ€é‡‡å–è¡ŒåŠ¨(ç­–ç•¥ä¼˜åŒ–æˆ– PO)ï¼Œå“ªäº›çŠ¶æ€-è¡ŒåŠ¨å¯¹äº§ç”Ÿè‰¯å¥½çš„å›æŠ¥(Q-Learning)ï¼Œæˆ–è€…ä¸¤è€…åŒæ—¶è¿›è¡Œã€‚

**PPO** å±äº PO ç®—æ³•å®¶æ—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹æ¥å­¦ä¹  PPO ç®—æ³•ã€‚PO å’Œ Q-Learning ç®—æ³•ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒPO ç®—æ³•å¯ä»¥ç”¨äºå…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒä¸­(å³ï¼Œæˆ‘ä»¬çš„åŠ¨ä½œå…·æœ‰çœŸå®å€¼çš„ç¯å¢ƒä¸­)ï¼Œå¹¶ä¸”å¯ä»¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå³ä½¿è¯¥ç­–ç•¥æ˜¯éšæœºçš„(å³ï¼Œæ¦‚ç‡æ€§åœ°åŠ¨ä½œ)ï¼Œè€Œ Q-Learning ç®—æ³•ä¸èƒ½åšè¿™äº›äº‹æƒ…ã€‚è¿™æ˜¯æ›´å–œæ¬¢ PO ç®—æ³•çš„å¦ä¸€ä¸ªåŸå› ã€‚å¦ä¸€æ–¹é¢ï¼ŒQ å­¦ä¹ ç®—æ³•å¾€å¾€æ›´ç®€å•ã€æ›´ç›´è§‚ã€æ›´å¥½è®­ç»ƒã€‚

## ç­–ç•¥ä¼˜åŒ–(åŸºäºæ¢¯åº¦)

PO ç®—æ³•å°è¯•ç›´æ¥å­¦ä¹ ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬è¦ä¹ˆä½¿ç”¨æ— æ¢¯åº¦ç®—æ³•(å¦‚é—ä¼ ç®—æ³•)ï¼Œè¦ä¹ˆä½¿ç”¨æ›´å¸¸è§çš„åŸºäºæ¢¯åº¦çš„ç®—æ³•ã€‚

å¯¹äºåŸºäºæ¢¯åº¦çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯æ‰€æœ‰è¯•å›¾ä¼°è®¡æ‰€å­¦æ”¿ç­–ç›¸å¯¹äºç´¯ç§¯å›æŠ¥çš„æ¢¯åº¦çš„æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“è¿™ä¸ªæ¢¯åº¦(æˆ–å®ƒçš„è¿‘ä¼¼å€¼)ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ”¿ç­–çš„å‚æ•°å‘æ¢¯åº¦çš„æ–¹å‘ç§»åŠ¨ï¼Œä»¥ä½¿å›æŠ¥æœ€å¤§åŒ–ã€‚

![](img/979b9dea533516e58766d689258a5b29.png)

Objective to be maximized with PO algorithms. Image from [Lilâ€™Logâ€™s blog.](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)

è¯·æ³¨æ„ï¼Œæœ‰å¤šç§æ–¹æ³•å¯ä»¥ä¼°ç®—æ¢¯åº¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°åˆ—å‡ºäº† 6 ä¸ªä¸åŒçš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å®ƒä»¬ä½œä¸ºæˆ‘ä»¬çš„æœ€å¤§åŒ–ç›®æ ‡:æ€»å›æŠ¥ã€ä¸€æ¬¡è¡ŒåŠ¨åçš„å›æŠ¥ã€å‡å»åŸºçº¿ç‰ˆæœ¬çš„å›æŠ¥ã€çŠ¶æ€-è¡ŒåŠ¨å€¼å‡½æ•°ã€ä¼˜åŠ¿å‡½æ•°(åœ¨æœ€åˆçš„ PPO è®ºæ–‡ä¸­ä½¿ç”¨)å’Œæ—¶é—´å·®(TD)æ®‹å·®ã€‚åŸåˆ™ä¸Šï¼Œå®ƒä»¬éƒ½æä¾›äº†æˆ‘ä»¬æ„Ÿå…´è¶£çš„çœŸå®æ¢¯åº¦çš„ä¼°è®¡ã€‚

# èšè‹¯é†šï¼ˆPolyphenylene Oxide çš„ç¼©å†™ï¼‰

**PPO** æ˜¯ä¸€ç§(æ— æ¨¡å‹)ç­–ç•¥ä¼˜åŒ–çš„åŸºäºæ¢¯åº¦çš„ç®—æ³•ã€‚è¯¥ç®—æ³•æ—¨åœ¨å­¦ä¹ ä¸€ç§ç­–ç•¥ï¼Œåœ¨è®­ç»ƒæœŸé—´ç»™å®šç»éªŒçš„æƒ…å†µä¸‹ï¼Œæœ€å¤§åŒ–æ‰€è·å¾—çš„ç´¯ç§¯å¥–åŠ±ã€‚

å®ƒç”±ä¸€ä¸ª**æ¼”å‘˜** **Ï€Î¸(ã€‚| st)** ï¼Œå…¶è¾“å‡ºç»™å®šæ—¶é—´æˆ³ t çš„çŠ¶æ€ä¸‹çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»¥åŠç”±**è¯„è®ºå®¶** **V(st)** ï¼Œå…¶ä¼°è®¡æ¥è‡ªè¯¥çŠ¶æ€(æ ‡é‡)çš„é¢„æœŸç´¯ç§¯å›æŠ¥ã€‚å› ä¸ºæ¼”å‘˜å’Œè¯„è®ºå®¶éƒ½å°†çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œæ‰€ä»¥å¯ä»¥åœ¨æå–é«˜çº§ç‰¹å¾çš„ä¸¤ä¸ªç½‘ç»œä¹‹é—´å…±äº«ä¸»å¹²æ¶æ„ã€‚

PPO çš„ç›®çš„æ˜¯ä½¿æ”¿ç­–æ›´æœ‰å¯èƒ½é€‰æ‹©å…·æœ‰é«˜åº¦â€œä¼˜åŠ¿â€çš„è¡ŒåŠ¨ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå…·æœ‰æ¯”æ‰¹è¯„å®¶æ‰€èƒ½é¢„æµ‹çš„é«˜å¾—å¤šçš„è¡¡é‡ç´¯ç§¯å›æŠ¥ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›åœ¨ä¸€ä¸ªæ­¥éª¤ä¸­æ›´æ–°ç­–ç•¥å¤ªå¤šï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå¯¼è‡´ä¼˜åŒ–é—®é¢˜ã€‚æœ€åï¼Œå¦‚æœè¯¥ç­–ç•¥å…·æœ‰é«˜ç†µï¼Œæˆ‘ä»¬å°†ä¸ºå…¶æä¾›å¥–é‡‘ï¼Œä»¥æ¿€åŠ±æ¢ç´¢è€Œä¸æ˜¯å¼€å‘ã€‚

æ€»æŸå¤±å‡½æ•°(å°†è¢«æœ€å¤§åŒ–)ç”±ä¸‰é¡¹ç»„æˆ:å‰Šæ³¢é¡¹ã€ä»·å€¼å‡½æ•°(VF)é¡¹å’Œç†µå¥–åŠ±ã€‚

æœ€ç»ˆç›®æ ‡å¦‚ä¸‹:

![](img/4eee1d33404f1b2b761471a13990c1cd.png)

The loss function of PPO to be maximized.

å…¶ä¸­ï¼Œc1 å’Œ c2 æ˜¯è¶…å‚æ•°ï¼Œåˆ†åˆ«è¡¡é‡ç­–ç•¥çš„æ‰¹è¯„å‡†ç¡®æ€§å’Œæ¢ç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚

## å‰ªè¾‘æœ¯è¯­

æ­£å¦‚æˆ‘ä»¬æ‰€è¯´çš„ï¼ŒæŸå¤±å‡½æ•°ä¿ƒä½¿äº§ç”Ÿä¼˜åŠ¿çš„è¡ŒåŠ¨çš„æ¦‚ç‡æœ€å¤§åŒ–(æˆ–è€…ï¼Œå¦‚æœè¡ŒåŠ¨äº§ç”Ÿè´Ÿé¢ä¼˜åŠ¿ï¼Œåˆ™ä½¿æ¦‚ç‡æœ€å°åŒ–):

![](img/ce776e6339715fda94243c3426efa1dd.png)

First loss term. We maximize and minimize the probability of picking

å…¶ä¸­:

![](img/f7bac037dd0d4bf8900570879e4c7a85.png)

Coefficient rt(Î¸). This is the term that gradients are going to go through.

æ˜¯ä¸€ä¸ªæ¯”ç‡ï¼Œç”¨äºè¡¡é‡æˆ‘ä»¬ç°åœ¨(ä½¿ç”¨æ›´æ–°çš„ç­–ç•¥)ç›¸å¯¹äºä»¥å‰æ‰§è¡Œä»¥å‰çš„æ“ä½œçš„å¯èƒ½æ€§ã€‚åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬ä¸å¸Œæœ›è¿™ä¸ªç³»æ•°å¤ªé«˜ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ”¿ç­–çªç„¶æ”¹å˜ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å–å®ƒçš„æœ€å°å€¼å’Œ[1+Ïµ1-Ïµ]ä¹‹é—´çš„å‰ªè¾‘ç‰ˆæœ¬ï¼Œå…¶ä¸­Ïµæ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚

ä¼˜åŠ¿è®¡ç®—å¦‚ä¸‹:

![](img/dcb251ae3343ef3432352da91060b17b.png)

Advantage estimate. We simply take a difference between what we estimated the cumulative reward would have been given the initial state and the real cumulative reward observed up to a step t plus the estimate from that state onward. We apply a stop-gradient operator to this term in the CLIP loss.

æˆ‘ä»¬çœ‹åˆ°ï¼Œå®ƒåªæ˜¯ç®€å•åœ°è¡¡é‡äº†æ‰¹è¯„è€…å¯¹ç»™å®šçŠ¶æ€ st çš„é”™è¯¯ç¨‹åº¦ã€‚å¦‚æœæˆ‘ä»¬è·å¾—äº†æ›´é«˜çš„ç´¯ç§¯å¥–åŠ±ï¼Œä¼˜åŠ¿ä¼°è®¡å°†ä¸ºæ­£ï¼Œæˆ‘ä»¬å°†æ›´æœ‰å¯èƒ½åœ¨è¿™ç§çŠ¶æ€ä¸‹é‡‡å–è¡ŒåŠ¨ã€‚åä¹‹äº¦ç„¶ï¼Œå¦‚æœæˆ‘ä»¬æœŸæœ›æ›´é«˜çš„å›æŠ¥ï¼Œè€Œæˆ‘ä»¬å¾—åˆ°äº†æ›´ä½çš„å›æŠ¥ï¼Œä¼˜åŠ¿ä¼°è®¡å°†æ˜¯è´Ÿçš„ï¼Œæˆ‘ä»¬å°†å‡å°‘åœ¨è¿™ä¸€æ­¥é‡‡å–è¡ŒåŠ¨çš„å¯èƒ½æ€§ã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬ä¸€ç›´åˆ°æœ€åçš„çŠ¶æ€ sTï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¾èµ–æ‰¹è¯„å®¶æœ¬èº«ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ‰¹è¯„å®¶ä¸å®é™…çš„ç´¯ç§¯å›æŠ¥è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹ä¼˜åŠ¿çš„ä¼°è®¡å°±æ˜¯çœŸæ­£çš„ä¼˜åŠ¿ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨å®ç°è½¦æ†é—®é¢˜æ—¶è¦åšçš„äº‹æƒ…ã€‚

## ä»·å€¼å‡½æ•°é¡¹

ç„¶è€Œï¼Œä¸ºäº†å¯¹ä¼˜åŠ¿æœ‰ä¸€ä¸ªå¥½çš„ä¼°è®¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¯ä»¥é¢„æµ‹ç»™å®šçŠ¶æ€çš„å€¼çš„è¯„è®ºå®¶ã€‚è¯¥æ¨¡å‹æ˜¯ä»¥ç›‘ç£çš„æ–¹å¼å­¦ä¹ çš„ï¼Œå…·æœ‰ç®€å•çš„ MSE æŸå¤±:

![](img/c99c51b76bff4edaad1cf444e252a799.png)

The loss function for our critic is simply the Mean-Squared-Error between its predicted expected reward and the observed cumulative reward. We apply a stop-gradient operator only to the observed reward in this case and optimize the critic.

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ›´æ–° criticï¼Œè¿™æ ·éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›è¶Šæ¥è¶Šç²¾ç¡®çš„çŠ¶æ€å€¼ã€‚

## ç†µé¡¹

æœ€åï¼Œæˆ‘ä»¬é¼“åŠ±æ¢ç´¢ï¼Œå¯¹æ”¿ç­–çš„äº§å‡ºåˆ†å¸ƒçš„ç†µæœ‰ä¸€ç‚¹å¥–åŠ±ã€‚æˆ‘ä»¬è€ƒè™‘æ ‡å‡†ç†µ:

![](img/d72cf63e4034c628f33a3bd4630c298b.png)

Entropy formula for the output distribution given by the policy model.

# å±¥è¡Œ

ä¸è¦æ‹…å¿ƒè¿™ä¸ªç†è®ºæ˜¯å¦ä»ç„¶æœ‰ç‚¹å¯ç–‘ã€‚è¿™ä¸ªå®ç°æœ‰æœ›è®©ä¸€åˆ‡å˜å¾—æ¸…æ™°ã€‚

## ç‹¬ç«‹äº PPO çš„ä»£ç 

è®©æˆ‘ä»¬ä»è¿›å£å¼€å§‹:

```
from argparse import ArgumentParser

import gym
import numpy as np
import wandb

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import LinearLR
from torch.distributions.categorical import Categorical

import pytorch_lightning as pl
```

PPO çš„é‡è¦è¶…å‚æ•°æ˜¯*æ•°é‡çš„æ¼”å‘˜*ã€*è§†é‡*ã€*Îµ*ã€æ¯ä¸ªä¼˜åŒ–é˜¶æ®µçš„*æ¬¡æ•°*ã€å­¦ä¹ ç‡*ã€æŠ˜ç°å› å­*Î³*ä»¥åŠå¯¹ä¸åŒæŸå¤±é¡¹ *c1* å’Œ *c2* è¿›è¡ŒåŠ æƒçš„å¸¸æ•°ã€‚æˆ‘ä»¬é€šè¿‡ç¨‹åºå‚æ•°æ”¶é›†è¿™äº›ä¿¡æ¯ã€‚*

```
def parse_args():
    """Pareser program arguments"""
    # Parser
    parser = ArgumentParser()

    # Program arguments (default for Atari games)
    parser.add_argument("--max_iterations", type=int, help="Number of iterations of training", default=100)
    parser.add_argument("--n_actors", type=int, help="Number of actors for each update", default=8)
    parser.add_argument("--horizon", type=int, help="Number of timestamps for each actor", default=128)
    parser.add_argument("--epsilon", type=float, help="Epsilon parameter", default=0.1)
    parser.add_argument("--n_epochs", type=int, help="Number of training epochs per iteration", default=3)
    parser.add_argument("--batch_size", type=int, help="Batch size", default=32 * 8)
    parser.add_argument("--lr", type=float, help="Learning rate", default=2.5 * 1e-4)
    parser.add_argument("--gamma", type=float, help="Discount factor gamma", default=0.99)
    parser.add_argument("--c1", type=float, help="Weight for the value function in the loss function", default=1)
    parser.add_argument("--c2", type=float, help="Weight for the entropy bonus in the loss function", default=0.01)
    parser.add_argument("--n_test_episodes", type=int, help="Number of episodes to render", default=5)
    parser.add_argument("--seed", type=int, help="Randomizing seed for the experiment", default=0)

    # Dictionary with program arguments
    return vars(parser.parse_args())
```

è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå‚æ•°è®¾ç½®å¦‚æœ¬æ–‡æ‰€è¿°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘ä»¬çš„ä»£ç åº”è¯¥åœ¨ GPU ä¸Šè¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„å®ç”¨å‡½æ•°ã€‚

```
def get_device():
    """Gets the device (GPU if any) and logs the type"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Found GPU device: {torch.cuda.get_device_name(device)}")
    else:
        device = torch.device("cpu")
        print("No GPU found: Running on CPU")
    return device
```

å½“æˆ‘ä»¬åº”ç”¨ RL æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰ä¸€ä¸ªç¼“å†²åŒºæ¥å­˜å‚¨å½“å‰æ¨¡å‹é‡åˆ°çš„çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ã€‚è¿™äº›ç”¨äºæ›´æ–°æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•ˆç”¨å‡½æ•°`run_timestamps`ï¼Œå®ƒå°†åœ¨ç»™å®šçš„ç¯å¢ƒä¸­è¿è¡Œç»™å®šçš„æ¨¡å‹ï¼Œè¿è¡Œå›ºå®šæ•°é‡çš„æ—¶é—´æˆ³(å¦‚æœå‰§é›†ç»“æŸï¼Œåˆ™é‡æ–°è®¾ç½®ç¯å¢ƒ)ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ªé€‰é¡¹`render=False`,ä»¥é˜²æˆ‘ä»¬åªæ˜¯æƒ³çœ‹çœ‹ç»è¿‡è®­ç»ƒçš„æ¨¡å‹è¡¨ç°å¦‚ä½•ã€‚

```
@torch.no_grad()
def run_timestamps(env, model, timestamps=128, render=False, device="cpu"):
    """Runs the given policy on the given environment for the given amount of timestamps.
     Returns a buffer with state action transitions and rewards."""
    buffer = []
    state = env.reset()[0]

    # Running timestamps and collecting state, actions, rewards and terminations
    for ts in range(timestamps):
        # Taking a step into the environment
        model_input = torch.from_numpy(state).unsqueeze(0).to(device).float()
        action, action_logits, value = model(model_input)
        new_state, reward, terminated, truncated, info = env.step(action.item())

        # Rendering / storing (s, a, r, t) in the buffer
        if render:
            env.render()
        else:
            buffer.append([model_input, action, action_logits, value, reward, terminated or truncated])

        # Updating current state
        state = new_state

        # Resetting environment if episode terminated or truncated
        if terminated or truncated:
            state = env.reset()[0]

    return buffer
```

è¯¥å‡½æ•°çš„è¾“å‡º(ä¸å‘ˆç°æ—¶)æ˜¯ä¸€ä¸ªç¼“å†²åŒºï¼ŒåŒ…å«æ¯ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€ã€é‡‡å–çš„åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡(logits)ã€ä¼°è®¡çš„è¯„è®ºå®¶å€¼ã€å¥–åŠ±å’Œæ‰€æä¾›ç­–ç•¥çš„ç»ˆæ­¢çŠ¶æ€ã€‚æ³¨æ„ï¼Œè¿™ä¸ªå‡½æ•°ä½¿ç”¨äº†è£…é¥°å™¨ **@torch.no_grad()ï¼Œ**ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å­˜å‚¨åœ¨ä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹ä¸­æ‰€é‡‡å–çš„åŠ¨ä½œçš„æ¸å˜ã€‚

## PPO çš„ä»£ç 

æ—¢ç„¶æˆ‘ä»¬å·²ç»è§£å†³äº†çç¢çš„äº‹æƒ…ï¼Œæ˜¯æ—¶å€™å®ç°æ ¸å¿ƒç®—æ³•äº†ã€‚

ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„**ä¸»**å‡½æ•°çœ‹èµ·æ¥åƒè¿™æ ·:

```
def main():
    # Parsing program arguments
    args = parse_args()
    print(args)

    # Setting seed
    pl.seed_everything(args["seed"])

    # Getting device
    device = get_device()

    # Creating environment (discrete action space)
    env_name = "CartPole-v1"
    env = gym.make(env_name)

    # Creating the model, training it and rendering the result
    # (We are missing this part ğŸ˜…)
    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)
    training_loop(env, model, args)
    model = load_best_model()
    testing_loop(env, model)
```

æˆ‘ä»¬å·²ç»æ‹¿åˆ°å¤§éƒ¨åˆ†äº†ã€‚æˆ‘ä»¬åªéœ€è¦å®šä¹‰ PPO æ¨¡å‹ã€è®­ç»ƒå’Œæµ‹è¯•åŠŸèƒ½ã€‚

PPO æ¨¡å‹çš„æ¶æ„ä¸æ˜¯è¿™é‡Œæœ‰è¶£çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬åªéœ€è¦ä¸¤ä¸ªåœ¨ç¯å¢ƒä¸­è¡¨æ¼”çš„æ¨¡ç‰¹(æ¼”å‘˜å’Œè¯„è®ºå®¶)ã€‚å½“ç„¶ï¼Œæ¨¡å‹æ¶æ„åœ¨æ›´å›°éš¾çš„ä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†æ˜¯æœ‰äº† cart poleï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿¡ä¸€äº› MLP å°†å®Œæˆè¿™é¡¹å·¥ä½œã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ¼”å‘˜å’Œè¯„è®ºå®¶æ¨¡å‹çš„ MyPPO ç±»ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šä¸¤è€…ä¹‹é—´çš„éƒ¨åˆ†æ¶æ„æ˜¯å…±äº«çš„ã€‚å½“å¯¹æŸäº›çŠ¶æ€è¿è¡Œ forward æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬è¿”å›å‚ä¸è€…çš„æŠ½æ ·åŠ¨ä½œã€æ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„ç›¸å¯¹æ¦‚ç‡(logits)ä»¥åŠè¯„è®ºå®¶å¯¹æ¯ä¸ªçŠ¶æ€çš„ä¼°è®¡å€¼ã€‚

```
class MyPPO(nn.Module):
    """Implementation of a PPO model. The same backbone is used to get actor and critic values."""

    def __init__(self, in_shape, n_actions, hidden_d=100, share_backbone=False):
        # Super constructor
        super(MyPPO, self).__init__()

        # Attributes
        self.in_shape = in_shape
        self.n_actions = n_actions
        self.hidden_d = hidden_d
        self.share_backbone = share_backbone

        # Shared backbone for policy and value functions
        in_dim = np.prod(in_shape)

        def to_features():
            return nn.Sequential(
                nn.Flatten(),
                nn.Linear(in_dim, hidden_d),
                nn.ReLU(),
                nn.Linear(hidden_d, hidden_d),
                nn.ReLU()
            )

        self.backbone = to_features() if self.share_backbone else nn.Identity()

        # State action function
        self.actor = nn.Sequential(
            nn.Identity() if self.share_backbone else to_features(),
            nn.Linear(hidden_d, hidden_d),
            nn.ReLU(),
            nn.Linear(hidden_d, n_actions),
            nn.Softmax(dim=-1)
        )

        # Value function
        self.critic = nn.Sequential(
            nn.Identity() if self.share_backbone else to_features(),
            nn.Linear(hidden_d, hidden_d),
            nn.ReLU(),
            nn.Linear(hidden_d, 1)
        )

    def forward(self, x):
        features = self.backbone(x)
        action = self.actor(features)
        value = self.critic(features)
        return Categorical(action).sample(), action, value
```

è¯·æ³¨æ„ï¼Œ`Categorical(action).sample()`åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰åŠ¨ä½œé€»è¾‘çš„åˆ†ç±»åˆ†å¸ƒï¼Œå¹¶ä»ä¸­æŠ½å–äº†ä¸€ä¸ªåŠ¨ä½œæ ·æœ¬(é’ˆå¯¹æ¯ä¸ªå·)ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨`training_loop`å‡½æ•°ä¸­å¤„ç†å®é™…çš„ç®—æ³•ã€‚æ­£å¦‚æˆ‘ä»¬ä»è®ºæ–‡ä¸­æ‰€çŸ¥ï¼Œå‡½æ•°çš„å®é™…ç­¾ååº”è¯¥æ˜¯è¿™æ ·çš„:

```
def training_loop(env, model, max_iterations, n_actors, horizon, gamma, 
epsilon, n_epochs, batch_size, lr, c1, c2, device, env_name=""):
  # TODO...
```

ä¸‹é¢æ˜¯æœ¬æ–‡ä¸­ä¸º PPO åŸ¹è®­ç¨‹åºæä¾›çš„ä¼ªä»£ç :

![](img/adf9438a5b6a7b4fab6124db0fcc8de8.png)

Pseudo code for PPO training provided in the [original paper](https://arxiv.org/abs/1707.06347).

PPO çš„ä¼ªä»£ç ç›¸å¯¹ç®€å•:æˆ‘ä»¬ç®€å•åœ°é€šè¿‡æˆ‘ä»¬çš„ç­–ç•¥æ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬(ç§°ä¸ºè¡ŒåŠ¨è€…)æ”¶é›†ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå¹¶ä½¿ç”¨å…ˆå‰å®šä¹‰çš„ç›®æ ‡æ¥ä¼˜åŒ–è¡ŒåŠ¨è€…å’Œæ‰¹è¯„è€…ç½‘ç»œã€‚

ç”±äºæˆ‘ä»¬éœ€è¦è¡¡é‡æˆ‘ä»¬å®é™…è·å¾—çš„ç´¯ç§¯å¥–åŠ±ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œç»™å®šä¸€ä¸ªç¼“å†²åŒºï¼Œç”¨ç´¯ç§¯å¥–åŠ±æ›¿æ¢æ¯ä¸ªæ—¶é—´æˆ³çš„å¥–åŠ±:

```
def compute_cumulative_rewards(buffer, gamma):
    """Given a buffer with states, policy action logits, rewards and terminations,
    computes the cumulative rewards for each timestamp and substitutes them into the buffer."""
    curr_rew = 0.

    # Traversing the buffer on the reverse direction
    for i in range(len(buffer) - 1, -1, -1):
        r, t = buffer[i][-2], buffer[i][-1]

        if t:
            curr_rew = 0
        else:
            curr_rew = r + gamma * curr_rew

        buffer[i][-2] = curr_rew

    # Getting the average reward before normalizing (for logging and checkpointing)
    avg_rew = np.mean([buffer[i][-2] for i in range(len(buffer))])

    # Normalizing cumulative rewards
    mean = np.mean([buffer[i][-2] for i in range(len(buffer))])
    std = np.std([buffer[i][-2] for i in range(len(buffer))]) + 1e-6
    for i in range(len(buffer)):
        buffer[i][-2] = (buffer[i][-2] - mean) / std

    return avg_rew
```

è¯·æ³¨æ„ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†ç´¯ç§¯å¥–åŠ±æ ‡å‡†åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æŠ€å·§ï¼Œå¯ä»¥ä½¿ä¼˜åŒ–é—®é¢˜æ›´å®¹æ˜“ï¼Œè®­ç»ƒæ›´æµç•…ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ªåŒ…å«çŠ¶æ€ã€é‡‡å–çš„è¡ŒåŠ¨ã€è¡ŒåŠ¨æ¦‚ç‡å’Œç´¯ç§¯å›æŠ¥çš„ç¼“å†²åŒºï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œç»™å®šä¸€ä¸ªç¼“å†²åŒºï¼Œè®¡ç®—æˆ‘ä»¬æœ€ç»ˆç›®æ ‡çš„ä¸‰ä¸ªæŸå¤±é¡¹:

```
def get_losses(model, batch, epsilon, annealing, device="cpu"):
    """Returns the three loss terms for a given model and a given batch and additional parameters"""
    # Getting old data
    n = len(batch)
    states = torch.cat([batch[i][0] for i in range(n)])
    actions = torch.cat([batch[i][1] for i in range(n)]).view(n, 1)
    logits = torch.cat([batch[i][2] for i in range(n)])
    values = torch.cat([batch[i][3] for i in range(n)])
    cumulative_rewards = torch.tensor([batch[i][-2] for i in range(n)]).view(-1, 1).float().to(device)

    # Computing predictions with the new model
    _, new_logits, new_values = model(states)

    # Loss on the state-action-function / actor (L_CLIP)
    advantages = cumulative_rewards - values
    margin = epsilon * annealing
    ratios = new_logits.gather(1, actions) / logits.gather(1, actions)

    l_clip = torch.mean(
        torch.min(
            torch.cat(
                (ratios * advantages,
                 torch.clip(ratios, 1 - margin, 1 + margin) * advantages),
                dim=1),
            dim=1
        ).values
    )

    # Loss on the value-function / critic (L_VF)
    l_vf = torch.mean((cumulative_rewards - new_values) ** 2)

    # Bonus for entropy of the actor
    entropy_bonus = torch.mean(torch.sum(-new_logits * (torch.log(new_logits + 1e-5)), dim=1))

    return l_clip, l_vf, entropy_bonus
```

æ³¨æ„ï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª*é€€ç«*å‚æ•°ï¼Œå®ƒè¢«è®¾ç½®ä¸º 1ï¼Œå¹¶åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å‘ 0 çº¿æ€§è¡°å‡ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œéšç€è®­ç»ƒçš„è¿›å±•ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ”¿ç­–æ”¹å˜è¶Šæ¥è¶Šå°‘ã€‚è¿˜è¦æ³¨æ„çš„æ˜¯ï¼Œ*ä¼˜åŠ¿*å˜é‡æ˜¯å¼ é‡ä¹‹é—´çš„ç®€å•å·®å¼‚ï¼Œæˆ‘ä»¬ä¸è·Ÿè¸ªæ¢¯åº¦ï¼Œä¸åƒ *new_logits* å’Œ *new_values* ã€‚

æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†ä¸ç¯å¢ƒäº¤äº’å’Œå­˜å‚¨ç¼“å†²åŒºã€è®¡ç®—(çœŸå®)ç´¯ç§¯å›æŠ¥å’Œè·å¾—æŸå¤±é¡¹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç¼–å†™æœ€ç»ˆçš„è®­ç»ƒå¾ªç¯äº†:

```
def training_loop(env, model, max_iterations, n_actors, horizon, gamma, epsilon, n_epochs, batch_size, lr,
                  c1, c2, device, env_name=""):
    """Train the model on the given environment using multiple actors acting up to n timestamps."""

    # Starting a new Weights & Biases run
    wandb.init(project="Papers Re-implementations",
               entity="peutlefaire",
               name=f"PPO - {env_name}",
               config={
                   "env": str(env),
                   "number of actors": n_actors,
                   "horizon": horizon,
                   "gamma": gamma,
                   "epsilon": epsilon,
                   "epochs": n_epochs,
                   "batch size": batch_size,
                   "learning rate": lr,
                   "c1": c1,
                   "c2": c2
               })

    # Training variables
    max_reward = float("-inf")
    optimizer = Adam(model.parameters(), lr=lr, maximize=True)
    scheduler = LinearLR(optimizer, 1, 0, max_iterations * n_epochs)
    anneals = np.linspace(1, 0, max_iterations)

    # Training loop
    for iteration in range(max_iterations):
        buffer = []
        annealing = anneals[iteration]

        # Collecting timestamps for all actors with the current policy
        for actor in range(1, n_actors + 1):
            buffer.extend(run_timestamps(env, model, horizon, False, device))

        # Computing cumulative rewards and shuffling the buffer
        avg_rew = compute_cumulative_rewards(buffer, gamma)
        np.random.shuffle(buffer)

        # Running optimization for a few epochs
        for epoch in range(n_epochs):
            for batch_idx in range(len(buffer) // batch_size):
                # Getting batch for this buffer
                start = batch_size * batch_idx
                end = start + batch_size if start + batch_size < len(buffer) else -1
                batch = buffer[start:end]

                # Zero-ing optimizers gradients
                optimizer.zero_grad()

                # Getting the losses
                l_clip, l_vf, entropy_bonus = get_losses(model, batch, epsilon, annealing, device)

                # Computing total loss and back-propagating it
                loss = l_clip - c1 * l_vf + c2 * entropy_bonus
                loss.backward()

                # Optimizing
                optimizer.step()
            scheduler.step()

        # Logging information to stdout
        curr_loss = loss.item()
        log = f"Iteration {iteration + 1} / {max_iterations}: " \
              f"Average Reward: {avg_rew:.2f}\t" \
              f"Loss: {curr_loss:.3f} " \
              f"(L_CLIP: {l_clip.item():.1f} | L_VF: {l_vf.item():.1f} | L_bonus: {entropy_bonus.item():.1f})"
        if avg_rew > max_reward:
            torch.save(model.state_dict(), MODEL_PATH)
            max_reward = avg_rew
            log += " --> Stored model with highest average reward"
        print(log)

        # Logging information to W&B
        wandb.log({
            "loss (total)": curr_loss,
            "loss (clip)": l_clip.item(),
            "loss (vf)": l_vf.item(),
            "loss (entropy bonus)": entropy_bonus.item(),
            "average reward": avg_rew
        })

    # Finishing W&B session
    wandb.finish()
```

æœ€åï¼Œä¸ºäº†æŸ¥çœ‹æœ€ç»ˆæ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„`testing_loop`å‡½æ•°:

```
def testing_loop(env, model, n_episodes, device):
    """Runs the learned policy on the environment for n episodes"""
    for _ in range(n_episodes):
        run_timestamps(env, model, timestamps=128, render=True, device=device)
```

æˆ‘ä»¬çš„ä¸»è¦è®¡åˆ’å¾ˆç®€å•:

```
def main():
    # Parsing program arguments
    args = parse_args()
    print(args)

    # Setting seed
    pl.seed_everything(args["seed"])

    # Getting device
    device = get_device()

    # Creating environment (discrete action space)
    env_name = "CartPole-v1"
    env = gym.make(env_name)

    # Creating the model (both actor and critic)
    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)

    # Training
    training_loop(env, model, args["max_iterations"], args["n_actors"], args["horizon"], args["gamma"], args["epsilon"],
                  args["n_epochs"], args["batch_size"], args["lr"], args["c1"], args["c2"], device, env_name)

    # Loading best model
    model = MyPPO(env.observation_space.shape, env.action_space.n).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))

    # Testing
    env = gym.make(env_name, render_mode="human")
    testing_loop(env, model, args["n_test_episodes"], device)
    env.close()
```

è€Œè¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†å®ç°ï¼å¦‚æœä½ èƒ½èµ°åˆ°è¿™ä¸€æ­¥ï¼Œæ­å–œä½ ã€‚æ‚¨ç°åœ¨çŸ¥é“å¦‚ä½•å®ç° PPO ç®—æ³•äº†ã€‚

# ç»“æœ

æƒé‡å’Œåå·®æ—¥å¿—è®©æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°è®°å½•çš„æŒ‡æ ‡å’ŒæŸå¤±ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®æŸå¤±å›¾åŠå…¶é¡¹ï¼Œä»¥åŠæ¯æ¬¡è¿­ä»£çš„å¹³å‡å›æŠ¥ã€‚

![](img/7de6c9dce20f3b6bf976714726b1d5e8.png)

Training losses through training iterations. The total loss (blue) is the sum of L_CLIP (orange) minus the L_VF (pink) plus a small constant times the entropy bonus (green)

![](img/637d6b0a331a89cd568e26eed25e471b.png)

Average reward through iterations. PPO quickly learns to maximize the cumulative reward.

ç”±äºæ¨è½¦æ†ç¯å¢ƒå¹¶ä¸æå…·æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¾ˆå¿«å°±æ‰¾åˆ°äº†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¤§çº¦ 20 æ­¥åæœ€å¤§åŒ–äº†å¹³å‡å¥–åŠ±ã€‚æ­¤å¤–ï¼Œç”±äºç¯å¢ƒåªæœ‰ä¸¤ç§å¯èƒ½çš„è¡Œä¸ºï¼Œç†µé¡¹åŸºæœ¬ä¸Šä¿æŒä¸å˜ã€‚

æœ€åï¼Œå¦‚æœæˆ‘ä»¬å°†æœ€ç»ˆç­–ç•¥ä»˜è¯¸å®æ–½ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼

![](img/5f14ae3564f14fb000ba7fa324183eff.png)

Trained PPO model balancing the cart pole

# ç»“è®º

PPO æ˜¯ä¸€ç§æœ€å…ˆè¿›çš„ RL ç­–ç•¥ä¼˜åŒ–(å› æ­¤æ˜¯æ— æ¨¡å‹çš„)ç®—æ³•ï¼Œå› æ­¤ï¼Œå®ƒå‡ ä¹å¯ä»¥åœ¨ä»»ä½•ç¯å¢ƒä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒPPO å…·æœ‰ç›¸å¯¹ç®€å•çš„ç›®æ ‡å‡½æ•°å’Œç›¸å¯¹è¾ƒå°‘çš„å¾…è°ƒèŠ‚çš„è¶…å‚æ•°ã€‚

å¦‚æœä½ æƒ³ç©è¿™ä¸ªåŠ¨æ€ç®—æ³•ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªé“¾æ¥æŒ‡å‘ [Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1u7YTohPaQFJPud8289pV6H65f9ZqSKWp?usp=sharing)ã€‚ä½ å¯ä»¥åœ¨ [GitHub åº“](https://github.com/BrianPulfer/PapersReimplementations)ä¸‹æ‰¾åˆ°æˆ‘ä¸ªäººæœ€æ–°çš„ PPO ç®—æ³•çš„é‡æ–°å®ç°(ä½œä¸º. py æ–‡ä»¶)ã€‚æ‚¨å¯ä»¥éšæ„ä½¿ç”¨å®ƒæˆ–å°†å…¶åº”ç”¨åˆ°æ‚¨è‡ªå·±çš„é¡¹ç›®ä¸­ï¼

å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•…äº‹ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼è¯·éšæ—¶è”ç³»æˆ‘ä»¬è¿›è¡Œè¿›ä¸€æ­¥çš„è®¨è®ºã€‚ç¥ä½ å’Œ PPOâœŒï¸ä¸€èµ·å¿«ä¹é»‘å®¢

# æ›´æ·±å…¥çš„é“¾æ¥:

èˆ’å°”æ›¼ç­‰äººã€‚è‰¾å°”ã€‚â– åŸæ–‡[](https://arxiv.org/abs/1707.06347)

*OpenAI çš„ [" *æ—‹è½¬ä¸Šå‡å¯¼è‡³æ·±åº¦ RL* "](https://spinningup.openai.com/en/latest/index.html)*

*è‰è²ç¿çš„[*ä¸€(é•¿)çª¥å¼ºåŒ–å­¦ä¹ *](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)*

*ç¿è‰è²çš„[*æ”¿ç­–æ¢¯åº¦ç®—æ³•*](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)*

*[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)*