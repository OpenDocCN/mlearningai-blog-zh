# è§†è§‰å˜å½¢é‡‘åˆšä»é›¶å¼€å§‹(PyTorch):ä¸€æ­¥ä¸€æ­¥çš„æŒ‡å—

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c?source=collection_archive---------0----------------------->

è§†è§‰å˜å½¢é‡‘åˆš(ViT)ï¼Œè‡ª Dosovitskiy ç­‰äººæ¨å‡ºä»¥æ¥ã€‚è‰¾å°”ã€‚[å‚è€ƒæ–‡çŒ®](https://arxiv.org/abs/2010.11929)2020 å¹´ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œé¦–å…ˆåœ¨å›¾åƒåˆ†ç±»æ–¹é¢è·å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œéšååœ¨å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

ç„¶è€Œï¼Œä¸å…¶ä»–æ¶æ„ä¸åŒï¼Œå®ƒä»¬æœ‰ç‚¹éš¾ä»¥ç†è§£ï¼Œå°¤å…¶æ˜¯å¦‚æœæ‚¨è¿˜ä¸ç†Ÿæ‚‰è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ä½¿ç”¨çš„è½¬æ¢å™¨æ¨¡å‹çš„è¯ã€‚

å¦‚æœæ‚¨å¯¹è®¡ç®—æœºè§†è§‰(CV)æ„Ÿå…´è¶£ï¼Œä½†ä»ç„¶ä¸ç†Ÿæ‚‰ ViT æ¨¡å‹ï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼æˆ‘ä¹Ÿæ˜¯ï¼

åœ¨è¿™æ®µç®€çŸ­çš„æ–‡å­—ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºæˆ‘æ˜¯å¦‚ä½•ä»å¤´å¼€å§‹å®ç°æˆ‘çš„ç¬¬ä¸€ä¸ª ViT çš„(ä½¿ç”¨ PyTorch)ï¼Œå¹¶ä¸”æˆ‘å°†æŒ‡å¯¼æ‚¨å®Œæˆä¸€äº›è°ƒè¯•ï¼Œè¿™å°†å¸®åŠ©æ‚¨æ›´å¥½åœ°å¯è§†åŒ– ViT ä¸­åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚

è™½ç„¶è¿™ç¯‡æ–‡ç« æ˜¯ä¸“é—¨é’ˆå¯¹ ViT çš„ï¼Œä½†æ˜¯ä½ åœ¨è¿™é‡Œä¼šå‘ç°ä¸€äº›æ¦‚å¿µï¼Œä¾‹å¦‚å¤šå¤´è‡ªæˆ‘æ³¨æ„(MSA)å—ï¼Œå®ƒä»¬åœ¨äººå·¥æ™ºèƒ½çš„å„ä¸ªå­é¢†åŸŸä¸­éƒ½å­˜åœ¨ï¼Œå¹¶ä¸”å½“å‰éå¸¸ç›¸å…³ï¼Œä¾‹å¦‚ CVã€NLP ç­‰

# å®šä¹‰ä»»åŠ¡

å› ä¸ºç›®æ ‡åªæ˜¯å­¦ä¹ æ›´å¤šå…³äº ViT æ¶æ„çš„çŸ¥è¯†ï¼Œæ‰€ä»¥æ˜æ™ºçš„åšæ³•æ˜¯é€‰æ‹©ä¸€ä¸ªç®€å•ä¸”ä¼—æ‰€å‘¨çŸ¥çš„ä»»åŠ¡å’Œæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä»»åŠ¡æ˜¯ç”±ä¼Ÿå¤§çš„ **LeCun ç­‰äººå¯¹æµè¡Œçš„ MNIST æ•°æ®é›†è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚è‰¾å°”ã€‚**[å‚è€ƒ](http://yann.lecun.com/exdb/mnist/)ã€‚

å¦‚æœä½ è¿˜ä¸çŸ¥é“ï¼ŒMNIST æ˜¯ä¸€ä¸ªæ‰‹å†™æ•°å­—([0â€“9])çš„æ•°æ®é›†ï¼Œå…¨éƒ¨åŒ…å«åœ¨ 28x28 äºŒè¿›åˆ¶åƒç´ å›¾åƒä¸­ã€‚è¿™ä¸ªä»»åŠ¡å¯¹äºä»Šå¤©çš„ç®—æ³•æ¥è¯´æ˜¯å¾®ä¸è¶³é“çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é¢„æœŸä¸€ä¸ªæ­£ç¡®çš„å®ç°å°†ä¼šæ‰§è¡Œå¾—å¾ˆå¥½ã€‚

è®©æˆ‘ä»¬ä»è¿›å£å¼€å§‹:

```
import numpy as np

from tqdm import tqdm, trange

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.nn import CrossEntropyLoss
from torch.utils.data import DataLoader

from torchvision.transforms import ToTensor
from torchvision.datasets.mnist import MNIST

np.random.seed(0)
torch.manual_seed(0)
```

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª**ä¸»å‡½æ•°**ï¼Œå®ƒå‡†å¤‡ MNIST æ•°æ®é›†ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸ºå…¶è®­ç»ƒ 5 ä¸ªæ—¶æœŸã€‚ä¹‹åï¼Œåœ¨æµ‹è¯•é›†ä¸Šæµ‹é‡æŸè€—å’Œç²¾åº¦ã€‚

```
def main():
    # Loading data
    transform = ToTensor()

    train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)
    test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)
    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)

    # Defining model and training options
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device: ", device, f"({torch.cuda.get_device_name(device)})" if torch.cuda.is_available() else "")
    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)
    N_EPOCHS = 5
    LR = 0.005

    # Training loop
    optimizer = Adam(model.parameters(), lr=LR)
    criterion = CrossEntropyLoss()
    for epoch in trange(N_EPOCHS, desc="Training"):
        train_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} in training", leave=False):
            x, y = batch
            x, y = x.to(device), y.to(device)
            y_hat = model(x)
            loss = criterion(y_hat, y)

            train_loss += loss.detach().cpu().item() / len(train_loader)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}")

    # Test loop
    with torch.no_grad():
        correct, total = 0, 0
        test_loss = 0.0
        for batch in tqdm(test_loader, desc="Testing"):
            x, y = batch
            x, y = x.to(device), y.to(device)
            y_hat = model(x)
            loss = criterion(y_hat, y)
            test_loss += loss.detach().cpu().item() / len(test_loader)

            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()
            total += len(x)
        print(f"Test loss: {test_loss:.2f}")
        print(f"Test accuracy: {correct / total * 100:.2f}%")
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ¨¡æ¿ï¼Œä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä»¬å¯ä»¥åªå…³æ³¨æ¨¡å‹(ViT)ï¼Œå®ƒå¿…é¡»ç”¨å½¢çŠ¶( **N** x 1 x 28 x 28)å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚

è®©æˆ‘ä»¬ä»å®šä¹‰ä¸€ä¸ªç©ºçš„ *nn å¼€å§‹ã€‚æ¨¡å—*ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é€æ­¥å¡«å……è¿™ä¸ªç±»ã€‚

```
class MyViT(nn.Module):
  def __init__(self):
    # Super constructor
    super(MyViT, self).__init__()

  def forward(self, images):
    pass
```

# å‰è¿›ä¼ çƒ

ç”±äº Pytorch å’Œå¤§å¤šæ•° DL æ¡†æ¶éƒ½æä¾›äº†*è‡ªåŠ¨ç­¾åçš„*è®¡ç®—ï¼Œæˆ‘ä»¬åªå…³å¿ƒå®ç° ViT æ¨¡å‹çš„å‘å‰ä¼ é€’ã€‚å› ä¸ºæˆ‘ä»¬å·²ç»å®šä¹‰äº†æ¨¡å‹çš„ä¼˜åŒ–å™¨ï¼Œæ‰€ä»¥æ¡†æ¶å°†è´Ÿè´£åå‘ä¼ æ’­æ¢¯åº¦å’Œè®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚

å½“å®ç°ä¸€ä¸ªæ–°çš„æ¨¡å‹æ—¶ï¼Œæˆ‘å–œæ¬¢åœ¨æŸä¸ªæ ‡ç­¾ä¸Šä¿å­˜ä¸€ä¸ªæ¶æ„çš„å›¾ç‰‡ã€‚è¿™æ˜¯æˆ‘ä»¬ä» **Bazi et å¾—åˆ°çš„ ViT çš„å‚è€ƒå›¾ç‰‡ã€‚é“** (2021)[ [å‚è€ƒ](https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification):

![](img/3f9f0e3248e7dd9154e0f4b4ee5a8827.png)

The architecture of the ViT with specific details on the transformer encoder and the MSA block. Keep this picture in mind. Picture from [Bazi et. al.](https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification)

é€šè¿‡è¯¥å›¾ï¼Œæˆ‘ä»¬çœ‹åˆ°è¾“å…¥å›¾åƒ(a)è¢«â€œåˆ‡å‰²â€æˆå¤§å°ç›¸ç­‰çš„å­å›¾åƒã€‚

æ¯ä¸ªè¿™æ ·çš„å­å›¾åƒç»å†çº¿æ€§åµŒå…¥ã€‚ä»é‚£æ—¶èµ·ï¼Œæ¯ä¸ªå­å›¾åƒåªæ˜¯ä¸€ä¸ªä¸€ç»´å‘é‡ã€‚

ç„¶åå°†ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¿™äº›å‘é‡(è®°å·)ä¸­ã€‚ä½ç½®åµŒå…¥å…è®¸ç½‘ç»œçŸ¥é“æ¯ä¸ªå­å›¾åƒæœ€åˆåœ¨å›¾åƒä¸­çš„ä½ç½®ã€‚æ²¡æœ‰è¿™äº›ä¿¡æ¯ï¼Œç½‘ç»œå°†æ— æ³•çŸ¥é“æ¯ä¸ªè¿™æ ·çš„å›¾åƒå°†è¢«æ”¾ç½®åœ¨å“ªé‡Œï¼Œä»è€Œå¯¼è‡´æ½œåœ¨çš„é”™è¯¯é¢„æµ‹ï¼

ç„¶åï¼Œè¿™äº›ä»¤ç‰Œä¸ç‰¹æ®Šçš„åˆ†ç±»ä»¤ç‰Œä¸€èµ·è¢«ä¼ é€’åˆ°å˜æ¢å™¨ç¼–ç å™¨å—ï¼Œæ¯ä¸ªç¼–ç å™¨å—åŒ…æ‹¬:å±‚æ ‡å‡†åŒ–(LN ),éšåæ˜¯å¤šå¤´è‡ªå…³æ³¨(MSA)å’Œæ®‹å·®è¿æ¥ã€‚ç„¶åæ˜¯ç¬¬äºŒä¸ª LNï¼Œä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨(MLP)ï¼Œå†æ¬¡æ˜¯ä¸€ä¸ªå‰©ä½™è¿æ¥ã€‚è¿™äº›ç§¯æœ¨æ˜¯èƒŒé èƒŒè¿æ¥çš„ã€‚

æœ€åï¼Œåˆ†ç±» MLP å—ä»…ç”¨äºç‰¹æ®Šåˆ†ç±»æ ‡è®°ä¸Šçš„æœ€ç»ˆåˆ†ç±»ï¼Œè¯¥ç‰¹æ®Šåˆ†ç±»æ ‡è®°åœ¨è¯¥è¿‡ç¨‹ç»“æŸæ—¶å…·æœ‰å…³äºå›¾åƒçš„å…¨å±€ä¿¡æ¯ã€‚

è®©æˆ‘ä»¬æŒ‰ç…§**çš„ 6 ä¸ªä¸»è¦æ­¥éª¤æ¥æ„å»º ViTã€‚**

## æ­¥éª¤ 1:ä¿®è¡¥å’Œçº¿æ€§æ˜ å°„

transformer ç¼–ç å™¨çš„å¼€å‘è€ƒè™‘åˆ°äº†åºåˆ—æ•°æ®ï¼Œä¾‹å¦‚è‹±è¯­å¥å­ã€‚ç„¶è€Œï¼Œå›¾åƒä¸æ˜¯åºåˆ—ã€‚å®ƒåªæ˜¯ï¼Œå—¯â€¦ä¸€ä¸ªå›¾åƒâ€¦é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å¯¹ä¸€ä¸ªå›¾åƒâ€œæ’åºâ€ï¼Ÿæˆ‘ä»¬æŠŠå®ƒåˆ†è§£æˆå¤šä¸ªå­å›¾ï¼ŒæŠŠæ¯ä¸ªå­å›¾æ˜ å°„æˆä¸€ä¸ªå‘é‡ï¼

æˆ‘ä»¬é€šè¿‡ç®€å•åœ°å°†å¤§å°ä¸º(Nï¼ŒCï¼ŒHï¼ŒW)(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä¸º(Nï¼Œ1ï¼Œ28ï¼Œ28))çš„è¾“å…¥æ•´å½¢ä¸ºå¤§å°ä¸º(Nï¼Œ#é¢ç‰‡ï¼Œé¢ç‰‡ç»´æ•°)çš„è¾“å…¥æ¥å®ç°ï¼Œå…¶ä¸­é¢ç‰‡çš„ç»´æ•°è¢«ç›¸åº”åœ°è°ƒæ•´ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ª(1ï¼Œ28ï¼Œ28)åˆ†æˆ **7x7 ä¸ªå°å—**(å› æ­¤ï¼Œæ¯ä¸ªå¤§å°ä¸º 4x4)ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å°†ä»å•ä¸ªå›¾åƒä¸­è·å¾— 7Ã—7 = 49 ä¸ªå­å›¾åƒã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å°†è¾“å…¥(Nï¼Œ1ï¼Œ28ï¼Œ28)æ•´å½¢ä¸º:

> *(Nï¼ŒPxPï¼ŒHxC/P x WxC/P) = (Nï¼Œ7x7ï¼Œ4x4) = (Nï¼Œ49ï¼Œ16)*

è¯·æ³¨æ„ï¼Œè™½ç„¶æ¯ä¸ªå°å—éƒ½æ˜¯å¤§å°ä¸º 1x4x4 çš„å›¾ç‰‡ï¼Œä½†æˆ‘ä»¬å°†å…¶å±•å¹³ä¸º 16 ç»´å‘é‡ã€‚æ­¤å¤–ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå•ä¸€çš„é¢œè‰²é€šé“ã€‚å¦‚æœæˆ‘ä»¬æœ‰å¤šä¸ªé¢œè‰²é€šé“ï¼Œè¿™äº›é€šé“ä¹Ÿä¼šè¢«å±•å¹³åˆ°çŸ¢é‡ä¸­ã€‚

![](img/8e9405e6a9975e637f6e8f4bc9d56f7c.png)

Raffiguration of how an image is split into patches. The 1x28x28 image is split into 49 (7x7) patches, each of size 16 (4x4x1)

æˆ‘ä»¬ä¿®æ”¹æˆ‘ä»¬çš„ **MyViT** ç±»ï¼Œåªå®ç°ä¿®è¡¥ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä»å¤´å¼€å§‹æ“ä½œçš„æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ç§æ‰§è¡Œæ“ä½œçš„ä½æ•ˆæ–¹å¼ï¼Œä½†æ˜¯å¯¹äºå­¦ä¹ æ ¸å¿ƒæ¦‚å¿µæ¥è¯´ï¼Œä»£ç æ˜¯ç›´è§‚çš„ã€‚

```
def patchify(images, n_patches):
    n, c, h, w = images.shape

    assert h == w, "Patchify method is implemented for square images only"

    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)
    patch_size = h // n_patches

    for idx, image in enumerate(images):
        for i in range(n_patches):
            for j in range(n_patches):
                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]
                patches[idx, i * n_patches + j] = patch.flatten()
    return patches
```

```
class MyViT(nn.Module):
  def __init__(self, chw=(1, 28, 28), n_patches=7):
    # Super constructor
    super(MyViT, self).__init__()

    # Attributes
    self.chw = chw # (C, H, W)
    self.n_patches = n_patches

    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"

  def forward(self, images):
    patches = patchify(images, self.n_patches)
    return patches
```

ç±»æ„é€ å‡½æ•°ç°åœ¨è®©ç±»çŸ¥é“æˆ‘ä»¬çš„è¾“å…¥å›¾åƒçš„å¤§å°(é€šé“æ•°ã€é«˜åº¦å’Œå®½åº¦)ã€‚æ³¨æ„ï¼Œåœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œ *n_patches* å˜é‡æ˜¯æˆ‘ä»¬å°†åœ¨å®½åº¦å’Œé«˜åº¦ä¸Šæ‰¾åˆ°çš„é¢ç‰‡æ•°(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯ 7ï¼Œå› ä¸ºæˆ‘ä»¬å°†å›¾åƒåˆ†æˆ 7x7 ä¸ªé¢ç‰‡)ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„ä¸»ç¨‹åºæ¥æµ‹è¯•æˆ‘ä»¬çš„ç±»çš„åŠŸèƒ½:

```
if __name__ == '__main__':
  # Current model
  model = MyViT(
    chw=(1, 28, 28),
    n_patches=7
  )

  x = torch.randn(7, 1, 28, 28) # Dummy images
  print(model(x).shape) # torch.Size([7, 49, 16])
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†å±•å¹³çš„é¢ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡çº¿æ€§æ˜ å°„æ¥æ˜ å°„å®ƒä»¬ã€‚è™½ç„¶æ¯ä¸ªå°å—æ˜¯ 4Ã—4 = 16 ç»´å‘é‡ï¼Œä½†æ˜¯çº¿æ€§æ˜ å°„å¯ä»¥æ˜ å°„åˆ°ä»»ä½•ä»»æ„å¤§å°çš„å‘é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç»™æˆ‘ä»¬çš„ç±»æ„é€ å‡½æ•°æ·»åŠ äº†ä¸€ä¸ªå‚æ•°ï¼Œç§°ä¸º *hidden_d* è¡¨ç¤ºâ€œéšè—ç»´åº¦â€ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ 8 ä¸ªéšè—ç»´åº¦ï¼Œä½†æ˜¯åŸåˆ™ä¸Šï¼Œä»»ä½•æ•°å­—éƒ½å¯ä»¥æ”¾åœ¨è¿™é‡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æŠŠæ¯ä¸ª 16 ç»´çš„é¢ç‰‡æ˜ å°„åˆ°ä¸€ä¸ª 8 ç»´çš„é¢ç‰‡ã€‚

æˆ‘ä»¬ç®€å•åœ°åˆ›å»ºä¸€ä¸ª*ç¥ç»ç½‘ç»œã€‚çº¿æ€§*å±‚ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ­£å‘å‡½æ•°ä¸­è°ƒç”¨å®ƒã€‚

```
class MyViT(nn.Module):
  def __init__(self, chw=(1, 28, 28), n_patches=7):
    # Super constructor
    super(MyViT, self).__init__()

    # Attributes
    self.chw = chw # (C, H, W)
    self.n_patches = n_patches

    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)

    # 1) Linear mapper
    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])
    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)

  def forward(self, images):
    patches = patchify(images, self.n_patches)
    tokens = self.linear_mapper(patches)
    return tokens
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬é€šè¿‡(16ï¼Œ8)çº¿æ€§æ˜ å°„å™¨(æˆ–çŸ©é˜µ)è¿è¡Œ(Nï¼Œ49ï¼Œ16)å¼ é‡ã€‚çº¿æ€§è¿ç®—åªå‘ç”Ÿåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šã€‚

## æ­¥éª¤ 2:æ·»åŠ åˆ†ç±»ä»¤ç‰Œ

å¦‚æœæ‚¨ä»”ç»†è§‚å¯Ÿæ¶æ„å›¾ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°è¿˜æœ‰ä¸€ä¸ªâ€œ *v_class* â€ä»¤ç‰Œè¢«ä¼ é€’ç»™äº† Transformer ç¼–ç å™¨ã€‚è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

ç®€å•åœ°è¯´ï¼Œè¿™æ˜¯æˆ‘ä»¬æ·»åŠ åˆ°æ¨¡å‹ä¸­çš„ä¸€ä¸ªç‰¹æ®Šä»¤ç‰Œï¼Œå®ƒçš„ä½œç”¨æ˜¯æ•è·å…³äºå…¶ä»–ä»¤ç‰Œçš„ä¿¡æ¯ã€‚è¿™å°†åœ¨ MSA å—ä¸­å‘ç”Ÿ(ç¨å)ã€‚å½“å…³äºæ‰€æœ‰å…¶ä»–æ ‡è®°çš„ä¿¡æ¯å°†å‡ºç°åœ¨è¿™é‡Œæ—¶ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿä»…ä½¿ç”¨è¿™ä¸ªç‰¹æ®Šçš„æ ‡è®°æ¥å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚ç‰¹æ®Šä»¤ç‰Œçš„åˆå§‹å€¼(é¦ˆé€ç»™å˜æ¢å™¨ç¼–ç å™¨çš„é‚£ä¸ª)æ˜¯éœ€è¦å­¦ä¹ çš„æ¨¡å‹çš„å‚æ•°ã€‚

è¿™æ˜¯ä¸€ä¸ªå¾ˆé…·çš„å˜å½¢é‡‘åˆšæ¦‚å¿µï¼å¦‚æœæˆ‘ä»¬æƒ³åšå¦ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸ºå¦ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡æ·»åŠ å¦ä¸€ä¸ªç‰¹æ®Šçš„ä»¤ç‰Œ(ä¾‹å¦‚ï¼Œå°†ä¸€ä¸ªæ•°å­—åˆ†ç±»ä¸ºé«˜äº 5 æˆ–ä½äº 5)å’Œä¸€ä¸ªæ¥å—è¿™ä¸ªæ–°ä»¤ç‰Œä½œä¸ºè¾“å…¥çš„åˆ†ç±»å™¨ã€‚å¾ˆèªæ˜ï¼Œå¯¹å§ï¼Ÿ

æˆ‘ä»¬ç°åœ¨å¯ä»¥å‘æˆ‘ä»¬çš„æ¨¡å‹æ·»åŠ ä¸€ä¸ªå‚æ•°ï¼Œå¹¶å°†æˆ‘ä»¬çš„(Nï¼Œ49ï¼Œ8)ä»¤ç‰Œå¼ é‡è½¬æ¢ä¸º(Nï¼Œ50ï¼Œ8)å¼ é‡(æˆ‘ä»¬å‘æ¯ä¸ªåºåˆ—æ·»åŠ ç‰¹æ®Šä»¤ç‰Œ)ã€‚

```
class MyViT(nn.Module):
  def __init__(self, chw=(1, 28, 28), n_patches=7):
    # Super constructor
    super(MyViT, self).__init__()

    # Attributes
    self.chw = chw # (C, H, W)
    self.n_patches = n_patches

    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)

    # 1) Linear mapper
    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])
    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)

    # 2) Learnable classifiation token
    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))

  def forward(self, images):
    patches = patchify(images, self.n_patches)
    tokens = self.linear_mapper(patches)

    # Adding classification token to the tokens
    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])
    return tokens
```

è¯·æ³¨æ„ï¼Œåˆ†ç±»æ ‡è®°ä½œä¸ºæ¯ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚å½“æˆ‘ä»¬éšåæ£€ç´¢åˆ†ç±»ä»¤ç‰Œä»¥æä¾›ç»™æœ€ç»ˆçš„ MLP æ—¶ï¼Œè®°ä½è¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚

## æ­¥éª¤ 3:ä½ç½®ç¼–ç 

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä½ç½®ç¼–ç å…è®¸æ¨¡å‹ç†è§£æ¯ä¸ªè¡¥ç‰‡åœ¨åŸå§‹å›¾åƒä¸­çš„ä½ç½®ã€‚è™½ç„¶ç†è®ºä¸Šæœ‰å¯èƒ½å­¦ä¹ è¿™æ ·çš„ä½ç½®åµŒå…¥ï¼Œä½†æ˜¯ Vaswani ç­‰äººä»¥å‰çš„å·¥ä½œã€‚è‰¾å°”ã€‚ã€[å‚è€ƒæ–‡çŒ®](https://arxiv.org/abs/1706.03762)å»ºè®®æˆ‘ä»¬å¯ä»¥åªæŠŠæ­£å¼¦å’Œä½™å¼¦æ³¢ç›¸åŠ ã€‚

å…·ä½“æ¥è¯´ï¼Œä½ç½®ç¼–ç å°†ä½é¢‘å€¼æ·»åŠ åˆ°ç¬¬ä¸€ç»´åº¦ï¼Œå°†é«˜é¢‘å€¼æ·»åŠ åˆ°åä¸€ç»´åº¦ã€‚

åœ¨æ¯ä¸ªåºåˆ—ä¸­ï¼Œå¯¹äºä»¤ç‰Œ *i* ,æˆ‘ä»¬å‘å…¶*ç¬¬ j ä¸ª*åæ ‡æ·»åŠ ä»¥ä¸‹å€¼:

![](img/ef81de628b5880f367482e1d7bdb2ac5.png)

Value to be added to the i-th tensor in its j-th coordinate. [Image source](https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp).

è¿™ç§ä½ç½®åµŒå…¥æ˜¯åºåˆ—ä¸­å…ƒç´ æ•°é‡å’Œæ¯ä¸ªå…ƒç´ ç»´æ•°çš„å‡½æ•°ã€‚å› æ­¤ï¼Œå®ƒæ€»æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡æˆ–â€œçŸ©å½¢â€ã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œåœ¨ç»™å®šè®°å·çš„æ•°é‡å’Œæ¯ä¸ªè®°å·çš„ç»´æ•°çš„æƒ…å†µä¸‹ï¼Œè¾“å‡ºä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸ªåæ ‡(Iï¼Œj)æ˜¯è¦æ·»åŠ åˆ°ç»´åº¦ j ä¸­çš„è®°å· I çš„å€¼ã€‚

```
def get_positional_embeddings(sequence_length, d):
    result = torch.ones(sequence_length, d)
    for i in range(sequence_length):
        for j in range(d):
            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))
    return result

if __name__ == "__main__":
  import matplotlib.pyplot as plt

  plt.imshow(get_positional_embeddings(100, 300), cmap="hot", interpolation="nearest")
  plt.show()
```

![](img/8d109c34551aec340e5a852727793e58.png)

Heatmap of Positional embeddings for one hundred 300-dimensional samples. Samples are on the y-axis, whereas the dimensions are on the x-axis. Darker regions show higher values.

ä»æˆ‘ä»¬ç»˜åˆ¶çš„çƒ­å›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰çš„â€œæ°´å¹³çº¿â€éƒ½äº’ä¸ç›¸åŒï¼Œå› æ­¤å¯ä»¥åŒºåˆ†æ ·å“ã€‚

åœ¨çº¿æ€§æ˜ å°„å’Œæ·»åŠ ç±»æ ‡è®°ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†è¿™ç§ä½ç½®ç¼–ç æ·»åŠ åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚

```
class MyViT(nn.Module):
  def __init__(self, chw=(1, 28, 28), n_patches=7):
    # Super constructor
    super(MyViT, self).__init__()

    # Attributes
    self.chw = chw # (C, H, W)
    self.n_patches = n_patches

    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"
    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)

    # 1) Linear mapper
    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])
    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)

    # 2) Learnable classifiation token
    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))

    # 3) Positional embedding
    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))
    self.pos_embed.requires_grad = False

  def forward(self, images):
    patches = patchify(images, self.n_patches)
    tokens = self.linear_mapper(patches)

    # Adding classification token to the tokens
    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])

    # Adding positional embedding
    pos_embed = self.pos_embed.repeat(n, 1, 1)
    out = tokens + pos_embed
    return out
```

æˆ‘ä»¬å°†ä½ç½®åµŒå…¥å®šä¹‰ä¸ºæ¨¡å‹çš„ä¸€ä¸ªå‚æ•°(æˆ‘ä»¬ä¸ä¼šé€šè¿‡å°†å…¶ requires_grad è®¾ç½®ä¸º False æ¥æ›´æ–°å®ƒ)ã€‚æ³¨æ„ï¼Œåœ¨å‰å‘æ–¹æ³•ä¸­ï¼Œç”±äºè®°å·çš„å¤§å°ä¸º(Nï¼Œ50ï¼Œ8)ï¼Œæˆ‘ä»¬å¿…é¡»é‡å¤ N æ¬¡(50ï¼Œ8)ä½ç½®ç¼–ç çŸ©é˜µã€‚

## æ­¥éª¤ 4:ç¼–ç å™¨æ¨¡å—(ç¬¬ 1/2 éƒ¨åˆ†)

è¿™å¯èƒ½æ˜¯æœ€éš¾çš„ä¸€æ­¥ã€‚ç¼–ç å™¨æ¨¡å—å°†æˆ‘ä»¬çš„å½“å‰å¼ é‡[Nï¼ŒSï¼ŒD]ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºç›¸åŒç»´æ•°çš„å¼ é‡ã€‚

ç¼–ç å™¨æ¨¡å—çš„ç¬¬ä¸€éƒ¨åˆ†å¯¹æˆ‘ä»¬çš„ä»¤ç‰Œåº”ç”¨å±‚æ ‡å‡†åŒ–ï¼Œç„¶åæ˜¯å¤šå¤´è‡ªæˆ‘å…³æ³¨ï¼Œæœ€åæ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥ã€‚

**å›¾å±‚å½’ä¸€åŒ–**

å›¾å±‚å½’ä¸€åŒ–æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æ¨¡å—ï¼Œå®ƒåœ¨ç»™å®šä¸€ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå‡å»å…¶å¹³å‡å€¼ï¼Œç„¶åé™¤ä»¥æ ‡å‡†å·®ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬é€šå¸¸å¯¹(Nï¼Œd)è¾“å…¥åº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œå…¶ä¸­ d æ˜¯ç»´åº¦ã€‚å¹¸è¿çš„æ˜¯ï¼Œå›¾å±‚è§„èŒƒåŒ–æ¨¡å—ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å¤šä¸ªç»´åº¦ï¼Œè¯·çœ‹:

![](img/59614d0b8f462de843a99ddcf2d06b55.png)

nn.LayerNorm can be applied in multiple dimensions. We can normalize fifty 8-dimensional vectors, but we can also normalize sixteen by fifty 8-dimensional vectors.

å›¾å±‚è§„èŒƒåŒ–ä»…é€‚ç”¨äºæœ€åä¸€ä¸ªå°ºå¯¸ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿æˆ‘ä»¬çš„æ¯ä¸ª 50Ã—8 çŸ©é˜µ(ä»£è¡¨å•ä¸ªåºåˆ—)çš„å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚åœ¨æˆ‘ä»¬é€šè¿‡ LN è¿è¡Œæˆ‘ä»¬çš„(Nï¼Œ50ï¼Œ8)å¼ é‡ä¹‹åï¼Œæˆ‘ä»¬ä»ç„¶å¾—åˆ°ç›¸åŒçš„ç»´æ•°ã€‚

**å¤šå¤´è‡ªæˆ‘å…³æ³¨**

æˆ‘ä»¬ç°åœ¨éœ€è¦å®ç°æ¶æ„å›¾çš„å­å›¾ *c* ã€‚é‚£é‡Œå‘ç”Ÿäº†ä»€ä¹ˆäº‹ï¼Ÿ

ç®€è€Œè¨€ä¹‹:å¯¹äºå•ä¸ªå›¾åƒï¼Œæˆ‘ä»¬å¸Œæœ›åŸºäºä¸å…¶ä»–å›¾åƒçš„ç›¸ä¼¼æ€§åº¦é‡æ¥æ›´æ–°æ¯ä¸ªå›¾åƒå—ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªé¢ç‰‡(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ç°åœ¨æ˜¯ä¸€ä¸ª 8 ç»´å‘é‡)çº¿æ€§æ˜ å°„åˆ° 3 ä¸ªä¸åŒçš„å‘é‡æ¥åšåˆ°è¿™ä¸€ç‚¹: **q** ã€ **k** å’Œ **v** (æŸ¥è¯¢ã€é”®ã€å€¼)ã€‚

ç„¶åï¼Œå¯¹äºå•ä¸ªè¡¥ä¸ï¼Œæˆ‘ä»¬å°†è®¡ç®—å…¶ **q** å‘é‡ä¸æ‰€æœ‰ **k** å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼Œé™¤ä»¥è¿™äº›å‘é‡çš„ç»´æ•°çš„å¹³æ–¹æ ¹(sqrt(8))ï¼Œsoftmax è¿™äº›æ‰€è°“çš„*æ³¨æ„åŠ›çº¿ç´¢*ï¼Œæœ€åå°†æ¯ä¸ªæ³¨æ„åŠ›çº¿ç´¢ä¹˜ä»¥ä¸ä¸åŒçš„ **k** å‘é‡ç›¸å…³è”çš„ **v** å‘é‡ï¼Œå¹¶æ±‚å’Œã€‚

ä»¥è¿™ç§æ–¹å¼ï¼Œæ¯ä¸ªå°å—é‡‡ç”¨åŸºäºå…¶ä¸å…¶ä»–å°å—çš„ç›¸ä¼¼æ€§(åœ¨çº¿æ€§æ˜ å°„åˆ° **q** ã€ **k** å’Œ **v** ä¹‹å)çš„æ–°å€¼ã€‚ç„¶è€Œï¼Œè¿™æ•´ä¸ªè¿‡ç¨‹æ˜¯åœ¨æˆ‘ä»¬å½“å‰ 8 ç»´é¢ç‰‡çš„ **H** å­å‘é‡ä¸Šæ‰§è¡Œ **H** æ¬¡ï¼Œå…¶ä¸­ **H** æ˜¯**å¤´çš„æ•°é‡ã€‚**å¦‚æœä½ å¯¹æ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸ç†Ÿæ‚‰ï¼Œæˆ‘å»ºè®®ä½ é˜…è¯»[è¿™ä¸ªç”±](https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/) [Yasuto Tamura](https://data-science-blog.com/blog/author/yasuto/) å†™çš„å¥½å¸–å­ã€‚

ä¸€æ—¦è·å¾—æ‰€æœ‰ç»“æœï¼Œå®ƒä»¬å°±è¢«è¿æ¥åœ¨ä¸€èµ·ã€‚æœ€åï¼Œç»“æœé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚(ä¸ºäº†æ›´å¥½çš„æµ‹é‡)ã€‚

æ³¨æ„åŠ›èƒŒåçš„ç›´è§‚æƒ³æ³•æ˜¯ï¼Œå®ƒå…è®¸å¯¹è¾“å…¥ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä½¿â€œ0â€æˆä¸ºé›¶çš„ä¸æ˜¯å•ä¸ªåƒç´ å€¼ï¼Œè€Œæ˜¯å®ƒä»¬å¦‚ä½•ç›¸äº’å…³è”ã€‚

ç”±äºæ‰§è¡Œäº†ç›¸å½“å¤šçš„è®¡ç®—ï¼Œå› æ­¤æœ‰å¿…è¦ä¸º MSA åˆ›å»ºä¸€ä¸ªæ–°ç±»:

```
class MyMSA(nn.Module):
    def __init__(self, d, n_heads=2):
        super(MyMSA, self).__init__()
        self.d = d
        self.n_heads = n_heads

        assert d % n_heads == 0, f"Can't divide dimension {d} into {n_heads} heads"

        d_head = int(d / n_heads)
        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])
        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])
        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])
        self.d_head = d_head
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, sequences):
        # Sequences has shape (N, seq_length, token_dim)
        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)
        # And come back to    (N, seq_length, item_dim)  (through concatenation)
        result = []
        for sequence in sequences:
            seq_result = []
            for head in range(self.n_heads):
                q_mapping = self.q_mappings[head]
                k_mapping = self.k_mappings[head]
                v_mapping = self.v_mappings[head]

                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]
                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)

                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))
                seq_result.append(attention @ v)
            result.append(torch.hstack(seq_result))
        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])
```

æ³¨æ„ï¼Œå¯¹äºæ¯ä¸ªå¤´éƒ¨ï¼Œæˆ‘ä»¬åˆ›å»ºä¸åŒçš„ Qã€K å’Œ V æ˜ å°„å‡½æ•°(åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯å¤§å°ä¸º 4x4 çš„æ–¹é˜µ)ã€‚

ç”±äºæˆ‘ä»¬çš„è¾“å…¥å°†æ˜¯å¤§å°ä¸º(Nï¼Œ50ï¼Œ8)çš„åºåˆ—ï¼Œå¹¶ä¸”æˆ‘ä»¬åªä½¿ç”¨ 2 ä¸ªå¤´ï¼Œæˆ‘ä»¬å°†åœ¨æŸä¸ªç‚¹æœ‰ä¸€ä¸ª(Nï¼Œ50ï¼Œ2ï¼Œ4)å¼ é‡ï¼Œä½¿ç”¨ä¸€ä¸ª *nnã€‚çº¿æ€§(4ï¼Œ4)* æ¨¡ä¸Šï¼Œç„¶åå›æ¥ï¼Œä¸²è”åï¼Œå¾—åˆ°ä¸€ä¸ª(Nï¼Œ50ï¼Œ8)å¼ é‡ã€‚

è¿˜è¦æ³¨æ„ï¼Œä½¿ç”¨å¾ªç¯å¹¶ä¸æ˜¯è®¡ç®—å¤šå¤´è‡ªæˆ‘å…³æ³¨çš„æœ€æœ‰æ•ˆæ–¹å¼ï¼Œä½†å®ƒä½¿ä»£ç æ›´æ¸…æ™°ï¼Œä¾¿äºå­¦ä¹ ã€‚

**æ®‹ç•™è¿æ¥**

å‰©ä½™è¿æ¥åªæ˜¯å°†åŸå§‹è¾“å…¥æ·»åŠ åˆ°ä¸€äº›è®¡ç®—çš„ç»“æœä¸­ã€‚è¿™ç›´è§‚åœ°å…è®¸ç½‘ç»œå˜å¾—æ›´å¼ºå¤§ï¼ŒåŒæ—¶è¿˜ä¿ç•™äº†æ¨¡å‹å¯ä»¥è¿‘ä¼¼çš„ä¸€ç»„å¯èƒ½çš„å‡½æ•°ã€‚

æˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªå‰©ä½™è¿æ¥ï¼Œå®ƒå°†æŠŠæˆ‘ä»¬åŸæ¥çš„(Nï¼Œ50ï¼Œ8)å¼ é‡æ·»åŠ åˆ° LN å’Œ MSA ä¹‹åè·å¾—çš„(Nï¼Œ50ï¼Œ8)å¼ é‡ä¸­ã€‚æ˜¯æ—¶å€™åˆ›å»º transformer encoder block ç±»äº†ï¼Œå®ƒå°†æ˜¯ MyViT ç±»çš„ä¸€ä¸ªç»„ä»¶:

```
class MyViTBlock(nn.Module):
    def __init__(self, hidden_d, n_heads, mlp_ratio=4):
        super(MyViTBlock, self).__init__()
        self.hidden_d = hidden_d
        self.n_heads = n_heads

        self.norm1 = nn.LayerNorm(hidden_d)
        self.mhsa = MyMSA(hidden_d, n_heads)

    def forward(self, x):
        out = x + self.mhsa(self.norm1(x))
        return out
```

å”·ï¼Œé‚£æ˜¯ç›¸å½“å¤šçš„å·¥ä½œï¼ä½†æˆ‘ä¿è¯è¿™æ˜¯æœ€éš¾çš„éƒ¨åˆ†ã€‚ä»ç°åœ¨å¼€å§‹ï¼Œä¸€åˆ‡éƒ½åœ¨èµ°ä¸‹å¡è·¯ã€‚

æœ‰äº†è¿™ç§è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œç±»æ ‡è®°(N ä¸ªåºåˆ—ä¸­æ¯ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°)ç°åœ¨æœ‰äº†å…³äºæ‰€æœ‰å…¶ä»–æ ‡è®°çš„ä¿¡æ¯ï¼

## æ­¥éª¤ 5:ç¼–ç å™¨æ¨¡å—(ç¬¬ 2/2 éƒ¨åˆ†)

æ‰€æœ‰ç•™ç»™å˜æ¢å™¨ç¼–ç å™¨çš„åªæ˜¯æˆ‘ä»¬å·²ç»æ‹¥æœ‰çš„å’Œæˆ‘ä»¬é€šè¿‡å¦ä¸€ä¸ª LN å’Œ MLP ä¼ é€’å½“å‰å¼ é‡åå¾—åˆ°çš„ä¹‹é—´çš„ç®€å•å‰©ä½™è¿æ¥ã€‚MLP ç”±ä¸¤ä¸ªå›¾å±‚ç»„æˆï¼Œå…¶ä¸­éšè—å›¾å±‚é€šå¸¸æ˜¯å…¶å››å€å¤§(è¿™æ˜¯ä¸€ä¸ªå‚æ•°)

```
class MyViTBlock(nn.Module):
    def __init__(self, hidden_d, n_heads, mlp_ratio=4):
        super(MyViTBlock, self).__init__()
        self.hidden_d = hidden_d
        self.n_heads = n_heads

        self.norm1 = nn.LayerNorm(hidden_d)
        self.mhsa = MyMSA(hidden_d, n_heads)
        self.norm2 = nn.LayerNorm(hidden_d)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_d, mlp_ratio * hidden_d),
            nn.GELU(),
            nn.Linear(mlp_ratio * hidden_d, hidden_d)
        )

    def forward(self, x):
        out = x + self.mhsa(self.norm1(x))
        out = out + self.mlp(self.norm2(out))
        return out
```

æˆ‘ä»¬ç¡®å®å¯ä»¥çœ‹åˆ°ï¼Œç¼–ç å™¨æ¨¡å—è¾“å‡ºç›¸åŒç»´æ•°çš„å¼ é‡:

```
if __name__ == '__main__':
  model = MyVitBlock(hidden_d=8, n_heads=2)

  x = torch.randn(7, 50, 8)  # Dummy sequences
  print(model(x).shape)      # torch.Size([7, 50, 8])
```

æ—¢ç„¶ç¼–ç å™¨æ¨¡å—å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬åªéœ€è¦å°†å®ƒæ’å…¥åˆ°æˆ‘ä»¬æ›´å¤§çš„ ViT æ¨¡å‹ä¸­ï¼Œå®ƒè´Ÿè´£åœ¨å˜å‹å™¨æ¨¡å—ä¹‹å‰è¿›è¡Œä¿®è¡¥ï¼Œå¹¶åœ¨ä¹‹åè¿›è¡Œåˆ†ç±»ã€‚

æˆ‘ä»¬å¯ä»¥æœ‰ä»»æ„æ•°é‡çš„å˜å‹å™¨æ¨¡å—ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†åªä½¿ç”¨ 2ã€‚æˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€ä¸ªå‚æ•°ï¼Œä»¥äº†è§£æ¯ä¸ªç¼–ç å™¨æ¨¡å—å°†ä½¿ç”¨å¤šå°‘å¤´ã€‚

```
class MyViT(nn.Module):
    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):
        # Super constructor
        super(MyViT, self).__init__()

        # Attributes
        self.chw = chw # ( C , H , W )
        self.n_patches = n_patches
        self.n_blocks = n_blocks
        self.n_heads = n_heads
        self.hidden_d = hidden_d

        # Input and patches sizes
        assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
        assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"
        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)

        # 1) Linear mapper
        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])
        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)

        # 2) Learnable classification token
        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))

        # 3) Positional embedding
        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)

        # 4) Transformer encoder blocks
        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])

    def forward(self, images):
        # Dividing images into patches
        n, c, h, w = images.shape
        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)

        # Running linear layer tokenization
        # Map the vector corresponding to each patch to the hidden size dimension
        tokens = self.linear_mapper(patches)

        # Adding classification token to the tokens
        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)

        # Adding positional embedding
        out = tokens + self.positional_embeddings.repeat(n, 1, 1)

        # Transformer Blocks
        for block in self.blocks:
            out = block(out)

        return out
```

åŒæ ·ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡æˆ‘ä»¬çš„æ¨¡å‹è¿è¡Œä¸€ä¸ªéšæœºçš„(7ï¼Œ1ï¼Œ28ï¼Œ28)å¼ é‡ï¼Œæˆ‘ä»¬ä»ç„¶å¾—åˆ°ä¸€ä¸ª(7ï¼Œ50ï¼Œ8)å¼ é‡ã€‚

## æ­¥éª¤ 6:MLP åˆ†ç±»

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä»æˆ‘ä»¬çš„ N ä¸ªåºåˆ—ä¸­æå–åˆ†ç±»æ ‡è®°(ç¬¬ä¸€ä¸ªæ ‡è®°)ï¼Œå¹¶ä½¿ç”¨æ¯ä¸ªæ ‡è®°è·å¾— N ä¸ªåˆ†ç±»ã€‚

ç”±äºæˆ‘ä»¬å†³å®šæ¯ä¸ªè®°å·æ˜¯ä¸€ä¸ª 8 ç»´å‘é‡ï¼Œå¹¶ä¸”ç”±äºæˆ‘ä»¬æœ‰ 10 ä¸ªå¯èƒ½çš„æ•°å­—ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆ†ç±» MLP å®ç°ä¸ºä¸€ä¸ªç®€å•çš„ 8Ã—10 çŸ©é˜µï¼Œç”¨ SoftMax å‡½æ•°æ¿€æ´»ã€‚

```
class MyViT(nn.Module):
    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):
        # Super constructor
        super(MyViT, self).__init__()

        # Attributes
        self.chw = chw # ( C , H , W )
        self.n_patches = n_patches
        self.n_blocks = n_blocks
        self.n_heads = n_heads
        self.hidden_d = hidden_d

        # Input and patches sizes
        assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"
        assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"
        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)

        # 1) Linear mapper
        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])
        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)

        # 2) Learnable classification token
        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))

        # 3) Positional embedding
        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)

        # 4) Transformer encoder blocks
        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])

        # 5) Classification MLPk
        self.mlp = nn.Sequential(
            nn.Linear(self.hidden_d, out_d),
            nn.Softmax(dim=-1)
        )

    def forward(self, images):
        # Dividing images into patches
        n, c, h, w = images.shape
        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)

        # Running linear layer tokenization
        # Map the vector corresponding to each patch to the hidden size dimension
        tokens = self.linear_mapper(patches)

        # Adding classification token to the tokens
        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)

        # Adding positional embedding
        out = tokens + self.positional_embeddings.repeat(n, 1, 1)

        # Transformer Blocks
        for block in self.blocks:
            out = block(out)

        # Getting the classification token only
        out = out[:, 0]

        return self.mlp(out) # Map to output dimension, output category distribution
```

æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºç°åœ¨æ˜¯ä¸€ä¸ª(Nï¼Œ10)å¼ é‡ã€‚ä¸‡å²ï¼Œæˆ‘ä»¬å®Œæˆäº†ï¼

# ç»“æœ

æˆ‘ä»¬ä¿®æ”¹äº†ä¸»ç¨‹åºä¸­å”¯ä¸€ä¸€è¡Œä¹‹å‰æ²¡æœ‰å®šä¹‰çš„ä»£ç ã€‚

```
model = MyVit((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)
```

æˆ‘ä»¬ç°åœ¨åªéœ€è¦è¿è¡Œè®­ç»ƒå’Œæµ‹è¯•å¾ªç¯ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¦‚ä½•ã€‚å¦‚æœæ‚¨å·²ç»æ‰‹åŠ¨è®¾ç½®äº† torch seed(è®¾ç½®ä¸º 0)ï¼Œæ‚¨åº”è¯¥æ‰“å°å‡ºä»¥ä¸‹å†…å®¹:

![](img/02110057745bed8e0eb3f5e64fe70b8e.png)

Training losses, test loss, and test accuracy obtained.

å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬ç°åœ¨å·²ç»ä»å¤´å¼€å§‹åˆ›å»ºäº†ä¸€ä¸ª ViTã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä»…ä»… 5 ä¸ªå†å…ƒå’Œå¾ˆå°‘çš„å‚æ•°ä¸‹å°±è¾¾åˆ°äº† 80%çš„å‡†ç¡®ç‡ã€‚

ä½ å¯ä»¥åœ¨ä¸‹é¢çš„[é“¾æ¥](https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/vit_torch.py)æ‰¾åˆ°å®Œæ•´çš„è„šæœ¬ã€‚è€ƒè™‘é¼“æŒğŸ‘å¦‚æœä½ è§‰å¾—è¿™ä¸ªæ•…äº‹æœ‰ç”¨ï¼Œè®©æˆ‘çŸ¥é“ä½ æ˜¯å¦è®¤ä¸ºæœ‰ä»€ä¹ˆä¸æ¸…æ¥šçš„åœ°æ–¹ï¼

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)