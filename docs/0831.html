<html>
<head>
<title>PLACE Method: Human Body Generator on a 3D Scenes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">放置方法:三维场景上的人体生成器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/place-method-human-body-generator-on-a-3d-scenes-f43215ca6bad?source=collection_archive---------2-----------------------#2021-07-30">https://medium.com/mlearning-ai/place-method-human-body-generator-on-a-3d-scenes-f43215ca6bad?source=collection_archive---------2-----------------------#2021-07-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a168" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来自<a class="ae jc" href="https://is.mpg.de/" rel="noopener ugc nofollow" target="_blank">马普智能系统研究所</a>和<a class="ae jc" href="https://ethz.ch/en.html" rel="noopener ugc nofollow" target="_blank">苏黎世联邦理工学院</a>的研究人员提出了一种非常优雅的方法，在给定的3D场景中生成逼真的人类。让我们讨论一下为什么它是相关的，并深入细节。</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><h1 id="2c5c" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">当前危机</h1><p id="1281" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">工业界和学术界正在推动现实世界环境的数字化。当苹果(和其他智能手机/平板电脑制造商)推出带有深度传感器和激光雷达的新设备时(点击这里查看<a class="ae jc" href="https://www.youtube.com/watch?v=fS3J4V_BgP0" rel="noopener ugc nofollow" target="_blank">演示</a>，来自世界各地的研究人员正在努力寻找使用新型数据的新方法。这些数据主要由普通起居室、浴室、办公室和食堂的3D扫描组成，细节化程度相当高。有大量的任务跟踪与这些数据一起工作，例如在occuse<a class="ae jc" href="https://arxiv.org/abs/2003.06537" rel="noopener ugc nofollow" target="_blank">论文</a>中完成的家具的语义和实例分割，或者在我的<a class="ae jc" href="http://adase.group/" rel="noopener ugc nofollow" target="_blank">科学小组</a>基于部件的RGB-D扫描理解的<a class="ae jc" href="https://arxiv.org/abs/2012.02094" rel="noopener ugc nofollow" target="_blank">论文</a>中完成的家具的部件完成。这些任务(以及许多其他任务)对于创造智能助手是至关重要的，智能助手可以帮助痴呆症患者、老年人或残疾人在他们自己的房子里充分生活。<br/>我们知道真实世界的室内环境可能包含人，但现有的大部分数据集都没有。PLACE论文的作者(<a class="ae jc" href="https://arxiv.org/abs/2008.05570" rel="noopener ugc nofollow" target="_blank">PLACE:Proximity Learning of Articulation and Contact in 3D</a>)，他们正在解决现有3D虚拟现实环境(如<a class="ae jc" href="https://www.youtube.com/watch?v=NCt4_I7lgfs" rel="noopener ugc nofollow" target="_blank"> Habitat </a>)的这一重大限制。他们提供了一种在3D环境中生成人体网格的方法</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/fcc830fba93964e69f8283fefa109e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4P_Io3SvKQ4HEHo6.jpg"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Renders from the paper (<a class="ae jc" href="https://github.com/sanweiliti/PLACE" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="d72f" class="jk jl hh bd jm jn ld jp jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh bi translated">体系结构</h1><p id="4ec5" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">让我们仔细看看这个方法的组成部分</p><h2 id="0f35" class="li jl hh bd jm lj lk ll jq lm ln lo ju ip lp lq jy it lr ls kc ix lt lu kg lv bi translated">基点集</h2><p id="d0d4" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">将ICCV 2019的第<a class="ae jc" href="https://arxiv.org/abs/1908.09186" rel="noopener ugc nofollow" target="_blank">条</a>中提出的基点的想法进行组合。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lw"><img src="../Images/34a97bed362a9531b44c63715c69bde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o53yA0GcZFfmCi6QMEOuag.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx"><a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="48cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们用到空间中固定基点的距离对原始身体点云进行编码，然后使用简单的MLP，使用编码从<a class="ae jc" href="https://smpl-x.is.tue.mpg.de/" rel="noopener ugc nofollow" target="_blank"> SMPL-X </a>推断全身高分辨率网格。固定的基点意味着对于每个<em class="lx">不同的</em>输入身体点云，我们将计算到空间中<em class="lx">相同的</em>点的距离。在原始论文中，固定点的数量是1024，而在位置论文中，这个数字上升到10k。正如您在幻灯片中看到的，在不了解人所在场景的情况下，创建这样的编码是可能的。但是作者走得更远，给了我们创建人-场景交互表示的方法。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ly"><img src="../Images/bba9f2ff4b74a607b5204da4c2c8c895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXYzaXgvLsAWVgpIJDvgtg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx"><strong class="bd jm">Scene BPS</strong> and <strong class="bd jm">BPS features (</strong><a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" rel="noopener ugc nofollow" target="_blank"><strong class="bd jm">source</strong></a><strong class="bd jm">)</strong></figcaption></figure><p id="ba64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">给定一个场景网格(3D环境)和一个人体网格，可能<a class="ae jc" href="https://www.arxiv-vanity.com/papers/2008.05570/" rel="noopener ugc nofollow" target="_blank">在场景顶点</a>上有基点。由此衍生出两个定义<strong class="ig hi">场景</strong> <strong class="ig hi"> BPS </strong> —场景上设置的固定基点和<strong class="ig hi"> BPS特征</strong> —基点到人体网格顶点的距离。在这个<a class="ae jc" href="https://youtu.be/zJ1hbtMHGrw?t=110" rel="noopener ugc nofollow" target="_blank">视频</a>中，你可以看到环境中固定的一组点和不同的身体实际上有不同的一组距离，因为不同的姿势有不同的BPS特征。</p><h2 id="175f" class="li jl hh bd jm lj lk ll jq lm ln lo ju ip lp lq jy it lr ls kc ix lt lu kg lv bi translated">基于距离的人体网格生成器</h2><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lz"><img src="../Images/3646e491460608f33f8e27fdd6b3b276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwNwaZkrk3VwAfDvu4Xx3Q.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Human mesh generator architecture (<a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="cde5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了创造一个能制造出似是而非的人体的发生器，作者提出了以下途径。给定场景和人类网格，他们计算BPS特征(距离)并训练<strong class="ig hi">变分自动编码器</strong> ( <strong class="ig hi"> VAE </strong>)来重建这样的距离。从这一点，重建的身体特征去MLP回归到完整的身体顶点。MLP输出两个东西:所有身体顶点的全局3D平移和中间重建顶点，它们的总和给出重建的身体顶点。本文所有训练使用的初始数据源是<a class="ae jc" href="https://prox.is.tue.mpg.de/" rel="noopener ugc nofollow" target="_blank"> PROX </a>数据集。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ma"><img src="../Images/9f7b01876b0f03425e80caa0027b7b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpprBay7LF974jEyJMzZxA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Generation pipeline (<a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="f7a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在测试过程中，可以从高斯分布中抽取随机向量，并将其传递给VAE解码器。这给了我们一个人体网格生成管道。</p><p id="52aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人们可能会注意到，这个模型只对场景网格上的<em class="lx">单个场景</em>和<em class="lx">单个</em> <em class="lx">固定点集</em>有效。在上面提到的对VAE的简短介绍之后，我们将讨论作者们打算如何克服这个问题</p><h2 id="b4e9" class="li jl hh bd jm lj lk ll jq lm ln lo ju ip lp lq jy it lr ls kc ix lt lu kg lv bi translated">变分自动编码器简介</h2><p id="95b1" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">这项工作使用VAEs来重建人类网格，正如我们稍后将看到的环境表现。为了达成共识，让我们快速讨论一下VAE是如何工作的。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mb"><img src="../Images/0b89117bc912d5f3d0a0e4dd991e986b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mwlv3WPwcq_R7FwC.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Work example of autoencoder on one of the <a class="ae jc" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">MNIST</a> images</figcaption></figure><p id="f2f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标准自动编码器由编码器和解码器两部分组成。编码器将三维网格等高维数据压缩成低维表示，通常压缩成大小为<em class="lx"> N. </em>的向量，解码器则相反，将这个向量扩展成原始数据。这两个是使用<em class="lx">重建损失联合训练的神经网络。</em>这种损失改善了编码器丢弃不必要信息的能力和解码器创建接近源数据的输出的能力。</p><p id="7e13" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以使用自动编码器作为类似于训练数据的新数据样本的生成器。但是普通自动编码器的解码器只能从训练过程中出现的潜在向量中产生有效数据。为了克服这些限制，我们可以使用<strong class="ig hi">变型自动编码器</strong>或<strong class="ig hi"> VAEs </strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mc"><img src="../Images/cff22c4109be4eb3c5976be9d5804127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/0*sRdVAIveoJ3_zHMX.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">VAE encodes parameters of distribution instead of samples</figcaption></figure><p id="3614" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看一下来自<a class="ae jc" href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf" rel="noopener" target="_blank">这个</a>帖子的图片。在网络中间，我们看到<strong class="ig hi"> μ </strong>、<strong class="ig hi"> σ </strong>和<strong class="ig hi">样本</strong>。两个模型之间的主要区别在于，每个输入的普通AE产生一个长度为30 <em class="lx">的潜在表示，例如</em>，而VAE创建两个正态分布参数向量(mu和sigma)。输入片段的确切潜在表示将是在<strong class="ig hi">样本</strong>层获得的30个随机变量的实现。</p><p id="7b5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这使得我们的编码偏差稳健，因为在这种情况下，解码器被教导不仅为一个向量预测相同的输出，而且为潜在空间中的一组接近点(分布在<strong class="ig hi"> μ </strong>周围，偏差为<strong class="ig hi"> σ </strong>)预测相同的输出。这整件事就是用两个损失训练出来的:<em class="lx">重建</em>和<em class="lx"> KL发散。</em>在这种情况下，后者用于强制所有参数<strong class="ig hi"> μ，σ </strong>偏离标准正态偏差更小(指参数<strong class="ig hi"> 0，I </strong>)。你可以在这里了解更多关于这些损失如何加权的信息<a class="ae jc" href="https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder?newreg=fe83a46ff69d4f63953ea3b8fc346d23" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="12c7" class="li jl hh bd jm lj lk ll jq lm ln lo ju ip lp lq jy it lr ls kc ix lt lu kg lv bi translated">基于距离的两阶段人体场景编码</h2><p id="07c2" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">回到人类生成的任务，这是不可能不同意，它是非常可取的，能够生成人类不是在唯一的场景，而是在任何给定的场景。为了克服这样的问题，afters提出使用使用基点和VAE的相同技术来编码场景上的人体和场景本身。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es md"><img src="../Images/4c922a7e640cbb80e3443b44ea2cddae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPcOxrNIF-50s5p3zDy7hw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Distance based encoding of the scene (in blue, upper most) and of the human body in (in orange, bottom) (<a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw&amp;t=345s" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="014e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基点固定在三维空间中立方体笼子的墙壁和天花板上。相同的一组基点用于来自PROX数据集的输入场景的任何裁剪。这种方法有助于了解人体网格周围的环境和身体特征本身。人类自动编码器变成了一个<strong class="ig hi">条件VAE </strong>，因为它受场景潜在向量的制约。在实践中，这种调节可以通过在将来自两个网络的潜在向量传递到人类发生器的解码器之前将其级联来实现。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es me"><img src="../Images/b4a80c46ccfaa9979beeb64492fb7fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVCUmOmrNk-SvztepvXUfA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">The rest of the network stays the same (<a class="ae jc" href="https://www.youtube.com/watch?v=zJ1hbtMHGrw&amp;t=345s" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="ee23" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在提出的两个VAEs之上，仍然有MLP回归高分辨率人体网格。根据作者的说法，这种方法导致了好的结果，但是当他们增加一个条件时，结果会变得更好。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mf"><img src="../Images/1c6af9e851a1a625b2d2d10e6c14d7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hf1pnAN9r5_ybIajmyhTg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">VAE for absolute (x,y,z) locations of the scene (in green, upper most) <a class="ae jc" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">(source)</a></figcaption></figure><p id="2163" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">研究人员提出不仅用距离，而且用网格表面的绝对<em class="lx"> (x，y，z)坐标</em>对场景进行编码。负责这种编码的其他VAE及其潜在向量被馈送到流水线末端的回归MLP</p><h2 id="949f" class="li jl hh bd jm lj lk ll jq lm ln lo ju ip lp lq jy it lr ls kc ix lt lu kg lv bi translated">基于交互的优化</h2><p id="3782" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">全位置方法的最后一步是基于交互的优化。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mg"><img src="../Images/01d3241c191aa0981c0514b1e36ebe3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uaFZ8dkmVOKVY7TgPEEl_w.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Top row: results before optimization, bottom row: after optimization <a class="ae jc" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">(source)</a></figcaption></figure><p id="eab5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它们在身体形状的<strong class="ig hi">θ</strong>参数上引入了复杂的损失。这种损失一方面有助于克服相互渗透(如右数第三列)，另一方面迫使网络产生更自然的身体姿势(见第一列)。</p><h1 id="8379" class="jk jl hh bd jm jn ld jp jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh bi translated">结果评估</h1><p id="fff9" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">为了评估结果的质量，作者让评估者参与进来。他们编写了一个工具，用户可以在其中比较两个不同的人体网格，并决定哪个模型是<em class="lx"/></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mh"><img src="../Images/20b7d2b3b1a2930b4ac473e98fb9d369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Er16a62NgazsFMKgYTEjhg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx">Comparison tool <a class="ae jc" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">(source)</a></figcaption></figure><p id="7152" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这导致了以下结果</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mi"><img src="../Images/c61baa1e719fbfc762ba89b5090c3af1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*jE30dcKSd3gjjVl3h0k-Bw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx"><a class="ae jc" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="2c62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在评估阶段，大约70%的用户认为在两个数据集上，提出的模型比以前的模型好于<a class="ae jc" href="https://arxiv.org/abs/1912.02923" rel="noopener ugc nofollow" target="_blank"> PSI模型</a>(来自相同作者)。非常令人鼓舞和有趣的是，48.5%的用户认为人造人比地面真相本身更可信。您可能还会注意到，结果仅适用于PROX，并且模型是在PROX上训练的。</p><h1 id="635a" class="jk jl hh bd jm jn ld jp jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh bi translated">给我看看代码！</h1><p id="0463" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">本文展示了三维人体形状生成领域的重大成果。这些结果可能对研究室内3D模型的研究人员和开发人员有用，也许我们会在不久的将来看到虚拟辅助技术或计算机游戏中使用类似的方法，谁知道呢。但是今天你可以复制这个结果，因为作者提供了他们在github上实验的代码库。</p><h1 id="77b3" class="jk jl hh bd jm jn ld jp jq jr le jt ju jv lf jx jy jz lg kb kc kd lh kf kg kh bi translated">参考</h1><p id="fc5e" class="pw-post-body-paragraph ie if hh ig b ih ki ij ik il kj in io ip kk ir is it kl iv iw ix km iz ja jb ha bi translated">[1]地点:在3D环境中近距离学习发音和接触【https://arxiv.org/abs/2012.12877 T2】</p><p id="5e8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]直观理解变分自动编码器【https://medium.com/r? T4】URL = https % 3A % 2F % 2f towards data science . com % 2f直观-理解-变分-自动编码器-1bfe67eb5daf </p></div></div>    
</body>
</html>