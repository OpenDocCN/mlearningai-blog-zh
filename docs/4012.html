<html>
<head>
<title>RNN for sequence-to-sequence classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于序列间分类的RNN</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/rnn-for-sequence-to-sequence-classification-5d41a2b6d8fd?source=collection_archive---------1-----------------------#2022-11-24">https://medium.com/mlearning-ai/rnn-for-sequence-to-sequence-classification-5d41a2b6d8fd?source=collection_archive---------1-----------------------#2022-11-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="fd7a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="1663" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">为了理解一系列信息，我们需要在理解之前数据的基础上理解每一个输入的数据。我们不能抛弃一切，开始处理另一个序列。这是RNNs背后的主要思想。</p><p id="7de2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">传统的神经网络不能做到这一点，这似乎是一个重大的限制。假设你想对电影中每一点发生的事件进行分类。目前还不清楚传统的神经网络如何利用电影中之前的事件来通知后续的事件。</p><p id="c5a7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">递归神经网络解决了这个问题。它们是带有环路的网络，允许信息持续存在。rnn最初是在论文“通过错误传播学习内部表征”(1984)中介绍的，它们彻底改变了我们进行语音识别、语言建模、翻译、图像字幕等的方式。在本文中，我们将介绍如何对简单的RNNs、GRUs、LSTM和双向LSTM进行建模，以预测序列数据的心脏病(二元分类)。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/209f7d3af09c3d060acdd64890ae4dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhS1qKclhuXF9BGihnzx1w.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Architecture</figcaption></figure><h1 id="e3f5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">数据准备和导入</h1><p id="7bcf" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们的数据集已经标准化，如<strong class="je hi">图1 </strong>所示。它总共有140列和1621行。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kv"><img src="../Images/f99258b59c2c1214449d9560388ef757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ncUdWQoOi8wNpErg"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 1</figcaption></figure><p id="fef2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下面的代码块显示了我们如何将数据存储在独立字典中的数据帧中。</p><pre class="kg kh ki kj fd kw kx ky bn kz la bi"><span id="3956" class="lb if hh kx b be lc ld l le lf">transformed_df = {"X_train": [], "Y_train": [], "X_test": [], "Y_test": []}<br/>transformed_df["X_train"] = dataset_train.drop("1", axis=1)<br/>transformed_df["Y_train"] = dataset_train["1"]<br/><br/>transformed_df["X_test"] = dataset_test.drop("1", axis=1)<br/>transformed_df["Y_test"] = dataset_test["1"]</span></pre><p id="7649" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">虽然我们还没有对我们的数据进行任何规范化或标准化(因为数据已经标准化)，但对于递归神经网络来说，这样做很重要。为什么有必要对我们的数据进行标准化或规范化？这是因为特性的数量级会影响某些模型的性能。例如，如果一个要素的数量级等于1000，而另一个要素的数量级等于10，则一些模型可能“认为”一个特征比另一个特征更重要。数量级没有告诉我们任何关于预测能力的信息，因此它是有偏差的。我们可以通过改变变量使它们具有相同的数量级来消除这种偏差。</p><p id="b0c3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">标准化和规范化是这些转换中的两种，它们将每个变量转换为0-1区间(将每个变量转换为0均值和单位方差变量)。理论上，标准化优于规范化，因为它不会导致变量的概率分布在异常值出现时缩小</p><h1 id="fb45" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">构建模型:简单的RNN</h1><p id="12ec" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">rnn首先在论文“通过错误传播学习内部表征”(1984)中介绍。这个模型中的第一层将是一个简单的RNN层。然后是一个脱落层，后面是另外两组相同的架构。我们在接近末端的地方有一个平坦而致密的层。模型如下所示。</p><pre class="kg kh ki kj fd kw kx ky bn kz la bi"><span id="38e4" class="lb if hh kx b be lc ld l le lf">model = keras.Sequential([<br/> keras.layers.SimpleRNN(units=50, return_sequences= True),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.SimpleRNN(units=50, return_sequences= True),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.SimpleRNN(units=50, return_sequences= True),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Flatten(),<br/> keras.layers.Dense(10, activation='relu'),<br/> keras.layers.Dense(1, activation='sigmoid')<br/>])<br/><br/>model.compile(loss='binary_crossentropy',<br/> optimizer='adam',<br/> metrics=['accuracy'])<br/>model.build(X_train.shape)<br/><br/>callback = keras.callbacks.EarlyStopping(monitor="val_loss",<br/> min_delta=0,<br/> patience=3,<br/> verbose=1,<br/> mode="auto",<br/> baseline=None,<br/> restore_best_weights=True)<br/>history_gru = model.fit(x=X_train,y=Y_train, validation_split=0.20, batch_size=32, epochs=30, verbose=2, callbacks=[callback])</span></pre><p id="fc73" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">第一层的SimpleRNN层的输入形状是32，输出形状也是32。我们使用二进制交叉熵损失函数、“adam”优化器和精确度指标编译了该模型。我们还使用早期停止来最小化验证损失。早期停止是一种用于正则化机器学习模型的创新方法。它通过在验证误差最小时停止训练来实现这一点。</p><p id="9d0c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在12个时期之后，训练准确度变为98.84，而验证准确度达到98.85。这显示在<strong class="je hi">图2.2 </strong>中。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lg"><img src="../Images/bb6fa6d0f09c6182867919ffd2033dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*FnbVSDfIqlAf7qsz"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 2</figcaption></figure><p id="fe42" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">同时，训练损失降低到0.3034，验证损失降低到0.2796。这显示在<strong class="je hi">图3中。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lg"><img src="../Images/0ead60d9e89d594911ca469e0452fe9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*luSb2sM7UngfkC_7"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 3</figcaption></figure><h2 id="aaca" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">模型评估</h2><p id="feb5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">图4 </strong>显示了我们的最后一个模型，即简单RNN的表现，以及它通过使用混淆矩阵对患病和未患病之间的关系的近似程度。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lv"><img src="../Images/1eca7063357e1f054ec5ac5fd8734f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/0*6ib_nyWFruEPBxd0"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 4</figcaption></figure><p id="368c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">图5 </strong>显示了ROC曲线，<strong class="je hi">图2.6 </strong>报告了模型在一系列指标上的表现，如精确度、召回率、准确度和F1分数。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lw"><img src="../Images/65dce52f1637e23295f14cc0c35973da.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*2DY-gRZ4Yp9hbuou"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 5</figcaption></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lx"><img src="../Images/ebe9bacc31362a6355d43416329b94a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D1voNjPANp-8Rnsc"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 6</figcaption></figure><h1 id="aa76" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">建立模型:GRU</h1><p id="56ec" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们开发的第二个模型是门控循环单元，它是LSTM的变体。由Cho等人(2014年)介绍。它将遗忘门和输入门合并成一个“更新门”该模型中的第一层将是双向GRU层。然后是一个脱落层，后面是另外两组相同的架构。我们在接近末端的地方有一个平坦而致密的层。模型如下所示。</p><pre class="kg kh ki kj fd kw kx ky bn kz la bi"><span id="1515" class="lb if hh kx b be lc ld l le lf">model = keras.Sequential([<br/> keras.layers.Bidirectional(keras.layers.GRU(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Bidirectional(keras.layers.GRU(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Bidirectional(keras.layers.GRU(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Flatten(),<br/> keras.layers.Dense(10, activation='relu'),<br/> keras.layers.Dense(1, activation='sigmoid')<br/>])<br/><br/>model.compile(loss='binary_crossentropy',<br/> optimizer='adam',<br/> metrics=['accuracy'])<br/><br/>model.build(X_train.shape)<br/><br/>callback = keras.callbacks.EarlyStopping(monitor="val_loss",<br/> min_delta=0,<br/> patience=3,<br/> verbose=1,<br/> mode="auto",<br/> baseline=None,<br/> restore_best_weights=True)<br/><br/>history_rnn = model.fit(x=X_train,y=Y_train, validation_split=0.20, batch_size=32, epochs=30, verbose=2, callbacks=[callback])</span></pre><p id="a6b7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">第一层中GRU层的输入形状是32，输出形状也是32。我们使用二进制交叉熵损失函数、“adam”优化器和精确度指标编译了该模型。这个模型中也使用了早期停止。</p><p id="f483" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">经过13个周期后，训练准确率达到98.84%，验证准确率达到98.15%。如图7 中的<strong class="je hi">所示。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lg"><img src="../Images/025bf9ad058b6fe1028f0fe12f29aa8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*M6dtk2CeKKnmwH1W"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 7</figcaption></figure><p id="7b3f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">同时，训练损失降低到0.03，验证损失降低到0.05。如图8中的<strong class="je hi">所示。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lg"><img src="../Images/17c1e40c8917cf316b260f37c6e47350.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*cG_TLvobS_93wLO5"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 8</figcaption></figure><h2 id="d975" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">模型评估</h2><p id="1d3e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">图9 </strong>显示了我们的最后一个模型，即GRU的表现，以及通过使用混淆矩阵，它对患病和未患病之间的关系的近似程度。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lv"><img src="../Images/82a9f0318c1e861bbec9660c7f48cac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/0*oGMJKHSYlsHGWDWo"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 9</figcaption></figure><p id="b06e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">图10 </strong>显示了ROC曲线，而<strong class="je hi">图11 </strong>报告了该模型在精确度、召回率、准确度和F1分数等一系列指标上的表现。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lw"><img src="../Images/22abc6d7c4ad3716ed7efa297117bd07.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*S7lzetVPMtV1-6x7"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 10</figcaption></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ly"><img src="../Images/0e6a8ab36124b13f30716f722d21be6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IFo9SBoKeYFmkNFQ"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 11</figcaption></figure><h1 id="9664" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">构建模型:双向LSTM</h1><p id="c2a4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们开发的第三个也是最后一个模型是双向LSTM，它也是LSTM的变体。长短期记忆网络，也称为“LSTMs”，是一种可以学习长期依赖性的RNN。Hochreiter和Schmidhuber (1997)介绍了它们，许多人在随后的工作中对它们进行了改进和推广。它们在广泛的问题上表现得非常好，现在被广泛使用。</p><p id="1059" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型中的第一层将是双向LSTM层。然后是一个脱落层，后面是另外两组相同的架构。我们在接近末端的地方有一个平坦而致密的层。模型如下所示。</p><pre class="kg kh ki kj fd kw kx ky bn kz la bi"><span id="e9c1" class="lb if hh kx b be lc ld l le lf">model = keras.Sequential([<br/> keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences= True)),<br/> keras.layers.Dropout(0.2),<br/> keras.layers.Flatten(),<br/> keras.layers.Dense(10, activation='relu'),<br/> keras.layers.Dense(1, activation='sigmoid')<br/>])<br/><br/>model.compile(loss='binary_crossentropy',<br/> optimizer='adam',<br/> metrics=['accuracy'])<br/><br/>model.build(X_train.shape)</span></pre><p id="b833" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">第一层中LSTM层的输入形状是32，输出形状也是32。我们使用二进制交叉熵损失函数、“adam”优化器和精确度指标编译了该模型。这个模型中也使用了早期停止。</p><p id="e2e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">经过13个周期后，训练准确率达到99.69%，验证准确率达到99.08%。如图12 中的<strong class="je hi">所示。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lz"><img src="../Images/224318e87e319545f5e51c89affe5414.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/0*DLGXq-60s-u_WJtX"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 12</figcaption></figure><p id="1ed9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">同时，训练损失降低到0.03，验证损失降低到0.05。这显示在<strong class="je hi">图13中。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lg"><img src="../Images/5fd92761572a5f6d06713cb3c80c27d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*RnFD-9OBu4jVc1f2"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 13</figcaption></figure><h2 id="d9e4" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">模型评估</h2><p id="5b42" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">图14 </strong>显示了我们的最后一个模型，即LSTM的表现，以及它通过使用混淆矩阵对患病和未患病之间的关系的近似程度。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lv"><img src="../Images/9b9dd784dea2f62888d52445382239b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/0*adibPrVF3DusOXXo"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 14</figcaption></figure><p id="b5b3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">图15 </strong>显示了ROC曲线，<strong class="je hi">图16 </strong>报告了模型在一系列指标上的表现，如精确度、召回率、准确度和F1分数。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lw"><img src="../Images/794c0b1a2dfb49137f9520a3f0788c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*AsPq9h1kSqn5grJP"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 2.15</figcaption></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ma"><img src="../Images/d026743cfec4e4131af64ca131ff3247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JnA80kydfkK3znuX"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Figure 16</figcaption></figure><h1 id="0f33" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">理由和模型评估</h1><h2 id="bcb2" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">短期记忆是个问题</h2><p id="3d3b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">短期记忆是递归神经网络的一个问题。如果序列足够长，他们将很难将信息从较早的时间步骤传递到较晚的时间步骤。因此，如果你试图从一段文字中预测什么，RNNs可能会在开始时遗漏重要的信息。</p><p id="dae1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">消失梯度问题影响反向传播过程中的递归神经网络。梯度是用于更新神经网络权重的值。当梯度随着时间向后传播而收缩时，出现消失梯度问题。当梯度值变得非常小时，它对学习没有显著贡献。</p><h2 id="900a" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">LSTMs和GRUs作为解决方案</h2><p id="bd73" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作为短期记忆的解决方案，LSTMs和GRUs被开发出来。它们有被称为“门”的内部机制，允许它们控制信息的流动。这些门可以知道序列中的哪些数据应该被保留或丢弃。这使得它能够将相关信息传递到长长的序列链中，以便进行预测。几乎所有最新的递归神经网络结果都是通过这两个网络实现的。总的来说，rnn对于处理序列数据进行预测是很好的，但是有短期记忆的问题。LSTMs和GRUs是利用被称为gate的机制来减轻短期记忆的。门是简单的神经网络，它控制着序列链中的信息流。</p><h2 id="31db" class="lh if hh bd ig li lj lk ik ll lm ln io jn lo lp is jr lq lr iw jv ls lt ja lu bi translated">评价:RNN vs GRU vs LSTM</h2><p id="7799" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如我们可以从实验结果中看到的，当用相似的网络架构训练时，所有的模型给出了接近100%的准确度。我们注意到的一个很大的区别是，与GRU和LSTM模型相比，训练简单RNN所需的训练时间要多得多。</p><p id="a00a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">根据RNN、LSTM和GRU的所有模型的工作原理，GRU使用的训练参数更少，因此使用的内存更少，执行速度也比LSTM快，而LSTM则更准确。另一方面，RNN使用的内存最多，执行速度最慢。我们可以得出结论，当处理大序列并且准确性很重要时，应该使用LSTM；当可用内存较少且需要更快的结果时，应使用GRU。</p><h1 id="79d4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">RNNs与集成学习</h1><p id="19ac" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正如我们在<a class="ae mb" rel="noopener" href="/mlearning-ai/learning-based-models-for-classification-c21513e14621">的<strong class="je hi">图1 </strong>和<strong class="je hi">图4 </strong>本文</a><strong class="je hi"/>中所示的实验结果中所看到的，我们的集成分类器的准确度勉强击败了我们的RNN模型的准确度。与此同时，训练RNN模型所需的时间和CPU资源的数量大大多于为集成学习任务创建的投票分类器。相比之下，RNN总共19个历元，每个历元平均花费400毫秒。另一方面，我们的投票分类器几乎不需要一秒钟来训练和拟合数据。</p><p id="20ba" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">构建和分析这两个模型的一些关键要点是</p><p id="a124" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">更容易解释</strong>:因为经典的ML采用直接特征工程，所以投票分类器中使用的所有算法都易于解释和理解。此外，由于我们对数据和底层算法有了更好的理解，调整超参数和改变模型设计变得更加容易。另一方面，RNN是一个“黑匣子”,在这个意义上，我们甚至现在也没有完全理解深层网络的“内部”。由于缺乏理论基础，超参数和网络设计也相当困难。</p><p id="c89b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">训练时间和计算成本:</strong> RNNs要求高端GPU在合理的时间内用大数据进行训练，无论是财务上还是计算上。这些GPU非常昂贵，但没有它们，将深度网络训练到高性能是不可能的。快速CPU、固态硬盘存储和快速大容量RAM都是有效使用此类高端GPU所必需的。经典的ML算法，如BN，KNN，EL和SVM，可以用一个像样的CPU来训练，而不需要尖端的硬件。因为它们的计算成本更低，所以允许我们更快地迭代，在更短的时间内尝试更多的技术。</p><p id="9b2f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">集成学习在小数据集上表现更好</strong>:深度网络需要极大的数据集来实现高性能。对于许多应用来说，这样大的数据集并不容易获得，并且获取起来将是昂贵且耗时的。虽然我们提供的数据集非常小，但我们仍然可以看到经典的最大似然算法，即BN，KNN，EL和SVM在更小的数据集上优于RNN。</p><p id="4694" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">适应性</strong>:就领域和应用而言，RNNs远比集成学习算法更具适应性和可移植性。我们建立的RNN可以很容易地转移到许多不同的任务，如语音识别，语言建模，翻译，图像字幕等。集成学习和其他经典的机器学习算法不太灵活，在将其应用于新领域之前需要大量的学习和知识。不同领域和应用的经典ML知识库是非常不同的，并且经常需要在每个单独的领域内进行广泛的专业研究。</p><h1 id="1670" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="197b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">RNN、GRU和LST被用于开发我们的二元分类模型。当在相似的架构上训练时，所有的模型都给出了大约99%的相似的高精度。虽然与较新的LSTMs和GRUs相比，RNNs被证明是较慢的，但是我们的二元分类器的准确性没有受到影响。</p><h1 id="1e63" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><ol class=""><li id="4ab8" class="mc md hh je b jf jg jj jk jn me jr mf jv mg jz mh mi mj mk bi translated">GitHub:<a class="ae mb" href="https://github.com/nandangrover/blood-cell-detection-cnn" rel="noopener ugc nofollow" target="_blank">https://github.com/nandangrover/blood-cell-detection-cnn</a></li><li id="699e" class="mc md hh je b jf ml jj mm jn mn jr mo jv mp jz mh mi mj mk bi translated">舒斯特，m .，，帕利瓦尔，K. K. (1997)。双向递归神经网络。<em class="mq"> IEEE信号处理汇刊</em>，<em class="mq"> 45 </em> (11)，2673–2681。<a class="ae mb" href="https://doi.org/10.1109/78.650093" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/78.650093</a></li><li id="009f" class="mc md hh je b jf ml jj mm jn mn jr mo jv mp jz mh mi mj mk bi translated">戴伊和塞勒姆。<em class="mq">门控循环单元(GRU)神经网络的门控变量</em>。</li><li id="b35b" class="mc md hh je b jf ml jj mm jn mn jr mo jv mp jz mh mi mj mk bi translated">瑞内哈特，D. E .，Hint，G. E .，，威廉姆斯，R. J. (1985)。<em class="mq">学习内部表示错误传播二</em>。</li></ol><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hi fi z dy mz ea eb na ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kp mu"/></div></div></a></div></div></div>    
</body>
</html>