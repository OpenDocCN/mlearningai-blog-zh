<html>
<head>
<title>Image Deblurring using Convolutional Autoencoders (Deep Learning Project Tutorial)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用卷积自动编码器的图像去模糊(深度学习项目教程)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/image-deblurring-using-convolutional-autoencoders-deep-learning-project-tutorial-329f87a4c6ad?source=collection_archive---------0-----------------------#2022-03-27">https://medium.com/mlearning-ai/image-deblurring-using-convolutional-autoencoders-deep-learning-project-tutorial-329f87a4c6ad?source=collection_archive---------0-----------------------#2022-03-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f93c6b734018fddf8545fdd5b7dacda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFkPpvASyO360dHWr_rywQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: <a class="ae it" href="https://www.dailymail.co.uk/sciencetech/article-2047831/Adobe-shows-astonishing-unblur-feature-Photoshop.html" rel="noopener ugc nofollow" target="_blank">dailymail.uk</a></figcaption></figure><p id="b3f8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">模糊是由相机或主体移动、对焦不准确或使用景深较浅的光圈造成的模糊图像区域。这导致图像的分辨率降低，使得图像的轮廓和特征不清楚。因此，为了获得更清晰的照片，我们可以用相机镜头的首选焦点重新拍摄同一张照片，或者使用我们的深度学习知识，再现一幅去模糊的图像。由于我的专业不是摄影，我可以使用深度学习技术来帮助你消除图像模糊！</p><blockquote class="js jt ju"><p id="7e6e" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr ha bi translated">这是<strong class="iw hi">非初学者友好教程</strong>。在开始这个项目之前，你应该知道深度学习的基本概念，如神经网络，CNN。还要稍微熟悉一下Keras，Tensorflow和OpenCV。</p></blockquote><p id="1c7c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有各种类型的模糊-运动模糊，高斯模糊，平均模糊等。但是我们将集中在去模糊<strong class="iw hi">高斯模糊</strong>图像。在这种模糊类型中，<strong class="iw hi">像素权重不相等</strong>。<strong class="iw hi">模糊在中心较高，在边缘</strong>降低，遵循钟形曲线。</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jz"><img src="../Images/06d24dd5d1c38ac0e9ac7ec38c26d91e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ot-nR1uq9q45LtGRNNzhaA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Gaussian Function (Photo from <a class="ae it" href="https://www.rastergrid.com/blog/2010/09/efficient-gaussian-blur-with-linear-sampling/" rel="noopener ugc nofollow" target="_blank">RasterGrid</a>)</figcaption></figure><h1 id="f90b" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">资料组</h1><p id="fe1d" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">在开始编写代码之前，我首先建议您获取一个由两组图像组成的数据集——模糊图像和清晰图像。如果您找不到任何预先存在的可用数据集，您也可以自己创建一个数据集。如果你不知道怎么做，(或者懒得自己做)，请随时给我发邮件请求数据集链接，我很乐意与你分享！</p><p id="12c7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我的数据集的大小是大约50张图像(50张干净和50张模糊)，如果你想使用更多的图像，请随意。</p><h1 id="d70c" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">密码</h1><p id="3dc3" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">现在我们已经准备好了数据集，我们可以开始编写代码了。你可以把它写在你的Jupyter笔记本或Google Colab或任何你喜欢的平台上。我在这个项目中使用了Google Colab，因为通过将我的Google Drive链接到它，我可以更容易地使用数据集。</p><h2 id="4ffe" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated"><strong class="ak">导入依赖关系</strong></h2><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="b1a0" class="lh kf hh lw b fi ma mb l mc md"><strong class="lw hi">import</strong> numpy <strong class="lw hi">as</strong> np<br/><strong class="lw hi">import</strong> pandas <strong class="lw hi">as</strong> pd<br/><strong class="lw hi">import</strong> matplotlib.pyplot <strong class="lw hi">as</strong> plt<br/><br/><strong class="lw hi">%</strong>matplotlib inline<br/><strong class="lw hi">import</strong> random<br/><strong class="lw hi">import</strong> cv2<br/><strong class="lw hi">import</strong> os<br/><strong class="lw hi">import</strong> tensorflow <strong class="lw hi">as</strong> tf<br/><strong class="lw hi">from</strong> tqdm <strong class="lw hi">import</strong> tqdm<br/><strong class="lw hi">from</strong> google.colab <strong class="lw hi">import</strong> drive #If you're using Colab and importing the images from the drive</span></pre><p id="0a4e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这里，我已经导入了<strong class="iw hi"> tqdm库</strong>来帮助我创建进度条，让我知道运行一个特定的代码块需要多长时间。</p><h2 id="46eb" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">导入数据集</h2><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="abc1" class="lh kf hh lw b fi ma mb l mc md">drive<strong class="lw hi">.</strong>mount('/content/drive', force_remount<strong class="lw hi">=True</strong>)</span></pre><p id="5229" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用上述代码，您可以将google drive与colab笔记本连接起来，然后进一步复制清晰和模糊图像的路径，如下所示:</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="bcfc" class="lh kf hh lw b fi ma mb l mc md">good_frames <strong class="lw hi">=</strong> '/content/drive/MyDrive/mini_clean'</span><span id="6f28" class="lh kf hh lw b fi me mb l mc md">bad_frames <strong class="lw hi">=</strong> '/content/drive/MyDrive/mini_blur'</span></pre><p id="ad8b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在为清晰和模糊的图像分别创建2个列表。我们将使用keras预处理库来转换图像。jpg '，' jpeg '或'.将它们添加到我们新创建的列表中。我选择图像尺寸为128x128。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="0c85" class="lh kf hh lw b fi ma mb l mc md">clean_frames <strong class="lw hi">=</strong> []<br/><strong class="lw hi">for</strong> file <strong class="lw hi">in</strong> tqdm(sorted(os<strong class="lw hi">.</strong>listdir(good_frames))):<br/>  <strong class="lw hi">if</strong> any(extension <strong class="lw hi">in</strong> file <strong class="lw hi">for</strong> extension <strong class="lw hi">in</strong> ['.jpg', 'jpeg', '.png']):<br/>    image <strong class="lw hi">=</strong> tf<strong class="lw hi">.</strong>keras<strong class="lw hi">.</strong>preprocessing<strong class="lw hi">.</strong>image<strong class="lw hi">.</strong>load_img(good_frames <strong class="lw hi">+</strong> '/' <strong class="lw hi">+</strong> file, target_size<strong class="lw hi">=</strong>(128,128))<br/>    image <strong class="lw hi">=</strong> tf<strong class="lw hi">.</strong>keras<strong class="lw hi">.</strong>preprocessing<strong class="lw hi">.</strong>image<strong class="lw hi">.</strong>img_to_array(image)<strong class="lw hi">.</strong>astype('float32') <strong class="lw hi">/</strong> 255<br/>    clean_frames<strong class="lw hi">.</strong>append(image)<br/><br/>clean_frames <strong class="lw hi">=</strong> np<strong class="lw hi">.</strong>array(clean_frames)</span><span id="5652" class="lh kf hh lw b fi me mb l mc md">blurry_frames <strong class="lw hi">=</strong> []<br/><strong class="lw hi">for</strong> file <strong class="lw hi">in</strong> tqdm(sorted(os<strong class="lw hi">.</strong>listdir(bad_frames))):<br/>  <strong class="lw hi">if</strong> any(extension <strong class="lw hi">in</strong> file <strong class="lw hi">for</strong> extension <strong class="lw hi">in</strong> ['.jpg', 'jpeg', '.png']):<br/>    image <strong class="lw hi">=</strong> tf<strong class="lw hi">.</strong>keras<strong class="lw hi">.</strong>preprocessing<strong class="lw hi">.</strong>image<strong class="lw hi">.</strong>load_img(bad_frames <strong class="lw hi">+</strong> '/' <strong class="lw hi">+</strong> file, target_size<strong class="lw hi">=</strong>(128,128))<br/>    image <strong class="lw hi">=</strong> tf<strong class="lw hi">.</strong>keras<strong class="lw hi">.</strong>preprocessing<strong class="lw hi">.</strong>image<strong class="lw hi">.</strong>img_to_array(image)<strong class="lw hi">.</strong>astype('float32') <strong class="lw hi">/</strong> 255<br/>    blurry_frames<strong class="lw hi">.</strong>append(image)<br/><br/>blurry_frames <strong class="lw hi">=</strong> np<strong class="lw hi">.</strong>array(blurry_frames)</span></pre><h2 id="c866" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">为模型导入库</h2><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="9578" class="lh kf hh lw b fi ma mb l mc md"><strong class="lw hi">from</strong> keras.layers <strong class="lw hi">import</strong> Dense, Input<br/><strong class="lw hi">from</strong> keras.layers <strong class="lw hi">import</strong> Conv2D, Flatten<br/><strong class="lw hi">from</strong> keras.layers <strong class="lw hi">import</strong> Reshape, Conv2DTranspose<br/><strong class="lw hi">from</strong> keras.models <strong class="lw hi">import</strong> Model<br/><strong class="lw hi">from</strong> keras.callbacks <strong class="lw hi">import</strong> ReduceLROnPlateau, ModelCheckpoint<br/><strong class="lw hi">from</strong> keras.utils.vis_utils <strong class="lw hi">import</strong> plot_model<br/><strong class="lw hi">from</strong> keras <strong class="lw hi">import</strong> backend <strong class="lw hi">as</strong> K<br/><br/>seed = 21<br/>random.seed = seed<br/>np.random.seed = seed</span></pre><h2 id="16d7" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">将数据集分为训练集和测试集</h2><p id="60a7" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">现在，我们将主数据集分成两部分，即训练数据集和测试数据集。我会把它们按80:20的比例分开，你也可以试试其他的。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="1a89" class="lh kf hh lw b fi ma mb l mc md">x <strong class="lw hi">=</strong> clean_frames;<br/>y <strong class="lw hi">=</strong> blurry_frames;</span><span id="c7a6" class="lh kf hh lw b fi me mb l mc md"><strong class="lw hi">from</strong> sklearn.model_selection <strong class="lw hi">import</strong> train_test_split<br/>x_train, x_test, y_train, y_test <strong class="lw hi">=</strong> train_test_split(x, y, test_size<strong class="lw hi">=</strong>0.2, random_state<strong class="lw hi">=</strong>42)</span></pre><p id="fca2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">检查训练和测试数据集的形状</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="b915" class="lh kf hh lw b fi ma mb l mc md">print(x_train[0]<strong class="lw hi">.</strong>shape)<br/>print(y_train[0]<strong class="lw hi">.</strong>shape)</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/6812c477e7fceea38961d477b4cac82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mGLnkp3p1xZ5ASmAEF0ug.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Each image is 128x128 and 3 is for RGB tunnels</figcaption></figure><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="f030" class="lh kf hh lw b fi ma mb l mc md">r <strong class="lw hi">=</strong> random<strong class="lw hi">.</strong>randint(0, len(clean_frames)<strong class="lw hi">-</strong>1)<br/>print(r)<br/>fig <strong class="lw hi">=</strong> plt<strong class="lw hi">.</strong>figure()<br/>fig<strong class="lw hi">.</strong>subplots_adjust(hspace<strong class="lw hi">=</strong>0.1, wspace<strong class="lw hi">=</strong>0.2)<br/>ax <strong class="lw hi">=</strong> fig<strong class="lw hi">.</strong>add_subplot(1, 2, 1)<br/>ax<strong class="lw hi">.</strong>imshow(clean_frames[r])<br/>ax <strong class="lw hi">=</strong> fig<strong class="lw hi">.</strong>add_subplot(1, 2, 2)<br/>ax<strong class="lw hi">.</strong>imshow(blurry_frames[r])</span></pre><p id="1453" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以通过上面的代码检查来自训练和测试数据集的图像，输出将是:</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/dcd85ce8a9ad50b480c8ae3ae2b92bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E02B8MuaRVTpvJqWQvmvIw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Left image being the clean image or Ground Truth and right side image being blurred image</figcaption></figure><p id="21df" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">接下来，我们可以初始化一些参数，以便在编写模型代码时使用它们。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="771a" class="lh kf hh lw b fi ma mb l mc md"><em class="jv"># Network Parameters</em><br/>input_shape <strong class="lw hi">=</strong> (128, 128, 3)<br/>batch_size <strong class="lw hi">=</strong> 32<br/>kernel_size <strong class="lw hi">=</strong> 3<br/>latent_dim <strong class="lw hi">=</strong> 256<br/><br/><em class="jv"># Encoder/Decoder number of CNN layers and filters per layer</em><br/>layer_filters <strong class="lw hi">=</strong> [64, 128, 256]</span></pre><h2 id="c910" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">构建编码器模型</h2><p id="09d2" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">如果你对Autoencoders了解不多，不用担心，你可以参考<a class="ae it" href="https://www.youtube.com/watch?v=1icvxbAoPWc" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi"> <em class="jv">这个链接</em> </strong> </a>了解一下基本情况。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="471c" class="lh kf hh lw b fi ma mb l mc md">inputs <strong class="lw hi">=</strong> Input(shape <strong class="lw hi">= </strong>input_shape, name <strong class="lw hi">= </strong>'encoder_input')<br/>x <strong class="lw hi">=</strong> inputs</span></pre><p id="6559" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了避免繁琐的手工计算，我们将直接使用这些库。我们将构建一个Conv2D(64)-Conv2D(128)-Conv2D(256)堆栈。该模型将具有输入形状(128，128，3)和等于3的内核大小，编码器将把该形状压缩为(16，16，256)，并进一步将其展平为一维数组，该数组将成为我们解码器的输入。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="d121" class="lh kf hh lw b fi ma mb l mc md"><strong class="lw hi">for</strong> filters <strong class="lw hi">in</strong> layer_filters:<br/>    x <strong class="lw hi">=</strong> Conv2D(filters<strong class="lw hi">=</strong>filters,<br/>               kernel_size<strong class="lw hi">=</strong>kernel_size,<br/>               strides<strong class="lw hi">=</strong>2,<br/>               activation<strong class="lw hi">=</strong>'relu',<br/>               padding<strong class="lw hi">=</strong>'same')(x)</span><span id="70bb" class="lh kf hh lw b fi me mb l mc md">shape <strong class="lw hi">=</strong> K<strong class="lw hi">.</strong>int_shape(x)<br/>x <strong class="lw hi">=</strong> Flatten()(x)<br/>latent <strong class="lw hi">=</strong> Dense(latent_dim, name<strong class="lw hi">=</strong>'latent_vector')(x)</span></pre><p id="e29a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的K.int_shape()有助于将张量转换成整数元组。</p><p id="ce6e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实例化编码器模型，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="a7e5" class="lh kf hh lw b fi ma mb l mc md">encoder <strong class="lw hi">=</strong> Model(inputs, latent, name<strong class="lw hi">=</strong>'encoder')<br/>encoder<strong class="lw hi">.</strong>summary()</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/f3241511918474d4c0f810aac4302907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVTw7PUsUsBopeWqZpCC8A.png"/></div></div></figure><h2 id="79b9" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">构建解码器模型</h2><p id="e6bf" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">解码器模型将类似于编码器模型，但它将做相反或相反的计算。我们将首先手动将一维数组从编码器模型转换为(16，16，256)形状，然后将其发送到解码器，将其解码回(128，128，3)形状。所以这里的栈会是conv 2d transpose(256)-conv 2d transpose(128)-conv 2d transpose(64)。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="4dbb" class="lh kf hh lw b fi ma mb l mc md">latent_inputs <strong class="lw hi">=</strong> Input(shape<strong class="lw hi">=</strong>(latent_dim,), name<strong class="lw hi">=</strong>'decoder_input')<br/>x <strong class="lw hi">=</strong> Dense(shape[1]<strong class="lw hi">*</strong>shape[2]<strong class="lw hi">*</strong>shape[3])(latent_inputs)<br/>x <strong class="lw hi">=</strong> Reshape((shape[1], shape[2], shape[3]))(x)</span><span id="ff13" class="lh kf hh lw b fi me mb l mc md"><strong class="lw hi">for</strong> filters <strong class="lw hi">in</strong> layer_filters[::<strong class="lw hi">-</strong>1]:<br/>    x <strong class="lw hi">=</strong> Conv2DTranspose(filters<strong class="lw hi">=</strong>filters,<br/>                        kernel_size<strong class="lw hi">=</strong>kernel_size,<br/>                        strides<strong class="lw hi">=</strong>2,<br/>                        activation<strong class="lw hi">=</strong>'relu',<br/>                        padding<strong class="lw hi">=</strong>'same')(x)<br/><br/>outputs <strong class="lw hi">=</strong> Conv2DTranspose(filters<strong class="lw hi">=</strong>3,<br/>                          kernel_size<strong class="lw hi">=</strong>kernel_size,<br/>                          activation<strong class="lw hi">=</strong>'sigmoid',<br/>                          padding<strong class="lw hi">=</strong>'same',<br/>                          name<strong class="lw hi">=</strong>'decoder_output')(x)</span></pre><p id="285a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实例化解码器模型，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="811d" class="lh kf hh lw b fi ma mb l mc md">decoder <strong class="lw hi">=</strong> Model(latent_inputs, outputs, name<strong class="lw hi">=</strong>'decoder')<br/>decoder<strong class="lw hi">.</strong>summary()</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mi"><img src="../Images/52ef9a688f2068b1df459fd5c96c2e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkiGTHmVulLEF9AIWzJl1g.png"/></div></div></figure><h2 id="8e07" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">构建自动编码器模型</h2><p id="f8f4" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">现在我们有了编码器和解码器模型，我们可以将它们结合起来，最终构建我们的自动编码器模型。</p><blockquote class="mj"><p id="f284" class="mk ml hh bd mm mn mo mp mq mr ms jr dx translated"><strong class="ak">自动编码器型号=编码器型号+解码器型号</strong></p></blockquote><p id="f820" class="pw-post-body-paragraph iu iv hh iw b ix mt iz ja jb mu jd je jf mv jh ji jj mw jl jm jn mx jp jq jr ha bi translated">实例化自动编码器模型，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="a4f8" class="lh kf hh lw b fi ma mb l mc md">autoencoder <strong class="lw hi">=</strong> Model(inputs, decoder(encoder(inputs)), name<strong class="lw hi">=</strong>'autoencoder')<br/>autoencoder<strong class="lw hi">.</strong>summary()</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es my"><img src="../Images/031d7a18e2aabf48bace3f9b80cf917f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kS2gCQeWRtLTa0xdMCv7jw.png"/></div></div></figure><h2 id="c89f" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">最后但并非最不重要</h2><p id="2f38" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">在训练我们的模型之前，我们还需要做最后一件事，选择超参数。</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="e03e" class="lh kf hh lw b fi ma mb l mc md">autoencoder<strong class="lw hi">.</strong>compile(loss<strong class="lw hi">=</strong>'mse', optimizer<strong class="lw hi">=</strong>'adam',metrics<strong class="lw hi">=</strong>["acc"])</span></pre><p id="55f1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我选择损失函数为均方误差，优化器为Adam，评估指标为准确性。</p><p id="9d4f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在定义学习率降低器，如果度量没有改善，则降低学习率，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="39c5" class="lh kf hh lw b fi ma mb l mc md">lr_reducer <strong class="lw hi">=</strong> ReduceLROnPlateau(factor<strong class="lw hi">=</strong>np<strong class="lw hi">.</strong>sqrt(0.1),<br/>                               cooldown<strong class="lw hi">=</strong>0,<br/>                               patience<strong class="lw hi">=</strong>5,<br/>                               verbose<strong class="lw hi">=</strong>1,<br/>                               min_lr<strong class="lw hi">=</strong>0.5e-6)</span></pre><p id="0cf6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在每个时代都称之为，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="fd08" class="lh kf hh lw b fi ma mb l mc md">callbacks <strong class="lw hi">=</strong> [lr_reducer]</span></pre><h2 id="c9b4" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">训练模型</h2><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="43b1" class="lh kf hh lw b fi ma mb l mc md">history = autoencoder<strong class="lw hi">.</strong>fit(blurry_frames,<br/>                      clean_frames,<br/>                      validation_data<strong class="lw hi">=</strong>(blurry_frames, clean_frames),<br/>                      epochs<strong class="lw hi">=</strong>100,<br/>                      batch_size<strong class="lw hi">=</strong>batch_size,<br/>                      callbacks<strong class="lw hi">=</strong>callbacks)</span></pre><p id="6b2b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">运行此代码后，可能需要大约5-6分钟甚至更长时间才能看到最终输出，即第100个纪元。对我来说是的，</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/9b02231dbdf376e96e74bdb1ffcd858d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C69LvUJHs50qzj9RmdMcsg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The accuracy is 78.07% which is pretty decent</figcaption></figure><h2 id="c858" class="lh kf hh bd kg li lj lk kk ll lm ln ko jf lo lp ks jj lq lr kw jn ls lt la lu bi translated">决赛成绩</h2><p id="407c" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">现在我们已经成功地训练了我们的模型，让我们看看我们的模型的预测，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="071a" class="lh kf hh lw b fi ma mb l mc md">print("\n       Input                        Ground Truth                  Predicted Value")</span><span id="008a" class="lh kf hh lw b fi me mb l mc md"><strong class="lw hi">for</strong> i <strong class="lw hi">in</strong> range(3):<br/>    <br/>    r <strong class="lw hi">=</strong> random<strong class="lw hi">.</strong>randint(0, len(clean_frames)<strong class="lw hi">-</strong>1)<br/><br/>    x, y <strong class="lw hi">=</strong> blurry_frames[r],clean_frames[r]<br/>    x_inp<strong class="lw hi">=</strong>x<strong class="lw hi">.</strong>reshape(1,128,128,3)<br/>    result <strong class="lw hi">=</strong> autoencoder<strong class="lw hi">.</strong>predict(x_inp)<br/>    result <strong class="lw hi">=</strong> result<strong class="lw hi">.</strong>reshape(128,128,3)<br/><br/>    fig <strong class="lw hi">=</strong> plt<strong class="lw hi">.</strong>figure(figsize<strong class="lw hi">=</strong>(12,10))<br/>    fig<strong class="lw hi">.</strong>subplots_adjust(hspace<strong class="lw hi">=</strong>0.1, wspace<strong class="lw hi">=</strong>0.2)<br/><br/>    ax <strong class="lw hi">=</strong> fig<strong class="lw hi">.</strong>add_subplot(1, 3, 1)<br/>    ax<strong class="lw hi">.</strong>imshow(x)<br/><br/>    ax <strong class="lw hi">=</strong> fig<strong class="lw hi">.</strong>add_subplot(1, 3, 2)<br/>    ax<strong class="lw hi">.</strong>imshow(y)<br/><br/>    ax <strong class="lw hi">=</strong> fig<strong class="lw hi">.</strong>add_subplot(1, 3, 3)<br/>    plt<strong class="lw hi">.</strong>imshow(result)</span></pre><p id="d0df" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其输出将会是，</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es na"><img src="../Images/d1f99fbd01aacd24eb6404ca0d880247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbGVV4E42q4JyHazv1rtHA.png"/></div></div></figure><p id="af3e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，我们可以看到，该模型在消除图像模糊方面做得非常好，几乎能够恢复原始图像。我们可以通过超参数调整来进一步优化我们的解决方案，这可能会使我们的模型更准确。为此，我们可以绘制损失函数和精确度的图表，以便做出更好的决策。</p><p id="3cfb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于损失函数，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="e830" class="lh kf hh lw b fi ma mb l mc md">plt<strong class="lw hi">.</strong>figure(figsize<strong class="lw hi">=</strong>(12,8))<br/>plt<strong class="lw hi">.</strong>plot(history<strong class="lw hi">.</strong>history['loss'])<br/>plt<strong class="lw hi">.</strong>plot(history<strong class="lw hi">.</strong>history['val_loss'])<br/>plt<strong class="lw hi">.</strong>legend(['Train', 'Test'])<br/>plt<strong class="lw hi">.</strong>xlabel('Epoch')<br/>plt<strong class="lw hi">.</strong>ylabel('Loss')<br/>plt<strong class="lw hi">.</strong>xticks(np<strong class="lw hi">.</strong>arange(0, 101, 25))<br/>plt<strong class="lw hi">.</strong>show()</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/e87f26779021afd84fb412b2c862e3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fnwr_pJNauXChtsoyuuPYA.png"/></div></div></figure><p id="a5b4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由此我们可以看出，损失在显著减少，然后从大约第80纪元开始停滞。</p><p id="060b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了准确起见，</p><pre class="ka kb kc kd fd lv lw lx ly aw lz bi"><span id="4977" class="lh kf hh lw b fi ma mb l mc md">plt<strong class="lw hi">.</strong>figure(figsize<strong class="lw hi">=</strong>(12,8))<br/>plt<strong class="lw hi">.</strong>plot(history<strong class="lw hi">.</strong>history['acc'])<br/>plt<strong class="lw hi">.</strong>plot(history<strong class="lw hi">.</strong>history['val_acc'])<br/>plt<strong class="lw hi">.</strong>legend(['Train', 'Test'])<br/>plt<strong class="lw hi">.</strong>xlabel('Epoch')<br/>plt<strong class="lw hi">.</strong>ylabel('Accuracy')<br/>plt<strong class="lw hi">.</strong>xticks(np<strong class="lw hi">.</strong>arange(0, 101, 25))<br/>plt<strong class="lw hi">.</strong>show()</span></pre><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/f61785963da39689b18a6db8b4634dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tr9RAnlHF-rfCbwHuvDuSA.png"/></div></div></figure><p id="68a6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从这里我们可以看到精确度显著增加，如果有更多的时期，它可能会进一步增加。因此，您可以尝试增加历元大小，并检查准确性是否确实提高了。</p><h1 id="e944" class="ke kf hh bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结论和未来工作</h1><p id="eeb9" class="pw-post-body-paragraph iu iv hh iw b ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn lg jp jq jr ha bi translated">我们已经取得了相当不错的准确率，为78.07%。但这并不是结束，该项目还可以通过以下方式进行改进:</p><ul class=""><li id="2ac6" class="nd ne hh iw b ix iy jb jc jf nf jj ng jn nh jr ni nj nk nl bi translated">获取更多数据以提高模型的准确性</li><li id="3b50" class="nd ne hh iw b ix nm jb nn jf no jj np jn nq jr ni nj nk nl bi translated">超参数调谐</li><li id="4271" class="nd ne hh iw b ix nm jb nn jf no jj np jn nq jr ni nj nk nl bi translated">深入研究过拟合问题</li></ul><p id="f019" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">谢谢你读到这里，如果你有任何反馈，请告诉我！</p></div><div class="ab cl nr ns go nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ha hb hc hd he"><p id="9b79" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="jv">参考文献</em> </strong></p><p id="32aa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">【https://citeseerx.ist.psu.edu/viewdoc/download? T4】doi = 10 . 1 . 1 . 303 . 227&amp;rep = re P1&amp;type = pdf</p><div class="ny nz ez fb oa ob"><a href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">TF . keras . utils . load _ img | tensor flow Core v 2 . 8 . 0</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">将图像加载为PIL格式。</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">www.tensorflow.org</p></div></div></div></a></div><div class="ny nz ez fb oa ob"><a href="https://keras.io/api/layers/convolution_layers/convolution2d/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">Keras文档:Conv2D层</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">2D卷积层(例如图像上的空间卷积)。这一层创建一个卷积核是卷积…</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">keras.io</p></div></div><div class="ok l"><div class="ol l om on oo ok op in ob"/></div></div></a></div><div class="ny nz ez fb oa ob"><a href="https://pyimagesearch.com/2020/02/17/autoencoders-with-keras-tensorflow-and-deep-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">具有Keras、TensorFlow和深度学习的自动编码器- PyImageSearch</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">在本教程中，您将学习如何使用Keras、TensorFlow和深度学习来实现和训练自动编码器。</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">pyimagesearch.com</p></div></div><div class="ok l"><div class="oq l om on oo ok op in ob"/></div></div></a></div><div class="ny nz ez fb oa ob"><a href="https://analyticsindiamag.com/guide-to-autoencoders-with-python-code/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">自动编码器指南，带Python代码</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">自动编码器是一种人工神经网络，用于在无监督的情况下压缩和解压缩输入数据</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">analyticsindiamag.com</p></div></div><div class="ok l"><div class="or l om on oo ok op in ob"/></div></div></a></div><div class="ny nz ez fb oa ob"><a href="https://keras.io/examples/vision/autoencoder/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">Keras文档:用于图像去噪的卷积自动编码器</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">作者:圣地亚哥·l·瓦尔达拉马创建日期:2021/03/01最后修改时间:2021/03/01描述:如何训练一个深…</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">keras.io</p></div></div><div class="ok l"><div class="os l om on oo ok op in ob"/></div></div></a></div><div class="ny nz ez fb oa ob"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="oc ab dw"><div class="od ab oe cl cj of"><h2 class="bd hi fi z dy og ea eb oh ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="oi l"><h3 class="bd b fi z dy og ea eb oh ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oj l"><p class="bd b fp z dy og ea eb oh ed ef dx translated">medium.com</p></div></div><div class="ok l"><div class="ot l om on oo ok op in ob"/></div></div></a></div></div></div>    
</body>
</html>