<html>
<head>
<title>Automated ChatBot for College Queries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于大学查询的自动聊天机器人</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/automated-chatbot-for-college-queries-19b03d72e3c8?source=collection_archive---------3-----------------------#2020-09-05">https://medium.com/mlearning-ai/automated-chatbot-for-college-queries-19b03d72e3c8?source=collection_archive---------3-----------------------#2020-09-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="1807" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是聊天机器人？</h1><p id="bad0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">聊天机器人是一种人工智能软件，可以通过消息应用程序、网站、移动应用程序或电话模拟与用户用自然语言进行的对话。<br/>为什么聊天机器人很重要？聊天机器人经常被描述为最先进和最有前途的人机交互方式之一。然而，从技术的角度来看，聊天机器人只代表了利用自然语言处理(NLP)的问答系统的自然演变。用自然语言表达对问题的回答是自然语言处理在各种企业的终端应用中应用的最典型的例子之一。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/fc372839827123a61944430f789cebc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oj9TSZS2tHuHaQKsDbZVXg.jpeg"/></div></div></figure><h1 id="cbad" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">目录</h1><ol class=""><li id="44d6" class="kn ko hh je b jf jg jj jk jn kp jr kq jv kr jz ks kt ku kv bi translated">什么是大学查询自动化聊天机器人？</li><li id="e8ae" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">聊天机器人的历史</li><li id="f9d5" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">关于数据集</li><li id="aeb7" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">应用模型</li><li id="461f" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">结果和分析</li><li id="e246" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">结论</li></ol><h1 id="b42a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是大学查询自动化聊天机器人？</h1><p id="665f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们的目标是建立一个自动聊天机器人来回答学生经常问的与大学相关的问题。学生们有很多疑问，而且这些疑问是多种多样的。他们需要快速得到问题的答案，但这是不可能的，因为他们需要浏览整个网站或联系管理员，这既费时又麻烦。因此，为了促进这一进程，我们需要自动化这一进程。我们的聊天机器人将有效地服务于这一目的，在眨眼之间给出最合适的查询答案。</p><h1 id="548c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">聊天机器人的历史</h1><p id="90bf" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">创建聊天机器人的过程从最基本的“关键词搜索”机制开始，答案只是基于数据集中匹配的单词。<a class="ae ka" href="https://www.researchgate.net/publication/235664166_A_Survey_of_Chatbot_Systems_through_a_Loebner_Prize_Competition" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> Levy </strong> </a>在1997年改进了基于关键字搜索的基本方法，使用加权模块的模式匹配来生成对查询的响应。</p><p id="b5cc" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated"><a class="ae ka" href="https://www.irjet.net/archives/V4/i11/IRJET-V4I11367.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="je hi">k . Bala</strong></a>教授在2014年引入了本体和知识图方法来回答特定领域聊天机器人的查询。知识库将数据集与单个实体进行映射，聊天机器人可以从创建的关系中直接获得答案。</p><p id="09d3" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated"><a class="ae ka" href="https://ieeexplore.ieee.org/document/8473172" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">吉里什·瓦德瓦</strong> </a>教授在2017年将人工智能标记语言(AIML)与大学特定领域的问答系统结合在一起。AIML是一种上下文相关的多功能语言，可以从之前的对话中学习。</p><p id="d8d4" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated">2018年，使用SQL查询来回答来自结构化数据库的查询。一个基于网络的模型是由<a class="ae ka" href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2007GL030692" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">萨加尔·潘瓦尔</strong> </a>使用各种常见的相似性度量开发的。关系数据库是人类最常见和最容易理解的方式。他们提出的算法是将查询转换成SQL格式，然后根据数据检索结果。</p><h1 id="52b9" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">关于数据集</h1><p id="364d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">由于我们的目标是为大学查询建立一个自动聊天机器人，数据集不是公开可用的，所以我们收集了数据集(w.r.t IIITD College)。以下是数据集收集的步骤。</p><h2 id="00ee" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">数据集采集</strong></h2><p id="a4f1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">该数据集是通过废弃各种网站和页面(如IIITD的招生页面、专门用于IIITD的Reddit页面以及Quora中与IIITD相关的所有问题)并将它们组装起来而获得的。此外，段落涉及所有领域，如招生，费用，教员等。数据集以这种格式构建，用于应用生成模型，生成模型要求这种格式的数据集以文本问答的形式存在，即针对每个问题；我们从流行的社交网站——脸书、Quora和Reddit上找到了最相关的答案。我们还从学院网站上删除了一些数据，以获取一些事实性的答案，如教师和课程的详细信息。在训练数据集中，有3400个问题和答案使用工具FacePager、OctoParser和ScrapeStorm废弃。此外，我们还为问题添加了标签，以及我们获取所有数据集的网站链接。</p><h2 id="2477" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">数据清理</strong></h2><p id="8c6b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">从不同的社交网站获得的数据非常反常，使用起来也不一致。因此，为了使数据适合于训练，我们在以下步骤中处理数据— <br/> 1。我们从数据集中删除了“nan”值，用适当的答案填充它。<br/> 2。我们用原来的词代替俚语词，否则相似度分数会大大降低。<br/> 3。我们删除了非字母数字字符，如表情符号和模糊的符号，以消除不一致。</p><h2 id="f9c6" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">数据预处理</strong></h2><p id="38ab" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">应用以下步骤来预处理文本</p><ol class=""><li id="d0e4" class="kn ko hh je b jf lb jj lc jn lu jr lv jv lw jz ks kt ku kv bi translated">文本的大小写折叠。</li><li id="05c1" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">文本的标记化。</li><li id="e5ac" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">从文本中删除停用词。</li><li id="cd2c" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz ks kt ku kv bi translated">文本的词条化。</li></ol><h1 id="1c87" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">应用模型</h1><p id="a0c3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们尝试对查询搜索应用不同的模型。</p><h2 id="1017" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak"> TF-IDF </strong></h2><p id="3275" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们从最基本的模型“Tf-Idf”开始，它考虑了单词的重要性，并给出了答案，其中术语的重要性与查询的重要性相似。接下来，在查询和数据集中的问题之间的余弦相似性的基础上进行比较。给出的答案是基于问题中术语之间的相似性。然而，这种模型不能完美地处理文本的语义。</p><h2 id="b014" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak"> Word2Vec </strong></h2><p id="6bad" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">接下来，我们在Word2Vec(单词嵌入无监督模型)上建立了我们的模型，该模型通过计算单词在句子中的出现和共现来创建单词的向量，而Tf-Idf产生一个分数来测量相关性，而不是频率。Word2vec是一组用于产生单词嵌入的相关模型。这些模型是浅层的两层神经网络，被训练来重建单词的语言上下文。该模型给出了比余弦相似性更好的结果，但是仍然远离准确的实际结果，因为该模型没有定义次线性关系。Word2Vec没有考虑模型的理论方面。</p><h2 id="c34b" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">快速文本</strong></h2><p id="97b4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">因此，为了提高所获得结果的准确性，我们尝试了来自脸书“FastText”的更好的单词嵌入模型来训练我们的模型。FastText将语料库中的每个单词视为由n元语法组成的字符，而Word2Vec将每个单词视为一个原子实体。在FastText的情况下，每个向量都是字符n-grams的总和。在FastText上训练的模型提供了更好的结果，但是它相对较慢，并且需要更多的存储空间来保存n元语法。它是通过使用脸书文本文档语料库建立的。这是一个真实世界的数据集，因此它是真实世界文本语料库的良好表示。FastText通过跨语言和多种俚语进行训练。它将单词的内部结构与单词嵌入进行映射，并生成上下文值。</p><h2 id="289c" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak"> Doc2Vec </strong></h2><p id="bb29" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Doc2Vec使用word2vec作为底层模型创建文档向量。它以这样一种方式来表示特征，即它考虑单词集合中单词的顺序，并且它以比Word2Vec更好的方式来度量文档的表示。Doc2Vec为文档创建特征向量，记住每个术语相对于该文档的上下文，而不管其长度。它被称为段落向量的<strong class="je hi">分布式内存</strong>版本。它记得上下文是段落的主题。但是，doc2vec高度依赖于根据所使用的数据集进行的参数调整。doc2vec模型创建文档的数字表示。它学习一个随机初始化的向量来捕获段落的上下文。当有很多文档时，随机初始化会产生很多问题，并且对于某些迭代，结果可能是任意的。<strong class="je hi"> WM距离</strong>是将单词的语义及其在句子中的含义考虑在内的相似性度量。它计算查询和数据集中的句子之间的相似度。然后对相似性得分进行排序，并找出具有最大相似性的句子。主要优点是，即使句子之间没有共同的单词，它也能很好地执行。</p><h2 id="943e" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">手套</strong></h2><p id="b72a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">glove是“矢量嵌入的全局表示”的缩写。gloVe是一种无监督学习算法，用于获取单词的矢量表示。在来自语料库的聚集的全局单词-单词共现统计上执行训练，并且所得的表示展示了单词向量空间的有趣的线性子结构。这种手套可用于查找同义词、公司-产品关系、邮政编码和城市等词之间的关系。手套被应用于创建单词嵌入，但是我们的聊天机器人的结果没有改进，因为手套模型不具有用于计算两个单词向量之间的距离的wmdistance函数。</p><h2 id="838d" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak">谷歌伯特</strong></h2><p id="32d5" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">没有记忆就像一个对象，一个除了捕获单词的语义之外，还跟踪所有导致特定单词的单词的对象。BERT模型代表来自变压器的双向编码器表示，这在机器学习领域引起了不小的轰动。BERT的关键警告是它非常关注语言建模。相似性函数我们实现了我们自己的相似性度量，取查询中每个词的权重，乘以嵌入词的平均值，使其成为加权平均模型。</p><h2 id="3c1e" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated"><strong class="ak"> RNN+LSTM </strong></h2><p id="9637" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">使用具有最健壮的递归神经网络的LSTM来创建生成模型，因为它将单词嵌入的世代保存在它自身内部的存储器中，这有助于它们记住输入和答案。LSTM(长短期记忆)与RNN一起使用，当它被反馈机制结合时，使得对顺序数据的预测更好。就文本数据而言，它取得了良好的效果。</p><h1 id="ef06" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结果和分析</h1><h2 id="0467" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated">数据集可视化</h2><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lx"><img src="../Images/50c2acc39298cc87e6c48b5ddd404272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vpGiAP8XRsVvVOBA"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx">Figure shows that most of the queries are admission related.</figcaption></figure><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="ab fe cl mc"><img src="../Images/31573030d1a65a61fbd5e375cc6c1f55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*b31hiO4ynbDLRrXWEFF4aQ.png"/></div><figcaption class="ly lz et er es ma mb bd b be z dx">Figure shows that Admission has higher TF-IDF among all the words.</figcaption></figure><h2 id="9042" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated">RNN+LSTM模型的观察结果</h2><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es md"><img src="../Images/d2538be187d808695c1b6ed29ab1ad95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/0*b71_GGLzdVyBgclY"/></div></figure><p id="a8fa" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated">它解释了在增加时段数以正确拟合模型时，验证准确性会增加。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es me"><img src="../Images/96b60d9b9526a845ffbfeed6ba54c020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/0*NSpqADjOF9T5xgGD"/></div></figure><p id="ce21" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated">随着隐藏层数的增加，验证精度增加，隐藏层的最佳计数为50。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es mf"><img src="../Images/ba46b23f426d7c451aff9bc5ebf884e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/0*HslqIjkw3m3a1yPv"/></div></figure><p id="1957" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated">图中显示了随着历元数的增加而发生的损失的比较。由于训练和测试精度在一段时间后收敛，因此训练的模型非常适合我们的数据集。</p><h2 id="91ac" class="lg if hh bd ig lh li lj ik lk ll lm io jn ln lo is jr lp lq iw jv lr ls ja lt bi translated">应用不同模型的比较。</h2><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es mg"><img src="../Images/890aa1e9807470377af61265ad00abbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/0*_-K2IWJhOHLALlUn"/></div></figure><ul class=""><li id="3d61" class="kn ko hh je b jf lb jj lc jn lu jr lv jv lw jz mh kt ku kv bi translated">这表明FastText在所有模型中表现最好，紧随其后的是Word2Vec。其余两个模型的性能对于我们的数据集来说不是很好。</li><li id="aa91" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz mh kt ku kv bi translated">为一组25个问题绘制图表，并由人类注释者进行评估。</li></ul><h1 id="f235" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><ul class=""><li id="c902" class="kn ko hh je b jf jg jj jk jn kp jr kq jv kr jz mh kt ku kv bi translated">对于与数据集中的查询具有最小相似度的查询，FastText模型表现良好。</li><li id="e7e7" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz mh kt ku kv bi translated">当其他类型的查询被提供给我们的模型时，我们的模型不能给出适当的答案。</li><li id="de90" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz mh kt ku kv bi translated">BERT和Glove预计会比其他模型表现更好，因为他们更好地理解了单词的语义，但他们在我们的小数据集上表现不佳。</li><li id="6860" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz mh kt ku kv bi translated">在50，000个词汇的情况下，生成模型(RNN + LSTM)收敛到5–6%的精度，当我们为数据集加载全部词汇时，收敛到大约24%的精度。</li><li id="7ad4" class="kn ko hh je b jf kw jj kx jn ky jr kz jv la jz mh kt ku kv bi translated">随着数据集大小的增加，BERT、Glove和生成模型将给出更好的结果。</li></ul><p id="d52d" class="pw-post-body-paragraph jc jd hh je b jf lb jh ji jj lc jl jm jn ld jp jq jr le jt ju jv lf jx jy jz ha bi translated">代码可在<a class="ae ka" href="https://github.com/Devashi-Choudhary/Automated-ChatBot-for-College-Queries" rel="noopener ugc nofollow" target="_blank"><strong class="je hi">g</strong></a><strong class="je hi">ithub.com/Devashi-Choudhary/Automated聊天机器人进行学院查询</strong>。如有任何问题或疑问，请直接联系我，电话:<a class="ae ka" href="https://github.com/Devashi-Choudhary" rel="noopener ugc nofollow" target="_blank"><strong class="je hi">github.com/Devashi-Choudhary</strong>。</a></p><figure class="kc kd ke kf fd kg"><div class="bz dy l di"><div class="mi mj l"/></div></figure></div></div>    
</body>
</html>