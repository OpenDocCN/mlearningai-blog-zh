<html>
<head>
<title>News Article Classification Task using SOTA models and their comparison</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于SOTA模型的新闻文章分类任务及其比较</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/news-article-classification-task-using-sota-models-and-their-comparison-a0f87abdc0c1?source=collection_archive---------1-----------------------#2022-03-06">https://medium.com/mlearning-ai/news-article-classification-task-using-sota-models-and-their-comparison-a0f87abdc0c1?source=collection_archive---------1-----------------------#2022-03-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="64c6" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">自然语言理解(NLU)和处理(NLP)</h2><div class=""/><div class=""><h2 id="8a90" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">BERT和DistilBERT案例研究</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/b6a0602df9f6650cb6f240e1fef40227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hdH6nF4JR7zzeo07"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Photo by <a class="ae jv" href="https://unsplash.com/@matoga?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Manuel Torres Garcia</a> on <a class="ae jv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="65d7" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">文本分类</strong></h1><p id="bc0c" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di"> T </span> ext分类用于根据内容对自然语言文本进行分类。例如，考虑按主题对新闻文章进行分类，或者根据积极或消极的回应对书评进行分类。文本分类还有助于语言检测、组织客户反馈和欺诈检测。虽然手动完成这个过程非常耗时，但可以通过机器学习模型实现自动化。类别分类，对于新闻来说，是一个多标签的文本分类问题。目标是为一篇新闻文章分配一个或多个类别。</p><h1 id="a2c5" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak"> NLP世界及其简史</strong></h1><p id="7b8e" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated"><strong class="kq hr">自然语言处理(NLP) </strong>是计算机程序理解人类口头和书面语言的能力，称为自然语言。互联网上有大量关于BERT算法及其应用的文章，理解自然语言处理科学从相当原始的形式到现在的历程可能也很重要。从嵌入技术、改进的语言模型、神经架构等等，在新技术的道路上已经有了相当多的里程碑。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lt"><img src="../Images/ea528cce0e0238ce0f626fc761443bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*DHoXVeF6WLPXRIhsFEHpAg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Source: Author</figcaption></figure><h1 id="41a8" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">伯特及其好处:</strong></h1><p id="30c7" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">BERT代表来自变压器的双向编码器表示，建立在自然语言处理和计算机视觉的多种最新技术之上。本质上，BERT是一种基于深度学习的自然语言表示方法，它将'<strong class="kq hr">迁移学习'</strong>的概念引入到NLP中，并可以进行微调，以处理大量下游NLP用例，如文本分类、问题回答、文本摘要、语义相似性等。这得益于它是一个双向模型，这意味着它利用了句子的正向序列和反向序列。包含反向序列有助于从上下文和关键字中学习，这些上下文和关键字在自然正向序列处理中还没有被“看到”,并且在构成句子的整体含义中具有潜在的重要性。然而，双向对伯特来说并不新奇，而且已经存在了一段时间。同样,<strong class="kq hr">“注意力”</strong>更像是深度学习中的一个通用概念，在伯特之前就被认为存在了。伯特基于类似的概念使用了'<strong class="kq hr">自我关注'</strong>。</p><p id="51b8" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">另一个关键要素是<strong class="kq hr">“语境”</strong>，它是基于这样一种想法而构思的，即一个词的意义通常不是字面上的，而是从它被使用的语境中推导出来的。它解决了著名的单词嵌入模型(如word2vec和fastext)的上下文无关性。我将在本文的后面部分对这些想法进行更多的思考。</p><h1 id="9f32" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">我们为什么需要蒸馏器？</strong></h1><p id="9abd" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">BERT模型面临的主要挑战是其庞大的参数数量(1.1亿个)，因此此类模型主要用于研究工作，从那时起，人们感兴趣的领域之一是“低资源”模型，该模型具有接近最先进的结果，同时更小，运行速度更快，还可进行生产部署。这就是为什么<strong class="kq hr">蒸馏BERT </strong>，BERT 的蒸馏版本被<strong class="kq hr">拥抱脸</strong>构思出来:它的参数少了40%，运行速度快了60%，同时保留了BERT在<strong class="kq hr"> GLUE </strong>语言理解基准测试中97%的性能。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es lz"><img src="../Images/1d73dc5bdd88ecca206f892bf2226898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOlspgJARCj25Lg0UBNOPA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source: Hugging Face</figcaption></figure><p id="f864" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">在<strong class="kq hr"> DistilBERT </strong>中，它将知识提炼应用于BERT，这是一种压缩技术，其中一个小模型被训练来重现一个更大模型(或模型集合)的行为，由<a class="ae jv" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> Hinton等人</strong> </a>演示。</p><h1 id="5e6f" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">案例研究:数据策略</strong></h1><p id="7019" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">该数据集包含2012年至2018年从<a class="ae jv" href="https://www.huffpost.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr">【赫芬顿邮报</strong> </a>获得的约20万条新闻标题。但是，由于培训资源的限制，为分析考虑了40k行的分层样本。总体策略如下所示，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ma"><img src="../Images/177a1e3c9541fa33eba5d2275065bebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AE5d45SQnhRTIp_ZLX8M7w.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source : Author</figcaption></figure><h1 id="6843" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">案例研究:模型策略</strong></h1><p id="0c17" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">基于所用模型的不同文献，一些超参数被认为如下:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mb"><img src="../Images/3069c28b4bd85a78f4305f685b34eef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyEKEGhA6SXY1N3zpWeq5Q.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source : Author</figcaption></figure><h1 id="25e6" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">案例研究:模型架构</strong></h1><div class="jg jh ji jj fd ab cb"><figure class="mc jk md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><img src="../Images/907a15713cc7880f9eed9d38bfacfcd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*F0k1LhQDH_jaqmV98HR0BQ.png"/></div></figure><figure class="mc jk mi me mf mg mh paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><img src="../Images/2000541c21e3afb8c4c4ccbebaf98414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*iJ1-9ujsMnDdaKwnu_i4Dw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx mj di mk ml">Source : Author</figcaption></figure></div><h1 id="db94" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">案例研究:模型对比</strong></h1><p id="d7b2" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">尝试了不同的GPU，保持两个模型的超参数相同，比较如下:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mm"><img src="../Images/5e2f5ed1f6f3a7d6463dfdf784ace541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uI20XAjz7pGzosqTsqIc2A.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source : Author</figcaption></figure><div class="jg jh ji jj fd ab cb"><figure class="mc jk mn me mf mg mh paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><img src="../Images/a08abdd9bcedf6d4c6237d2ff5d188ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*zzBDg83dNo1MLUD0D1TZpw.png"/></div></figure><figure class="mc jk mn me mf mg mh paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><img src="../Images/29aa92398b01e7f1efd54d40e06573a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*KZ4BXGPPcx__o3DJsAe5Hw.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx mj di mk ml">Source : Author</figcaption></figure></div><h1 id="30ab" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">伯特土著及其用途</strong>:</h1><p id="cf07" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">谷歌的BERT和最近基于transformer的方法已经席卷了NLP领域，在几个任务上超过了最先进的技术。</p><p id="e56b" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated"><a class="ae jv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> BERT </strong> </a>是一个双向转换器，用于对大量未标记的文本数据进行预训练，以学习一种可用于微调特定机器学习任务的语言表示。</p><p id="0534" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated"><a class="ae jv" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> XLNet </strong> </a>是一个大型双向转换器，它使用改进的训练方法、更大的数据和更强的计算能力，在20个语言任务上实现了优于BERT的预测指标。</p><p id="c172" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated"><a class="ae jv" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> RoBERTa </strong> </a>在脸书推出，RoBERTa是一种稳健优化的BERT方法，是BERT的再培训，具有改进的培训方法，1000%以上的数据和计算能力。</p><p id="3852" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated"><a class="ae jv" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> DistilBERT </strong> </a>学习一个经过提炼的(近似)版本的BERT，保留了97%的性能但只使用了一半数量的参数(paper)。具体来说，它没有令牌类型的嵌入，pooler，并且只保留了Google的BERT的一半层。DistilBERT使用一种称为蒸馏的技术，这种技术近似于谷歌的BERT，即一个较小的神经网络代替一个较大的神经网络。</p><p id="33db" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">下表很好地总结了这一点，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mo"><img src="../Images/ccb9d51a9134f24fc3e3cd14de8c6c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_owo3xiE1M6bXTwXpMJXg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source : Medium</figcaption></figure><h1 id="70aa" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated"><strong class="ak">结论和关键措施</strong>:</h1><p id="0aaf" class="pw-post-body-paragraph ko kp hh kq b kr ks ir kt ku kv iu kw kx ky kz la lb lc ld le lf lg lh li lj ha bi translated">-如果我们真的需要一个<strong class="kq hr">更快的推理速度</strong>但是<strong class="kq hr">可以在预测指标上妥协</strong>几个百分点的话，<strong class="kq hr"> DistilBERT </strong>是一个合理的选择。</p><p id="0027" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">-如果我们在寻找最好的预测指标，我们应该选择脸书的罗伯塔。</p><p id="8e9f" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">-理论上，<strong class="kq hr"> XLNet的</strong>基于排列的训练应该可以很好地处理依赖性，并且在长期运行中可能会更好。</p><p id="f080" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated">-然而，<strong class="kq hr">谷歌的BERT </strong>确实是一个很好的工作基准，如果我们没有上述任何关键需求，我们可以用<strong class="kq hr"> BERT </strong>保持系统运行。</p><ul class=""><li id="8d03" class="mp mq hh kq b kr lu ku lv kx mr lb ms lf mt lj mu mv mw mx bi translated">如果完整的数据集将用于<strong class="kq hr">训练</strong>，则<strong class="kq hr">模型可能会进一步改进</strong>。</li></ul><h1 id="65aa" class="jw jx hh bd jy jz ka kb kc kd ke kf kg iw kh ix ki iz kj ja kk jc kl jd km kn bi translated">研究的局限性:</h1><ul class=""><li id="c8e4" class="mp mq hh kq b kr ks ku kv kx my lb mz lf na lj mu mv mw mx bi translated">这项研究已经<strong class="kq hr">在超过40000个文本和33个nos类数据集</strong>和<strong class="kq hr">上进行了，其中一些SOTA预训练模型用于NLP </strong>作为<strong class="kq hr">的嵌入</strong>。因此，对于<strong class="kq hr">其他可用模型</strong>和<strong class="kq hr">数据集</strong>,<strong class="kq hr">模型性能可能会有所不同</strong>。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nb"><img src="../Images/25e7467fb1e362967f67deb3eb8d2ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKy6StF0fWHx9D8twUYqGQ.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Source : Lexalytics</figcaption></figure></div><div class="ab cl nc nd go ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ha hb hc hd he"><p id="1f96" class="pw-post-body-paragraph ko kp hh kq b kr lu ir kt ku lv iu kw kx lw kz la lb lx ld le lf ly lh li lj ha bi translated"><em class="nj">如果您认为本文有用，请喜欢并分享，也请随意评论。请随意查看</em> <a class="ae jv" href="https://github.com/pathakchiranjit" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hr"> <em class="nj">我的Github个人资料</em> </strong> </a> <em class="nj">和</em> <a class="ae jv" href="https://github.com/pathakchiranjit/Natural-languge-Processing" rel="noopener ugc nofollow" target="_blank"> <em class="nj">回购f </em>或代码</a> <em class="nj">您也可以在</em><a class="ae jv" href="http://www.linkedin.com/in/pathakchiranjit" rel="noopener ugc nofollow" target="_blank"><strong class="kq hr"><em class="nj">LinkedIn</em></strong></a><em class="nj">上找到我。有兴趣了解更多工程领域的数据分析、数据科学和机器学习应用？通过访问我的</em> <a class="ae jv" rel="noopener" href="/@pathakc"> <strong class="kq hr"> <em class="nj">中剖面</em> </strong> </a> <em class="nj">来探索我之前的文章。谢谢你的阅读。</em></p><ul class=""><li id="a98c" class="mp mq hh kq b kr lu ku lv kx mr lb ms lf mt lj mu mv mw mx bi translated"><em class="nj"> Chiranjit </em></li></ul><div class="nk nl ez fb nm nn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hr fi z dy ns ea eb nt ed ef hq bi translated">Mlearning.ai提交建议</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">如何成为移动人工智能的作者</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jp nn"/></div></div></a></div></div></div>    
</body>
</html>