<html>
<head>
<title>Word2Vec: Key Takeaways</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec:关键要点</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/word2vec-key-insights-ff08f5a1f698?source=collection_archive---------8-----------------------#2022-01-18">https://medium.com/mlearning-ai/word2vec-key-insights-ff08f5a1f698?source=collection_archive---------8-----------------------#2022-01-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="90a5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">基于Mikolov等人2013年对向量空间中单词表示的有效估计</h1><h1 id="16ec" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">概观</h1><p id="1146" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">问题:</strong>如何才能创建一个语言模型，从拥有数百万不同单词的数据集中准确地学习单词向量？</p><p id="2422" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">他们做了什么来回答这个问题:</strong>作者使用词向量算法开发了一个词相似性任务，他们用这个任务研究了现有语言模型的准确性，然后分析了他们基于之前的分析结果创建的另外两个模型。</p><p id="ce92" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">动机/原理:</strong>之前的研究没有成功训练出超过几亿个单词。如果我们比较以前模型的准确性，并根据研究结果提出新的模型方法，我们可以提出更好的模型。</p><p id="3e71" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">发现:</strong>使用非常简单的模型架构训练高质量的单词向量是可能的。</p><p id="5f62" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">解读:</strong>由于计算复杂度低，所以可以从非常大的数据集中计算出非常精确的高维词向量。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="776a" class="ie if hh bd ig ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb bi translated">重要概念</h1><h2 id="8d3b" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated">词向量</h2><h2 id="42be" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated">神经网络语言模型(NNLM)</h2><ul class=""><li id="fdec" class="lf lg hh je b jf jg jj jk jn lh jr li jv lj jz lk ll lm ln bi translated">一个版本由具有线性投影层和非线性隐藏层的前馈神经网络组成。这两层学习单词向量表示和统计语言模型。</li><li id="55ed" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz lk ll lm ln bi translated">另一个版本由一个具有一个隐藏层的神经网络组成，该网络首先学习单词向量。这些单词向量然后被用于训练NNLM。Mikolov等人后来用一个模型扩展了这个架构，该模型只关注学习单词向量的第一步。他们这样做是因为单词向量已经被证明可以显著地改进和简化许多NLP应用程序。</li></ul><h2 id="c9b5" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated">估计单词的连续表示</h2><p id="d64f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者考虑了许多模型来估计单词的连续表征，包括潜在语义分析(LSA)和潜在狄利克雷分配(LDA)。</p><p id="0163" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">潜在语义分析(LSA) </strong></p><p id="392a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">使用SVD在文档中查找主题。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="027a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">潜在狄利克雷分配(LDA) </strong></p><p id="b46e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">一种按主题对文档进行排序的方法。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="fcee" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">神经网络</strong></p><p id="8398" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者决定使用神经网络模型，因为LSA在保持单词之间的线性规则性方面不太有效，并且LDA在大型数据集上的计算非常昂贵。</p><h2 id="587a" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated">比较模型架构</h2><p id="6dd6" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">步骤:</p><ol class=""><li id="d405" class="lf lg hh je b jf ka jj kb jn ma jr mb jv mc jz md ll lm ln bi translated">定义模型的计算复杂度</li><li id="ff79" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz md ll lm ln bi translated">尝试最大化精确度，同时最小化计算复杂度</li></ol><p id="3343" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">对于分析的所有模型体系结构，训练复杂性与以下因素成正比:</p><p id="ff6a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">O = E × T × Q</p><ul class=""><li id="9722" class="lf lg hh je b jf ka jj kb jn ma jr mb jv mc jz lk ll lm ln bi translated">e:训练时期的数量</li><li id="927c" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz lk ll lm ln bi translated">t:训练集中的单词数</li><li id="5bdd" class="lf lg hh je b jf lo jj lp jn lq jr lr jv ls jz lk ll lm ln bi translated">问:特定于每个模型架构的数字</li></ul><p id="4f5c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">使用随机梯度下降和反向传播来训练所有模型。</p><p id="bb56" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">前馈神经网络语言模型(NNLM) </strong></p><p id="6bec" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">模型的层是:输入、投影、隐藏和输出。</p><p id="d6a5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者在他们的模型中使用了层次化的softmax，其中词汇表被表示为霍夫曼二叉树。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="9328" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">关于霍夫曼树的更多信息:</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="67c6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">递归神经网络语言模型(RNNLM) </strong></p><p id="0f93" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">模型的层是:输入、投影、隐藏和输出(除了投影层之外，与NNLM相同)。</p><p id="07b3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">与NNLM不同，不需要指定上下文长度(模型N的阶)，并且，理论上，RNNs可以比浅层神经网络有效地表示更复杂的模式。</p><p id="4235" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">想了解更多关于什么使循环神经网络独一无二的信息，请看这个视频。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="aae3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">神经网络的并行训练</strong></p><p id="388d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作者使用一个框架来并行运行同一模型的多个副本。</p><h2 id="41b9" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated"><strong class="ak">建议型号</strong></h2><p id="eb8e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">作者利用以前模型的信息提出了两个新的更简单的模型，这两个模型将降低计算的复杂性。</p><p id="8ba4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">连续单词包模型(CBOW) </strong></p><p id="3ffd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型学习在给定周围单词的情况下预测缺失的单词。</p><p id="1d81" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似于前馈NNLM，但是非线性隐藏层被移除，并且投影层被所有单词共享。与标准的词袋模型不同，它使用上下文的连续分布式表示。</p><p id="4534" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">连续跳格模型</strong></p><p id="4773" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该模型学习预测给定输入单词周围的单词。</p><p id="2337" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">与CBOW类似，但它不是根据上下文来预测当前单词，而是试图根据同一句子中的另一个单词来最大化地分类一个单词。</p><p id="2fd2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">有关这两个模型的更多信息，请查看DeepLearning的Coursera <a class="ae me" href="https://www.coursera.org/learn/probabilistic-models-in-nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理与概率模型</a>课程。人工智能</p><h2 id="2358" class="kr if hh bd ig ks kt ku ik kv kw kx io jn ky kz is jr la lb iw jv lc ld ja le bi translated">相似性任务</h2><p id="0f1a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">要找到一个与small相似的单词，就像maximum与big相似一样，我们可以简单地计算vector X = vector(" maximum ")-vector(" big ")+vector(" small ")，然后使用余弦距离来确定向量空间中最近的单词向量。</p><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw mx mi"/></div></div></a></div></div></div>    
</body>
</html>