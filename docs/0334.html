<html>
<head>
<title>Twitter Disaster Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推特灾难预测</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/twitter-disaster-prediction-79dae17d5e?source=collection_archive---------11-----------------------#2021-03-23">https://medium.com/mlearning-ai/twitter-disaster-prediction-79dae17d5e?source=collection_archive---------11-----------------------#2021-03-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用tensorflow 2.0预测灾难微博</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/657fbdb59eda656f92362c9dab620e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qNds-aBGRNY4t4PX"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx"><a class="ae js" href="https://www.google.com/search?q=disaster+large+size+image&amp;tbm=isch&amp;ved=2ahUKEwi76Yvflq3vAhVuiksFHW8uBWoQ2-cCegQIABAA&amp;oq=disaster+large+size+image&amp;gs_lcp=CgNpbWcQAzoGCAAQCBAeUOD4BFi7mwVg8pwFaABwAHgCgAG7B4gB2SOSAQ0wLjMuMi4yLjEuMS4ymAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&amp;sclient=img&amp;ei=oqNMYLuQMe6UrtoP79yU0AY&amp;bih=625&amp;biw=1366#imgrc=0-4EUvAjuRufhM" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="ac94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我们将应用基于深度学习的NLP方法，根据推文预测灾难。我从<a class="ae js" href="https://github.com/laxmimerit/twitter-disaster-prediction-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">这里</strong> </a> <strong class="ig hi">取了数据集。</strong>数据集以csv格式呈现。数据集中存在各种特征，即:- <strong class="ig hi"> id、位置、关键字、文本和目标。Target </strong>列以二进制格式给出，其中1代表异常情况(灾难)，0代表正常情况(无灾难)。让我们开始吧……</p><h2 id="9fb8" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">第一步:-加载数据集并导入必要的库</strong></h2><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="36d2" class="jt ju hh kp b fi kt ku l kv kw">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from tensorflow import keras<br/>from keras.layers import Embedding,Dense,Dropout,Bidirectional,LSTM<br/>from keras.models import Sequential<br/>from sklearn import metrics<br/>from sklearn.metrics import accuracy_score,confusion_matrix<br/>import scikitplot as splt</span></pre><p id="fe29" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们将加载数据集</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="6242" class="jt ju hh kp b fi kt ku l kv kw">df=pd.read_csv("train.csv")<br/>df.head()  #printing first five rows</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kx"><img src="../Images/01242a3dc1d0079feb63b71d3a476203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*u_7vcMz3RxEEDB0Ba7oCXw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 1</figcaption></figure><p id="06af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从csv文件中提取文本和目标</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="0b4f" class="jt ju hh kp b fi kt ku l kv kw">x=df.loc[:,"text"]<br/>y=df.loc[:,"target"]<br/>print(f"shape of text: {x.shape}")<br/>print(f"shape of target {y.shape}")</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ky"><img src="../Images/138c456c7ee17e3720795f872455202f.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*Syc8KjOYw4DZcLUF2IXufg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 2</figcaption></figure><h2 id="8d92" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">步骤二:-文本预处理</h2><p id="9bb1" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">文本预处理包括以下步骤:</p><ol class=""><li id="62d5" class="le lf hh ig b ih ii il im ip lg it lh ix li jb lj lk ll lm bi translated"><strong class="ig hi">移除Html标签</strong> :- Html标签在实际语言中没有任何意义，因此我们将移除文本中出现的所有此类标签</li><li id="6d22" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated"><strong class="ig hi">删除标点符号</strong> :-标点符号，即:-空格，"，"，" # "，" @ "等没有任何意义，因此我们将从文本中删除所有这些数据</li><li id="a706" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated"><strong class="ig hi">移除停用词</strong> :-停用词只会增加数据中的冗余，特别是在我们处理文本分类等问题时。但是在机器翻译的情况下，我们不需要删除停用词。由于我们正在处理文本分类，因此我们可以从文本中删除停用词。</li><li id="bc66" class="le lf hh ig b ih ln il lo ip lp it lq ix lr jb lj lk ll lm bi translated"><strong class="ig hi">将文本转换成小写</strong> :-我们需要将所有文本转换成小写。让我们举个例子来更精确地理解它。</li></ol><p id="7291" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设你有<strong class="ig hi"> text 1:-这个披萨味道不错。还有文字2:-这个披萨味道不错。作为人类，我们可以说这两个词有相同的意思，但是机器会把这两个词当作不同的词。因为机器不会将<strong class="ig hi">披萨</strong>和<strong class="ig hi">披萨(因为P在这里是大写的)</strong>理解为同一个单词，因此为了避免这种误导，我们将所有文本转换为小写。</strong></p><p id="7c98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.<strong class="ig hi">短词删除</strong> :-删除所有小于三的单词。根据语言学专家的说法，任何小于三的单词除了停用词之外没有任何语义。</p><p id="3c69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">6.将所有单词转换成它的词根。例如，如果我们有<strong class="ig hi"> go、GoE和GoE</strong>，那么通过将这些单词与动词相关联的词汇化，我们将得到词根为<strong class="ig hi"> go。</strong></p><p id="ef2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">导入用于文本预处理的库</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="58e5" class="jt ju hh kp b fi kt ku l kv kw">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem.porter import PorterStemmer<br/>import spacy<br/>import textblob</span></pre><p id="a97a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们做文字清理；</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="550d" class="jt ju hh kp b fi kt ku l kv kw">nltk.download('stopwords')<br/>stop_words=set(stopwords.words("english")<br/>stopwords<br/>ps=PorterStemmer()<br/>#code to convert all the numerical values into text<br/>import inflect<br/>q=inflect.engine()<br/>def numtotext(data):<br/>    temp_str=data.split()<br/>    new_string=[]<br/>    for word in temp_str:<br/>        if word.isdigit():<br/>            temp=q.number_to_words(word)<br/>            new_string.append(temp) <br/>        else:<br/>            new_string.append(word)<br/>    new_string=" ".join(new_string)<br/>    return new_string</span></pre><p id="8f8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">最终文本清理:- </strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ls"><img src="../Images/84a52386cc4b56e2d56b03a392e0ffb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgAO_H5vmMT3eUYgMdo2tQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Figure 3</figcaption></figure><h2 id="ad1e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">步骤三:-文本可视化:</h2><p id="7e33" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated"><strong class="ig hi">一个</strong>。单词云可视化，查看文本文件中最重要单词的贡献。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/beddb69835322f1c5dce22bae1fe2a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*6wKRyE-m3B4u1JvQKBYBtg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 4</figcaption></figure><p id="cca6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> b </strong>。每个字母在干净的文本数据中出现的频率分布。在这里，我将打印出数据集中最常出现前20个单词。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/176b1f5b078726ba0350f27cdf4d1b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*xCiVyCatINkgVOfXer8jQg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 5</figcaption></figure><p id="8564" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> c </strong>。打印灾难性和非灾难性推文的计数</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/8a4f00086163e459559ebce55a057c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*dQC2VZUng7QK9NLfpCj2gg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Figure 6</figcaption></figure><h2 id="df94" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">第四步:-超参数调整和列车测试分离</h2><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="bb4f" class="jt ju hh kp b fi kt ku l kv kw">vocab_size=5000<br/>len_train=0.8   #assigning 80% part as train set and 20% as test set<br/>oov_token="&lt;oov&gt;"<br/>len_sen=400<br/>train_sizeX=int(len(final_text)*len_train)<br/>train_sizeY=int(len(labels)*len_train)<br/>x_train=final_text[0:train_sizeX]<br/>x_test=final_text[train_sizeX:]<br/>y_train=df["target"][:train_sizeY]<br/>y_test=df["target"][train_sizeY:]</span></pre><p id="3d89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将20%的文本指定为测试集，剩余的80%的文本指定为训练集</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="ca91" class="jt ju hh kp b fi kt ku l kv kw">#extracting train and test dataset from the dataframe<br/>print(len(x_train))<br/>print(len(x_test))<br/>print(len(y_train))<br/>print(len(y_test))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/06ac966983d4bcbf78402453162c4e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*Ix8NyxrrnTNF4mXJpopcKw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 7</figcaption></figure><p id="745c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是vocab_size？</strong></p><p id="5719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们将有非常大的文本数据序列，并且我们试图一次处理所有的文本数据时，那么它将给出低计算设备的内存不足错误。因此，我们将通过另一条路径，即:-我们将使用某些<strong class="ig hi"> vocab_size </strong>来构建我们的NLP模型，如果<strong class="ig hi"> vocab_size=5000 </strong>我们将从文本数据集中取出<strong class="ig hi"> 5000 </strong>个最常出现的单词，然后构建我们的模型。</p><p id="345d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> oov_token？</strong></p><p id="2a61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文本中不属于前5000个单词的任何单词将被指定为<strong class="ig hi"> oov_token </strong>。</p><p id="219f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">让我们检查一下目标变量中有多少不同的变量。</strong></p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="895f" class="jt ju hh kp b fi kt ku l kv kw">&gt;&gt; set(y_train)<br/>[out]&gt;&gt; {0,1}</span></pre><h2 id="c623" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">步骤五:单词嵌入</h2><p id="9d3f" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">众所周知，机器只能理解数值。因此，每当我们有数值以外的值时，我们需要将所有这些值转换成向量或数值形式。在这里，将文本转换成数字的过程称为单词嵌入。这里我们将使用内置的方法<strong class="ig hi">标记器</strong>将文本转换成矢量。</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="7408" class="jt ju hh kp b fi kt ku l kv kw">tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_token)<br/>tokenizer.fit_on_texts(final_text)  #tokenizing the text</span></pre><p id="c73e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看这个<strong class="ig hi">分词器</strong>如何将我们的文本转换成序列</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="631c" class="jt ju hh kp b fi kt ku l kv kw">embed_x_train=tokenizer.texts_to_sequences(x_train)<br/>print(f"original text value :== {x_train[2]}")<br/>print(f"ector form of text value:== {embed_x_train[2]}")</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/750f96a86e795dc593a3040fab53e503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxAnGkGbcPufnsFQbr7Tmg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 8</figcaption></figure><p id="a3c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在的主要任务是使所有的句子长度相同，所以这里我们将应用填充。</p><p id="9ca0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">衬垫有什么作用？</strong></p><p id="d55c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们想用100个单词来构造每个句子。有些句子可能只有50个单词或60个单词或95个单词，在这种情况下,<strong class="ig hi"> pad_sequences </strong>将为剩余句子的位置添加零，并使所有句子大小相同。</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="6741" class="jt ju hh kp b fi kt ku l kv kw">padded_x_train=pad_sequences(embed_x_train,maxlen=len_sen)<br/>padded_x_test=pad_sequences(embed_x_test,maxlen=len_sen)<br/>print(f"original text value :== {x_train[2]}")<br/>print(f"vector form of text value:== {embed_x_train[2]}")<br/>print(f"embedded x_train after paddig:=={padded_x_train[2]}")</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ly"><img src="../Images/6d278a23844271461f9840123b0c431c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RoXWkYSDvPP0kmVH36rfg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 9</figcaption></figure><h2 id="4583" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">步骤六:-模型准备</strong></h2><p id="3003" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">这里我们将把顺序层添加到模型中</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="0a86" class="jt ju hh kp b fi kt ku l kv kw">model=Sequential()</span><span id="6780" class="jt ju hh kp b fi lz ku l kv kw">model.add(Embedding(vocab_size,output_dim=100,input_length=len_sen))#here we will use stacked LSTM layer to predict the result<br/>model.add(LSTM(128,return_sequences=True,activation="relu"))<br/>model.add(LSTM(128,return_sequences=True,activation="relu"))<br/>model.add(LSTM(128,return_sequences=True,activation="relu"))<br/>model.add(LSTM(128,return_sequences=False,activation="relu"))<br/>model.add(Dense(48,activation="relu"))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(24,activation="relu"))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(1,activation="sigmoid"))<br/>model.summary()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/b6ea75645941dc72546a006752e8297e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*T1gqXsUqqp3ferZRIhSjBQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 10</figcaption></figure><h2 id="cbc6" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">第七步:-模型验证</h2><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="21e2" class="jt ju hh kp b fi kt ku l kv kw">plt.plot(history.history['accuracy'])<br/>plt.plot(history.history['val_accuracy'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'])<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mb"><img src="../Images/5f469560d29bddd7e4a72aa065c424ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*YPBsut1Jtyqtp_ZccwE7Vw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 11</figcaption></figure><p id="5c64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型没有给出更好的准确性，因此这里基本的LSTM模型将不会很好地工作，我们将不得不使用基本的艺术方法，如<strong class="ig hi">伯特</strong>或<strong class="ig hi"> GPT </strong>。</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="8ef9" class="jt ju hh kp b fi kt ku l kv kw">plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='upper left')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mc"><img src="../Images/280464a534758b2e298e4f9ebbaa4eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*mncLUfG3kMyodNxMB-pgkQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 12</figcaption></figure><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="bc6c" class="jt ju hh kp b fi kt ku l kv kw">from sklearn import metrics<br/>splt.metrics.plot_confusion_matrix(y_test,predicted,figsize=(10,8),text_fontsize="large")<br/>plt.show()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es md"><img src="../Images/c7ee59391fd8754a792957d74f53503a.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*CpxwxEhrEd4yg0D8MemiWw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">figure 13</figcaption></figure><p id="9b36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们看到的混淆矩阵，我们可以说，我们得到了大量的假阴性结果，这意味着大多数时候我们的模型预测的是<strong class="ig hi">非灾难性</strong>情况，而实际上是灾难性的，这将导致巨大的财产和生计损失。</p><h2 id="cab6" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">第八步:-检查看不见的数据的结果</h2><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="f134" class="jt ju hh kp b fi kt ku l kv kw">def predict(data):<br/>    data=tokenizer.texts_to_sequences(data)<br/>    data=pad_sequences(data,maxlen=150,padding="post")<br/>    result=model.predict_classes(data)<br/>    if result==0:<br/>        return "Non Disastrous" <br/>    else:<br/>        return "Disastrous"</span></pre><p id="7b72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">选择任何一条你自己选择的推文来预测结果</p><pre class="jd je jf jg fd ko kp kq kr aw ks bi"><span id="8272" class="jt ju hh kp b fi kt ku l kv kw">predict(["Thank you Goa for the continuous support to BJP. The results of the Municipal Elections 2021 show the people’s appreciation towards our Party’s development agenda. I laud all hardworking BJP Karyakartas who went among people and worked hard during the campaign"])</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es me"><img src="../Images/95cada2ca617991aaa5f8e02489a7ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*gAkSexsXazJVXPMLesL8sA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Result</figcaption></figure><p id="b4c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结论:-</p><p id="131e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管这是我实现的最差的预测模型。我试图在这里展示LSTM的基本实现。请耐心等待，我们将使用BERT解决同样的问题，它将比我们现在实现的模型提供更好的准确性。非常感谢拉克西米·康德提供了这个惊人的数据集。不断学习，不断探索……..</p></div></div>    
</body>
</html>