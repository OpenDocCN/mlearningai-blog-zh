<html>
<head>
<title>Gradient Boosting for Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始回归的梯度推进</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/gradient-boosting-for-regression-from-scratch-bba968c16c57?source=collection_archive---------0-----------------------#2022-07-29">https://medium.com/mlearning-ai/gradient-boosting-for-regression-from-scratch-bba968c16c57?source=collection_archive---------0-----------------------#2022-07-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="f940" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">Python中梯度增强的解释和实现</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/ce773b9fcf1a2b3da70e2d8f0be14b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fetLh8S-gwVKzKBPwefVug.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@unarchive?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jeremy Bezanger</a> on <a class="ae jm" href="https://unsplash.com/s/photos/booster?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="becd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">梯度推进是集成机器学习技术之一。它使用像序列中其他学习者一样的弱学习者来产生健壮的模型。</p><p id="9112" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是一种灵活而强大的技术，可用于回归和分类问题。即使进行很小的调整，也可以获得良好的结果。它可以处理大量的功能，并且不偏向于任何特定的功能类型。</p><p id="d6e7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">另一方面，它比其他机器学习方法对过拟合更敏感，并且训练可能很慢，特别是在大型数据集上。</p><p id="ca46" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">尽管有缺点，梯度推进是许多机器学习任务的流行方法，因为它灵活、强大和相对较好的性能。</p><p id="68e7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这篇博文中，我将研究梯度推进技术在回归问题中的应用。在另一篇文章中，我将讨论分类问题。</p><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/@okanyenigun/gradient-boosting-for-classification-f9a93381e37c"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">用于分类的梯度推进</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">Python中梯度增强的解释和实现</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="kw l kx ky kz kv la jg km"/></div></div></a></div><p id="4c7b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在继续之前，您可能需要复习决策树或另一种集成技术AdaBoost:</p><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/mlearning-ai/adaboost-from-scratch-f8979d961948"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">从头开始</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">AdaBoost算法的解释和实现</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="lb l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a href="https://python.plainenglish.io/decision-tree-parameters-explanations-tuning-a2b0749976e5" rel="noopener  ugc nofollow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">决策树参数解释</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">Sklearn的决策树参数解释</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">python .平原英语. io</p></div></div><div class="kv l"><div class="lc l kx ky kz kv la jg km"/></div></div></a></div><p id="decd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">您可能还记得，AdaBoost使用深度为1的决策树，称为stump。每个新的树桩根据前一个树桩的误差减少或增加观测值的权重。</p><p id="5967" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">另一方面，梯度推进从单叶开始，这是一个初步的猜测。后来，它建造树木。然而，与AdaBoost不同，这些树通常比树桩大。在这种技术中，人们通常使用8到32个叶子的决策树。同样，与AdaBoost不同，梯度增强技术以相同的速率缩放树。</p><h2 id="30c4" class="ld le hh bd lf lg lh li lj lk ll lm ln jw lo lp lq ka lr ls lt ke lu lv lw lx bi translated">伪代码</h2><p id="c36a" class="pw-post-body-paragraph jn jo hh jp b jq ly ii js jt lz il jv jw ma jy jz ka mb kc kd ke mc kg kh ki ha bi translated"><strong class="jp hi">从</strong>开始，我们有一个数据集，x为观察值，y为目标特征。此外，我们有一个可微的损失函数。我们使用损失函数来评估我们的估计。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es md"><img src="../Images/1dcc9a86636516bd211f08be436278d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qnqrIpNnb_hJfN0QOUusxA.png"/></div></div></figure><p id="4acf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">损失函数就是残差的平方和，你可能还记得逻辑回归。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es me"><img src="../Images/10073d5ef5df0f98026b825b82a7dbc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*_sed2KT2XgSxvJd3h2KGzQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">One residual</figcaption></figure><p id="747b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">第一步:</strong>我们从一片叶子开始，这意味着我们将用一个常数来初始化模型。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mf"><img src="../Images/53f82c172851868c6d2b73acfb14bf7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*P2AOMTMVqUtz7DBk3aO2PA.png"/></div></figure><p id="bf81" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">上面的等式意味着我们将所有的残差(每次观察的损失)相加。然而，arg min/gamma意味着我们必须做出这样的预测，使得这个和最小化。作为数学计算的结果，最佳预测值是第一轮中所有y值的平均值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mg"><img src="../Images/998b8b9050da75bb53f2bbdf855fa868.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*Nr7azub07vhCrf7enCOIuQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Dummy data</figcaption></figure><p id="830c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">假设我们有一个虚拟数据集和目标特征，如上所示。在这种情况下，我们最初的预测将是平均值= 75。</p><p id="3144" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">第二步:</strong>对于m=1到M；我们将把M棵树做成一个循环。</p><p id="2fbe" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">步骤2-A: </strong>计算残差:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mh"><img src="../Images/d2c474fcec3e85a846f5ef391415b1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*bbHKoLLvnTb4E7F60FxNKw.png"/></div></figure><p id="b1ba" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">取损失函数的导数(这个导数就是梯度)；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mi"><img src="../Images/4d0c3741d6f88c3a4c95b00bda1162ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQ3Wh1_jqOBN7k-WFIB1Bw.png"/></div></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mj"><img src="../Images/adb24a5e51564f781ceaa963915afaa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*hczu0UgP2TQEfPF_-1oeiA.png"/></div></figure><p id="0156" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">步骤2-B: 在这一步，我们将<strong class="jp hi">构建一个基础学习器(在我们的例子中是决策树)。</strong>我们的目标特征<strong class="jp hi">将不是目标</strong>列，而是<strong class="jp hi">残差</strong>。</p><p id="dc40" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">步骤2-C: </strong>对于每片叶子，计算使下面总和最小化的伽马值；它将先前的预测考虑在内，并考虑树叶中的样本。我在这里的意思是，我们将分别计算每片叶子的伽马值，并且在所讨论的叶子上的任何观察都将包括在计算中。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mk"><img src="../Images/11b24211bdc62d2af0075cc4ec60738d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SS7wOoQf4pQ3CBjwRFKExw.png"/></div></div></figure><p id="5eaa" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">步骤2-D </strong>:更新预测:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ml"><img src="../Images/53800fc96a63e308aa4f961b20982a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*0Dp4T1yAkWRAgfUYlmU9tg.png"/></div></div></figure><p id="dc1c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下面我们来解释一下这个公式:对于每个样本，我们把之前的预测值和上一步中找到的gamma值相加。伽玛值乘以学习率(以避免过拟合)。</p><p id="10e5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">比如我们把上一步找到的值补上(学习率= 0.1)；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mm"><img src="../Images/5625aa8016c4c94e53b9209cafccc3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxsvdIywm-WeIjuvEZB98g.png"/></div></div></figure><p id="bf01" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">pred2，1 = 75 * 0.1 (-22) = 72.8</p><p id="2746" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这样，我们向更好的结果迈进了一小步。我们重复这些步骤M次。</p><h2 id="9e6e" class="ld le hh bd lf lg lh li lj lk ll lm ln jw lo lp lq ka lr ls lt ke lu lv lw lx bi translated">Python代码</h2><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mn mo l"/></div></figure><pre class="ix iy iz ja fd mp mq mr ms aw mt bi"><span id="3568" class="ld le hh mq b fi mu mv l mw mx">import pandas as pd<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeRegressor<br/>import matplotlib.pyplot as pt<br/>import seaborn as sns<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import roc_auc_score</span><span id="ef9b" class="ld le hh mq b fi my mv l mw mx">#READ DATA<br/>data = pd.read_csv("housing.csv")<br/>data.fillna(0,inplace=True)</span><span id="c9a2" class="ld le hh mq b fi my mv l mw mx">#X,y<br/>X = data.iloc[:,:-2]<br/>y = data['median_house_value']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=100)</span><span id="f64b" class="ld le hh mq b fi my mv l mw mx">#scaling<br/>scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.fit_transform(X_test)</span><span id="63b7" class="ld le hh mq b fi my mv l mw mx">y_train = np.array(y_train).reshape(X_train.shape[0],1)<br/>y_test = np.array(y_test).reshape(X_test.shape[0],1)</span><span id="b36d" class="ld le hh mq b fi my mv l mw mx">#TRAIN<br/>G = GradientBooster()<br/>models, losses, pred_0 = G.train(X_train,y_train)</span></pre><p id="b7c8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们绘制损失图；</p><pre class="ix iy iz ja fd mp mq mr ms aw mt bi"><span id="29ab" class="ld le hh mq b fi mu mv l mw mx">sns.set_style('darkgrid')<br/>ax = sns.lineplot(x=range(1000),y=losses)<br/>ax.set(xlabel='Epoch',ylabel='Loss',title='Loss vs Epoch')</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mz"><img src="../Images/2e59c1a532243b4780ce6290db9b373a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2I48Nitn0P1N7sSMPlSnA.png"/></div></div></figure><p id="315e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们来预测一下测试数据；</p><pre class="ix iy iz ja fd mp mq mr ms aw mt bi"><span id="7782" class="ld le hh mq b fi mu mv l mw mx">y_pred = G.predict(models, y_train, X_test)<br/>print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))</span><span id="1611" class="ld le hh mq b fi my mv l mw mx">#RMSE: 49396.079511786884</span></pre><h2 id="8417" class="ld le hh bd lf lg lh li lj lk ll lm ln jw lo lp lq ka lr ls lt ke lu lv lw lx bi translated">Sklearn</h2><pre class="ix iy iz ja fd mp mq mr ms aw mt bi"><span id="ce2f" class="ld le hh mq b fi mu mv l mw mx">from sklearn.ensemble import GradientBoostingRegressor</span><span id="da56" class="ld le hh mq b fi my mv l mw mx">model = GradientBoostingRegressor(n_estimators=1000,criterion='mse',<br/>                                    max_depth=8,min_samples_split=5,<br/>                                    min_samples_leaf=5,max_features=3)</span><span id="80c8" class="ld le hh mq b fi my mv l mw mx">model.fit(X_train,y_train)<br/>y_pred = model.predict(X_test)</span><span id="1401" class="ld le hh mq b fi my mv l mw mx">print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))</span><span id="b966" class="ld le hh mq b fi my mv l mw mx">#RMSE: 48744.67210701868</span></pre><p id="f2a1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">RMSE结果与手动实施一致。</p><p id="45cc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">感谢阅读。如果您有任何问题或意见，请随时写信给我！</p><h2 id="0214" class="ld le hh bd lf lg lh li lj lk ll lm ln jw lo lp lq ka lr ls lt ke lu lv lw lx bi translated">阅读更多内容…</h2><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/@okanyenigun/gradient-boosting-for-classification-f9a93381e37c"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">用于分类的梯度推进</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">Python中梯度增强的解释和实现</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="kw l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a href="https://python.plainenglish.io/random-forest-implementation-980b2d2c3c84" rel="noopener  ugc nofollow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">随机森林实现</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">Sklearn实现和参数解释</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">python .平原英语. io</p></div></div><div class="kv l"><div class="na l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/mlearning-ai/a-detailed-catalog-of-dimensionality-reduction-ca33d6f2744"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">降维的详细目录</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">用Python语言解释的多种降维方法</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="nb l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a href="https://towardsdev.com/scatter-charts-matplotlib-seaborn-plotly-39268cdbae41" rel="noopener  ugc nofollow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">散点图(Matplotlib、Seaborn、Plotly)</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">如何使用3个最流行的库创建散点图？</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">towardsdev.com</p></div></div><div class="kv l"><div class="nc l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a href="https://python.plainenglish.io/ridge-lasso-elasticnet-regressions-from-scratch-32bf9f1a03be" rel="noopener  ugc nofollow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">山脊，套索和弹性网从零开始回归</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">Python代码从头开始和Sklearn实现</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">python .平原英语. io</p></div></div><div class="kv l"><div class="nd l kx ky kz kv la jg km"/></div></div></a></div><h2 id="2c3c" class="ld le hh bd lf lg lh li lj lk ll lm ln jw lo lp lq ka lr ls lt ke lu lv lw lx bi translated">参考</h2><p id="f3d9" class="pw-post-body-paragraph jn jo hh jp b jq ly ii js jt lz il jv jw ma jy jz ka mb kc kd ke mc kg kh ki ha bi translated"><a class="ae jm" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_boosting</a></p><p id="3a8b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></p><p id="85cc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=311s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t = 311s</a></p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ne mo l"/></div></figure><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="nf l kx ky kz kv la jg km"/></div></div></a></div></div></div>    
</body>
</html>