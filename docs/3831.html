<html>
<head>
<title>Your Model Is Under Attack — An Adversarial Machine Learning Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的模型受到攻击——一个对抗性的机器学习项目</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/your-model-is-under-attack-an-adversarial-machine-learning-project-9d9f75d8fe42?source=collection_archive---------6-----------------------#2022-10-27">https://medium.com/mlearning-ai/your-model-is-under-attack-an-adversarial-machine-learning-project-9d9f75d8fe42?source=collection_archive---------6-----------------------#2022-10-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5e02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章旨在展示机器学习模型是如何被攻击的。这是我上两篇关于<a class="ae jc" rel="noopener" href="/mlearning-ai/adversarial-machine-learning-fight-against-attackers-e38d5f0917f6">对抗性机器学习</a>和<a class="ae jc" rel="noopener" href="/mlearning-ai/convolutional-neural-networks-learning-from-what-matters-b05e6121cb7a">卷积神经网络(CNN) </a>的文章的继续，因为它是CNN被攻击的代表。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/642c354905e9e488d44c867a1f19208c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i78IBilnGnqFQHS4"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jc" href="https://unsplash.com/@theblowup?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">the blowup</a> on <a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7175" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个对立的例子是一个图像或其他类似于正常输入的输入，用于训练模型，但其中的内容会造成失真。</p><p id="1255" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经在另一篇文章中概述了什么是对抗性机器学习，这篇文章将只关注模型训练后完成的攻击。我们称之为<strong class="ig hi">规避攻击</strong>，因为这个想法是发送愚弄模型的输入，而不是改变模型本身。</p><p id="a41c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">先说攻击用的方法…</p><h1 id="02ab" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">有哪些强大的攻击手段？</h1><p id="77cc" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这些攻击旨在创建相互矛盾的例子，以使ML系统做出错误的预测。这里我们介绍四种著名的方法:<strong class="ig hi">快速梯度符号法</strong>、<strong class="ig hi">投影梯度下降法</strong>、<strong class="ig hi">基本迭代法</strong>和<strong class="ig hi">卡里尼&amp;瓦格纳攻击。</strong></p><p id="8438" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">快速梯度符号法(FGSM) </strong>:这是一种旨在扭曲分类的“白盒”攻击。其背后的想法很简单，在根据输入图像的分类计算损失之后，它计算损失的梯度，但不是使用这个梯度来最小化损失，而是使用它来<strong class="ig hi">最大化后面的</strong>。在最大化损失和增加未命中分类的方向上修改像素。</p><blockquote class="kw kx ky"><p id="5e0c" class="ie if kz ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">在FGSM中，攻击者试图找到一个最大化关于被攻击模型权重的损失函数的输入。</p></blockquote><p id="75ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">投影梯度下降(PGD) </strong>:类似于FGSM方法，但是它迭代地工作以缓慢地修改输入，而不是像FGSM那样具有独特的步骤。此外，背后的想法是添加一个不会大幅改变输入的扰动，这是通过也被称为约束的<em class="kz">ε</em>项来管理的。约束的目标是保持对立的例子看起来和正常的例子一样，这是通过使用L或L∞范数来实现的。</p><p id="d7c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">基本迭代方法</strong><strong class="ig hi">【BIM】</strong>:顾名思义，它是一种迭代方法，与FGSM受相同的方法支持。它也被称为迭代FGSM (IFGSM ),和PGD一样，其思想是保持新像素与输入像素相对相似。PGD和BIM的不同之处在于感兴趣的球内的示例的初始化(来自规范)。虽然这是在PGD随机完成的，但初始化是在BIM中设置的。</p><p id="ec14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">Carlini&amp;Wagner Attack(C&amp;W):</strong>这种方法与之前的方法不同，因为它用于目标明确的攻击，目标是基于特定的类别创建敌对的例子。因此，C &amp; W不是简单地修改示例，而是基于图像的新类别(约束)来最小化损失函数。</p><p id="9415" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看如何用Python实现它…</p></div><div class="ab cl ld le go lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ha hb hc hd he"><h1 id="cd73" class="jt ju hh bd jv jw lk jy jz ka ll kc kd ke lm kg kh ki ln kk kl km lo ko kp kq bi translated">创造对立的例子</h1><p id="c54b" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">因为我们将模拟的攻击是白盒攻击，这意味着我们完全了解我们需要的模型架构:</p><ul class=""><li id="fb68" class="lp lq hh ig b ih ii il im ip lr it ls ix lt jb lu lv lw lx bi translated">具有权重和损失函数的<strong class="ig hi">训练模型</strong></li><li id="d663" class="lp lq hh ig b ih ly il lz ip ma it mb ix mc jb lu lv lw lx bi translated">可用于模型的一些<strong class="ig hi">输入(图像)</strong></li></ul><p id="567a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我选择的模型是我在以前的<a class="ae jc" rel="noopener" href="/mlearning-ai/convolutional-neural-networks-learning-from-what-matters-b05e6121cb7a">文章</a>中建立的模型，这是一个在数千个中国交通标志上训练的卷积神经网络(在这里找到<a class="ae jc" href="http://www.nlpr.ia.ac.cn/pal/trafficdata/recognition.html" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="e90e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输入必须经过预处理，因为CNN只接受134x128的图像。因此，每个输入都通过一个函数来完成。</p><p id="a612" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了所有这些，我们将能够为<strong class="ig hi">无目标攻击</strong>和<strong class="ig hi">有目标攻击</strong> <strong class="ig hi">和</strong>创建对抗性示例。第一组的目标只是愚弄模型，而第二组的目标是将修改后的图像分类到特定的类别中。</p><h2 id="8b64" class="md ju hh bd jv me mf mg jz mh mi mj kd ip mk ml kh it mm mn kl ix mo mp kp mq bi translated">无目标规避攻击</h2><p id="e6e7" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在这一节中，我们的想法是从测试数据集中创建修改后的例子，以欺骗模型。为此，我使用了<a class="ae jc" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">对抗性-鲁棒性工具箱(ART) </strong> </a> <strong class="ig hi"> </strong>，这是一个为许多对抗性任务提供工具的库。</p><p id="8206" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是从一系列图像中创建对立示例的代码:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="5bdd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它分两步工作:</p><ul class=""><li id="bddc" class="lp lq hh ig b ih ii il im ip lr it ls ix lt jb lu lv lw lx bi translated"><strong class="ig hi">加载将要被愚弄的模型</strong>并将其转换为<strong class="ig hi">艺术分类器</strong>。想法是从Keras创建一个分类器实例，允许对它进行攻击。</li><li id="928a" class="lp lq hh ig b ih ly il lz ip ma it mb ix mc jb lu lv lw lx bi translated">用模型初始化攻击方法，本例中<strong class="ig hi">投影梯度下降</strong>，<strong class="ig hi">基于测试数据集生成</strong> <strong class="ig hi">对抗性</strong> <strong class="ig hi">示例</strong>。</li></ul><p id="fb15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里你可以看到模特小姐是如何对对抗性例子进行分类的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mt"><img src="../Images/2be6415af0d94a7dac99208941d45b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lXXg-HH-eWf5e-D5ME5Img.gif"/></div></div></figure><p id="9f31" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个应用程序是用<a class="ae jc" href="https://gradio.app/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> Gradio </strong> </a>制作的，这是一个用于创建web应用程序的框架，可以在一分钟内演示模型系统。</p><h2 id="7088" class="md ju hh bd jv me mf mg jz mh mi mj kd ip mk ml kh it mm mn kl ix mo mp kp mq bi translated">有针对性的规避攻击</h2><p id="e9e4" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">生成有目标的例子比无目标的例子更困难，因为它要求攻击者知道分类器的内部结构。</p><p id="6bbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这个任务，库<a class="ae jc" href="https://github.com/cleverhans-lab/cleverhans" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> CleverHans </strong> </a>很棒，可以生成针对性攻击。我决定使用的方法是我之前介绍的<strong class="ig hi">基本迭代法</strong>。</p><p id="1907" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是从一系列图像中创建对立示例的代码:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="3e82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">代码背后的想法是首先<strong class="ig hi">加载模型</strong>，通过<strong class="ig hi">移除最后一层</strong>(输出)来改变它，以便在新的训练期间用<strong class="ig hi"> sigmoid函数/逻辑回归</strong>(用于二进制分类)替换先前的softmax函数(用于分类多个目标)。</p><p id="a2ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦完成，第二步也是最后一步是<strong class="ig hi">基于一些超参数和测试数据集生成对立的例子</strong>。<em class="kz"> y_attack_target </em>是与新图像数量相同大小的唯一目标(在本例中为3个)的列表。因此，训练考虑到所有图像都与该目标相关联，而不是真实图像。</p><p id="ed61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">看起来是这样的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mt"><img src="../Images/c9b85c9856bf6a82d64d6176c1eb9a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HVa4mq6V_xAuXBi7yaw7oA.gif"/></div></div></figure><p id="b971" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">选择的目标是<strong class="ig hi"> <em class="kz">极限速度40 </em> </strong>，我们看到它通过预测这个类别而不是像无目标攻击中的随机类别来有效地犯错误。</p><h1 id="4117" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="9d40" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">攻击机器学习模型不是一件容易的事情。在这篇文章中，我展示了一些攻击者在CNN上进行的<strong class="ig hi">白盒攻击</strong>。然而，当模型参数被隐藏时，这是非常困难的。</p><p id="20f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢它，并发现了一些新的方法来执行攻击！如果你对数据科学和机器学习感兴趣，可以在这里查看我的其他文章<a class="ae jc" href="https://www.npogeant.com/#articles" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="144d" class="md ju hh bd jv me mf mg jz mh mi mj kd ip mk ml kh it mm mn kl ix mo mp kp mq bi translated">资源</h2><div class="mu mv ez fb mw mx"><a href="https://www.manning.com/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">曼宁出版公司</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">Manning是计算机书籍、视频和课程的独立出版商。</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">www.manning.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl jn mx"/></div></div></a></div><div class="mu mv ez fb mw mx"><a href="https://art-demo.mybluemix.net/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">艺术- IBM研究</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">编辑描述</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">art-demo.mybluemix.net</p></div></div></div></a></div><div class="mu mv ez fb mw mx"><a href="https://neptune.ai/blog/adversarial-attacks-on-neural-networks-exploring-the-fast-gradient-sign-method" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">对神经网络的对抗性攻击:探索快速梯度符号方法</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">自发明以来，神经网络一直是机器学习算法中的精英。他们有…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">海王星. ai</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl jn mx"/></div></div></a></div><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/swlh/gradient-based-adversarial-attacks-an-introduction-526238660dc9"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">基于梯度的对抗性攻击:介绍</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">神经网络最近在大多数机器学习问题上提供了最先进的性能，甚至可以…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nn l ni nj nk ng nl jn mx"/></div></div></a></div><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hi fi z dy nc ea eb nd ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="no l ni nj nk ng nl jn mx"/></div></div></a></div></div></div>    
</body>
</html>