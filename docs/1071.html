<html>
<head>
<title>An illustration of next word prediction with state-of-the-art network architectures like BERT, GPT, and XLNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用伯特、GPT和XLNet等最先进的网络架构进行下一个单词预测的示例</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-illustration-of-next-word-prediction-with-state-of-the-art-network-architectures-like-bert-gpt-c0af02921f17?source=collection_archive---------0-----------------------#2021-09-25">https://medium.com/mlearning-ai/an-illustration-of-next-word-prediction-with-state-of-the-art-network-architectures-like-bert-gpt-c0af02921f17?source=collection_archive---------0-----------------------#2021-09-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="bc89" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated"><strong class="ak"><em class="iw"/></strong>使用Pytorch生成文本的动手演示</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/6979b3d480ed983156734e76bb64eea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXg3zEXnzRDzSBrYLKlnxA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image Source &amp; Credits: <a class="ae jn" href="https://kmkarakaya.medium.com/" rel="noopener">Assoc. Professor Murak Karakaya</a> [Image adapted &amp; Copyright, Requested permission]</figcaption></figure><p id="d5e7" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">嗨，朋友们！</p><p id="1129" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">欢迎来到自然语言处理(NLP)的世界。我们人类的一个重要方面是交流的能力。语言在我们相互交流的方式中起着重要的作用。我们的大脑是如何处理这些语言的？我们如何使用文字交流，这种交流是如何处理和解释的？这些都是我们可能面临的紧迫的基本问题。我不会过多地谈论<strong class="jq hi">神经语言学</strong> &amp; <strong class="jq hi">神经科学</strong>方面的内容，但我会鼓励所有感兴趣的读者浏览参考资料部分列出的参考资料。回到我们的关注点，<strong class="jq hi"> NLP通俗地说就是机器如何处理、分析和解释大量自然人类语言数据，即主要是机器和人类语言之间的交互</strong>。当前最先进的网络赋予了机器学习和模仿类似人类任务的能力。这不是很有趣吗？我们将通过一个演示来讨论一些重要的网络架构，看看对于给定的输入文本，我们如何使用这些语言模型来预测下一组单词。我们都非常熟悉<strong class="jq hi"> GMAIL-SMART COMPOSE </strong>功能，现在&amp;已经经常使用它，如下图所示。这是谷歌在2018年推出的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kk"><img src="../Images/c51aa276c5e309cfbe644c8bda1cef97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dqPwGtCPa5lbydoH4anOg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author: <strong class="bd kl">GMAIL-SMART-COMPOSE feature for automatic sentence completion</strong></figcaption></figure><p id="bcc6" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">在本教程中，我们将了解如何使用预训练的NLP模型，通过不同的先进深度神经网络模型来预测给定输入句子的下一组单词，例如</p><ul class=""><li id="66c8" class="km kn hh jq b jr js ju jv jx ko kb kp kf kq kj kr ks kt ku bi translated"><strong class="jq hi"> BERT </strong> —变压器的双向编码器表示</li><li id="4a4f" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj kr ks kt ku bi translated"><strong class="jq hi"> GPT </strong> —创成式预训练变压器</li><li id="210a" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj kr ks kt ku bi translated"><strong class="jq hi"> XLNET </strong> —使用自回归方法预训练的Transformer-XL模型</li></ul><p id="2b81" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">现在，如果你注意到，所有上述三种网络架构都基于<strong class="jq hi">变压器</strong>系列。那么，变压器到底是什么？转换器是许多神经网络设计中使用的组件，用于<strong class="jq hi">处理序列数据</strong>，如<strong class="jq hi">文本数据</strong>、<strong class="jq hi">基因组序列</strong>、<strong class="jq hi">声音信号</strong>或<strong class="jq hi">时间序列数据、</strong>等。变压器模型已经成为NLP任务的领跑者。因此，让我们快速浏览一下变压器模型的基础知识。关于这一点，有大量的材料、帖子和参考资料，所以我不打算详细讨论。让我们先了解一些事情，以获得一个基本的概述。</p><p id="8562" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">变压器网络的USP</strong></p><p id="7df0" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">变形金刚是一个非常有趣的深度学习架构家族，于2017年推出(谷歌大脑)。任何变压器架构的基本操作都是<strong class="jq hi">自关注</strong>操作。那么自我关注到底是什么呢？注意力机制使得这种网络架构能够超越典型的<strong class="jq hi">RNN</strong>或<strong class="jq hi"> LSTM </strong>模型的注意力限制。传统的<strong class="jq hi">序列到序列</strong>模型丢弃所有中间状态，并且在初始化解码器网络时仅使用最终状态/上下文向量来生成关于输入序列的预测。丢弃一切，但是当输入序列相当小时，最终的上下文向量工作正常。但是，当输入序列的长度增加时，使用这种方法时模型的性能会下降。这是因为很难将一个长的输入序列概括为一个向量。解决方案是增加模型的<strong class="jq hi">关注度</strong>，利用中间编码器状态为解码器构建上下文向量。<strong class="jq hi">因此，当为任何给定的标记创建编码时，注意力机制简单地定义了其他输入标记对模型的重要性。</strong></p><p id="f4be" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">为什么现在变压器架构更有意义？</strong></p><p id="b9d5" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">从2019年开始，谷歌搜索开始使用谷歌的transformer神经网络<strong class="jq hi"> BERT </strong>进行超过70种语言的搜索查询。在这一变化之前，许多信息检索是基于关键词的，这意味着谷歌在没有强有力的上下文线索的情况下检查其抓取的网站。以单词“bank”为例，它可以根据上下文有多种含义。谷歌搜索引入了transformer神经网络，这意味着谷歌可以更好地理解“从”或“到”等词影响含义的查询。用户可以用更自然的英语进行搜索，而不是根据他们认为谷歌能理解的内容来调整他们的搜索查询。来自谷歌博客的一个例子是查询“2019年巴西旅行者去美国需要签证。”单词“to”的位置对于正确解释查询非常重要。以前的Google Search实现不能捕捉到这种细微差别，并返回关于美国公民去巴西旅游的结果，而transformer模型返回更多相关页面。transformer架构的另一个优点是，一种语言的学习可以通过迁移学习转移到其他语言。谷歌能够采用经过训练的英语模型，并将其轻松应用于其他语言的谷歌搜索。</p><p id="a79d" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">此外，如果对任何计算机视觉问题感兴趣，那么我会鼓励检查视觉转换器模型如何用于分类任务<a class="ae jn" rel="noopener" href="/mlearning-ai/covid-non-covid-classifier-with-sota-vision-transformer-model-97375c774ff7">这里</a></p><p id="f3dd" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">SOTA概述:伯特、GPT、XLNET </strong></p><p id="a554" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hi"> BERT </strong> </a>是一个基于t<strong class="jq hi">transformer的</strong>语言模型，它通过联合处理左右两个&amp;上下文来学习从未标记文本中学习底层表示。它已经在维基百科和BooksCorpus上进行了预训练。</p><p id="fe60" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">如需更深入的详细信息，请查阅这些参考资料— <a class="ae jn" href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">链接</a>、<a class="ae jn" href="https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="abdb" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"><strong class="jq hi">GPT</strong></a><strong class="jq hi"/>是一个基于<strong class="jq hi">变换器的</strong>自回归语言模型，它以一种生成式的、无监督的方式进行预训练。它在大量未标记的文本(例如维基百科、书籍、电影剧本)上进行训练。该模型将学习估计任何给定单词序列的概率，甚至是它从未见过的单词序列。</p><p id="476a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">如需更深入的详细信息，请查阅这些参考资料— <a class="ae jn" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">链接</a>、<a class="ae jn" href="https://towardsai.net/p/latest/gpt-3-explained-to-a-5-year-old" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="b91a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"><strong class="jq hi">XLNET</strong></a><strong class="jq hi"/>是一种通用的自回归语言模型，它基于<strong class="jq hi">基于变换器的</strong>架构，通过递归输出一系列记号的联合概率，其中它使用置换语言建模来捕获双向上下文。它将最先进的自回归模型Transformer-XL的思想集成到预训练中。</p><p id="fd91" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">如需更深入的详细信息，请查阅这些参考资料— <a class="ae jn" href="https://researchdatapod.com/paper-reading-xlnet-explained/" rel="noopener ugc nofollow" target="_blank">链接</a>、<a class="ae jn" href="https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0" rel="noopener" target="_blank">链接</a>、<a class="ae jn" href="https://towardsdatascience.com/xlnet-explained-in-simple-terms-255b9fb2c97c" rel="noopener" target="_blank">链接</a></p><p id="eb11" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">下面提供了一个快速摘要</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es la"><img src="../Images/cb89a928219d9c4870bf35a760bb8258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPjsTuoooqe0XHuYCzLc5Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author: <strong class="bd kl">Side-by-Side Comparison of BERT, GPT, and XLNET</strong></figcaption></figure><p id="0b07" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">示例演示</strong></p><p id="7386" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">现在我们对这些神经网络架构有了一些非常基本的想法，让我们来看一个使用这些<strong class="jq hi">预训练的</strong> NLP模型来预测下一组单词的简单演示。在这里，我将使用pytorch框架来完成这项任务。但是，替代方案也可以使用其他深度学习框架，如tensorflow等。那么，我们开始吧。对于本教程，我们将使用下面的主包“<strong class="jq hi">火炬</strong>”、“<strong class="jq hi">变形金刚</strong>”。安装完库后，我们将导入所有必要的库，声明用于设置模型配置的变量，创建用于从用户处获取参数的函数，加载相应模型&amp;的相应标记器的函数，用于相应模型的编码和解码的函数，用于从所选模型获取所有预测的函数，用于预测输入文本结束后的单词集的函数，最后运行程序</p><p id="429b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤1)导入库</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="6c07" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤2)声明变量</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="1a95" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤3)创建作为用户输入的初始模型设置的函数</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="31f5" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤4)加载相应的模型和标记器</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="bedd" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤5)为各个模型创建编码和解码输入文本的函数</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="28bd" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤6)编码器和解码器的包装函数</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="8266" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">步骤7)我们程序的主要执行</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="2c67" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">伯特模型输出</strong> :-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ld"><img src="../Images/36ce40c90754a85c2e7313856d120794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UzXQk5yWb8hNQpzBtuImGw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author: <strong class="bd kl">Predicted next set of words by BERT</strong></figcaption></figure><p id="fb59" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi"> GPT模型输出</strong> :-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es le"><img src="../Images/8967a4c3efd9775d4e3500b35f5fbfd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTE6ptFCJUCFPrDvncXwjQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author: <strong class="bd kl">Predicted next set of words by GPT</strong></figcaption></figure><p id="428b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi"> XLNet模型输出</strong> :-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es le"><img src="../Images/327ee1bed2232f12502fd07734d68607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QpiqM5q-nKgUnOeq1YsdLg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author: <strong class="bd kl">Predicted next set of words by XLNET</strong></figcaption></figure><p id="1fa1" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">进一步待办事宜</strong></p><ol class=""><li id="8c69" class="km kn hh jq b jr js ju jv jx ko kb kp kf kq kj lf ks kt ku bi translated">用这些自然语言处理模型的实时推理开发一个gmail风格的网络应用程序。</li><li id="a97c" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj lf ks kt ku bi translated">交叉验证"<strong class="jq hi">预应变卷积比预应变变压器好吗？</strong>此处所指的<a class="ae jn" href="https://research.google/pubs/pub50306/" rel="noopener ugc nofollow" target="_blank">为</a></li><li id="d682" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj lf ks kt ku bi translated">使用<strong class="jq hi">预训练模型</strong> &amp; <strong class="jq hi">对您的<strong class="jq hi">自定义数据集</strong>进行微调</strong>(这个我留给读者去探索)</li><li id="ae85" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj lf ks kt ku bi translated">尝试更长的信息序列和不常用的句子，使用不在词汇表中的单词(这个我留给读者去探索)</li><li id="543f" class="km kn hh jq b jr kv ju kw jx kx kb ky kf kz kj lf ks kt ku bi translated">检查一下<strong class="jq hi">填充文本</strong>是否真的有助于<strong class="jq hi"> XLNet </strong>执行得更好，如果是的话，与没有任何填充相比(这个我留给读者去探索)</li></ol><p id="0123" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">结论</strong></p><p id="f748" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">预训练语言模型在很大程度上主导了自然语言处理的神经历史，尤其是基于<strong class="jq hi">转换器的</strong>预训练模型。那么，沿着这个方向的下一件大事是什么呢？我们仍然可以触及几个方面，如“下游NLP任务的机器推理”，“即兴视觉QAS”，等等。</p><p id="be74" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">以下是这篇文章的<a class="ae jn" href="https://github.com/ajayarunachalam/nlp_demo" rel="noopener ugc nofollow" target="_blank">完整代码</a>。</p><p id="ac0b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">联系人</strong></p><p id="d1b0" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">你可以在ajay.arunachalam08@gmail.com找到我</p><p id="e95e" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们上<a class="ae jn" href="https://www.linkedin.com/in/ajay-arunachalam-4744581a/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>连线吧，继续学习，干杯:)</p><p id="73da" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><strong class="jq hi">参考文献</strong></p><p id="eab9" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Language_processing_in_the_brain" rel="noopener ugc nofollow" target="_blank"> https://en .维基百科. org/wiki/Language _ processing _ in _ the _ brain</a></p><p id="65cf" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">https://search enterprise ai . tech target . com/definition/natural-language-processing-NLP</p><p id="6925" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Natural_language_processing</a></p><p id="66df" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://machinelearningmastery.com/natural-language-processing/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/natural-language-processing/</a></p><p id="467a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Transformer _(机器学习模型)</a></p><p id="cf43" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/BERT _(language _ model)</a></p><p id="8600" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/GPT-3</a></p><p id="dcc6" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.08237</a></p><p id="3d06" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://huggingface.co/transformers/model_doc/xlnet.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/xlnet.html</a></p><p id="db33" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335" rel="noopener" target="_blank">https://towards data science . com/what-is-xlnet-and-why-it-performs-Bert-8d 8 FCE 710335</a></p><p id="08b8" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710" rel="noopener" target="_blank">https://towards data science . com/xlnet-auto regressive-pre-training-for-language-understanding-7ea4e 0649710</a></p><p id="816f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">【https://pytorch.org/ T4】</p><p id="3bff" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/PyTorch" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/PyTorch</a></p><p id="d8ef" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://huggingface.co/transformers/model_doc/xlnet.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/xlnet.html</a></p><p id="624b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/xlnet-explained-in-simple-terms-255b9fb2c97c" rel="noopener" target="_blank">https://towards data science . com/xlnet-explained-in-simple-terms-255 b 9 FB 2 c 97 c</a></p><p id="a800" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://www.borealisai.com/en/blog/understanding-xlnet/" rel="noopener ugc nofollow" target="_blank">https://www.borealisai.com/en/blog/understanding-xlnet/</a></p><p id="a434" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://github.com/rusiaaman/XLNet-gen" rel="noopener ugc nofollow" target="_blank">https://github.com/rusiaaman/XLNet-gen</a></p><p id="5a29" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsai.net/p/latest/gpt-3-explained-to-a-5-year-old" rel="noopener ugc nofollow" target="_blank">https://toward sai . net/p/latest/GPT-3-向一个5岁的孩子解释</a></p><p id="7591" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03" rel="noopener" target="_blank">https://towards data science . com/Bert-for-dummies-step-by-step-tutorial-FB 90890 FFE 03</a></p><p id="bec4" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/gpt-3-explained-in-under-2-minutes-9c977ccb172f" rel="noopener" target="_blank">https://towards data science . com/GPT-3-2分钟内解释-9c977ccb172f </a></p><p id="cf7b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://360digitmg.com/gpt-vs-bert" rel="noopener ugc nofollow" target="_blank">https://360digitmg.com/gpt-vs-bert</a></p><p id="a118" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335" rel="noopener" target="_blank">https://towards data science . com/what-is-xlnet-and-why-it-performs-Bert-8d 8 FCE 710335</a></p><p id="af73" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec5-pretraining2.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Princeton . edu/courses/archive/spring 20/cos 598 c/lectures/le C5-pre training 2 . pdf</a></p><p id="d0fc" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0" rel="noopener" target="_blank">https://towards data science . com/xlnet-a-clever-language-modeling-solution-ab41e 87798 b 0</a></p><p id="f123" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" href="https://researchdatapod.com/paper-reading-xlnet-explained/" rel="noopener ugc nofollow" target="_blank">https://researchdatapod.com/paper-reading-xlnet-explained/</a></p><p id="019c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">【https://research.google/pubs/pub50306/ T2】号</p><p id="8f66" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated"><a class="ae jn" rel="noopener" href="/deep-learning-with-keras/text-generation-in-deep-learning-with-tensorflow-keras-e403aee375c1">https://medium . com/deep-learning-with-keras/text-generation-in-deep-learning-with-tensor flow-keras-e 403 aee 375 C1</a></p></div></div>    
</body>
</html>