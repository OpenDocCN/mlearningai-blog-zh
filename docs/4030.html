<html>
<head>
<title>Get Going with GloVe Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">着手进行手套嵌入</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/get-going-with-glove-embedding-53b69e14f97d?source=collection_archive---------3-----------------------#2022-11-27">https://medium.com/mlearning-ai/get-going-with-glove-embedding-53b69e14f97d?source=collection_archive---------3-----------------------#2022-11-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b60da81b509424b537897f049b1dbc30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u_cl2x3V5pZyMxBX"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@nickmorrison?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nick Morrison</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="eb8a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你想在你的项目中加入手套嵌入吗？各种术语给你带来麻烦了吗？恭喜你。你来对地方了。</p><p id="0225" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="js">注:</em> </strong> <em class="js">本文不讨论手套嵌入背后的数学。</em></p><p id="4ef9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将学习如何使用手套嵌入将任何文本数据转换成数字。我们将学习使用短文本语料库的步骤，然后我们将应用这些步骤来获得IMDB电影评论数据集的嵌入。我们将使用获得的嵌入在同一数据集上训练二元情感分类器。</p><p id="6146" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们开始吧！</p><h1 id="4696" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">简介</strong></h1><p id="c3f1" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">有各种预先训练好的手套单词嵌入可供下载。关于不同手套嵌入的训练语料库的更多信息可以在<a class="ae it" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <em class="js"> this </em> </a>网站上找到。在本教程中，我们将使用glovetwitter27b50d嵌入，它有50个维度，并在twitter的2B推文中进行了训练。</p><p id="d4a5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">嵌入可以作为文本文件使用，其中每一行都有一个包含单词及其矢量表示的字符串。我们将把这个文本文件的内容转换成一个字典。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="2f90" class="lf ju hh lb b be lg lh l li lj"># Read the text file<br/>glovetwitter27b50d = "pathe_to_glovetwitter27b50d.txt"<br/>file = open(glovetwitter27b50d)<br/>glovetwitter27b50d = file.readlines()<br/><br/><br/># Convert the text file into a dictionary<br/>def ConvertToEmbeddingDictionary(glovetwitter27b50d):<br/>    embedding_dictionary = {}<br/>    for word_embedding in tqdm(glovetwitter27b50d):<br/>        word_embedding = word_embedding.split()<br/>        word = word_embedding[0]<br/>        embedding = np.array([float(i) for i in word_embedding[1:]])<br/>        embedding_dictionary[word] = embedding<br/>    return embedding_dictionary<br/>embedding_dictionary = ConvertToEmbeddingDictionary(glovetwitter27b50d)<br/><br/># Let's look at the embedding of the word "hello."<br/>embedding_dictionary['hello']</span></pre><pre class="lk la lb lc bn ld le bi"><span id="1094" class="lf ju hh lb b be lg lh l li lj">Output:<br/>array([ 0.28751  ,  0.31323  , -0.29318  ,  0.17199  , -0.69232  ,<br/>       -0.4593   ,  1.3364   ,  0.709    ,  0.12118  ,  0.11476  ,<br/>       -0.48505  , -0.088608 , -3.0154   , -0.54024  , -1.326    ,<br/>        0.39477  ,  0.11755  , -0.17816  , -0.32272  ,  0.21715  ,<br/>        0.043144 , -0.43666  , -0.55857  , -0.47601  , -0.095172 ,<br/>        0.0031934,  0.1192   , -0.23643  ,  1.3234   , -0.45093  ,<br/>       -0.65837  , -0.13865  ,  0.22145  , -0.35806  ,  0.20988  ,<br/>        0.054894 , -0.080322 ,  0.48942  ,  0.19206  ,  0.4556   ,<br/>       -1.642    , -0.83323  , -0.12974  ,  0.96514  , -0.18214  ,<br/>        0.37733  , -0.19622  , -0.12231  , -0.10496  ,  0.45388  ])</span></pre><p id="654e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用下面的样本语料库来学习如何使用手套嵌入来转换任何文本数据集。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="389e" class="lf ju hh lb b be lg lh l li lj">sample_corpus = ['The woods are lovely, dark and deep',<br/>                 'But I have promises to keep',   <br/>                 'And miles to go before I sleep', <br/>                 'And miles to go before I sleep']</span></pre><p id="c897" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用Keras标记器从数据中提取标记。Tokenizer为每个标记分配一个索引，我们可以使用texts_to_sequences函数将任何文本转换成一系列索引。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="a427" class="lf ju hh lb b be lg lh l li lj"># This is the maximum number of tokens we wish to consider from our dataset.<br/># When there are more tokens, the tokens with the highest frequency are chosen.<br/>max_number_of_words = 5<br/><br/># Note: Keras tokenizer selects only top n-1 tokens if the num_words is set to n<br/>tokenizer = Tokenizer(num_words=max_number_of_words)<br/>tokenizer.fit_on_texts(sample_corpus)<br/>sample_corpus_tokenized = tokenizer.texts_to_sequences(sample_corpus)<br/>print(tokenizer.word_index)</span></pre><pre class="lk la lb lc bn ld le bi"><span id="379c" class="lf ju hh lb b be lg lh l li lj">Output:<br/>{'and': 1, 'i': 2, 'to': 3, 'miles': 4, 'go': 5, 'before': 6, 'sleep': 7, 'the': 8, 'woods': 9, 'are': 10, 'lovely': 11, 'dark': 12, 'deep': 13, 'but': 14, 'have': 15, 'promises': 16, 'keep': 17}</span></pre><pre class="lk la lb lc bn ld le bi"><span id="74a9" class="lf ju hh lb b be lg lh l li lj">print("But I have promises to keep: ", sample_corpus_tokenized[1])</span></pre><pre class="lk la lb lc bn ld le bi"><span id="241c" class="lf ju hh lb b be lg lh l li lj">Output:<br/>But I have promises to keep:  [2, 3]</span></pre><p id="f2cf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的句子被转换成两个标记的列表，因为我们已经将标记的最大数量设置为5。选择标记“I”和“to”的索引是因为它们在前4个频繁出现的标记中。</p><p id="51dc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">既然我们已经从文本语料库中选择了一组标记，我们必须为它们开发一个嵌入矩阵。嵌入矩阵将具有等于嵌入维度的<strong class="iw hi">列和等于记号数量</strong>的<strong class="iw hi">行。</strong></p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="2ada" class="lf ju hh lb b be lg lh l li lj"># Create embedding matrix<br/>total_number_of_words = min(max_number_of_words, len(tokenizer.word_index))<br/>embedding_matrix = np.zeros((total_number_of_words,50))<br/>for word, i in tokenizer.word_index.items():<br/>    if i &gt;= total_number_of_words: break<br/>    if word in embedding_dictionary.keys():<br/>        embedding_vector = embedding_dictionary[word]<br/>        embedding_matrix[i] = embedding_vector</span></pre><p id="1369" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上面的例子中，嵌入矩阵的维数是4×50，因为我们有4个记号，并且我们使用50维嵌入。</p><p id="75f8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">人工神经网络和最大似然算法不能处理可变长度的输入，因此我们需要将每个输入序列的嵌入转换为固定的长度。有许多方法可以做到这一点，但最简单的方法是对句子中每个标记的嵌入进行求和，并对向量进行归一化。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="fe53" class="lf ju hh lb b be lg lh l li lj">def convertToSentenceVector(sentences):<br/>    new_sentences = []<br/>    for sentence in sentences:<br/>        sentence_vector = []<br/>        for word_index in sentence:<br/>            sentence_vector.append(embedding_matrix[word_index])<br/>        sentence_vector = np.array(sentence_vector).sum(axis=0)<br/>        embedding_vector / np.sqrt((embedding_vector ** 2).sum())<br/>        new_sentences.append(sentence_vecto<br/><br/>sample_corpus_vectorized = convertToSentenceVector(sample_corpus_tokenized)<br/><br/># Print the 50-dimensional embedding of the first sentence in our text corpus.<br/>print(sample_corpus_vectorized[0])</span></pre><pre class="lk la lb lc bn ld le bi"><span id="7ee7" class="lf ju hh lb b be lg lh l li lj">Output:<br/>[-0.43196   -0.18965   -0.028294  -0.25903   -0.4481     0.53591<br/>  0.94627   -0.07806   -0.54519   -0.72878   -0.030083  -0.28677<br/> -6.464     -0.31295    0.12351   -0.2463     0.029458  -0.83529<br/>  0.19647   -0.15722   -0.5562    -0.027029  -0.23915    0.18188<br/> -0.15156    0.54768    0.13767    0.21828    0.61069   -0.3679<br/>  0.023187   0.33281   -0.18062   -0.0094163  0.31861   -0.19201<br/>  0.35759    0.50104    0.55981    0.20561   -1.1167    -0.3063<br/> -0.14224    0.20285    0.10245   -0.39289   -0.26724   -0.37573<br/>  0.16076   -0.74501  ]</span></pre><p id="ff08" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我们可以执行上面讨论的步骤来获得IMDB电影评论数据集的嵌入。</p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="ae4c" class="iu iv js iw b ix iy iz ja jb jc jd je lv jg jh ji lw jk jl jm lx jo jp jq jr ha bi translated"><strong class="iw hi">情感分类:IMDB电影评论数据集</strong></p></blockquote><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="1892" class="lf ju hh lb b be lg lh l li lj"># Read the data<br/>df = pd.read_csv("IMDB_Dataset.csv")<br/>X = df['review']<br/>y = df['sentiment']<br/># Convert labels to numbers<br/>le = LabelEncoder()<br/>y = le.fit_transform(y)<br/># Split the data into train and test sets.<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)</span></pre><pre class="lk la lb lc bn ld le bi"><span id="298f" class="lf ju hh lb b be lg lh l li lj">#Set the maximum number of tokens to 50000<br/>max_number_of_words = 50000<br/><br/># Tokenize training set<br/>tokenizer = Tokenizer(num_words=max_number_of_words)<br/>tokenizer.fit_on_texts(X_train)<br/>X_train = tokenizer.texts_to_sequences(X_train)<br/>X_test = tokenizer.texts_to_sequences(X_test)<br/><br/># Create embedding matrix<br/>total_number_of_words = min(max_number_of_words, len(tokenizer.word_index))<br/>embedding_matrix = np.zeros((total_number_of_words+1,50))<br/>for word, i in tokenizer.word_index.items():<br/>    if i &gt;= total_number_of_words: break<br/>    if word in embedding_dictionary.keys():<br/>        embedding_vector = embedding_dictionary[word]<br/>        embedding_matrix[i] = embedding_vector<br/><br/># Get a fixed-size embedding for every comment using the function defined earlier<br/>X_train = convertToSentenceVector(X_train)<br/>X_test = convertToSentenceVector(X_test)</span></pre><p id="2fef" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们训练一个人工神经网络。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="2d3e" class="lf ju hh lb b be lg lh l li lj"># Define a sequential model<br/>model = Sequential()<br/>model.add(Dense(100, input_shape = (50,), activation = "relu"))<br/>model.add(Dense(1000, activation = "relu"))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(1, activation = "sigmoid"))<br/>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.summary()<br/><br/># Fit the model<br/>model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1)<br/><br/># Get the classification report on the test set<br/>y_pred = model.predict(X_test)<br/>y_pred = y_pred.round()<br/>print(classification_report(y_test, y_pred))</span></pre><pre class="lk la lb lc bn ld le bi"><span id="6f02" class="lf ju hh lb b be lg lh l li lj">Output:<br/>              precision    recall  f1-score   support<br/><br/>           0       0.82      0.77      0.79      2553<br/>           1       0.77      0.82      0.80      2447<br/><br/>    accuracy                           0.80      5000<br/>   macro avg       0.80      0.80      0.80      5000<br/>weighted avg       0.80      0.80      0.80      5000</span></pre><p id="493d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由人工神经网络产生的分类报告不是很好，因为人工神经网络不能很好地处理序列数据。为了获得更好的结果，我们现在将在同一数据集上使用双向长短期记忆(LSTM)网络。双向LSTM是一种递归神经网络，可以处理不同大小的输入。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="d629" class="lf ju hh lb b be lg lh l li lj">max_number_of_words = 50000<br/>max_length = 100<br/><br/>tokenizer = Tokenizer(num_words=max_number_of_words)<br/>tokenizer.fit_on_texts(X_train)<br/>X_train = tokenizer.texts_to_sequences(X_train)<br/>X_test = tokenizer.texts_to_sequences(X_test)<br/><br/># We can use padding to make the length of every comment equal to max_length<br/>X_train = pad_sequences(X_train, maxlen=max_length)<br/>X_test = pad_sequences(X_test, maxlen=max_length)</span></pre><p id="3fb2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">先前生成的嵌入矩阵将被用作嵌入层中的权重。</p><pre class="kw kx ky kz fd la lb lc bn ld le bi"><span id="490f" class="lf ju hh lb b be lg lh l li lj"># Define a sequential model<br/>model = Sequential()<br/># Add an embedding layer and pass the embedding matrix as weights<br/>model.add(Embedding(max_number_of_words+1, 50, input_shape = (100,), weights=[embedding_matrix]))<br/>model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))<br/>model.add(GlobalMaxPool1D())<br/>model.add(Dense(50, activation="relu"))<br/>model.add(Dropout(0.1))<br/>model.add(Dense(1, activation="sigmoid"))<br/>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.summary()<br/><br/># Fit the model<br/>model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1);<br/><br/># Get the classification report<br/>y_pred = model.predict(X_test)<br/>y_pred = y_pred.round()<br/>print(classification_report(y_test, y_pred))</span></pre><pre class="lk la lb lc bn ld le bi"><span id="aa27" class="lf ju hh lb b be lg lh l li lj">Output:<br/>              precision    recall  f1-score   support<br/><br/>           0       0.87      0.83      0.85      2553<br/>           1       0.83      0.87      0.85      2447<br/><br/>    accuracy                           0.85      5000<br/>   macro avg       0.85      0.85      0.85      5000<br/>weighted avg       0.85      0.85      0.85      5000</span></pre><p id="b2f8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">呜哇！我们得到了更好的分类报告。</p><p id="d0bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，您对手套嵌入有了更好的理解，您已经准备好将其应用于各种NLP问题。</p><p id="c899" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="js">注:</em> </strong> <em class="js">你可以从</em> <a class="ae it" href="https://www.kaggle.com/code/adwaitkesharwani/get-going-with-glove-embedding" rel="noopener ugc nofollow" target="_blank"> <em class="js">这个</em> </a> <em class="js">链接访问完整的代码。</em></p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><h1 id="31c3" class="jt ju hh bd jv jw ly jy jz ka lz kc kd ke ma kg kh ki mb kk kl km mc ko kp kq bi translated"><strong class="ak">参考文献</strong>:</h1><ol class=""><li id="93e8" class="md me hh iw b ix kr jb ks jf mf jj mg jn mh jr mi mj mk ml bi translated"><a class="ae it" href="https://www.tensorflow.org/text/guide/word_embeddings" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/text/guide/word_embeddings</a></li><li id="c2fd" class="md me hh iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated"><a class="ae it" href="https://www.kaggle.com/code/jhoward/improved-lstm-baseline-glove-dropout" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/jhoward/improved-lstm-baseline-glove-dropout</a></li><li id="e48f" class="md me hh iw b ix mm jb mn jf mo jj mp jn mq jr mi mj mk ml bi translated"><a class="ae it" href="https://www.kaggle.com/code/abhishek/approaching-almost-any-nlp-problem-on-kaggle" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/abhishek/approximating-almost-any-NLP-problem-on-ka ggle</a></li></ol><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hi fi z dy mz ea eb na ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni in mu"/></div></div></a></div></div></div>    
</body>
</html>