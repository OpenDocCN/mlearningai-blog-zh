<html>
<head>
<title>Deep Learning can solve differential equations (theory &amp; pytorch implementation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Deep Learning可以解决微分方程(理论和pytorch实现)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/deep-learning-solves-differential-equations-better-than-any-other-numerical-method-14126c7a2a7c?source=collection_archive---------0-----------------------#2022-03-14">https://medium.com/mlearning-ai/deep-learning-solves-differential-equations-better-than-any-other-numerical-method-14126c7a2a7c?source=collection_archive---------0-----------------------#2022-03-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/75463ea1416a551720415b444f84b89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l95ebOvnk-i7Sq2yZTU4CQ.png"/></div></div></figure><p id="8507" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">嘿！欢迎阅读其他数学文章！在之前的一篇文章中，我描述了我的论文方法，这是一种受神经常微分方程启发的方法，其动机是<strong class="ir hi">如何使用神经网络进行外推</strong>、<a class="ae jn" href="https://sevent-christina.medium.com/a-different-approach-inspired-by-neural-odes-extrapolation-of-neural-networks-9766f846bd02" rel="noopener">https://seven t-Christina . media . com/a-不同的方法-受神经常微分方程启发-神经网络外推-9766f846bd02 </a>下面我们来看一看！我使用的一个非常有趣的方法是使用神经网络来求解ode。</p><h1 id="8c11" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">求解常微分方程和偏微分方程的人工神经网络</h1><p id="d39b" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">这种方法最早是由A. Likas，E. Lagaris和D. Fotiadis在论文<a class="ae jn" href="https://www.cs.uoi.gr/~lagaris/papers/TNN-LLF.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.uoi.gr/~lagaris/papers/TNN-LLF.pdf</a>中提出的，这是一种启发了很多人的前沿方法。尤其是现在关于<strong class="ir hi">科学机器学习</strong>的话题已经非常“热门”而且我们手中已经有了强大的工具来实现和测试，这篇论文更是必读。在这里，我用一个简单的方法来解释和实现本文！让我们深入探讨一下这个方法吧！</p><h1 id="6f63" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数学背景</h1><p id="89b6" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">在这里，我将介绍一个一阶常微分方程的例子，然而，它也很容易工作为较大的秩序常微分方程或偏微分方程。</p><ul class=""><li id="9fab" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated"><strong class="ir hi"> <em class="la">通用:</em> </strong></li></ul><p id="27fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在数值分析<em class="la"> </em>中，我们用以下形式表示ODE:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/6c6280b032c14de3deb38727f620f4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*v61Kl-S6r34QzTAR3AAjrw.png"/></div></figure><p id="c3fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，对于一阶颂歌，我们有</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lg"><img src="../Images/175ce89476c9161f92cd9930b57b69cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*-3dsZku05fYsuv3D1GFp5A.png"/></div></figure><p id="f469" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将研究本文的示例2:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/19d76bb2429d370e3c7d9408b5bcae2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*teb9uMRBAPwxhDxcFUS6kA.png"/></div></figure><p id="b93a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该一阶常微分方程的解析解为:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es li"><img src="../Images/82ef040d8e67be4c3afa1eb1bec6d1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*nCZKxOcw2yM8xZRyqBFp_w.png"/></div></figure><ul class=""><li id="523c" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated"><strong class="ir hi"> <em class="la">欧拉法:</em> </strong></li></ul><p id="575c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有很多已知的数值方法来解决一个微分方程。最流行的是欧拉方法。欧拉的基本思想是我们以均匀的方式在N个点上离散区域[α，b]，</p><p id="f271" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">[a=t0，t1，t2，t3，… tN=b]我们通过一个公式找到解，欧拉公式是:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/4880d4883632f1d6ee4fbd548c6b5fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*HcoSR2z4s7WF1iXMmF4HQQ.png"/></div></figure><p id="367c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的例子中，我们在[0，5]=[α，b]中有t，最后的公式是:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/eaf95cd8b0ba5eb766ca4918fc08e0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*eos2FcyDA3itAC3thK6FaA.png"/></div></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ll"><img src="../Images/6025c9a95ec11596a2ff3d1868bbffa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8PH__VDvO7Dt9MU-DcCmA.png"/></div></div></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/a0bf28b28e4db5569a22d546deff2d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRfwnNmsVyVO5BEnVsXN_A.png"/></div></div></figure><p id="9038" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的一个两分钟实现方法给出了一个额外的好结果！例如，如果我们有一个更复杂的微分方程，情况就不总是这样。幸运的是，在这种情况下，我们也可以使用一些方法，比如龙格库塔(Runge Kutta)或改进的欧拉(Euler)方法。</p><h1 id="41fd" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">建议的方法</h1><blockquote class="ln lo lp"><p id="e62e" class="ip iq la ir b is it iu iv iw ix iy iz lq jb jc jd lr jf jg jh ls jj jk jl jm ha bi translated"><strong class="ir hi">理论上:我们假设一个试验解依赖于一个神经网络。该神经网络将被训练成使得试解适合于ode的分析解。</strong></p></blockquote><ul class=""><li id="aee8" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">根据本文的等式(11)，对于具体的例子，我们有下面的y试验。N(x，θ)是要训练的神经网络。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/69a801659fe99e9f353742e1aa2a76b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*y4RiHT9QaoQTONwh1qD5og.png"/></div></figure><ul class=""><li id="1456" class="kr ks hh ir b is it iw ix ja kt je ku ji kv jm kw kx ky kz bi translated">损失函数:神经网络是连续和可微的函数，所以我们可以有一个神经网络的导数。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/f513297b300150542f51339a149fb80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*-Gj9sHzLZCGV7BNvH5aOvw.png"/></div></figure><blockquote class="ln lo lp"><p id="1d44" class="ip iq la ir b is it iu iv iw ix iy iz lq jb jc jd lr jf jg jh ls jj jk jl jm ha bi translated"><strong class="ir hi">实现:假设N(t，theta)是一个架构简单的神经网络。它有一个输入层，一个神经元对应于[a，b]的每个点ti，一个隐藏层有10个神经元，一个输出层有一个神经元对应于试解y(ti)，我们使用sigmoid作为激活函数。</strong></p></blockquote><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lv"><img src="../Images/1c24bfdea37951c68aa2df3071ce40cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJ1assDdlpzn4CMHS2w19w.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">I define the network and the derivative of the network by hand. Below is also the same network using the Sequential module of Pytorch and then use autograd package for the derivative. It is also very handy! I did it from scratch in order to dive deep into the paper’s method! The derivative comes from formulas (5) and (6) in the paper.</figcaption></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/8d2a7dd7cafe563393763a110205a645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7ESKyJR0cK9O0f2kb0y0Q.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">The definition of y trial, f function(the ode) and the loss function.</figcaption></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/723c45b1cfc634ea41d40d16528ad849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9c1tWyFxnF1ZhjS_P48rw.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">training of this simple Neural Network achieves great loss.</figcaption></figure><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/c3b523a2b9ada31ad860b3f267e98baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPEUmp4z610wm1p1_7NIuQ.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">final result</figcaption></figure><blockquote class="ln lo lp"><p id="4358" class="ip iq la ir b is it iu iv iw ix iy iz lq jb jc jd lr jf jg jh ls jj jk jl jm ha bi translated"><strong class="ir hi">实施亲笔签名。我们可以使用pytorch顺序模块，而不是手动定义神经网络和导数。</strong></p></blockquote><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ll"><img src="../Images/bb1f9b8f55a4ddcb6fa7056dc88e5ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYIuxeqy7VxZMu7c1Cye-w.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">The result using Sequential module, autograd and Adam or BFGS optimizer is the same and also very fast 👐</figcaption></figure><h1 id="4bef" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="99a9" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">我们可以使用神经网络来解决一个常微分方程或偏微分方程，我们可以很快获得很好的结果。对于更高阶的常微分方程或偏微分方程，想法是相同的，但在实现中有些事情发生了变化。</p><p id="b8ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的阅读！联系我以获得评论和更多讨论..:)</p><div class="md me ez fb mf mg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hi fi z dy ml ea eb mm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu in mg"/></div></div></a></div></div></div>    
</body>
</html>