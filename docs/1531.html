<html>
<head>
<title>Regularization : What? Why? and How? (part -2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化:什么？为什么？又是怎么做到的？(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/regularization-what-why-and-how-part-2-4a075ad68ad2?source=collection_archive---------4-----------------------#2021-12-31">https://medium.com/mlearning-ai/regularization-what-why-and-how-part-2-4a075ad68ad2?source=collection_archive---------4-----------------------#2021-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="0bca" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">这是我们两部分正规化讨论的第二部分，我鼓励你先看完第一部分；确保我们在同一页上。这是它的链接</p></blockquote><p id="551a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><a class="ae jj" rel="noopener" href="/@rsiddhant73/regularization-what-why-and-how-part-1-ef6bdb6bafea">https://medium . com/@ rsiddhant 73/regulation-what-why-why-how-part-1-ef 6 BDB 6 bafea</a></p><p id="5da4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">在前面的部分中，我们试图用一种非常通用的数学方法来理解这个主题，即使我们的由惩罚项和目标函数组成的组合目标函数也是非常通用的，那么，我们是否应该从机器学习的角度来研究正则化，以获得更专业的观点？让我们从我们离开的地方开始，即规范。</em></p><p id="e7cc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">什么是规范？</strong></p><p id="5782" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">它们一般代表矢量的大小/幅度。</em>L-p范数的一般形式是</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/fd13768afcda4682679370a6820f62b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*CUeKs_0Sfq4xVcn_jfDMsw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">L-p Norm equation</figcaption></figure><p id="f10b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">一些常用的规范有:</p><pre class="jl jm jn jo fd jw jx jy jz aw ka bi"><span id="d5a1" class="kb kc hh jx b fi kd ke l kf kg"><strong class="jx hi">L0 -&gt; number of zeros, though pointed as wrong definition in Ian Goodfellows’ Deep learning book.</strong></span><span id="6eef" class="kb kc hh jx b fi kh ke l kf kg"><strong class="jx hi">L1 -&gt; sum of absolute values of vector</strong></span><span id="824c" class="kb kc hh jx b fi kh ke l kf kg"><strong class="jx hi">L2 -&gt; sum of squared values of vector</strong></span><span id="ef48" class="kb kc hh jx b fi kh ke l kf kg"><strong class="jx hi">L-infinity -&gt; in general you can think it as max value in a vector or absolute value of element with largest magnitude in a vector.</strong></span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ki"><img src="../Images/a69bffa6056b310fed6ee62cd54a18b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*-BitjxZb0hb6Gkg2y31rpQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Different type of Norms</figcaption></figure><p id="e918" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">让我们讨论一些基于这些规范的<strong class="ik hi">流行的正则化技术。</strong></p><h2 id="da71" class="kb kc hh bd kj kk kl km kn ko kp kq kr jg ks kt ku jh kv kw kx ji ky kz la lb bi translated">L1正则化或套索</h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lc"><img src="../Images/d7a47af0aab1f959c2c4409437ca90c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*qr0CzbEpIT-CGDNz9au2Tw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">penalty increasing linearly with increase in w1</figcaption></figure><p id="156e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里，<strong class="ik hi"><em class="ij">P(W)=</em></strong><em class="ij">β</em>*<strong class="ik hi"><em class="ij">∑| W |</em></strong></p><p id="9806" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">也就是说。<em class="ij">绝对权重之和</em>，表示<em class="ij">罚值随着参数值</em>线性增加。但是L1最重要的方面是它执行<em class="ij">特征选择</em>的能力，我们将在后面的章节中讨论这一点。</p><h2 id="fb2b" class="kb kc hh bd kj kk kl km kn ko kp kq kr jg ks kt ku jh kv kw kx ji ky kz la lb bi translated">L2正则化或岭</h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ld"><img src="../Images/1d7bd2a1787ab61c2e5c55f486d09d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*LpLmYakN9ecUm_KbSIXr4Q.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">penalty increasing quadratically with increase in w1</figcaption></figure><p id="6482" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里，<strong class="ik hi"><em class="ij">P(W)=</em></strong><em class="ij">λ</em>*<strong class="ik hi"><em class="ij">∑W</em></strong></p><p id="d2e0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">也就是说。<em class="ij">权值的平方和</em>，这意味着<em class="ij">罚值随参数值</em>的平方增加。但是L2最重要的方面是它的平均效果和重量分配。随着惩罚越来越重，限制也越来越严格，因此<em class="ij">通常在实际场景中更受青睐</em>。</p><p id="de59" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij"> L1也被称为曼哈顿距离，l2被称为平方距离或欧几里得距离。</em></p><h2 id="1e7e" class="kb kc hh bd kj kk kl km kn ko kp kq kr jg ks kt ku jh kv kw kx ji ky kz la lb bi translated">弹力网/ L1 + L2</h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/9af1f3416b99893f8d9d6b6ddd665dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*4OB94ndhO8ZUauC2SE9OWg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">graph of elastic net, penalty is somewhere between L1 and L2, it is neither a straight line nor quadratic by nature, thus no problem with quadratic changes, also it is lot more more robust</figcaption></figure><p id="ef93" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> <em class="ij">这里P(W)=</em></strong>【λ*<strong class="ik hi"><em class="ij">∑W+</em></strong>【β*<strong class="ik hi"><em class="ij">∑| W |</em></strong></p><p id="b31f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">它基本上<em class="ij">结合了L1和L2 </em>，非常类似于我们在推导组合目标函数时所做的。你应该已经从你的作品中得出了这个，伟大的作品。它提供了L1和L2的好处，但也有一些折衷。有许多关于这方面的好文章，我鼓励你去探索一下。<em class="ij">一般来说λ &lt; β，就像我们要更强调L1一样，为什么？我们将在本文的后面讨论这个问题。</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lj"><img src="../Images/27af0bd98a1119a52fd8de0ff3df1a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*Mjyeol6YSUn62-DPKPbxqA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">All the generally used regularizations</figcaption></figure><p id="ba71" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">从上图可以看出，Lasso的<strong class="ik hi"> <em class="ij">特征选择概率最高，其次是elastic，最后是Ridge的</em> </strong>。<em class="ij">在完成下一节关于稀疏性和特征选择的内容后，你会对图形w.r.t .特征选择有更好的理解。</em></p><p id="6e27" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">有人可能会问，为什么我们不用l2以外的规范，可能有几个可能的原因，</p><p id="26ac" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">1.非常<em class="ij">大罚</em>超出二次项。</p><p id="4c96" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">2.它开始形成某种方方正正的形状，这扼杀了我们将一切都限制在一定范围内的动机，因为这些目标函数的最大化可能会超过我们的阈值，这是我们不希望的，因此像这样的形状也会<em class="ij">抑制我们对稀疏性的渴望</em>。</p><p id="0255" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">3.对于L-3范数，我们的惩罚项的导数将是二次的，这将导致梯度非常<em class="ij">快速衰减</em>，甚至比遵循线性衰减的L2快得多，这将随着范数的增加而变得越来越陡，这将导致梯度非常<em class="ij">突然变化，使整个设置非常敏感，从而不稳定</em>，我确信这不是我们想要的。在后面的部分有一个跟进图。</p><p id="b0fd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">让我们简单讨论一下<strong class="ik hi">其他著名的正则化技术</strong>:</p><p id="0884" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> <em class="ij"> Dropout : </em> </strong>它通过提升稀疏度来执行<em class="ij">正则化，我们可以将其形象化为，假设存在一个大型图(我们的神经网络)，在每次迭代中随机使用Dropout后，一些激活将被设置为0，这将<em class="ij">在该大型图中创建一个子图。许多这些子图会导致某种集合</em>，我们知道集合如何帮助<em class="ij">减少方差</em>等等。几乎没有数学证据表明辍学是如何类似于L2标准的。</em></p><p id="cd3b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"/></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lk"><img src="../Images/3436393e9fe6d4ccd86a0d0338252e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*YLm5T5lCy_AzE5Tg9wDO8w.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">batch-normalization equations</figcaption></figure><p id="12f8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">第三个等式中分母的这个<strong class="ik hi"> <em class="ij"> ϵ </em> </strong>是额外的值，用于防止一些讨厌的数学复杂情况，如零除法，但暗地里它也充当噪声。主要噪声实际上是由计算出的平均值和方差造成的，而不是真实的统计数据，即整个数据，因此存在一些标准误差，进而导致噪声。如果数据中有噪声，在任何情况下你都无法拟合采样分布，因此过度拟合的机会减少，正则化应用。</p><p id="27cb" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> <em class="ij">早停:</em> </strong>其实最容易理解，简单来说，你训练的时间越长，越有可能超负荷。那么，我们该怎么办呢？如果指标没有明显变化，尽早停止？对吗？对，就是这样。</p><p id="4ca5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">还有更多，但我们留下一些东西供你探索。</em></p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><h1 id="2dba" class="ls kc hh bd kj lt lu lv kn lw lx ly kr lz ma mb ku mc md me kx mf mg mh la mi bi translated">稀疏性和特征选择:神话与真理</h1><p id="4f04" class="pw-post-body-paragraph ih ii hh ik b il mj in io ip mk ir is jg ml iv iw jh mm iz ja ji mn jd je jf ha bi translated">让我们进入主题，我一直在等待解释，稀疏使用正则化。</p><p id="ba7e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">什么是特征选择的稀疏性？你可以这样想，如果特征选择正在发生，那么<em class="ij">少数特征被挑选，而其他特征被拒绝，即对应于那些特征的权重/参数被推到0 </em>，从而产生稀疏的权重矩阵/向量。稀疏矩阵是主要由零值组成的矩阵。因此，如果存在特征选择器，其输出将是稀疏向量/矩阵。</p><p id="ed7e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">主要问题是，为什么L1做特征选择，而l2不做？</p><p id="14d4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">一个明显的答案是，L1本身并不促进特征选择，其他规范在这项任务中非常弱，以至于L1看起来像一个专门的特征选择器<em class="ij">(除了</em> L0 <em class="ij">规范，它是促进稀疏性的理想选择)</em>。</p><p id="a0f1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我这话是什么意思？</p><p id="f993" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我来解释一下，给你两种思考方式，</p><p id="463c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> <em class="ij"> 1。数学</em> </strong></p><p id="5989" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我们将讨论P对w的导数。</p><p id="371c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">对L2来说，</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mo"><img src="../Images/32d17d4a7d06af69f21dae9d69b47fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*Zdrws2k7BGb9x2S-db0CQQ.png"/></div></figure><p id="28ed" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这意味着在L的导数和W的导数之间存在线性关系，</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mp"><img src="../Images/9424cab2f437c0a6300fb06ff74ee8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*cT7HH0vm7ZXhObj1KeiXtg.png"/></div></figure><p id="363a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">因此当W变小时。惩罚也变小了，这<em class="ij">导致了一个递减/衰减的惩罚</em>。这就是为什么<em class="ij"> l2正则化也被称为权重衰减</em>。现在，由于存在衰变，我们无论如何也不能达到0。</p><p id="1fc1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">你可以这样想</p><pre class="jl jm jn jo fd jw jx jy jz aw ka bi"><span id="c463" class="kb kc hh jx b fi kd ke l kf kg">w = 16</span><span id="78de" class="kb kc hh jx b fi kh ke l kf kg">while w != 0:</span><span id="cc25" class="kb kc hh jx b fi kh ke l kf kg">     w = w/2</span></pre><p id="6c85" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这是一个无限循环，因为w将继续衰减/减少，并且永远不会等于零。片段和实际的解释之间没有直接的关联，这只是将你的理解推向那个方向。好的，我们知道l2永远不会将w推到精确的零，因为这不可能进行特征选择，<em class="ij">对于更高的规范可以推导出类似的想法</em>。</p><p id="f4ae" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">但是L1呢，让我们看看，</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mq"><img src="../Images/d851b0d6adbf48063b55e91ff28d8357.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*j4TIUUztkkaiWLUfNflgKg.png"/></div></figure><p id="84fb" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这意味着无论W 的大小如何<em class="ij">恒定递减。</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mr"><img src="../Images/2f9e0608002adfd505782cac8431d634.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*UnD_OZ9itXVymntw4kKtAA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">constant updates by L1-norm</figcaption></figure><p id="08ef" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">因此，与l2 相比，W有更多的机会被推到0。</p><p id="81d8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">满意吗？没有吗？太好了！！！</p><p id="3075" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> <em class="ij"> 2。目测</em> </strong></p><p id="fcb3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">欢迎来到视觉化部分，这会让事情变得更清楚。</p><p id="8ae1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">假设我们的L2范数的圆周(一个圆)是由下面标记为红点的100个点构成的。在多少点上我们能够最大化这个函数？在每个地方，对吗？为<em class="ij">，所有点与中心</em>等距。你能在多少点上做<em class="ij">特征选择？</em>，即在四个位置将其中一个参数设为0，<em class="ij">。这些位置在下面用蓝色突出显示。那么，你在所有的点中选择四个点之一的概率是多少？4/100，实际上一个圆是由无限多的点组成的，那么我们的概率大约是4/N，其中N非常非常大，将特征选择的整个概率推向0。</em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mr"><img src="../Images/486f516303e2af11b768c1a6f16da8fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*OI9qaULqH3OQML08yFhDwQ.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Red points are all the locations at which P(W) would maximize, and blue points are location where feature selection could take place as one axis is 0 completely. There is very slight overlap. In reality these red points are very large in number, i.e. tending to infinity.</figcaption></figure><p id="05ee" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这意味着<em class="ij">当P(W)增长时，发生特征选择的那4个点不太可能是与分布</em>(还记得我们讨论过的甜蜜点)接触的第一个点，因为有N个点具有最大化概率<em class="ij"> P(W) = w + w . </em></p><p id="c10a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">那么L1呢，它很特别吗？，让我们弄清楚。<em class="ij">能使L1最大化的点数只有4个</em>并且令人惊奇的是<em class="ij">它们都位于会导致特征选择的位置，因为轴上的一个参数值被推到0</em>。因此，<em class="ij">如果目标最大化</em>，进行特征选择的概率非常高，即1。因此，如果L1能够最大化自己，那么它肯定只会选择少数几个特征，而抑制其他特征。见下图。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ms"><img src="../Images/178194f68fdca0b7c3399f04cbe2003a.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*LD3Dm68p1t75nYzhZGxQtA.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Red points are all the locations at which P(W) would maximize, and blue points are location where feature selection could take place as one axis is 0 completely, each red and blue point is overlapping.</figcaption></figure><p id="642f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这意味着<em class="ij">当P(W)将增长时，很可能发生特征选择的那4个点中的任何一个都将是与分布</em>(还记得我们谈论过的甜蜜点)接触的第一个点，因为只有4个点具有最大化P(W)的最高概率，由| <em class="ij"> w1|+|w2| </em>给出。</p><p id="b589" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我认为这将使我关于L1的特征选择的论点具体化。因此，L1比L2更擅长特色选择。</p><p id="17f0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">如果我们谈论弹性网络，使用我们在上面看到的相同的论点，<em class="ij">弹性网络是比L2更好的特征选择器，但是，在现实中，它不能生成稀疏特征集</em>，简单的原因实际上存在于公式中，因为两个项(L1和L2)被添加，因此w的零值是不可能的，正如我们在上面看到的；L2不会轻易给出0值。那我为什么说它在功能选择上比L2好呢？简单地说，因为它提出了比L2更严格的限制，<em class="ij">记得我们看到λ(L2的正则化常数)&lt;β(L1的正则化常数)，因此L2被给予较低的权重，并且不能对组合的解决方案产生太大影响</em>，正因为如此，不相关特征的值变得几乎可以忽略不计(因为L1已经将其推至0，尽管L2在那里，但是小得多)，这通常不会在L2发生。</p><p id="efbe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">让我们通过一个小例子来理解L1正则化和L2正则化之间的区别，</p><p id="cd45" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">假设给你一个任务，告诉你一个球员是否会被选入篮球队。我们收到的数据有3个特征(球员的身高，球员的头发颜色，球员的婚姻状况)，为了简单起见；让我们假设，只有身高是一个重要的因素，其余两个特征是无用的。</em></p><p id="3473" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">在约束优化下训练之后，输出y可以被看作是，</p><p id="3c51" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">对L2来说，</p><p id="08e8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">W = [1.88，0.18，0.04]</p><p id="95e3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij"> y=σ(W.X+B) </em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mt"><img src="../Images/3658668c9c944e1614dfcbb18267ddbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZ_BAXOefUWxedZyZeRIxw.png"/></div></div></figure><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lj"><img src="../Images/e51706aa457285166197b8d93419cce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*i0mBhbJVHd_rCeEVYIVC0g.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">red axis-&gt;height, green axis-&gt; hair color, blue axis-&gt;marriage status</figcaption></figure><p id="522b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">您可以清楚地看到，重要的特征被赋予较大的权重，而不重要的特征被赋予较低的值，但仍然有一些权重被赋予它们，这在少数情况下是可以接受的，但如果我们想要选择特征，这并不理想。</p><p id="943d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">对L1来说，</p><p id="f9bd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">W = [1.55，0.0，0.0]</p><p id="62f8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij"> y=σ(W.X+B) </em></p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mu"><img src="../Images/4048a6056f1ea34b33e94081306a7c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9yWr97UMZg8mdbfgPx-cw.png"/></div></div></figure><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lj"><img src="../Images/87e3e1cb6e7290700dd2c4b0ea63a26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*ZTXLW7eLqv8kc40zp9Me9Q.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">red axis-&gt;height, green axis-&gt; hair color, blue axis-&gt;marriage status</figcaption></figure><p id="875e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">在这里，L1能够选择重要的特征而完全忽略其他特征。这使得模型对噪声更加鲁棒；因为它对额外的特征不敏感。</p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><p id="296f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我们谈到了2D空间中的正则化，如果你需要一点方向来思考高维空间中的所有这些，下面是如果我们有两个以上的特征，我们的约束/L-p范数看起来会是什么样子的表示。这是一个有三个特点的视觉。到目前为止，我们讨论的每件事都保持不变。现在，我们的W将在所有三维空间中增长，数据分布也将在三维空间中。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mv"><img src="../Images/2c3047da74c6cafed327a396884eb327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gBAgf4CrucYSq8cYJfXNA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">L-p norms in 3d</figcaption></figure></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><p id="cfe7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">结论 <br/> <em class="ij">这真是一次漫长的旅行，但我们学到了很多，不是吗？让我们快速浏览一下我们在这篇文章中看到的内容。我们从过拟合和无约束优化开始，然后我们在它们之间建立了联系，同时还讨论了一些重要的事情，如复杂性以及如何防止它。从那里，我们给出了优化及其各种组件的鹰眼视图。基于我们对最优化中凸性的了解，我们导出了我们自己的拉格朗日函数，它后来成为我们的一般正则化方程。在那之后，我们快速地浏览了一般的定义，然后理解了稀疏性和范数。然后我们用一些证据来证明特征选择是如何在L1而不是在L2发生的，接着是一些规范的3d表示来满足我们可视化的渴望。</em></p></div><div class="ab cl ll lm go ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ha hb hc hd he"><p id="4ebe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">到此为止，我们结束了这次讨论，这是一次信息的过山车，我希望你能学到一些新的东西，最重要的是以不同的方式。<em class="ij">正规化本身还有很多需要理解和探索的地方，假设你从这里学到的东西是基础</em>，我会鼓励你去寻找。感谢你有足够的耐心通读这篇文章，是的，再次拍拍你的肩膀，你今天学到了一些新东西。</p><p id="339c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">将来的某个时候，我会赶上一篇新文章。在那之前，继续探索。再见。</p><div class="mw mx ez fb my mz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hi fi z dy ne ea eb nf ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">medium.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn jq mz"/></div></div></a></div></div></div>    
</body>
</html>