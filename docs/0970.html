<html>
<head>
<title>Adversarial Attacks: Introduction and Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性攻击:介绍和例子</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/adversarial-attacks-introduction-and-example-3224ccffc720?source=collection_archive---------3-----------------------#2021-09-05">https://medium.com/mlearning-ai/adversarial-attacks-introduction-and-example-3224ccffc720?source=collection_archive---------3-----------------------#2021-09-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3d97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">攻击机器学习模型。把猫变成柠檬。</p><h1 id="99e4" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">概念</h1><p id="8b36" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">对立的例子是特别设计的输入，被认为是欺骗机器学习(ML)模型，导致高置信度的错误分类。然而，有趣的是，对图像所做的修改是温和的，但却足以欺骗ML模型。在这篇文章中，我将展示微小的变化可能会导致灾难性的后果。下图总结了对抗性攻击的过程:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es kf"><img src="../Images/69200e68f33d59a60ff842c5963e1126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*KiE1zjt3VcJAHDzYm0D4MQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx">Image by the author.</figcaption></figure><p id="15ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑上面的一幅猫的图像，我们添加了一个小扰动，这个小扰动已经被计算过，以使图像被高置信度地识别为柠檬。更具体地说，我们将获取图像，并计算相对于所需标签的损失，在本例中是“柠檬”。因此，我们可以得到为输入图像计算的梯度的符号，并将其乘以某个小常数ε。经过多次这样的迭代，我们能够得到猫的图像，使我们的ML模型以很高的可信度将其归类为柠檬。</p><p id="b66b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种方法非常健壮，而且简单易懂。然而，对立的例子可能是非常危险的。例如，攻击者可能会让我的人工智能柠檬水制作机器人挤压我的猫，然后制作另一种柠檬水。那会很难过:(</p><h1 id="cebe" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">例子</h1><p id="17cf" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">举个例子，我要考一个预先在Imagenet上培训过的ResNet50。列表上总共有1000个类，我使用一只暹罗猫作为初始输入，我想要的标签是柠檬。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/1c1d458404d3f28b70f38af3bdb4fca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KxirB1HgV673fA5gOnsFGw.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">Image by the author.</figcaption></figure><p id="9acc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如所见，模型正确地将我的图像分类为“暹罗猫，暹罗猫”。注意，由于图像的尺寸大于用于训练的尺寸，所以置信度较低。现在，我们将试图欺骗我们的模型，将其归类为柠檬。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="412b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我用来做预测的辅助函数。输入是我的PIL猫的图像。它接受我的输入并打印出预测的类及其概率。</p><figure class="kg kh ki kj fd kk"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="0de1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我已经描述过的，我如何攻击的过程被总结在“攻击”方法中。我运行这个函数10次，这足以让我们的ResNet50把它误归类为一个柠檬。注意，我们只取梯度的符号，要么是1，要么是-1，然后乘以ε，也就是1e-6。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ky"><img src="../Images/fbf4b245b6caf387033d5fe8ca371204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AJaASahkbV1GFu9pm3waQ.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">Image by the author.</figcaption></figure><p id="3d89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">瞧！我实现了我们的目标。该模型现在以非常高的概率将我们的猫归类为柠檬，但是我们可以清楚地看到图像在视觉上仍然是一只猫。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kz"><img src="../Images/f8240dbc53eda83affbebe168649d82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bjKFtqmolC7lxcxZ"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">by <a class="ae la" href="https://unsplash.com/@plhnk" rel="noopener ugc nofollow" target="_blank">Paul Hanaoka</a> via <a class="ae la" href="https://unsplash.com/photos/w2DsS-ZAP4U" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="47b4" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">一些遗言</h1><p id="45d5" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">如你所见，对抗性攻击非常简单有趣。然而，这可能是潜在的危险，它可能会质疑人工智能的可靠性。这是最近的一个主要研究领域。强化学习代理也可以被对立的例子操纵。我会通过OpenAI留下一个有用的博客。如果我设法让你感兴趣，我鼓励你自己研究它。感谢您花时间阅读我的文章。保持自信，不要让小变化影响你:)</p><div class="lb lc ez fb ld le"><a href="https://openai.com/blog/adversarial-example-research/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab dw"><div class="lg ab lh cl cj li"><h2 class="bd hi fi z dy lj ea eb lk ed ef hg bi translated">用对立的例子攻击机器学习</h2><div class="ll l"><h3 class="bd b fi z dy lj ea eb lk ed ef dx translated">敌对的例子是攻击者有意设计的机器学习模型的输入，以引起…</h3></div><div class="lm l"><p class="bd b fp z dy lj ea eb lk ed ef dx translated">openai.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls kl le"/></div></div></a></div></div></div>    
</body>
</html>