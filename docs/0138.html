<html>
<head>
<title>A Comprehensive Guide to Transformers (Part 1: The Encoder)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器综合指南(第一部分:编码器)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-comprehensive-guide-to-transformers-part-1-the-encoder-187f6b58bce0?source=collection_archive---------1-----------------------#2021-02-19">https://medium.com/mlearning-ai/a-comprehensive-guide-to-transformers-part-1-the-encoder-187f6b58bce0?source=collection_archive---------1-----------------------#2021-02-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a306" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">对颠覆NLP的模型的深入研究(带代码)</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/c82a2d0972f60ec944bc354cdaef757b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GEvlxdOdoZJolQfU"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@randvmb?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Randy Jacob</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8f10" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">递归神经网络在2017年年中非常棒。他们能够做一个成功的序列模型应该做的一切，尽管有一些缺点。然后变形金刚(<a class="ae jm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你需要的全部</a>)出现了，很快NLP中的每一个最先进的模型都是变形金刚。最近，随着OpenAI的<a class="ae jm" href="https://openai.com/blog/image-gpt/" rel="noopener ugc nofollow" target="_blank"> ImageGPT </a>和<a class="ae jm" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank"> DALL-E </a>，变形金刚也越来越多地被用作图像的自回归模型。越来越清楚的是，变形金刚是非常通用的模型，可以应用于许多任务，而不仅仅是文本。让我们先从高层次了解一下变压器的工作原理。</p><h1 id="5d05" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">高级直觉</h1><p id="c23f" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">考虑转换器最简单的方式是编码器-解码器模型，它可以处理序列内部和序列之间的成对连接。这意味着转换器可以理解任何两个序列之间的联系，只要它们在编码器和解码器端的标记之间有某种联系。例如，这意味着转换器可以很容易地在任何两种语言之间进行翻译，因为每种语言在句子中的单词之间都有关系。另一个例子是如何使用变压器自回归预测图像中的下一个像素。</p><p id="2a31" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这也被证明在字幕模型中有效，例如<a class="ae jm" href="https://arxiv.org/pdf/1912.08226.pdf" rel="noopener ugc nofollow" target="_blank">网状记忆变压器</a> (Cornia <em class="lg"> et。艾尔。</em>)其中转换器在图像中的对象和英语语言之间进行翻译。由于图像中的对象有成对的关系，所以transformer非常适合这项任务。稍后将会清楚为什么变形金刚对成对连接的序列很有用。</p><h1 id="26a0" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">深潜</h1><p id="ce7b" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">现在让我们进入变压器的内部。转换器的特征在于其不同的训练和推理过程。在训练期间，它同时计算所有输出，而在推理期间，它顺序计算输出，与传统的RNNs没有什么不同。然而，在训练过程中，RNN会花费更多的时间，变形金刚可以使用他们的新架构来加速训练，变得更有效率。在本文中，我们将深入讨论编码器架构，而在下一篇文章中，我们将讨论解码器以及编码器和解码器之间的关系。下面的代码是一个在英语和德语之间转换的转换器。</p><blockquote class="lh li lj"><p id="ba46" class="jn jo lg jp b jq jr ii js jt ju il jv lk jx jy jz ll kb kc kd lm kf kg kh ki ha bi translated">接下来的所有代码都取自这里的<a class="ae jm" href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" rel="noopener ugc nofollow" target="_blank">和</a>，并将进行深入解释。</p></blockquote><p id="2d1f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们将从进口开始。当它们出现时，我会一一解释。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="2bd8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下一个单元格处理随机播种。这个单元格确保所有随机调用在笔记本开始时是一致的。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="7e20" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们在这里下载spacy词汇表(如果你愿意，你可以改变语言，但是现在我们使用德语)。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="b778" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这个单元中，我们使用标记化函数来获取两种语言中的一个句子，并将其分解成单词。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="3d7c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下面的单元格有torchtext字段对象。field对象是一个非常通用的工具，可用于创建词汇表，将单词转换为整数标记索引，以及将标记索引转换为单词。本质上，它为您提供的所有单词创建了一个查找表，每个单词恰好对应一个令牌索引。以这种方式，不是将字编码为非常稀疏并且会浪费存储器的独热向量，而是将其编码为单个整数，作为其对应的独热向量中的1的索引。参数<em class="lg"> batch_first </em>意味着转换器的输入将是【T4(N，S)】的形状，其中<em class="lg"> N </em>是批次大小，<em class="lg"> S </em>是序列长度。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="fdce" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们可以加载数据了。对于这个特定的示例，我们将加载Multi30k数据集，并将字段设置为我们之前指定的字段。这样，当我们创建训练、测试和验证数据集时，它们将与我们的自定义字段一起预构建。有了数据之后，我们可以用数据集中的词汇初始化字段。<em class="lg"> min_freq </em>参数只是一个过滤器，任何在数据集中只出现一次的单词都不会包含在词汇表中。当模型遇到一个对它来说是陌生的单词时，它会用一个<em class="lg"> &lt; unk &gt; </em>标记来表示它。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="1ac0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下面是初始化设备、批处理大小和迭代器的代码(在torchtext中它们被称为迭代器，而在torch中它们被称为数据加载器)。与我们上面初始化的数据集不同，迭代器不能被索引，所以在训练期间，通常对它们调用一个<em class="lg"> enumerate() </em>。我们使用的这种特殊类型的迭代器叫做桶迭代器。本质上，它将数据(句子)分成几批，这样需要的填充量最少。为什么我们需要衬垫？简单地说，因为所有的句子长度不一样，把它们分成几批需要我们在较短的句子上加一些填充。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="dd10" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们开始研究构成转换器的方法和类。我们从这个<em class="lg">positionwiseedforward layer</em>类开始，它由两个线性层组成。它的目的是将输入维度(<em class="lg"> hid_dim </em>)转换成一个大得多的维度(<em class="lg"> pf_dim </em>)，然后将其转换回未来层的输入维度。论文中没有提到这背后的原因，但这背后的直觉是，引入更多的神经元可以刺激模型在相同的维度上表示不同的信息。之后，应用一个脱落层来阻止过度拟合。输入大小为<strong class="jp hi"> (N，S，H) </strong>，其中<em class="lg"> N </em>为批量大小，<em class="lg"> S </em>为序列长度，<em class="lg"> H </em>为隐藏维度大小。在两个完全连接的层之间，隐藏尺寸将改变为<em class="lg"> pf_dim </em>大小，并在最后一个完全连接的层之后恢复为<em class="lg"> H </em>。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="a52e" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">注意力</h1><p id="9da1" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">变形人模型的DNA，多头注意力允许变形人推断句子中标记之间的关系。要理解多头注意力，首先必须理解成比例的点积注意力。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lp"><img src="../Images/0cc3aa5232d355c615b6076d474013d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*1MFnkYIOo9JQmba5mUMkrQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source: <a class="ae jm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="ff03" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在比例点积注意中，输入由一个查询矩阵、一个键矩阵和一个值矩阵组成。接下来，将查询矩阵和转置的关键字矩阵相乘。这导致大小为<strong class="jp hi"> (N，Q，H)</strong><strong class="jp hi">(N，H，K) </strong> = <strong class="jp hi"> (N，Q，K) </strong>，其中<em class="lg"> Q </em>和<em class="lg"> K </em>分别是查询序列和键序列的长度。直观上，这种乘法产生了查询序列中的元素和关键字序列中的元素之间的2D映射，提供了查询序列和关键字序列中的所有单词之间的关系的表示。在这之后，这个点积图通过softmax在keys维度上运行，它实际上压缩了0和1之间的值，以表示模型应该关注一对令牌的程度。然后，这个注意力矩阵乘以值矩阵，值矩阵通常与关键矩阵的大小相同，这给出了最后的大小<strong class="jp hi"> (N，Q，H) </strong>。我们可以用一个浓缩的例子来理解正在发生的事情:如果我们使用一个英语句子作为查询，一个德语句子作为关键字，那么注意力将代表每个英语单词与每个德语单词的对应程度。当它与德语句子相乘时，它代表了每个英语单词与德语单词相比应该被赋予的重要性。另请注意，输入大小和输出大小是相同的，允许这些关注层相互堆叠。以上是对这种注意力的直观描述。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lq"><img src="../Images/3277fb13199f3a416854d36c0b71f17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*ks2hz_JXrsTioYVolwCyNA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source: <a class="ae jm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="c64b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">多头注意力是规模点积注意力的一种更为进化的类型，它使用多个头部，类似于CNN中的多个过滤器，以鼓励模型理解查询和键序列之间的多种类型的成对连接(考虑到值通常与键相同)。首先，输入通过三个相应的线性层。请注意，线性图层的大小是相同的，但实际图层本身是不同的。这种关注的主要亮点是，输入<em class="lg"> Q </em>、<em class="lg"> K </em>和<em class="lg"> V </em>的<em class="lg"> hid_dim </em>方面被分成两个维度，一个是头的数量，另一个是<em class="lg"> head_dim </em>(如下面的代码所示)<em class="lg">。</em>这实质上是将每个向量分成<em class="lg"> h </em>个向量，每个向量的大小都是<em class="lg"> head_dim </em>。正常比例的点积注意力也有一个关键的区别。就在softmax之前，查询和密钥的乘积除以<em class="lg"> head_dim的平方根。</em>根据论文作者的说法，这可以防止点积的结果变得太大，抵消消失梯度。在计算完所有的头部之后，它们被连接起来，并且最终的线性层被应用到这个输出。在下面的代码中，有一个掩码参数，对于编码器来说，它只是将所有带有<em class="lg"> pad_token </em>的索引设置为基本的<em class="lg"> -inf </em>，因此当调用softmax时，这些值变成0。这防止了模型关注填充标记，填充标记会使训练和推断不一致。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="8414" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">编码器</h1><p id="3666" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">下面是EncoderLayer类，它包含编码器所包含的大部分操作。它从自我关注层开始，本质上是发现输入序列中的单词如何与自己相关联。然后，应用图层归一化，使每个要素的平均值为0，标准差为1。接下来，如前所述，应用位置式前馈层。应用另一层标准化，编码器层完成。大量使用层规范来防止过度拟合，这是此类大型神经网络的一个大问题。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="b076" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Encoder类既涉及EncoderLayer类，也涉及用位置编码将单词标记序列转换成特征向量。用<em class="lg"> nn将输入序列中的每个索引转换成一个<em class="lg"> hid_dim </em>大小的向量。嵌入()</em>类。<em class="lg"> nn。Embedding() </em>是用向量表示整数值的查找表。这类似于将一个热点向量转换成每个单词的学习特征向量。此后，由于模型没有位置感，使用相同的<em class="lg"> nn对输入进行位置编码。Embedding() </em>使用的令牌嵌入。之后，使用填充索引作为注意掩码，遍历编码器层堆栈。它的输出是一个编码的输入序列，其中包含有关多头自关注输入序列的信息。在下一篇文章中，我们将讨论解码器如何工作，以及编码器如何馈入解码器。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="036a" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">结论</h1><p id="91e4" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">简单地说，变压器是一个复杂的网络。复杂到需要多篇长文才能解释透彻。然而，它的复杂性不仅仅是一个借口，而是它允许变压器理解和操作序列前所未有的模式。由于ML中无数的任务都可以用序列来表示，所以transformer开始统治这个领域也就不足为奇了。请务必关注即将推出的第2部分！</p></div></div>    
</body>
</html>