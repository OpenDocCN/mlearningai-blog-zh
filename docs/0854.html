<html>
<head>
<title>Inverse Pendulum — Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逆摆—深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/inverse-pendulum-deep-reinforcement-learning-a22689e14e34?source=collection_archive---------2-----------------------#2021-08-04">https://medium.com/mlearning-ai/inverse-pendulum-deep-reinforcement-learning-a22689e14e34?source=collection_archive---------2-----------------------#2021-08-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1c88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我将在CartPole环境中比较不同的深度强化学习方法。首先，我将通过基本方程和背景。然后我们将解释我们使用的方法——一个随机的方法作为这个项目的基线，深度Q学习，带记忆回放的深度Q学习，以及双重深度Q学习。我将比较每种方法，看看哪种方法的性能最好。<br/>完整的代码可以在<a class="ae jc" href="https://github.com/shirkozlovsky/Deep-Learning-Course.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="74cf" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">内容:</h2><h2 id="b1d2" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">1.摘要</h2><h2 id="7ada" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.导言和背景</h2><p id="91d1" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">2.1.环境<br/> 2.2。q学习<br/> 2.3。深度Q学习(DQN) <br/> 2.4。回放记忆<br/> 2.5。双重深度Q学习</p><h2 id="6212" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.代码和参数</h2><p id="9db1" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">3.1.基础搜索<br/> 3.2。q-学习<br/> 3.3。深度Q学习(DQN) <br/> 3.4。深度Q学习带回放记忆<br/> 3.5。双Q学习</p><h2 id="c66e" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">4.结果</h2><p id="971d" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">4.1.基本搜索<br/> 4.2。深度Q学习<br/> 4.3。带回放记忆的深度Q学习<br/> 4.4。具有重放记忆的双Q学习</p><h2 id="b72c" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">5.结论</h2><h2 id="2420" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">6.参考</h2></div><div class="ab cl kd ke go kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ha hb hc hd he"><h1 id="c55d" class="kk je hh bd jf kl km kn jj ko kp kq jn kr ks kt jq ku kv kw jt kx ky kz jw la bi translated"><span class="l lb lc ld bm le lf lg lh li di"> 2。</span>简介和背景</h1><h2 id="6698" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.1环境</h2><p id="cec0" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">CartPole，也称为倒立摆，是一种游戏，你试图尽可能长时间地平衡杆子。据推测，在杆子的顶端，有一个物体使它不稳定，很可能摔倒。该任务旨在左右移动手推车，以便在可能的情况下使杆子能够站立(在一定角度内)。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lj"><img src="../Images/a12620cf83f1798b8bb37e06837dcbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*fvXTd-0JDwtOLMss-qd-fA.png"/></div></figure><p id="cfbf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">状态空间由四个值表示:小车位置、小车速度、磁极角度和磁极尖端的速度。动作空间由两个动作组成:向左移动或向右移动。杆保持直立的每个时间步长提供+1的奖励。当柱子偏离垂直方向超过15度，或者手推车偏离中心超过2.4个单位时，该集结束。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lr"><img src="../Images/a2c03b06206f8f3a31e041b9874450b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/0*cp5pnUKPpmr9XLB_"/></div></figure><h2 id="7b2e" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.2.q学习</h2><p id="f8a7" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">强化学习是一种机器学习方法，它基于从环境中获得的奖励，而不是例子。代理和环境不断交互，如图2所示。在给定环境状态及其策略的情况下，代理决定执行哪些操作。代理人的行动改变了环境的状态，并导致了适当的回报。代理人的目标是学习一种策略，使其在长期内获得最大的回报。</p><p id="4e40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q值Q(s，a)是每个状态-动作对的预期回报。</p><p id="dad9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">国家-行动对的潜在未来回报描述为(贝尔曼方程):</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ls"><img src="../Images/8ae070e8bffa984b214d48f45a548aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/1*PYyj4UWW4GIEOUDvXEPlpg.gif"/></div></figure><p id="34be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中γ∈[0，1]是折扣因子。Q(s，a)是当我们处于状态并采取行动时的期望回报。在通过做出一个动作(a)获得一个奖励(r)之后，我们会到达另一个状态(s’)。然后，我们只需查找Q表，找到在状态(s’)下采取的最佳行动。因此，这里的想法是我们不需要考虑整个未来的行动，而只需要考虑下一个时间点的行动。</p><p id="542f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q(s，a)的基本更新规则由下式给出:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lt"><img src="../Images/0e8c2069902ddfea3ffaf7990611d748.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/1*nByaKE215fBDHf2GLnbaHQ.gif"/></div></figure><p id="17ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率在哪里。这个规则基于我们认为我们对Q(s，a)的了解和当前事件告诉我们的Q(s，a) <em class="lu">之间的差异。</em></p><p id="5328" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就我们的情况而言，当想要在给定的状态下采取任何行动时，使我们的回报最大化的政策:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lv"><img src="../Images/5e85f6f80dfc855f1a0379456a2f683f.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/1*udMKGFLbS61eJo9F1Y9kJw.gif"/></div></figure><p id="a16a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们没有访问Q*的权限，所以我们将创建一个并将其训练为类似Q*的形式。</p><p id="f565" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">某些策略的每个Q函数都服从贝尔曼方程，因此训练更新规则:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lw"><img src="../Images/e2b6eaed7ec2baf81ce2e103bcffa920.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/1*h0eaXERy6DBFTowvf-7L9g.gif"/></div></figure><p id="6906" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">等式两边的差异称为时间差异误差:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lx"><img src="../Images/1e0f5426b80ae15f0ee9e71bac238217.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/1*iX1vHrDscoIrv8d_uceNfQ.gif"/></div></figure><p id="0241" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了最小化这种误差，我们将使用Huber损失。当误差较小时，Huber损失类似于均方误差，但当误差较大时，类似于平均绝对误差，这使得当Q的估计值非常嘈杂时，Huber损失对异常值更加稳健。我们通过从重放存储器中采样的一批转换B来计算:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ly"><img src="../Images/8f26b875a4bf1ddb4b1085d2ed1b6aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*OAvW5hneUMhyFJkVZOIwfQ.png"/></div></figure><p id="d569" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lz"><img src="../Images/30e7de80fa2ae7a3442a691ce1e1ccc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*qBAWYjEgrMhdxt78PvkPIg.png"/></div></figure><h2 id="8cde" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.3.深度Q学习(DQN)</h2><p id="54b4" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我们的环境是确定性的，所以为了简单起见，这里给出的所有方程也是确定性的。</p><p id="3868" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的目标是制定一项政策，试图最大限度地提高折扣和累积奖励:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ma"><img src="../Images/10c456dacd5890e2bbc84ae4737a7ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*WjYBZOkiXJ5W3wloRDdIew.png"/></div></figure><p id="8a19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中γ∈[0，1]<em class="lu">T3是折扣因子。</em></p><p id="d844" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，我们需要一些东西来近似一个函数，该函数接受一个状态-动作对(s，a ),并返回该对的预期回报。这就是深度学习的用武之地。它以仅根据训练数据逼近函数而闻名。</p><p id="036b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在深度Q学习中，我们使用神经网络来逼近Q值函数。状态作为输入给出，所有可能动作的Q值作为输出产生。</p><p id="30b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用深度Q学习网络(dqn)进行强化学习的步骤包括:</p><ul class=""><li id="d289" class="mb mc hh ig b ih ii il im ip md it me ix mf jb mg mh mi mj bi translated">所有的体验都由用户存储在存储器中</li><li id="fc0c" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">下一个动作由Q网络的最大输出决定</li><li id="204d" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">这里，损失函数是预测Q值和目标Q值的均方误差— Q*。这基本上是一个回归问题。然而，我们不知道这里的目标或实际值，因为我们正在处理一个强化学习问题。(王等，2016)</li></ul><h2 id="8a88" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.4.重放记忆</h2><p id="106d" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">DQN的主要问题之一是稳定。当我们将观察结果输入我们的网络时，我们的网络会对过去的经验进行归纳。</p><p id="7738" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回放记忆有助于我们了解观察到的情况下应该采取什么行动。这就是为什么重放缓冲器有助于稳定训练。(刘、邹，2019)</p><h2 id="4105" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">2.5.双重深度Q学习</h2><p id="e1c3" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">该解决方案包括使用两个独立的Q值估计器(Q，Q’)，其中一个用于更新另一个。使用这些独立的估计，我们可以无偏Q值估计的行动选择使用相反的估计。因此，我们可以通过从有偏差的估计中分离出我们的更新来避免最大化偏差。</p><p id="09ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本Q学习的更新过程是:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mp"><img src="../Images/abd63860cd7980514792a46b7dde5b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*q5v9BGgeEx_YIBCRnB3Jjg.png"/></div></figure><p id="e743" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学习率在哪里。</p><p id="d315" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于双Q学习:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mq"><img src="../Images/73c9f918cbe29ffa9cb4b735e9f7e8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*qYi2rh-Hn5BZBOuZC1bLWw.png"/></div></figure><p id="ab82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q函数用于选择具有下一状态的最大Q值的最佳动作<em class="lu"> a </em>，而Q’函数用于通过使用上面选择的动作a<em class="lu">T3来计算期望的Q值。(哈瑟尔特、古埃兹和西尔弗，2016年)</em></p><h1 id="ad21" class="kk je hh bd jf kl mr kn jj ko ms kq jn kr mt kt jq ku mu kw jt kx mv kz jw la bi translated">3.代码和参数</h1><p id="d79d" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">如前所述，状态数为4(小车位置、小车速度、极角和极尖速度)，动作数为2(左、右)。</p><p id="414d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们选择100集，DQN 50个隐藏节点，学习率0.001</p><h2 id="d1ab" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.1.基本搜索</h2><p id="d5fa" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">这部分我们是这个项目的基线。</p><h2 id="5781" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.2.q学习</h2><p id="6f7b" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">这是所有方法的基类。它包含了撑杆跳的环境，我们想玩的游戏的数量(集)。</p><p id="9ea2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该函数获取:</p><ul class=""><li id="f34a" class="mb mc hh ig b ih ii il im ip md it me ix mf jb mg mh mi mj bi translated">env——钢管混凝土柱的开放式人工智能健身环境</li><li id="8973" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">模型—我们想要运行的模型</li><li id="a64e" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">γ—γ= 0.9</li><li id="f2f9" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">ε——代表随机行为相对于由行为者在事件中积累的现有“知识”所告知的行为的比例。在玩游戏之前，代理没有任何经验，因此通常会将epsilon设置为较高的值，然后逐渐降低其值。</li><li id="ba36" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">eps_decay —当代理学习时，ε减小的速度。</li><li id="69b8" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">重放—我们是否要使用重放内存。</li><li id="f54b" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">replay_size —我们需要的内存大小。</li><li id="cc34" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">双——如果我们想不想使用双DQN。</li><li id="1486" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">标题—图表的标题</li><li id="3ca1" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">n _ update参数指定更新目标网络的时间间隔。</li></ul><h2 id="c3b8" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.3.深度Q学习(DQN)</h2><p id="aea4" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">DQN类实现是一个在PyTorch中实现的神经网络，它有两个主要方法，预测和更新。网络将代理的状态作为输入，并返回每个动作的Q值。代理选择最大Q值来执行下一个操作。</p><p id="08a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该函数获取:</p><ul class=""><li id="75e0" class="mb mc hh ig b ih ii il im ip md it me ix mf jb mg mh mi mj bi translated">state _ dim状态维度</li><li id="cf8b" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">action _ dim操作维度</li><li id="068e" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">hidden _ dim隐藏层的尺寸</li><li id="aaf2" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb mg mh mi mj bi translated">α—学习率(0.001)</li></ul><p id="447a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个NN的架构是一个线性层(state_dim，hidden_dim)，然后是LeakyReLU，然后是另一个线性层(hidden_dim，2∙ hidden_dim)，LEakyRelu，linear (2∙ hidden_dim，action_dim)</p><p id="db08" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用优化的亚当。</p><h2 id="1ff4" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.4.具有重放记忆的深度Q学习</h2><p id="42f7" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">体验回放将代理的体验存储在内存中。成批的经验是从记忆中随机抽取的，并用于训练神经网络。这种学习包括两个阶段——获得经验和更新模型。重放的大小控制了用于网络更新的体验的数量。内存是一个数组，存储代理的状态、奖励和动作，以及动作是否完成游戏和下一个状态。</p><h2 id="8864" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.5.双Q学习</h2><p id="c219" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">Q值将取自这个新网络，这意味着反映主DQN的状态。然而，它没有相同的权重，因为它只在一定数量的集后更新。</p><h1 id="b57c" class="kk je hh bd jf kl mr kn jj ko ms kq jn kr mt kt jq ku mu kw jt kx mv kz jw la bi translated">4.结果</h1><h2 id="3f7d" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">4.1.基本搜索</h2><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="er es mw"><img src="../Images/c5ec016d65dac1b66565e007d48ede33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*84NmnOQ_z4vzHOuO"/></div></div></figure><p id="57b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所见，因为这个策略是随机的，所以不可能解决这个问题。代理没有从他的经验中学习。平均性能低于10级。</p><h2 id="dc21" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">4.2.深度Q学习</h2><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="er es mw"><img src="../Images/3a78fce25269500029300480f96487a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gnLILUSm88mhdpuf"/></div></div></figure><p id="e867" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，代理的性能提高了。我们走了将近100步。我们可以看到，性能随着时间的推移而提高，趋势线为正。但是，代理仍然没有能力达到目标。</p><h2 id="c912" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">4.3.具有重放记忆的深度Q学习</h2><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="er es mw"><img src="../Images/217f89fc93f3da10398853b4d96a994e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pHMb0v4ukEK1zTNE"/></div></div></figure><p id="e150" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与只记住最后一个动作的神经网络相比，具有重放功能的神经网络更加健壮。我们可以看到代理的性能显著提高。业绩随着时间的推移而增加，趋势线是正的。代理达到目标，但看起来不稳定。</p><h2 id="586c" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">4.4.具有重放记忆的双Q学习</h2><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="er es mw"><img src="../Images/ff11ceef1a67695d2d3473edc91d25d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XCqALHcLPuthTt9M"/></div></div></figure><p id="673c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们可以看到，代理仅在40集之后就实现了目标，它是稳定且健壮的。代理人得到500英镑的高额报酬。</p><h1 id="c745" class="kk je hh bd jf kl mr kn jj ko ms kq jn kr mt kt jq ku mu kw jt kx mv kz jw la bi translated">5.结论</h1><p id="c3b7" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">随着我们改进方法，我们看到了更好的结果。从我们实现的方法来看，给出最健壮、稳定和最高回报的方法是具有重放记忆的双Q学习。我们检查的最低效的方法是随机方法。当然，我们可以通过添加更多功能或尝试另一种方法(例如，软更新)来改善我们的结果。</p><h1 id="0d8c" class="kk je hh bd jf kl mr kn jj ko ms kq jn kr mt kt jq ku mu kw jt kx mv kz jw la bi translated">6.参考</h1><ol class=""><li id="d7ec" class="mb mc hh ig b ih jy il jz ip nb it nc ix nd jb ne mh mi mj bi translated">用于深度强化学习的决斗网络结构。<em class="lu">2016年第33届ICML国际会议马赫学会</em>。2016;4(9):2939–2947.</li><li id="4c0e" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb ne mh mi mj bi translated">记忆回放在强化学习中的作用。<em class="lu"> 2018年第56届年度通信控制计算机会议。2019:478–485.多伊:10.1109/阿勒顿。36800.68668686661</em></li><li id="64c5" class="mb mc hh ig b ih mk il ml ip mm it mn ix mo jb ne mh mi mj bi translated">哈塞尔特H范，格斯A，银d。双DQN.pdf。<em class="lu">第30届AAAI会议纪要。2016:2094–2100.</em></li></ol></div></div>    
</body>
</html>