<html>
<head>
<title>Curse of Dimensionality Explained!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">维数灾难解释！！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/curse-of-dimensionality-explained-1a725ea4df14?source=collection_archive---------4-----------------------#2022-06-12">https://medium.com/mlearning-ai/curse-of-dimensionality-explained-1a725ea4df14?source=collection_archive---------4-----------------------#2022-06-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/4c724343d6cb403e006c7fe50771167a.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*uCseCD0SC3uX4iMaNkPTcA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://livewallpapers4free.com/sphere-abstract-4k-free-live-wallpaper/" rel="noopener ugc nofollow" target="_blank"><strong class="bd iu">High dimensional space</strong></a></figcaption></figure><p id="daa2" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated"><strong class="ix hi">诅咒:</strong>对某物造成巨大伤害，这里指的是机器学习模型。</p><p id="0ed0" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated"><strong class="ix hi">维度</strong> : <strong class="ix hi"> </strong>空间范围的度量，尤其是宽度、高度或长度等。在这种情况下，特征的数量代表维度的数量。所以，简而言之，高特征对我们的机器学习模型造成伤害。</p><h2 id="4640" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">维度的诅咒-</h2><p id="2d33" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">在使用高维数据的ML模型中，存在一个最佳特征数量，在该数量之后，模型不会增加精度。模型性能反过来会降低准确性，并使模型在计算上更加复杂。</p><p id="c809" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">你处理的维度越多，标准的计算和统计技术就变得越低效。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/5643229b745b954ea23a591344be65be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJ2gfy1MlZM_WyP8wU-Yrw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/" rel="noopener ugc nofollow" target="_blank">1st,2nd and 3rd Dimensions in space</a></figcaption></figure><h2 id="d791" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">高维数据</h2><ol class=""><li id="95f9" class="kx ky hh ix b iy kn jc ko jg kz jk la jo lb js lc ld le lf bi translated">图像</li><li id="797d" class="kx ky hh ix b iy lg jc lh jg li jk lj jo lk js lc ld le lf bi translated">文本数据</li></ol><h2 id="059b" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">为什么在更高维度会有问题？</h2><p id="654e" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">数据的稀疏性是造成这种情况的主要原因。</p><p id="a96b" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">假设有一颗钻石装在上面图片的盒子里。在第一种情况下，有5个盒子，更容易找到，因为它是一维的。在第二种情况下，比第一种情况更难找到钻石，因为有25个盒子。第三种情况非常困难，因为有125个盒子。</p><p id="8fbf" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">同样，增加维度会导致更高维度的复杂性增加(指数级)。随着数据变得稀疏，即彼此远离，统计和最大似然模型失效。</p><blockquote class="ll lm ln"><p id="5cd2" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated">例如，k-nearest neighborhood，其中如果数据点非常远，比如它们相距50维，那么距离度量也彼此远离，我们无法将其视为邻居，因为这些点相距如此之远。</p></blockquote><h2 id="56ac" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">维数灾难的问题</h2><ol class=""><li id="80ab" class="kx ky hh ix b iy kn jc ko jg kz jk la jo lb js lc ld le lf bi translated">性能下降</li><li id="4dec" class="kx ky hh ix b iy lg jc lh jg li jk lj jo lk js lc ld le lf bi translated">计算量增加</li><li id="c245" class="kx ky hh ix b iy lg jc lh jg li jk lj jo lk js lc ld le lf bi translated">距离度量复杂度增加。</li></ol><h2 id="7aad" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">解决方案-</h2><p id="b197" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">为了解决这个问题，有一种技术叫做降维。我们减少数据集中的列数，并将其修改为新列。</p><blockquote class="ll lm ln"><p id="8bb7" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><em class="hh">降维有2种技术:</em> <strong class="ix hi"> <em class="hh">特征选择</em> </strong> <em class="hh">和</em> <strong class="ix hi"> <em class="hh">特征提取</em> </strong>。</p></blockquote><h1 id="5a0e" class="ls ju hh bd iu lt lu lv jy lw lx ly kc lz ma mb kf mc md me ki mf mg mh kl mi bi translated">特征提取-</h1><blockquote class="ll lm ln"><p id="65b1" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><em class="hh">从现有的列集创建一个全新的列集。</em></p></blockquote><h2 id="05bb" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">i) PCA(主成分分析)</h2><p id="e159" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">通过旋转轴将高维数据降低到较低的维度，其中主成分存储了最大的方差。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mj"><img src="../Images/a6f43c27d8b9ee1860e3df1ecf370ade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aD-LT2W-hiNog75CazTgrw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/" rel="noopener ugc nofollow" target="_blank"><strong class="bd iu">PCA Visual understanding</strong></a><strong class="bd iu"> (Image-Free code camp)</strong></figcaption></figure><h2 id="ec0e" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">ii)线性鉴别分析</h2><p id="496b" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">通过旋转轴将高维数据降维为低维，提高了类分离。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mk"><img src="../Images/97f6d2d4743045914834761ff22e18dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWRxBkq5Sqfw9FT-VwlgGQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/" rel="noopener ugc nofollow" target="_blank"><strong class="bd iu">LDA visualized (image-freecodecamp)</strong></a></figcaption></figure><h2 id="a08e" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">iii)t-SNE(t-分布式随机邻居嵌入)</h2><p id="e596" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">它是一种非线性降维技术，专注于在低维空间中将非常相似的数据点保持在一起。它非常适合嵌入高维数据，以便在二维或三维的低维空间中可视化。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/9a31215a543249badb02cedfb90eff4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*aVP4aGs07359Xf7KefTtzg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd iu">t-SNE visualised in MNIST Dataset</strong></figcaption></figure></div><div class="ab cl mm mn go mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ha hb hc hd he"><h1 id="c460" class="ls ju hh bd iu lt mt lv jy lw mu ly kc lz mv mb kf mc mw me ki mf mx mh kl mi bi translated">功能选择-</h1><blockquote class="ll lm ln"><p id="bc6c" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated">从给定的具有高信息量的数据集中选择一组列。</p></blockquote><h2 id="2f8a" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">I)向前选择-</h2><p id="3bfb" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">在每个阶段，该估计器基于ML算法中估计器的交叉验证分数来选择最佳特征子集。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es my"><img src="../Images/d49e472dd8750b05955a02ccf2ce49f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*kAAASU0bCWXWHMjj_jJFPw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://towardsdatascience.com/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09" rel="noopener" target="_blank"><strong class="bd iu">Forward selection process</strong></a></figcaption></figure><h2 id="ac91" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">二)逆向淘汰-</h2><p id="1c16" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">这是一种迭代方法，我们最初从所有特性开始，在每次迭代之后，我们删除最不重要的特性。当移除特征后模型的性能没有改善时，我们停止。</p><blockquote class="ll lm ln"><p id="4d28" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><strong class="ix hi"> <em class="hh">步骤1) </em> </strong> <em class="hh">选择一个显著的p值</em></p><p id="c85a" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><strong class="ix hi"> <em class="hh">步骤2) </em> </strong> <em class="hh">拟合模型</em></p><p id="080c" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><strong class="ix hi"> <em class="hh">步骤3) </em> </strong> <em class="hh">选择具有最大P值的预测值和</em></p><p id="6f6e" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><strong class="ix hi"> <em class="hh">步骤4) </em> </strong> <em class="hh">丢弃该预测值。</em></p></blockquote><h2 id="366e" class="jt ju hh bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">iii)双向消除-</h2><p id="7a8f" class="pw-post-body-paragraph iv iw hh ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js ha bi translated">它类似于向前选择，但不同之处在于，添加新要素时，它还会检查先前添加的要素的重要性，如果发现任何先前选择的要素不重要，它会通过向后消除来简单地移除该要素。</p><blockquote class="ll lm ln"><p id="e122" class="iv iw lo ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js ha bi translated"><em class="hh">组合</em> <strong class="ix hi">正向选择<em class="hh"> </em> </strong> <em class="hh">和</em> <strong class="ix hi">反向淘汰</strong> <em class="hh">。</em></p></blockquote><p id="36a1" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">感谢阅读！</p><p id="0c91" class="pw-post-body-paragraph iv iw hh ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">关注我了解更多关于DS和ML的内容。</p><div class="mz na ez fb nb nc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">medium.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq in nc"/></div></div></a></div></div></div>    
</body>
</html>