<html>
<head>
<title>Object Detection Explained: YOLO v3.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体探测解释:YOLO v3。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/object-detection-explained-yolo-v3-fdf83cc78d25?source=collection_archive---------0-----------------------#2021-11-03">https://medium.com/mlearning-ai/object-detection-explained-yolo-v3-fdf83cc78d25?source=collection_archive---------0-----------------------#2021-11-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="ebf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我终于在YOLO v3上写了这篇文章。YOLO v2有一些重要的改进。总的来说，它更大更准确。然而，它仍然很快，因为在320 x 320时，它能够在28.2 mAP下在22毫秒内运行，这与SSD一样准确，但速度快三倍。这里我只关注YOLO v2的不同和改进。</p><p id="d720" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">原文:YOLOv3:一个增量的改进<a class="ae jc" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767.pdf</a></p><p id="fdb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">先前的</strong>:</p><p id="5c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://towardsdatascience.com/object-detection-explained-r-cnn-a6c813937a76" rel="noopener" target="_blank"> RCNN </a></p><p id="fe4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-fast-r-cnn-bc11e607411f">快速RCNN </a></p><p id="ddb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-feature-pyramid-networks-cf2621c8f7cc"> FPN </a></p><p id="ead8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-faster-r-cnn-23e7ab57991d">更快的RCNN </a></p><p id="d01e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-single-shot-multibox-detector-c45e6a7af40">固态硬盘</a></p><p id="f3f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-yolo-v1-fb4bcd3d87a1"> YOLO v1 </a></p><p id="133a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/object-detection-explained-yolo-v2-3e3086789ffb"> YOLO v2 </a></p><h1 id="3c66" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">暗网-53</h1><p id="727c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">YOLO v2推出了darknet-19，这是一个19层的网络，补充了11层用于对象检测。然而，由于拥有30层架构，YOLO v2常常难以检测到小物体。因此，作者引入了连续的3 × 3和1 × 1卷积层，之后是一些允许网络更深的快捷连接。因此，作者介绍了他们的Darknet-53，如下所示。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/1432cafed37b401877e617851a587db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*3UdwC9S91Qmn8cq5dLB3Ig.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Source: <a class="ae jc" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767.pdf</a></figcaption></figure><p id="7452" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">暗网-53比暗网-19强大得多。此外，它比ResNet-101或ResNet-152更有效。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ks"><img src="../Images/ae8ce06dddc2c391e9c4f1bc476aa228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*S_z8HFwg1RtUd6cNCjswzg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Results on ImageNet. Source: <a class="ae jc" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767.pdf</a></figcaption></figure><p id="4878" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们得出结论，Darknet-53更好地利用了GPU，使其评估更有效，从而更快。这可以通过观察来解释，ResNets有太多的层，并且不是很有效。</p><h1 id="312c" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">跨尺度预测</h1><p id="4672" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">YOLOv3预测了3种不同比例的方框，在这些比例下，使用与特征金字塔网络(FPN)相似的概念提取特征。在COCO的实验中，YOLOv3在每个尺度上预测3个框，因此张量是N×N×[3∫(4+1+80)]，其中4是边界框偏移，1是对象预测，80是类预测。</p><p id="65bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">和在FPN一样，作者采用了2张先前的特征地图，并将它们向上采样了2倍。同时，它们还从网络的早期获取特征图，并将其与向上采样的特征图连接，之后我们从向上采样的特征中获得更有意义的语义信息，并从早期的特征图中获得更细粒度的信息。最后，它们通过更多的卷积层，最终预测出一个相似的张量，尽管现在是原来的两倍。</p><p id="c9a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，再次执行相同的过程，以预测最终比例的盒子。因此，第三规模受益于网络早期的所有先前计算和细粒度特征。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">we use</figcaption></figure><p id="24f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的pytorch代码显示了YOLO v3修改后的架构。通过<em class="kv">配置</em>对象<em class="kv">可以看到概览。</em>如我们所见，在我们对特征地图进行上采样后，我们将它与我们在早期阶段获得的地图连接起来，后者存储在<em class="kv"> route_connections </em>列表中。因此，我们总共执行3次规模预测，并返回3个预测张量来计算损失。</p><h1 id="1554" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">损失函数的变化</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kw"><img src="../Images/5173d2777787d0877dc44926eab81681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/0*sfsmTEfoTSUdLxYB.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Loss function. Paper: <a class="ae jc" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></figcaption></figure><p id="099d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上图描述了YOLO版本1中引入的损失函数。YOLO v3用交叉熵误差项代替最后三个平方误差项。因此，通过逻辑回归来预测对象置信度和类别预测。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">&lt;script src=”</figcaption></figure><p id="ff4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以上要点显示了Pytorch对YOLO v3使用的loss的实现。注意，我假设你已经知道YOLO v1和YOLO v2的细节。</p><h1 id="1e10" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">类别预测</h1><p id="d268" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">作者通过移除softmax来允许多类分类，因为它对于良好的性能是不必要的。所以他们简单地使用独立的逻辑分类器。在训练期间，作者利用众所周知的二元交叉熵损失进行分类预测。</p><p id="5dc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该公式在更复杂的领域中是有效的，如开放图像数据集，其具有许多重叠的标签(即，女人和人)。多标记方法可以更好地模拟数据。</p><h1 id="030f" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">一些遗言</h1><p id="43af" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">您可能注意到了，作者对YOLO v2做了最小的修改。然而，YOLO v3仍然是强大的，准确的，比YOLO v2更好。此外，作者讨论了他们试图引入的一些方法，但这些方法对性能增益没有贡献。如果您有兴趣阅读它们，我鼓励您阅读原始文档，因为它提供了更详细的见解。然而，我希望我能让你熟悉YOLO v3。</p><p id="e3a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Joseph Redmon一直致力于YOLO v1、v2和v3的研究，由于伦理问题，他在CV研究中名列前茅。他在推特上说，他已经停止了他的计算机视觉研究，以避免这项技术被滥用。然而，对YOLO的研究并没有停止，我期待着引入YOLO v4和YOLO v5。考虑到我在中型上的生产力，也许当我到达YOLO v5的时候，已经有YOLO v6了…</strong></p><p id="bc9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢你。</p></div></div>    
</body>
</html>