<html>
<head>
<title>Understanding how backpropagation works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解反向传播的工作原理</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/understanding-how-backpropagation-works-3ed6c6c96f2e?source=collection_archive---------1-----------------------#2022-03-07">https://medium.com/mlearning-ai/understanding-how-backpropagation-works-3ed6c6c96f2e?source=collection_archive---------1-----------------------#2022-03-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="78fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">反向传播算法是训练神经网络的基本算法之一。它使用链规则方法来找出改变权重和偏差如何影响我们想要最小化的成本函数。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="b922" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么要反向传播？</p><p id="fee2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嗯，当训练神经网络时，我们首先要计算神经元的所有值，并查看它产生的输出层(有时称为前向传递)。当面对实际结果时，我们可以计算损失函数。但是请记住，我们是在训练网络，因此我们实际上是在尽量减少我们的损失分数。在这一步，我们需要回过头来检查权重的微小变化会如何影响结果，并根据其对损失函数的影响信息更新权重。为此，我们需要计算损失函数相对于网络中任何权重和偏差的偏导数。在本文中，将推导和解释计算反向传播所需的所有公式。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/1f47cb46854adf4654ee2baa94829e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vh22XTVNC6NeBdatCPm9Zw.png"/></div></div></figure><p id="49b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于实现反向传播非常重要的两个假设是:</p><ol class=""><li id="525d" class="jv jw hh ig b ih ii il im ip jx it jy ix jz jb ka kb kc kd bi translated">成本函数被表示为所有数据样本的平均和。这是因为在反向传播中，我们计算单个训练样本的偏导数，然后对所有梯度进行平均以形成一个步长。</li><li id="d925" class="jv jw hh ig b ih ke il kf ip kg it kh ix ki jb ka kb kc kd bi translated">成本函数是神经网络输出的函数</li></ol><p id="b9fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将在本文的其余部分使用神经元的符号:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kj"><img src="../Images/5b75c5affd988c594b61cdd4792f4b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*ArL3iQLwtSQb9QutZ5WPLQ.png"/></div></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kk"><img src="../Images/e459bde241b89c4d91fbf5103d939aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*80TCRIMifnHMQsdZqJrRVA.png"/></div></figure><p id="c08f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kl"> z </em>值是神经元的预激活输出，权重是<em class="kl"> w </em>，而<em class="kl"> b </em>是该神经元的偏置。激活函数是sigmoid函数。</p><p id="75c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">层L中神经元j的误差定义为:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es km"><img src="../Images/b5ac6e438dfbf797bcedecea079bea95.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*s6rQJXyzZ1loJOOcMBAe0A.png"/></div></figure><p id="a624" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c是成本函数。这个表达式将用于计算重量的偏导数。</p><p id="e9a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们知道:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kn"><img src="../Images/58a7d8c7a7d1242ac7a1ab6323e69b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*opmC4ZPHAfElj_SMAkvhow.png"/></div></figure><p id="a88f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们定义了成本函数，所以相对于网络输出的偏导数是容易计算的。</p><p id="e9ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以将错误重写为:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ko"><img src="../Images/d15281ad0cbc5c6ebe0dc716fc6b6c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*nN7CaDaygHelWhCUPGcgPg.png"/></div></div></figure><p id="8128" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里我们使用elementwise产品。</p><p id="3774" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们现在开始。</p><p id="fbdd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了确定成本函数相对于任何预激活输出的偏导数(这实际上是我们定义的误差),我们仅应用链式法则:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kp"><img src="../Images/e33fea3ec76a78c8eab0322e01c2919f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*g9ISjbrpzQqopzhZD1jMCw.png"/></div></figure><p id="1473" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经可以看到，我们可以使用第L层的误差来计算第(L-1)层的误差，以基于矩阵的形式写成如下:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kq"><img src="../Images/a56403423c919d15d1c03bf4d4c4d3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*-QnrclR1pVGqoDZyq-DkgQ.png"/></div></figure><p id="1531" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">利用这个公式，从最后一层开始，我们可以计算任何一层的误差。</p><p id="ed34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">成本相对于任何重量的变化率为:</strong></p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kr"><img src="../Images/94cfa07c0de99036262e72e0994b2e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*oFXl0tnfVhVW0XbxmgYO-Q.png"/></div></figure><p id="e4d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">j-k符号意味着它是连接从层L-1的第k个神经元到层L的第j个神经元的权重。</p><p id="f870" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">成本函数相对于偏差的变化率:</strong></p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ks"><img src="../Images/dabb0d6806c539d6e838007ef3360642.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*a53GwBvC0MO8LP_QdaxeUg.png"/></div></figure><p id="79ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们知道如何计算误差，我们已经完成了。</p><p id="6b1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些方程适用于反向传播。它们适用于每一个激活函数，因为我们使用的规则不是特定于函数的，但是要经常检查你的模型是否满足开头所述的假设。</p><div class="kt ku ez fb kv kw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kx ab dw"><div class="ky ab kz cl cj la"><h2 class="bd hi fi z dy lb ea eb lc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ld l"><h3 class="bd b fi z dy lb ea eb lc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="le l"><p class="bd b fp z dy lb ea eb lc ed ef dx translated">medium.com</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk jt kw"/></div></div></a></div></div></div>    
</body>
</html>