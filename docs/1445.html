<html>
<head>
<title>How to setup and optimize CUDA and TensorFlow on Ubuntu 20.04 — 2022</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Ubuntu 20.04 — 2022上设置和优化CUDA和TensorFlow</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/how-to-setup-cuda-and-tensorflow-on-ubuntu-20-04-2022-7d240cffdf1e?source=collection_archive---------0-----------------------#2021-12-19">https://medium.com/mlearning-ai/how-to-setup-cuda-and-tensorflow-on-ubuntu-20-04-2022-7d240cffdf1e?source=collection_archive---------0-----------------------#2021-12-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="4715" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="9222" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">通常，公司将基于云的服务器用于机器学习应用，如<strong class="je hi">微软Azure和谷歌云平台</strong>，主要是因为它们的<strong class="je hi">随时可用且易于使用的服务</strong>，这些服务是为广泛的问题而构建的，并为日常ML操作提供<strong class="je hi">低代码解决方案</strong>。</p><p id="1629" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">尽管大多数公司可以通过这些工具快速而廉价地交付价值，但也有一些企业主要依赖于它们的ML能力作为核心产品或战略，并且最常见的是<strong class="je hi">在它们的</strong> <strong class="je hi"> AI基础设施和代码</strong>上投资大量金钱和时间。</p><p id="5167" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果你是其中一个项目的数据科学家，你可能会面临<strong class="je hi">用CUDA/cuDNN和TensorFlow栈</strong>设置Ubuntu服务器的需求，以便为<strong class="je hi">训练/预测任务和管道</strong>提供服务。</p><p id="c850" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">此外，处理人工智能时最大的负担之一是<strong class="je hi">在训练大型模型(例如用于图像和视频检测/分割的神经网络)时耗尽内存</strong>，这使得<strong class="je hi">GPU使用的优化</strong>对于维持整个训练过程而不崩溃是必要的。</p><p id="848c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在本文中，我将介绍正确安装和优化CUDA/cuDNN的<strong class="je hi">设置步骤，</strong>主题组织如下:</p><ol class=""><li id="29d3" class="kf kg hh je b jf ka jj kb jn kh jr ki jv kj jz kk kl km kn bi translated">发现和安装nvidia驱动程序。</li><li id="a7f9" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">匹配nvidia驱动与CUDA和TensorFlow版本。</li><li id="94c9" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">安装CUDA工具包和cuDNN。</li><li id="cfb3" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">CUDA环境变量。</li><li id="4167" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">安装TensorFlow。</li><li id="6f78" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">优化您的GPU处理。</li></ol><h1 id="1f32" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">1️⃣发现并安装NVIDIA驱动程序</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/59cdcc92000ca0494928f1ab6c74939f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3JQYMXBjOGkrBwsW"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">Photo by <a class="ae lj" href="https://unsplash.com/@christianw?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Christian Wiediger</a> on <a class="ae lj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9b28" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">就机器学习库和<strong class="je hi">与常见框架(如PyTorch或TensorFlow)的集成而言，NVIDIA GPUs是最好的</strong>。</strong><strong class="je hi">NVIDIA CUDA toolkit包括GPU加速库</strong>，一个C和C++编译器和运行时，以及优化和调试工具。它使您能够立即开始，而不用担心构建定制集成。(<em class="lk">如果你的机器上没有nvidia GPU，这个教程不适合你。</em>)</p><p id="160e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">首先，检测您的nvidia显卡的型号和推荐的驱动程序。为此，请执行以下命令。请注意，您的输出和推荐的驱动程序很可能不同:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="0856" class="lq if hh lm b fi lr ls l lt lu">$ ubuntu-drivers devices</span></pre><p id="2a63" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这将给你一个类似这样的输出:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="a262" class="lq if hh lm b fi lr ls l lt lu">== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==<br/>modalias : pci:v000010DEd00001C03sv00001043sd000085ABbc03sc00i00<br/>vendor   : NVIDIA Corporation<br/>model    : GP106 [GeForce GTX 1060 6GB]<br/>driver   : nvidia-driver-390 - distro non-free<br/>driver   : nvidia-driver-435 - distro non-free<br/>driver   : nvidia-driver-440 - distro non-free recommended<br/>driver   : xserver-xorg-video-nouveau - distro free builtin</span></pre><p id="5f37" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">从上面的输出我们可以得出结论，当前系统已经安装了<strong class="je hi">NVIDIA GeForce GTX 1060 6GB</strong>显卡，推荐安装的驱动程序是<strong class="je hi"> nvidia-driver-440 </strong>。</p><p id="3b6b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果您同意建议，请随意再次使用<code class="du lv lw lx lm b"><strong class="je hi">ubuntu-drivers</strong></code>命令来安装所有推荐的驱动程序:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="71e7" class="lq if hh lm b fi lr ls l lt lu">$ sudo ubuntu-drivers autoinstall</span></pre><p id="9b6b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">或者，使用<code class="du lv lw lx lm b"><strong class="je hi">apt</strong></code>命令选择性地安装所需的驱动程序。例如:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="fbcd" class="lq if hh lm b fi lr ls l lt lu">$ sudo apt install nvidia-driver-440</span></pre><p id="68e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">安装完成后，重启系统，就大功告成了。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="8975" class="lq if hh lm b fi lr ls l lt lu">$ sudo reboot</span></pre><p id="c270" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">重启后，检查你是否能通过你的nividia驱动找到你的GPU:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="5206" class="lq if hh lm b fi lr ls l lt lu">$ nvidia-smi</span></pre><h1 id="dde3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2️⃣将nvidia驱动程序与CUDA和TensorFlow版本匹配</h1><p id="a28b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当我第一次用CUDA堆栈设置Ubuntu服务器时，我的一个噩梦是我<strong class="je hi">不小心为我的nvidia GPU安装了错误的CUDA工具包版本</strong>。我花了很长时间才发现哪里出了问题，因为我在任何地方都找不到它。为了避免这种错误，你应该首先<strong class="je hi">检查CUDA、cuDNN、Tensorflow和你的nvidia驱动之间的所有版本兼容性。</strong></p><p id="e1a8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">参考</strong> <a class="ae lj" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-831/support-matrix/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">此链接</strong> </a> <strong class="je hi">来自nvidia官方文档</strong>显示了一个<strong class="je hi"> </strong>支持矩阵，该矩阵提供了对操作系统、NVIDIA CUDA、CUDA驱动程序和NVIDIA cuDNN 8.3.1硬件的支持版本的了解— <strong class="je hi">如果您想要更旧或更早的版本，请参考</strong> <a class="ae lj" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">此链接</strong> </a> <strong class="je hi">。</strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ly"><img src="../Images/4eba250d14e80a5923aa366d2f0efa10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8zI9LWZ34fXqe_nrb49uQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx"><em class="lz">Image by the author — screenshot from public Nvidia docs (link above)</em></figcaption></figure><p id="70bc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在我的情况下，因为我安装了r440 nvidia驱动程序，我的CUDA工具包版本将是10.2。</p><h1 id="9801" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3️⃣安装CUDA工具包和cuDNN</h1><h2 id="0e73" class="lq if hh bd ig ma mb mc ik md me mf io jn mg mh is jr mi mj iw jv mk ml ja mm bi translated">CUDA工具包</h2><p id="3923" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">CUDA是NVIDIA发明的并行计算平台和编程模型。它通过利用图形处理单元(GPU)的能力来大幅提高计算性能，是运行TensorFlow应用程序的先决条件。</p><p id="1b47" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">检查完您需要的版本后，您可以按照 <a class="ae lj" href="https://developer.nvidia.com/cuda-toolkit-archive" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">这个链接</strong> </a> <strong class="je hi">上的说明安装<strong class="je hi">CUDA工具包。</strong></strong></p><p id="ee0e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在我的例子中，我只是一次执行一个命令:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="7827" class="lq if hh lm b fi lr ls l lt lu">$ wget <a class="ae lj" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin" rel="noopener ugc nofollow" target="_blank">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin</a><br/>$ sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600<br/>$ sudo apt-key adv --fetch-keys <a class="ae lj" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub" rel="noopener ugc nofollow" target="_blank">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub</a><br/>$ sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"<br/>$ sudo apt-get update<br/>$ sudo apt-get -y install cuda</span></pre><h2 id="5865" class="lq if hh bd ig ma mb mc ik md me mf io jn mg mh is jr mi mj iw jv mk ml ja mm bi translated">cuDNN</h2><p id="a9a4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">NVIDIA CUDA深度神经网络库(cuDNN)是一个针对<a class="ae lj" href="https://developer.nvidia.com/deep-learning" rel="noopener ugc nofollow" target="_blank">深度神经网络</a>的GPU加速原语库。cuDNN为标准例程提供了<strong class="je hi">高度优化的实现，例如前向和后向卷积、池化、规范化和激活层</strong>。这也是运行TensorFlow应用程序的先决条件。</p><p id="8910" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">其安装指南可在</strong> <a class="ae lj" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-831/install-guide/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">此链接</strong> </a>找到。几乎所有的先决条件都在前面的步骤中安装好了(nvidia驱动程序和CUDA工具包)——唯一剩下的是zlib:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="5122" class="lq if hh lm b fi lr ls l lt lu">$ sudo apt-get install zlib1g</span></pre><p id="5d98" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，为了<strong class="je hi">下载cuDNN </strong>，请确保您注册了<a class="ae lj" href="https://developer.nvidia.com/accelerated-computing-developer" rel="noopener ugc nofollow" target="_blank"> NVIDIA开发者计划</a>。它会给你一个. tar文件进行解压和安装。</p><p id="c8b8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">去吧。tar文件位置，并执行以下操作来<strong class="je hi">解压缩它</strong>:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="6d59" class="lq if hh lm b fi lr ls l lt lu">$ tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz</span></pre><p id="eed0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">您需要将X.Y和v8.x.x.x替换为您特定的CUDA和cuDNN版本以及打包日期。 </p><p id="73c2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例如，在为CUDA 11.5安装cuDNN v8.3.1时，它将如下所示:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="e4f0" class="lq if hh lm b fi lr ls l lt lu">$ tar -xvf cudnn-linux-x86_64–8.3.1.22_cuda11.5-archive.tar.xz</span></pre><p id="5f78" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后，在解压文件的同一个文件夹中，<strong class="je hi">将这些文件移动到CUDA toolkit存储库中，并授予访问权限</strong>:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="2c33" class="lq if hh lm b fi lr ls l lt lu">$ sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include <br/>$ sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 <br/>$ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*</span></pre><h1 id="ee47" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">4️⃣ CUDA环境变量</h1><p id="4b16" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">正确安装CUDA/cuDNN堆栈后，现在您需要<strong class="je hi">导出运行ML应用程序</strong>时定位安装路径所需的环境变量。</p><p id="f5fc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我建议你将这些导出集成到你的Python代码中。sh脚本所以每次你运行它时，环境变量都会自动设置。</p><p id="57b7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在我的应用程序中，我总是定义一个函数来导出所有需要的变量，叫做<code class="du lv lw lx lm b"><strong class="je hi">exports()</strong></code>:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="b16a" class="lq if hh lm b fi lr ls l lt lu">def exports():   </span><span id="30a4" class="lq if hh lm b fi mn ls l lt lu">    # Set CUDA and CUPTI paths  <br/>    os.environ['CUDA_HOME'] = '/usr/local/cuda'<br/>    os.environ['PATH']= '/usr/local/cuda/bin:$PATH'  <br/>    os.environ['CPATH'] = '/usr/local/cuda/include:$CPATH'  <br/>    os.environ['LIBRARY_PATH'] = '/usr/local/cuda/lib64:$LIBRARY_PATH'  <br/>    os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH'  <br/>    os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:$LD_LIBRARY_PATH'</span></pre><p id="87dc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">上面，我们导出了定位CUDA堆栈所需的每个环境变量。<strong class="je hi"> CUPTI ( <em class="lk"> CUDA剖析工具接口)</em>支持创建针对CUDA应用的剖析和跟踪工具</strong>，并已随工具包一起提供。</p><h1 id="493c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">5️⃣安装张量流</h1><h2 id="4b35" class="lq if hh bd ig ma mb mc ik md me mf io jn mg mh is jr mi mj iw jv mk ml ja mm bi translated">系统安装</h2><p id="2d48" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">你可以直接在你的机器上安装TF，尽管推荐的<strong class="je hi">是在你正在使用的库内的虚拟环境</strong>中安装，以避免<a class="ae lj" href="https://en.wikipedia.org/wiki/Dependency_hell" rel="noopener ugc nofollow" target="_blank">依赖地狱</a>。可以通过pip软件包管理器完成:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="b412" class="lq if hh lm b fi lr ls l lt lu">$ pip3 install --user --upgrade tensorflow  # install in $HOME</span></pre><p id="8c71" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后，验证执行Python脚本的安装，该脚本将运行简单的TensorFlow操作:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="882f" class="lq if hh lm b fi lr ls l lt lu">$ python3 -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"</span></pre><h2 id="36f8" class="lq if hh bd ig ma mb mc ik md me mf io jn mg mh is jr mi mj iw jv mk ml ja mm bi translated">虚拟环境(推荐)</h2><p id="47f2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">只需在您的虚拟环境中执行以下操作:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="3d9a" class="lq if hh lm b fi lr ls l lt lu">$ pip install --upgrade tensorflow</span></pre><p id="825d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后，通过执行系统安装中提到的相同TF操作来验证它:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="c051" class="lq if hh lm b fi lr ls l lt lu">$ python -c "import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))"</span></pre><p id="f806" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="lk">如果你不知道如何创建一个Python虚拟环境，</em> <a class="ae lj" href="https://realpython.com/python-virtual-environments-a-primer/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">跟随这个教程。</em> </a></p><h1 id="1fa0" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">6️⃣优化您的GPU处理</h1><p id="e594" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在你已经准备好了，你可能想要<strong class="je hi">优化你的筹码</strong>。</p><p id="a182" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为此，我们将与一些<strong class="je hi"> CUDA C和</strong><strong class="je hi">cud nn API交互，以执行更快的操作</strong>。这在<strong class="je hi">处理繁重的神经网络</strong>时特别有用，并且对于不使你的训练应用崩溃是决定性的。</p><p id="5416" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这将通过<strong class="je hi">导出一些环境变量</strong>来完成，这些变量将告诉CUDA如何操作。</p><ol class=""><li id="98d0" class="kf kg hh je b jf ka jj kb jn kh jr ki jv kj jz kk kl km kn bi translated"><strong class="je hi"> CUDA_CACHE_DISABLE </strong></li></ol><p id="6c75" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为实时编译禁用缓存(设置为1时)或启用缓存(设置为0时)。禁用时，不会向缓存添加二进制代码，也不会从缓存中检索二进制代码。使用:</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="4799" class="lq if hh lm b fi lr ls l lt lu">os.environ[‘CUDA_CACHE_DISABLE’] = ‘0’</span></pre><p id="42f1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">2.<strong class="je hi">TF _ FORCE _ GPU _ ALLOW _ GROWTH</strong></p><p id="1051" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在某些情况下，希望进程只分配可用内存的子集，或者只根据进程的需要增加内存使用量。TensorFlow提供了两种方法来控制这一点。</p><p id="03ee" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个环境变量试图只分配运行时分配所需的GPU内存:<strong class="je hi">它开始分配很少的内存，随着程序的运行，需要更多的GPU内存，GPU内存区域将为TensorFlow进程扩展</strong>。不释放内存，因为这会导致内存碎片。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="75f1" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'</span></pre><p id="598a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">3.<strong class="je hi"> TF_CPP_MIN_LOG_LEVEL </strong></p><p id="4379" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个变量只是为了禁用TF的警告和日志记录。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="0a8c" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'</span><span id="e780" class="lq if hh lm b fi mn ls l lt lu">'''<br/>   Legend:</span><span id="aa07" class="lq if hh lm b fi mn ls l lt lu">   0 = all messages are logged (default behavior)<br/>   1 = INFO messages are not printed<br/>   2 = INFO and WARNING messages are not printed<br/>   3 = INFO, WARNING, and ERROR messages are not printed<br/>'''</span></pre><p id="8a36" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">4.<strong class="je hi"> TF_GPU_THREAD_MODE </strong></p><p id="c668" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这确保了<strong class="je hi"> GPU内核从它们自己的专用线程</strong>中启动，并且不会在<code class="du lv lw lx lm b">tf.data</code>工作之后排队，并且<strong class="je hi">防止CPU端线程干扰GPU活动</strong>。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="fe04" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'</span></pre><p id="4121" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">5.<strong class="je hi">TF _ USE _ cud nn _ batch norm _ SPATIAL _ PERSISTENT</strong></p><p id="0fbd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当输入张量很小时，持续时间不随输入大小而变化。这是因为张量足够小，以至于内存带宽没有被充分利用。然而，对于较大的输入，持续时间随着大小接近线性地增加；移动两倍的输入和输出值需要两倍的时间。</p><p id="f594" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当输入足够小时，cuDNN可以使用更好的单遍算法(持续批量归一化),在这里，<strong class="je hi">输入被读入片内GPU存储器一次，然后从那里执行统计计算和归一化，而不需要任何额外的数据读取</strong>。更少的数据读取导致片外存储器的流量减少，对于恒定带宽，这意味着持续时间减少。换句话说，<strong class="je hi">空间持久批量规范化比其非持久变体更快。</strong></p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="d345" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'</span></pre><p id="b12e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">6.<strong class="je hi">TF _ ENABLE _ wino grad _ NONFUSED</strong></p><p id="8e70" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该变量支持使用非融合Winograd卷积算法<strong class="je hi">，其中算法的所有步骤都由单独的内核调用</strong>执行。最初的两个内核用于转换输入和滤波器，在此之后，卷积首先计算乘法，其次计算转换，最终获得输出。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="72b1" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'</span></pre><p id="ac29" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">7.<strong class="je hi"> TF_AUTOTUNE_THRESHOLD </strong></p><p id="6622" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该变量提高了用于选择最快卷积算法的自动调谐过程的<strong class="je hi">稳定性。将其设置为较高的值可以提高稳定性，但是在找到最佳算法之前，需要在训练开始时进行大量的尝试。</strong></p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="72c6" class="lq if hh lm b fi lr ls l lt lu">os.environ[‘TF_AUTOTUNE_THRESHOLD’] = ‘1’</span></pre><p id="98cd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">8.<strong class="je hi">TF _ ENABLE _ Cu blas _ TENSOR _ OP _ MATH _ FP32</strong></p><p id="16be" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">此变量启用和禁用TensorFlow中float32矩阵乘法运算的张量核心数学。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="12f6" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_ENABLE_CUBLAS_TENSOR_OP_MATH_FP32'] = '1'</span></pre><p id="8744" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">9.<strong class="je hi">TF _ ENABLE _ cud nn _ TENSOR _ OP _ MATH _ FP32</strong></p><p id="62a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">此变量启用和禁用TensorFlow中float32卷积运算的张量核数学。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="4ecd" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32'] = '1'</span></pre><p id="fa28" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">10.<strong class="je hi">TF _ ENABLE _ cud nn _ RNN _张量_OP_MATH_FP32 </strong></p><p id="db7e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">此变量在TensorFlow中启用和禁用float32 cuDNN RNN运算的张量核数学。默认情况下，float32运算的张量核心数学是禁用的，但可以通过将此变量设置为1来启用。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="0fd0" class="lq if hh lm b fi lr ls l lt lu">os.environ['TF_ENABLE_CUDNN_RNN_TENSOR_OP_MATH_FP32'] = '1'</span></pre><p id="14dd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">可以为特定的神经网络操作导出附加的环境变量。这是一个正在进行的研究课题，本机<strong class="je hi">tensor flow profiler——一种跟踪硬件资源利用率的工具——有助于确定进一步优化应用所需的资源。</strong></p><h1 id="490e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">摘要</h1><p id="a439" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在本文中，您了解了:</p><ol class=""><li id="c79a" class="kf kg hh je b jf ka jj kb jn kh jr ki jv kj jz kk kl km kn bi translated">检查兼容的图形处理器及其驱动程序。</li><li id="411e" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">为了<strong class="je hi">将您的nvidia驱动程序版本</strong>与您的CUDA/cuDNN和TensorFlow堆栈相匹配。</li><li id="6f76" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">为了<strong class="je hi">安装使用TensorFlow及其深度学习库的所有先决条件</strong>。</li><li id="6626" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">为了<strong class="je hi">通过pip包管理器安装TensorFlow </strong>。</li><li id="53e8" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">为了<strong class="je hi">导出工作CUDA堆栈所需的环境变量</strong>。</li><li id="4811" class="kf kg hh je b jf ko jj kp jn kq jr kr jv ks jz kk kl km kn bi translated">导出环境变量，使卷积和矩阵算法运行更快，从而优化您的机器学习操作<strong class="je hi">。</strong></li></ol><div class="mo mp ez fb mq mr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">medium.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf ld mr"/></div></div></a></div></div></div>    
</body>
</html>