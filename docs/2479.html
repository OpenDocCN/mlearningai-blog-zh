<html>
<head>
<title>Manifold Mixup: Learning Better Representations by Interpolating Hidden States</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流形混合:通过插入隐藏状态学习更好的表示</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/manifold-mixup-learning-better-representations-by-interpolating-hidden-states-8a2c949d5b5b?source=collection_archive---------6-----------------------#2022-05-07">https://medium.com/mlearning-ai/manifold-mixup-learning-better-representations-by-interpolating-hidden-states-8a2c949d5b5b?source=collection_archive---------6-----------------------#2022-05-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0ee723ea372fdc106eefa104a48bcde1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3r6eJFUwCgWnWVjC"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@mattmoloney?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Moloney</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="24eb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">流形混合早在2019年就在<a class="ae it" href="https://arxiv.org/abs/1806.05236" rel="noopener ugc nofollow" target="_blank">这篇</a>论文中介绍过，与之前发表的论文<code class="du js jt ju jv b"><a class="ae it" href="https://arxiv.org/abs/1710.09412" rel="noopener ugc nofollow" target="_blank">mixup: Beyond Empirical Risk Minimization</a></code>类似。我们可以把<code class="du js jt ju jv b">Mixup</code>看作流形混搭的一个特例。</p><h1 id="9f28" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">混合(或输入混合)</h1><p id="65b9" class="pw-post-body-paragraph iu iv hh iw b ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr ha bi translated">混合可以通过以下公式实现:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4e6a" class="lh jx hh jv b fi li lj l lk ll">new_image = alpha * image_1 + (1-alpha) * image_2<br/>new_target = alpha * target_1 + (1-alpha) * target_2</span></pre><p id="cb07" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们正在混合两个图像(<code class="du js jt ju jv b">image_1</code>和<code class="du js jt ju jv b">image_2</code>)来创建一个新的图像。我们相应地更新目标值。</p><p id="62ee" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du js jt ju jv b">alpha</code>值是从贝塔分布中取样的，并且在范围<code class="du js jt ju jv b">[0,1]</code>内。</p><p id="37d0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下面是一个与<code class="du js jt ju jv b">alpha=0.4</code>混淆的例子。</p><figure class="kz la lb lc fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/947f99c3519a503370426ce7405ca8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*QJj2h0Kn1cVb-f7tQps3PQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Mixup example</figcaption></figure><p id="4166" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里<code class="du js jt ju jv b">image_1</code>的目标是<code class="du js jt ju jv b">[1,0]</code>，<code class="du js jt ju jv b">image_2</code>的目标是<code class="du js jt ju jv b">[0,1]</code>。因此，最终目标将是<code class="du js jt ju jv b">[0.4, 0.6]</code>。</p><h2 id="10d0" class="lh jx hh bd jy ln lo lp kc lq lr ls kg jf lt lu kk jj lv lw ko jn lx ly ks lz bi translated">履行</h2><ul class=""><li id="3ea3" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">跟随本笔记本一起<a class="ae it" href="https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb" rel="noopener ugc nofollow" target="_blank">执行。</a></li><li id="be65" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">我们可以在训练时使用下面的代码来实现它。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4405" class="lh jx hh jv b fi li lj l lk ll">def mixup_data(x, y, alpha=1.0):<br/>    '''Returns mixed inputs, targets, and lambda<br/>    Parameters<br/>    ----------<br/>    x: input data<br/>    y: target<br/>    alpha: value of alpha and beta in beta distribution <br/>    '''<br/>    if alpha &gt; 0:<br/>        lam = np.random.beta(alpha, alpha)<br/>    else:<br/>        lam = 1</span><span id="2030" class="lh jx hh jv b fi mo lj l lk ll">batch_size = x.size()[0]<br/>    index = torch.randperm(batch_size) # shuffle index</span><span id="86ac" class="lh jx hh jv b fi mo lj l lk ll">mixed_x = lam * x + (1 - lam) * x[index, :] # mixup between original image order and shuffled image order<br/>    y_a, y_b = y, y[index] # return target of both images order<br/>    <br/>    return mixed_x, y_a, y_b, lam</span></pre><ul class=""><li id="d177" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">我们返回<code class="du js jt ju jv b">y_a</code>和<code class="du js jt ju jv b">y_b</code>而不是<code class="du js jt ju jv b">y_mix (alpha * y_a + (1-alpha)*y_b)</code>原因是因为我们在损失函数中进行混合标签操作，这样我们就不必改变损失函数。</li><li id="3b69" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">下面是在混乱中损失的代码。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="8119" class="lh jx hh jv b fi li lj l lk ll">def mixup_criterion(criterion, pred, y_a, y_b, lam):<br/>    """ Updated loss for mixup.<br/>    Args:<br/>    -----<br/>    criterion: loss function to use, example: crossentropy loss<br/>    preds: predictions from network<br/>    y_a: original labels<br/>    y_b: labels of the shuffled batch<br/>    lam: alpha used for mixup<br/>    """<br/>    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)</span></pre><ul class=""><li id="972c" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">所以，如果我们取两幅图像(<code class="du js jt ju jv b">image_1</code>和<code class="du js jt ju jv b">image_2</code>)和它们的目标(<code class="du js jt ju jv b">target_1</code>和<code class="du js jt ju jv b">target_2</code>)。将有一个新的图像<code class="du js jt ju jv b">new_image = alpha * image_1 + (1-alpha) * image_2</code>和一个新的目标<code class="du js jt ju jv b">new_target = alpha * target_1 + (1-alpha) * target_2</code>。</li><li id="2179" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">让我们假设我们通过模型传递<code class="du js jt ju jv b">new_image</code>，并在softmax之后得到<code class="du js jt ju jv b">preds</code>结果向量。那么损失如下:</li></ul><figure class="kz la lb lc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/cc79d4fffab961b6c418527f89d6efbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2z-gUwzJT0j3uRFvjufnLg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Loss with Mixup augmentation</figcaption></figure><ul class=""><li id="d5a5" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">我们使用了最后一个等式来计算损失，而不是上图中的第一个。</li><li id="06b8" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">类似于损耗，我们计算输入混合批次的准确度如下:</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="0457" class="lh jx hh jv b fi li lj l lk ll">def mixup_accuracy(metric, preds, y_a, y_b, lam):<br/>    """<br/>    Updated metric calculation:<br/>    Args:<br/>    -----<br/>    metric: metric to use, example: accuracy<br/>    preds: predictions from network<br/>    y_a: original labels<br/>    y_b: labels of the shuffled batch<br/>    lam: alpha used for mixup<br/>    """</span><span id="3d45" class="lh jx hh jv b fi mo lj l lk ll">return lam * metric(preds, y_a) + (1 - lam) * metric(preds, y_b)</span></pre><p id="9064" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们用它来训练一个分类器。</p><ul class=""><li id="0799" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">创建一个支持混搭和传统训练的闪电模型:</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="87e8" class="lh jx hh jv b fi li lj l lk ll">class Model(pl.LightningModule):<br/>    """<br/>    Lightning model<br/>    """<br/>    def __init__(self, model_name, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):<br/>        """Model trainer class<br/>        Parameters<br/>        ----------<br/>        model_name: Name of the timm model<br/>        num_classes: number of classes in the dataset<br/>        lr: learning rate<br/>        max_iter: maximum iterations<br/>        mix_up: use mixup augmentation or not<br/>        alpha: alpha for beta distribution in mixup<br/>        """<br/>        super().__init__()<br/>        # setup the model<br/>        self.num_classes = num_classes<br/>        self.model = timm.create_model(model_name=model_name, pretrained=True, num_classes=num_classes)<br/>        # setup accuracy metric<br/>        self.metric = torchmetrics.functional.accuracy<br/>        # setup cross entropy loss function <br/>        self.loss = torch.nn.CrossEntropyLoss()<br/>        self.lr = lr<br/>        self.max_iter = max_iter<br/>        self.mix_up = mix_up<br/>        self.alpha = alpha</span><span id="41ad" class="lh jx hh jv b fi mo lj l lk ll">def forward(self, x):<br/>        return self.model(x)<br/>        <br/>    def shared_step(self, batch, batch_idx, is_train=False):<br/>        x, y = batch<br/>        if is_train and self.mix_up: # if mixup is true and train<br/>            # prepare the mixup date<br/>            x, y_a, y_b, lam = mixup_data(x, y, self.alpha)<br/>            x, y_a, y_b = map(Variable, (x, y_a, y_b))<br/>            # pass the new data through model<br/>            logits = self(x)<br/>            # calculate loss<br/>            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)<br/>            # calculate accuracy<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)<br/>        else: # if mixup is false or validation<br/>            # no change in data, we padd the batch data as is<br/>            # pass the data through model<br/>            logits = self(x)<br/>            # calculate loss<br/>            loss = self.loss(logits, y)<br/>            # calculate accuracy<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = self.metric(preds, y)<br/>        <br/>        return loss, acc<br/>    <br/>    def training_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=True)<br/>        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def validation_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=False)<br/>        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def configure_optimizers(self):<br/>        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)<br/>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)<br/>        <br/>        return [optim], [scheduler]</span></pre><p id="496a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">注意</strong>:在mixup等式中提到的alpha和lightning model论证中提到的alpha(姑且称之为<code class="du js jt ju jv b">arg_alpha</code>)可能会混淆。这两个不一样。我们通过从<code class="du js jt ju jv b">(-arg_alpha, arg_alpha)</code>之间的beta分布中选择一个随机值来获得mixup方程的alpha。我们正在用<code class="du js jt ju jv b">mixup_data</code>函数做这件事。</p><p id="9074" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">关于上面的lightning模型，有几点需要注意:</p><ul class=""><li id="fef4" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">基于<code class="du js jt ju jv b">mixup</code>是否启用，我们改变<code class="du js jt ju jv b">shared_step</code>中的数据处理、度量和损失函数。</li><li id="0b18" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">我们只在训练步骤中做<code class="du js jt ju jv b">mixup</code>，为了验证，我们做正常处理。</li></ul><p id="5c06" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用CIFAR-10数据集、数据加载器和转换，如下所示:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="888f" class="lh jx hh jv b fi li lj l lk ll"># standard image transform for classifier<br/>transform = transforms.Compose(<br/>    [transforms.Resize(224),<br/>     transforms.ToTensor(),<br/>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])<br/># batch size, reduce if cuda out of memory (should work fine in colab with gpu)<br/>batch_size = 128<br/># get cifar-10 train set<br/>trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,<br/>                                        download=True, transform=transform)<br/># split train-full set <br/># used trainset have 20000<br/>trainset, trainset_remains = torch.utils.data.random_split(trainset_full, [20000, len(trainset_full)-20000])<br/># create train dataloader<br/>trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,<br/>                                          shuffle=True, num_workers=2)<br/># val dataloader<br/>testset = torchvision.datasets.CIFAR10(root='./data', train=False,<br/>                                       download=True, transform=transform)<br/>testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,<br/>                                         shuffle=False, num_workers=2)<br/># classes in cifar 10<br/>classes = ('plane', 'car', 'bird', 'cat',<br/>           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')<br/></span></pre><p id="981f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">定义教练和检查点回叫。我们可以通过使用<code class="du js jt ju jv b">Model</code>中的<code class="du js jt ju jv b">mix_up</code>参数进行有无混淆的训练</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="19ae" class="lh jx hh jv b fi li lj l lk ll">model = Model(model_name="resnet18", num_classes=len(classes), lr = 0.001, max_iter=20, mix_up=False)<br/>checkpoint_callback = ModelCheckpoint(<br/>    monitor='val_loss',<br/>    dirpath='./checkpoints',<br/>    filename='resnet_18_org-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'<br/>)<br/>trainer = Trainer(<br/>    deterministic=True, <br/>    logger=True, <br/>    callbacks=[checkpoint_callback], <br/>    gpus=[0], # change it based on gpu or cpu availability<br/>    max_epochs=5)</span></pre><p id="04e1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">训练分类器:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="f98d" class="lh jx hh jv b fi li lj l lk ll">trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=testloader)</span></pre></div><div class="ab cl mt mu go mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ha hb hc hd he"><h1 id="267e" class="jw jx hh bd jy jz na kb kc kd nb kf kg kh nc kj kk kl nd kn ko kp ne kr ks kt bi translated">流形混合</h1><ul class=""><li id="fb0e" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">流形混合进一步将混合思想扩展到隐藏层和输入层。我们可以定义流形混合如下。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="d9bd" class="lh jx hh jv b fi li lj l lk ll">new_input = alpha * input_1 + (1-alpha) * input_2<br/>new_target = alpha * target_1 + (1-alpha) * target_2</span></pre><p id="b7d2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，我们已经将<code class="du js jt ju jv b">image_1</code>更改为<code class="du js jt ju jv b">input_1</code>、<code class="du js jt ju jv b">image_2</code>更改为<code class="du js jt ju jv b">input_2</code>并将<code class="du js jt ju jv b">new_image</code>更改为<code class="du js jt ju jv b">new_input</code>。这里，当我们说输入时，它可以在任何深层神经网络层输入。当该层是第一层时，则<code class="du js jt ju jv b">input</code>将是<code class="du js jt ju jv b">image</code>。</p><ul class=""><li id="f529" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">我们随机选择这一层。</li><li id="a9e5" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">我们拍摄两张图像，并通过神经网络传递它们，直到我们到达那一层。</li><li id="f497" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">我们取出中间特征表示(<code class="du js jt ju jv b">image_1</code>的<code class="du js jt ju jv b">input_1</code>和<code class="du js jt ju jv b">image_2</code>的<code class="du js jt ju jv b">input_2</code>)。</li><li id="1e61" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">我们使用上面提到的等式将它们混合起来，以获得新的表示(<code class="du js jt ju jv b">new_input</code>、<code class="du js jt ju jv b">new_target</code>)。</li><li id="004d" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">对于具有混合数据的其余层，我们继续在网络中向前传递。</li><li id="4c4c" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">混合数据的输出用于计算损耗和梯度。</li></ul><h2 id="95a2" class="lh jx hh bd jy ln lo lp kc lq lr ls kg jf lt lu kk jj lv lw ko jn lx ly ks lz bi translated">履行</h2><ul class=""><li id="f578" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">我们将创建一个修改过的<code class="du js jt ju jv b">resnet18</code>模型，它支持流形混合。</li><li id="ae46" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">首先让我们看看timm库提供的基本resnet18模型。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4dd2" class="lh jx hh jv b fi li lj l lk ll">model = timm.create_model(model_name=model_name, pretrained=pretrained, num_classes=classes)</span></pre><ul class=""><li id="f455" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">我们需要选择一些图层来进行混音。resnet18架构有4层，类型为<code class="du js jt ju jv b">nn.Sequential</code>。因此，我们将在此基础上拆分模型。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="b8f9" class="lh jx hh jv b fi li lj l lk ll">def _model_setup(model):<br/>        model_list = []<br/>        count=0<br/>        d = [] # start and end index of the layer blocks<br/>        start_index = 0<br/>        for index, layer in enumerate(model.children()): # check all the layers of the model<br/>            count+=1<br/>            if isinstance(layer, nn.Sequential): # if it is nn.Sequential then update the list d with start and end index<br/>                d.append((start_index, count-1)) <br/>                start_index = count-1<br/>        <br/>        d.append((start_index, len(list(model.children())))) # append any remaining layers<br/>        module_blocks = [ <br/>                nn.Sequential(*list(model.children())[index[0]: index[1]]) for index in d<br/>        ] # insert each module blocks into a list, blocks are created based on the start and end index of each block<br/>        <br/>        return nn.ModuleList(module_blocks) # return the list as ModuleList</span></pre><ul class=""><li id="0b82" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">我们将创建一个中间转发函数，它将开始和结束索引与输入一起接受。它将只通过起始和结束索引内的模型块传递输入。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="0f22" class="lh jx hh jv b fi li lj l lk ll">def _forward(self, x, i=0, j=None):<br/>    assert i&gt;=0 # make sure start index is &gt;=0<br/>    assert j is None or j&lt;=len(self.model_list)-1 # max end index is num of blocks - 1<br/>    assert j is None or i&lt;=j # start index is &lt; the end index<br/>    if j is None: # if j is None pass till the end block<br/>        j = len(self.model_list)</span><span id="2cbb" class="lh jx hh jv b fi mo lj l lk ll">    for model_layer in self.model_list[i:j]:<br/>        x = model_layer(x)</span><span id="91c8" class="lh jx hh jv b fi mo lj l lk ll">    return x</span></pre><ul class=""><li id="58e0" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">现在我们将创建转发函数。</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="ccb2" class="lh jx hh jv b fi li lj l lk ll">def forward(self, x, mixup=False):<br/>    index = None<br/>    lam = None<br/>    if mixup:<br/>        k = np.random.randint(0, self.num_model) # select a random intermediate layer to mixup the o/p<br/>        batch_size = x.size()[0]<br/>        index = torch.randperm(batch_size) # shuffle index<br/>        lam = np.random.beta(self.alpha, self.alpha) # select alpha randomly <br/>        if hasattr(self, "log"): # logging<br/>            self.log("k", k, on_step=True, on_epoch=False, logger=True, prog_bar=True)<br/>            self.log("lam", lam, on_step=True, on_epoch=False, logger=True, prog_bar=True)<br/>        op_int = lam * self._forward(x, i=0, j=k) + (1 - lam) * self._forward(x[index, :], i=0, j=k) # mixup the op of k the layer<br/>        op = self._forward(op_int, i=k, j=None) # pass the mixup remaining layers<br/>    else: # if not mixup pass through all model blocks<br/>        op = self._forward(x, i=0, j=None)</span><span id="0dac" class="lh jx hh jv b fi mo lj l lk ll">    return op, index, lam # return model output, shuffle order, lambda </span></pre><ul class=""><li id="dbe0" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">所以如果我们把所有的部分组合在一起，我们的<code class="du js jt ju jv b">resnet18</code>模型将会是</li></ul><figure class="kz la lb lc fd ii"><div class="bz dy l di"><div class="nf ng l"/></div></figure><ul class=""><li id="b8d7" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">训练时我们可以使用<code class="du js jt ju jv b">Resnet18MM</code>跟随方式</li></ul><figure class="kz la lb lc fd ii"><div class="bz dy l di"><div class="nf ng l"/></div></figure><p id="cffd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，用于流形混合的闪电训练器可以如下实现</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="5e22" class="lh jx hh jv b fi li lj l lk ll">class ModelMM(pl.LightningModule):<br/>    """<br/>    Lightning model<br/>    """<br/>    def __init__(self, mixup_model, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):<br/>        """Model trainer class for manifold mixup<br/>        Parameters<br/>        ----------<br/>        mixup_model: mixup model<br/>        num_classes: number of classes in the dataset<br/>        lr: learning rate<br/>        max_iter: maximum iterations<br/>        mix_up: use mixup augmentation or not<br/>        alpha: alpha for beta distribution in mixup<br/>        """<br/>        super().__init__()<br/>        # setup the model<br/>        self.num_classes = num_classes<br/>        self.model = mixup_model<br/>        # setup accuracy metric<br/>        self.metric = torchmetrics.functional.accuracy<br/>        # setup cross entropy loss function <br/>        self.loss = torch.nn.CrossEntropyLoss()<br/>        self.lr = lr<br/>        self.max_iter = max_iter<br/>        self.mix_up = mix_up<br/>        self.alpha = alpha</span><span id="890b" class="lh jx hh jv b fi mo lj l lk ll">def forward(self, x, mixup=False):<br/>        return self.model(x, mixup)<br/>        <br/>    def shared_step(self, batch, batch_idx, is_train=False):<br/>        x, y = batch<br/>        if is_train and self.mix_up:<br/>            logits, index, lam = self(x, True)<br/>            y_a, y_b = y, y[index]</span><span id="faf2" class="lh jx hh jv b fi mo lj l lk ll"># # x, y_a, y_b, lam = mixup_data(x, y, self.alpha)<br/>            # # x, y_a, y_b = map(Variable, (x, y_a, y_b))<br/>            # logits = self(x)<br/>            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)<br/>        else:<br/>            logits, _, _ = self(x, False)<br/>            loss = self.loss(logits, y)<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = self.metric(preds, y)<br/>        <br/>        return loss, acc<br/>    <br/>    def training_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=True)<br/>        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def validation_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=False)<br/>        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def configure_optimizers(self):<br/>        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)<br/>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)<br/>        <br/>        return [optim], [scheduler]</span></pre><ul class=""><li id="2b32" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">对于流形混合，我们可以使用相同的数据集、数据加载器和转换。</li></ul><h1 id="35a2" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结果</h1><p id="15f0" class="pw-post-body-paragraph iu iv hh iw b ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr ha bi translated">下表显示了在CIFAR-10和CIFAR-100数据集上混合(输入混合)和流形混合的结果。五次重复的标准偏差。</p><figure class="kz la lb lc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nh"><img src="../Images/b4db5606d98b54972b592a41c7992178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2SBmDGDCJnBZELaUzqZkQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Performance of Mixup augmentations from the paper</figcaption></figure></div><div class="ab cl mt mu go mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ha hb hc hd he"><p id="dbb5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">流形混合改进了多层神经网络的隐藏表示和决策边界。这解决了分布变化、异常值和对立例子等问题。</p><p id="6c34" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我个人喜欢使用这种增强，因为它有助于创建健壮的模型。希望这对您有所帮助。喜欢就拍拍文章，喜欢就关注我。过得愉快😃。</p><p id="f687" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">资源:</p><ul class=""><li id="05c8" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">实施笔记本:<a class="ae it" href="https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a></li></ul><div class="ni nj ez fb nk nl"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz in nl"/></div></div></a></div></div></div>    
</body>
</html>