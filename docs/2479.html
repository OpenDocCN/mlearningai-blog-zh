<html>
<head>
<title>Manifold Mixup: Learning Better Representations by Interpolating Hidden States</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">æµå½¢æ··åˆ:é€šè¿‡æ’å…¥éšè—çŠ¶æ€å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/manifold-mixup-learning-better-representations-by-interpolating-hidden-states-8a2c949d5b5b?source=collection_archive---------6-----------------------#2022-05-07">https://medium.com/mlearning-ai/manifold-mixup-learning-better-representations-by-interpolating-hidden-states-8a2c949d5b5b?source=collection_archive---------6-----------------------#2022-05-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0ee723ea372fdc106eefa104a48bcde1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3r6eJFUwCgWnWVjC"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@mattmoloney?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Moloney</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="24eb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">æµå½¢æ··åˆæ—©åœ¨2019å¹´å°±åœ¨<a class="ae it" href="https://arxiv.org/abs/1806.05236" rel="noopener ugc nofollow" target="_blank">è¿™ç¯‡</a>è®ºæ–‡ä¸­ä»‹ç»è¿‡ï¼Œä¸ä¹‹å‰å‘è¡¨çš„è®ºæ–‡<code class="du js jt ju jv b"><a class="ae it" href="https://arxiv.org/abs/1710.09412" rel="noopener ugc nofollow" target="_blank">mixup: Beyond Empirical Risk Minimization</a></code>ç±»ä¼¼ã€‚æˆ‘ä»¬å¯ä»¥æŠŠ<code class="du js jt ju jv b">Mixup</code>çœ‹ä½œæµå½¢æ··æ­çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚</p><h1 id="9f28" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">æ··åˆ(æˆ–è¾“å…¥æ··åˆ)</h1><p id="65b9" class="pw-post-body-paragraph iu iv hh iw b ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr ha bi translated">æ··åˆå¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼å®ç°:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4e6a" class="lh jx hh jv b fi li lj l lk ll">new_image = alpha * image_1 + (1-alpha) * image_2<br/>new_target = alpha * target_1 + (1-alpha) * target_2</span></pre><p id="cb07" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">æˆ‘ä»¬æ­£åœ¨æ··åˆä¸¤ä¸ªå›¾åƒ(<code class="du js jt ju jv b">image_1</code>å’Œ<code class="du js jt ju jv b">image_2</code>)æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾åƒã€‚æˆ‘ä»¬ç›¸åº”åœ°æ›´æ–°ç›®æ ‡å€¼ã€‚</p><p id="62ee" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du js jt ju jv b">alpha</code>å€¼æ˜¯ä»è´å¡”åˆ†å¸ƒä¸­å–æ ·çš„ï¼Œå¹¶ä¸”åœ¨èŒƒå›´<code class="du js jt ju jv b">[0,1]</code>å†…ã€‚</p><p id="37d0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">ä¸‹é¢æ˜¯ä¸€ä¸ªä¸<code class="du js jt ju jv b">alpha=0.4</code>æ··æ·†çš„ä¾‹å­ã€‚</p><figure class="kz la lb lc fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/947f99c3519a503370426ce7405ca8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*QJj2h0Kn1cVb-f7tQps3PQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Mixup example</figcaption></figure><p id="4166" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">è¿™é‡Œ<code class="du js jt ju jv b">image_1</code>çš„ç›®æ ‡æ˜¯<code class="du js jt ju jv b">[1,0]</code>ï¼Œ<code class="du js jt ju jv b">image_2</code>çš„ç›®æ ‡æ˜¯<code class="du js jt ju jv b">[0,1]</code>ã€‚å› æ­¤ï¼Œæœ€ç»ˆç›®æ ‡å°†æ˜¯<code class="du js jt ju jv b">[0.4, 0.6]</code>ã€‚</p><h2 id="10d0" class="lh jx hh bd jy ln lo lp kc lq lr ls kg jf lt lu kk jj lv lw ko jn lx ly ks lz bi translated">å±¥è¡Œ</h2><ul class=""><li id="3ea3" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">è·Ÿéšæœ¬ç¬”è®°æœ¬ä¸€èµ·<a class="ae it" href="https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb" rel="noopener ugc nofollow" target="_blank">æ‰§è¡Œã€‚</a></li><li id="be65" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥å®ç°å®ƒã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4405" class="lh jx hh jv b fi li lj l lk ll">def mixup_data(x, y, alpha=1.0):<br/>    '''Returns mixed inputs, targets, and lambda<br/>    Parameters<br/>    ----------<br/>    x: input data<br/>    y: target<br/>    alpha: value of alpha and beta in beta distribution <br/>    '''<br/>    if alpha &gt; 0:<br/>        lam = np.random.beta(alpha, alpha)<br/>    else:<br/>        lam = 1</span><span id="2030" class="lh jx hh jv b fi mo lj l lk ll">batch_size = x.size()[0]<br/>    index = torch.randperm(batch_size) # shuffle index</span><span id="86ac" class="lh jx hh jv b fi mo lj l lk ll">mixed_x = lam * x + (1 - lam) * x[index, :] # mixup between original image order and shuffled image order<br/>    y_a, y_b = y, y[index] # return target of both images order<br/>    <br/>    return mixed_x, y_a, y_b, lam</span></pre><ul class=""><li id="d177" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æˆ‘ä»¬è¿”å›<code class="du js jt ju jv b">y_a</code>å’Œ<code class="du js jt ju jv b">y_b</code>è€Œä¸æ˜¯<code class="du js jt ju jv b">y_mix (alpha * y_a + (1-alpha)*y_b)</code>åŸå› æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¿›è¡Œæ··åˆæ ‡ç­¾æ“ä½œï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…æ”¹å˜æŸå¤±å‡½æ•°ã€‚</li><li id="3b69" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">ä¸‹é¢æ˜¯åœ¨æ··ä¹±ä¸­æŸå¤±çš„ä»£ç ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="8119" class="lh jx hh jv b fi li lj l lk ll">def mixup_criterion(criterion, pred, y_a, y_b, lam):<br/>    """ Updated loss for mixup.<br/>    Args:<br/>    -----<br/>    criterion: loss function to use, example: crossentropy loss<br/>    preds: predictions from network<br/>    y_a: original labels<br/>    y_b: labels of the shuffled batch<br/>    lam: alpha used for mixup<br/>    """<br/>    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)</span></pre><ul class=""><li id="972c" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬å–ä¸¤å¹…å›¾åƒ(<code class="du js jt ju jv b">image_1</code>å’Œ<code class="du js jt ju jv b">image_2</code>)å’Œå®ƒä»¬çš„ç›®æ ‡(<code class="du js jt ju jv b">target_1</code>å’Œ<code class="du js jt ju jv b">target_2</code>)ã€‚å°†æœ‰ä¸€ä¸ªæ–°çš„å›¾åƒ<code class="du js jt ju jv b">new_image = alpha * image_1 + (1-alpha) * image_2</code>å’Œä¸€ä¸ªæ–°çš„ç›®æ ‡<code class="du js jt ju jv b">new_target = alpha * target_1 + (1-alpha) * target_2</code>ã€‚</li><li id="2179" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬é€šè¿‡æ¨¡å‹ä¼ é€’<code class="du js jt ju jv b">new_image</code>ï¼Œå¹¶åœ¨softmaxä¹‹åå¾—åˆ°<code class="du js jt ju jv b">preds</code>ç»“æœå‘é‡ã€‚é‚£ä¹ˆæŸå¤±å¦‚ä¸‹:</li></ul><figure class="kz la lb lc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ms"><img src="../Images/cc79d4fffab961b6c418527f89d6efbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2z-gUwzJT0j3uRFvjufnLg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Loss with Mixup augmentation</figcaption></figure><ul class=""><li id="d5a5" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æˆ‘ä»¬ä½¿ç”¨äº†æœ€åä¸€ä¸ªç­‰å¼æ¥è®¡ç®—æŸå¤±ï¼Œè€Œä¸æ˜¯ä¸Šå›¾ä¸­çš„ç¬¬ä¸€ä¸ªã€‚</li><li id="06b8" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">ç±»ä¼¼äºæŸè€—ï¼Œæˆ‘ä»¬è®¡ç®—è¾“å…¥æ··åˆæ‰¹æ¬¡çš„å‡†ç¡®åº¦å¦‚ä¸‹:</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="0457" class="lh jx hh jv b fi li lj l lk ll">def mixup_accuracy(metric, preds, y_a, y_b, lam):<br/>    """<br/>    Updated metric calculation:<br/>    Args:<br/>    -----<br/>    metric: metric to use, example: accuracy<br/>    preds: predictions from network<br/>    y_a: original labels<br/>    y_b: labels of the shuffled batch<br/>    lam: alpha used for mixup<br/>    """</span><span id="3d45" class="lh jx hh jv b fi mo lj l lk ll">return lam * metric(preds, y_a) + (1 - lam) * metric(preds, y_b)</span></pre><p id="9064" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">è®©æˆ‘ä»¬ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚</p><ul class=""><li id="0799" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">åˆ›å»ºä¸€ä¸ªæ”¯æŒæ··æ­å’Œä¼ ç»Ÿè®­ç»ƒçš„é—ªç”µæ¨¡å‹:</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="87e8" class="lh jx hh jv b fi li lj l lk ll">class Model(pl.LightningModule):<br/>    """<br/>    Lightning model<br/>    """<br/>    def __init__(self, model_name, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):<br/>        """Model trainer class<br/>        Parameters<br/>        ----------<br/>        model_name: Name of the timm model<br/>        num_classes: number of classes in the dataset<br/>        lr: learning rate<br/>        max_iter: maximum iterations<br/>        mix_up: use mixup augmentation or not<br/>        alpha: alpha for beta distribution in mixup<br/>        """<br/>        super().__init__()<br/>        # setup the model<br/>        self.num_classes = num_classes<br/>        self.model = timm.create_model(model_name=model_name, pretrained=True, num_classes=num_classes)<br/>        # setup accuracy metric<br/>        self.metric = torchmetrics.functional.accuracy<br/>        # setup cross entropy loss function <br/>        self.loss = torch.nn.CrossEntropyLoss()<br/>        self.lr = lr<br/>        self.max_iter = max_iter<br/>        self.mix_up = mix_up<br/>        self.alpha = alpha</span><span id="41ad" class="lh jx hh jv b fi mo lj l lk ll">def forward(self, x):<br/>        return self.model(x)<br/>        <br/>    def shared_step(self, batch, batch_idx, is_train=False):<br/>        x, y = batch<br/>        if is_train and self.mix_up: # if mixup is true and train<br/>            # prepare the mixup date<br/>            x, y_a, y_b, lam = mixup_data(x, y, self.alpha)<br/>            x, y_a, y_b = map(Variable, (x, y_a, y_b))<br/>            # pass the new data through model<br/>            logits = self(x)<br/>            # calculate loss<br/>            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)<br/>            # calculate accuracy<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)<br/>        else: # if mixup is false or validation<br/>            # no change in data, we padd the batch data as is<br/>            # pass the data through model<br/>            logits = self(x)<br/>            # calculate loss<br/>            loss = self.loss(logits, y)<br/>            # calculate accuracy<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = self.metric(preds, y)<br/>        <br/>        return loss, acc<br/>    <br/>    def training_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=True)<br/>        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def validation_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=False)<br/>        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def configure_optimizers(self):<br/>        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)<br/>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)<br/>        <br/>        return [optim], [scheduler]</span></pre><p id="496a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">æ³¨æ„</strong>:åœ¨mixupç­‰å¼ä¸­æåˆ°çš„alphaå’Œlightning modelè®ºè¯ä¸­æåˆ°çš„alpha(å§‘ä¸”ç§°ä¹‹ä¸º<code class="du js jt ju jv b">arg_alpha</code>)å¯èƒ½ä¼šæ··æ·†ã€‚è¿™ä¸¤ä¸ªä¸ä¸€æ ·ã€‚æˆ‘ä»¬é€šè¿‡ä»<code class="du js jt ju jv b">(-arg_alpha, arg_alpha)</code>ä¹‹é—´çš„betaåˆ†å¸ƒä¸­é€‰æ‹©ä¸€ä¸ªéšæœºå€¼æ¥è·å¾—mixupæ–¹ç¨‹çš„alphaã€‚æˆ‘ä»¬æ­£åœ¨ç”¨<code class="du js jt ju jv b">mixup_data</code>å‡½æ•°åšè¿™ä»¶äº‹ã€‚</p><p id="9074" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">å…³äºä¸Šé¢çš„lightningæ¨¡å‹ï¼Œæœ‰å‡ ç‚¹éœ€è¦æ³¨æ„:</p><ul class=""><li id="fef4" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">åŸºäº<code class="du js jt ju jv b">mixup</code>æ˜¯å¦å¯ç”¨ï¼Œæˆ‘ä»¬æ”¹å˜<code class="du js jt ju jv b">shared_step</code>ä¸­çš„æ•°æ®å¤„ç†ã€åº¦é‡å’ŒæŸå¤±å‡½æ•°ã€‚</li><li id="0b18" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æˆ‘ä»¬åªåœ¨è®­ç»ƒæ­¥éª¤ä¸­åš<code class="du js jt ju jv b">mixup</code>ï¼Œä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬åšæ­£å¸¸å¤„ç†ã€‚</li></ul><p id="5c06" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">æˆ‘ä»¬å°†ä½¿ç”¨CIFAR-10æ•°æ®é›†ã€æ•°æ®åŠ è½½å™¨å’Œè½¬æ¢ï¼Œå¦‚ä¸‹æ‰€ç¤º:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="888f" class="lh jx hh jv b fi li lj l lk ll"># standard image transform for classifier<br/>transform = transforms.Compose(<br/>    [transforms.Resize(224),<br/>     transforms.ToTensor(),<br/>     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])<br/># batch size, reduce if cuda out of memory (should work fine in colab with gpu)<br/>batch_size = 128<br/># get cifar-10 train set<br/>trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,<br/>                                        download=True, transform=transform)<br/># split train-full set <br/># used trainset have 20000<br/>trainset, trainset_remains = torch.utils.data.random_split(trainset_full, [20000, len(trainset_full)-20000])<br/># create train dataloader<br/>trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,<br/>                                          shuffle=True, num_workers=2)<br/># val dataloader<br/>testset = torchvision.datasets.CIFAR10(root='./data', train=False,<br/>                                       download=True, transform=transform)<br/>testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,<br/>                                         shuffle=False, num_workers=2)<br/># classes in cifar 10<br/>classes = ('plane', 'car', 'bird', 'cat',<br/>           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')<br/></span></pre><p id="981f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">å®šä¹‰æ•™ç»ƒå’Œæ£€æŸ¥ç‚¹å›å«ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨<code class="du js jt ju jv b">Model</code>ä¸­çš„<code class="du js jt ju jv b">mix_up</code>å‚æ•°è¿›è¡Œæœ‰æ— æ··æ·†çš„è®­ç»ƒ</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="19ae" class="lh jx hh jv b fi li lj l lk ll">model = Model(model_name="resnet18", num_classes=len(classes), lr = 0.001, max_iter=20, mix_up=False)<br/>checkpoint_callback = ModelCheckpoint(<br/>    monitor='val_loss',<br/>    dirpath='./checkpoints',<br/>    filename='resnet_18_org-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'<br/>)<br/>trainer = Trainer(<br/>    deterministic=True, <br/>    logger=True, <br/>    callbacks=[checkpoint_callback], <br/>    gpus=[0], # change it based on gpu or cpu availability<br/>    max_epochs=5)</span></pre><p id="04e1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">è®­ç»ƒåˆ†ç±»å™¨:</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="f98d" class="lh jx hh jv b fi li lj l lk ll">trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=testloader)</span></pre></div><div class="ab cl mt mu go mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ha hb hc hd he"><h1 id="267e" class="jw jx hh bd jy jz na kb kc kd nb kf kg kh nc kj kk kl nd kn ko kp ne kr ks kt bi translated">æµå½¢æ··åˆ</h1><ul class=""><li id="fb0e" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">æµå½¢æ··åˆè¿›ä¸€æ­¥å°†æ··åˆæ€æƒ³æ‰©å±•åˆ°éšè—å±‚å’Œè¾“å…¥å±‚ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰æµå½¢æ··åˆå¦‚ä¸‹ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="d9bd" class="lh jx hh jv b fi li lj l lk ll">new_input = alpha * input_1 + (1-alpha) * input_2<br/>new_target = alpha * target_1 + (1-alpha) * target_2</span></pre><p id="b7d2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»å°†<code class="du js jt ju jv b">image_1</code>æ›´æ”¹ä¸º<code class="du js jt ju jv b">input_1</code>ã€<code class="du js jt ju jv b">image_2</code>æ›´æ”¹ä¸º<code class="du js jt ju jv b">input_2</code>å¹¶å°†<code class="du js jt ju jv b">new_image</code>æ›´æ”¹ä¸º<code class="du js jt ju jv b">new_input</code>ã€‚è¿™é‡Œï¼Œå½“æˆ‘ä»¬è¯´è¾“å…¥æ—¶ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•æ·±å±‚ç¥ç»ç½‘ç»œå±‚è¾“å…¥ã€‚å½“è¯¥å±‚æ˜¯ç¬¬ä¸€å±‚æ—¶ï¼Œåˆ™<code class="du js jt ju jv b">input</code>å°†æ˜¯<code class="du js jt ju jv b">image</code>ã€‚</p><ul class=""><li id="f529" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æˆ‘ä»¬éšæœºé€‰æ‹©è¿™ä¸€å±‚ã€‚</li><li id="a9e5" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æˆ‘ä»¬æ‹æ‘„ä¸¤å¼ å›¾åƒï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œä¼ é€’å®ƒä»¬ï¼Œç›´åˆ°æˆ‘ä»¬åˆ°è¾¾é‚£ä¸€å±‚ã€‚</li><li id="f497" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æˆ‘ä»¬å–å‡ºä¸­é—´ç‰¹å¾è¡¨ç¤º(<code class="du js jt ju jv b">image_1</code>çš„<code class="du js jt ju jv b">input_1</code>å’Œ<code class="du js jt ju jv b">image_2</code>çš„<code class="du js jt ju jv b">input_2</code>)ã€‚</li><li id="1e61" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢æåˆ°çš„ç­‰å¼å°†å®ƒä»¬æ··åˆèµ·æ¥ï¼Œä»¥è·å¾—æ–°çš„è¡¨ç¤º(<code class="du js jt ju jv b">new_input</code>ã€<code class="du js jt ju jv b">new_target</code>)ã€‚</li><li id="004d" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">å¯¹äºå…·æœ‰æ··åˆæ•°æ®çš„å…¶ä½™å±‚ï¼Œæˆ‘ä»¬ç»§ç»­åœ¨ç½‘ç»œä¸­å‘å‰ä¼ é€’ã€‚</li><li id="4c4c" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">æ··åˆæ•°æ®çš„è¾“å‡ºç”¨äºè®¡ç®—æŸè€—å’Œæ¢¯åº¦ã€‚</li></ul><h2 id="95a2" class="lh jx hh bd jy ln lo lp kc lq lr ls kg jf lt lu kk jj lv lw ko jn lx ly ks lz bi translated">å±¥è¡Œ</h2><ul class=""><li id="f578" class="ma mb hh iw b ix ku jb kv jf mc jj md jn me jr mf mg mh mi bi translated">æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä¿®æ”¹è¿‡çš„<code class="du js jt ju jv b">resnet18</code>æ¨¡å‹ï¼Œå®ƒæ”¯æŒæµå½¢æ··åˆã€‚</li><li id="ae46" class="ma mb hh iw b ix mj jb mk jf ml jj mm jn mn jr mf mg mh mi bi translated">é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹timmåº“æä¾›çš„åŸºæœ¬resnet18æ¨¡å‹ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="4dd2" class="lh jx hh jv b fi li lj l lk ll">model = timm.create_model(model_name=model_name, pretrained=pretrained, num_classes=classes)</span></pre><ul class=""><li id="f455" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€äº›å›¾å±‚æ¥è¿›è¡Œæ··éŸ³ã€‚resnet18æ¶æ„æœ‰4å±‚ï¼Œç±»å‹ä¸º<code class="du js jt ju jv b">nn.Sequential</code>ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨æ­¤åŸºç¡€ä¸Šæ‹†åˆ†æ¨¡å‹ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="b8f9" class="lh jx hh jv b fi li lj l lk ll">def _model_setup(model):<br/>        model_list = []<br/>        count=0<br/>        d = [] # start and end index of the layer blocks<br/>        start_index = 0<br/>        for index, layer in enumerate(model.children()): # check all the layers of the model<br/>            count+=1<br/>            if isinstance(layer, nn.Sequential): # if it is nn.Sequential then update the list d with start and end index<br/>                d.append((start_index, count-1)) <br/>                start_index = count-1<br/>        <br/>        d.append((start_index, len(list(model.children())))) # append any remaining layers<br/>        module_blocks = [ <br/>                nn.Sequential(*list(model.children())[index[0]: index[1]]) for index in d<br/>        ] # insert each module blocks into a list, blocks are created based on the start and end index of each block<br/>        <br/>        return nn.ModuleList(module_blocks) # return the list as ModuleList</span></pre><ul class=""><li id="0b82" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä¸­é—´è½¬å‘å‡½æ•°ï¼Œå®ƒå°†å¼€å§‹å’Œç»“æŸç´¢å¼•ä¸è¾“å…¥ä¸€èµ·æ¥å—ã€‚å®ƒå°†åªé€šè¿‡èµ·å§‹å’Œç»“æŸç´¢å¼•å†…çš„æ¨¡å‹å—ä¼ é€’è¾“å…¥ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="0f22" class="lh jx hh jv b fi li lj l lk ll">def _forward(self, x, i=0, j=None):<br/>    assert i&gt;=0 # make sure start index is &gt;=0<br/>    assert j is None or j&lt;=len(self.model_list)-1 # max end index is num of blocks - 1<br/>    assert j is None or i&lt;=j # start index is &lt; the end index<br/>    if j is None: # if j is None pass till the end block<br/>        j = len(self.model_list)</span><span id="2cbb" class="lh jx hh jv b fi mo lj l lk ll">    for model_layer in self.model_list[i:j]:<br/>        x = model_layer(x)</span><span id="91c8" class="lh jx hh jv b fi mo lj l lk ll">    return x</span></pre><ul class=""><li id="58e0" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">ç°åœ¨æˆ‘ä»¬å°†åˆ›å»ºè½¬å‘å‡½æ•°ã€‚</li></ul><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="ccb2" class="lh jx hh jv b fi li lj l lk ll">def forward(self, x, mixup=False):<br/>    index = None<br/>    lam = None<br/>    if mixup:<br/>        k = np.random.randint(0, self.num_model) # select a random intermediate layer to mixup the o/p<br/>        batch_size = x.size()[0]<br/>        index = torch.randperm(batch_size) # shuffle index<br/>        lam = np.random.beta(self.alpha, self.alpha) # select alpha randomly <br/>        if hasattr(self, "log"): # logging<br/>            self.log("k", k, on_step=True, on_epoch=False, logger=True, prog_bar=True)<br/>            self.log("lam", lam, on_step=True, on_epoch=False, logger=True, prog_bar=True)<br/>        op_int = lam * self._forward(x, i=0, j=k) + (1 - lam) * self._forward(x[index, :], i=0, j=k) # mixup the op of k the layer<br/>        op = self._forward(op_int, i=k, j=None) # pass the mixup remaining layers<br/>    else: # if not mixup pass through all model blocks<br/>        op = self._forward(x, i=0, j=None)</span><span id="0dac" class="lh jx hh jv b fi mo lj l lk ll">    return op, index, lam # return model output, shuffle order, lambda </span></pre><ul class=""><li id="dbe0" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰çš„éƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬çš„<code class="du js jt ju jv b">resnet18</code>æ¨¡å‹å°†ä¼šæ˜¯</li></ul><figure class="kz la lb lc fd ii"><div class="bz dy l di"><div class="nf ng l"/></div></figure><ul class=""><li id="b8d7" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">è®­ç»ƒæ—¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code class="du js jt ju jv b">Resnet18MM</code>è·Ÿéšæ–¹å¼</li></ul><figure class="kz la lb lc fd ii"><div class="bz dy l di"><div class="nf ng l"/></div></figure><p id="cffd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">å› æ­¤ï¼Œç”¨äºæµå½¢æ··åˆçš„é—ªç”µè®­ç»ƒå™¨å¯ä»¥å¦‚ä¸‹å®ç°</p><pre class="kz la lb lc fd ld jv le lf aw lg bi"><span id="5e22" class="lh jx hh jv b fi li lj l lk ll">class ModelMM(pl.LightningModule):<br/>    """<br/>    Lightning model<br/>    """<br/>    def __init__(self, mixup_model, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):<br/>        """Model trainer class for manifold mixup<br/>        Parameters<br/>        ----------<br/>        mixup_model: mixup model<br/>        num_classes: number of classes in the dataset<br/>        lr: learning rate<br/>        max_iter: maximum iterations<br/>        mix_up: use mixup augmentation or not<br/>        alpha: alpha for beta distribution in mixup<br/>        """<br/>        super().__init__()<br/>        # setup the model<br/>        self.num_classes = num_classes<br/>        self.model = mixup_model<br/>        # setup accuracy metric<br/>        self.metric = torchmetrics.functional.accuracy<br/>        # setup cross entropy loss function <br/>        self.loss = torch.nn.CrossEntropyLoss()<br/>        self.lr = lr<br/>        self.max_iter = max_iter<br/>        self.mix_up = mix_up<br/>        self.alpha = alpha</span><span id="890b" class="lh jx hh jv b fi mo lj l lk ll">def forward(self, x, mixup=False):<br/>        return self.model(x, mixup)<br/>        <br/>    def shared_step(self, batch, batch_idx, is_train=False):<br/>        x, y = batch<br/>        if is_train and self.mix_up:<br/>            logits, index, lam = self(x, True)<br/>            y_a, y_b = y, y[index]</span><span id="faf2" class="lh jx hh jv b fi mo lj l lk ll"># # x, y_a, y_b, lam = mixup_data(x, y, self.alpha)<br/>            # # x, y_a, y_b = map(Variable, (x, y_a, y_b))<br/>            # logits = self(x)<br/>            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)<br/>        else:<br/>            logits, _, _ = self(x, False)<br/>            loss = self.loss(logits, y)<br/>            preds = torch.argmax(logits, dim=1)<br/>            acc = self.metric(preds, y)<br/>        <br/>        return loss, acc<br/>    <br/>    def training_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=True)<br/>        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def validation_step(self, batch, batch_idx):<br/>        loss, acc = self.shared_step(batch, batch_idx, is_train=False)<br/>        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)<br/>        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)<br/>        <br/>        return loss<br/>    <br/>    def configure_optimizers(self):<br/>        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)<br/>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)<br/>        <br/>        return [optim], [scheduler]</span></pre><ul class=""><li id="2b32" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">å¯¹äºæµå½¢æ··åˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ã€æ•°æ®åŠ è½½å™¨å’Œè½¬æ¢ã€‚</li></ul><h1 id="35a2" class="jw jx hh bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">ç»“æœ</h1><p id="15f0" class="pw-post-body-paragraph iu iv hh iw b ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn ky jp jq jr ha bi translated">ä¸‹è¡¨æ˜¾ç¤ºäº†åœ¨CIFAR-10å’ŒCIFAR-100æ•°æ®é›†ä¸Šæ··åˆ(è¾“å…¥æ··åˆ)å’Œæµå½¢æ··åˆçš„ç»“æœã€‚äº”æ¬¡é‡å¤çš„æ ‡å‡†åå·®ã€‚</p><figure class="kz la lb lc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nh"><img src="../Images/b4db5606d98b54972b592a41c7992178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2SBmDGDCJnBZELaUzqZkQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Performance of Mixup augmentations from the paper</figcaption></figure></div><div class="ab cl mt mu go mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ha hb hc hd he"><p id="dbb5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">æµå½¢æ··åˆæ”¹è¿›äº†å¤šå±‚ç¥ç»ç½‘ç»œçš„éšè—è¡¨ç¤ºå’Œå†³ç­–è¾¹ç•Œã€‚è¿™è§£å†³äº†åˆ†å¸ƒå˜åŒ–ã€å¼‚å¸¸å€¼å’Œå¯¹ç«‹ä¾‹å­ç­‰é—®é¢˜ã€‚</p><p id="6c34" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">æˆ‘ä¸ªäººå–œæ¬¢ä½¿ç”¨è¿™ç§å¢å¼ºï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºåˆ›å»ºå¥å£®çš„æ¨¡å‹ã€‚å¸Œæœ›è¿™å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚å–œæ¬¢å°±æ‹æ‹æ–‡ç« ï¼Œå–œæ¬¢å°±å…³æ³¨æˆ‘ã€‚è¿‡å¾—æ„‰å¿«ğŸ˜ƒã€‚</p><p id="f687" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">èµ„æº:</p><ul class=""><li id="05c8" class="ma mb hh iw b ix iy jb jc jf mp jj mq jn mr jr mf mg mh mi bi translated">å®æ–½ç¬”è®°æœ¬:<a class="ae it" href="https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb" rel="noopener ugc nofollow" target="_blank">è¿™é‡Œ</a></li></ul><div class="ni nj ez fb nk nl"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz in nl"/></div></div></a></div></div></div>    
</body>
</html>