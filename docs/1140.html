<html>
<head>
<title>Tuning Neural Networks Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调整神经网络第一部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8?source=collection_archive---------0-----------------------#2021-10-10">https://medium.com/mlearning-ai/tuning-neural-networks-part-i-normalize-your-data-6821a28b2cd8?source=collection_archive---------0-----------------------#2021-10-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="8e15" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">标准化数据的重要性</h2></div><p id="38eb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">本系列旨在通过研究调整参数如何影响学习内容和学习方式，提供对神经网络的深入理解。内容假设了一些神经网络的先验知识，可以通过阅读</em> <a class="ae jt" rel="noopener" href="/@gallettilance/neural-networks-a-very-simple-derivation-from-logistic-regression-b2b972f29138"> <em class="js">本系列</em> </a> <em class="js">获得。</em></p><p id="c65c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69">第二部分:初始化注意事项</a></p><p id="01cd" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-iii-43dfd0c8600f">第三部分:哪些激活功能让你学会了</a></p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="20c4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在实践中，<strong class="iy hi">神经网络很少对非标准化数据起作用</strong>。除了功能扩展之外，<strong class="iy hi">规范化您的数据将防止您的网络在初始化时冻结</strong>(从而使得以后极不可能甚至不可能了解任何东西)。</p><p id="7f18" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本系列的第一部分中，我们将看到数据标准化对网络学习能力的影响有多大。</p><p id="9982" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们从涉及两个同心圆的分类任务开始</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/e32c3eae596fe0f758122cf2428f5ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*GysGT8XF9ydZ64WBwxewog.png"/></div></figure><p id="57f5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们根据以下规则生成数据:如果x[0] + x[1] ≥ 1，则Y = 1(绿色)，否则Y = 0(蓝色)。我们的目标是从我们生成的样本中找到这个规则的一个很好的近似。为了完成这项任务，我们建立了一个简单的神经网络，它有一个包含两个神经元的隐藏层:</p><pre class="kc kd ke kf fd kj kk kl km aw kn bi"><span id="b6d6" class="ko kp hh kk b fi kq kr l ks kt">model = keras.models.Sequential()<br/>model.add(layers.Dense(2, input_dim=2, activation=SOME_ACTIVATION))<br/>model.add(layers.Dense(1, activation='sigmoid'))<br/>model.compile(loss="binary_crossentropy")</span></pre><p id="76ac" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以把网络想象成这样:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ku"><img src="../Images/27b1ca852300c775d61a3b4577398880.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*qFNkgcga7ZwbWEg4u59rrg.png"/></div></figure><p id="584e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">注意:</strong>我们使用最后一层的sigmoid激活和<code class="du kv kw kx kk b">binary_crossentropy</code>损失，以便利用来自逻辑回归的直觉，其中我们两个类之间的决策边界将是h0和h1的线性函数。回想一下，如果我们不激活隐藏层，那么h0和h1是x0和x1的线性函数，这意味着我们回到了传统的逻辑回归。</p><p id="9e4f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用<strong class="iy hi"> ReLU </strong>激活我们的隐藏层(我们将在<a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-iii-43dfd0c8600f">第三部分</a>中深入查看激活函数)，因为如果<strong class="iy hi"> ReLU </strong>的输入大于0，神经元将被激活，否则将被禁用。这将使关于激活的推理更加容易。</p><p id="9a15" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在训练上述模型时，我们可以在学习过程中定期绘制决策边界，以获得以下动画:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/b404cc95c7f4c0db095271224d3de97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*SzCpYxm0LKJxC7Baex4Y-Q.gif"/></div></figure><p id="9f19" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">ReLU似乎给了我们接近决策边界的线性片段。回想一下，我们的网络正在对学习到的特征h0和h1执行逻辑回归，因此我们可以在学习到的特征空间中绘制决策边界:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/0a43323f9116799aec702f74b2efdcb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*X8CyCTG2MQwzNgPLOxQNTA.png"/></div></figure><p id="b1f6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在很大程度上，似乎当h0被激活时，h1没有被激活，反之亦然。当h0或h1被激活时，它输出激活它的x0和x1的线性组合——我们的数据的变换，然后网络线性分离它，如上所示。</p><p id="91c9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">回想一下，在给定层中负责轻推权重的梯度与前一层中神经元的激活成比例。如果用于估计损失的一批数据的神经元激活为0，则在反向传播期间不更新权重。</p><p id="002e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">激活的神经元数量会对我们的网络能够学习的东西产生很大的影响。<strong class="iy hi">例如，有可能我们的数据在初始化时没有激活任何神经元。</strong>这很糟糕，因为我们的网络将无法学习。</p><p id="fee4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种情况发生的可能性有多大？</p><p id="b04a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">阅读<a class="ae jt" href="https://keras.rstudio.com/reference/initializer_glorot_uniform.html" rel="noopener ugc nofollow" target="_blank"> Keras文档</a>我们看到默认的初始化方法是在区间[-L，L]上均匀随机生成权重，其中L取决于层。</p><p id="a8ea" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们继续用上面的网络，假设所有权重都来自U(-1，1)。如果x[0]和x[1]各自也来自U(-1，1)，那么激活神经元的数量分布是怎样的？暂时忽略偏差，网络中两个隐藏神经元的WX大于0的频率是多少？</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/9ff34ca0cf54d2e1186b47fa3ca065a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ZtYlw3_X2Ix6c3IbCSqK1g.png"/></div></figure><p id="c2fb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">平均来看，我们似乎可以预期有一半的神经元被激活。但是数据的比例是多少呢？</p><p id="cc61" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们在上面看到，我们的网络将h0和h1线性分离，这是使它们各自激活的数据部分的线性组合。<strong class="iy hi">如果<em class="js">所有</em>数据激活h0和h1，那么学习到的特征只是输入的线性函数，这意味着我们回到逻辑回归。这种情况发生的可能性有多大？</strong></p><p id="f67e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使给定神经元在初始化时激活的数据比例的分布很难计算，因此让我们运行一个模拟(现在再次忽略偏差项):</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/bf6c95bcdb4b2cccd877eb4ff2378dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zNJ36-D_oeQmeCFHt4aPAA.png"/></div></figure><p id="bb2b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，对于这个特定的网络，我们很有可能有大约一半的数据会在初始化后激活给定的神经元——每个神经元随机获得我们数据的50%。这似乎是由于我们的数据在0附近的对称性。</p><p id="ccb1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">如果我们的数据不以0为中心会怎么样？</strong>这是当我们的数据从U(-1，1)移动到U(8，10)而权重保持U(-1，1)时，负责激活给定神经元的数据比例分布的动画。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/5f7218b896b8b025f45f86c263781750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*KH6UPbSN91LeYWFAIPnf-w.gif"/></div></figure><p id="40c3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随着我们的数据远离0，神经元在初始化时要么基于所有数据激活，要么不基于任何数据。这正是我们想要避免的情况。</p><p id="c47e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<strong class="iy hi"> ReLU </strong>的情况下，如果一个神经元从不激活(或者只对一小部分数据激活)，那么它的关联权重将永远不会(或者很少)得到更新。如果神经元在我们所有的数据上激活，那么神经元正在学习数据的全局线性转换——将我们带回逻辑回归。</p><p id="0452" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">这是所有神经元的情况，所以增加更多的层和神经元对我们没有帮助。</strong></p><p id="83c2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以0为中心是否影响激活神经元数量的分布？同样，将权重分布固定为U(-1，1)，但改变数据的分布:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/3fb61e68379ba77f91bef0cc7bb83ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Ap2zV6_6XOadNrqoWp4tJw.gif"/></div></figure><p id="53ee" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，我们仍然可以预计大约一半的神经元会被激活——但随着数据远离0，这些神经元将被接近所有数据或没有数据激活。</p><p id="45d2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这有直观的意义:如果X的所有值都是正的，那么WX &gt; 0的几率是W &gt; 0的几率——如果W以零为中心，这个几率是一半。</p><p id="3e8c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们在一个实际的神经网络上想象这种现象。</p><p id="c2b5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于我们提供给网络的每个数据点，我们可以可视化每个神经元的激活。左边是初始化后的激活(如上面的隐藏层——绿色——使用<strong class="iy hi"> ReLU </strong>激活，最后一层使用sigmoid ),右边是训练后的激活:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/788637c0ec06d83cb079def55605def6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-DxICMO2Ctb-At_5djBjeA.gif"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Left = at initialization, Right = after training | Green/Blue = activated, White= not activated</figcaption></figure><p id="0e8c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">我们看到了许多运动和可变性，因为每个神经元都可以访问我们数据的随机部分(我们预计约为50%</strong>。任何两个神经元的这些部分之间有多少重叠是随机的(事实上U(0，1/2))。</p><p id="dd27" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下面是学习过程的动画:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/1eaa248ae7721b59d65357326c2ed89d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Q6aqVVdbeEqbSIQDB5qGxA.gif"/></div></figure><p id="28be" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">现在来看看数据非规格化会发生什么！</strong>对于以[10，10]为中心的数据，您将在左侧看到初始化后的网络，在右侧看到训练后的网络:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/cfab5e3463accd0e9e4d3ecedc0aefd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jYJipfox8zFtruUuoGFe3w.gif"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Left = at initialization, Right = after training | Green/Blue = activated, White= not activated</figcaption></figure><p id="29f1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">网络似乎冻结了</strong>。这些特征或者是数据的全局线性变换，或者是恒定的0。经过一段时间的训练后(右图),这种情况并没有改善，因为它需要找到只激活部分数据的权重，而我们已经确定这种情况发生的概率非常低。我们没有学到任何新东西…下面是学习过程的动画:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/8103fd8a7f77ef48a6c297e5960acb4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*FW7fqoW6xvNi2KFB4FW3Hw.gif"/></div></figure><p id="afb5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">那么这仅仅是一个<strong class="iy hi"> ReLU </strong>的问题吗？或者我们可以使用不同的激活函数来绕过我们的非标准化数据所创建的学习死胡同吗？</p><p id="65a7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这取决于数据偏离零有多远。考虑sigmoid函数:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/e646dd57e99f4eedd7c4589447a2c7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zRubj1E-e6JpJVGVabuXtw.png"/></div></figure><p id="0aa6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果数据如上所述在[10，10]附近，那么由于WX可以采取非常大的幅度，我们可以预期<strong class="iy hi"> 𝞂(WX) </strong>在大多数时间有效地为0或有效地为1。在这两种情况下，神经元都学习了一个对学习过程没有帮助的恒定特征。</p><p id="5168" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以<strong class="iy hi">我们可以预期其他激活函数也会有同样的行为。</strong></p><p id="adde" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是初始化后的激活(左侧)和训练后的激活(右侧),用于与上述相同的网络，但对所有隐藏层使用sigmoid激活:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/7e19b200eadf6c7a8e525c044fa93df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hvf2kTEqfb0MNCIZpGjghg.gif"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Left = at initialization, Right = after training | Green/Blue = activated, White= not activated</figcaption></figure><p id="442d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">初始化时的激活和学习后的激活之间似乎没有太大的区别。我们可以在学习过程中绘制决策边界，看看学到了什么:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/eaa2aae287d4d3f0f8c6d192e4a32716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*WXAU8QXCdNSmXT8qozLdgg.gif"/></div></figure><p id="8dd2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">看起来我们的模型仍然只是在做逻辑回归。</p><p id="b8f1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了让模型有效学习，需要分而治之。<strong class="iy hi">每个神经元应该学习对一部分数据的转换。然后可以聚集来自每个神经元的局部变换来完成全局学习任务。</strong></p><p id="299c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">所以在尝试各种激活功能、初始化方法、架构等之前，先看看数据</strong>。<strong class="iy hi">标准化数据不仅仅是利用神经网络力量的良好实践</strong>—<strong class="iy hi">这基本上是一项要求。</strong></p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="6d24" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-ii-considerations-for-initialization-4f82e525da69">第二部分:初始化注意事项</a></p><p id="9bbe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae jt" rel="noopener" href="/@gallettilance/tuning-neural-networks-part-iii-43dfd0c8600f">第三部分:哪些激活功能让你学会了</a></p><h1 id="43dc" class="lh kp hh bd li lj lk ll lm ln lo lp lq in lr io ls iq lt ir lu it lv iu lw lx bi translated">感谢</h1><p id="3a96" class="pw-post-body-paragraph iw ix hh iy b iz ly ii jb jc lz il je jf ma jh ji jj mb jl jm jn mc jp jq jr ha bi translated">感谢James Kunstle、Yijin Yang、Cameron Garrison、Maria Shevchuk、Lin、Christina Xu和的贡献。</p></div></div>    
</body>
</html>