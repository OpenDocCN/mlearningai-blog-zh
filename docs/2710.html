<html>
<head>
<title>Dimensionality Reduction: Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维:主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/dimensionality-reduction-principal-component-analysis-d990233070e9?source=collection_archive---------7-----------------------#2022-06-01">https://medium.com/mlearning-ai/dimensionality-reduction-principal-component-analysis-d990233070e9?source=collection_archive---------7-----------------------#2022-06-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="eb5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">萨提亚·克里希南·苏雷什</p><p id="f0c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据集由许多要素组成。只要这些特征以某种方式与目标相关，并且在数量上是最优的，机器学习模型就能够在从数据中学习之后产生体面的结果。但是，如果特征的数量很大，并且大多数特征对模型的学习没有贡献，则模型的性能会下降，并且输出预测所花费的时间也会增加。通过将原始特征空间变换到子空间来减少维数的过程是执行维数减少的一种方法，主成分分析(PCA)就是这样做的。因此，让我们来看看PCA的构建概念。与文章相关的代码可以在<a class="ae jc" href="https://github.com/SathyaKrishnan1211/Low-key-ML/blob/master/Notebooks/Principal_Component_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="1031" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">维数灾难:<br/> </strong>在开始PCA之前，我们先来了解一下为什么拥有大量特征是个问题。请看下图，该图显示了著名的“mnist”数据集中的实例。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/0df7bf8205982b69a3e4fa2221689ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*6fbeTsvlNlKF4FsGSUFVcQ.png"/></div></figure><p id="afd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个图像的形状是28x28，但是正如你所看到的，超过一半的区域对模型的学习没有贡献，但是模型仍然适合它们。空白占据了大量的内存，并且减慢了模型的学习过程。像这样的特征不仅存在于这个数据集中，而且存在于大多数真实世界的数据集中。看一下下图。在不应用PCA的情况下，我们得到0.78的f1_score，并且在应用PCA之后，特征的数量减少到200，并且f1_score大约为0.71。特征的数量少于原始特征数量的三分之一，并且f1_score相当接近用所有特征获得的分数，并且训练过程已经被加速。这就是降维的好处。现在我们来了解一下PCA。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/462d75b9e3defb63234b1ba3eb2ada94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FQCzn4-DORiBYDGlutIjw.png"/></div></div></figure><p id="1021" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">主成分分析:<br/> </strong>主成分分析是一种无监督的降维技术。PCA将高维空间(n)中存在的数据转换到较低维的子空间(d和d&lt;T11】n)中，使得信息损失最小。PCA通过使用协方差矩阵、特征向量和特征值来实现这一点。那么什么是协方差矩阵、特征向量和特征值呢？</p><p id="119c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">协方差矩阵:<br/> </strong>协方差矩阵是从n x d数据集构建的d x d矩阵，其中n是实例的数量，d是数据集中存在的要素的数量。它由数据集中所有可能的要素对的协方差组成。因为所有的行和列都表示特征，所以出现在第<em class="jq"> i </em>行和第<em class="jq"> j </em>列的值与出现在第<em class="jq"> j </em>行和第<em class="jq"> i </em>列的值相同。从公式中很容易看出对角线是由每个特征的方差组成的。如果两个特征之间的协方差为正，则当一个特征增加时，另一个特征也会增加。如果协方差为零，则一个要素的变化不会影响另一个要素，如果协方差为负，则两个要素都将向相反方向增加。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jr"><img src="../Images/e78f59ac53e08b9953115387bd9b57c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*pu4r9R2ChIhhdi0nxAJ1Og.png"/></div></figure><p id="a2d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">本征向量和本征值:<br/> </strong>本征向量是这样一种向量，当其经历线性变换时，仅改变其长度，就好像它被乘以标量一样。特征向量在PCA中用于形成低维子空间的轴，因此它们也被称为<strong class="ig hi">主分量</strong>。每个特征向量捕获数据中存在的一定量的方差。捕捉最多的将形成子空间。这里最大的优点是没有两个特征向量捕获相同的信息，因为它们彼此垂直，因此没有数据重复。在下面给出的等式中,“C”是一个矩阵,“v”是一个向量。标量“psi”称为特征值。当矩阵乘以“v”时，它不会导致“v”改变方向或移位，而是收缩或扩张。本征向量的幅度由相应的本征值给出，因为对于给定的矩阵可能有一个以上的本征向量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/18bcc186038d9ca6ef483f33e2e63675.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*5MKluutYyaW4LTqa7iJEDQ.png"/></div></figure><p id="cec4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> PCA步骤:<br/> </strong>现在我们已经知道了进行PCA所需的概念，下面我们就来列出PCA进行降维所遵循的步骤。<br/> 1。将数据标准化或规范化。<br/> 2。形成协方差矩阵。<br/> 3。将协方差矩阵分解成特征向量和特征值。<br/> 4。按照特征值的降序对特征向量进行排序。<br/> 5。选择前k个特征向量，其中k是子空间的维数。<br/> 6。形成维数为d×k的矩阵W，其中d是原特征空间的维数，k是子空间的维数。7。通过将数据与w相乘来转换数据。</p><ol class=""><li id="2972" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated"><strong class="ig hi">标准化或规范化数据<br/> </strong>在构建协方差矩阵之前标准化或规范化数据非常重要，因为如果每个轴具有不同的尺度，将很难将数据转换到更低维度的子空间。</li><li id="1ff7" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">形成协方差矩阵<br/> </strong>协方差矩阵由使用上述公式提供的数据集构建而成。下图显示了如何使用numpy来完成。它的形状是13 x 13，因为“葡萄酒”数据集中有13个特征。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/67062df4ea555aa2bcde401d9fed4672.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*S-ugFIdDMSMdHpewXbguQA.png"/></div></figure><p id="b664" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.<strong class="ig hi">将协方差矩阵分解为本征向量和本征值<br/>T5】使用numpy中的线性代数包从协方差矩阵中获得本征值和向量。找到协方差矩阵的十三个特征向量，每个向量的维数为1×13。特征值代表由每个特征向量捕获的变化量。第一个特征向量捕捉最大的方差，随后是第二个向量，依此类推。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ki"><img src="../Images/874979bb8457a3913e9a7e063fed5974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4MU1I272eU7r8Oc3VQuuQ.png"/></div></div></figure><p id="e86e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在是引入解释方差比率和总解释方差的时候了。解释方差比是特征值和所有特征值之和的分数，而总解释方差是各个主成分的所有方差之和。下图显示了每个特征向量的解释方差比和累积解释比。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kj"><img src="../Images/439a7fb523fa3fb6a644f4d0849aadd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kUiqxB3AmqUNxKXYiuE98w.png"/></div></div></figure><p id="92c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后四个步骤可以合并，因为它们相当简单。下面的代码展示了所有这些。首先，用具有最高特征值的两个特征向量形成W矩阵。然后，数据从13维数据集转换为2维数据集。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/8445c3a83d8588865aa8d34f69d8f240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*e-FqcNctUSgDgR3huJXPLA.png"/></div></figure><p id="5f1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我讨论的所有内容都被合并到下面的代码中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kl"><img src="../Images/3f4fa5abe5331f1508309fd448a776e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z8WU_thzr6uvv1-46kmoaA.png"/></div></div></figure><p id="2905" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:<br/> </strong>本文讨论了最流行的降维技术之一PCA。在以后的文章中，将讨论后续的技术。希望你和我一样喜欢这篇文章。</p><div class="km kn ez fb ko kp"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hi fi z dy ku ea eb kv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">medium.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld jj kp"/></div></div></a></div></div></div>    
</body>
</html>