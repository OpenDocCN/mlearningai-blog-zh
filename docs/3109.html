<html>
<head>
<title>AdaBoost from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/adaboost-from-scratch-f8979d961948?source=collection_archive---------3-----------------------#2022-07-21">https://medium.com/mlearning-ai/adaboost-from-scratch-f8979d961948?source=collection_archive---------3-----------------------#2022-07-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="c94f" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">AdaBoost算法的解释和实现</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/fe1be60e5cdcc8e741b5fd29339ac1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oR_RoR1jPzrxNwGrRtzr3g.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@timmarshall?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Tim Marshall</a> on <a class="ae jm" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5b5c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">AdaBoost (Adaptive Boosting)是由Yoav Freund和Robert Schapire于1995年开发的一种分类Boosting算法。他们因此获得了2003年的哥德尔奖。AdaBoost(以及其他助推方法)从大量弱学习者中创建强学习者。</p><p id="1779" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">弱学习器(或分类器)是可以给出比随机猜测更好的结果，但是仍然不能提供足够的性能的模型。使用Adaboost，我们可以组合许多弱分类器。这些弱分类器从彼此的错误中逐步学习，整体上创建了一个强模型。这些弱单元分类器被称为基分类器，通常是决策树。</p><h1 id="e871" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">伪代码</h1><p id="5842" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">为每次观察设置相等的权重(w=1/n)</p><p id="7289" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于t=1:T:</p><p id="1b58" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">训练弱分类器</p><p id="e693" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">选择系数α(重要性)</p><p id="d035" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">更新权重</p><p id="7e24" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">结束于</p><p id="ff4f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">输出:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lg"><img src="../Images/d8986db290256f850d1724eb5b040cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*ordJVxl1ak-oEJxyDbDNtA.png"/></div></figure><h1 id="daf9" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">算法</h1><p id="0246" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">让我们通过一个例子来更好地理解这个算法。如下表所示，我们有一个由8个观察值和3个特征(2个分类特征和1个连续特征)组成的数据集。4个观察值为正值，4个为负值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lh"><img src="../Images/41163816973979ee8df9d311a0c8dde1.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*wbGFEDz75DXWeBzpAh0KnQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Dataset</figcaption></figure><p id="1b9a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">首先，我们相等地初始化权重；w = (1/n) = 0.125</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es li"><img src="../Images/ad82f4cc8b3a586fee67483f82489ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*VXAREdKsEoddJu4Jhltzfg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Weights initialized.</figcaption></figure><p id="7211" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我们创建我们的基础学习者(决策树)。每个决策树只有一个深度。这些树被称为树桩。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lj"><img src="../Images/c59a4e9c543de8e7adf66114f9b5da95.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*xpaJhgfskVKSbQzlfyQ9cA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Stump. Source: <a class="ae jm" href="https://en.wikipedia.org/wiki/Decision_stump" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="7125" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于每个特征，我们创建一个树桩。为了选择在序列中首先从哪个基础学习者开始，计算树桩的熵(或基尼指数，无论你选择哪个)。选择具有最低熵的树桩。</p><p id="1ccf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果你想记住决策树；</p><div class="lk ll ez fb lm ln"><a href="https://python.plainenglish.io/decision-tree-parameters-explanations-tuning-a2b0749976e5" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">决策树参数解释</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">Sklearn的决策树参数解释</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">python .平原英语. io</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb jg ln"/></div></div></a></div><h2 id="53db" class="mc kk hh bd kl md me mf kp mg mh mi kt jw mj mk kv ka ml mm kx ke mn mo kz mp bi translated"><strong class="ak">创造树桩</strong></h2><p id="b0ab" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">对于分类特征；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mq"><img src="../Images/975a7cd378165f017bc68fff129ec889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSz5xQ0ooowCkGmKbr9KIQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">First stumps for categorical features</figcaption></figure><p id="2571" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果我们根据特征1分割目标；</p><ul class=""><li id="3315" class="mr ms hh jp b jq jr jt ju jw mt ka mu ke mv ki mw mx my mz bi translated">对于黑色类别:3个正确的预测和2个错误的预测。</li><li id="1feb" class="mr ms hh jp b jq na jt nb jw nc ka nd ke ne ki mw mx my mz bi translated">红色类别:2个正确预测和1个错误预测。</li></ul><p id="b7c0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">为连续的数值特征；</strong></p><ul class=""><li id="4ca2" class="mr ms hh jp b jq jr jt ju jw mt ka mu ke mv ki mw mx my mz bi translated">对要素进行排序</li><li id="dd9f" class="mr ms hh jp b jq na jt nb jw nc ka nd ke ne ki mw mx my mz bi translated">获取相邻值的中点</li><li id="d5d7" class="mr ms hh jp b jq na jt nb jw nc ka nd ke ne ki mw mx my mz bi translated">计算每个分割点的基尼系数(或信息增益)</li></ul><p id="86b6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在我们的情况下；</p><ul class=""><li id="3e1f" class="mr ms hh jp b jq jr jt ju jw mt ka mu ke mv ki mw mx my mz bi translated">排序:98，119，127，128，135，136，154，157</li><li id="2bd7" class="mr ms hh jp b jq na jt nb jw nc ka nd ke ne ki mw mx my mz bi translated">中点:108.5，123，127.5，131.5，<strong class="jp hi"> 135.5，</strong> 145，155.5</li><li id="f51f" class="mr ms hh jp b jq na jt nb jw nc ka nd ke ne ki mw mx my mz bi translated">基尼系数:0.429，0.333，0.467，0.375，<strong class="jp hi"> 0.2 </strong>，0.333，0.429</li></ul><p id="c80e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">135.5是分割的最佳值，因为它具有最低的杂质。</p><p id="dcba" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">3个树桩的基尼系数分别为:</p><p id="7fd0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">S1= 0.47，S2=0.5，S3=0.2</p><p id="d066" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">具有最低基尼值的树桩将是序列中的第一个树桩。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nf"><img src="../Images/5669d0f4c4cf0a1a78ad3259b92af8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*B_uNXrfiNDzqzr9Te5G3Sg.png"/></div></figure><h2 id="5de2" class="mc kk hh bd kl md me mf kp mg mh mi kt jw mj mk kv ka ml mm kx ke mn mo kz mp bi translated">总误差</h2><p id="c468" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">总误差是不正确预测的权重之和。我们有1个不正确的预测，因此总误差为0.125。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ng"><img src="../Images/a0fc6e750b3510a33b6ca7b4e936f02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*itQK5U7TWu0uEXvQqJRBlg.png"/></div></figure><h2 id="8c44" class="mc kk hh bd kl md me mf kp mg mh mi kt jw mj mk kv ka ml mm kx ke mn mo kz mp bi translated">重要</h2><p id="158b" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">我们现在将计算基本分类器(stump)的重要性(发言权或影响力的数量)。从公式中可以理解，总误差越小，基本分类器的重要性越大。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nh"><img src="../Images/ce470a10c4f24af58c5132018ed20275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*yQjcsUGfdSjb0b5hXAy_WQ.png"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ni"><img src="../Images/3dd622fc5c000d5177e856ed02465673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*JjJ72j7c61dl2fA6b3dYCw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Total error vs importance. <a class="ae jm" href="https://vitalflux.com/adaboost-algorithm-explained-with-python-example/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e52d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当总误差为0.5时，分类器的显著值为0(抛硬币)。如果树桩一直做得不好，它的重要性就变成了负数。</p><p id="f17d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在我们的例子中；总误差为0.125。所以，重要性= 0.97</p><p id="228d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我们将更新权重，以便下一个树桩将当前树桩犯下的错误计入帐户。</p><p id="797b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">第一个树桩误判了一个观察。我们现在将增加错误观察的权重，减少其他的权重，以便下一个stump考虑到这一点。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nj"><img src="../Images/2f3abc2a4d86e8d84e380da0ad846c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*KaeqtrMBwhCKZdbwCrzb-A.png"/></div></figure><p id="cf43" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">重量= (1/8 x 2.64) = 0.33 = &gt;要增加的新重量。</p><p id="503f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了减轻重量，我们将使用重要性的负值(^-importance).)</p><p id="24bd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">重量= (1/8 * 0.38) = 0.5 = &gt;要减少的新重量</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nk"><img src="../Images/28be99a56b533ce7fceb5a60a1003e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*V6T95liXN2-CHqSuN_StQA.png"/></div></figure><p id="f93d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们应该把新的权重归一化，使之加起来等于1。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nl"><img src="../Images/aae06385cd337600791c16894b1dbf21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrQhhkDbh7J1sC-8EACY5w.png"/></div></div></figure><p id="8ca1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，在这一阶段，我们将随机选择值，当该值落在权重的累积分布中时，我们将该观察值移动到新创建的空数据集。也就是说，我们创建一个随机选择的新数据集。例如，如果随机选择的值在0.14和0.21之间，我们将移动第三个观察值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nm"><img src="../Images/966af69e77fe7a94d9088d1b598c67bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTlDODTxpzGSCCA4OzdZVg.png"/></div></div></figure><p id="6c2a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">权重较高的观测值在新数据集中出现的概率较高。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nn"><img src="../Images/e1641408200e5cd7db09e671bdaf84fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*fcARSHozttpU8zNEsKcD4Q.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Random dataset</figcaption></figure><p id="df6f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">从这一点上，我们从头开始我们的新数据集。</p><h1 id="6bac" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">Python代码</h1><h2 id="7839" class="mc kk hh bd kl md me mf kp mg mh mi kt jw mj mk kv ka ml mm kx ke mn mo kz mp bi translated">从头做起</h2><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="no np l"/></div></figure><p id="1eca" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们创建一个虚拟数据集进行测试；</p><pre class="ix iy iz ja fd nq nr ns nt aw nu bi"><span id="c185" class="mc kk hh nr b fi nv nw l nx ny">from sklearn.datasets import make_gaussian_quantiles</span><span id="68fd" class="mc kk hh nr b fi nz nw l nx ny">def generate_dummy(n):<br/>    n_per_class = int(n/2)<br/>    X, y = make_gaussian_quantiles(n_samples=n, n_features=2, n_classes=2)<br/>    return X, y*2-1</span><span id="606c" class="mc kk hh nr b fi nz nw l nx ny">X, y = make_toy_dataset(n=100)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oa"><img src="../Images/91f1825f737f38edb02e5ffac8e6b6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDgp_j65lyX9MbdSl9v1zw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Plot of dummy data</figcaption></figure><p id="f795" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">测试:</p><pre class="ix iy iz ja fd nq nr ns nt aw nu bi"><span id="180d" class="mc kk hh nr b fi nv nw l nx ny">from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import roc_auc_score</span><span id="afb7" class="mc kk hh nr b fi nz nw l nx ny">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)</span><span id="7cc6" class="mc kk hh nr b fi nz nw l nx ny">A = AdaBooster(400)<br/>A.fit(X_train, y_train)<br/>y_pred = A.predict(X_test)<br/>print('The ROC-AUC:', round(roc_auc_score(y_test, y_pred), 4))</span><span id="b92e" class="mc kk hh nr b fi nz nw l nx ny">#OUT: <br/>0th iteration; error: 0.3500000000000007<br/>100th iteration; error: 0.388889913934225<br/>200th iteration; error: 0.43300048902688026<br/>300th iteration; error: 0.4141223413353688<br/>The ROC-AUC score of the model is: 0.9286</span></pre><h2 id="ea5e" class="mc kk hh bd kl md me mf kp mg mh mi kt jw mj mk kv ka ml mm kx ke mn mo kz mp bi translated">Sklearn</h2><pre class="ix iy iz ja fd nq nr ns nt aw nu bi"><span id="fee1" class="mc kk hh nr b fi nv nw l nx ny">from sklearn.ensemble import AdaBoostClassifier</span></pre><p id="399f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">参数</strong>；</p><p id="52dd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">base_estimator: <em class="ob">对象</em> = &gt;模型类型；默认为决策树。</p><p id="eb9b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">n_estimators: <em class="ob"> int = &gt; </em>迭代次数；默认值为50。</p><p id="bb48" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">learning _ rate:<em class="ob">float</em>=&gt;应用于基本分类器的权重可以通过此参数进行调整。较高的值会增加每个树桩的贡献。默认值为1。</p><pre class="ix iy iz ja fd nq nr ns nt aw nu bi"><span id="f4ab" class="mc kk hh nr b fi nv nw l nx ny">A = <strong class="nr hi">AdaBoostClassifier</strong>(n_estimators = 400)<br/>A.fit(X_train, y_train)<br/>y_pred_sk = A.predict(X_test)<br/>print('The ROC-AUC:', round(roc_auc_score(y_test, y_pred), 4))</span><span id="632c" class="mc kk hh nr b fi nz nw l nx ny">#OUT:<br/>The ROC-AUC: 0.9286</span></pre><h1 id="214b" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">阅读更多内容…</h1><div class="lk ll ez fb lm ln"><a href="https://python.plainenglish.io/random-forest-implementation-980b2d2c3c84" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">随机森林实现</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">Sklearn实现和参数解释</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">python .平原英语. io</p></div></div><div class="lw l"><div class="oc l ly lz ma lw mb jg ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://python.plainenglish.io/decision-tree-parameters-explanations-tuning-a2b0749976e5" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">决策树参数解释</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">Sklearn的决策树参数解释</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">python .平原英语. io</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb jg ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://python.plainenglish.io/logistic-regression-from-scratch-7b707662c8b9" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">从零开始的逻辑回归</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">解释和实施</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">python .平原英语. io</p></div></div><div class="lw l"><div class="od l ly lz ma lw mb jg ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://python.plainenglish.io/ridge-lasso-elasticnet-regressions-from-scratch-32bf9f1a03be" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">山脊，套索和弹性网从零开始回归</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">Python代码从头开始和Sklearn实现</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">python .平原英语. io</p></div></div><div class="lw l"><div class="oe l ly lz ma lw mb jg ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://towardsdev.com/modeling-interactions-and-behavior-915901d1266e" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">交互和行为建模</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">对建模图的简单介绍</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">towardsdev.com</p></div></div><div class="lw l"><div class="of l ly lz ma lw mb jg ln"/></div></div></a></div><h1 id="5467" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">参考</h1><p id="cf1f" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated"><a class="ae jm" href="https://blog.paperspace.com/adaboost-optimizer/" rel="noopener ugc nofollow" target="_blank">https://blog.paperspace.com/adaboost-optimizer/</a></p><p id="1cba" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . adaboostclassifier . html</a></p><p id="1e99" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=LsK-xG1cLYA" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=LsK-xG1cLYA</a></p><div class="lk ll ez fb lm ln"><a href="https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">AdaBoost算法-初学者完全指南-分析</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">Boosting是一种集合建模技术，由Freund和Schapire于1997年首次提出，自…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="lw l"><div class="og l ly lz ma lw mb jg ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="oh l ly lz ma lw mb jg ln"/></div></div></a></div></div></div>    
</body>
</html>