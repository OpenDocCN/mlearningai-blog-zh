<html>
<head>
<title>Image Classification on Azure with Dagshub Direct Data Access</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Dagshub直接数据访问的Azure图像分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/image-classification-on-azure-with-dagshub-direct-data-access-e67f3b1d597e?source=collection_archive---------8-----------------------#2022-11-13">https://medium.com/mlearning-ai/image-classification-on-azure-with-dagshub-direct-data-access-e67f3b1d597e?source=collection_archive---------8-----------------------#2022-11-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8cd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用Dagshub直接数据访问和Azure ML SDK在Azure上训练图像分类模型，而不在本地存储数据</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/df3e788f477798ecac76a69ed5348a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhjk2SJr54AxIOYE_lRGqw.jpeg"/></div></div></figure><h1 id="04e8" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">简介</strong></h1><p id="dab2" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">机器学习和人工智能已经变得无处不在，数据科学家的关键技能之一是能够建立可扩展的模型，处理敏感数据，并建立可靠的数据和模型管道，这些数据和模型管道是可重复的。虽然云技术允许我们建立可扩展的管道，但我们需要将所有文件存储在我们的云上，然后才能将它们用于训练模型，这会导致GPU成本和时间来提取数据。</p><p id="f906" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将使用Dagshub Repo上存储的数据在Azure上训练一个模型，使用Dagshub的<a class="ae kr" href="https://dagshub.com/blog/launching-data-streaming-and-upload/?utm_source=dagshub_homepage&amp;utm_medium=banner&amp;utm_campaign=DDA&amp;utm_id=DDA" rel="noopener ugc nofollow" target="_blank">直接数据访问</a> (DDA)功能，该功能允许我们在需要时批量提取数据——减少在Azure上提取和存储数据所需的时间，从而降低GPU成本。</p><p id="a5f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用Azure ML SDK从本地机器创建培训作业，并将其推送到Azure。我们还将看到如何从我们的jupyter-notebook上监控作业。</p><p id="38b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用来自Kaggle，Mayo Clinic-STRIP AI challenge的数据，其中重点是区分两种主要的急性缺血性中风(AIS)病因亚型:心脏动脉粥样硬化和大动脉粥样硬化。检测中风的病因可以帮助医生减少中风的复发。</p><p id="4c2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本分析中使用的代码和数据可在<a class="ae kr" href="https://dagshub.com/AiswaryaSrinivas/Mayo_Stroke_Blood_Clot_Origin" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><div class="ks kt ez fb ku kv"><a href="https://dagshub.com/AiswaryaSrinivas/Mayo_Stroke_Blood_Clot_Origin" rel="noopener  ugc nofollow" target="_blank"><div class="kw ab dw"><div class="kx ab ky cl cj kz"><h2 class="bd hi fi z dy la ea eb lb ed ef hg bi translated">AiswaryaSrinivas/Mayo _中风_血块_来源</h2><div class="lc l"><h3 class="bd b fi z dy la ea eb lb ed ef dx translated">这个项目的目标是对缺血性卒中的血凝块来源进行分类。使用全切片数字病理学…</h3></div><div class="ld l"><p class="bd b fp z dy la ea eb lb ed ef dx translated">dagshub.com</p></div></div><div class="le l"><div class="lf l lg lh li le lj jm kv"/></div></div></a></div><h1 id="9de3" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">关于数据</strong></h1><p id="ad4c" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">最初为挑战赛提供的数据是tiff格式，大小为356GB。为了便于分析，我们缩小了图像的尺寸，并将其存储为png格式。数据缩减的代码可以在<a class="ae kr" href="https://www.kaggle.com/code/dschettler8845/mcsai-how-to-interact-with-large-tif-files" rel="noopener ugc nofollow" target="_blank">这里</a>找到。缩小的图像可以在<a class="ae kr" href="https://www.kaggle.com/datasets/robertlangdonvinci/mcsaidownscaledimages" rel="noopener ugc nofollow" target="_blank">这里</a>找到。为了将图像映射到它们所属的类，竞赛数据为我们提供了一个包含数据信息的CSV文件。这些数据可以在<a class="ae kr" href="https://www.kaggle.com/competitions/mayo-clinic-strip-ai/data" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="fd50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练数据集中的“标签”列由图像所属的类组成。训练数据文件夹包括754幅图像，其中547幅属于CE类(心脏), 207幅属于LAA类(大动脉粥样硬化)。在我们的分析中，我们创建了一个名为“int_labels”的列，如果图像属于“LAA”类，则该列的值为1，否则该列的值为0。</p><h1 id="fedc" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">接近</strong></h1><p id="9aee" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">为了使用Azure来训练我们的模型，我们将使用Azure ML SDK并在Azure上运行我们的代码。这将需要我们在Azure上创建一个ML工作空间和一个GPU计算。一般来说，我们将数据上传到Azure存储上，并创建数据存储以允许ML Workspace访问我们的数据。我们不打算将数据存储在Azure上，而是从Dagshub存储库中流式传输数据。</p><p id="bdc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要设置Azure Workspace并使用Azure ML SDK创建一台计算机，请参考此处的<a class="ae kr" rel="noopener" href="/analytics-vidhya/building-ml-pipelines-in-azure-using-python-sdk-part-1-448d241d3a7c"/>。</p><p id="79ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Dagshub被称为“Github for ML ”,因为它不仅允许我们维护我们的代码，还允许对我们的数据进行版本控制。</p><p id="c70e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据版本控制(DVC)允许我们跟踪大型数据文件中的变化，类似于Git帮助我们跟踪代码中的变化。为了支持DVC，我们可以使用任何云存储，如AWS、GCP和Azure，这些都很昂贵。类似于我们通过git pull从git获取最新代码，我们通过dvc pull获取使用dvc进行版本控制的数据。与Git命令类似，我们也有dvc push、pull和commit命令</p><p id="b9f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了这个项目，我在Dagshub上创建了一个Repo，并上传了我所有的数据文件和代码。所有使用DVC的文件和文件夹在回购中被标记为DVC。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lk"><img src="../Images/6c1d9080808743f7ffdd65e1eeb19ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-Fqb1ucY1VvSrgnPIBiVA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Repository for Blood Clot Prediction. The folder data and models are marked DVC since they are data folders.</figcaption></figure><p id="3948" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们将数据推送到Dagshub，我们将创建脚本，这些脚本将使用DDA的流功能从存储库中获取数据，而不必执行可能需要更长时间的<strong class="ig hi"> dvc拉取</strong>。</p><h1 id="6cb9" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">使用Daghub直接数据访问的数据流</strong></h1><p id="c462" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">传统上，如果我们想要构建一个模型或读取任何数据版本控制系统中存在的数据，我们需要提取所有存在的数据文件。但是，很多时候我们并不需要现有的所有文件，提取大文件会非常耗时，也会占用CPU/GPU时间。为了避免这种情况，Dagshub引入了流式API，这是Dagshub的直接数据访问(DDA)特性的一部分。让我们看看如何在本地系统上访问流式API。</p><p id="79f5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用以下命令安装流式API:</p><pre class="jd je jf jg fd lp lq lr bn ls lt bi"><span id="45ca" class="lu jp hh lq b be lv lw l lx ly">pip install dagshub</span></pre><p id="7cce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们使用Gitpython库克隆DagshubRepo。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Cloning a Repo Using GitPython</figcaption></figure><p id="7931" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">克隆该存储库后，您可以在本地系统中看到该存储库，但是您将不会在该存储库中找到使用DVC(数据和模型文件夹)进行版本控制的文件夹。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/2bb67fd9d3bf6e670753efcdbf256f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hu79Q4Xn6NTldp0-hiO2w.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">List of Files and Directories in the Repo after cloning</figcaption></figure><p id="8cff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用串流客户端有两种方式。</p><p id="81bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Python专用的“Lite”钩子</strong>可以自动检测对Python内置文件操作的调用，如open()、listdir()等，并兼容大多数python ML库。使用install_hooks()，可以简单地访问该文件，就好像它存储在本地机器上一样。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Reading the csv file on Dagshub Repo using Python Lite Hooks</figcaption></figure><p id="f7e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用这种方法，当我们使用os.listdir()时，我们可以看到数据和模型文件夹，它们也是DVC版本。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/fbd17b9757fbe8b3abeab32f1f4366b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Z2ljwcnD801gvNE_2g18w.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">On calling install_hooks() we can see that “data” and “models” folders are visible using os.listdir()</figcaption></figure><p id="9a1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种方法可能不适用于Tensorflow和OpenCV——因为它们的输入/输出框架是用C/C++等低级语言编写的，与Pytorch等库相比，需要以不同的方式处理。在这种情况下，您可以通过加载Dagshub文件系统来使用文件流。</p><p id="f34c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用DDA的文件流就像加载<strong class="ig hi"> Dagshub文件系统一样简单。</strong></p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Creating Dagshub Streaming Client</figcaption></figure><p id="9010" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们可以分别用fs.open()、fs.stat()、fs.listdir()和fs.scandir()替换open()、os.stat()、os.listdir()和os.scandir()的任何用法。使用fs.listdir()我们现在可以看到data和models文件夹存在，即使它们不在我们的机器上。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/9552cd136c800eac0ecc8b18100a3137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pghkyLVy1ba4WzgNQUB7Kw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Listing the Directories present in Dagshub Repo using Streaming Client</figcaption></figure><p id="4a85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为检查，让我们使用fs和os来列出目录中的文件。如下所示，使用操作系统时，我们无法访问数据文件夹，这意味着我们正在使用流功能访问Dagshub Repo上的文件</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/d83e37fbf2f4292f494320ab07b158e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVU2ZW6gGoiGxv59ymUG9A.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Listing Directories using os and fs (Dagshub Streaming Client)</figcaption></figure><p id="0c86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要读取data/raw/文件夹中的train.csv文件，我们可以使用open函数</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/295cfa3db43653489af74307a575dddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Da36IY4TP3Q73OCvBbehA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Reading a File using Dagshub Streaming Client</figcaption></figure><p id="6d7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦你打开了一个文件，这个文件就被存储在你本地机器的缓存中，因此当你使用os.listdir()函数时，这个文件是可见的。</p><p id="a66d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">打开的train.csv文件存储在缓存中，并在本地系统的Repo中可见</p><p id="48c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用流式传输功能的主要优势在于，您不必一次下载所有文件，而是只访问您需要的文件。这意味着我们不必等到所有文件下载完毕后再开始训练我们的模型，从而节省了GPU的成本和时间。</p><h1 id="814d" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">使用DDA可视化图像</strong></h1><p id="116b" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们创建了一个show_image函数，它将图像和流媒体客户端作为输入，读取图像，将它们转换为(512，512)图像，并水平翻转它们。这里，我们使用fs.open()将文件缓存到本地系统上，然后使用im.read()函数读取图像。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Code to Read the Image File from Dagshub using Streaming Client and then Visualising</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/36a84985c24ed2ddc10f9c2c10a64617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPOWyc1VcqCrUhEOnFHQ4w.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Plot of Image ID 008e5c_0</figcaption></figure><h1 id="627a" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">在Azure上创建用于训练图像分类模型的脚本</strong></h1><p id="6307" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">在Azure上训练模型的第一步是创建自己的工作空间和计算。这可以使用Azure ML SDK来完成(请参考<a class="ae kr" rel="noopener" href="/analytics-vidhya/building-ml-pipelines-in-azure-using-python-sdk-part-1-448d241d3a7c">这里的</a>)或者你可以去<a class="ae kr" href="http://portal.azure.com" rel="noopener ugc nofollow" target="_blank">portal.azure.com</a>创建一个工作区。为了确保可再现性，我使用Azure ML SDK创建了我的工作空间和计算集群。</p><p id="4b7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一步是创建用于训练的python脚本。所有要使用的脚本都将放在同一个文件夹中，因为该文件夹将被上传到Azure。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">creating a script folder to store all the scripts that are needed for training</figcaption></figure><p id="1005" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在scripts文件夹中—因为我需要能够访问Dagshub Repo，所以我创建了dagshub_config.py，其中包含Dagshub用户名、令牌和Repo名称。令牌是要保密的，不应该被共享。T3】</p><p id="1d87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae kr" href="https://dagshub.com/AiswaryaSrinivas/Mayo_Stroke_Blood_Clot_Origin/src/main/src/azure_ml_scripts/utils.py" rel="noopener ugc nofollow" target="_blank"> util.py </a>文件，包含创建流客户端、克隆git repo、使用数据流列出training文件夹中的图像、读取train.csv数据帧、将数据拆分为train和test集以及使用数据流下载EfficientNet模型的函数的代码。</p><p id="7ec4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，最后一个也是最后一个脚本(<a class="ae kr" href="https://dagshub.com/AiswaryaSrinivas/Mayo_Stroke_Blood_Clot_Origin/src/main/src/azure_ml_scripts/train.py" rel="noopener ugc nofollow" target="_blank">train . py</a>)——所有神奇的事情都发生在这里。为了训练图像分类模型，我们使用ImageDataGenerator类，但是这需要文件存在于系统中。因为，我们将使用流客户端读取图像以及train.csv文件以获取图像文件名和类，并创建一个<strong class="ig hi">自定义数据生成器</strong>，它将使用流功能从Dagshub Repo中批量读取图像。</p><p id="dcfd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用EfficientNet B5模型进行血凝块预测。EfficientNet允许通过平衡网络深度、宽度和分辨率，提供一种系统的方法来缩放CNN模型。我们已将此模型存储在repo的“数据/原始/预训练_高效_模型”中，该repo包含多个高效网络模型。</p><p id="6bce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，因为我们只需要B5型号，所以我们只读取该型号的重量。由于流式API，这是可能的——否则我们将不得不下载整个文件夹，然后读取那个特定的文件。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Code to download EfficientNet B5 Model using Streaming API</figcaption></figure><p id="dcc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的<a class="ae kr" href="https://dagshub.com/AiswaryaSrinivas/Mayo_Stroke_Blood_Clot_Origin/src/main/src/azure_ml_scripts/train.py" rel="noopener ugc nofollow" target="_blank">训练脚本</a>中还定义了将图像大小调整为(512，512)以及使用我们的CustomDataGenerator创建验证和训练数据生成器。我们使用Azure中的MLFlow来监控模型指标，并使用<strong class="ig hi"> mlflow.autolog()来跟踪指标。</strong></p><h1 id="5d70" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">在Azure上创建环境和运行脚本</strong></h1><p id="4c32" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">现在，我们已经准备好了脚本，我们需要在Azure上有一个环境来运行我们的脚本——这意味着安装我们的库和其他依赖项。虽然我们可以用conda包和pip包创建自己的环境，但是我们也可以使用管理的环境。</p><p id="251f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本例中，我使用安装了CUDA的现有docker映像创建了自己的环境。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Creating an Environment on Azure using Azure ML SDK</figcaption></figure><p id="4e35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一步是创建一个实验。把一个实验想象成一个文件夹，在里面你的工作被组织起来。</p><p id="5d81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们创建ScriptRunConfig对象。你可以把ScriptRunConfig看作是一个保存在Azure上运行作业所需的所有细节的配置。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Creating an experiment and ScriptRunConfig for Azure to access</figcaption></figure><p id="be88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了在Azure上运行作业，我们使用ScriptRunConfig对象提交实验，并使用RunDetails小部件从我们的笔记本中监控进度。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/9e4a13c49d007bc613ca0b0a729c6ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWtWVWVULyxsZNMgBOW_nw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Submitting Experiment to Run on Azure</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/456e7d6ad19db278735abe4c7e85abee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKWNOldCY04ysPS-IXovkw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Monitoring Run From Azure</figcaption></figure><p id="0f19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用run.get_metrics()从笔记本中跟踪指标</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/81d63ea199652a18807d24eb336f6d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FTO7_8Ahhf65OoarpMvkQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Run Metrics from Notebook</figcaption></figure><p id="dacd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="mc">我们可以看到模型达到了0.75的验证精度和0.76的训练精度。</em>T3】</strong></p><p id="7abd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还可以使用run.download_files()下载保存的模型。</p><pre class="jd je jf jg fd lp lq lr bn ls lt bi"><span id="e224" class="lu jp hh lq b be lv lw l lx ly">## Download the model folder saved after training into our local system in the current folder<br/>run.download_files(output_directory="outputs/efficientNet_Model")</span></pre><p id="73df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">哦！！但是，如果我想将指标写入一个文件并将其推送到我的Dagshub Repo，我需要提取所有数据然后再推吗？？</p><p id="c007" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">惊喜！！！</strong></p><p id="4d8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Dagshub引入了上传功能，作为DDA特性的一部分，这将允许我们将文件上传到Repo，而无需进行拉取。</strong></p><pre class="jd je jf jg fd lp lq lr bn ls lt bi"><span id="3387" class="lu jp hh lq b be lv lw l lx ly">## Uploading files to Dagshub Repo using upload() functionality of DDA<br/>from dagshub.upload import Repo<br/><br/>repo = Repo(DAGSHUB_USERNAME,DAGSHUB_REPO_NAME,branch="main",<br/>             username=DAGSHUB_USERNAME,token=DAGSHUB_TOKEN)<br/><br/>repo.upload(file="../metrics.txt",path="efficientNet_Metrics.txt",<br/>            commit_message="Updating Metrics File",versioning="dvc")</span></pre><p id="cc35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">瞧啊。！！使用DVC版本控制将文件上传到Dagshub Repo。</p><p id="12fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">包装完毕……</strong></p><p id="ded0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们看到了如何在Azure上部署我们的代码，并训练一个模型，而不必花费大量时间将数据传输到Azure。</p><p id="4524" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然我以图像分类为例来展示我们如何将Azure等云技术与Dagshub的DDA功能相结合，但这也可以扩展到其他数据类型，如音频或视频数据类型。通常，视频文件很大，通常会占用大量存储空间，每次提取视频文件都非常耗时。</p><p id="36d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的是，使用流功能并不仅限于深度学习模型，当您将多个表中的数据存储为频繁更新的文件，并且您的分析只需要几个文件时，也可以使用流功能-通过这种方式使用数据流，可以避免下载所有文件，并且只需重新运行脚本即可获得更新的结果。</p><p id="e0c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用Azure，我们可以用它们的依赖项创建我们自己的环境，而不需要太熟悉docker容器。此外，对于那些习惯于python的人来说，Azure ML SDK为我们提供了一种在Azure上运行模型的方法，甚至无需离开熟悉的jupyter-notebook环境。</p><p id="1e19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="mc">希望这篇文章对你有用。一如既往，非常感谢您的任何反馈。请随时分享您的评论。</em>T13】</strong></p><p id="0a62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mc">本文原载于</em><a class="ae kr" href="https://hackernoon.com/image-classification-on-azure-with-dagshub-direct-data-access" rel="noopener ugc nofollow" target="_blank"><em class="mc">hacker noon</em></a></p><div class="ks kt ez fb ku kv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kw ab dw"><div class="kx ab ky cl cj kz"><h2 class="bd hi fi z dy la ea eb lb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lc l"><h3 class="bd b fi z dy la ea eb lb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ld l"><p class="bd b fp z dy la ea eb lb ed ef dx translated">medium.com</p></div></div><div class="le l"><div class="md l lg lh li le lj jm kv"/></div></div></a></div></div></div>    
</body>
</html>