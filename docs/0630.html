<html>
<head>
<title>Retirement of convolutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积的报废</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/retirement-of-convolutions-c589d084c679?source=collection_archive---------0-----------------------#2021-06-01">https://medium.com/mlearning-ai/retirement-of-convolutions-c589d084c679?source=collection_archive---------0-----------------------#2021-06-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="67a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="jc jd ge" href="https://medium.com/u/8cc2ede908a6?source=post_page-----c589d084c679--------------------------------" rel="noopener" target="_blank"> theamitnikhade </a>访问:【amitnikhade.com】T2</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/857b90307f5fd3b12a6b6c716d7c2c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VNx6jhgUSKmx3FMj"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">Photo by <a class="ae je" href="https://unsplash.com/@randomlies?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ashim D’Silva</a> on <a class="ae je" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="6341" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">介绍</h2><p id="8c24" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi kv translated"><span class="l kw kx ky bm kz la lb lc ld di">“C</span><a class="ae je" href="https://www.sas.com/en_in/insights/analytics/computer-vision.html#:~:text=Computer%20vision%20is%20a%20field,to%20what%20they%20%E2%80%9Csee.%E2%80%9D" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>”<a class="ae je" href="https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp#:~:text=Artificial%20intelligence%20(AI)%20refers%20to,as%20learning%20and%20problem%2Dsolving." rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">人工智能</strong> </a>的一个领域，帮助机器可视化这个美丽的世界。计算机视觉带来了增强人工智能的奇迹。从模式识别到人类姿态估计，从机器人导航到固态物理，计算机视觉有更多有用和有益的应用。利用计算机视觉和深度学习，我们成功地赋予机器可视化和理解图像、视频等的能力。但是革命是固定的。</p><p id="f278" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更早的<a class="ae je" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">卷积</strong> </a>已经为医学研究、商业、技术和许多其他领域的计算机视觉和深度学习做出了最大的贡献。最后技术不能稳定，必须翻新。</p><p id="509d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CNN于20世纪80年代由Yann LeCun首次推出。像素值与其权重的乘积之和是卷积背后的实际机制。CNN主要专注于从图像中提取特征，如角、边缘、颜色梯度等等。它通常包括3层，即卷积层、池层和全连接层。2015年，微软的研究制造了一个高度深度的CNN网络，其性能超过了AlexNet，该网络大约有200层深。AlexNet被认为是计算机视觉领域有史以来发表的最有影响力的论文。</p><h2 id="0f61" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">让我们回顾一下NLP。</h2><p id="16f2" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">在<a class="ae je" href="https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32#:~:text=Natural%20Language%20Processing%2C%20usually%20shortened,a%20manner%20that%20is%20valuable." rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>的情况下，正如我们所知，它是人工智能的子分支，帮助人类和计算机相互交流，从技术上讲，它是赋予计算机理解人类语言并获取其本质的能力的技术。NLP也是一个轻松处理文本数据的工具包。一些流行的自然语言处理应用包括情感分析、文本分类、语音到文本、神经机器翻译等。</p><blockquote class="le lf lg"><p id="906c" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">但是CNN到底缺什么呢？</p></blockquote><p id="13ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2017年是<a class="ae je" href="https://en.wikipedia.org/wiki/Google_Brain" rel="noopener ugc nofollow" target="_blank">谷歌大脑</a>、<a class="ae je" href="https://research.google/teams/brain/" rel="noopener ugc nofollow" target="_blank">谷歌研究</a>和<a class="ae je" href="https://www.utoronto.ca/" rel="noopener ugc nofollow" target="_blank">多伦多大学推出变形金刚</a>的一年，它突然将NLP带到了一个新的水平，它带来了seq to seq模型的嬗变。序列到序列模型(LSTM/GRU，即RNN模型)用于将序列从一种形式转换到另一种形式，但是它们也遭受一些不利的问题，例如消失梯度和用于逐字处理序列的模型，即它花费很多时间并且是并行化的障碍。注意力在从输入数据中提取显著特征的变压器架构中起着非常关键的作用。变形金刚我们就不深究了。我不会深入研究变形金刚，希望你一定知道让你阅读视觉变形金刚的<strong class="ig hi"/><a class="ae je" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">变形金刚架构</strong> </a>。</p><p id="eb90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">各种模型建立在变压器的架构，如伯特，GPT，变压器XL，XLNET和有更多的赋予国家的艺术性能。</p><p id="6f9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最近，谷歌的BERT刚刚对其编码器架构进行了轻微的改变，这使得BERT比以前更快更准确。如果你想了解更多，下面就是。</p><div class="ll lm ez fb ln lo"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/a-sudden-change-to-the-encoder-368fd9a72bc7"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">编码器突然变了！</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">theamitnikhade</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">medium.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc jp lo"/></div></div></a></div><p id="0adc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能想知道视觉转换器什么时候开始发挥作用，要理解视觉转换器，你需要理解独立转换器的工作原理。视觉变形金刚也是变形金刚的一个小改动。</p><h2 id="58f2" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">视觉变压器</h2><blockquote class="le lf lg"><p id="d760" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">2020年10月22日</p></blockquote><p id="05c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae je" href="https://arxiv.org/pdf/2006.03677.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="lh">一幅图像相当于16×16个字:按比例进行图像识别的变形金刚</em> </strong> </a> <em class="lh"> </em>规定只需将图像分割成固定大小的小块，将它们线性嵌入加上位置嵌入，并将输出向量平行馈送到变形金刚编码器。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="md me l"/></div><figcaption class="jr js et er es jt ju bd b be z dx">Vision transformer working and architecture [<a class="ae je" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">source</a>]</figcaption></figure><p id="274a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如上图所示，数据集中的图像被分成大小相同的<em class="lh"> n </em>个小块，即</p><ol class=""><li id="df4b" class="mf mg hh ig b ih ii il im ip mh it mi ix mj jb mk ml mm mn bi translated">大小为H *W的2D图像按顺序被分成N个小块，其中N=H*W/P，并且通过连接小块中的所有像素通道，每个小块被展平为1D小块嵌入。</li><li id="4171" class="mf mg hh ig b ih mo il mp ip mq it mr ix ms jb mk ml mm mn bi translated">它们从线性投影进一步传递，以获得所需的输入尺寸。</li><li id="62d6" class="mf mg hh ig b ih mo il mp ip mq it mr ix ms jb mk ml mm mn bi translated">然后向量被位置嵌入，并且额外的可学习嵌入被添加到其中，正如我们在<a class="ae je" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> BERT </strong> </a>中看到的我们添加分类标记的方式。</li><li id="b9dd" class="mf mg hh ig b ih mo il mp ip mq it mr ix ms jb mk ml mm mn bi translated">最后，补丁嵌入向量的序列被馈送到变换器编码器，其中的工作与本地变换器编码器中的相同。</li><li id="79fa" class="mf mg hh ig b ih mo il mp ip mq it mr ix ms jb mk ml mm mn bi translated">输入被归一化，并通过<a class="ae je" href="https://www.csie.ntu.edu.tw/~miulab/s107-adl/doc/190409_Transformer.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">多头注意力</strong> </a>从图像中学习局部和全局相关性，添加来自输入的残差，我们将其传递给来自归一化层的多层感知，并再次添加残差。</li></ol><h2 id="60a0" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">视觉转换器的代码实现</h2><p id="a28e" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">我们将尝试使用Python中的PyTorch来实现视觉转换器模型</p><blockquote class="le lf lg"><p id="97bf" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">导入库</p></blockquote><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="70f4" class="jv jw hh mu b fi my mz l na nb">import torch</span><span id="f1de" class="jv jw hh mu b fi nc mz l na nb">from torch._C import dtype</span><span id="0107" class="jv jw hh mu b fi nc mz l na nb">import torch.nn as nn</span><span id="5a3d" class="jv jw hh mu b fi nc mz l na nb">from torch.nn.modules.conv import Conv2d</span><span id="f183" class="jv jw hh mu b fi nc mz l na nb">import torch.nn.functional as F</span></pre><blockquote class="le lf lg"><p id="b3d4" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">定义要使用的设备对象</p></blockquote><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="be84" class="jv jw hh mu b fi my mz l na nb">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</span></pre><blockquote class="le lf lg"><p id="dcb4" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">空缓存</p></blockquote><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="29c9" class="jv jw hh mu b fi my mz l na nb">torch.cuda.empty_cache()</span></pre><p id="8ae4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定义图像大小、补丁大小、嵌入维度大小、多层感知器维度、层数、人头数、注意力丢失率、类别数等参数。</p><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="de64" class="jv jw hh mu b fi my mz l na nb">img_s = 224</span><span id="1ddc" class="jv jw hh mu b fi nc mz l na nb">patch_s = 16</span><span id="802e" class="jv jw hh mu b fi nc mz l na nb">emb_dim = 128</span><span id="d168" class="jv jw hh mu b fi nc mz l na nb">mlp_dim = 128</span><span id="e806" class="jv jw hh mu b fi nc mz l na nb">num_heads = 16</span><span id="2ae2" class="jv jw hh mu b fi nc mz l na nb">num_layers = 5</span><span id="a405" class="jv jw hh mu b fi nc mz l na nb">atten_dropout = .0</span><span id="f4f4" class="jv jw hh mu b fi nc mz l na nb">num_classes = 2</span></pre><blockquote class="le lf lg"><p id="a08e" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">构建视觉转换器类</p></blockquote><p id="d4c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">VIT类接受这些参数，并开始用一个定义的补丁大小分割整形后的图像。补丁数量的计算方法如下</p><p id="6aa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">n_patches =(图像高度*图像宽度)/(补丁高度*补丁宽度)</p><p id="c57f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">进一步的过程包括嵌入补丁，你可能会奇怪为什么我用卷积嵌入补丁。Lol，我的题目是反驳卷积。卷积提取特征更适合于适当的归纳基础，这导致性能的提高。CNN用于提取图像中的低层特征，ViT用于关联高层概念。<strong class="ig hi"> </strong> ResNet或EfficientNet也可以修剪到某些层，以提取特征。补丁嵌入也可以通过线性层来完成。这是许多人常用的。此外，我们在补丁序列中添加类标记，并通过丢失规则传递它们的位置嵌入。添加位置嵌入有助于模型理解图像的结构及其补丁位置。</p><p id="3eb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">rest流程与原生transformer编码器相同。</p><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="aec8" class="jv jw hh mu b fi my mz l na nb">class VIT(nn.Module):</span><span id="a6a9" class="jv jw hh mu b fi nc mz l na nb">    def __init__(self, img_size= (img_s,img_s),patch_size= (patch_s, patch_s), emb_dim = emb_dim, mlp_dim= mlp_dim ,num_heads=num_heads,n_classes=2, dropout_rate=0., at_d_r=atten_dropout):</span><span id="068a" class="jv jw hh mu b fi nc mz l na nb">        super(VIT, self).__init__()</span><span id="e843" class="jv jw hh mu b fi nc mz l na nb">        ih, iw = img_size</span><span id="10ae" class="jv jw hh mu b fi nc mz l na nb">        ph, pw = patch_size</span><span id="93c4" class="jv jw hh mu b fi nc mz l na nb">        num_patches = int((ih*iw)/(ph*pw))</span><span id="f6ce" class="jv jw hh mu b fi nc mz l na nb">        self.cls_tokens = nn.Parameter(torch.rand(1, 1, emb_dim))</span><span id="a9de" class="jv jw hh mu b fi nc mz l na nb">        self.patch_embed = Conv2d(in_channels=3,</span><span id="165a" class="jv jw hh mu b fi nc mz l na nb">        out_channels=emb_dim,</span><span id="420e" class="jv jw hh mu b fi nc mz l na nb">        kernel_size=patch_size,</span><span id="4fb0" class="jv jw hh mu b fi nc mz l na nb">        stride=patch_size)</span><span id="4bb2" class="jv jw hh mu b fi nc mz l na nb">        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))</span><span id="ca17" class="jv jw hh mu b fi nc mz l na nb">        self.dropout = nn.Dropout(dropout_rate)</span><span id="4394" class="jv jw hh mu b fi nc mz l na nb">        self.enco = transencoder(emb_dim, mlp_dim, num_heads, at_d_r)</span><span id="f051" class="jv jw hh mu b fi nc mz l na nb">        self.mlp_head = nn.Sequential(</span><span id="53ac" class="jv jw hh mu b fi nc mz l na nb">        nn.LayerNorm(emb_dim),</span><span id="fc37" class="jv jw hh mu b fi nc mz l na nb">        nn.Linear(emb_dim, n_classes)</span><span id="490a" class="jv jw hh mu b fi nc mz l na nb">        )</span><span id="9c72" class="jv jw hh mu b fi nc mz l na nb">    def forward(self,x):</span><span id="25e0" class="jv jw hh mu b fi nc mz l na nb">        x = self.patch_embed(x)</span><span id="a515" class="jv jw hh mu b fi nc mz l na nb">        x = x.permute(0, 2, 3, 1)</span><span id="64ba" class="jv jw hh mu b fi nc mz l na nb">        b, h, w, c = x.shape</span><span id="0a1c" class="jv jw hh mu b fi nc mz l na nb">        x = x.reshape(b, h * w, c)</span><span id="48cc" class="jv jw hh mu b fi nc mz l na nb">        cls_token = self.cls_tokens.repeat(b, 1, 1)</span><span id="f2a6" class="jv jw hh mu b fi nc mz l na nb">        x= torch.cat([cls_token, x], dim=1)</span><span id="7fa1" class="jv jw hh mu b fi nc mz l na nb">        embeddings = x + self.pos_embed</span><span id="ea1b" class="jv jw hh mu b fi nc mz l na nb">        embeddings = self.dropout(embeddings)</span><span id="b697" class="jv jw hh mu b fi nc mz l na nb">        enc = layer(embeddings)</span><span id="143b" class="jv jw hh mu b fi nc mz l na nb">        mlp_head = self.mlp_head(enc[:, 0])</span><span id="3bc6" class="jv jw hh mu b fi nc mz l na nb">        return mlp_head</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nd"><img src="../Images/d0912c1e0a8bbbabb498f23aba31cfee.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*I8z3BpnCWWmvG_OgLRSQ5Q.png"/></div></div></figure><p id="141a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">变压器编码器部分。该编码器层可以堆叠n次，以每次从每层提取一些新信息，这导致对变压器的良好预测能力</p><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="f344" class="jv jw hh mu b fi my mz l na nb">class transencoder(nn.Module):</span><span id="a925" class="jv jw hh mu b fi nc mz l na nb">    def __init__(self,emb_dim, mlp_dim, num_heads, at_d_r):</span><span id="2b1e" class="jv jw hh mu b fi nc mz l na nb">        super(transencoder, self).__init__()</span><span id="99f4" class="jv jw hh mu b fi nc mz l na nb">        self.norm = nn.LayerNorm(emb_dim, eps=1e-6)</span><span id="f964" class="jv jw hh mu b fi nc mz l na nb">        self.mha = mha(emb_dim, num_heads, at_d_r)</span><span id="f2c1" class="jv jw hh mu b fi nc mz l na nb">        self.mlp = Mlp(emb_dim, mlp_dim)</span><span id="5557" class="jv jw hh mu b fi nc mz l na nb">    def forward(self, x):</span><span id="ebaa" class="jv jw hh mu b fi nc mz l na nb">        n = self.norm(x)</span><span id="4844" class="jv jw hh mu b fi nc mz l na nb">        attn = self.mha(n,n,n)</span><span id="79b9" class="jv jw hh mu b fi nc mz l na nb">        output = attn+x</span><span id="f7d7" class="jv jw hh mu b fi nc mz l na nb">        n2 = self.norm(output)</span><span id="f3ba" class="jv jw hh mu b fi nc mz l na nb">        ff = self.mlp(n2)</span><span id="d29b" class="jv jw hh mu b fi nc mz l na nb">        out = ff+output</span><span id="2ce6" class="jv jw hh mu b fi nc mz l na nb">        return out</span></pre><p id="5db4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">变压器编码器由一个多头自关注和多层感知器组成。其中多头注意力通过关注输入序列来发挥关键作用。多头机制不是只计算一次注意力，而是并行地多次运行经缩放的点积注意力。</p><blockquote class="le lf lg"><p id="4c49" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">注意功能可以描述为将查询和一组键-值对映射到输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重由查询与相应键的兼容性函数来计算</p></blockquote><p id="063e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据报纸报道</p><h2 id="a89a" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">多头注意力</h2><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="4944" class="jv jw hh mu b fi my mz l na nb">class mha(nn.Module):</span><span id="9553" class="jv jw hh mu b fi nc mz l na nb">    def __init__(self, h_dim, n_heads, at_d_r):</span><span id="e897" class="jv jw hh mu b fi nc mz l na nb">        super().__init__()</span><span id="99cc" class="jv jw hh mu b fi nc mz l na nb">        self.h_dim=h_dim</span><span id="7d40" class="jv jw hh mu b fi nc mz l na nb">        self.linear = nn.Linear(h_dim, h_dim, bias=False)</span><span id="981a" class="jv jw hh mu b fi nc mz l na nb">        self.num_heads = n_heads</span><span id="f3a0" class="jv jw hh mu b fi nc mz l na nb">        self.norm = nn.LayerNorm(h_dim)</span><span id="88b9" class="jv jw hh mu b fi nc mz l na nb">        self.dropout = nn.Dropout(at_d_r)</span><span id="757e" class="jv jw hh mu b fi nc mz l na nb">        self.softmax = nn.Softmax(dim=2)</span><span id="e6f7" class="jv jw hh mu b fi nc mz l na nb">    def forward(self, q, k ,v):</span><span id="fd44" class="jv jw hh mu b fi nc mz l na nb">        rs = q.size()[0]</span><span id="be74" class="jv jw hh mu b fi nc mz l na nb">        batches, sequence_length, embeddings_dim = q.size()</span><span id="6dc2" class="jv jw hh mu b fi nc mz l na nb">        q1= nn.ReLU()(self.linear(q))</span><span id="ed37" class="jv jw hh mu b fi nc mz l na nb">        k1= nn.ReLU()(self.linear(k))</span><span id="ca2c" class="jv jw hh mu b fi nc mz l na nb">        v1= nn.ReLU()(self.linear(v))</span><span id="a834" class="jv jw hh mu b fi nc mz l na nb">        q2 = torch.cat(torch.chunk(q1, self.num_heads, dim=2), dim=0)</span><span id="6061" class="jv jw hh mu b fi nc mz l na nb">        k2 = torch.cat(torch.chunk(k1, self.num_heads, dim=2), dim=0)</span><span id="00f9" class="jv jw hh mu b fi nc mz l na nb">        v2 = torch.cat(torch.chunk(v1, self.num_heads, dim=2), dim=0)</span><span id="7924" class="jv jw hh mu b fi nc mz l na nb">        outputs = torch.bmm(q2, k2.transpose(2, 1))</span><span id="888f" class="jv jw hh mu b fi nc mz l na nb">        outputs = outputs / (k2.size()[-1] ** 0.5)</span><span id="d03f" class="jv jw hh mu b fi nc mz l na nb">        outputs = F.softmax(outputs, dim=-1)</span><span id="f42f" class="jv jw hh mu b fi nc mz l na nb">        outputs = self.dropout(outputs)</span><span id="d196" class="jv jw hh mu b fi nc mz l na nb">        outputs = torch.bmm(outputs, v2)</span><span id="f7b9" class="jv jw hh mu b fi nc mz l na nb">        outputs = outputs.split(rs, dim=0)</span><span id="c109" class="jv jw hh mu b fi nc mz l na nb">        outputs = torch.cat(outputs, dim=2)</span><span id="dcaa" class="jv jw hh mu b fi nc mz l na nb">        outputs += outputs + q</span><span id="7d94" class="jv jw hh mu b fi nc mz l na nb">        outputs = self.norm(outputs)</span><span id="ed6e" class="jv jw hh mu b fi nc mz l na nb">        return outputs</span></pre><h2 id="9d2f" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">多层感知器</h2><p id="84f3" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">使用简单的神经网络来推断二进制结果。感知器是一个线性分类器，它通过分离两个类别来对输入进行分类。多层感知器由线性函数、GELU激活<em class="lh">(高斯误差线性单元)、</em>和退出器组成。</p><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="4fd1" class="jv jw hh mu b fi my mz l na nb">class Mlp(nn.Module):</span><span id="411b" class="jv jw hh mu b fi nc mz l na nb">    def __init__(self, emb_dim, mlp_dim, dropout_rate=0.):</span><span id="0e8b" class="jv jw hh mu b fi nc mz l na nb">        super(Mlp, self).__init__()</span><span id="f4f6" class="jv jw hh mu b fi nc mz l na nb">        self.fc1 = nn.Linear(emb_dim, mlp_dim)</span><span id="bb10" class="jv jw hh mu b fi nc mz l na nb">        self.fc2 = nn.Linear(mlp_dim, emb_dim)</span><span id="dfd1" class="jv jw hh mu b fi nc mz l na nb">        self.act = nn.GELU()</span><span id="da38" class="jv jw hh mu b fi nc mz l na nb">        self.dropout= nn.Dropout(dropout_rate)</span><span id="e5aa" class="jv jw hh mu b fi nc mz l na nb">    def forward(self, x):</span><span id="32ea" class="jv jw hh mu b fi nc mz l na nb">        out = self.fc1(x)</span><span id="0bd7" class="jv jw hh mu b fi nc mz l na nb">        out = self.act(out)</span><span id="cb5c" class="jv jw hh mu b fi nc mz l na nb">        out = self.dropout(out)</span><span id="a0d1" class="jv jw hh mu b fi nc mz l na nb">        out = self.fc2(out)</span><span id="63ea" class="jv jw hh mu b fi nc mz l na nb">        out = self.dropout(out)</span><span id="f64c" class="jv jw hh mu b fi nc mz l na nb">        return out</span></pre><p id="a172" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们打印模型的流程</p><pre class="jg jh ji jj fd mt mu mv mw aw mx bi"><span id="80bc" class="jv jw hh mu b fi my mz l na nb">model = VIT()</span><span id="3ad1" class="jv jw hh mu b fi nc mz l na nb">print(model)</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ne"><img src="../Images/ecec4115b6a2c784da15a6f7f48039c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*bCveVO8yoNK-4_8vhOhE6A.png"/></div></figure><div class="ll lm ez fb ln lo"><a href="https://github.com/AmitNikhade/Vision-Transformer" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">AmitNikhade/视觉转换器</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">使用视觉转换器py torch-AmitNikhade/视觉转换器进行图像分类</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">github.com</p></div></div><div class="lx l"><div class="nf l lz ma mb lx mc jp lo"/></div></div></a></div><p id="1b38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经使用ViT执行了一个分类任务，如果您觉得有帮助，请启动存储库，如果您发现任何问题，请创建一个新问题。</p><h2 id="9d6d" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">变体</h2><p id="36e0" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">本文发布了三种视觉转换器的变体，它们是从BERT中采用的，基本的有12层，大的有24层，大的有32层，参数为632M。如面片数公式所示，输入面片越小，计算模型越大</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ng"><img src="../Images/d9275fe957c3ffaaf027998ee95be863.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*69DKSZPM_cpdBHM51rjPSw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx"><a class="ae je" href="https://arxiv.org/pdf/2006.03677.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h2 id="1282" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">培训和微调</h2><p id="f5e8" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated">由于视觉转换器对图像数据的理解能力较低，因此需要较长的时间，因此需要更高的数量。该模型根据大型数据集进行预训练，并使用较小的数据进行微调。Vision-Transformer在可持续的大型数据集上具有更高的准确性，同时减少了训练时间。</p><p id="f49d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在微调过程中，如果与模型预训练的面片大小相比较，面片大小应该是相同的。使用更高分辨率的图像微调模型可以获得更好的性能。</p><h2 id="5898" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">表演</h2><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nh"><img src="../Images/2810ae8d4aaae0a0ec89ffe913a1c8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4sysTZuOYA9TKq0qq2vxmg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx">performance comparison [ <a class="ae je" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">source</a> ]</figcaption></figure><p id="bff7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如<a class="ae je" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">谷歌博客</strong> </a>所显示的，当用更少的数据量进行预训练时，视觉变形金刚表现不佳，而在足够大的训练数据下，它的表现优于SOTA。</p><h2 id="5f5a" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">结论</h2><blockquote class="le lf lg"><p id="96dc" class="ie if lh ig b ih ii ij ik il im in io li iq ir is lj iu iv iw lk iy iz ja jb ha bi translated">当在足够的数据上进行训练时，Vit表现出优异的性能，在计算资源减少四倍的情况下，胜过可比的最先进的CNN。— <a class="ae je" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">谷歌博客</a></p></blockquote><p id="9a6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">视觉变压器假装是真实的，在缓解更好的性能，这击败了最先进的卷积神经网络。自我关注也是ViT中最重要的组成部分，它关注图像的主要特征。Vit可用作卷积管道的替代品。Vit是实现计算机视觉中可扩展体系结构的手段。随着数据和计算能力的增加，这是计算机视觉领域中一项必要的发明。</p><h2 id="b824" class="jv jw hh bd jx jy jz ka kb kc kd ke kf ip kg kh ki it kj kk kl ix km kn ko kp bi translated">参考</h2><p id="9142" class="pw-post-body-paragraph ie if hh ig b ih kq ij ik il kr in io ip ks ir is it kt iv iw ix ku iz ja jb ha bi translated"><a class="ae je" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/12/transformers-for-image-recognition-at . html</a></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="ni me l"/></div></figure><div class="ll lm ez fb ln lo"><a href="https://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">图示的变压器</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">jalammar.github.io</p></div></div><div class="lx l"><div class="nj l lz ma mb lx mc jp lo"/></div></div></a></div><div class="ll lm ez fb ln lo"><a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">变形金刚(机器学习模型)-维基百科</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">变压器是一种深度学习模型，采用注意力机制，权衡不同因素的影响</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">en.wikipedia.org</p></div></div><div class="lx l"><div class="nk l lz ma mb lx mc jp lo"/></div></div></a></div><p id="0010" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你觉得我的文章有帮助和有见地，请鼓掌。</p><div class="ll lm ez fb ln lo"><a href="https://www.linkedin.com/in/theamitnikhade/" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">Amit Nikhade - JSPM拉贾什胡莎工程学院-马哈拉施特拉邦阿科拉</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">在我的杯子里装了很多咖啡，为普通智力革命而工作。我是一个热爱享受的人…</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">www.linkedin.com</p></div></div><div class="lx l"><div class="nl l lz ma mb lx mc jp lo"/></div></div></a></div><p id="f722" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢，不要忘记对任何问题发表评论，并关注我的时事通讯以保持更新。</p></div></div>    
</body>
</html>