# 使用 GPT-尼奥的文本生成

> 原文：<https://medium.com/mlearning-ai/text-generation-using-gpt-neo-41877ef586c7?source=collection_archive---------0----------------------->

## 推理和微调 GPT-尼奥使用快乐变形金刚

![](img/095c0c0558640c517da5175e001a37b1.png)

Image from [Source](https://unsplash.com/photos/vuMTQj6aQQ0)

# 介绍

去年， [OpenAI 的 GPT-3](https://github.com/openai/gpt-3) 发布，截至今天，它是现存的第二大语言模型([谷歌大脑的 1.6 万亿](https://www.kdnuggets.com/2021/01/google-trillion-parameter-switch-transformer-model.html)参数语言模型是最大的)。这是一个基于[变压器的神经网络](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))，它的训练目标很简单…