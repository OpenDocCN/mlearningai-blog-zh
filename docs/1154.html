<html>
<head>
<title>An out-and-out view of Transformer Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器架构的完整视图</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-out-and-out-view-of-transformer-architecture-6926da4c8080?source=collection_archive---------2-----------------------#2021-10-13">https://medium.com/mlearning-ai/an-out-and-out-view-of-transformer-architecture-6926da4c8080?source=collection_archive---------2-----------------------#2021-10-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a14c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">为什么会推出变压器？</h1><p id="653e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">对于顺序任务，最广泛使用的网络是RNN。但是RNN不能处理消失梯度。因此，他们引入了LSTM、GRU网络，借助存储单元和门来克服消失梯度。但就长期依赖性而言，甚至GRU和LSTM也缺乏，因为我们依赖这些新的门/记忆机制将信息从旧步骤传递到当前步骤。如果你不了解LSTM和GRU，没什么好担心的，只是因为《变形金刚》的评估才提到的，这篇文章与LSTM或格鲁无关</p><p id="5d55" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，transformer克服了长期依赖，绕过了整个句子，而不是逐字逐句(顺序)。它可以直接访问所有其他单词，并引入了不允许任何信息丢失的自我关注机制。几个新的自然语言处理模型在人工智能行业，特别是自然语言处理领域产生了巨大的变化，如BERT，GPT-3和T5，都是基于transformer架构的。《变形金刚》之所以成功，是因为他们使用了一种叫做<strong class="je hi">自我关注</strong>的特殊类型的关注机制。我们将深入了解自我关注机制。</p><h1 id="bf6f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">变压器架构</h1><p id="9164" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">NLP中的Transformer是一种新颖的体系结构，旨在解决序列到序列的任务，同时轻松处理长距离依赖性。论文中提出的变压器<a class="ae kf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">就是你所需要的全部注意力</a>。下图是变压器架构。我们将把Transformer架构分解成子部分，以便更好地理解它。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/220d77957b78e04d010b55fb8ee43b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*WZUutWV7mSYgf5ZHXNBtAA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx"><em class="ks">source: </em><a class="ae kf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"><em class="ks">arXiv:1706.03762</em></a><strong class="bd ig"><em class="ks">.</em></strong></figcaption></figure><h1 id="c1b9" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">本文中分解变压器架构如下:</h1><p id="ed03" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi"> 1。编码器-解码器架构</strong></p><p id="f024" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 2。编码器架构</strong></p><blockquote class="kt ku kv"><p id="64ad" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">2.1输入嵌入和位置编码</p><p id="d14a" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">2.2自我关注机制</p><p id="1990" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">2.3多头关注机制</p><p id="0da7" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">2.4前馈网络</p><p id="c46b" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">2.5添加和定额组件</p></blockquote><p id="252f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 3。解码器架构</strong></p><blockquote class="kt ku kv"><p id="b31b" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">3.1掩盖多头注意力</p><p id="adc0" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">3.2多头关注</p><p id="4d80" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">3.3前馈网络</p><p id="7625" class="jc jd kw je b jf ka jh ji jj kb jl jm kx kc jp jq ky kd jt ju kz ke jx jy jz ha bi translated">3.4添加和定额组件</p></blockquote><p id="51b4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 4。线性和softmax层</strong></p><p id="cce7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> 5。将编码器-解码器组装在一起</strong></p><h1 id="0fcd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">1.编码器-解码器架构:</h1><p id="a8e4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><em class="kw">我们将输入的句子输入编码器。编码器使用一些注意力和网络来学习输入句子的表示。解码器接收由编码器学习的表示作为输入，并生成输出。</em></p><p id="6571" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="kw">例如，如果我们正在构建一个从英语到德语的机器翻译模型。让我们假设给编码器的输入句子是“你好吗？”而解码器的输出应该是“威格特的？”。请参考下面的图2。</em></p><p id="de30" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="kw">因此，原始输入数据“你好吗”被提供给编码器，该编码器以向量捕获句子的语义，例如每个单词的100维向量。因此，该表示可以是(3，100)矩阵的形式，其中3是单词的数量，100是每个单词向量的维数。来自编码器的向量表示被提供给解码器，解码器建立机器翻译模型，将向量表示转换成人类可读形式的输出。</em></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es la"><img src="../Images/a51e63bb90344f9479a6387aba8c57aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/0*XKYD_ukFgyd49FN9.PNG"/></div></figure><p id="df07" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="kw">这种机器翻译背后的过程对我们来说始终是一个黑箱。但我们现在将详细了解转换器中的编码器和解码器如何将英语句子转换为德语句子</em></p><h1 id="b749" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.编码器架构:</h1><p id="d5df" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><em class="kw">变压器不仅仅由一个如图2所示的编码器组成。它有几个编码器堆叠在一起。编码器1的输出作为输入发送到编码器2，编码器2作为输入发送到编码器3，依此类推，直到编码器n和编码器n返回句子“你好吗？”作为输入提供给解码器。如下图3所示。</em></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lb"><img src="../Images/6221738b57977cc5bd03fad59182c9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/0*4c2_yhQoL_XBIObN.PNG"/></div></figure><p id="ed99" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">每个块由2个子层<strong class="je hi">多头关注</strong>和<strong class="je hi">前馈网络</strong>组成，如上图4<strong class="je hi">所示。</strong>这在每个编码器块中都是相同的，所有编码器块都有这两个子层。在深入多头注意力的第一子层之前，我们先来看看什么是自我注意力机制。</p><h1 id="fcc5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.1.输入嵌入和位置编码</h1><p id="97e1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">输入嵌入:</strong></p><p id="abd8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输入嵌入只是嵌入层。嵌入层获取单词序列，并学习每个单词的矢量表示。向量表示法</p><p id="5786" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">位置编码:</strong></p><p id="4e53" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">单词的位置和顺序决定了句子的语法和实际语义。在RNN的例子中，它通过一个词一个词地分析一个句子来考虑单词的顺序。位置编码块对嵌入矩阵应用一个函数，该函数允许神经网络理解每个单词向量的相对位置。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/7de0b111feb85589571173ef140086dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WFuPlIJc9qOnv84i.PNG"/></div></div></figure><p id="748a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">非常简单，创建了一个新的向量，每个条目都是它的索引号。这是绝对位置编码。但是有一个错误的方法，因为数字的尺度不同。如果我们有一个由500个记号组成的序列，我们的向量最终会是500。一般来说，神经网络喜欢它们的权重在零附近徘徊，并且通常正负平衡。如果没有，你会让自己暴露在各种各样的问题面前，比如爆炸性的梯度和不稳定的训练。</p><p id="0f42" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们借助正弦和余弦函数创建一个位置编码器:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lh"><img src="../Images/8b51cc84a361bbce21c5b3a51df50105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jlAeCmkWEDLhgXsG.png"/></div></div></figure><p id="37c7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">取公式中的sin部分。让我们假设这个句子里有5个单词。其中d=5，(p0，p1，p2，p3，p4)将是每个单词的位置。保持内径不变并改变位置。我们有，</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es li"><img src="../Images/13ac7bac456f889176ee5307c33fae88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1QcqRBgD6oF8RROU.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx"><em class="ks">source: </em><a class="ae kf" href="https://www.youtube.com/watch?v=dichIcUZfOw" rel="noopener ugc nofollow" target="_blank"><em class="ks">Visual Guide to Transformer Neural Networks — (Part 1) Position Embeddings</em></a></figcaption></figure><p id="3cfc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果我们绘制一条正弦曲线并改变“位置”(在x轴上)，您将在y轴上得到不同的位置值。因此，不同位置的词会有不同的位置嵌入值。</p><p id="9bde" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">但是有一个问题。由于“sin”曲线在间隔中重复，您可以在上图中看到P0和P6具有相同的位置嵌入值，尽管它们位于两个非常不同的位置。这就是等式中“我”部分发挥作用的地方。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lj"><img src="../Images/a8d0097a04a648b0594dc476c5fcd2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*21zITdftNexXYAIQ.PNG"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx"><em class="ks">source: </em><a class="ae kf" href="https://www.youtube.com/watch?v=dichIcUZfOw" rel="noopener ugc nofollow" target="_blank"><em class="ks">Visual Guide to Transformer Neural Networks — (Part 1) Position Embeddings</em></a></figcaption></figure><p id="2e97" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果你改变上面等式中的“I ”,你会得到一串不同频率的曲线。相对于不同频率的位置嵌入值的读取结果给出了P0和P6在不同嵌入维度的不同值。</p><h1 id="598d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.2.自我注意机制</h1><p id="20dc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">自我注意机制确保句子中的每个单词都有一些关于上下文单词的知识。例如，我们使用这些著名的句子"<strong class="je hi">动物没有穿过街道是因为它太长了</strong>"和"<strong class="je hi">动物没有穿过街道是因为它太累了</strong>"</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lk"><img src="../Images/db7e6909a9177465f106894115f4f562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3O377N8jhVC9gg5a.PNG"/></div></div></figure><p id="8cb0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">所以“它”完全取决于“长”和“累”两个字。“长”字靠“街”，“累”字靠“兽”。我们如何让模型理解它！？这就是我们使用自我关注机制的地方。自我关注机制确保每个单词都与所有单词相关。</p><p id="3bda" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">让我们再来看看句子“你好吗”,每个单词的单词嵌入是100维。那么输入矩阵的维数将是X[3，100]，其中3是单词的数目，100是每个单词的维数。</p><p id="6a3e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">自我关注机制通过使用<strong class="je hi">查询</strong>(Q)<strong class="je hi">键</strong>(K)<strong class="je hi">值</strong> (V) <strong class="je hi">矩阵进行学习。</strong>这些查询、关键字和值矩阵是通过将输入矩阵X乘以权重矩阵WQ、WK、WV而创建的。权重矩阵WQ、WK、WV被随机初始化，并且它们的最佳值将在训练期间被学习。</p><p id="7f80" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这就是我们计算查询、键和值矩阵的方式。我们将会看到Q、K和V是如何在自我注意机制中使用的。自我注意机制包括四个步骤。</p><p id="5a8e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第一步:</strong></p><p id="580f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">计算查询和关键字矩阵之间的点积。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ll"><img src="../Images/45bf0688f6894e8860cca2cd083a5a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DR1bOq_zLvqKOYoV.PNG"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lm"><img src="../Images/8cba6b507f3e4ea31e773b55e0c027f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Lc0fd2TQOv4qcwrC.PNG"/></div></div></figure><p id="20f0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，我们可以说，计算查询矩阵(Q)和关键矩阵(KT)之间的点积本质上给了我们相似性得分，这有助于我们理解句子中的每个单词与所有其他单词有多相似。</p><p id="171c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第二步:</strong></p><p id="64ab" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">自我注意机制的第二步是将Q.KT矩阵除以关键向量维数的平方根。我们这样做是为了获得稳定的梯度。</p><h1 id="461d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Q.KT / √dk</h1><p id="0164" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">第三步:</strong></p><p id="de2f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们通过应用softmax函数将<strong class="je hi"> Q.KT / √dk </strong>的非规范化形式转换为规范化形式，这有助于将分数带到0到1的范围内，并且分数之和等于1。</p><h1 id="f42e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">得分矩阵= softmax(Q.KT / √dk)</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ln"><img src="../Images/c202834d216786bd22745f70c809db2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZIsguGudNEFsN2W6.PNG"/></div></div></figure><p id="9128" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第四步:</strong></p><p id="4b23" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们通过将得分矩阵乘以价值矩阵来计算注意力矩阵<strong class="je hi"> z </strong>。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lo"><img src="../Images/bc70ad8277484a61bbfa7e79590fd970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H_H1nOuzA4RrNvu9.PNG"/></div></div></figure><p id="6161" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，<strong class="je hi"> ZHow </strong>的值将包含来自价值向量(How)的值的98%，来自价值向量(you)的值的1%，来自价值向量(doing)的值的1%。参考上面的图9。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lp"><img src="../Images/969aa84debcad3e59b62e6ffd8d97bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bI7Xw3_cQ3GJ_4X4.PNG"/></div></div></figure><p id="7dfc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">同样，在“动物因为太长而没有过马路”的例子中，可以通过上面提到的4个步骤计算出<strong class="je hi"> Zit </strong>的值。那么<strong class="je hi">青春痘</strong>就会:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lq"><img src="../Images/aaeec722cb1407802626cedd00f3b91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lyHLmo7r398kGkjS.PNG"/></div></div></figure><p id="907f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">单词“它”的自我关注值包含来自值向量V6(街道)的值的81%。这有助于从上面的句子中看出单词“它”实际上指的是“街道”而不是“动物”。因此，我们可以通过使用自我注意机制来理解一个单词与句子中所有其他单词的关系。</p><h1 id="c78d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.3.多头注意力机制</h1><p id="210b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们将计算多个单注意力矩阵并连接它们的结果，而不是计算单注意力矩阵。因此，通过使用这种多头注意力，我们的注意力模型将更加准确。</p><p id="15b6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，对于短语“你好吗”，我们将通过创建查询(Q1)、关键(K1)和值(V1)矩阵来计算第一个单一关注矩阵。其计算方法是将输入矩阵(X)乘以加权矩阵WQ、WK和WV。那么我们的第一个注意力矩阵将是，</p><h1 id="2814" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Z1 = Softmax(Q1。K1T / √dk1)</h1><p id="8965" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然后，我们将通过将输入矩阵(X)乘以加权矩阵WQ、WK和WV来创建查询(Q2)、键(K2)和值(V2)矩阵，从而计算第二关注矩阵。那么我们的第二个注意力矩阵将会是，</p><h1 id="9ea4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Z2 = Softmax(Q2。K12T / √dk2)</h1><p id="e536" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">同样，我们将计算n个注意力矩阵(z1，z2，z3，…zn ),然后连接所有的注意力矩阵。所以我们的多头注意力矩阵是:</p><h1 id="95ce" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">多头关注=串联(Z1，Z2，…)。Zn)*W0</h1><p id="3f53" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">其中W0是权重矩阵。</p><h1 id="72da" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.4.前馈网络</h1><p id="f756" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在前向神经网络层中，它由两个具有相关活动的密集层组成。这适用于每一个注意力向量。以便它是下一个编码器和解码器关注层可接受的形式。</p><h1 id="8b52" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.5.添加和定额组件</h1><p id="df19" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">add和Norm组件基本上是层规范化之后的剩余连接。它连接子层的输入和输出。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lr"><img src="../Images/abfb3bb6dec0cc7be1674a83bd8dd27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/0*DzC__XO3qaXufH7E.PNG"/></div></figure><p id="bada" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">其将多头注意力子层的输入连接到其输出前馈神经网络层。然后将前馈子层的输入连接到其输出。</p><h1 id="6e3a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.解码器架构</h1><p id="452e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是一个解码器单元的堆栈，每个单元将编码器的表示作为前一个解码器的输入。因此，每个解码器接收两个输入。这样，每个预测在时间步长t的输出。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ls"><img src="../Images/08c68da0a19be5f499d6da6577c60b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*85OSmMh1v5Xl2Wfy.gif"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx"><em class="ks">Source: </em><a class="ae kf" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank"><em class="ks">jalammar’s</em></a></figcaption></figure><p id="a4e6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">来自上面的GIF图片来自<a class="ae kf" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">贾勒马的</a>博客。</p><p id="39e5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">解码器将输入<sos>作为第一个令牌。在时间步长t=2，解码器接收两个输入:一个来自先前解码器预测的先前输出，另一个是预测“am”的编码器表示。在时间步长t=3，解码器接收来自先前输出和编码器表示的输出，并预测“a”。同样，它预测直到它到达结束标记<eos>。</eos></sos></p><h1 id="5548" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.1.掩蔽的多头注意力</h1><p id="fab9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">对于底部解码器或第一个解码器，应给出输入。我们不是将输入直接提供给解码器，而是将其转换为输出嵌入，并添加位置编码，然后将其提供给解码器。</p><p id="75e9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi"> X =输出嵌入+位置编码</strong></p><p id="b8e3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">x将作为第一解码器的输入。现在，我们通过将权重矩阵<strong class="je hi"> WQ、WK和WV </strong>乘以<strong class="je hi"> X </strong>来创建查询(Q)、键(K)和值(V)矩阵，就像我们在编码器中所做的那样。</p><p id="57a6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后我们计算<strong class="je hi">气。KiT / √dki </strong>等于下面给出的矩阵</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lt"><img src="../Images/97dde71556202bc17db372d54e7f4e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/0*CNzgmCWrDOLAe6RQ.PNG"/></div></figure><p id="9255" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在归一化上面得到的矩阵之前。我们需要用∞来屏蔽目标单词右边的单词。以便使用句子中的前一个单词，并屏蔽其他单词。这允许转换器学习预测下一个单词。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lu"><img src="../Images/44a84688bf7b334d44e22e9f50d8849f.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*TB4VbxnVslzXd7_l.PNG"/></div></figure><p id="710e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通过将softmax应用于屏蔽的<strong class="je hi"> Qi，该屏蔽的注意块的输出被相加并归一化。KiT / √dki </strong>矩阵，然后传递给另一个注意模块。</p><h1 id="1271" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.2.多头注意力</h1><p id="1535" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">每个解码器接收两个输入:一个来自前一子层掩蔽的多头注意力，另一个是编码器表示。</p><p id="15df" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">让我们用R表示编码器表示，用m表示作为掩蔽多注意子层的结果而获得的注意矩阵。因为我们有编码器和解码器之间的交互，所以这一层被称为<strong class="je hi">编码器-解码器注意层</strong>。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lv"><img src="../Images/07c1cd65b9ebcfa57276f0e7b87dfc7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vv8eC51FSzBNCbu1.PNG"/></div></div></figure><p id="6fe8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">查询矩阵实质上保存了目标句子。因为它是从M中获得的，并且键和值矩阵保存源句子的表示。因为它是从r获得的。</p><p id="be06" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">然后，我们像在编码器中一样计算得分矩阵。但是这次我们从两个不同的矩阵中得到Q，K和V矩阵。</p><h1 id="aee1" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Z1 = Softmax(Q1。K1T / √dk1)，Z2 = Softmax(Q2。K12T / √dk2)…</h1><p id="de5b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">然后我们用z1，z2，z3，…计算多头注意力。上面的zn。</p><h1 id="a1a2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">多头关注=串联(Z1，Z2，…)。Zn)*W0</h1><h1 id="d339" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">3.3前馈神经网络及加法和范数</h1><p id="e4e7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这与编码器的工作方式相同。</p><h1 id="1d0f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">4.线性和Softmax层</h1><p id="b2b9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">解码器根据问题学习目标句子/目标类别/的表示。我们将最顶层解码器的表示提供给线性和Softmax层。</p><p id="9855" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">线性层生成其大小等于词汇表大小的logits。假设我们的词汇只有三个词“你好”。那么线性层返回的logits的大小将为3。然后，我们使用softmax函数将对数转换为概率，解码器输出其索引具有较高概率值的单词。</p><h1 id="482a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">5.将编码器和解码器组装在一起</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/0f10946d329271f4ec1060d43167553f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/0*3JcWMW151auaT00X.PNG"/></div></figure><h1 id="4da4" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考:</h1><p id="e83f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">1.灵感来自Sudharsan Ravichandiran先生的《Google Bert 入门》一书。</p><p id="05b0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">2.<a class="ae kf" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">可视化神经机器翻译</a>模型博客作者<a class="ae kf" href="https://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛</a>。</p><p id="9191" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">3.《变形金刚》图解指南博客。</p><p id="0ec2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">4.<a class="ae kf" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">位置编码</a>博客。</p><p id="7179" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">5.<a class="ae kf" href="https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model" rel="noopener ugc nofollow" target="_blank"> StackExchange线程—变压器中的位置编码</a>。位置编码最佳解释</p><p id="4ca1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">6.<a class="ae kf" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> Colah的博客。</a></p></div></div>    
</body>
</html>