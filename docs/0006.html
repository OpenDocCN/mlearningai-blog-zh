<html>
<head>
<title>Using BERT with Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过Pytorch使用BERT</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/using-bert-with-pytorch-b9624edcda4e?source=collection_archive---------0-----------------------#2019-06-10">https://medium.com/mlearning-ai/using-bert-with-pytorch-b9624edcda4e?source=collection_archive---------0-----------------------#2019-06-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="396b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">一个超级简单实用的指南，使用Pytorch构建你自己的基于BERT的架构。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es iw"><img src="../Images/1b1aa2e5cc9126d63df8bb70711eafdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*2XpE-VjhhLGkFDYg.jpg"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Bert image — sesame street</figcaption></figure><p id="a9f0" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这篇文章中，我假设你知道BERT模型和原理。如果没有，我强烈建议你阅读这篇文章[1]和这篇文章,或者听听我关于情境化嵌入的演讲。如果您仍然缺少一些背景知识，您可能需要阅读位置嵌入和转换器。</p><p id="0347" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在本帖中，你会发现一个超级简单实用的指南，其中包含使用Pytorch构建你自己的微调过的基于BERT的架构的代码示例。我们将使用https://github.com/huggingface/pytorch-pretrained-BERT<a class="ae ke" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank">的</a>奇妙套装。</p><p id="386d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果你理解BERT，你可能会发现你需要在你的代码中采取这两个步骤:标记样本和构建你自己的微调架构。</p><ol class=""><li id="e84e" class="kf kg hh jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">对样品进行标记(BPE):</li></ol><p id="63ad" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">伯特使用了一种特殊的词汇化(BPE)。此外，根据你的任务，每个句子可以在第一句的开头用[CLS]填充，在每个句子的结尾用[SEP]标记填充。<br/>【CLS】标记主要用于分类任务，而【SEP】标记用于SNLI或问答等任务的多个句子。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/ccf2e85dc14d43c709c7535d9dce8db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fywysLzEbXxEqkPeIUWaJw.png"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">BERT input presentation [1]</figcaption></figure><pre class="ix iy iz ja fd kt ku kv kw aw kx bi"><span id="6f31" class="ky kz hh ku b fi la lb l lc ld"><strong class="ku hi">from</strong> pytorch_pretrained_bert.tokenization <strong class="ku hi">import </strong>BertTokenizer</span><span id="f971" class="ky kz hh ku b fi le lb l lc ld">tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)</span><span id="d920" class="ky kz hh ku b fi le lb l lc ld">def get_tokenized_samples(samples, max_seq_length, tokenizer):<br/>    <em class="lf">"""<br/>    we assume a function </em><strong class="ku hi"><em class="lf">label_map </em></strong><em class="lf">that maps each label to an index or vector encoding. Could also be a dictionary.<br/>    </em><strong class="ku hi"><em class="lf">:param</em></strong><em class="lf"> </em>samples<em class="lf">: we assume struct {.text, .label) <br/>    </em><strong class="ku hi"><em class="lf">:param</em></strong><em class="lf"> max_seq_length: the maximal sequence length<br/>    </em><strong class="ku hi"><em class="lf">:param</em></strong><em class="lf"> tokenizer: BERT tokenizer<br/>    </em><strong class="ku hi"><em class="lf">:return</em></strong><em class="lf">: list of features<br/>    """<br/></em><br/>    features = []<br/>    for sample in samples:<br/>        textlist = sample.text.split(' ')<br/>        labellist = sample.label<br/>        tokens = []<br/>        labels = []<br/>        for i, word in enumerate(textlist):<br/>            token = tokenizer.tokenize(word) #tokenize word according to BERT<br/>            tokens.extend(token)<br/>            label = labellist[i]<br/>            # fit labels to tokenized size of word<br/>            for m in range(len(token)):<br/>                if m == 0:<br/>                    labels.append(label)<br/>                else:<br/>                    labels.append("X")<br/>        # if we exceed max sequence length, cut sample<br/>        if len(tokens) &gt;= max_seq_length - 1:<br/>            tokens = tokens[0:(max_seq_length - 2)]<br/>            labels = labels[0:(max_seq_length - 2)]<br/>            <br/>        ntokens = []<br/>        segment_ids = []<br/>        label_ids = []<br/>        # start with [CLS] token<br/>        ntokens.append("[CLS]")<br/>        segment_ids.append(0)<br/>        label_ids.append(label_map(["[CLS]"]))<br/>        for i, token in enumerate(tokens):<br/>            # append tokens<br/>            ntokens.append(token)<br/>            segment_ids.append(0)<br/>            label_ids.append(label_map(labels[i]))<br/>        # end with [SEP] token<br/>        ntokens.append("[SEP]")<br/>        segment_ids.append(0)<br/>        label_ids.append(label_map(["[SEP]"]))<br/>        # convert tokens to IDs<br/>        input_ids = tokenizer.convert_tokens_to_ids(ntokens)<br/>        # build mask of tokens to be accounted for<br/>        input_mask = [1] * len(input_ids) <br/>        while len(input_ids) &lt; max_seq_length:<br/>            # pad with zeros to maximal length<br/>            input_ids.append(0)<br/>            input_mask.append(0)<br/>            segment_ids.append(0)<br/>            label_ids.append([0] * (len(label_list) + 1))<br/><br/>        features.append((input_ids,<br/>                              input_mask,<br/>                              segment_ids,<br/>                              label_id))<br/>    return features</span></pre><p id="8367" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">2.基于BERT构建您自己的架构</p><p id="45d8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">与传统的嵌入不同，BERT嵌入是与上下文相关的，因此我们需要依赖预训练的BERT架构。在完整句子分类任务中，我们在[CLS]标记的输出之上添加一个分类层。在序列标记中，我们需要序列的完整输出。这个简单的例子是一个序列标签。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lg"><img src="../Images/79b7b00d18daea8596f38c7d50c7ff42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77DLGWK_bm-ij1o8QHPxUA.png"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">Fine Tune BERT pre-training to your task [1]</figcaption></figure><pre class="ix iy iz ja fd kt ku kv kw aw kx bi"><span id="149a" class="ky kz hh ku b fi la lb l lc ld">from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel</span><span id="86dc" class="ky kz hh ku b fi le lb l lc ld">class MyBertBasedModel(BertPreTrainedModel):<br/>    <em class="lf">"""<br/>    MyBertBasedModel inherits from BertPreTrainedModel which is an abstract class to handle weights initialization and<br/>        a simple interface for downloading and loading pre-trained models.<br/>    """<br/><br/>    </em>def __init__(self, config, num_labels):<br/>        super(MyBertBasedModel, self).__init__(config)<br/>        self.num_labels = num_labels<br/>        self.bert = BertModel(config) # basic BERT model<br/>        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)<br/>        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)<br/>        self.apply(self.init_bert_weights)<br/><br/><br/>    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):<br/>        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)<br/>        # now you can implement any architecture that receives bert sequence output<br/>        sequence_output = self.dropout(sequence_output)<br/>        logits = self.classifier(sequence_output)<br/><br/>        if labels is not None:<br/>            loss_fct = MyLoss()<br/>            # it is important to activate the loss only on un-padded inputs<br/>            active_loss = attention_mask.view(-1) == 1<br/>            active_logits = logits.view(-1, self.num_labels)[active_loss]<br/>            active_labels = labels.view(-1, self.num_labels)[active_loss]<br/>            loss = loss_fct(active_logits, active_labels)<br/>            return loss<br/>        else:<br/>            return logits</span></pre><p id="f8ea" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">3.这一切是如何结合在一起的</p><pre class="ix iy iz ja fd kt ku kv kw aw kx bi"><span id="641b" class="ky kz hh ku b fi la lb l lc ld">train_tokenized_samples = get_tokenized_samples(<br/>    train_samples, args.max_seq_length, tokenizer)</span><span id="51ed" class="ky kz hh ku b fi le lb l lc ld">model = MyBertBasedModel.from_pretrained(args.bert_model,<br/>          num_labels = num_labels)</span><span id="f89e" class="ky kz hh ku b fi le lb l lc ld">model.train()<br/>for range(n_epochs):<br/>    for sample in train_tokenized_samples:<br/>        input_ids, input_mask, segment_ids, label_ids = sample<br/>        loss = model(input_ids, segment_ids, input_mask, label_ids)<br/>        loss.backward()<br/>        optimizer.step()</span></pre><p id="8905" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我希望这使得在Pytorch中使用预先训练好的BERT模型更加容易。</p><p id="ad5f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">[1]伯特:语言理解深度双向变形金刚前期训练:雅各布·德夫林，张明蔚，肯顿·李，克里斯蒂娜·图塔诺娃:<a class="ae ke" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a>，2018</p><div class="lh li ez fb lj lk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hi fi z dy lp ea eb lq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">medium.com</p></div></div><div class="lt l"><div class="lu l lv lw lx lt ly jc lk"/></div></div></a></div><p id="8811" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><a class="ae ke" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">成为ML写手</a></p></div></div>    
</body>
</html>