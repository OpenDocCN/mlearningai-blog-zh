<html>
<head>
<title>Doodling AI: Attention Is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">涂鸦人工智能:注意力是你所需要的</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/doodling-ai-attention-is-all-you-need-8fc4fa4343d?source=collection_archive---------3-----------------------#2021-09-30">https://medium.com/mlearning-ai/doodling-ai-attention-is-all-you-need-8fc4fa4343d?source=collection_archive---------3-----------------------#2021-09-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e0cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我对我觉得有趣的ML/AI主题做了一个温和的概述。</p><p id="4b90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天的涂鸦是我从<a class="ae jd" href="https://people.cs.umass.edu/~miyyer/" rel="noopener ugc nofollow" target="_blank">莫希特·伊耶</a>教授的<a class="ae jd" href="https://people.cs.umass.edu/~miyyer/cs685/index.html" rel="noopener ugc nofollow" target="_blank">高级自然语言处理</a>讲座(<a class="ae jd" href="https://youtu.be/YaYELBT9Z0I" rel="noopener ugc nofollow" target="_blank">视频</a>)中得到的笔记。本文简要介绍了谷歌开创性论文中介绍的<strong class="ig hi">变压器自我关注机制</strong>—<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">关注是你所需要的全部</a>。</p><p id="765e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">请注意，这不是一篇详尽的综述，为了简单起见，本文省略了一些技术细节。详情请参考</em> <a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="jc">论文</em> </a> <em class="jc">(或本惊人</em> <a class="ae jd" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <em class="jc">博客</em> </a> <em class="jc">)。</em></p><p id="feff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自我关注<br/> </strong>下面是对自我关注的简化描述。<em class="jc"> q3 </em>负责所有之前的嵌入。注意力权重用于创建值向量<em class="jc"> z3 </em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/5764894b237359822d1a6e4846b20647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8OhroU1aQjtdzDc6mNVBkQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Self Attention</figcaption></figure><p id="ea2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意并行<br/> </strong>我们可以把自我注意中的每一个操作表示为一个高度并行的矩阵操作。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/d08b1dcab19d78aaee1f2f306e22806a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wM38GYnkpa8EYIthVE-HkQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Self-Attention and Masking. The image on top right is by Emma Strubell and taken from lecture slides.</figcaption></figure><p id="b4e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">介绍<strong class="ig hi">位置嵌入<br/>T38】为什么？与RNN不同，变形金刚不知道任何记号在给定序列中的位置。此信息必须单独提供。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/fe1125bdec6b9fe37ebdc6a807f0ef8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ft5qvlXMCdIPK9vL3yggDw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Position embeddings in Transformers</figcaption></figure><p id="4b6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">多头注意力<br/> </strong>另一个重要的创新是多头注意力将不同空间的向量独立投射到他们身上。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/6793668d0edcd63b4031385831907803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5t6Av3SGGKp9DCQ7JefUg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Multi-head attention in Transformers</figcaption></figure><p id="4c7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">深层变压器神经网络<br/> </strong>直到现在，我们描述的只是单层变压器。要添加更多层，只需堆叠它们。每一层的输出都作为下一层的输入。下图显示了具有单个头的深度网络，但是相同的概念可以扩展到具有多个头的深度网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/8213342236e47055878b7496c59208ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EolClpSGT84P7pRPQ8JMBQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Deep Transformer NN</figcaption></figure><p id="8616" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">仅此而已。这就结束了我们对变形金刚中自我关注的概述。感谢阅读！</p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><h1 id="62ab" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">摘要</h1><p id="67de" class="pw-post-body-paragraph ie if hh ig b ih kz ij ik il la in io ip lb ir is it lc iv iw ix ld iz ja jb ha bi translated">在这篇文章中，我只讲述了变形金刚的自我关注机制，如下图所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es le"><img src="../Images/cca419270472eb74ee3a32a66f067510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoUepEhPHI1voP-_RwAePg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image obtained from original Transformers <a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="46b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">注意</em> </strong> <em class="jc">:查询-键矩阵相乘后的缩放很重要，避免权重过大。这可确保权重的softmax规格化不会将权重推向低梯度区域。</em></p><p id="b4c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来是变压器的实际编码器和解码器模块，将在另一篇文章中讨论。</p></div><div class="ab cl ju jv go jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="ha hb hc hd he"><p id="5978" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">其他涂鸦AI帖子:<br/> </em> <a class="ae jd" href="https://shan101.medium.com/doodling-ai-man-is-to-computer-programmer-as-woman-is-to-homemaker-ce5e3e2cdf2e" rel="noopener">涂鸦AI:男人对于电脑程序员就像女人对于家庭主妇</a>？<br/> <a class="ae jd" href="https://shan101.medium.com/doodling-ai-crows-pairs-challenge-4c5d00c3f581" rel="noopener">涂鸦AI:乌鸦对挑战</a></p></div></div>    
</body>
</html>