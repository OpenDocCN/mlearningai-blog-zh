<html>
<head>
<title>Application of Mixed Integer Quadratic Programming (MIQP) in Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">混合整数二次规划(MIQP)在特征选择中的应用</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/application-of-mixed-integer-quadratic-programming-miqp-in-feature-selection-for-regression-3985d2ab95a7?source=collection_archive---------3-----------------------#2022-01-15">https://medium.com/mlearning-ai/application-of-mixed-integer-quadratic-programming-miqp-in-feature-selection-for-regression-3985d2ab95a7?source=collection_archive---------3-----------------------#2022-01-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0dbc4a7ec7330dcae25b3e3f96244ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WZ5rVopsgw5MDGnxFF96zg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">A fancy pic about cherry picking on <a class="ae it" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="23c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">线性回归是一种用于预测定量反应的监督学习算法。它假设在特征向量xi∈R(d)和响应yi∈R之间存在线性关系。预测分析中最常见的问题之一是回归的变量选择。由于普通最小二乘法(OLS)很少产生精确为零的估计值，因此丢弃了与响应无关的特征，我们需要求助于特征选择方法。流行的方法包括:</p><p id="5cfb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">子集选择，例如逐步选择。<br/>降维，如主成分分析。<br/>收缩，如套索。</p><p id="5310" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于计算困难，使用优化的直接变量选择长期以来一直被统计/分析社区所摒弃。这个计算问题是LASSO和ridge回归发展的部分动机。然而，在最近的过去，优化软件有了巨大的进步，如Gurobi，特别是解决混合整数二次规划(MIQP)的能力。</p><p id="2970" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将提出回归的变量选择问题作为MIQP，并与LASSO进行比较。我们将讨论每种方法的优缺点，并就哪种方法更适合哪种场景提出建议。</p><p id="f575" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">请注意，本文中的建模示例可能需要您具备一些关于使用Gurobi Python API构建数学优化模型的知识。</em></p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="2fbc" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> <em class="ky">临近</em> </strong></h1><p id="0010" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">在我们的实验中，有2个数据集，包括x和y数据。一个数据集是训练数据集，一个是测试数据集。我们将首先对训练集进行10重交叉验证，以选择k或λ。然后，利用k或λ的最佳值，我们将使用整个训练集来拟合βs。然后，利用这些β，我们将对测试集上的y值进行预测，并将我们对y的预测与测试集中y的真实值进行比较。</p><p id="6a2b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了使我们的结果完全可复制，所有相关的源代码和数据集都在我们的<a class="ae it" href="https://github.com/ping2022/Mixed-Integer-Quadratic-Programming-MIQP-in-Feature-Selection" rel="noopener ugc nofollow" target="_blank"> github </a>上公开。</p><blockquote class="le lf lg"><p id="0d82" class="iu iv js iw b ix iy iz ja jb jc jd je lh jg jh ji li jk jl jm lj jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="hh">方法一:直接变量选择— MIQP </em> </strong></p></blockquote><p id="726e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">MIQP是一种优化方法，它将常规的普通最小二乘损失函数最小化，并包括几个约束条件，如<a class="ae it" href="https://en.wikipedia.org/wiki/Big_M_method#:~:text=In%20operations%20research%2C%20the%20Big,%22greater%2Dthan%22%20constraints." rel="noopener ugc nofollow" target="_blank">大M方法</a>，它涉及使用二元变量，以便在损失计算期间在计算中包括或排除某些变量。另一个重要的约束是超参数k，它是函数中包含的最大变量数。</p><p id="8c0e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们现在提出一个MIQP公式，它可以找到线性回归问题的权重估计，其中正好k个权重可以是非零的。</p><p id="d677" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">参数</strong></p><p id="e3b7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(1) k:包括在模型中的特征数量，忽略截距。有50个X变量，我们将尝试k = [5，10，15，20，25，30，35，40，45，50]</p><p id="7a38" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(2) Q矩阵:目标函数的二次分量。这里是一个(2m+1) * (2m+1)矩阵，其中矩阵的左上角等于X(T)X，其他所有值为零。</p><p id="0017" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(3) c:目标函数的线性分量。这里它是一个(2m+1) * 1向量，其中前(m+1)个分量是-2y(T)X，其余的为零。</p><p id="f12a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">决策变量</strong></p><p id="28c5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(1) βj:特征Xj和连续变量的权重，代表特征Xj每单位变化的响应变量的变化。</p><p id="4276" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(2) zj:二元变量。如果βj正好等于零，则为1，否则为0。用于管理预算约束的辅助变量。</p><p id="6b89" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">目标函数</strong></p><p id="517e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">训练误差</strong>:最小化误差平方和(SSE):</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/291f23afd8f80426090ca419d0689fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*Bgr7ReEqmd5U977aX1ILiQ.png"/></div></figure><p id="54d3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用线性代数中的一些技巧，我们可以将目标函数设定为:</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/5de5ac5bf576cb535ced9e9e0b386e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*YSRhmpWJUJdLzmar84PGBQ.png"/></div></figure><p id="0c2f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">约束条件</strong></p><p id="f66d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(1) <strong class="iw hi">互斥</strong>:若zj为零，则对应的βj为零。否则，βj的值介于-M和M之间。选择足够大的M，使得βj的值不等于M或-M。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/70b42b4513ed001f73f655a4979dc19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*91nSjKdIn5Cc4Mm2op3AeQ.png"/></div></figure><p id="9b2b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(2) <strong class="iw hi">预算约束</strong>:非零特性数量不能超过k。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/e734331edebf0b7a85d04596a3b0d60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*85rjiE6CT-wxybw8ifAsEw.png"/></div></figure><p id="8e73" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">基于以上，我们建立了如下的MIQP模型。该函数将返回由MIQP模型选择的k个权重。</p><pre class="ll lm ln lo fd lr ls lt lu aw lv bi"><span id="81c8" class="lw kb hh ls b fi lx ly l lz ma">def beta(X, y, k):<br/>    new_column = [1] * len(X)<br/>    X_addConst = np.insert(X, 0, new_column, axis=1)<br/>    <br/>    # define quadratic term Q<br/>    Q = np.zeros((2*m+1, 2*m+1))<br/>    Q[0:m+1, 0:m+1] = np.transpose(X_addConst) @ X_addConst<br/>    <br/>    # define linear term c<br/>    X_expand = np.zeros((len(X_addConst), 2*m+1))<br/>    X_expand[0:len(X_addConst), 0:m+1] = X_addConst<br/>    c = -2 * np.transpose(y) @ X_expand<br/>    <br/>    # define left hand side of constraint matrix A<br/>    A = np.zeros((2*m+1, 2*m+1))<br/>    A[:,0] = 0<br/>    A[0:m, 1:m+1] = np.diag(np.ones(m))<br/>    A[0:m, m+1:2*m+1] = np.diag(np.ones(m))*M<br/>    A[m:2*m, 1:m+1] = np.diag(np.ones(m))<br/>    A[m:2*m, m+1:2*m+1] = np.diag(np.ones(m))*(-M)<br/>    A[2*m:2*m+1, m+1:2*m+1] = [1]*m<br/>    <br/>    # define right hand side of constraint matrix B<br/>    b = np.array([0]*(2*m+1))<br/>    b[-1] = k<br/>    <br/>    # define lower boundary lb<br/>    lb = np.array([np.NINF] + [-M]*m + [np.NINF]*m)  # lb = np.array([-M] * (2*m+1))<br/>    <br/>    # define optimization sense<br/>    sense = np.array(['&gt;']*m + ['&lt;']*(m+1))<br/>    <br/>    # set up model<br/>    MIQPMod = gp.Model()<br/>    MIQPMod_x = MIQPMod.addMVar(len(Q),lb=lb,vtype=['C']*(m+1)+['B']*m)<br/>    MIQPMod_con = MIQPMod.addMConstrs(A, MIQPMod_x, sense, b)<br/>    MIQPMod.setMObjective(Q,c,0,sense=gp.GRB.MINIMIZE)<br/>    <br/>    MIQPMod.Params.TimeLimit = time<br/>    MIQPMod.optimize()<br/>    <br/>    coef = MIQPMod_x.x<br/>    loss = MIQPMod.objVal<br/>    <br/>    # return coefficients<br/>    return coef[0:m+1]</span></pre><p id="9347" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请注意，训练误差会随着更多要素的考虑而单调下降(这相当于放松了MIQP)，但我们还需要找到k的值，以最大限度地提高对未知观测值的回归性能。因此，建议通过交叉验证来估计SSE。我们随机打乱数据，将其分成10份，并使用所有可能的k值进行10份交叉验证。下面显示了我们如何计算SSE并执行交叉验证。</p><pre class="ll lm ln lo fd lr ls lt lu aw lv bi"><span id="c051" class="lw kb hh ls b fi lx ly l lz ma">def least_squares(coef, X, y):<br/>    new_column = [1] * len(X)<br/>    X = np.insert(X, 0, new_column, axis=1)<br/>    a = X @ coef - y<br/>    b = np.transpose(a)<br/>    c = b @ a<br/>    return c</span><span id="66a6" class="lw kb hh ls b fi mb ly l lz ma">def cross_val(X, y, k):<br/>    accuracies = []<br/>    coefs = []<br/>    df = pd.DataFrame(columns = ['coef'])<br/>    df['coef'] = list(df_train.columns)<br/>    i = 1<br/>    for train_index, holdout_index in kf.split(X):<br/>        X_train, X_holdout = X[train_index], X[holdout_index]<br/>        y_train, y_holdout = y[train_index], y[holdout_index]<br/>        coef = beta(X_train, y_train, k)<br/>        sse = least_squares(coef, X_holdout, y_holdout)<br/>        df[str(i)] = coef<br/>        i = i+1<br/>        accuracies.append(sse)<br/>    sum_sse = sum(accuracies)<br/>    df.set_index(['coef'],inplace=True)<br/>    return sum_sse, df, accuracies</span></pre><p id="688c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">接下来，我们选择对应于最小交叉验证错误的k值。在运行MIQP模型几个小时后，基于10文件夹交叉验证的最佳选择k是10，相应的sse是724.7876。</p><pre class="ll lm ln lo fd lr ls lt lu aw lv bi"><span id="d354" class="lw kb hh ls b fi lx ly l lz ma">keys = []<br/>values = []<br/>coefs = []  # store each ten sets of coefficients for every k<br/>accuracies = [] # store each ten sets of sse for every k<br/>dict = {} # Store k and corresponding sse into dictionary<br/>for k in k_all:<br/>    sse, coef, acc = cross_val(X_train, y_train, k)<br/>    keys.append(k)<br/>    values.append(sse)<br/>    coefs.append(coef)<br/>    accuracies.append(acc)<br/>dict['k'] = keys<br/>dict['sse'] = values</span><span id="d151" class="lw kb hh ls b fi mb ly l lz ma">df = pd.DataFrame(dict) # Convert dictionary into dataframe<br/>best_result = df.loc[df['sse'].idxmin()] #find the best chosen k based on minimun sse</span><span id="620e" class="lw kb hh ls b fi mb ly l lz ma">print('The best chosen k based on 10-folder cross validation is', best_result[0])<br/>print('Corresponding sse is', best_result[1])</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/4c9375bdc5ee09150a4ebecad3f5fb60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*OwnEERE_fAy5E7O8Jd0lmw.png"/></div></figure><blockquote class="le lf lg"><p id="48df" class="iu iv js iw b ix iy iz ja jb jc jd je lh jg jh ji li jk jl jm lj jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="hh">趋近ⅱ:间接变量选择— LASSO: </em> </strong></p></blockquote><p id="aad8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">LASSO回归是线性回归的一种形式，它使用收缩来从模型中删除变量，这样做可以使损失函数最小化。LASSO使用超参数λ(λ)来执行这种收缩，其中λ越大，越多的X变量被强制到绝对零。下面是公式:</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es md"><img src="../Images/9e2806a176742066f437f8236b8fcd45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*kgjCrl7sKkqaUAyw943-cQ.png"/></div></figure><p id="9f21" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们首先将λ设置为在间隔[0.001，1000]内计算的100个均匀间隔的数。接下来，我们对训练集进行10次交叉验证以选择λ。在同一数据集上运行LASSO回归只需要几秒钟。基于10文件夹交叉验证选择的最佳λ为0.0756，相应的sse为695.00097。</p><pre class="ll lm ln lo fd lr ls lt lu aw lv bi"><span id="1836" class="lw kb hh ls b fi lx ly l lz ma"># Generate 100 uniform values between -3 to 3 as power series<br/>alphas = 10**np.linspace(3,-3,100)</span><span id="d74e" class="lw kb hh ls b fi mb ly l lz ma"># Write function that computes the sse over all 10 folds for lasso.<br/>def cv_accuracy_score_lasso(X, y, alpha):<br/>    model = Lasso(alpha=alpha)<br/>    accuracies = []<br/>    for train_index, holdout_index in kf.split(X):<br/>        X_train, X_holdout = X[train_index], X[holdout_index]<br/>        y_train, y_holdout = y[train_index], y[holdout_index]<br/>    <br/>        model.fit(X_train, y_train) # Fit the model<br/>        <br/>        #mse = mean_squared_error(y_holdout, model.predict(X_holdout)) # mse: average of squared error<br/>        sse = sum((y_holdout-model.predict(X_holdout))**2) # sse: sum of squared error<br/>        accuracies.append(sse)<br/>    <br/>    sum_sse = sum(accuracies)<br/>    return sum_sse</span><span id="f0c3" class="lw kb hh ls b fi mb ly l lz ma">keys = []<br/>values = []<br/>dict = {} # Store alpha and corresponding sse into dictionary<br/>for alpha in alphas:<br/>    keys.append(alpha)<br/>    values.append(cv_accuracy_score_lasso(X_train, y_train, alpha))<br/>    <br/>dict['alpha_value'] = keys<br/>dict['sse_value'] = values</span><span id="4e71" class="lw kb hh ls b fi mb ly l lz ma">df_lr = pd.DataFrame(dict) # Convert dictionary into dataframe<br/>best_alpha = df_lr.loc[df_lr['sse_value'].idxmin()] #find the best chosen alpha based on minimun sse</span><span id="0fcf" class="lw kb hh ls b fi mb ly l lz ma">print('The best chosen alpha based on 10-folder cross validation is', best_alpha[0])<br/>print('Corresponding sse is', best_alpha[1])</span></pre><figure class="ll lm ln lo fd ii er es paragraph-image"><div class="er es me"><img src="../Images/a24bad3a9dcd07a5c7f87b8e6bf1e1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*AcT3p9uAXXDctAj9viz6ow.png"/></div></figure><blockquote class="le lf lg"><p id="3800" class="iu iv js iw b ix iy iz ja jb jc jd je lh jg jh ji li jk jl jm lj jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="hh">测试集评估</em> </strong></p></blockquote><p id="1f28" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="js">直接变量选择——MIQP:</em></strong></p><p id="00c1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">找到最佳k后，使用k=10在整个训练集上拟合MIQP模型，并获得截距和系数，如下所示:</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/fb5cbecf004ad9a613989fe254caa703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ula-9Hzch5HzFgfZNj59Vg.png"/></div></div></figure><p id="5aa9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们使用上述截距和从MIQP获得的βj来预测测试集中的y值。测试集上的MIQP上交所是116.8272。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/dd51c5187fa3ed3c9e39242ccd057fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iu0lyVDpKxFwrTORqphPGQ.png"/></div></div></figure><p id="fcba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="js">间接变量选择-LASSO:</em></strong></p><p id="e7b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在找到最佳λ之后，我们使用λ =0.0756在整个训练集上拟合LASSO回归，并获得如下截距和系数。由此可以看出，LASSO选择了17个变量，其中包括MIQP模型选择的10个X变量，但是它们的系数不同。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/5261e691ce624aee2516a31929231496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtJheGPZBPHMVWPT1_JhKQ.png"/></div></div></figure><p id="c985" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，我们使用上述截距和从LASSO获得的βj来预测测试集中的y值。LASSO在测试集上的SSE是117.4688。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/b0a093ea137f6fef1c05f16f710204e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8cyu9pliG903jeyQhmFqg.png"/></div></div></figure><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/dcb91e5034acbe171e1bf3c0b3c7329d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2KfqDh3OCSVSJGq2PvzLA.png"/></div></div></figure><h1 id="e5d1" class="ka kb hh bd kc kd mg kf kg kh mh kj kk kl mi kn ko kp mj kr ks kt mk kv kw kx bi translated"><span class="l ml mm mn bm mo mp mq mr ms di">I</span><strong class="ak"><em class="ky">n-样本比较</em> </strong></h1><p id="f4fc" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">根据样本内sse以及MIQP模型和LASSO回归返回的系数，可以看出:</p><p id="8b83" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(1)总体而言，MIQP模型的样本内sse低于LASSO回归。</p><p id="0c0a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(2)在训练误差与特征个数的关系上，MIQP和拉索呈现出不同的趋势。关于MIQP，随着k减小到10，训练误差(sse)大部分时间都在减小。关于LASSO，随着λ从0.0756增加，训练误差(sse)也增加。</p><p id="6926" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(3)使用LASSO，特征选择的过程更平滑，因为LASSO允许我们逐渐将系数减小到0，而不是直接消除一个变量。</p><p id="9f12" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">(4)利用该数据集，MIQP选取10个变量，训练sse为724.7876，LASSO选取17个变量，训练sse为695.00097。由于训练错误是封闭的，我们可能会认为MIQP用更少的特征达到了同样的效果。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mt"><img src="../Images/70a84458151bb3c928ba365a8522fc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHwNqLuyYRNW8z2gEtNDxA.png"/></div></div></figure><h1 id="33e0" class="ka kb hh bd kc kd mg kf kg kh mh kj kk kl mi kn ko kp mj kr ks kt mk kv kw kx bi translated"><strong class="ak"> <em class="ky">样本外比较</em> </strong></h1><p id="5631" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">根据MIQP模型和LASSO回归返回的样本外上证综指，MIQP略好于LASSO。我们认为这是因为通过缩小β，我们给估计值增加了偏差。此外，我们观察到MIQP用更少的特征获得了更好的性能。这很方便，因为它导致了一个更易解释的模型。</p><figure class="ll lm ln lo fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/ed8a7eb9124d2ac973149dbb8e261c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHrI5DH1Y7JrNj76wP9Lqw.png"/></div></div></figure><p id="3ceb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一方面，正如我们所知，LSAAO不是规模不变的，因为预算约束是基于L1准则的。记住，βj被解释为特性Xj每单位变化的响应变化。由于L1范数取绝对值之和，βj消耗多少预算取决于与其相关的特性的测量单位。</p><p id="cc8c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，MIQP是尺度不变的，不会给权重估计增加偏差。此外，这种方法符合附加线性约束[1]的规范，例如:</p><p id="e8ac" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">加强特征间的组稀疏性。</p><p id="2b16" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">限制成对多重共线性。</p><p id="74ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">限制全球多重共线性。</p><p id="010a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">考虑一组固定的非线性变换。</p><h1 id="3d9b" class="ka kb hh bd kc kd mg kf kg kh mh kj kk kl mi kn ko kp mj kr ks kt mk kv kw kx bi translated"><strong class="ak">结论</strong></h1><p id="9e1f" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">基于以上所述，我们已经展示了如何使用优化来对线性回归问题执行特征选择。由于直接变量选择的计算时间随着更好的解算器的出现而减少，它实际上是套索的一个很好的替代方案，假定MIQP是比例不变的，并且不会给重量估计引入偏差。因此，如果在这样的数据集规模下，解决任务的3小时时间跨度是可以接受的，我们将推荐使用MIQP。换句话说，如果我们有足够的计算能力，或者愿意为了更好的结果而投资计算能力。</p><p id="1223" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与LASSO相比，MIQP更适合高维数据，不容易出现共线性，性能更好，但运行时间更长。即使我们通常的任务有较低维度的较小数据集，我们也建议在遇到较高维度或需要高级性能的任务时合并它。</p><p id="32bd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，如果我们想要迅速的结果，愿意用有限的计算资源来解决体面的解决方案，或者通常用较低维度的数据来处理任务，那么可能就没有必要引入MIQP。尤其是如果合并过程成本太高或耗时太长。由于套索可以在几分钟内解决这样的任务，而MIQP需要几个小时，套索也适合流媒体。</p><p id="873e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">总而言之，我们的建议是“统计学中没有免费的午餐”。也就是说，在所有可能的数据集下，没有算法比其他算法更好。在分析数据集时，我们将考虑多种学习算法。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="d3a0" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">感谢</h1><p id="db73" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">我们要特别感谢丹尼尔·米切尔博士在整个分析过程中的指导。</p><h1 id="1e46" class="ka kb hh bd kc kd mg kf kg kh mh kj kk kl mi kn ko kp mj kr ks kt mk kv kw kx bi translated">参考</h1><p id="5b34" class="pw-post-body-paragraph iu iv hh iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr ha bi translated">[1]资料来源:贝特西马斯博士和金博士(2015年)。或者论坛——线性回归的算法方法。运筹学，64(1)，2–16页。</p><h1 id="a499" class="ka kb hh bd kc kd mg kf kg kh mh kj kk kl mi kn ko kp mj kr ks kt mk kv kw kx bi translated">Github链接</h1><div class="mv mw ez fb mx my"><a href="https://github.com/ping2022/Mixed-Integer-Quadratic-Programming-MIQP-in-Feature-Selection" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">GitHub-ping 2022/混合整数二次规划-MIQP-特征选择:比较…</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">混合整数二次规划(MIQP)和LASSO在特征选择中的比较我们知道，最…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm in my"/></div></div></a></div><div class="mv mw ez fb mx my"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">medium.com</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm in my"/></div></div></a></div></div></div>    
</body>
</html>