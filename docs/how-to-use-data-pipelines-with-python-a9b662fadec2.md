# Tensorflow æ•°æ®ç®¡é“åˆå­¦è€…æŒ‡å—

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/how-to-use-data-pipelines-with-python-a9b662fadec2?source=collection_archive---------0----------------------->

## å¦‚ä½•ä½¿ç”¨ Tensorflow ä¸ºæ–‡æœ¬ã€å›¾åƒå’Œ numpy æ•°ç»„æ•°æ®é›†æ„å»ºæ•°æ®ç®¡é“ï¼Ÿ

![](img/d1ec91eda6d6a5eeb60cf6c45063aca7.png)

Photo by [Jeremy Bishop](https://unsplash.com/@jeremybishop?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

åœ¨æ„å»ºæ¨¡å‹æ—¶ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºé€‚å½“çš„æ ¼å¼éå¸¸é‡è¦ã€‚æ•°æ®åˆ†æä¸­æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€æ˜¯æ•°æ®é¢„å¤„ç†ã€‚æ•°æ®é¢„å¤„ç†å ç”¨äº†æ•°æ®ç§‘å­¦å®¶çš„å¤§éƒ¨åˆ†æ—¶é—´ã€‚è‡ªåŠ¨åŒ–æ•°æ®é¢„å¤„ç†æ­¥éª¤ç®€åŒ–äº†æ•°æ®åˆ†æã€‚æ•°æ®ç®¡é“æ˜¯ä»ä¸åŒæ¥æºè·å–åŸå§‹æ•°æ®å¹¶å°†æ•°æ®ç§»åŠ¨åˆ°ç›®çš„åœ°è¿›è¡ŒåŠ è½½ã€è½¬æ¢å’Œåˆ†æçš„ä¸€ç³»åˆ—æ­¥éª¤ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è®¨è®ºä»¥ä¸‹ä¸»é¢˜

*   ä¸ºæ–‡æœ¬æ•°æ®é›†æ„å»ºæ•°æ®ç®¡é“
*   ä¸ºå½±åƒæ•°æ®é›†æ„å»ºæ•°æ®ç®¡é“
*   ä¸º numpy æ•°ç»„æ•°æ®é›†æ„å»ºæ•°æ®ç®¡é“

è¯·ä¸è¦å¿˜è®°è®¢é˜…[æˆ‘ä»¬çš„ youtube é¢‘é“](https://youtube.com/c/tirendazacademy)ï¼Œåœ¨é‚£é‡Œæˆ‘åˆ›å»ºäº†å…³äºäººå·¥æ™ºèƒ½ã€æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å†…å®¹ã€‚

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

# æ–‡æœ¬æ•°æ®é›†çš„æ•°æ®ç®¡é“

![](img/92eea9e8bcddb602d68519da3500ad8e.png)

Photo by [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

è¦åˆ†ææ–‡æœ¬æ•°æ®ï¼Œæ‚¨éœ€è¦æ­£ç¡®åœ°ç»„ç»‡ç›®å½•ç»“æ„ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¿…é¡»å°†è®­ç»ƒæ–‡æœ¬ç»„ç»‡æˆé˜³æ€§å’Œé˜´æ€§ï¼Œä»¥è¿›è¡Œå¦‚ä¸‹æ–‡æœ¬åˆ†ç±»:

```
main_directory/
....pos 
........pst1.txt 
........pst2.txt 
....neg 
........ngt1.txt 
........ngt2.txt
```

æ‚¨å¯ä»¥ä½¿ç”¨ TensorFlow ä¸­çš„ text_dataset_from_directory æ–¹æ³•ä¸ºæ–‡æœ¬æ•°æ®é›†åˆ›å»ºç®¡é“ã€‚è¿™ä¸ªæ–¹æ³•å°†è¿”å›ä¸€ä¸ª`[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)`ï¼Œå®ƒç»™å‡ºäº†æ¥è‡ªå­ç›®å½• pos å’Œ neg çš„ä¸€æ‰¹æ–‡æœ¬ã€‚è®©æˆ‘ä»¬æƒ³åˆ°äº’è”ç½‘ç”µå½±æ•°æ®åº“(IMDB)æ•°æ®é›†ï¼Œå°†ç”µå½±è¯„è®ºåˆ†ä¸ºæ­£é¢å’Œè´Ÿé¢ã€‚è¯¥æ•°æ®é›†é€šå¸¸ç”¨äºæ•™æˆæ–‡æœ¬åˆ†æï¼Œç”± 25ï¼Œ000 æ¡ç”¨äºè®­ç»ƒçš„é«˜åº¦ææ€§ç”µå½±è¯„è®ºå’Œ 25ï¼Œ000 æ¡ç”¨äºæµ‹è¯•çš„ç”µå½±è¯„è®ºç»„æˆã€‚

## ä¸‹è½½æ•°æ®é›†

é¦–å…ˆï¼Œæˆ‘æ¥å¯¼å…¥åº“ã€‚

```
import io
import os
import re
import shutil
import string
import tensorflow as tf
```

ç°åœ¨ï¼Œæˆ‘å°†åŠ è½½ IMDb æ•°æ®é›†ã€‚å¦‚æœä½ æƒ³ç›´æ¥ä¸‹è½½ï¼Œä½ å¯ä»¥ä½¿ç”¨ get_file æ–¹æ³•ã€‚get_file æ–¹æ³•ä»ä¸€ä¸ª URL ä¸‹è½½ä¸€ä¸ªä¸åœ¨ç¼“å­˜ä¸­çš„æ–‡ä»¶ã€‚ä¸ºæ­¤ï¼Œè®©æˆ‘åˆ›å»ºä¸€ä¸ªåä¸º url çš„å˜é‡ã€‚

```
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä½¿ç”¨ get_file æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
data = tf.keras.utils.get_file(
          "aclImdb_v1.tar.gz", 
          url,
          untar=True, 
          cache_dir='.', 
          cache_subdir='')
```

![](img/e35b5de0fafacd053051ca266de91d71.png)

å› æ­¤ï¼Œæ•°æ®é›†è¢«ä¸‹è½½ï¼Œå¹¶åœ¨å½“å‰ç›®å½•ä¸­åˆ›å»ºäº†ä¸€ä¸ªåä¸º aclImdb çš„ç›®å½•ã€‚æˆ‘å°†åˆ›å»ºä¸€ä¸ªå˜é‡ï¼Œå®ƒä»£è¡¨è¿™ä¸ªæ–‡ä»¶è·¯å¾„ã€‚

```
data_dir = os.path.join(os.path.dirname(data), 'aclImdb')
```

è®©æˆ‘ä»¬çœ‹çœ‹ data_dir ç›®å½•ä¸­ train_dir çš„å†…éƒ¨ã€‚

```
train_dir = os.path.join(data_dir, 'train')
os.listdir(train_dir)
```

![](img/340a1a79b28b3d3f57db0415c38b3d01.png)

å¦‚æ‚¨æ‰€è§ï¼Œæœ‰ä¸€ä¸ªåä¸º unsup çš„ç›®å½•ã€‚æˆ‘ä¸éœ€è¦ unsup ç›®å½•ã€‚è®©æˆ‘åˆ é™¤è¿™ä¸ªç›®å½•ã€‚

```
unused_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(unused_dir)
```

æˆ‘è¦å»çœ‹çœ‹ train_dirã€‚

```
os.listdir(train_dir)
```

![](img/38c91cc1ba4d3d93dd4c389ff9e01a23.png)

å¦‚æ‚¨æ‰€è§ï¼Œunsup å·²ä»æ•°æ®ç›®å½•ä¸­åˆ é™¤ã€‚ç¡®ä¿åªæœ‰ç›®å½•åè¢«ç”¨ä½œæ ‡ç­¾ã€‚è®©æˆ‘çœ‹çœ‹åŸ¹è®­ç›®å½•é‡Œçš„å†…å®¹ã€‚

```
ls aclImdb\train
```

![](img/1edfd45df4037dba92976638a3cf32c4.png)

å¦‚ä½ æ‰€è§ï¼Œæœ‰ pos å’Œ neg ç›®å½•ã€‚

## åˆ›å»ºæ•°æ®ç®¡é“

ç°åœ¨æˆ‘è¦åˆ›å»ºä¸€ä¸ªç®¡é“ã€‚é¦–å…ˆï¼Œè®©æˆ‘åˆ›å»ºä¸€ä¸ª batch_size å’Œç§å­å˜é‡:

```
batch_size = 1024
seed = 123
```

æ‰¹é‡å¤§å°ä½¿ç”¨åœ¨ä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¸­ä½¿ç”¨å¤šå°‘æ ·æœ¬ã€‚æˆ‘æŒ‡å®šäº† seed å‚æ•°æ¥ä»¥ç›¸åŒçš„é¡ºåºä¼ è¾“æ–‡ä»¶ã€‚ç°åœ¨æˆ‘å°†ä½¿ç”¨ test_dataset_from_directory()æ–¹æ³•åˆ›å»ºä¸€ä¸ªæ•°æ®ç®¡é“ã€‚

```
train_ds = tf.keras.preprocessing.text_dataset_from_directory(    
    'aclImdb/train', 
    batch_size = batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
```

![](img/a8d1a32cef49fa7e929652fc83235d82.png)

æ‰€ä»¥æˆ‘å¾ˆå®¹æ˜“åœ°ä½¿ç”¨ç®¡é“åˆ›å»ºäº†è®­ç»ƒæ•°æ®é›†ã€‚ä¸ºäº†å¾®è°ƒè¶…å‚æ•°ï¼Œæˆ‘å°†ä½¿ç”¨ä¸€ä¸ªéªŒè¯æ•°æ®é›†ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªéªŒè¯æ•°æ®é›†ã€‚

```
val_ds = tf.keras.preprocessing.text_dataset_from_directory( 
        'aclImdb/train', 
        batch_size=batch_size,   
        validation_split=0.2, 
        subset='validation', 
        seed=seed)
```

![](img/da32d83dc113764820eaa5347cd5331e.png)

## æ¢ç´¢æ•°æ®é›†

ç°åœ¨æˆ‘å°†æ¢ç©¶è¿™äº›æ–‡ä»¶çš„å†…å®¹ã€‚è®©æˆ‘ä»¬åœ¨ç¬¬ä¸€æ‰¹ä¸­éšæœºé€‰æ‹© 5 è¡Œå¹¶æ‰“å°å‡ºæ¥ã€‚

```
import random
idx = random.sample(range(1, batch_size), 5)
for text_batch, label_batch in train_ds.take(1):
    for i in idx:
        print(label_batch[i].numpy(),      
              text_batch.numpy()[i])
```

![](img/b7d05ac39dcad748b6765c95add13e17.png)![](img/a70877b87fa9f379c63426e56083a6b4.png)

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç®¡é“å¤„ç†æ–‡æœ¬æ•°æ®é›†ã€‚å¦‚æ‚¨æ‰€è§ï¼Œä½¿ç”¨ç®¡é“å¯ä»¥è½»æ¾å®Œæˆæ•°æ®é¢„å¤„ç†ã€‚

# å›¾åƒæ•°æ®é›†çš„æ•°æ®ç®¡é“

![](img/06911694befc4be0a079faf40a4c7be2.png)

Photo by [virginia lackinger](https://unsplash.com/@nowyouknowgini?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

æ‚¨å¯ä»¥å°†ç®¡é“ç”¨äºå›¾åƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶èšåˆæ•°æ®ï¼Œå¯¹æ¯ä¸ªå›¾åƒåº”ç”¨éšæœºæ‰°åŠ¨ï¼Œå¹¶å°†éšæœºé€‰æ‹©çš„å›¾åƒåˆå¹¶åˆ°ä¸€ä¸ªæ‰¹å¤„ç†ä¸­è¿›è¡Œè®­ç»ƒã€‚

ä¾‹å¦‚ï¼Œåœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­æœ‰å¤šä¸ªå›¾åƒã€‚è¯¥æ–‡ä»¶åŒ…æ‹¬ä¸¤åˆ—:ä¸€åˆ—åŒ…å«æ‰€æœ‰æ–‡ä»¶åï¼Œå¦ä¸€åˆ—åŒ…å«æ ‡ç­¾ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­å¤„ç†å›¾åƒæ•°æ®é›†çš„æ•°æ®ç®¡é“ã€‚

ä¸ºäº†å±•ç¤ºå¦‚ä½•ä¸ºå›¾åƒæ•°æ®é›†åˆ›å»ºç®¡é“ï¼Œæˆ‘å°†ä½¿ç”¨[èŠ±å‰æ•°æ®é›†](https://data.mendeley.com/datasets/jxmfrvhpyz/1)ã€‚æˆ‘å°†æ•°æ®é›†ä¸‹è½½åˆ°å·¥ä½œç›®å½•ä¸­çš„ flower_photos æ–‡ä»¶ä¸­ã€‚

## æ¢ç´¢æ•°æ®é›†

è®©æˆ‘å¯¼å…¥åº“ï¼Œè¿™å°†åœ¨æœ¬èŠ‚ä¸­ä½¿ç”¨ã€‚

```
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

è¯·æ³¨æ„ï¼Œæ•°æ®é›†æœ‰ä¸€ä¸ªæ ‡ç­¾æ–‡ä»¶ã€‚æˆ‘å°†ä½¿ç”¨ç†ŠçŒ«å›¾ä¹¦é¦†æŸ¥çœ‹å®ƒçš„å†…å®¹ã€‚

```
traindf=pd.read_csv(
    'flower_photos/all_labels.csv',
    dtype=str)
# Take a look first five fows of the dataset
traindf.head()
```

![](img/9c2f8ad03abed057602681e023e2f472.png)

## åˆ›å»ºæ•°æ®ç®¡é“

è®©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œå°†è¿™äº›å›¾åƒè¾“å…¥å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚ä¸ºäº†æ„å»ºæ¨¡å‹ï¼Œæˆ‘å°†ä½¿ç”¨ TensorFlow Hub ä¸­é¢„å…ˆæ„å»ºçš„ ResNet æ¨¡å‹ã€‚

ç°åœ¨æˆ‘è¦åˆ›å»ºä¸€äº›è¶…å‚æ•°ï¼Œç¨åä¼šç”¨åˆ°ã€‚è¯·æ³¨æ„ï¼ŒResNet æ¨¡å‹æœŸæœ›å›¾åƒçš„åƒç´ å°ºå¯¸ä¸º 224*224ï¼Œæˆ‘éœ€è¦ç¡®å®šæ‰¹é‡å¤§å°ã€‚

```
data_root = 'flower_photos/flowers'
IMAGE_SIZE = (224, 224)
TRAINING_DATA_DIR = str(data_root)
BATCH_SIZE = 32
```

è®©æˆ‘ä»¬æ ‡å‡†åŒ–æ•°æ®é›†ï¼Œå¹¶ä¸ºéªŒè¯æ•°æ®é›†ä¿ç•™ 20%çš„å›¾åƒã€‚è®©æˆ‘ç”¨ä¸€ä¸ªå­—å…¸ç»“æ„ã€‚

```
datagen_kwargs = dict(
    rescale=1./255, 
    validation_split=.20)
```

ç°åœ¨è®©æˆ‘åˆ›å»ºä¸€ä¸ªæ–°å˜é‡ï¼Œå¹¶å°† datagen_kwargs ä¼ é€’åˆ°è¿™ä¸ªå˜é‡ä¸­ã€‚

```
dataflow_kwargs = dict(
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE, 
    interpolation="bilinear")
```

æ‚¨å¯ä»¥ä½¿ç”¨ ImageDataGenerator å°†è¿™äº›å›¾åƒæµå¼ä¼ è¾“åˆ°åŸ¹è®­è¿‡ç¨‹ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†å®šä¹‰ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå¹¶å°† datagen_kwargs ä¼ é€’ç»™è¿™ä¸ªç”Ÿæˆå™¨ã€‚

```
train_datagen =   tf.keras.preprocessing.image.ImageDataGenerator(
**datagen_kwargs)
```

ä¸ºäº†åˆ›å»ºæ•°æ®ç®¡é“ï¼Œæˆ‘å°†ä½¿ç”¨ flow_from_dataframe æ–¹æ³•ã€‚

```
train_generator = train_datagen.flow_from_dataframe(
    dataframe=traindf, 
    directory=data_root, 
    x_col="file_name", 
    y_col="label", 
    subset="training", 
    seed=10, shuffle=True, 
    class_mode="categorical", 
    **dataflow_kwargs)
```

## æ£€æŸ¥æ•°æ®é›†

è®©æˆ‘å±•ç¤ºæ•°æ®é›†ä¸­çš„å›¾åƒã€‚

```
image_batch, label_batch = next(iter(train_generator))
fig, axes = plt.subplots(8, 4, figsize=(20, 40))
axes = axes.flatten()
for img, lbl, ax in zip(image_batch, label_batch, axes):
    ax.imshow(img)
    label_ = np.argmax(lbl)
    label = idx_labels[label_]
    ax.set_title(label)
    ax.axis('off')
    plt.show()
```

![](img/78a94c850e5d95be2d366ddcc6287662.png)

å› æ­¤ï¼Œæ•°æ®æ¥æ”¶ç®¡é“å·²ç»å¯ä»¥ä½¿ç”¨äº†ã€‚è®©æˆ‘ä»¬è®­ç»ƒæ¨¡å‹ã€‚

## è®­ç»ƒæ¨¡å‹

æˆ‘å°†ä½¿ç”¨ ResNet æ¨¡å‹æ¥æ„å»ºæ¨¡å‹ã€‚

```
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(
        input_shape=IMAGE_SIZE + (3,)),
    hub.KerasLayer( 
        "https://tfhub.dev/tensorflow/resnet_50/feature_vector/1", trainable=False),
    tf.keras.layers.Dense(
        5, activation='softmax', 
        name = 'custom_class')])
model.build([None, 224, 224, 3])
```

è®©æˆ‘ä»¬ç¼–è¯‘æ¨¡å‹ã€‚

```
model.compile(
    optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9),       
    loss=tf.keras.losses.CategoricalCrossentropy(
        from_logits=True, label_smoothing=0.1), 
    metrics=['accuracy'])
```

ç°åœ¨æˆ‘è¦è®­ç»ƒæ¨¡å‹äº†ã€‚

```
steps_per_epoch = train_generator.samples // train_generator.batch_sizevalidation_steps = valid_generator.samples // valid_generator.batch_sizemodel.fit(
    train_generator, 
    epochs=13, 
    steps_per_epoch=steps_per_epoch,   
    validation_data=valid_generator, 
    validation_steps=validation_steps)
```

![](img/e1e02c6de3235a9edfe14c803885e4e2.png)

å¦‚æ‚¨æ‰€è§ï¼Œè®­ç»ƒå›¾åƒç”Ÿæˆå™¨å’ŒéªŒè¯å›¾åƒç”Ÿæˆå™¨è¢«ä¼ é€’åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚è®­ç»ƒç²¾åº¦å’ŒéªŒè¯ç²¾åº¦éƒ½å¯ä»¥ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•å¯¹å›¾åƒæ•°æ®é›†ä½¿ç”¨æ•°æ®æ¥æ”¶ç®¡é“ã€‚

# NumPy æ•°ç»„æ•°æ®é›†çš„æ•°æ®ç®¡é“

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»çœ‹åˆ°äº†å¦‚ä½•å°†æ•°æ®ç®¡é“ç”¨äºæ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•ä¸º NumPy æ•°ç»„æ•°æ®é›†åˆ›å»ºæ•°æ®ç®¡é“ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†ä½¿ç”¨ from_tensor_slices æ–¹æ³•ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨æ—¶å°š MNIST æ•°æ®é›†ï¼Œå®ƒç”± 10 ç§ç°åº¦æœè£…ç»„æˆã€‚è¿™äº›å›¾åƒä½¿ç”¨ NumPy ç»“æ„è¡¨ç¤ºï¼Œè€Œä¸æ˜¯å…¸å‹çš„å›¾åƒæ ¼å¼ï¼Œå¦‚ JPEG æˆ– PNGã€‚ä½ å¯ä»¥ç”¨ tf è½»æ¾ä¸‹è½½ã€‚Keras APIã€‚

## åŠ è½½æ•°æ®é›†

é¦–å…ˆï¼Œæˆ‘æ¥å¯¼å…¥åº“ã€‚

```
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
```

è®©æˆ‘ä»¬ä½¿ç”¨ tf.keras API ä¸­çš„ load_data å‡½æ•°åŠ è½½æ•°æ®é›†ã€‚

```
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```

è®©æˆ‘çœ‹çœ‹æ•°æ®é›†çš„ç»“æ„ã€‚

```
print(type(train_images), type(train_labels))
```

![](img/27b181ac94ae0376ee5867437637a543.png)

å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†çš„ç»“æ„æ˜¯ NumPy æ•°ç»„ã€‚ç°åœ¨ï¼Œæˆ‘å°†ä½¿ç”¨ shape å±æ€§æŸ¥çœ‹æ•°æ®é›†çš„å½¢çŠ¶ã€‚

```
print(train_images.shape, train_labels.shape)
```

![](img/696e28d43b14d9e8ecf7173bf7536169.png)

## æ¢ç´¢æ•°æ®é›†

ä¸ºäº†å¯è§†åŒ–ä¸€ä¸ª NumPy æ•°ç»„ï¼Œæˆ‘å°†ä½¿ç”¨ matplotlib åº“ã€‚

```
plt.figure()
plt.imshow(train_images[5])
plt.colorbar()
plt.grid(False)
plt.show()
```

![](img/b010eb57df2f9e8326fdc67bddbe92b2.png)

## é¢„å¤„ç†æ•°æ®é›†

å›¾åƒç”± 0 åˆ° 255 ä¹‹é—´çš„åƒç´ å€¼ç»„æˆã€‚ä¸ºäº†æ›´å¿«åœ°æ„å»ºæ¨¡å‹å¹¶è·å¾—æ›´å¥½çš„å‡†ç¡®æ€§ï¼Œæˆ‘å°†å¯¹åƒç´ å€¼è¿›è¡Œå½’ä¸€åŒ–ã€‚

```
train_images = train_images/255
```

æˆ‘å°†ä½¿ç”¨ from_tensor_slices æ–¹æ³•å»ºç«‹ä¸€ä¸ªæµç®¡é“ã€‚

```
train_dataset = tf.data.Dataset.from_tensor_slices(
        (train_images, train_labels))
```

è®©æˆ‘å°†è¿™ä¸ªæ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚è¶…å‚æ•°ç”¨éªŒè¯æ•°æ®é›†å¾®è°ƒï¼Œæ¨¡å‹ç”¨è®­ç»ƒæ•°æ®é›†å»ºç«‹ã€‚

```
SHUFFLE_BUFFER_SIZE = 10000
TRAIN_BATCH_SIZE = 50
VALIDATION_BATCH_SIZE = 10000#Creating the data pipelines
validation_ds = train_dataset.shuffle( SHUFFLE_BUFFER_SIZE).take( VALIDATION_SAMPLE_SIZE).batch(VALIDATION_BATCH_SIZE)train_ds = train_dataset.skip( VALIDATION_BATCH_SIZE).batch( TRAIN_BATCH_SIZE).repeat()
```

## æ„å»ºæ¨¡å‹

æ•°æ®é›†å·²å‡†å¤‡å¥½æ„å»ºæ¨¡å‹ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘å°†ä½¿ç”¨åºåˆ—æ¨¡å‹ã€‚

```
# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),    
    tf.keras.layers.Dense(30, activation='relu'),   
    tf.keras.layers.Dense(10)# Compiling the model
model.compile(
    optimizer=tf.keras.optimizers.RMSprop(),   
    loss=tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True), 
    metrics=['sparse_categorical_accuracy'])#Trainging the model
model.fit(
    train_ds, 
    epochs=13, 
    steps_per_epoch=steps_per_epoch,    
    validation_data=validation_ds,   
    validation_steps=validation_steps)
```

![](img/40515f9665931305c0431584971bd7ee.png)![](img/5d9e413a873653155c1cbd03568423bf.png)

å› æ­¤ train_ds å’Œ validation_ds è¢«ä¼ é€’åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ from_tensor_slices ä¸ºåŒ…å« NumPy æ•°ç»„çš„æ•°æ®é›†åˆ›å»ºæ•°æ®ç®¡é“ã€‚

# æ‘˜è¦

æ‚¨å¯ä»¥ä½¿ç”¨æ–‡æœ¬ã€å›¾åƒå’Œ Numpy æ•°ç»„æ•°æ®é›†çš„æ•°æ®ç®¡é“è½»æ¾æ„å»ºæ¨¡å‹ã€‚ä½¿ç”¨æ•°æ®ç®¡é“ï¼Œæ‚¨å¯ä»¥ä»å­˜å‚¨æ•°æ®çš„ä½ç½®è®¿é—®æ•°æ®ï¼Œè½¬æ¢ã€ç¼©æ”¾æ•°æ®ï¼Œç„¶åå°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚

åœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°ç¬”è®°æœ¬[ã€‚æ„Ÿè°¢æ‚¨çš„é˜…è¯»ã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢å®ƒã€‚åˆ«å¿˜äº†å…³æ³¨æˆ‘ä»¬çš„](https://github.com/TirendazAcademy/DEEP-LEARNING-WITH-TENSORFLOW/blob/main/08-How%20to%20Use%20Data%20Pipelines%20with%20Python.ipynb)[YouTube](https://youtube.com/c/TirendazAcademy)|[GitHub](https://github.com/tirendazacademy)||[|*Twitter*](https://twitter.com/TirendazAcademy)*|[ka ggle](https://www.kaggle.com/tirendazacademy)|*|*[*LinkedIn*](https://www.linkedin.com/in/tirendaz-academy)*ğŸ‘**

**[](https://levelup.gitconnected.com/7-differences-between-deep-learning-and-machine-learning-b5f2ff0ae00a) [## æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„ 7 ä¸ªåŒºåˆ«

### æ·±åº¦å­¦ä¹ ä¸æœºå™¨å­¦ä¹ â€”â€”æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

levelup.gitconnected.com](https://levelup.gitconnected.com/7-differences-between-deep-learning-and-machine-learning-b5f2ff0ae00a) [](/geekculture/6-steps-to-become-a-machine-learning-expert-5a1f155f7207) [## æˆä¸ºæœºå™¨å­¦ä¹ ä¸“å®¶çš„ 6 ä¸ªæ­¥éª¤

### æˆä¸ºæœºå™¨å­¦ä¹ ä¸“å®¶éœ€è¦çŸ¥é“çš„ä¸€åˆ‡ã€‚

medium.com](/geekculture/6-steps-to-become-a-machine-learning-expert-5a1f155f7207) 

# èµ„æº

*   [TensorFlow 2 è¢–çå‚è€ƒ](https://www.oreilly.com/library/view/tensorflow-2-pocket/9781492089179/)
*   [TensorFlow æ•™ç¨‹](https://www.tensorflow.org/tutorials)

å¦‚æœè¿™ç¯‡æ–‡ç« æœ‰å¸®åŠ©ï¼Œè¯·ç‚¹å‡»æ‹æ‰‹ğŸ‘æŒ‰é’®å‡ ä¸‹ï¼Œä»¥ç¤ºæ”¯æŒğŸ‘‡**