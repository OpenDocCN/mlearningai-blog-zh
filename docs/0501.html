<html>
<head>
<title>Naïve Machine Translation in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的简单机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/na%C3%AFve-machine-translation-in-nlp-13cf02b9400?source=collection_archive---------6-----------------------#2021-05-04">https://medium.com/mlearning-ai/na%C3%AFve-machine-translation-in-nlp-13cf02b9400?source=collection_archive---------6-----------------------#2021-05-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9bb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">天真的机器翻译:</strong></p><p id="b77a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个项目的目的是使用单词嵌入和向量空间模型将英语单词翻译成法语。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/b9c4be564c89ea5a68786f3e2a04ba2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*XpQal-mzVltbTHT8.jpeg"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Image Source: <a class="ae jo" href="https://fiverr-res.cloudinary.com/images/t_main1,q_auto,f_auto,q_auto,f_auto/gigs/151330916/original/c616ef73c76b73204e888c9d8460c2ca9b596964/translate-from-french-to-english.jpeg" rel="noopener ugc nofollow" target="_blank">Fiverr</a></figcaption></figure><p id="d9aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们为一个词汇表训练单词嵌入时，主要的焦点是优化单词嵌入，使得核心含义和单词之间的关系得以保持。这个概念背后的思想是由约翰·鲁珀特·弗斯在20世纪50年代提出的:“你应该从他所交往的人那里知道一个词”——弗斯，J.R. (1957)</p><p id="0b2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它的工作原理是，一个词的语义或意义主要是通过上下文或它与其他词的使用来捕捉的。因此，任何单词的周围单词都有助于理解该单词的意思。</p><p id="c3dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Gensim是Python中的一个主题建模和相似性检索库，它提供对Word2Vec等单词嵌入算法的访问以进行训练，还提供预训练的单词嵌入以供下载。</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="f908" class="ju jv hh jq b fi jw jx l jy jz">from gensim.models import KeyedVectors</span><span id="85ee" class="ju jv hh jq b fi ka jx l jy jz">en_embeddings = KeyedVectors.load_word2vec_format (‘./GoogleNews-vectors-negative300.bin’, binary = True)<br/>fr_embeddings = KeyedVectors.load_word2vec_format(‘./wiki.multi.fr.vec’)</span></pre><p id="ac0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这个项目，我使用了Coursera workspace中提供的嵌入子集，这是一个字典，其中关键字是单词，值是一个300维的数组，包含特定单词的单词嵌入。</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="e174" class="ju jv hh jq b fi jw jx l jy jz">en_embeddings_subset = pickle.load(open(“en_embeddings.p”, “rb”))<br/>fr_embeddings_subset = pickle.load(open(“fr_embeddings.p”, “rb”))</span></pre><p id="0421" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">加载英语到法语数据字典，将英语单词作为关键字映射到法语单词作为值。</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="274e" class="ju jv hh jq b fi jw jx l jy jz">en_fr_train = get_dict(‘en-fr.train.txt’)<br/>en_fr_test = get_dict(‘en-fr.test.txt’)</span></pre><p id="4c94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在训练数据集上训练模型，并在测试集上测试它。</p><p id="522c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">创建一个函数，该函数将英语到法语词典、英语嵌入和法语嵌入作为输入，并将它们转换成2个矩阵X和Y，使得矩阵X在单行中包含每个英语单词的单词嵌入，而矩阵Y在相应行中包含对应法语单词的单词嵌入。</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="608e" class="ju jv hh jq b fi jw jx l jy jz">for en_word, fr_word in en_fr.items():<br/>    #check if the words have an embedding in the given dataset<br/>    if fr_word in french_set and en_word in english_set:<br/>        #Get the english and french embedding of corresponding words and append them to the list<br/>        en_vec = english_vecs[en_word]<br/>        fr_vec = french_vecs[fr_word]<br/>        X.append(en_vec)<br/>        Y.append(fr_vec)<br/>return X,Y</span></pre><p id="3840" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该函数返回用于训练机器翻译器的X_train和Y_train矩阵。</p><p id="b931" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了X和Y矩阵，机器翻译问题可以转化为最小化问题。解决方案归结为找到一个变换矩阵R，当乘以X时，给出一个嵌入F的新单词。然后我们可以计算Y中F的最近邻居，并推荐最相似的单词</p><p id="ac54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们将尝试找到一个变换矩阵R，使得该方程最小化，其中x R是计算的输出，Y是目标</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kb"><img src="../Images/047611ef272eea878394db331adfb75d.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*fPbR8Xr54myBnKIXtRpBiw.png"/></div></figure><p id="3e4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，我们使用矩阵的Frobenoius范数来计算损失函数。Frobenoius范数由下式给出:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kc"><img src="../Images/e75ce1c0c5eb625909b31de58b52c86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*oQvxc851G5JttZwDa0EIyQ.png"/></div></figure><p id="0b8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中I是矩阵a的行，j是矩阵a的列</p><p id="3567" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失函数将是矩阵与其近似值之差的Frobenoius范数的平方，除以训练样本数𝑚</p><ol class=""><li id="2758" class="kd ke hh ig b ih ii il im ip kf it kg ix kh jb ki kj kk kl bi translated">计算损失函数:</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es km"><img src="../Images/65df25a570681b47b794dadf9a99348a.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*ywWvfov_AR4qoXSNfBRgWg.png"/></div></figure><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="a2de" class="ju jv hh jq b fi jw jx l jy jz">loss = np.sum(np. square (np.dot (X, R) — Y))/m</span></pre><p id="f8c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.计算损失函数的梯度:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kn"><img src="../Images/74303d955092b49705272d9275904aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*ojJgMLeHW7Fd_3bfruCw8w.png"/></div></figure><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="9144" class="ju jv hh jq b fi jw jx l jy jz">gradient = np.dot (X.T, (np.dot (X, R) — Y)) *(2 /m)</span></pre><p id="6d5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.用梯度下降算法寻找最优R</p><p id="f820" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">计算损失相对于矩阵R的梯度g，并迭代更新R:</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="e004" class="ju jv hh jq b fi jw jx l jy jz">def tranformation_matrix():<br/>    for i in range(steps):<br/>        gradient = compute_gradient(X, Y, R) <br/>        R = R — learning_rate*gradient<br/>    return R</span></pre><p id="3b30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个函数返回变换矩阵R。</p><p id="2305" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k-NN是一种算法，它将一个向量作为输入，并在数据集中找到与其最接近的其他向量。我们将使用k-NN来获得与来自变换向量XR的嵌入最接近的嵌入。</p><p id="e988" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">余弦相似性是量化两个文档之间相似性的两个向量之间角度的余弦。如果我们把向量的方向看作它的意义，它就能更好地捕捉语义的相似性。此外，向量之间的角度更不受字数等外部因素的影响。</p><p id="326b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用基本的三角函数，</p><p id="4f74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Cos (0) = 1(如果角度为0，向量在同一直线上，方向相同，因此非常相似)</p><p id="f3d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Cos (90) = 0(如果角度为90，则矢量是正交的，因此它们不相似)</p><p id="f439" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">cos(180)=-1(如果角度为180，则矢量完全不同)</p><p id="6b7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以当文件之间的角度θ在0°和90°之间时(0&lt;= Cos(θ) &lt;= 1), we consider the documents to be similar, else dissimilar.</p><p id="69dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Since distance and similarity are not the same, we define distance metric using cosine similarity as</p><p id="e986" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">d = 1 -cos (u, v)</p><p id="3185" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">So we create a nearest neighbor function that takes in the vector, all possible nearest neighbors candidates and number of neighbors as input and gives the indices of top n closest neighbors as output.</p><pre class="jd je jf jg fd jp jq jr js aw jt bi"><span id="d7a5" class="ju jv hh jq b fi jw jx l jy jz">def nearest_neighbor(v, candidates, k):<br/>    for row in candidates:<br/>        cos_similarity = cosine_similarity(row,v)<br/>        similarity_l.append(cos_similarity)<br/>        sorted_ids = np.argsort(similarity_l) <br/>    return sorted_ids[-k:]</span><span id="6488" class="ju jv hh jq b fi ka jx l jy jz">def get_translation(X, Y, R):<br/>    pred = np.dot(X,R)<br/>    for i in range(len(pred)):<br/>        pred_idx = nearest_neighbor(pred[i], Y, 1)<br/>    return pred_idx</span></pre><p id="3e2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">So the closest neighbor returned by this function is the word embedding in French, which can be translated into its corresponding French word using the French embedding dictionary.</p><p id="5f37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">The machine translation accuracy obtained using this model is 56%</p><p id="a547" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Note : This blog is based on the new specialization NLP at <a class="ae jo" href="https://www.deeplearning.ai/" rel="noopener ugc nofollow" target="_blank">https://www.deeplearning.ai/</a></p></div></div>    
</body>
</html>