<html>
<head>
<title>K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/k-nearest-neighbors-a8a1cd5c52ef?source=collection_archive---------4-----------------------#2022-01-30">https://medium.com/mlearning-ai/k-nearest-neighbors-a8a1cd5c52ef?source=collection_archive---------4-----------------------#2022-01-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/fa56284a6dc7df4b36530aed33ce4c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yP3yNw4hSW2ruwR-F9vorg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://keepingupwithdatascience.files.wordpress.com/2022/01/knn.png" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="6a96" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><blockquote class="js jt ju"><p id="385a" class="jv jw jx jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ha bi translated">人以群分——<strong class="jy hi"><em class="hh">威廉调谐器</em> </strong></p></blockquote><p id="f1b4" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">上面的引用完美地总结了我们在这篇文章中将要讨论的算法。<strong class="jy hi"> KNN </strong>代表<strong class="jy hi">K-最近邻</strong>。这是一种简单、易于实现的监督机器学习算法，可用于解决<strong class="jy hi">分类</strong>和<strong class="jy hi">回归</strong>问题。</p><p id="fae4" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">在这篇博文中，我们将讨论K近邻算法的理论。因此，不再赘述，让我们深入KNN算法的理论方面。</p></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="002d" class="iu iv hh bd iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr bi translated">k近邻:理论</h1><figure class="lk ll lm ln fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/0e6db0dc18c49add63b2723d52830a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W3CFchPmDJfqnGb9.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://cdn.elearningindustry.com/wp-content/uploads/2019/08/learning-theories-in-elearning-affect.jpg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="65b2" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">KNN算法是一种<strong class="jy hi">懒惰算法</strong>。我说的懒惰算法是什么意思？惰性算法是一种不从给定的训练集中归纳出简明假设的算法，而是将归纳过程延迟到给出测试实例。<strong class="jy hi">简而言之，算法不会开始归纳，直到对其进行查询</strong>。与懒惰算法相反的是<strong class="jy hi">急切算法</strong>，其中系统试图在系统训练期间构建一个通用的、独立于输入的目标函数。</p><p id="ad6c" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">KNN算法假设<strong class="jy hi">相似的东西</strong>存在于<strong class="jy hi">附近</strong>。例如，如果我们有一个二进制分类任务，其中我们想要将一个测试数据点分类为两个类中的一个，那么K-最近邻算法将找到K个与测试数据点最接近的数据点，并将它分配到K个最接近的数据点集合中最常见的类。下图显示了虹膜数据集上3最近邻分类器的决策边界。</p><figure class="lk ll lm ln fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/ef845cf686e8629126fc9171722e1f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3NuFQiLHH-U7wJ9F.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="2a66" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">KNN算法使用各种<strong class="jy hi">距离函数</strong>来计算数据点之间的接近度。下表列出了其中的一些功能。</p><figure class="lk ll lm ln fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/18547032bf694e69aa47fa3a83a61445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kO-s7ZVz_kHdQ1Lygulr6A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div><div class="ab cl kx ky go kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ha hb hc hd he"><h1 id="5e1d" class="iu iv hh bd iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr bi translated">结论</h1><p id="063d" class="pw-post-body-paragraph jv jw hh jy b jz lq kb kc kd lr kf kg ku ls kj kk kv lt kn ko kw lu kr ks kt ha bi translated">因此，在本文中，我们讨论了机器学习中使用的<strong class="jy hi">基线模型之一</strong>背后的理论概念，称为<strong class="jy hi">K-最近邻</strong>。它可以用来解决回归和分类问题。</p><p id="3cc4" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">我已经在我的网站上写了一篇详细的博文，其中我还用Python编程语言从头实现了<strong class="jy hi"> KNN算法，并使用它解决了一个二进制分类问题。整篇博文和代码可以从我的网站，<a class="ae it" href="https://keepingupwithdatascience.wordpress.com/2022/01/30/knn-in-machine-learning-from-scratch/" rel="noopener ugc nofollow" target="_blank"> <strong class="jy hi">这里</strong> </a>。</strong></p><p id="9ca1" class="pw-post-body-paragraph jv jw hh jy b jz ka kb kc kd ke kf kg ku ki kj kk kv km kn ko kw kq kr ks kt ha bi translated">我希望这篇文章对你有所帮助。更多关于数据科学和机器学习的内容请访问我的<a class="ae it" href="https://keepingupwithdatascience.wordpress.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="jy hi">网站</strong> </a>。大家学习愉快:)</p><div class="lv lw ez fb lx ly"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hi fi z dy md ea eb me ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm in ly"/></div></div></a></div></div></div>    
</body>
</html>