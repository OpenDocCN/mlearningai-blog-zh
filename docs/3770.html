<html>
<head>
<title>Feature selection techniques for data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据的特征选择技术</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/feature-selection-techniques-for-data-57f0eacd8fa8?source=collection_archive---------0-----------------------#2022-10-19">https://medium.com/mlearning-ai/feature-selection-techniques-for-data-57f0eacd8fa8?source=collection_archive---------0-----------------------#2022-10-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="9196" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">启发式和进化特征选择技术</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/d89c84e21198da29a1048befa6523d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZYH6boF6ORVgoKJp"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@lukechesser?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Luke Chesser</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="jn"><p id="d5bb" class="jo jp hh bd jq jr js jt ju jv jw jx dx translated">“数据素养包括阅读、处理、分析和论证数据的能力”——(Jordan Morrow，Qlik) <a class="ae jm" href="http://www.dataliteracynetwork.org/definitions.html" rel="noopener ugc nofollow" target="_blank">数据素养</a></p></blockquote><p id="4672" class="pw-post-body-paragraph jy jz hh ka b kb kc ii kd ke kf il kg kh ki kj kk kl km kn ko kp kq kr ks jx ha bi translated">特征选择的意义是从数据集中选择<strong class="ka hi">最具信息性的特征</strong>。当数据集很大时，很难建立模型。庞大的数据集需要大量的时间和计算能力来工作，它们耗尽了模型。特征选择是这样一种方法，在这种方法中，我们可以只选择重要的或最有贡献的特征来训练，而损失很少或没有损失准确性。很多人误解了特征选择和特征提取的概念。</p><p id="7162" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">两者的基本区别在于，在特性选择中，您使用特性的组合或特性的子集来获得最高的性能/精度。而在特征提取中，我们根据数据集或特征的方差和其他因素，从现有特征中创建一组全新的特征。</p><p id="7be6" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">在黑暗中选择特征是一件非常复杂的事情。今天早上我从我的袜子抽屉里挑了两只袜子！天还是黑的，但这没关系，对吧？毕竟，它们的大小是一样的...一样的？！？大数据时代代表了人口统计学的终结(即，我们的模型不应再基于有限的属性和特征选择并受其影响)</p><p id="2aad" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">功能选择的用途</strong></p><ul class=""><li id="a674" class="ky kz hh ka b kb kt ke ku kh la kl lb kp lc jx ld le lf lg bi translated">为了更快地训练算法</li><li id="94fb" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">提高效率</li><li id="7ad1" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">减少冗余</li><li id="8623" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">减少过度拟合</li><li id="6e99" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">领域理解</li><li id="8cb8" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">模型和数据的简单解释</li></ul><p id="4604" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">我们使用两种方法</strong></p><ul class=""><li id="ebfd" class="ky kz hh ka b kb kt ke ku kh la kl lb kp lc jx ld le lf lg bi translated">ML算法中的启发式方法及其评价</li><li id="428e" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated">包装和过滤方法</li></ul><h1 id="bdab" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated">过滤方法</h1><p id="4269" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">过滤方法使用关于特征的精确排序信息。根据排名排列特征，不需要重复使用机器学习算法。它的准确性比包装器方法稍差，但是您仍然可以在特性和准确性之间进行权衡。</p><h1 id="50a4" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated">过滤方法:<strong class="ak">相关过滤</strong></h1><p id="09a1" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated"><a class="ae jm" href="https://en.wikipedia.org/wiki/Correlation" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">相关性</strong> </a>可以定义为两个实体之间的<strong class="ka hi">统计关系</strong>。在机器学习中，相关性是在两个或多个特征或属性之间检查它们彼此相关的程度。</p><p id="9940" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">如果两个特征中的任何一个高度相关或者两个都携带相同的信息，则其中一个是冗余的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mj"><img src="../Images/f224b252b04a2fc19311f4c286b1a5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tKbv175TSQ69nCdvhKuyJQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Correlation among features [Image by author]</figcaption></figure><p id="c13e" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">在该表中，描述符1和3以及描述符1和4和描述符3和4高度相关。移除差异较小的特征。这里，描述符4携带最小方差，而描述符3携带最大方差。第四个描述符被删除。像这样，我们减少了特征集。</p><p id="1f7e" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">相互信息</strong></p><p id="768f" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">互信息(MI)测量输入和目标变量之间的相关性。交互信息应始终&gt; = 0。MI = 0；即输入变量和目标变量之间没有关系，即输入变量独立于目标变量。更高的值意味着更高的依赖性。</p><p id="b736" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">信息增益</strong></p><p id="d5b1" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">信息增益计算数据集中熵的减少。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mk"><img src="../Images/daada9a5aea770a6a47301b7fbbb6062.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*dpiY_fb5fIsDtX-tsUTBgw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">[Image by author]</figcaption></figure><p id="22c7" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">对于训练数据集，通过评估每个变量的信息增益，并选择最大化信息增益，即最小化熵的变量。最大信息增益，将数据分成组，以便有效分类。在目标变量的背景下评估每个变量的增益</p><p id="5653" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">特征选择的卡方方法:</strong></p><p id="0264" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">计算每个非负特征和类之间的卡方统计。将统计检验应用于分类特征组，以使用它们的频率分布来评估它们之间相关或关联的可能性。测量特征在将数据点分类到一个可能的类别中的辨别能力，量化给定特征和分类类别之间缺乏的相互依赖。χ2值越高，意味着该特征提供的信息越多。</p><h1 id="699f" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated"><strong class="ak">包装方法:正向选择</strong></h1><p id="9908" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">这是一种迭代方法，从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ml"><img src="../Images/73250a2c0806dc5b2eb3ae82362b1811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UMIOeixuClhaOfSGuwuGg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Selection process [Image by author]</figcaption></figure><p id="4e59" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">在这里，我们相信每个描述符的个体准确性。描述符4在与描述符组合时具有最高的准确性。描述符4，1在与描述符组合时具有最高的准确性。描述符4、1和2与描述符组合具有最高的准确性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mm"><img src="../Images/e589a39417b7844a47c87fa75d505ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-aVR-KXOm_L5ALhIZAhSw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">This figure describes how the selection of features changes the accuracy of the model [Image by author]</figcaption></figure><h1 id="741f" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated"><strong class="ak">包装方法:向后选择</strong></h1><p id="32df" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">它从所有特征开始，并在每次迭代中删除最不重要的特征，从而提高模型的性能。我们重复这一过程，直到在特征的移除中没有观察到改进。</p><p id="973c" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">通过一次移除一个要素来计算精度。当我们去掉X5时，我们的精度得到了提高。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mn"><img src="../Images/62cff0f88aa77b8d2f869a127ac58378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kk_dz076FSnP7e2lYqu72A.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">This figure describes how the selection of features changes the accuracy of the model [Image by author]</figcaption></figure><p id="83c6" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">嵌入方法</strong></p><p id="15e7" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">嵌入式方法结合了<strong class="ka hi">过滤器</strong>和<strong class="ka hi">包装器</strong>方法的特性。它是由拥有自己的<strong class="ka hi">内置特征</strong>选择方法的算法实现的。嵌入式方法中的特征选择与学习算法有更深的联系，并且是分类本身的一部分。嵌入式方法耗时较少，也不容易过度拟合。例如<strong class="ka hi">套索</strong>回归和<strong class="ka hi">脊形</strong>回归，它们具有内置的惩罚功能以减少过度拟合。</p><h2 id="4ddd" class="mo ln hh bd lo mp mq mr ls ms mt mu lw kh mv mw ly kl mx my ma kp mz na mc nb bi translated"><strong class="ak">随机森林特征重要性</strong></h2><p id="d072" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">随机森林为特征选择提供了两种方法:</p><p id="883c" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">● <strong class="ka hi">均值减少杂质</strong>:对于分类树，度量是<a class="ae jm" href="https://www.learndatasci.com/glossary/gini-impurity/#:~:text=Gini%20Impurity%20is%20a%20measurement,nodes%20to%20form%20the%20tree." rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">基尼杂质</strong> </a>或信息增益/熵，对于回归树，度量是方差。训练树，计算加权杂质的减少。对于森林，平均每个特征的杂质减少量，并根据该度量对特征进行排序。</p><p id="b09a" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">● <strong class="ka hi">平均降低准确度</strong>:对于森林中生长的每一棵树，找到<a class="ae jm" href="https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/" rel="noopener ugc nofollow" target="_blank"><strong class="ka hi"/></a>【出袋】例题&amp;统计正确例题的投票数。随机排列OBB示例中的要素值，并将这些案例放入树中。从未接触过的OOB数据中的正确类别的投票数中减去可变排列的OOB数据中的正确类别的投票数。森林中所有树木数量的平均值就是变量的原始重要性分数</p><p id="940a" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated"><strong class="ka hi">进化算法</strong> <strong class="ka hi">用于特征选择</strong></p><p id="15a2" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">大自然一直是灵感的源泉。在过去的几十年中，它激发了许多有效的计算工具和算法的发展，以解决具有挑战性的优化问题。进化优化算法用于特征选择。一些例子如下</p><ul class=""><li id="f42c" class="ky kz hh ka b kb kt ke ku kh la kl lb kp lc jx ld le lf lg bi translated"><a class="ae jm" href="https://ieeexplore.ieee.org/abstract/document/558650" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">遗传算法</strong> </a></li><li id="8d5b" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated"><a class="ae jm" href="https://ieeexplore.ieee.org/abstract/document/4129846" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">蚁群优化</strong> </a></li><li id="eb8e" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated"><a class="ae jm" href="https://en.wikipedia.org/wiki/Simulated_annealing" rel="noopener ugc nofollow" target="_blank"><strong class="ka hi"/></a>模拟退火</li><li id="160f" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated"><a class="ae jm" href="https://link.springer.com/article/10.1007/s12065-013-0102-2" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">基于群体的算法</strong> </a></li><li id="295d" class="ky kz hh ka b kb lh ke li kh lj kl lk kp ll jx ld le lf lg bi translated"><a class="ae jm" href="https://www.sciencedirect.com/science/article/abs/pii/S0020025512005762" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">黑洞算法</strong> </a></li></ul><p id="87ad" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">进化算法的最大优势是它们不需要衍生信息。我们可以在全局最优解附近采样解的子集。</p><p id="d722" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">我在著名的<a class="ae jm" href="https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">分析vidya贷款预测</strong> </a>问题上尝试了一种简单的过滤特征选择方法，但最终，结果得到了改善。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nc"><img src="../Images/befe9898214aab73872288a47e01f21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7l5mGMXlKJO8yT2wcieDA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Results of comparison when we use feature selection and when we don’t [Image by author]</figcaption></figure><p id="4c18" class="pw-post-body-paragraph jy jz hh ka b kb kt ii kd ke ku il kg kh kv kj kk kl kw kn ko kp kx kr ks jx ha bi translated">就像这样，你可以用你现有的模型来改进你的结果。有趣的是，由于其有效性，特征选择目前吸引了很多注意力。</p><h1 id="5053" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated">如果你觉得这很有见地</h1><p id="e21f" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">如果你觉得这篇文章很有见地，请关注我的<a class="ae jm" href="https://www.linkedin.com/in/chinmay-bhalerao-6b5284137/" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi"> Linkedin </strong> </a>和<a class="ae jm" rel="noopener" href="/@BH_Chinmay"> <strong class="ka hi"> medium </strong> </a>。你也可以<a class="ae jm" rel="noopener" href="/@BH_Chinmay"> <strong class="ka hi">订阅</strong> </a>在我发表文章的时候得到通知。让我们创建一个社区！感谢您的支持！</p><h1 id="72b6" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated">如果你想支持我:</h1><p id="fbae" class="pw-post-body-paragraph jy jz hh ka b kb me ii kd ke mf il kg kh mg kj kk kl mh kn ko kp mi kr ks jx ha bi translated">因为你的跟随和鼓掌是最重要的事情，但是你也可以通过买咖啡来支持我。<a class="ae jm" href="https://www.buymeacoffee.com/chinmaybhalerao" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi">咖啡</strong> </a> <strong class="ka hi">。</strong></p><h1 id="b78f" class="lm ln hh bd lo lp lq lr ls lt lu lv lw in lx io ly iq lz ir ma it mb iu mc md bi translated">你也可以阅读我的博客</h1><blockquote class="nd ne nf"><p id="9bc7" class="jy jz ng ka b kb kt ii kd ke ku il kg nh kv kj kk ni kw kn ko nj kx kr ks jx ha bi translated"><a class="ae jm" rel="noopener" href="/mlearning-ai/ocr-the-incredible-reading-capability-of-machine-1bc120280ea9"> <strong class="ka hi"> <em class="hh">【光学字符识别】</em> </strong> </a></p><p id="0b5b" class="jy jz ng ka b kb kt ii kd ke ku il kg nh kv kj kk ni kw kn ko nj kx kr ks jx ha bi translated"><a class="ae jm" href="https://pub.towardsai.net/a-chatbot-with-the-least-number-of-lines-of-code-a42e3ba9d974" rel="noopener ugc nofollow" target="_blank"> <strong class="ka hi"> <em class="hh">线数最少的聊天机器人</em> </strong> </a></p><p id="90a7" class="jy jz ng ka b kb kt ii kd ke ku il kg nh kv kj kk ni kw kn ko nj kx kr ks jx ha bi translated"><a class="ae jm" href="https://pub.towardsai.net/an-introduction-to-federated-learning-7bed7dfa34bd" rel="noopener ugc nofollow" target="_blank"><strong class="ka hi"><em class="hh"/></strong></a></p><p id="4a6b" class="jy jz ng ka b kb kt ii kd ke ku il kg nh kv kj kk ni kw kn ko nj kx kr ks jx ha bi translated"><a class="ae jm" rel="noopener" href="/3-minute-thoughts/to-understand-humans-better-cognitive-science-and-ai-40a709d3f891"> <strong class="ka hi"> <em class="hh">认知科学与人工智能</em></strong>T5】</a></p></blockquote><div class="nk nl ez fb nm nn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hi fi z dy ns ea eb nt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jg nn"/></div></div></a></div></div></div>    
</body>
</html>