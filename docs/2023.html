<html>
<head>
<title>An Intro to Machine Learning — Simple Linear Regression and Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习介绍—简单线性回归和梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/gradient-descent-in-machine-learning-linear-regression-e94570f505e6?source=collection_archive---------6-----------------------#2022-02-23">https://medium.com/mlearning-ai/gradient-descent-in-machine-learning-linear-regression-e94570f505e6?source=collection_archive---------6-----------------------#2022-02-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2ad6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归是每个想学习机器学习的人的第一步。或者任何想成为有科学品味的算命先生的人。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/6db3641111deff4bdd0c2bff52e3ff85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBg4kJDmRgcRNbV4w4YftQ.jpeg"/></div></div></figure><h2 id="8585" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated">简单线性回归</h2><p id="acc6" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">也许你会问自己:为什么回归在机器学习中如此重要？你会经常发现自己处于这样一种情况，你不得不根据一组过去的信息来预测一些事情。这就是回归算法试图填补的地方，帮助你预测一些事情。嗯，这听起来很统计，但机器学习的主要任务是创建一个模型，从过去的输入预测未来的输出。你可以借用不同领域的任何概念来建立一个机器学习模型。好吧，我们开始吧。</p><p id="b7ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简单线性回归是一种线性模型，它假设输入和输出之间存在线性关系。为了澄清上面的陈述，让我们看看这些等式</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ko"><img src="../Images/27db11ab1d5fd327a79422b532ceb98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*h3biKQ_EMl5jXg2vaCiykQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Simple Linear Regression</figcaption></figure><p id="3074" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的方程叫做简单线性方程。之所以称之为简单，是因为因变量y的输出只是一个自变量x的函数，而<strong class="ig hi"> m </strong>和<strong class="ig hi"> b </strong>是常数，俗称斜率和截距。在机器学习术语中，y称为标签，x称为特征。当您将一个值作为特征x传递时，标签y将产生一个与x成线性比例的值，它将由x的斜率<strong class="ig hi"> m </strong>倍加上截距<strong class="ig hi"> b </strong>来加权。如果函数中没有输入任何特征x(x等于零)，可以将截距b视为y的默认值。如果你提高x，y会线性提高，反之，如果你降低x，y会线性降低。</p><p id="eed2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设你有这样一个数据集</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kt"><img src="../Images/40b7fb5861c94eba665a7d7c86aba881.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*8SzaAYk7FMe3BRgViC5KIw.jpeg"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">image by Author</figcaption></figure><p id="1a54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个测试分数数据集。从这个数据集中，你可以假设花在学习上的时间和考试成绩之间存在线性关系，这表明花在学习上的时间越多，考试成绩越高。这里有一个散点图，让你的假设有效。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ku"><img src="../Images/a56448f76d8fbabeaa5d4043ddc1afdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*fUFymDBoX00bK8P8h1HBxQ.png"/></div></figure><p id="e2fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能想知道，如果一个学生花了3.5个小时学习，或者10个小时学习，考试分数是多少？这些都是问题的类型，简单的线性回归已经准备好回答…或预测。简单来说，简单的线性回归任务是:</p><blockquote class="kv kw kx"><p id="2c99" class="ie if ky ig b ih ii ij ik il im in io kz iq ir is la iu iv iw lb iy iz ja jb ha bi translated"><strong class="ig hi">简单线性回归</strong>试图通过从<strong class="ig hi">已知特征x的和已知标签y的</strong>中找到最优<strong class="ig hi">斜率m和截距b </strong>来使数据集符合<strong class="ig hi">简单线性方程</strong>。</p></blockquote><p id="e12c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在确定斜率m和截距b的最佳值之后，作为特征xi的函数的所有数据集yi的预测标签为</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/7a2981def1294e99c32f0aae49a3d282.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*DiTxPHISQX_m1_-8DUmINA.png"/></div></figure><p id="201d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，将回答学生在学习<strong class="ig hi"> <em class="ky"> x </em> </strong>的任意小时数的考试分数<strong class="ig hi"> <em class="ky"> y </em> </strong>是多少的问题。</p><h2 id="66ce" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated">梯度下降</h2><p id="8cce" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">要找到斜率m和截距b的最佳值，您需要了解梯度下降。梯度下降是从<strong class="ig hi">成本函数</strong>中衍生出来的一种优化技术。成本函数有多种类型，您将使用的是这个函数:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/a57d758264f0553294bb6f4316d8fb99.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*lAP2Ry5r_Xl06vAjlTxsFQ.png"/></div></figure><p id="43e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">哪里；</p><p id="db48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">j =成本函数</p><p id="4323" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w =重量</p><p id="c711" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b =偏差</p><p id="3bb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y =实际值(已知标签)</p><p id="0f9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y(hat)=预测值(预测标签)</p><p id="32c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是已知标签(y)和预测标签(y hat)之差的平方的平均值，通常称为<strong class="ig hi">均方误差— MSE。</strong></p><p id="e888" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度下降的整个思想是最小化成本函数，使已知标签和预测标签之间的总差的平方尽可能小。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es le"><img src="../Images/3261cd0d993540a48da243bc3b8162e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykRTMpIdFqmyvTY6aEVoVw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Image by <a class="ae lf" href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/" rel="noopener ugc nofollow" target="_blank">Sebastian Raschka</a></figcaption></figure><p id="00ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">什么时候成本函数处于最小值？从上图来看，代价函数的最小值是谷底的一个点。在这一点上，赋予成本函数最小值的权重是最佳值。你怎么能确定在那个成本函数中只有一个谷，就是当前值是最最小值？你选择的成本函数是二次函数，每个二次函数只有一个谷。</p><p id="04fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在微积分中，函数的最小值是其梯度等于零的地方，或者在几何意义上，是曲线中斜率线最平坦的梯度(见上图)。因此，要计算最佳权重和偏差，您需要一个与权重和偏差相关的成本函数梯度:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lg"><img src="../Images/6373ff14b4925119d6754fd9200f2805.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*SxRnFBaa3rXcSjfhS2b93A.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/efc3b9abe07d432c3d082966f6aa264d.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*4pUTHviUXg072sxqHzm0lA.png"/></div></figure><p id="afb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于简单线性方程</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/45499bcc2b0dc54161cb9c0545ad108c.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*lINwzHlt0WBAlD1GbWhPpg.png"/></div></figure><p id="8c11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在您已经找到了成本函数的梯度，要达到成本函数的最小值，您必须从初始值开始调整权重和偏差，直到达到产生成本函数最小值的最佳值。这意味着你需要反复改变权重和偏差的值。但是体重和偏倚的变化有多大呢？你有梯度，但是梯度是如何与体重和偏见联系起来的？</p><p id="95f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">梯度也称为变化率。因此，成本函数相对于权重的梯度是成本函数变化多少的度量。如果权重改变，相同的原理也适用于偏差。因此，通过使用成本函数梯度作为更新步骤来近似权重和偏差更新值:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lj"><img src="../Images/7a2e3261bfb6c96dd0fbc26b516a2960.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*tZmkhjOUrRNUkxEHGD7pQg.png"/></div></figure><p id="aab9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在你可能会想，这些近似会收敛吗？换句话说，在一定的迭代次数之前，权重和偏差会被发现吗？</p><p id="adad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">成本函数的梯度在接近最小值时趋于平缓，这意味着梯度的方向总是朝向曲线的谷，因此我们逐步迭代w和b到梯度的方向。如果步长太大，您将错过最佳权重，但如果太小，您将需要很长时间来迭代。为此，您需要将学习率添加到近似值中。学习率是一个超参数，它决定梯度的步长将在多大程度上改变下一个权重和偏差。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lk"><img src="../Images/f77472863c63275c982d3067ef8fb352.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*ImJM8FJtOiNazQkEW-QHEw.png"/></div></figure><p id="5a0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中α是学习率。</p><h2 id="8dc9" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated">履行</h2><p id="bb46" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">现在，让我们实现简单线性回归的梯度下降，首先创建类模板，然后您将逐步完成这个模板</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="a02b" class="jo jp hh lm b fi lq lr l ls lt">import numpy as np</span><span id="3839" class="jo jp hh lm b fi lu lr l ls lt">class LinearRegression:</span><span id="5770" class="jo jp hh lm b fi lu lr l ls lt">    def __init__(self):<br/>        pass</span><span id="5150" class="jo jp hh lm b fi lu lr l ls lt">    def fit(self,X,y):<br/>        pass</span><span id="7c53" class="jo jp hh lm b fi lu lr l ls lt">    def predict(self,X):<br/>        pass</span></pre><p id="0fcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于构造器部分，您需要将学习率和迭代次数作为类的属性来传递。</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="f8c5" class="jo jp hh lm b fi lq lr l ls lt">## Set the learning rate and number of iterations<br/>def __init__(self,learning_rate=0.01,iter=1000):<br/>    self.learning_rate=learning_rate<br/>    self.iter=iter</span></pre><p id="2ecc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来是拟合方法，这是使用这些步骤实现梯度下降的地方(参见下面代码的注释):</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="6c13" class="jo jp hh lm b fi lq lr l ls lt">def fit(self,X,y):<br/>    #set the initial value for weights and bias<br/>    self.weights=0<br/>    self.bias=0<br/>    #get number of samples features X<br/>    n_samples=length(X)<br/>    #start the iteration<br/>    for i in range(self.iter):<br/>        #predict the known label y<br/>        y_predict=self.weights*X + self.bias<br/>        #calculate the gradients of weights and bias<br/>        grad_w=(2/n_samples)*np.sum(X*(y_predict-y))<br/>        grad_b=(2/n_samples)*np.sum(y_predict-y)<br/>        #update the weights and bias<br/>        self.weight-=grad_w*self.learning_rate<br/>        self.bias-=grad_b*self.learning_rate</span></pre><p id="64db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在对于预测方法，只需使用简单的线性方程计算新的标签</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="42de" class="jo jp hh lm b fi lq lr l ls lt">def predict(self,X):<br/>    return self.weights*X + self.bias</span></pre><p id="b933" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们试着预测一些事情，你将使用考试分数数据。您可以从文件中加载数据或直接写下数据，因为它的要素和标注数量很少</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="57bb" class="jo jp hh lm b fi lq lr l ls lt">X=np.array([1.,2.,3.,4.,5.,6.,7.,8.,9.])<br/>y=np.array([40,43,55,60,68,77,82,86,94])</span></pre><p id="d740" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实例化并拟合数据集</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="3fb1" class="jo jp hh lm b fi lq lr l ls lt">linreg=LinearRegression()<br/>linreg.fit(X,y)</span></pre><p id="d04e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设置新功能并预测</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="dcab" class="jo jp hh lm b fi lq lr l ls lt">new_hours=np.array([1.5,2.3,3.2,4.3,5.1,6.4,7.5,8.2])<br/>predict=linreg.predict(new_hours)</span></pre><p id="64d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看你的模型有多好，定义函数来计算成本函数(MSE)</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="88b5" class="jo jp hh lm b fi lq lr l ls lt">def mean_squared_error(y_true, y_pred):</span><span id="e177" class="jo jp hh lm b fi lu lr l ls lt">    return np.mean((y_true - y_pred) ** 2)</span><span id="ebe9" class="jo jp hh lm b fi lu lr l ls lt">existing=c.predict(X)<br/>mse=mean_squared_error(y,existing)<br/>print(mse)</span></pre><p id="6cf3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">结果是3.30左右。那很好。为了看看你的预测有多接近，让我们把它画出来</p><pre class="jd je jf jg fd ll lm ln lo aw lp bi"><span id="efad" class="jo jp hh lm b fi lq lr l ls lt">import matplotlib.pyplot as plt<br/>plt.scatter(X,y)<br/>plt.scatter(new_hours,predict)<br/>plt.plot(X,existing)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/6d4ad456daf7f7b1f4b1a370e09a313e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*JKenlrNzTYoRXJMaM0nRFw.png"/></div></figure><p id="73f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">蓝色点是从数据集绘制的数据，橙色点是新要素的标注，线是回归线。这是完整的代码</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h2 id="9f80" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated">结论</h2><p id="d689" class="pw-post-body-paragraph ie if hh ig b ih kj ij ik il kk in io ip kl ir is it km iv iw ix kn iz ja jb ha bi translated">在本文中，您已经了解了</p><ol class=""><li id="a12a" class="ly lz hh ig b ih ii il im ip ma it mb ix mc jb md me mf mg bi translated">线性回归在机器学习中的重要性</li><li id="496b" class="ly lz hh ig b ih mh il mi ip mj it mk ix ml jb md me mf mg bi translated">梯度下降的基本概念</li><li id="9af9" class="ly lz hh ig b ih mh il mi ip mj it mk ix ml jb md me mf mg bi translated">用Python实现</li></ol><p id="cc2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请分享这个帖子，如果你喜欢就给它一个掌声。对于多元线性回归你可以查看这个帖子:</p><div class="mm mn ez fb mo mp"><a rel="noopener follow" target="_blank" href="/@gilsatpray/multiple-linear-regression-and-gradient-descent-using-python-b931a2d8fb24"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">使用Python进行多元线性回归和梯度下降</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">这篇文章将解释多元线性回归及其在Python中的实现。</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd jm mp"/></div></div></a></div><div class="mm mn ez fb mo mp"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hi fi z dy mu ea eb mv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd jm mp"/></div></div></a></div></div></div>    
</body>
</html>