<html>
<head>
<title>Saying ‘Hello World’ to Deep Learning Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向深度学习第1部分问好</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/saying-hello-world-to-deep-learning-part-1-cb6ac50c5768?source=collection_archive---------0-----------------------#2022-12-31">https://medium.com/mlearning-ai/saying-hello-world-to-deep-learning-part-1-cb6ac50c5768?source=collection_archive---------0-----------------------#2022-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2f4b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">在本文的第一部分，我们熟悉了MNIST数据集和PyTorch。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es iw"><img src="../Images/57defb405d90af641f14b619b4e98508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*iCKoIYuUnumx5kvG.png"/></div><figcaption class="je jf et er es jg jh bd b be z dx">The MNIST Dataset</figcaption></figure><h1 id="9a51" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">那么什么是MNIST数据集呢？</h1><p id="4d18" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">MNIST数据集是由数字0-9的黑白图像组成的数据集。在我们使用的版本中，我们有70000张图片，我们的首要任务是使用PyTorch对每张图片进行正确分类。</p><h2 id="2513" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">加载数据</h2><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="acec" class="lp jj hh ll b be lq lr l ls lt">from sklearn import datasets<br/><br/>data = datasets.fetch_openml(<br/>    'mnist_784',<br/>    version = 1,<br/>    return_X_y = True<br/>)<br/>pixel_values, targets = data</span></pre><p id="e304" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">这个代码块获取MNIST数据。<em class="lz"> pixel_values </em>变量是每行存储的70000行<em class="lz"> 28x28 = 784 </em>像素值的数据帧。<em class="lz">目标</em>保存每一行即每一幅图像的标签。</p><p id="ffde" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">作为分类问题，我们来检查一下阶级不平衡。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="510d" class="lp jj hh ll b be lq lr l ls lt">targets.value_counts().plot.bar()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ma"><img src="../Images/2eabb8dc2692b316540a47109170cc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*BxHitKiPxChv1trEABhehQ.png"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Number of instances of each class</figcaption></figure><p id="0fd7" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">似乎没问题！继续前进，</p><p id="f32c" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">你可能已经注意到图像往往(大部分)是二维的。让我们尝试将代表每个图像的784像素的行转换为28×28像素。这些代码让我们可以对一张图片进行这样的操作，并将其可视化。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="7cec" class="lp jj hh ll b be lq lr l ls lt">import matplotlib.pyplot as plt<br/><br/>single_image = pixel_values.loc[1,:].values.reshape(28,28)<br/><br/>plt.imshow(single_image,cmap = 'gray')</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mb"><img src="../Images/9e35384a14bd6efdcb7b114156113fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*t57myvtfJEwkjVP2cThItw.png"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Image from the MNIST dataset</figcaption></figure><h2 id="cda8" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">PyTorch之前有:降维+ KNN</h2><p id="3c80" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">技术上来说，在神经网络出现之前有SVM。但是，这里有一种可能的方法可以解决我们的分类任务，甚至不涉及神经网络和PyTorch。首先，让我们尽可能地将28x28像素缩小为一个良好的可绘制(x，y)格式。这是机器学习中的经典范例，有多种方法可以实现。为了更好地理解，我推荐这篇文章:</p><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/practical-guide-to-dimesnioality-reduction-in-python-9da6c84ad8ee"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">2022年你应该知道的9种降维方法</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">降低数据维数所需的一切</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt jc mf"/></div></div></a></div><p id="6de2" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">现在，让我们使用<strong class="kc hi"> <em class="lz"> T分布随机邻居嵌入</em> </strong>方法。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="db68" class="lp jj hh ll b be lq lr l ls lt">from sklearn import manifold<br/>import pandas as pd<br/>import numpy as np<br/><br/>tsne = manifold.TSNE(n_components = 2, random_state = 42)<br/>reduced_pixels = tsne.fit_transform(pixel_values.iloc[:3500,:])<br/><br/>tsne_df = pd.DataFrame(<br/>    np.column_stack((reduced_pixels,targets[:3500].astype(int))),<br/>    columns = ['x','y','targets']<br/>)<br/><br/>tsne_df.head(5)</span></pre><p id="2c45" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">n_components是我们要将行转换成的组件数，在本例中为2。对于此任务，我们将使用前3500个值。要绘制数据:</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="ccb2" class="lp jj hh ll b be lq lr l ls lt">import seaborn as sns<br/><br/>grid = sns.FacetGrid(tsne_df,hue = 'targets',size = 8)<br/><br/>grid.map(plt.scatter,'x','y').add_legend()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mu"><img src="../Images/69ba751b434a384bc1a8535614dd2588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*TJh_YgHncCvQqR99dqOElQ.png"/></div><figcaption class="je jf et er es jg jh bd b be z dx">Distribution of</figcaption></figure><p id="0101" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">我们可以简单地对减少的数据运行K最近邻算法。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="ee83" class="lp jj hh ll b be lq lr l ls lt">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split<br/><br/>X = tsne_df.drop('targets',axis = 1)<br/>y = tsne_df['targets']<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.33, random_state=42)<br/><br/>neigh = KNeighborsClassifier(n_neighbors=10)<br/>neigh.fit(X_train, y_train)<br/><br/>print(f'The accuracy of the KNN approach is {neigh.score(X_test, y_test)}')</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="0933" class="lp jj hh ll b be lq lr l mw lt">The accuracy of the KNN approach is 0.8900432900432901</span></pre><p id="2bea" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">哇哦。那不算太寒酸。这应该描述两件事:</p><ol class=""><li id="49f2" class="mx my hh kc b kd lu kg lv kj mz kn na kr nb kv nc nd ne nf bi translated">MNIST数据集是一个简单的数据集。实际上，使用简单的工具就足够了。</li><li id="1a60" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">像素(也称为我们的“数据”)和我们赋予它的标签之间有一种内在的联系。那很好。数据和标签之间的关系很好。</li></ol></div><div class="ab cl nl nm go nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ha hb hc hd he"><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="er es ns"><img src="../Images/9db8ac134929a836dcdf6c76f994ae1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*teyu_Dt5h6D8IVgb"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">Photo by <a class="ae nx" href="https://unsplash.com/@ilyapavlov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ilya Pavlov</a> on <a class="ae nx" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="20da" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">PyTorch和Tensors</h1><p id="0aa3" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated"><a class="ae nx" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>是一个灵活的科学计算包，目标是基于梯度的深度学习。它的底层API紧跟<a class="ae nx" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>。</p><p id="014b" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">PyTorch有它自己的数据类型，它称之为'<strong class="kc hi"> <em class="lz">张量</em> </strong>'。它们本质上是<strong class="kc hi"><em class="lz">numpy ndarrays</em></strong>，是GPU友好的。</p><h2 id="ac92" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">(GPU？什么-</h2><p id="235f" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">在执行神经网络所需的特定类型的操作(矩阵乘法)时，GPU(图形处理单元)通常比CPU快得多。你的电脑上可能有。即使你没有，Kaggle笔记本和Google Colab也提供免费的GPU使用。)</p><p id="55c5" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">现在，回到火炬。如果您熟悉Numpy ndarrays，那么这段代码对您来说会非常熟悉:</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="67d7" class="lp jj hh ll b be lq lr l ls lt">import torch<br/><br/># Construct a bunch of ones<br/>some_ones = torch.ones(2, 2)<br/>print(some_ones)<br/><br/># Construct a bunch of zeros<br/>some_zeros = torch.zeros(3, 2)<br/>print(some_zeros)<br/><br/># Construct some normally distributed values<br/>some_normals = torch.randn(2, 2)<br/>print(some_normals)<br/><br/>#Get information about the tensors<br/>print()<br/>print(f'Dimensions of the tensor: {some_normals.shape}')<br/>print(f"Datatype of tensor: {some_normals.dtype}")</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="2baf" class="lp jj hh ll b be lq lr l ls lt">tensor([[1., 1.],<br/>        [1., 1.]])<br/>tensor([[0., 0.],<br/>        [0., 0.],<br/>        [0., 0.]])<br/>tensor([[-0.1676, -2.0426],<br/>        [ 0.4518,  1.6082]])<br/><br/>Dimensions of the tensor: torch.Size([2, 2])<br/>Datatype of tensor: torch.float32</span></pre><p id="2056" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated"><em class="lz">张量_形状</em>给出了张量的维数，在这里，<em class="lz">火炬。Size([2，2]) </em> <strong class="kc hi"> <em class="lz"> </em> </strong>暗示我们有一个2x2矩阵。类似地，<em class="lz"> tensor_name.dtype </em>给出了<em class="lz"> tensor_name </em>中对象的数据类型。张量和n数组是如此的相似，你可以把张量转换成n数组，几乎没有任何成本。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="0801" class="lp jj hh ll b be lq lr l ls lt">torch_tensor = torch.randn(5, 5) #Create a 5x5 tensor<br/>numpy_ndarray = torch_tensor.numpy()<br/>print(f'Type: {type(numpy_ndarray)}')<br/>back_to_torch = torch.from_numpy(numpy_ndarray)<br/>print(f'Type: {type(back_to_torch)}')</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="ff69" class="lp jj hh ll b be lq lr l ls lt">Type: &lt;class 'numpy.ndarray'&gt;<br/>Type: &lt;class 'torch.Tensor'&gt;</span></pre><h2 id="e078" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">张量的基本运算</h2><p id="50f8" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">在张量上可以执行五种基本操作。</p><ol class=""><li id="037c" class="mx my hh kc b kd lu kg lv kj mz kn na kr nb kv nc nd ne nf bi translated">添加</li><li id="74eb" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">减法</li><li id="a3a4" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">逐元素乘法</li><li id="2319" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">分开</li><li id="58e5" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">矩阵乘法</li></ol><p id="47a8" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">我希望这里没有惊喜。这就是你实现它们的方式</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="3b4a" class="lp jj hh ll b be lq lr l ls lt">a = torch.Tensor([2,2])<br/>print(f'Tensor 1: {a}')<br/><br/>b = torch.Tensor([3,3])<br/>print(f'Tensor 2: {b}')<br/><br/>c = a+b<br/>print(f'Addition of two tensors: {c}')<br/><br/>c = a-b<br/>print(f'Subtraction of two tensors: {c}')<br/><br/>c = a*b<br/>print(f'Element wise Multiplication of two tensors: {c}')<br/><br/>c = a/b<br/>print(f'Division of two tensors: {c}')<br/><br/>c = a@b<br/>print(f'Matrix multiplication of two tensors: {c}')</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="76eb" class="lp jj hh ll b be lq lr l ls lt">Tensor 1: tensor([2., 2.])<br/>Tensor 2: tensor([3., 3.])<br/>Addition of two tensors: tensor([5., 5.])<br/>Subtraction of two tensors: tensor([-1., -1.])<br/>Element wise Multiplication of two tensors: tensor([6., 6.])<br/>Division of two tensors: tensor([0.6667, 0.6667])<br/>Matrix multiplication of two tensors: 12.0</span></pre><h2 id="3415" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">矩阵乘法:很重要</h2><p id="d242" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated"><em class="lz">(重要到可以普及使用GPU来做的更快。)</em></p><p id="eb7b" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">如果你不确定两个矩阵相乘的概念，现在是复习的时候了。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="2f61" class="lp jj hh ll b be lq lr l ls lt">a = torch.randn(5, 5)<br/>b = torch.randn(5, 5)<br/><br/># Getting a whole row or column or range<br/>first_row = a[0, :]<br/>first_column = a[:, 0]<br/>combo = a[2:4, 2:4]<br/>print(combo.shape)<br/><br/># Matrix multiplication: c_ik = a_ij * b_jk<br/>c = a.mm(b)<br/><br/># Matrix vector multiplication<br/>c = a.matmul(b[:, 0])</span></pre><p id="6c84" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">在这个代码块中，我们首先声明两个5x5矩阵。<em class="lz"> Indexing [0，:] </em>给出了矩阵的第一行。建议使用<strong class="kc hi"> <em class="lz"> a.mm(b) </em> </strong>代替<strong class="kc hi"> <em class="lz"> a@b </em> </strong>。</p><h2 id="ba16" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">重塑尺寸</h2><p id="49b2" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">在本文的开始，我们使用shape(28，28)来改变数组的维数。对张量进行类似的操作:</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="2d0c" class="lp jj hh ll b be lq lr l ls lt">a = torch.rand(10)<br/><br/>print(a.shape)<br/><br/>print(a.unsqueeze(-1).shape) #Add an extra dimension<br/>print(a.unsqueeze_(-1).shape) #Notice the _ (underscore)?<br/>print(a.unsqueeze_(0).shape)<br/><br/>print(a.squeeze(0).shape)<br/>print(a.squeeze().shape)#Remove the extra dimensions</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="843e" class="lp jj hh ll b be lq lr l ls lt">torch.Size([10])<br/>torch.Size([10, 1])<br/>torch.Size([10, 1])<br/>torch.Size([1, 10, 1])<br/>torch.Size([10, 1])<br/>torch.Size([10])</span></pre><p id="6c57" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">有几点需要注意:</p><ol class=""><li id="6724" class="mx my hh kc b kd lu kg lv kj mz kn na kr nb kv nc nd ne nf bi translated">_(下划线)是一个原位运算符。使用<em class="lz"> a.squeeze_() </em>和使用<em class="lz"> a = a.squeeze()是一样的。</em>这个操作符不是这两个函数独有的。</li><li id="b040" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">使用<em class="lz"> a.squeeze() </em>删除所有数值为1的尺寸。</li><li id="f470" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">也可以用<em class="lz">。reshape() </em>如之前张量上所示。</li></ol><p id="5ce4" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">现在是张量特有的东西(关于时间)</p><h2 id="30dd" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">使用图形处理器进行张量计算；</h2><p id="a05f" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">GPU提高了矩阵运算的速度。但是要编写能够在GPU上工作的代码有点困难。幸运的是，代码已经为我们写好了，我们需要做的就是知道如何使用它。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="4763" class="lp jj hh ll b be lq lr l ls lt">import torch<br/><br/>a = torch.randn(2)<br/>do_i_have_cuda = torch.cuda.is_available() #1.<br/><br/>if do_i_have_cuda:<br/>  print('Using GPU')<br/>  #We are 'sending' the tensor a to the GPU<br/>  device = torch.device('cuda') #2.<br/>  a = a.to(device) #3.<br/>else:<br/>  print('Using CPU')<br/>  device = torch.device('cpu')<br/>  a = a.to(device)</span></pre><p id="3a3a" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">在这里，我们是:</p><ol class=""><li id="4e0f" class="mx my hh kc b kd lu kg lv kj mz kn na kr nb kv nc nd ne nf bi translated">检查我们是否有GPU能力</li><li id="2efb" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">相应地设置我们的设备</li><li id="60cf" class="mx my hh kc b kd ng kg nh kj ni kn nj kr nk kv nc nd ne nf bi translated">将张量“a”加载到相应的设备上，以便我们可以对其进行计算</li></ol><h2 id="5d74" class="kw jj hh bd jk kx ky kz jo la lb lc js kj ld le ju kn lf lg jw kr lh li jy lj bi translated">差异化:</h2><p id="f99c" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">如果你上过微积分课程，你可能还记得微分技术，以及它在求函数最小值中的应用。我们将在训练神经网络时，要求找到损失函数的<em class="lz">最小值，</em>换句话说，<em class="lz"> </em>找到使我们的神经网络出错次数最小化的正确数字。这使得寻找梯度的能力变得非常方便。如果你还不明白这是什么意思，把微分想象成我们可以对张量进行的另一种运算。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="er es ny"><img src="../Images/a04245a468493e5f3ca147a0b216404b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xo8vYGX20sk3PFET"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx">Andrew Ng has a plethora of great course on Deep Learning you can view on Coursera.</figcaption></figure><p id="ceb8" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">就代码而言，我们首先创建一个张量，我们可以对其执行梯度(客户端操作):</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="6e11" class="lp jj hh ll b be lq lr l ls lt">import torch<br/><br/>x = torch.randn(1, requires_grad=True)<br/>print(f'The Tensor object: {x}')<br/>print(f'Current gradient stored in x: {x.grad}')</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="1177" class="lp jj hh ll b be lq lr l ls lt">The Tensor object: tensor([1.1863], requires_grad=True)<br/>Current gradient stored in x: None</span></pre><p id="897f" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">让我们创建一个函数，y = x</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="b78e" class="lp jj hh ll b be lq lr l ls lt">y = x**2</span></pre><p id="cafd" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">我们现在可以调用魔法，<strong class="kc hi"> <em class="lz"> y.backward() </em> </strong>，计算x的微分(你也知道，就是2*x)。我们将计算出的差异存储在x的grad属性中。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="9a71" class="lp jj hh ll b be lq lr l ls lt">y.backward()<br/>print(f'Current gradient stored in x: {x.grad}')<br/>print(f'Value/attribute of y: {y}')</span></pre><pre class="mv lk ll lm bn ln lo bi"><span id="bc1a" class="lp jj hh ll b be lq lr l ls lt">Current gradient stored in x: tensor([2.3725])<br/>Value/attribute of y: tensor([1.4072], grad_fn=&lt;PowBackward0&gt;)</span></pre><p id="aa3a" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">一些简单的计算会告诉你这些值都加起来了。最后，如果您计划在程序中的任何地方更改x.grad中的值，您必须首先再次将它设置为<strong class="kc hi"> <em class="lz"> None </em> </strong>，否则已经存储的结果将会干扰grad，使其不一致(阅读:错误)。</p><pre class="ix iy iz ja fd lk ll lm bn ln lo bi"><span id="00f2" class="lp jj hh ll b be lq lr l ls lt">x.grad = None #consider commenting this line out before running the code block to see what happens<br/>y = x.exp()<br/>y.backward()<br/>print(y,x.grad)</span></pre></div><div class="ab cl nl nm go nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ha hb hc hd he"><p id="9156" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">在第二部分，我们将收集我们现在所知道的关于MNIST数据集和PyTorch的信息，并创建我们的第一个神经网络！您可以在此处查看:</p><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/@saffand03/saying-hello-world-to-deep-learning-part-2-2aaf60a7482d"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">向深度学习第二部分问好</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">在本文的第二部分，我们使用PyTorch为MNIST数据集创建了一个神经网络。</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="nz l mq mr ms mo mt jc mf"/></div></div></a></div></div><div class="ab cl nl nm go nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ha hb hc hd he"><p id="cce4" class="pw-post-body-paragraph ka kb hh kc b kd lu ii kf kg lv il ki kj lw kl km kn lx kp kq kr ly kt ku kv ha bi translated">考虑关注我:<a class="ae nx" href="https://www.linkedin.com/in/syed-affan-38b378216/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>| |<a class="ae nx" href="https://github.com/sulphatet" rel="noopener ugc nofollow" target="_blank">GitHub</a></p><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/@saffand03"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">赛义德·阿凡培养基</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">阅读赛义德·阿凡在媒介上的作品。印度的学生。我写的是机器学习等等。联系我关于…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="oa l mq mr ms mo mt jc mf"/></div></div></a></div><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="ob l mq mr ms mo mt jc mf"/></div></div></a></div></div></div>    
</body>
</html>