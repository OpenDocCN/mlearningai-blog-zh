<html>
<head>
<title>Having fun with CLIP features — Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">享受剪辑功能的乐趣—第一部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/having-fun-with-clip-features-part-i-29dff92bbbcd?source=collection_archive---------2-----------------------#2022-02-19">https://medium.com/mlearning-ai/having-fun-with-clip-features-part-i-29dff92bbbcd?source=collection_archive---------2-----------------------#2022-02-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="6020" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自从<a class="ae jc" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> openAI </a>发布连接图像和标题文本的剪辑模型以来，已经有一年多了。这个庞大的模型在400米(！)在网络上训练成对的图像和说明。在本帖中，我们将使用一些降维技术和<a class="ae jc" href="https://github.com/openai/CLIP" rel="noopener ugc nofollow" target="_blank">开源剪辑模型添加一些可视化和洞察力。</a></p><h2 id="394a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">我们将从剪辑的简短介绍开始:</h2><p id="2f7c" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我鼓励你阅读<a class="ae jc" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">的博客文章</a>，特别是关于CLIP以及它如何被训练/使用或者更好的——论文<a class="ae jc" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"/>。简而言之，CLIP旨在为图像和文本提示找到一个相互潜在的空间。这是通过获取带有匹配标题的成对图像，使它们通过一些编码器(分别是一个文本编码器和一个图像编码器)并鼓励它们的余弦相似性较高来完成的。他们还使用该批中的其他对作为反面例子，鼓励他们的余弦相似性较低。这导致在特定图像表示和其匹配文本之间具有深层联系的特征空间。我把这个过于简单化了——但这是它的要点。你可以看看这些真的很酷的帖子:<a class="ae jc" href="https://distill.pub/2021/multimodal-neurons/" rel="noopener ugc nofollow" target="_blank"> (1) </a>，<a class="ae jc" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">，</a>，<a class="ae jc" rel="noopener" href="/@JettChenT/image-classification-with-openai-clip-3ab5f1c23e35">，</a>。事实证明，仅仅通过以这种方式对许多对进行训练，就给了我们一个非常大的特征空间。有多棒？基本上它非常擅长在下游任务的<strong class="ig hi"> ALOT </strong>上<strong class="ig hi">零射击</strong>分类(它没有受过这方面的训练)。我们将在稍后的文章中深入探讨这个问题。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/e8fde61367fe1350f7109a336699960b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ww4oJEHzFsqe1tx0V0pdTQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Taken from the <a class="ae jc" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">CLIP Blogpost.</a></figcaption></figure><p id="46fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然我们理解了这个模型的目的是什么，那么CLIP训练的特征实际上是什么样的呢？在第一篇博文中，我们将使用UMAP(一种降维技术)来更好地理解这一点。大多数图形是交互式的——你可以玩它们/取消不同的标记/并得出你自己的一些结论！</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="b534" class="la je hh bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">让我们从CIFAR10嵌入空间开始</h1><p id="e484" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">让我们从一个简单的例子开始剪辑的几个镜头的潜在空间。我们知道，CLIP在许多数据集上的分类(零镜头)非常出色；包括CIFAR10。让我们看看这个数据集的图像在特征空间中的表现。在整篇文章中，我将使用<a class="ae jc" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> UMAP </a>进行降维。下面我们有从CIFAR10采样的图像，这些图像经过了<strong class="ig hi">夹子ViT-B/32 </strong>图像编码器。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx"><strong class="ak">CIFAR10</strong> feature space using the <strong class="ak">ViT-B/32 </strong>Image Encoder. Green stars represent class text-embedding.</figcaption></figure><p id="0e01" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">令人惊讶的是，尽管模型没有根据这些数据进行训练，但仍然可以看到非常清晰的聚类。您可以随意摆弄图表，看看模型哪里更混乱了。</p><p id="1e8e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">绿色星星是关于什么的？为此，我们需要记住零射击分类是如何完成的。每个图像都通过图像编码器。对于数据集中的每个类(在我们的例子中——{飞机，汽车，狗..})我们将句子<strong class="ig hi">“一张{class_name}的照片”</strong>输入文本编码器。然后，我们将类别预测指定为与图像特征具有最接近相似性的类别。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lt"><img src="../Images/4377d229d18eaaef3183577305dcbca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dM8_9FQ4u6uK7KfjRNaFQw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Taken from the <a class="ae jc" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">CLIP Blogpost.</a></figcaption></figure><p id="7951" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么现在关于绿色的星星。这些正是句子<strong class="ig hi">“一张{class_name}的照片”</strong>的潜在表征。我们能看到的东西很少:</p><ol class=""><li id="5629" class="lu lv hh ig b ih ii il im ip lw it lx ix ly jb lz ma mb mc bi translated">文本标签相对靠近类簇</li><li id="e9ab" class="lu lv hh ig b ih md il me ip mf it mg ix mh jb lz ma mb mc bi translated">似乎对于某些类(例如“狗”类)来说，标签离实际的聚类很远。</li></ol></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="a1c0" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">通过及时的工程改进</h2><p id="4b93" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">有人可能会问，我们是否可以用几个句子而不是一个句子来描述每一个类，从而提高零射击能力？CLIP的作者确实展示了一些精心制作的“提示工程”——试图捕捉整个类的特征。除了代表班级的<strong class="ig hi">“{ class _ name }”</strong>照片，他们还会查看</p><pre class="ke kf kg kh fd mi mj mk ml aw mm bi"><span id="e9e3" class="jd je hh mj b fi mn mo l mp mq">'a bad photo of a {}.',<br/>'a photo of many {}.',<br/>'a sculpture of a {}.',<br/>'a photo of the hard to see {}.',<br/>'a low resolution photo of the {}.',<br/>'a rendering of a {}.',<br/>'graffiti of a {}.',<br/>'a bad photo of the {}.',<br/>'a cropped photo of the {}.',<br/>'a tattoo of a {}.',<br/>...</span></pre><p id="69f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每幅图像总共有80个提示。然后我们可以看看这些提示是如何映射到特征空间的。我们采用添加了模板的相同特征空间。每个带叉的青色圆圈代表不同的提示，颜色代表类别。<strong class="ig hi">欢迎您将鼠标悬停在它们上面，查看不同的提示</strong>。正方形表示该特定类别的模板平均特征的UMAP投影。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx"><strong class="ak">CIFAR10</strong> embedding space, including different templates used in the paper, for each class.</figcaption></figure><p id="e603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们试着理解一下这里发生了什么。以<strong class="ig hi">卡车</strong>级(黄色)为例:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mr"><img src="../Images/ec0c308536681b14121df027b321d2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uKO3LYSadwP0HTWZVdREkw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Features of the ‘Truck’ class along with the different templates.</figcaption></figure><p id="dcad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们看到，基本上所有的模板都与来自同一类别的图像相对接近。因此，模板特征的平均值对于类特征是有意义的(尽管有点偏向边缘)。现在让我们看看<strong class="ig hi">狗</strong>类(深蓝)附近发生了什么</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mr"><img src="../Images/5faa4d2abbd1098c74895e8ed2b4aacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMGryNoD0RNy102cnWNoqw.png"/></div></div></figure><p id="73af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们看到类的模板:<strong class="ig hi">鸟，猫，飞机，</strong>和<strong class="ig hi">狗</strong>都混在一起了！事实上，这些类别的平均特征看起来非常接近。当试图使用模板对上述类中的实例进行分类时，这可能会有问题。在这个特征空间里你还能找到哪些有问题的关系？</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="4c0a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">潜在空间中的其他概念</strong></h2><p id="8337" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">CLIP确实擅长于许多不同的下游任务(无需微调)。凭直觉思考特征空间，我们愿意相信<strong class="ig hi">类狗</strong>附近会有不同类型的狗，对于<strong class="ig hi">船、【鹿】、</strong>和所有其他类也是如此。让我们试着看看事实是否确实如此。</p><p id="c750" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为此，我添加了一些关闭提示，以查看它们如何通过剪辑文本编码器进行映射。我使用以下提示:</p><pre class="ke kf kg kh fd mi mj mk ml aw mm bi"><span id="a7bf" class="jd je hh mj b fi mn mo l mp mq"><strong class="mj hi">'a photo of a helicopter', <br/>'a photo of a minivan',<br/>'a photo of a cruise',<br/>'a photo of the titanic',<br/>'a photo of a submarine',<br/>'a photo of a pony',<br/>'a photo of an aircraft carrier',</strong>(notice this one is super-tricky)<strong class="mj hi"><br/>'a photo of a toad'</strong></span></pre><p id="4f7d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看他们在潜空间里是如何行动的。这些示例在下图中用灰色三角形<strong class="ig hi">标记。将鼠标悬停在它们上面可以看到匹配的提示。</strong></p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Adding some custom prompts to see where they land in the embedding space.</figcaption></figure><p id="5644" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，这很酷！所有的例子都被映射到“正确的”集群(至少在我看来是这样)。请注意，我选择了总体上看起来不错的集群，并考虑了匹配示例。这不是一个彻底的调查，我相信我们可以找到这样不顺利的例子。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="6752" class="la je hh bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">MNIST特征空间</h1><p id="2715" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">所以我们尝试了CIFAR10，并得到了不错的结果。其他数据集呢？以下比较摘自该论文:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ms"><img src="../Images/b782f90c41d364c3ae86f60a6a0abcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQZQ1A9TO2FOWjpGXZo_xQ.png"/></div></div></figure><p id="b57b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是CLIP与监督训练的ResNet50(即在下游任务中从头开始训练ResNet50)相比的零射击性能。我们看到，对于更不像自然图片的数据集:<strong class="ig hi"> MNIST </strong>(黑白数字)<strong class="ig hi">patchcameleyon</strong>(病理切片)等。—零镜头片段的表现比监督模型差得多。让我们来看看<strong class="ig hi"> MNIST </strong>数据集的潜在特征是怎样的:</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Latent space for the <strong class="ak">MNIST </strong>dataset.</figcaption></figure><p id="8b61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先看到的是——这个潜在空间是聚集在一起的！每个类都有一个单独的集群，而且它们是完全分开的！这就产生了一个问题:<em class="mt">为什么零拍剪辑在这个数据集上表现不太好？</em>我们来看看<strong class="ig hi">的星星</strong>(即班文—<strong class="ig hi">‘一家三口的照片’</strong>)。在0、3、5类上，类文本看起来相当不错——其他类呢？他们的标签不在附近！当然，分类会遇到一些麻烦。查看特征空间，如果我们能找到文本和正确聚类之间的一些更好的映射，我们将是黄金！</p><p id="2de1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是等等！我们有快速工程技巧。行得通吗？</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="1f1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">答案是否定的。这是一个巨大的混乱——模板到处都是。我们看到，对于正常提示与类(0，3，5)匹配得很好的类，模板的平均值也非常准确。但是，在其他班级呢？相当凄凉。对我来说，这指出了一个事实，即MNIST类型的图像(黑白小图像)和它们的标题之间的配对与剪辑训练的配对非常不同。也许一些更聪明的提示工程可以以这种方式获得更好的结果。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="9fa1" class="la je hh bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">通过图层裁剪要素</h1><p id="e2bc" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">作为最后一个实验，我总是喜欢看中间层。这些特征是如何转变的，这让我很感兴趣。对于这一部分，我们将采用<strong class="ig hi"> CIFAR100 </strong>。我们知道CLIP的图像编码器具有良好的下游零拍性能。这意味着本质上，类被聚集在最后一层(正如我们之前看到的)。这种聚集是如何发生的？对于这种可视化，我从CIFAR100中随机选取9个类，并可视化它们在每个中间层中的UMAP维数减少。你可以播放视频，看看这里发生了什么。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="5fe5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对我来说，这非常类似于<a class="ae jc" href="https://www.manning.com/books/deep-learning-with-python" rel="noopener ugc nofollow" target="_blank">这本</a>书中提出的“球的展开原理”:</p><blockquote class="mu mv mw"><p id="b096" class="ie if mt ig b ih ii ij ik il im in io mx iq ir is my iu iv iw mz iy iz ja jb ha bi translated">“想象两张彩纸:一张红的，一张蓝的。把一个放在另一个上面。现在把它们揉成一个小球。那个皱巴巴的纸团就是你的输入数据，每一张纸就是一个分类问题中的一类数据。神经网络要做的是找出一种能使纸团不再卷曲的变换，从而使这两个类别再次清晰地分开”</p></blockquote><p id="0c47" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个视频中，我们很好地展示了这种非皱缩现象。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="719b" class="la je hh bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">结论</h1><p id="e543" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">在这篇文章中，我们看到了一些我们可以通过可视化剪辑嵌入空间得到的见解。CLIP是一个令人惊叹的模型，它对我来说是相当疯狂的，它对我来说是免费和公开的。我将上传以下部分，其中有更多类似于这里看到的见解。如果你喜欢这篇文章，我会感谢你的喜欢/分享。</p><p id="64df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]拉德福德，亚历克等人，“从自然语言监督中学习可转移的视觉模型。”<em class="mt"> ICML </em> (2021)。</p><p id="4ea8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]弗朗索瓦·乔莱。“用Python深度学习。”(2017).</p><p id="efc8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]麦金尼斯、利兰和约翰·希利。" UMAP:一致流形逼近和降维投影."<em class="mt">ArXiv</em>ABS/1802.03426(2018):n . PAG。</p><p id="077f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4] Goh，Gabriel等人，“人工神经网络中的多模态神经元”(2021).</p><div class="na nb ez fb nc nd"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hi fi z dy ni ea eb nj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr kn nd"/></div></div></a></div></div></div>    
</body>
</html>