<html>
<head>
<title>12 Activation Functions That You May Want To Consider — Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你可能要考虑的12个激活功能—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/12-activation-functions-that-you-may-want-to-consider-part-1-f863615c02c6?source=collection_archive---------3-----------------------#2022-02-27">https://medium.com/mlearning-ai/12-activation-functions-that-you-may-want-to-consider-part-1-f863615c02c6?source=collection_archive---------3-----------------------#2022-02-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/95ae985d11638e8c8f3b62bc7beaee62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CfcktVlI1G2ud-w6"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by Gertrūda Valasevičiūtė on <a class="ae it" href="https://unsplash.com/@skraidantisdrambliukas" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="efbf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在深入探讨激活函数之前，我们先来了解一下这三个问题的答案:</p><p id="3815" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">什么是神经网络？</strong></p><p id="2b9d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">什么是激活功能？</strong></p><p id="b5d1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">为什么需要它？</strong></p><p id="a154" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">什么是神经网络？</strong></p><p id="8f57" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经网络只不过是由相互连接的神经元组成的网络，这些神经元通过其<strong class="iw hi">权重</strong>、<strong class="iw hi">偏差</strong>和<strong class="iw hi">激活函数</strong>进一步识别。</p><p id="7e5c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经网络的要素:</p><p id="3d22" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">输入层:</strong></p><p id="9d57" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这一层的唯一目的是从外界获取信息(特征)并传递给网络。此外，在该层中不执行任何计算，并且给定的信息直接传递到下一层(隐藏层)。</p><p id="65f6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">隐藏层:</strong></p><p id="d357" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">顾名思义，这一层的节点是不暴露的。它是输入层和输出层之间的一层。这一层负责执行所有必要的计算，然后将计算结果传递给下一层(输出层)。此外，在神经网络中总会有最少的一个隐藏层。</p><p id="b719" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">输出图层:</strong></p><p id="576e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，这一层将通过隐藏层学习到的所有信息作为最终结果交付。</p><p id="47d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下面是一个神经网络的图像:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/c11aaae43bb1ffb632aaf396d865817e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkOs5hEnWE8kIMQfl0lCKg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.ibm.com/cloud/learn/neural-networks" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d735" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">什么是激活功能？</strong></p><p id="e44b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经网络激活函数基本上决定是否应该激活神经元/节点(应该传递信息),并且它通过检查该值是否高于某个阈值来实现。</p><p id="06f4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">为什么需要？</strong></p><ul class=""><li id="39f4" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">激活函数最重要的目的是将<strong class="iw hi">非线性</strong>添加到我们的神经网络中。在正向传播期间，激活函数在每一层都引入了一个额外的步骤，因此质疑它的存在是很明显的。那么我们就通过一个例子来了解一下吧！</li></ul><p id="9449" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们考虑一个没有激活函数的神经网络。在这种情况下，每个神经元将使用权重和偏差对输入执行线性变换(<strong class="iw hi"> W*x+b </strong>)。现在的要点是，这种线性变换对我们并没有真正的帮助，因为它的次数是1 ( <strong class="iw hi"> W*x </strong>)，即线性的，因此它只会像任何其他线性分类器一样工作，这不足以识别我们在计算机视觉或自然语言处理中遇到的复杂模式。因此，为了学习或识别这种非线性模式，使用了激活函数。</p><ul class=""><li id="3980" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">除了给我们的神经网络增加非线性之外，根据我们的要求，激活函数还有助于将来自神经元的输出值保持在一定的限度内。</li></ul><p id="6fb4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了前任。</p><p id="0d63" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我们的激活函数(<strong class="iw hi"> W*x+b </strong>)中发生线性变换操作，其中<strong class="iw hi"> W </strong>是权重，<strong class="iw hi"> x </strong>是输入，<strong class="iw hi"> b </strong>是偏差。如果不受限制，该操作的价值可以达到非常大的程度(就数量而言)，特别是在深度(具有一个以上的隐藏层)神经网络中，这会导致一些计算问题。</p></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="5b27" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">既然我们已经具备了学习激活功能所需的所有知识，让我们开始吧！</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/edf3c5e03b0e4c38ab22e233454f3d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*18IfLrjtvyVHfnaL.gif"/></div></figure><p id="8ec5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">二进制步进功能:</strong></p><p id="0d0e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是一个基于阈值的激活函数，这意味着如果该值能够越过特定阈值，则神经元将被激活，否则将被停用(输出不会传递到下一层)。</p><p id="8f55" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:[-∞，∞] </strong></p><p id="647d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表示:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ko"><img src="../Images/4975eb2622fef525ed1b33032327c015.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*O7fp-wfgfqEeBUSoezaxkw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="1dc0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图形:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/02298be1989a797917197a20a4d4ab68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOfpltWEwER9M9I7aM9GdQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="f692" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="5075" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">二元分类的好选择。</li><li id="4764" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">简单易懂。</li></ul><p id="559c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="b84c" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">不能用于多类分类。</li><li id="1ee1" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">这个函数的梯度(导数)是<strong class="iw hi">零，</strong>因此<strong class="iw hi"> </strong>在<strong class="iw hi"> </strong>反向传播过程中造成<strong class="iw hi"> </strong>阻碍<strong class="iw hi"> </strong>。</li></ul></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="a629" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">线性激活功能:</strong></p><p id="7510" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该功能也被称为<strong class="iw hi">身份功能。</strong>这里激活与输入成正比。</p><p id="be64" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:[-∞，∞] </strong></p><p id="d433" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表示:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ku"><img src="../Images/44d226177641c051e2c0c06ffc74c20e.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*IZfcTON9KINsr5DdhEC45Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by author</figcaption></figure><p id="ef60" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图形:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/e8fb0025ec17d288f8eafac32bae0679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnCibVuaBAvfk7r7HwqmpQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="f182" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="5f08" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">该函数的输出不限于任何范围。</li><li id="264d" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">简单易懂。</li></ul><p id="7991" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="95f7" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">这个函数的梯度是常数，因此不能用于反向传播。</li><li id="15f0" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">神经网络不会学习任何东西，因为它没有改善误差项。</li><li id="6aa9" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">因为它的本质是线性的，所以你添加多少隐藏层都没有关系，因为最终它们都将被压缩到一个层中，或者我们可以说线性激活函数将神经网络转换为一个层。</li></ul></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="1bbc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">乙状结肠/逻辑激活功能:</strong></p><p id="dbd8" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个函数基本上接受一个输入并输出另一个介于0和1之间的值。输入越积极，值就越接近1。同样，输入越负，值越接近0。</p><p id="bf48" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:[0，1] </strong></p><p id="b981" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表示:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/3006e2bc5ff3220896805105db1dd13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*U6U8kkcHrf2UBfHGUWI3NQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by author</figcaption></figure><p id="0134" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图表:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/be53acc3a8141ed6938cf776ca310a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VRbWeEAj-GlQOsc75ep1Ig.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d2f9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="d0d6" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">伟大的选择，我们必须以概率的形式预测结果，因为概率的范围在0和1之间。</li><li id="1c0d" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">该函数的梯度(导数)是可微分的，并且还提供了平滑的梯度(避免突然中断或中间跳跃)。</li><li id="9477" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">梯度计算很简单。</li></ul><p id="c43a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="0d64" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">它受到<strong class="iw hi">消失梯度问题</strong>的困扰，因为在反向传播期间，梯度变得非常接近零，使得权重难以更新，因此收敛变得非常慢。此外，如果梯度变为<strong class="iw hi">零</strong>，则没有学习发生。</li></ul></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="73ad" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">双曲正切函数:</strong></p><p id="bd1c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该激活函数类似于sigmoid激活函数，唯一的主要区别是，其范围位于[-1，1]之间，而不像[0，1]。该功能也有一条S形曲线，但由于最小范围是-1(而不是0)，所以是以<strong class="iw hi">零为中心的</strong>。</p><p id="915e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:[-1，1] </strong></p><p id="abe9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表示:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/66a078db728ce42dc24c29b54a8ae257.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*_x6aSvLAjCug-p4MRHU7Xw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by author</figcaption></figure><p id="aa31" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图形:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/e4080ce758e7ea44a280cfb25e6e8a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhKq8qjA-HE3nt3qhEFgyQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="3f6b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="2944" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">它是<strong class="iw hi">可微的</strong>。</li><li id="795d" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">它将负输入映射为强负，将零输入映射为中性，将正输入映射为强正。</li><li id="153b" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">该函数使选择考虑哪个值和忽略哪个值变得更容易，因为我们能够根据其范围获得不同符号的值，即[-1，1]。</li><li id="9d9f" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">这是一个<strong class="iw hi">零中心</strong>激活功能。</li></ul><p id="0136" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="4df7" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">该函数也遭受类似于sigmoid激活函数的<strong class="iw hi">消失梯度问题</strong>。</li><li id="8702" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">这在计算上是昂贵的。</li></ul><blockquote class="kx ky kz"><p id="264f" class="iu iv la iw b ix iy iz ja jb jc jd je lb jg jh ji lc jk jl jm ld jo jp jq jr ha bi translated">注:由于其<strong class="iw hi">零中心</strong>特性，双曲正切非线性始终优于sigmoid非线性。</p></blockquote></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="5e37" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> ReLU功能:</strong></p><p id="01bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">ReLU代表<strong class="iw hi">整流线性单元。</strong>该函数是最常用的激活函数，因为它用于大多数卷积神经网络或深度学习。</p><p id="b1b6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个函数的特别之处在于，它将所有负输入映射为零，并原样输出任何正值(像线性函数一样)。</p><p id="6b24" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:</strong><strong class="iw hi">【0，∞】</strong></p><p id="52c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表达式:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es le"><img src="../Images/8c35d96775a0b3f2faefb613f05a6435.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*3fzvkA7RI8iRgT0nx5o-QQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by author</figcaption></figure><p id="b6a1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图形:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/2cf44a771adc8e3cf3a6f8c652d8aaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-x8Ir97MWDANPAJIji_Yfg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://www.v7labs.com/blog/neural-networks-activation-functions#activation-function" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="8447" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="a901" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">它不会一次激活所有的神经元，因此与其他激活函数(sigmoid和tanh)相比，它的计算效率更高。</li><li id="aaa5" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">ReLU函数最重要的属性是它的<strong class="iw hi">非饱和</strong>属性，它鼓励梯度下降向它的全局最小值收敛。</li><li id="b3bf" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">最大阈值是<strong class="iw hi">无穷大，</strong>因此解决了<strong class="iw hi">消失梯度问题</strong>的问题。</li></ul><p id="9f81" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="feea" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">本质上是线性的。</li><li id="15bc" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">有时它面临<a class="ae it" href="https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24" rel="noopener" target="_blank">垂死的ReLU </a>问题。</li><li id="7fe6" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">它只能在神经网络的<strong class="iw hi">隐层</strong>中使用。</li></ul></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="e9bc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">漏ReLU功能:</strong></p><p id="e9fb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是ReLU函数的一个改进版本，创建它是为了纠正ReLU的死亡问题。它在负区域有一个小的正斜率<strong class="iw hi">。它还具有通常在0.1和0.3之间的<strong class="iw hi">α</strong>值。</strong></p><p id="64f6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">范围:</strong> [-∞，∞]</p><p id="f092" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数学表达式:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/1f5108a701dc4f0e7a50f6f7881a16b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*so8tg4Jf_CGvZv2YoTHLyg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by author</figcaption></figure><p id="986d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">图形:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/b77445ce268afabf12e4891128b2569e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3AIVe2s8dXYSL3QmopvWg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><a class="ae it" href="https://mlfromscratch.com/activation-functions-explained/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9544" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="a508" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">它修复了垂死的ReLU问题，因为它没有零中心的部分。</li><li id="e46c" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">解决了<strong class="iw hi">消失渐变</strong>的问题。</li><li id="8bbe" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">当平均激活接近于0时，它加速了训练。</li><li id="4832" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">ReLU激活功能的所有优点。</li></ul><p id="042a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="f68c" class="jx jy hh iw b ix iy jb jc jf jz jj ka jn kb jr kc kd ke kf bi translated">遭遇<strong class="iw hi">爆炸梯度</strong>问题。</li><li id="28d7" class="jx jy hh iw b ix kp jb kq jf kr jj ks jn kt jr kc kd ke kf bi translated">微分后，函数变为<strong class="iw hi">线性</strong>。</li></ul><p id="c91f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这篇文章里的都是这些家伙！</p><p id="1fba" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是该系列的下一部— <a class="ae it" href="https://pawarsaurav842.medium.com/12-activation-functions-that-you-may-want-to-consider-part-2-93037405c443" rel="noopener">第二部</a></p><p id="41c2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你有任何问题，请告诉我！！</p><p id="ce89" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">再见！</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es lg"><img src="../Images/3cb0f80996267344dd212e14804f86f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*C6nLYOsYVsZzRuqi.gif"/></div></figure><div class="lh li ez fb lj lk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hi fi z dy lp ea eb lq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">medium.com</p></div></div><div class="lt l"><div class="lu l lv lw lx lt ly in lk"/></div></div></a></div></div></div>    
</body>
</html>