<html>
<head>
<title>Saying ‘Hello World’ to Deep Learning Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向深度学习第二部分问好</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/saying-hello-world-to-deep-learning-part-2-2aaf60a7482d?source=collection_archive---------2-----------------------#2022-12-31">https://medium.com/mlearning-ai/saying-hello-world-to-deep-learning-part-2-2aaf60a7482d?source=collection_archive---------2-----------------------#2022-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2f4b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">在本文的第二部分，我们使用PyTorch为MNIST数据集创建了一个神经网络。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/65ea10b8911afa57ab65f0e1fcb11e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yn2GS0izq3z5jWOr"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@euwars?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Farzad</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c5ed" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在前一部分中，我们学习了PyTorch的基础知识，了解了什么是MNIST数据集。现在让我们建立一个神经网络来正确地分类每幅图像。</p><p id="21a6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">您可以在此处查看第1部分:</p><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/@saffand03/saying-hello-world-to-deep-learning-part-1-cb6ac50c5768"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">向深度学习第1部分问好</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">在这个深度学习系列的第一部分，我们让自己熟悉MNIST数据集和PyTorch。</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="kw l kx ky kz kv la jg km"/></div></div></a></div></div><div class="ab cl lb lc go ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ha hb hc hd he"><h2 id="311e" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">数据集:</h2><p id="8c1b" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">MNIST是一个受欢迎的数据集，您可以使用PyTorch本身将它转换成工作形式:</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="359f" class="mn lj hh mj b be mo mp l mq mr">import torchvision<br/>from torchvision import datasets<br/>from torchvision.transforms import ToTensor<br/><br/>train_data = datasets.MNIST(<br/>    root="data", # where download data to?<br/>    train=True, # get training data<br/>    download=True,<br/>    # images come as PIL format, we want to turn into Torch tensors<br/>    transform=ToTensor()<br/>)<br/>test_data = datasets.MNIST(<br/>    root="data",<br/>    train=False, # get test data<br/>    download=True,<br/>    transform=ToTensor()<br/>)</span></pre><p id="13b8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们快速检查一下这是否是我们所知道和喜爱的相同数据:</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="21dd" class="mn lj hh mj b be mo mp l mq mr">image, label = train_data[0]<br/>print(f'Dimensions of image: {image.shape}')<br/>print(f'Label of the image: {label}')</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="b386" class="mn lj hh mj b be mo mp l mq mr">Dimensions of image: torch.Size([1, 28, 28])<br/>Label of the image: 5</span></pre><h2 id="8723" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">火炬. nn .线性:</h2><p id="2f1e" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">你可能还记得<strong class="jp hi"> <em class="mt"> y = m.x + c </em> </strong>，一条直线的方程。PyTorch的特色是Linear类，它对输入张量执行类似的变换。比方说，我们可以用它将一个28×28的矩阵简化为一个1×10维的向量，其中每个向量值代表图像属于特定类别的概率。举个例子。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="68bf" class="mn lj hh mj b be mo mp l mq mr">import torch<br/>import torch.nn as nn<br/><br/>input_tensor = torch.randn(128, 20) #128x20 Matrix....<br/>print(input_tensor.shape)<br/>linear_transform = nn.Linear(in_features=20, out_features=10)<br/>output_tensor = linear_transform(input_tensor) #...converted into 128x10 Matrix<br/>print(output_tensor.shape)</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="c865" class="mn lj hh mj b be mo mp l mq mr">torch.Size([128, 20])<br/>torch.Size([128, 10])</span></pre><p id="4fca" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这段代码实际上模拟了将一个128x20维的矩阵乘以一个20x10的矩阵来创建一个128x10的矩阵。</p><h2 id="eeb9" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">火炬. nn .展平:</h2><p id="5d2f" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">在前一部分中，我们看到我们有28x28 = 784个像素。我们可以用<strong class="jp hi">的<em class="mt"> nn。Flatten() </em> </strong>将多维数据转换成一维。</p><h2 id="dd74" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">火炬. nn.Sequential:</h2><p id="5751" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">Sequential是一个容器，是一种可以组合多个层的方式，可以是线性的，也可以是其他方式，这样输出就可以从顶层传递到下一层，一直传递到底层。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="76e9" class="mn lj hh mj b be mo mp l mq mr">import torch<br/>import torch.nn as nn<br/><br/>linear_transforms = nn.Sequential(<br/>    nn.Flatten(),<br/>    nn.Linear(28*28,10)<br/>)<br/>output_vector = linear_transforms(image) <br/>#Remember 'image' from the dataset section?<br/>print(output_vector.shape)</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="ec1d" class="mn lj hh mj b be mo mp l mq mr">torch.Size([1, 10]) #A 1x10 dim vector.</span></pre><p id="d1c2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当我们调用<strong class="jp hi"><em class="mt">linear _ transforms</em></strong>时，28x28张量被展平成一个784维的线性向量，然后通过一个线性层，最终缩减成一个1x10的向量。而且，你刚刚创建了你的第一个<strong class="jp hi">神经网络。</strong></p><h2 id="4ac5" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">但那不是它(著名的遗言)</h2><p id="87b8" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">如果你把<em class="mt">输出向量</em>打印出来，你会发现它不像任何角度的概率向量(它们被称为逻辑)。要将其转换成概率向量，我们需要使用<strong class="jp hi"> <em class="mt"> softmax </em> </strong>函数。Softmax是一个数学函数，它将一组值推到0和1之间，因此它们类似于概率。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="8d04" class="mn lj hh mj b be mo mp l mq mr">#print(output_vector)<br/>softmax = nn.Softmax(dim =1) #instantiate softmax object for 1 dimensional tensor<br/>probabilities = softmax(output_vector)<br/>print(f'Sum of probabilities is {probabilities.sum().item()}')<br/>print(f'According to this model, the label of the image is: {probabilities.argmax().item()}')<br/>#print(probabilities)</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="787c" class="mn lj hh mj b be mo mp l mq mr">Sum of probabilities is 1.0<br/>According to this model, the label of the image is: 9</span></pre><p id="9b42" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">argmax是返回<strong class="jp hi"> <em class="mt">最大值的<em class="mt">索引</em>的函数调用。</em>T25】</strong></p><p id="d930" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">太好了！我们做了一个神经网络，我们有了概率。但是图像的标签是5，模型显示标签是9。事实上，对相同的数据再次调用<em class="mt"> linear_transforms </em>函数，您将获得非常不同的结果。问题在于线性层。</p><blockquote class="mu"><p id="dda1" class="mv mw hh bd mx my mz na nb nc nd ki dx translated">模型需要训练。</p></blockquote><p id="2147" class="pw-post-body-paragraph jn jo hh jp b jq ne ii js jt nf il jv jw ng jy jz ka nh kc kd ke ni kg kh ki ha bi translated">它的权重(也称为28*28x10维矩阵)是随机初始化的，我们的目标是拥有能够正确预测给定图像张量的类别的权重。为此，模型必须经过<strong class="jp hi">训练。</strong></p><h1 id="bcca" class="nj lj hh bd lk nk nl nm lo nn no np ls in nq io lv iq nr ir ly it ns iu mb nt bi translated">训练模型:</h1><p id="29d6" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">在我们开始训练模型之前，让我们为我们的模型创建一个稍微复杂和形式化的版本。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="5108" class="mn lj hh mj b be mo mp l mq mr">class Model(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.layer_stack = nn.Sequential(<br/>            nn.Flatten(), # flatten inputs into single vector<br/>            nn.Linear(in_features=28*28, out_features=300),<br/>            nn.ReLU(),<br/>            nn.Linear(in_features=300, out_features=10)<br/>        )<br/>    <br/>    def forward(self, x: torch.Tensor):<br/>        return self.layer_stack(x)</span></pre><p id="b832" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="mt">类</em> <strong class="jp hi"> <em class="mt">模型</em> </strong>很好地包装了模型中我们想要的一切。现在我们调用<em class="mt">模型(输入)</em> <strong class="jp hi"> <em class="mt"> </em> </strong>时会隐式调用forward方法。如果你有一些面向对象编程的知识，你会注意到我们继承了<em class="mt"> torch.nn.Module </em>类。</p><p id="bc36" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果你不确定这意味着什么:简单地认为这是简单的<em class="mt"> nn的一个更好更漂亮的版本。Sequential() </em>我们一直使用至今。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nu"><img src="../Images/a04245a468493e5f3ca147a0b216404b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xo8vYGX20sk3PFET"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Andrew Ng has a plethora of great courses on Deep Learning you can view on Coursera.</figcaption></figure><p id="dda2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在我们创建模型之前，也就是创建一个类型为<em class="mt"> Model </em>的对象，我们需要<a class="ae jm" rel="noopener" href="/@saffand03/saying-hello-world-to-deep-learning-part-1-cb6ac50c5768">指定我们的设备，并将模型发送到那个设备。</a></p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="f4b7" class="mn lj hh mj b be mo mp l mq mr">import torch<br/>device = "cuda" if torch.cuda.is_available() else "cpu"<br/>print(device)</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="801a" class="mn lj hh mj b be mo mp l mq mr">model_1 = Model().to(device)</span></pre><h2 id="bc45" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">nn。ReLU():</h2><p id="9b86" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">你可能注意到的一个问题是nn。Linear()就是那个，线性。只有线性映射是没有用的，因为我们遇到的大多数真实生活数据(包括MNIST)都有非线性关系。为了补救，我们需要增加非线性。ReLU是我们模型中包含的非线性函数。<a class="ae jm" rel="noopener" href="/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5"> <strong class="jp hi">了解更多激活功能。</strong>T25】</a></p><h2 id="cf3f" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">让我们得到正确的数据:</h2><p id="41a8" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">我们需要将数据输入数据加载器。数据加载器…为我们的模型加载数据。它有一个超参数:<strong class="jp hi"> <em class="mt"> batch_size，</em> </strong>将数据集转换成长度为<em class="mt"> batch_size </em>的可迭代块。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="531e" class="mn lj hh mj b be mo mp l mq mr">from torch.utils.data import DataLoader<br/><br/># Setup the batch size hyperparameter<br/>BATCH_SIZE = 32<br/><br/># Turn datasets into iterables (batches)<br/>train_dataloader = DataLoader(train_data, # dataset to turn into iterable<br/>    batch_size=BATCH_SIZE, # how many samples per batch? <br/>    shuffle=True # shuffle data every epoch?<br/>)<br/><br/>test_dataloader = DataLoader(test_data,<br/>    batch_size=BATCH_SIZE,<br/>    shuffle=False # don't necessarily have to shuffle the testing data<br/>)</span></pre><p id="9d78" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">数据集被分成多个块，每个块中有32幅图像。这32个图像的组，或者称为一批，将一次被输入到模型中。批处理是数据加载器的主要功能。我们的模型一次处理32幅图像，而不是一次处理一幅图像。那会使事情变得更快。</p><p id="a889" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们还需要一个助手函数，返回我们的模型得到正确的类的百分比。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="7631" class="mn lj hh mj b be mo mp l mq mr">def classification_accuracy(y_true, y_pred):<br/>    correct = torch.eq(y_true, y_pred).sum().item() <br/>    # torch.eq() calculates where two tensors are equal<br/>    vector_length = len(y_pred)<br/>    acc = (correct / vector_length) * 100<br/>    return acc</span></pre><h2 id="e18f" class="li lj hh bd lk ll lm ln lo lp lq lr ls jw lt lu lv ka lw lx ly ke lz ma mb mc bi translated">训练模型(这次是真的):</h2><p id="57e9" class="pw-post-body-paragraph jn jo hh jp b jq md ii js jt me il jv jw mf jy jz ka mg kc kd ke mh kg kh ki ha bi translated">权重需要更新。我们这样做的过程叫做训练模型。这个过程是迭代的。每个训练步骤都需要更新权重，以便减少损失。这分5步完成:</p><p id="25fe" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di"> 1。</span> <strong class="jp hi">正向传递:</strong>模型将输出返回给训练数据。输出=模型(输入)</p><p id="a395" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di"> 2。</span> <strong class="jp hi">损失:</strong>将模型的输出与实际值进行比较，并评估它们的误差。使用损失函数。</p><p id="6e5a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di"> 3。</span> <strong class="jp hi">反向传播</strong>:为每个权重计算损失的<a class="ae jm" rel="noopener" href="/@saffand03/saying-hello-world-to-deep-learning-part-1-cb6ac50c5768">梯度</a>。</p><p id="b61f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di"> 4。</span> <strong class="jp hi"> Zero_grad优化器:</strong>优化器是最终将更新权重的函数。它的渐变设置为0。</p><p id="2ac5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di"> 5。</span> <strong class="jp hi">步进优化器(梯度下降):</strong>权重更新。为此，我们使用一个称为优化器的对象。</p><p id="6f62" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">基于这五个步骤，我们仍然缺少两个功能。损失函数和优化器。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="e098" class="mn lj hh mj b be mo mp l mq mr">loss_fn = nn.CrossEntropyLoss()<br/>optimizer = torch.optim.SGD(params=model_1.parameters(),lr=0.1)</span></pre><p id="dce8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有许多不同的损失函数和优化可用。事实上，选择正确的功能本身就是一个超参数。另一个超级参数是优化器的学习率(lr)。反复试验，仔细阅读PyTorch官方文档，将有助于做出正确的设计选择。</p><p id="f117" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然而，有了它，现在训练就像:</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="21e7" class="mn lj hh mj b be mo mp l oe mr">output = loss(input) &lt;- Step 1<br/>loss = loss_fn(output,real_values) &lt;- Step 2<br/>optimizer.zero_grad() &lt;- Step 3<br/>loss.backward() &lt;- Step 4<br/>optimizer.step() &lt;- Step 5</span></pre><p id="e493" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们将这几行代码打包成一个漂亮的python函数:</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="472f" class="mn lj hh mj b be mo mp l mq mr">def train_step(model,data_loader,loss_fn,optimizer,accuracy_fn,device):<br/>    train_loss, train_acc = 0, 0<br/>    <br/>    for batch, (X, y) in enumerate(data_loader):<br/><br/>        # Send data to GPU<br/>        X, y = X.to(device), y.to(device)<br/><br/>        # 1. Forward pass<br/>        y_pred = model(X)<br/><br/>        # 2. Calculate loss<br/>        loss = loss_fn(y_pred, y)<br/>        train_loss += loss<br/>        train_acc += accuracy_fn(y_true=y,<br/>                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels<br/><br/>        # 3. Optimizer zero grad<br/>        optimizer.zero_grad()<br/><br/>        # 4. Loss backward<br/>        loss.backward()<br/><br/>        # 5. Optimizer step<br/>        optimizer.step()<br/><br/>    # Calculate loss and accuracy per epoch and print out what's happening<br/>    train_loss /= len(data_loader)<br/>    train_acc /= len(data_loader)<br/>    print(f"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%")</span></pre><p id="889f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">同样，为了测试模型:</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="22b1" class="mn lj hh mj b be mo mp l mq mr">def test_step(model,data_loader,loss_fn,accuracy_fn,device):<br/>    test_loss, test_acc = 0, 0<br/><br/>    model.eval() # put model in eval mode<br/>    <br/>    # Turn on inference context manager<br/>    with torch.inference_mode(): <br/>        for X, y in data_loader:<br/>            # Send data to GPU<br/>            X, y = X.to(device), y.to(device)<br/>            <br/>            # 1. Forward pass<br/>            test_pred = model(X)<br/>            <br/>            # 2. Calculate loss and accuracy<br/>            test_loss += loss_fn(test_pred, y)<br/>            test_acc += accuracy_fn(y_true=y,<br/>                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels<br/>            )<br/>        <br/>        # Adjust metrics and print out<br/>        test_loss /= len(data_loader)<br/>        test_acc /= len(data_loader)<br/>        print(f"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\n")</span></pre><p id="f18c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="mt"> model.eval() </em>和<em class="mt"> model.inference_mode() </em>向PyTorch表明，我们使用这个模型是为了预测，而不是训练。预测的返回速度要快得多。这些命令被视为最佳实践。</p><p id="1f42" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我们需要做的就是反复训练这个模型。请记住，训练模型是一个迭代过程。我们将根据数据训练我们的模型3次，即<strong class="jp hi">3个时期</strong>。</p><pre class="ix iy iz ja fd mi mj mk bn ml mm bi"><span id="240c" class="mn lj hh mj b be mo mp l mq mr">from tqdm.auto import tqdm #Colorful progress bar<br/><br/>epochs = 3<br/>for epoch in tqdm(range(epochs)):<br/><br/>    print(f"Epoch: {epoch}\n---------")<br/>    <br/>    train_step(data_loader=train_dataloader, <br/>        model=model_1, <br/>        loss_fn=loss_fn,<br/>        optimizer=optimizer,<br/>        accuracy_fn=classification_accuracy,<br/>        device = device<br/><br/>    )<br/>    test_step(data_loader=test_dataloader,<br/>        model=model_1,<br/>        loss_fn=loss_fn,<br/>        accuracy_fn=classification_accuracy,<br/>        device = device<br/>    )</span></pre><pre class="ms mi mj mk bn ml mm bi"><span id="15fe" class="mn lj hh mj b be mo mp l mq mr">Epoch: 0<br/>---------<br/>Train loss: 0.33264 | Train accuracy: 90.70%<br/>Test loss: 0.16921 | Test accuracy: 95.15%<br/><br/>Epoch: 1<br/>---------<br/>Train loss: 0.14738 | Train accuracy: 95.73%<br/>Test loss: 0.11719 | Test accuracy: 96.64%<br/><br/>Epoch: 2<br/>---------<br/>Train loss: 0.10253 | Train accuracy: 97.03%<br/>Test loss: 0.09454 | Test accuracy: 97.22%</span></pre></div><div class="ab cl lb lc go ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ha hb hc hd he"><p id="9e0b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在第三部分，我们将讨论卷积和迁移学习。从MNIST的数据集中毕业。</p></div><div class="ab cl lb lc go ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ha hb hc hd he"><p id="0024" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">考虑关注我:<a class="ae jm" href="https://www.linkedin.com/in/syed-affan-38b378216/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>| |<a class="ae jm" href="https://github.com/sulphatet" rel="noopener ugc nofollow" target="_blank">GitHub</a></p><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/@saffand03"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">赛义德·阿凡培养基</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">阅读赛义德·阿凡在媒介上的作品。印度的学生。我写的是机器学习等等。联系我关于…</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="of l kx ky kz kv la jg km"/></div></div></a></div><div class="kj kk ez fb kl km"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hi fi z dy kr ea eb ks ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">medium.com</p></div></div><div class="kv l"><div class="og l kx ky kz kv la jg km"/></div></div></a></div></div></div>    
</body>
</html>