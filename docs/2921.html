<html>
<head>
<title>An Overview of Ensemble Learning — United We Stand</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习概述——我们团结一致</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-overview-of-ensemble-learning-united-we-stand-11643f019e6e?source=collection_archive---------6-----------------------#2022-06-27">https://medium.com/mlearning-ai/an-overview-of-ensemble-learning-united-we-stand-11643f019e6e?source=collection_archive---------6-----------------------#2022-06-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a246" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将看看<strong class="ig hi">集合</strong> <strong class="ig hi">学习</strong>和集体智能之间的关系，背后的机器学习技术和可以实现的算法。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/7e0a5a5c6cf6976b110f86ee62fc5522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZKAYcNBMMZVDmpb7"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/es/@mitchel3uo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mitchell Luo</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8122" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">集成学习是基于Scott Page在他的<a class="ae js" href="https://www.jstor.org/stable/j.ctt7sp9c" rel="noopener ugc nofollow" target="_blank">研究论文</a>中所说的。他谈到了<strong class="ig hi">知识</strong>聚合<strong class="ig hi">的<strong class="ig hi">重要性</strong>并得出结论:</strong></p><blockquote class="jt ju jv"><p id="e048" class="ie if jw ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">多样性优先于能力。(斯科特·佩奇)</p></blockquote><p id="895c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">面对一个领域的专家和一群对这个领域有基本了解的人，会让这个群体做出比专家更好的决定和选择。这是基于<strong class="ig hi">集体</strong> <strong class="ig hi">智慧</strong>的领域，指 <strong class="ig hi">群众</strong>的<strong class="ig hi">智慧</strong> <strong class="ig hi">。</strong></p><blockquote class="ka"><p id="fbbb" class="kb kc hh bd kd ke kf kg kh ki kj jb dx translated">一群lambda个体能够形成相关的判断并解决复杂的问题。</p></blockquote><p id="bfb3" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">一个很好的例子是世界各地的<strong class="ig hi">加里</strong>卡斯帕罗夫<strong class="ig hi">卡斯帕罗夫</strong>和<strong class="ig hi">50</strong>T28】000T30】玩家之间的国际象棋比赛。所有50，000人在同一时间一起玩，人群的每个动作都由投票决定。比赛的目的不是结果，因为卡斯帕罗夫在4个月后赢得了比赛，而是一群普通的棋手能够对抗有史以来最好的棋手之一。</p><p id="981b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然我们看到了集体决策的要点和知识聚合的重要性，让我们看看机器能做些什么…</p></div><div class="ab cl kp kq go kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="ha hb hc hd he"><p id="be02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于人类可以通过分享他们的想法来做出决定，机器也可以结合他们的学习来获得更好的结果，这被称为集成学习。</p><p id="84c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本上，多个模型将被训练，它们的预测将被用来制作一个独特的模型，这将是更好的，因为它使用了从不同角度学习。</p><h1 id="7d1d" class="kw kx hh bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">集成学习的不同技术</h1><p id="b1b6" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">从多个模型中学习可以通过不同的方式完成。然而，第一个问题是关于模型本身。<strong class="ig hi">集成</strong> <strong class="ig hi">学习</strong>可以多次使用同一个模型，也可以使用基于不同机器学习算法的不同模型。</p><p id="35fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看技术…</p><h2 id="02a3" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">投票或平均</h2><p id="f636" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">最简单的集成学习就是选择不同模型(来自不同算法)预测的<strong class="ig hi"/><strong class="ig hi"/>最多的<strong class="ig hi">类</strong>。例如，如果训练4个模型，其中3个预测了正类(1)，则集成学习的整体预测将是正类(1)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mn"><img src="../Images/07a0cd3857d63ecd53f72f797f1383c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xc5ZP0dpIF_Dm1kI37ZFbg.jpeg"/></div></div></figure><p id="7017" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这也可以在回归问题中通过使用所有模型预测的平均值来完成。</p><h2 id="b3a0" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">装袋和粘贴</h2><p id="7ff3" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated"><strong class="ig hi">打包</strong>和<strong class="ig hi">粘贴</strong>是集成学习技术，它们使用相同的模型(例如具有固定超参数的决策树)在数据集的<strong class="ig hi">不同的</strong><strong class="ig hi"/><strong class="ig hi">样本</strong>上进行训练。</p><blockquote class="jt ju jv"><p id="8f9d" class="ie if jw ig b ih ii ij ik il im in io jx iq ir is jy iu iv iw jz iy iz ja jb ha bi translated">当模型在不同的数据上被训练时(或者在它们的模型中使用真正不同的算法)，集成方法工作得更好。<br/>(奥雷连恩·盖伦)</p></blockquote><p id="ec2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">知道这一点后，每个模型将从样本中学习，然后所有预测将按最频繁的类别或平均值(用于回归)进行聚合。</p><p id="03b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与第一种技术的最大区别在于<strong class="ig hi">每种模型都根据不同的数据进行训练</strong>。这使得最终模型受结果差异的影响更小，因为聚合只会收集数据集中的共性，而不会收集数据集中的某些特定特性。</p><p id="6ce9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">→ <strong class="ig hi">集合的每个</strong> <strong class="ig hi">唯一</strong> <strong class="ig hi">模型</strong>将具有比最终模型更大的<strong class="ig hi"/><strong class="ig hi">方差</strong>(从所有其他模型的结果中聚集)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mo"><img src="../Images/2061d8fe2888d5e53fc00a720f1c7e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XoKH9Fo8JaCCOoPehdVp8A.jpeg"/></div></div></figure><p id="8aba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么区分装袋和裱糊？</strong></p><p id="86fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi">Bagging</strong>(bootstrap aggregation)“训练集中的随机数据样本被选择<strong class="ig hi">并替换</strong>——这意味着单个数据点可以被选择不止一次”(<a class="ae js" href="https://www.ibm.com/cloud/learn/bagging" rel="noopener ugc nofollow" target="_blank"> IBM — Bagging </a>)，而在<strong class="ig hi">粘贴</strong>中，数据的随机样本在被集合的一个模型选择后不会在训练数据集中被替换。因此，在粘贴中，训练集的每个观察将用于学习，而在装袋中，平均而言，仅选择数据集的63%<strong class="ig hi">来进行训练。这意味着37%没有被任何模型看到，这些数据点被称为:<strong class="ig hi">出袋实例</strong> (OOB)。<br/> OOB实例因此可用于评估模型性能。</strong></p><h2 id="e938" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">助推</h2><p id="22ad" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">这种技术是第二常见的集成学习技术。基本上是<strong class="ig hi">顺序</strong> <strong class="ig hi">模型</strong> <strong class="ig hi">训练</strong>(一个接一个)，其中每个模型试图校正前一个模型(或者通过增加坏预测的权重作为AdaBoost，或者通过从前一个模型预测的残差训练作为梯度增强(我们将在下一节中看到这些算法))。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mp"><img src="../Images/97c100407e14a8b19b7d3826a4c84b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A3fJp1cMO2IVs9HOLvTjiA.jpeg"/></div></div></figure><h1 id="2d33" class="kw kx hh bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">著名算法</h1><p id="8a7e" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">现在我们知道了集成学习的不同方法，让我们用一点代码来看看一些著名的算法。</p><h2 id="bd68" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">随机森林</h2><p id="bfc9" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">该算法使用一个用打包技术训练的 <strong class="ig hi">决策树</strong> <strong class="ig hi">树</strong>的<strong class="ig hi">集合</strong> <strong class="ig hi">。这导致更高的偏差误差(和更低的方差)，因为每个决策树将只使用数据集的样本来为每个分裂寻找最佳特征(查看<a class="ae js" rel="noopener" href="/mlearning-ai/decision-tree-the-simplest-algorithm-to-explain-82889d300904">我的文章</a>关于决策树的更多信息)。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mq"><img src="../Images/b32b1c5bfd4cc29306355980c089db5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujqMI0iBzEYUynF2Cj7s9g.jpeg"/></div></div></figure><p id="34e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Scikit-Learn库提供了随机森林的实现:RandomForestClassifier(用于分类)或RandomForestRegressor(用于回归) :</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mr"><img src="../Images/9b7565d33c6a7a6bb9ff22c9a3a1bb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6f-ILOMNM0EqfMPCOzWIJA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">(made with <a class="ae js" href="https://carbon.now.sh" rel="noopener ugc nofollow" target="_blank">Carbon</a>)</figcaption></figure><h2 id="0544" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">adaboost算法</h2><p id="a5c2" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">AdaBoost是一种使用<strong class="ig hi"> Boosting </strong> <strong class="ig hi">方法</strong>训练预测器的算法。正如我之前提到的，算法在所有例子上实现了<strong class="ig hi">权重</strong>，并增加了那些错误预测的权重。因此，每个新训练的模型将集中于<strong class="ig hi">集合</strong> <strong class="ig hi">模型</strong> <strong class="ig hi">具有</strong> <strong class="ig hi">难度</strong> <strong class="ig hi">至</strong> <strong class="ig hi">预测</strong>的<strong class="ig hi">实例</strong>。它校正欠调整的观测值，以得到更准确的最终集合模型。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ms"><img src="../Images/d847d6caf3926f00efc54c55c827dc6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6P5V2wJl_9Wwst5F_et3g.jpeg"/></div></div></figure><p id="2ef8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Scikit-Learn还提供了AdaBoost算法的实现:AdaBoostClassifier(用于分类)或AdaBoostRegressor(用于回归) :</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mr"><img src="../Images/a7a9198d0f2a5a6b6011f4b2a835aedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBQujmSC2iZmy-lvfFi_lA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">(made with <a class="ae js" href="https://carbon.now.sh" rel="noopener ugc nofollow" target="_blank">Carbon</a>)</figcaption></figure><h2 id="d1ba" class="lz kx hh bd ky ma mb mc lc md me mf lg ip mg mh lk it mi mj lo ix mk ml ls mm bi translated">梯度推进</h2><p id="33c1" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">梯度提升是一种集成方法，通过使用先前模型预测的<strong class="ig hi">残差</strong> <strong class="ig hi">误差</strong> (y - ŷ)作为训练数据集中的新目标，依次训练每个模型。基本上，第一个模型是在数据集上训练的，第二个模型使用第一个模型预测的残差作为新的目标。因此，每个模型将试图<strong class="ig hi">减少</strong> <strong class="ig hi">残留</strong> <strong class="ig hi">错误</strong>，只需关注它(一个接一个)。最后，由集合做出的预测是来自集合中每个模型的所有<strong class="ig hi"/><strong class="ig hi">预测</strong>的<strong class="ig hi">和</strong>(因为每个模型预测总目标的一部分)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mt"><img src="../Images/87a5c1959ebe82f028cbe7c64dbba41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ai-2GWGqhTrr2JXEe6qqHA.jpeg"/></div></div></figure><p id="48d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习中最著名的算法之一是XGBoost，它是python中一个优化的梯度提升实现:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mr"><img src="../Images/8a5556601fc354f363e7b660501a70b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5FjmD8I4DMSGpdWSbOauA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">(made with <a class="ae js" href="https://carbon.now.sh" rel="noopener ugc nofollow" target="_blank">Carbon</a>)</figcaption></figure></div><div class="ab cl kp kq go kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="ha hb hc hd he"><h1 id="6821" class="kw kx hh bd ky kz mu lb lc ld mv lf lg lh mw lj lk ll mx ln lo lp my lr ls lt bi translated">结论</h1><p id="4f89" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">集成学习是收集<strong class="ig hi">多个</strong> <strong class="ig hi">模型</strong>的<strong class="ig hi">性能</strong>的好方法。正如 <strong class="ig hi">人群</strong>中的<strong class="ig hi">智慧</strong> <strong class="ig hi">所强调的，一个由多个个体组成的群体能够真正擅长解决问题和做出决策。这被应用在具有集成方法的机器学习中。<br/> <strong class="ig hi">套袋</strong>(粘贴)和<strong class="ig hi">助推</strong>是主要技术，而<strong class="ig hi">随机</strong> <strong class="ig hi">森林</strong>、<strong class="ig hi">阿达助推</strong>和<strong class="ig hi">坡度</strong> <strong class="ig hi">助推</strong>是最著名的。</strong></p><p id="5802" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢并学到了一些关于合奏学习的东西！</p><h1 id="d90e" class="kw kx hh bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">资源:</h1><div class="mz na ez fb nb nc"><a href="https://www.jstor.org/stable/j.ctt7sp9c" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">区别:多样性的力量如何创造更好的团体、公司、学校和社会…</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">在这本里程碑式的书中，斯科特·佩奇重新定义了我们理解彼此的方式。区别在于…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">www.jstor.org</p></div></div></div></a></div><div class="mz na ez fb nb nc"><a href="https://arxiv.org/abs/1106.0257" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">流行的集成方法:一项实证研究</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">集成由一组单独训练的分类器(如神经网络或决策树)组成，其…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">arxiv.org</p></div></div><div class="nl l"><div class="nm l nn no np nl nq jm nc"/></div></div></a></div><div class="mz na ez fb nb nc"><a href="https://machinelearningmastery.com/what-is-ensemble-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">集成学习的温和介绍-机器学习掌握</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">我们在生活中做出的许多决定都是基于许多其他人的意见。这包括选择一本书来…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nl l"><div class="nr l nn no np nl nq jm nc"/></div></div></a></div><div class="mz na ez fb nb nc"><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">通过最近的一系列突破，深度学习推动了整个机器学习领域。现在，甚至…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">www.oreilly.com</p></div></div><div class="nl l"><div class="ns l nn no np nl nq jm nc"/></div></div></a></div><div class="mz na ez fb nb nc"><a href="https://www.ibm.com/cloud/learn/bagging" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">什么是装袋？</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">了解引导聚合或bagging如何提高机器学习模型的准确性，使您能够…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">www.ibm.com</p></div></div><div class="nl l"><div class="nt l nn no np nl nq jm nc"/></div></div></a></div><div class="mz na ez fb nb nc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hi fi z dy nh ea eb ni ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">medium.com</p></div></div><div class="nl l"><div class="nu l nn no np nl nq jm nc"/></div></div></a></div></div></div>    
</body>
</html>