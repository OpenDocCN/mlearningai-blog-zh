<html>
<head>
<title>DeepLearning4J: Simple Image Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepLearning4J:简单图像分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/deeplearning4-for-image-classification-part-1-fc01cb2b1c62?source=collection_archive---------0-----------------------#2022-02-15">https://medium.com/mlearning-ai/deeplearning4-for-image-classification-part-1-fc01cb2b1c62?source=collection_archive---------0-----------------------#2022-02-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/32db4da9b7fda8f81fa9ddded15fd97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrzYzlaTdDcTcpSnrHWrSA.jpeg"/></div></div></figure><div class=""/><p id="bf05" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们回顾一下如何使用<strong class="ir ht"> Deeplearning4j </strong>库用神经网络实现图像分类。如果你是神经网络的新手，你会喜欢读我的入门故事，<a class="ae jn" rel="noopener" href="/analytics-vidhya/neural-networks-in-a-nutshell-with-java-b4a635a2c4af?sk=6d3e0924646beddc233c1edf875565c7"> <em class="jo">什么是神经网络？</em> </a>。而且，如果你不熟悉<strong class="ir ht"> DeepLearning4J </strong>库，我可以推荐看看我的故事，<a class="ae jn" rel="noopener" href="/mlearning-ai/neural-networks-getting-started-with-eclipse-deeplearning4j-897f3662832b"> <em class="jo"> DeepLearning4J:入门</em> </a> <em class="jo">。</em></p><p id="bc66" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用神经网络进行图像分类是一个非常简单的过程。我们需要一个包含多幅图像的数据集，用来训练我们的神经网络。我们将使用每张图片中的每一个像素作为输入。并且，我们期望得到我们期望识别的类别作为输出。</p><p id="d3e7" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们做一个简单的项目:一个识别手写数字0到9的模型。我们将执行以下操作:</p><ol class=""><li id="8707" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm ju jv jw jx bi translated">获取并加载数据集</li><li id="7261" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">创建、训练和评估模型</li><li id="d6ae" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">在应用程序中部署模型</li></ol><p id="c1b6" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我将这篇文章分成两部分。在这一部分，我将介绍数据集和模型。稍后我们可以回顾模型的部署。</p><h1 id="eb8a" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">1.MNIST数据集</h1><p id="1d1a" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们的目标是创建一个神经网络来识别手写数字0到9。因此，我们需要大量手写数字的图像。包含<strong class="ir ht">手写数字</strong>的最著名的数据集之一是由样本组成的MNIST数据集，如图1所示。这是一个庞大的手写数字数据库，由7个<strong class="ir ht"> 0，000个</strong>图像组成。这些数字是由高中生和美国人口普查局的员工写的。每个数字都存储为黑白抗锯齿图像，并被标准化以适合一个<strong class="ir ht"> 28x28 </strong>像素边界框。</p><figure class="lh li lj lk fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lg"><img src="../Images/5077f777f7abe80b96336d7c4d08680f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mg7LdwuuaF9YJ5Af-NRUpA.jpeg"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Figure 1. Sample images from the MNIST dataset; each digit is stored as a 28x28 pixels image</figcaption></figure><p id="04f6" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为参考，根据MNIST <a class="ae jn" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">网站</a>，一层神经网络(用这个数据集训练)可以达到12%的错误率(相当糟糕)。相比之下，深度卷积神经网络可以实现低于0.25%的错误率。</p><h1 id="3dc3" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">2.加载数据</h1><p id="a795" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated"><strong class="ir ht"> DeepLearning4j </strong>为标准数据集提供现成的数据集迭代器，包括MNIST数据集。类<strong class="ir ht"> MnistDataSetIterator </strong>允许我们加载这个公共数据集。<strong class="ir ht"> MnistDataSetIterator </strong>的构造函数接收三个参数:</p><ul class=""><li id="86e8" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">批量大小，即在比较预期输出和计算误差之前要处理的训练样本的数量。</li><li id="b7b6" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">数据集中的样本总数。</li><li id="7370" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">一个标志，用于指示数据集是否应该二进制化(图像被认为是黑白的，没有灰色阴影)。</li></ul><p id="1ca9" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们使用100的批量大小，并要求数据集被认为是黑白图像。此外，让我们加载60，000幅图像用于训练，10，000幅图像用于测试，如下所示:</p><figure class="lh li lj lk fd hj"><div class="bz dy l di"><div class="lq lr l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Source Code 1. Initialization of DataSetIterator objects for training and testing datasets</figcaption></figure><p id="3b2c" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">很简单。不需要担心处理具有数千个28 x 28图像的数据结构的技术细节。</p><h1 id="f7fe" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">3.建立第一个模型</h1><p id="815e" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">先说一个基本的<strong class="ir ht">单隐层</strong>神经网络。后来，我们可以改进这个最初的方法。用于解决这个问题的单个隐藏层神经网络可以如下:</p><ul class=""><li id="dcef" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">图像中的每个像素成为一个输入；因此，我们有28×28 =<strong class="ir ht">784个输入</strong>。</li><li id="2bfd" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">我们要预测的每一个数字都成为一个输出；因此，我们在输出层有<strong class="ir ht">十个神经元。</strong></li><li id="35cd" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">最后，单隐层模型中隐层神经元的数目建议为:介于输入层和输出层之间；输入层大小的2/3，加上输出层的大小。；或者小于输入层尺寸的两倍。这三条规则为你提供了一个考虑的起点。最终，为你的神经网络选择一个架构将归结为反复试验。我们用<strong class="ir ht">隐藏层</strong>的1000个神经元。</li></ul><p id="67bb" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们可以使用类<strong class="ir ht"> <em class="jo">多层配置</em> </strong>和<strong class="ir ht"> <em class="jo">多层网络、</em> </strong>创建单个隐藏层神经网络，模拟我们在<em class="jo">deep learning 4j</em>中所做的，如下所示:</p><figure class="lh li lj lk fd hj"><div class="bz dy l di"><div class="lq lr l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Source Code 2. A model with UNIFORM weight initialization, SIGMOID activation function, and MSE loss function</figcaption></figure><p id="cdec" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个类<strong class="ir ht"> <em class="jo">多层配置</em> </strong>是一个魔术:</p><ul class=""><li id="a3da" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">使用SIGMOID作为激活函数，将784个输入全部连接到中间层中的1000个神经元</li><li id="a3fa" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">中间层中的1000个神经元连接到输出层中的10个神经元，使用SIGMOID作为激活，使用MSE作为损失函数。</li></ul><p id="b1d8" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那个<strong class="ir ht"> <em class="jo">多层配置</em> </strong>对象<strong class="ir ht"> <em class="jo"> </em> </strong>被用作<strong class="ir ht"> <em class="jo">多层网络</em> </strong>对象的输入。然后对该对象做了两件重要的事情:</p><ul class=""><li id="1ad0" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">我们设置学习率(记住一个0到1之间的值)。</li><li id="989e" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">我们通过调用方法<strong class="ir ht"> <em class="jo"> fit() </em> </strong>来训练我们的模型，该方法对提供的输入数据集执行一次迭代。</li></ul><h1 id="1e80" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">4.评估我们的第一个模型</h1><p id="ca83" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们将初始化一个新的<strong class="ir ht"> <em class="jo">评估</em> </strong>对象来评估将存储批处理结果的模型。请注意，参数10代表我们的网络试图识别的10个类别。我们分批迭代数据集，以保持合理的内存消耗率，并将结果存储在<strong class="ir ht"> <em class="jo">评估</em> </strong>对象中。请记住，在创建<strong class="ir ht"> <em class="jo">数据集</em> </strong>对象时，我们建立了一个100的批处理大小。最后，我们通过调用<strong class="ir ht"> <em class="jo"> stats() </em> </strong>函数得到结果:</p><figure class="lh li lj lk fd hj"><div class="bz dy l di"><div class="lq lr l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Source Code 3. Evaluation object</figcaption></figure><p id="e2db" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们得到的结果如下:</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="387d" class="lx ke hs lt b fi ly lz l ma mb"> Accuracy:        0.6442<br/> Precision:       0.7334 (1 class excluded from average)<br/> Recall:          0.6254<br/> F1 Score:        0.6447 (1 class excluded from average)</span></pre><p id="4174" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">MNIST数据集上的这些数字非常糟糕。有多种方法可以改善这一点，首先是激活和丢失功能。另外，也许，隐藏层数。</p><h1 id="18eb" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">5.建立第二个模型</h1><p id="c5cf" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">让我们改进我们的基本<strong class="ir ht">单隐层</strong>神经网络。我们可以改变并将显著改进我们模型的三个要素:</p><ul class=""><li id="124a" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">重量初始化。太大的初始化导致爆炸梯度(偏导数)和大量更新。太小的初始化会导致渐变消失(偏导数)和最小的更新。统一初始化通常不是一个好主意。使用随机值可以提供一个更好的解决方案，不是所有的值最终都会变大或变小。此外，让我们考虑具有随机值的初始化，其中平均值(正值和负值)为零。此外，每一层的方差保持不变。这就是我们用XAVIER初始化得到的结果。</li><li id="d42e" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">隐藏层的激活功能。隐藏层中流行的激活函数是整流线性激活(<strong class="ir ht"> ReLU </strong>)函数。如果是正的，ReLU直接输出输入；否则，它将输出零。ReLU <strong class="ir ht">克服了消失梯度问题</strong>，让模型表现更好。</li></ul><figure class="lh li lj lk fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mc"><img src="../Images/a587a1277bdcf835ec48640fa7304cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XU5V_I7b2JDDdN3Gh6yZ6Q.jpeg"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Figure 2. SIGMOID vs ReLU activation functions</figcaption></figure><ul class=""><li id="f1b9" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">输出层的激活函数。需要考虑的一个基本问题是，SIGMOID函数是独立的；因此，对我们的问题来说，尝试对10类图片(数字0到9)进行分类并不是最好的主意。SOFTMAX函数是处理多个类的输出图层的常用激活函数。softmax函数将N个实数的向量<em class="jo"> n </em>作为输入。它将其归一化为由与输入数字的指数成比例的<em class="jo"> N </em>个概率组成的概率分布。在我们的例子中，我们从10个输出转移到这些输出发生的10个概率。</li></ul><figure class="lh li lj lk fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es md"><img src="../Images/3708391521026950706ecd5087d5ddef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaB33UVDIiIV2coycYjn9g.jpeg"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Figure 3. Example of SOFTMAX activation function with N = 3</figcaption></figure><ul class=""><li id="ce59" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated">误差或损失函数。均方差(MSE)是比较数值的好方法。但是，现在我们想比较概率，我们需要不同的东西。输出层中的SOFTMAX函数与<strong class="ir ht">负对数似然函数</strong>一起用于计算误差或损失。我们测量观察数据<strong class="ir ht"> <em class="jo"> y </em> </strong>由参数值<strong class="ir ht"> <em class="jo"> w. </em> </strong>产生的可能性。可能性值在0到1的范围内。将对数应用于可能性有助于梯度的计算。因此，我们这样做。最后，0到1范围内的对数值无穷大到0。我们让它们为负，取值范围从无穷大到0。</li></ul><p id="a299" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">应用这些变化，我们的新模型如下:</p><figure class="lh li lj lk fd hj"><div class="bz dy l di"><div class="lq lr l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Source Code 4. A model with XAVIER weight initialization, RELU and SOFTMAX activation function, and NEGATIVELOGLIKELIHOOD loss function</figcaption></figure><h1 id="9510" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">6.评估我们的第二个模型</h1><p id="59d2" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们的结果改进如下:</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="0fc4" class="lx ke hs lt b fi ly lz l ma mb">Accuracy:        0.9576<br/>Precision:       0.9586<br/>Recall:          0.9582<br/>F1 Score:        0.9574</span></pre><p id="44d0" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">还不错。但是我们仍然可以做得更多。</p><h1 id="d78f" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">7.构建第三个模型</h1><p id="8634" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们还能做什么？完善培养机制。在<em class="jo">什么是神经网络中，</em>我描述了<strong class="ir ht">梯度下降</strong>的基本方法。有一些方法可以比普通的梯度下降带来更好的训练。梯度下降的一个限制是，当梯度变得平坦或曲率变大时，搜索的进度会变慢。改进的一个选择是将<strong class="ir ht">动量</strong>包含在等式中。动量是一个物理概念，是运动物体的运动量(其质量和速度的乘积)。如果我们把这个想法应用到梯度下降计算中。可以将动量添加到梯度下降中，以将一些惯性结合到更新中。此外，如果我们把这个动量包含在方程中，作为梯度下降计算的一部分。<strong class="ir ht">内斯特罗夫</strong>动量或<strong class="ir ht">内斯特罗夫</strong>加速梯度是正常梯度下降的微小变化。是的，它有可能改善我们模型中的学习。下图总结了我在这里描述的内容。</p><figure class="lh li lj lk fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es me"><img src="../Images/b925b3ed1fb0c9deee111872cc2294f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1xrTAELdIj9Ke_lI6OJHMQ.jpeg"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx">Figure 4. Comparing Gradient descendant, Accelerated gradient, and <strong class="bd kf">Nesterov</strong> Accelerated Gradient</figcaption></figure><p id="2f93" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意到代表势头的<strong class="ir ht"> <em class="jo"> v </em> </strong>，先是独自一人，然后以内斯特罗夫的方式。</p><p id="b727" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以使用更新配置选项来设置训练机制。更新器方法的参数是一个<strong class="ir ht"> <em class="jo">更新器</em> </strong>对象。例如，一个<strong class="ir ht"> <em class="jo"> Nesterovs </em> </strong>类可用于将<strong class="ir ht">内斯特罗夫</strong>加速梯度作为训练机制。<strong class="ir ht"> <em class="jo"> Nesterovs </em> </strong>实体接收两个参数:学习率和动量系数。</p><p id="b98f" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后一点，我们可以将训练数据多次输入神经网络。每次用整个训练数据集训练神经网络被称为一个时期。训练中的单一时期是不够的，会导致不适应。鉴于现实世界问题的复杂性，训练一个神经网络可能需要数百个历元。请注意，如果我们将历元数设置得太低，训练甚至会在模型收敛之前停止。相反，如果我们设置的历元数太高，我们将面临过度拟合；此外，我们将浪费计算能力和时间。我们可以将历元数指定为<strong class="ir ht"><em class="jo"/></strong>方法的第二个参数。让我们在模型的训练中使用15个时期。应用这些变化，我们的新模型如下:</p><figure class="lh li lj lk fd hj"><div class="bz dy l di"><div class="lq lr l"/></div><figcaption class="ll lm et er es ln lo bd b be z dx">Source Code 5. A model with Nesterov Accelerated Gradient as updater and 15 Epochs</figcaption></figure><h1 id="7e88" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">8.评估我们的第三个模型</h1><p id="e07a" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们的结果改进如下:</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="bef9" class="lx ke hs lt b fi ly lz l ma mb">Accuracy:        0.9862<br/>Precision:       0.9860<br/>Recall:          0.9863<br/>F1 Score:        0.9861</span></pre><p id="c0c2" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，低于0.95。</p><p id="93ad" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">观察最后一个模型的混乱矩阵，注意我们所有的类，事情开始变得有意义</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="4622" class="lx ke hs lt b fi ly lz l ma mb">=========================Confusion Matrix=========================<br/>    0    1    2    3    4    5    6    7    8    9<br/>---------------------------------------------------<br/>  994    0    0    0    2    2    1    0    1    1 | 0 = 0<br/>    0 1118    0    0    2    0    0    1    4    2 | 1 = 1<br/>    3    4  974    4    0    0    0    4    1    1 | 2 = 2<br/>    1    0    4  997    0   13    0    8    3    6 | 3 = 3<br/>    0    2    2    0  970    0    1    0    0    5 | 4 = 4<br/>    1    1    1    1    1  857    0    0    1    0 | 5 = 5<br/>    3    1    0    0    3    4 1003    0    0    0 | 6 = 6<br/>    1    1    2    0    3    1    0 1056    2    4 | 7 = 7<br/>    1    2    0    0    2    4    2    2  930    1 | 8 = 8<br/>    3    1    0    2    4    0    1    4    0  963 | 9 = 9</span><span id="8c7f" class="lx ke hs lt b fi mf lz l ma mb">Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times<br/>==================================================================</span></pre></div><div class="ab cl mg mh go mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ha hb hc hd he"><p id="ddd4" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就是这样！这就是神经网络如何识别图像中的模式，并用<strong class="ir ht"> Deeplearning4j </strong>库实现它们。但是，如果我们在一幅大图片中寻找元素，比如照片中的猫，那该怎么办呢？神经网络仍然是一个很好的选择，但我们可以进入神经网络模型的下一个阶段:卷积神经网络。这是另一个故事的主题。</p><p id="68f0" class="pw-post-body-paragraph ip iq hs ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">之前使用的完整源代码可以在我的<a class="ae jn" href="https://github.com/javiergs/Medium/tree/main/NeuralNetwork/digitsModel" rel="noopener ugc nofollow" target="_blank"> GitHub资源库</a>中找到。感谢阅读。请在下面留下您的反馈和评论。</p><h1 id="f649" class="kd ke hs bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最后一件事</h1><p id="0ddc" class="pw-post-body-paragraph ip iq hs ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated"><strong class="ir ht"> DeepLearning4j </strong>中提供的其他数据集加载器包括:</p><ul class=""><li id="71ba" class="jp jq hs ir b is it iw ix ja jr je js ji jt jm lp jv jw jx bi translated"><a class="ae jn" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank"> Iris </a>，包含三个类，每个类50个实例，每个类指一种鸢尾植物；</li><li id="47b3" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated">TinyImageNet(<a class="ae jn" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank">ImageNet</a>的子集)，根据WordNet层次结构组织的图像数据集；</li><li id="66b4" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated"><a class="ae jn" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>，数据集由10类60000张32x32的彩色图像组成，每类6000张；</li><li id="341e" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated"><a class="ae jn" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank">野外标记人脸</a>，人脸照片数据库；而且，</li><li id="f343" class="jp jq hs ir b is jy iw jz ja ka je kb ji kc jm lp jv jw jx bi translated"><a class="ae jn" href="http://vision.lems.brown.edu/datasets/cfgd" rel="noopener ugc nofollow" target="_blank">曲线片段地面实况数据集</a>，用于评估边缘检测或边界检测方法。</li></ul><div class="hg hh ez fb hi mn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd ht fi z dy ms ea eb mt ed ef hr bi translated">Mlearning.ai提交建议</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ho mn"/></div></div></a></div></div></div>    
</body>
</html>