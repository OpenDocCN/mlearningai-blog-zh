<html>
<head>
<title>PLDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PLDA</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/plda-a07405127e3e?source=collection_archive---------1-----------------------#2021-05-23">https://medium.com/mlearning-ai/plda-a07405127e3e?source=collection_archive---------1-----------------------#2021-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a81c" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">概率线性判别分析如何训练和推理</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/f66fe509242283961ad403104f175d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weTCID-uLKnBWgfXbsm0Ag.jpeg"/></div></div></figure><h1 id="9262" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">介绍</h1><p id="b936" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">概率线性判别分析(PLDA)是一种降维技术，与线性判别分析(LDA)相比，它是一种进步。LDA特征是作为训练PLDA的结果而得到的，但是它们具有附加到它们上面的概率模型，该概率模型自动地给予更有区别的特征更大的权重(<a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">源</a>)。</p><p id="06fb" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">这篇文章的目的是展示PLDA是如何被训练和用于推理的。最后用几个例子说明了PLDA如何用于分类和聚类。我假设你熟悉<a class="ae kw" rel="noopener" href="/nerd-for-tech/linear-discriminant-analysis-c24d9729d3d2"> LDA </a>的概念，并对PLDA有所了解。如果你对PLDA的介绍感兴趣，这里有一个很棒的<a class="ae kw" href="https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96" rel="noopener" target="_blank">帖子</a>。本帖的主要来源是原创的PLDA <a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">文章</a>和python <a class="ae kw" href="https://github.com/RaviSoji/plda" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><h1 id="67d9" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">学习超参数</h1><h2 id="b32b" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">公式</h2><p id="440d" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">学习PLDA参数包括几个步骤。一般来说，从数据中去除奇异点，计算散射矩阵。然后学习并存储必要的参数，用于将来的推断。</p><p id="fafe" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">更详细地说，应遵循以下步骤(<a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>):</p><ul class=""><li id="b697" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">将数据投影到主成分分析(PCA)子空间，以消除奇异点。这也是一种正则化，因为我们转换要素来描述大部分方差并减少它们的数量(我们使数据集更加“密集”)。</li><li id="3afd" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">计算类内和类间散布矩阵(Sb和Sw)。</li><li id="10d2" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">求广义特征向量的矩阵W，使得:</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es me"><img src="../Images/aa7b9fb3d3fd2e7b8e042f8bab097283.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/1*q_wz_UBQRxmXX_X59oQTXg.gif"/></div></figure><p id="0f84" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">λ<strong class="kc hi">T13】代表特征值。矩阵W将用于计算将数据投影到PLDA子空间的投影/加载矩阵A。还要注意，类内和类间散布矩阵的特征值可以通过以下方式计算:</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mf"><img src="../Images/2e75cba195ab2eae84bd822ca3e3ba5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/1*SwspVMufHViyQVTLPL6eYg.gif"/></div></figure><p id="5c26" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">λb和λw都是对角线(对角线上下的所有其他值都将是0)。</p><ul class=""><li id="39e2" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">学习模型的PLDA参数m(均值)、A(负载矩阵，等价地，方差φb和φw，它们基本上是类内和类间散布矩阵)和ψ(协方差矩阵)，使得:</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mg"><img src="../Images/8df3b118f9dfc374ec655ca59f3c92b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*rRM67fDzXdpO0GoKnN7WSQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx"><a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="832a" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">注意n是每类的平均例子数:n=(例子总数)/(类数)。矩阵A(或者更准确地说是它的逆)将是把数据从原始空间转换到PLDA操作的潜在空间的矩阵。</p><ul class=""><li id="fb3c" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">为了降低维数，只保留ψ的d个最大元素，并将rest设置为零。</li></ul><p id="72e3" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">与LDA相比，PLDA有更多的参数。但这可能是有益的，因为我们可以计算数据点属于某一类的概率。现在让我们用python实现训练。</p><h2 id="2b60" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">Python实现</h2><p id="cc4d" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">大多数代码样本都来自(或受其启发)库<a class="ae kw" href="https://github.com/RaviSoji/plda" rel="noopener ugc nofollow" target="_blank">这里</a>。我稍微修改了一些部分。python中的分步实现:</p><ul class=""><li id="81fe" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">执行<code class="du ml mm mn mo b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">PCA</a></code>，这是scikit-learn的简单实现:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><ul class=""><li id="48e2" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">类内和类间散布矩阵的计算也非常简单:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="a669" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">请注意，它的结果与此处<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank">的版本</a>相似(您可以在此处看到比较<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/scatter_matrices.ipynb" rel="noopener ugc nofollow" target="_blank">，它们可能有不同的数值，但它们之间的余弦相似度实际上为1，这意味着它们之间的角度为0，它们指向相同的方向，因此非常相似)。</a></p><ul class=""><li id="9800" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">可以使用<code class="du ml mm mn mo b"><a class="ae kw" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html" rel="noopener ugc nofollow" target="_blank">scipy.linalg.eigh</a></code>计算广义特征向量的W:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="7bd0" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">如果你对广义特征值问题感兴趣，这个<a class="ae kw" href="https://arxiv.org/pdf/1903.11240.pdf" rel="noopener ugc nofollow" target="_blank">源</a>可能会感兴趣(尤其是第5.2.2章。FISHER判别分析，因为我们试图最大化类别之间的距离，最小化每个类别内的方差)。</p><ul class=""><li id="78b3" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">学习参数m，A和ψ。从<a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>第537页可以看出，我们要计算类内和类间散布矩阵的λ(特征值)。使用点积是微不足道的:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="d8c9" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">一旦我们有了类内和类间散布矩阵的特征值，我们可以很容易地使用<code class="du ml mm mn mo b">numpy</code> : <code class="du ml mm mn mo b">X.mean(axis=0)</code>计算m</p><p id="c542" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">为了计算A(加载矩阵)，我们可以从<a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">第537页</a>执行公式:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="e581" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">此外，我们将计算A的逆，因为它需要进一步的计算:<code class="du ml mm mn mo b">np.linalg.inv(A)</code></p><p id="a410" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">类似地，ψ的计算遵循原始文章中的公式:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><ul class=""><li id="0ae0" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">为了只保留ψ的d个最大元素，我们将只找到非零元素ψ的位置:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="6e94" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">现在我们几乎有了一个可以用于推断的模型参数。再走几步，我们就到了。</p><h1 id="0c42" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">推理</h1><h2 id="280b" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">公式</h2><p id="a264" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">这里我将展示分类的例子。对于分类，我们有一些探测示例xᵖ，我们知道它属于m个类别中的一些类别(该模型被训练来识别)。我们也有M个类中每个类的样本(训练数据)。任务是找到探测器最可能来自的类。首先，我们必须将数据转换到具有更好的类间分离和更小的类内方差的潜在空间。这是使用平均值m和A的倒数来完成的:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/b9730ef4631e90a2e84bc99979739f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/1*YHIL3WYFNtqzuIhsAPa76w.gif"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">formula to transform data from original space to latent PLDA space. <a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="7ae0" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">要找到样本属于某一类的概率，我们需要知道每一类的均值和协方差。之后，我们可以使用正态分布计算该示例属于每个类别的概率(准确地说是对数概率密度)。为了将探针与每一类样品进行比较，我们可以使用以下公式:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ms"><img src="../Images/8732039af7da9f0d3d64b09400e1ff7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/1*1wobJCep_L5unupZmcRegQ.gif"/></div><figcaption class="mh mi et er es mj mk bd b be z dx"><a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e320" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">其中:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mt"><img src="../Images/6fe19d44f7e51c349abbbe0419ffa364.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/1*QPbNkKLUTVxJPXe1THsCmA.gif"/></div><figcaption class="mh mi et er es mj mk bd b be z dx"><a class="ae kw" href="https://link.springer.com/content/pdf/10.1007%2F11744085_41.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0184" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">并且:</p><ul class=""><li id="e272" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">ψ是先验协方差矩阵(基于训练数据)</li><li id="9865" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">n是训练数据中某类样本的数量</li></ul><p id="2613" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">第一个公式表明，我们将计算探针uᵖ的概率给定类别uᵍ样本均值及其协方差ψ:探针属于具有其自身均值和协方差的某个类别的概率是多少。最后一个公式显示了uᵍ.类别的平均值</p><h2 id="35d2" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">Python代码</h2><p id="c7ec" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">在python代码中，我们将执行以下步骤:</p><ul class=""><li id="d712" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">将训练数据变换到潜在空间:<code class="du ml mm mn mo b">x_in_u = (X-m)@inv_A.T</code>，注意X已经使用PCA进行了变换。现在我们必须只保留相关维度的数据:<code class="du ml mm mn mo b">U_model = x_in_u[:,relevant_U_dims]</code></li><li id="06c6" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">现在，我们可以使用上一节中提到的公式，计算训练数据中每个类别的均值和协方差:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="1be7" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">这两个步骤可以在训练期间完成，因为它们仅使用训练数据，并且在推断期间不需要重新计算它们。</p><p id="eca4" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">在推论中我们必须做以下步骤:</p><ul class=""><li id="5c49" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">将探测数据转换到潜在空间。首先我们要应用PCA: <code class="du ml mm mn mo b">data_test_pca = pca.transform(testing_data)</code>。在此之后，转换探针进入潜在空间(减去平均值并应用A的倒数):<code class="du ml mm mn mo b">data_test_u = (data_test_pca-m)@inv_A.T</code>。最后只保留相关维度:<code class="du ml mm mn mo b">data_test_u = data_test_u[:,relevant_U_dims]</code>。</li><li id="e006" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">计算探针来自每个类别的对数概率，并为每个探针样本找到最可能的类别:</li></ul><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="21e9" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">训练和推理的完整示例可在<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/probabilistic_linear_discriminant_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。现在，我们可以为每个类别保留最可能的预测类别。</p><h1 id="f565" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">例子</h1><h2 id="d903" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">分类</h2><p id="581c" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">为了评估PLDA有多适合分类，我做了一些简单的实验。首先，我使用MNIST数据集来测试不同的分类降维技术。这里的<a class="ae kw" href="https://github.com/RRisto/learning/tree/master/linear_algebra_learn/linear_discriminant_analysis/mnist_demo/mnist_data" rel="noopener ugc nofollow" target="_blank">是</a>用于实验的数据。实验结果和代码在<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/probabilistic_linear_discriminant_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>可用(参见“分类mnist数据”一节)。只有300个样本和10个类(数字从0到9)。实验具有以下结构:</p><ul class=""><li id="aa9d" class="lq lr hh kc b kd kx kg ky kj ls kn lt kr lu kv lv lw lx ly bi translated">对于范围为10到100(步长为10)的每个组件数量，执行以下操作:</li><li id="2566" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">对于0到50范围内的I，使用I作为随机种子，对MNIST数据进行随机分割。测试大小设置为0.3</li><li id="ee80" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">使用训练数据训练PLDA、PCA和LDA</li><li id="53e6" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">预测每个测试数据样本的最可能类别。直接使用PLDA作为分类器(从PLDA分数中找到最可能的类)。对于PCA和LDA，使用降维数据来训练<code class="du ml mm mn mo b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="noopener ugc nofollow" target="_blank">LinearSVC</a></code>分类器，并根据测试数据进行预测。</li></ul><p id="1ab2" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">与LDA相关的区别是:组件的最大数量与训练数据中的类的数量相关。最多LDA个组件数可以是min(n_classes -1，n_features) ( <a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html" rel="noopener ugc nofollow" target="_blank"> source </a>)。在当前实验中，这意味着最多只能有9个组件。PLDA没有此限制，组件数可以是原始数据中的最大要素数(尽管使用这么多组件可能会导致过度拟合)。</p><p id="a787" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">为了进行准确性比较，使用了F1分数。LDA的F1平均得分最低，约为0.68。PLDA和主成分分析对不同数量的成分有不同的准确性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mu"><img src="../Images/ddbac7629c81edbb6d60c8859401f531.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*5QP8Vgj6I437tERKRjEIEQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">PCA and PLDA median F1-scores with different number of components (n_comps).</figcaption></figure><p id="ae40" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">从图中我们可以看出，在少量成分的区域，PCA和PLDA精度是相当的。两者都急剧上升到30个成分。大约30种成分的主成分分析具有最高的F1值(大约0.81)。但是PLDA有40个分量，最大精度约为0.82。</p><p id="6a13" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">该实验的目的不是制造最准确的分类器，而是表明PLDA具有与组件数量相关的强性能。我们可以看到，与不使用来自类别标签的信息的PCA相比，PLDA应该潜在地为分类提供更合适的变换。当然，不同数据集之间的性能差距可能会有所不同。</p><p id="deba" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">对于第二个分类实验，我训练了简单的<code class="du ml mm mn mo b">LinearSVC</code>模型，并将其与基于PLDA特征训练的<code class="du ml mm mn mo b">LinearSVC</code>模型进行比较(使用40个组件)。实验设置为只有10%的数据用于测试。简单SVC模型具有以下准确性:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mv"><img src="../Images/21e90b8f144d2ab432dca9a85467bd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*Y0kskK7ONHAPwCTkLvQxiA.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Simple linear SVC model accuracy</figcaption></figure><p id="05ce" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">作为对比，我们可以看到PLDA提供了更高的精确度:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mw"><img src="../Images/a58172935b0658d96513d1aeb0117d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*q3lo73FzIHgtmvaOnDrR1Q.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Simple linear SVC model accuracy if PLDA features are used</figcaption></figure><p id="f7e5" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">这个实验的一个注意事项是，如果使用不同的训练和测试分割大小，它具有不同的准确性。在当前的实验中，测试数据中的样本非常少。但是PLDA可能会提高分类的准确性。</p><h2 id="d7b4" class="lc jj hh bd jk ld le lf jo lg lh li js kj lj lk ju kn ll lm jw kr ln lo jy lp bi translated">使聚集</h2><p id="dace" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">对于聚类，我使用了来自<a class="ae kw" href="https://github.com/AcademiaSinicaNLPLab/sentiment_dataset/tree/master/data" rel="noopener ugc nofollow" target="_blank">的一个NLP数据集(文件TREC.train.all.txt和TREC.test.all.txt)。该数据集比MNIST数据集更具挑战性，涉及将问题分为6种类型(无论问题是关于人、地点、数字信息等。)(</a><a class="ae kw" href="https://www.aclweb.org/anthology/C02-1150.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)。此实验的代码可从<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/probabilistic_linear_discriminant_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得(参见“问题数据”一节)。</p><p id="771f" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">想法是看看PLDA是否有助于转换聚类数据，以及它是否可以直观地看到。读入数据后，使用<code class="du ml mm mn mo b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">CountVectorizer</a></code>进行转换，并使用<code class="du ml mm mn mo b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank">TSNE</a></code>将数据维数降低到2，以便绘图，我们可以看到以下绘图:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mx"><img src="../Images/c69e93527918e85817d99d1f3c0dac44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*Sp0S4bVFYNyjPXCd9X24Zw.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">TREC original dataset after using TSNE to reduce dimensionality to 2</figcaption></figure><p id="39e1" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">我们可以看到，数据有一些集群，但许多类是混合的。有些班级很容易聚集，有些分散在各处。PLDA分离后变得更加清晰:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es my"><img src="../Images/08b0037b41c46442a4fa80780179a644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*lYE1lTUE6ztLebDK05meEg.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">TREC PLDA transformed dataset after using TSNE to reduce dimensionality to 2</figcaption></figure><p id="f4e3" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">尽管仍然有一些混合的例子，但是集群现在更加分离。和LDA一样，PLDA试图最小化类内方差，最大化类间方差。PLDA变换(将数据投影到PLDA潜在空间)使数据更容易分离。</p><p id="5cb4" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">另一种聚类方法是PLDA分数——记录样本属于6个类别之一的概率(在分类中，我们选择了最大的类别分数，这里我们包括所有类别)。这为每个数据点创建了一个分数向量，我们可以使用<code class="du ml mm mn mo b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank">cosine_similarity</a></code>来衡量这些点彼此之间的相似程度。为了比较，一个简单的余弦相似性矩阵是从文档术语矩阵计算的，一个是从变换的PLDA特征计算的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mz"><img src="../Images/0f42f7d0ff1ffc821b59c0494d2145be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGSlAOHBZA8_Zhcwf6jdyA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Questions dataset cosine similarity matrices heatmaps from CountVectorizer features (left) and PLDA transformed features (middle) and from PLDA scores (right). Sample are ordered by categories, red lines show class changes</figcaption></figure><p id="82fb" class="pw-post-body-paragraph ka kb hh kc b kd kx ii kf kg ky il ki kj kz kl km kn la kp kq kr lb kt ku kv ha bi translated">我们可以看到，视觉上的PLDA分数和PLDA变换特征使得分离更具对比性。原始数据在类别之间已经有一些差异，但是它们在视觉上不如在另外两个图中那么清晰。因此，PLDA可用于使用变换后的特征/PLDA分数进行聚类。</p><h1 id="721c" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">结论</h1><p id="5872" class="pw-post-body-paragraph ka kb hh kc b kd ke ii kf kg kh il ki kj kk kl km kn ko kp kq kr ks kt ku kv ha bi translated">我希望这篇文章强调了PLDA训练背后的一些逻辑。它可以被看作是LDA扩展，包括每个数据点属于某个类别的概率。PLDA可用于分类和聚类。</p><h1 id="d741" class="ji jj hh bd jk jl jm jn jo jp jq jr js in jt io ju iq jv ir jw it jx iu jy jz bi translated">参考</h1><ul class=""><li id="d36b" class="lq lr hh kc b kd ke kg kh kj na kn nb kr nc kv lv lw lx ly bi translated">余弦_相似度，<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank"> skicit-learn </a></li><li id="3206" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">计数矢量器，<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">skict-learn</a></li><li id="4fea" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">特征值和广义特征值问题:教程，<a class="ae kw" href="https://arxiv.org/pdf/1903.11240.pdf" rel="noopener ugc nofollow" target="_blank">便雅悯·高霍，法克里·卡雷，马克·克劳利</a></li><li id="a4b5" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">线性判别分析，<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Risto Hinno </a></li><li id="1c06" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">PCA，<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> skicit-learn </a></li><li id="8496" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">概率线性判别分析<a class="ae kw" href="https://github.com/RaviSoji/plda" rel="noopener ugc nofollow" target="_blank"/></li><li id="649b" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">概率线性判别分析</li><li id="db55" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">概率线性判别分析(PLDA)解释说，<br/> <a class="ae kw" href="https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96" rel="noopener" target="_blank">普拉奇·辛格</a></li><li id="ee77" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">概率_线性_判别_分析，<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/probabilistic_linear_discriminant_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">里斯托·欣诺</a></li><li id="e5b8" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">散布矩阵，<a class="ae kw" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/scatter_matrices.ipynb" rel="noopener ugc nofollow" target="_blank"> Risto Hinno </a></li><li id="dee1" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">SciPy.org</li><li id="0952" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">情感_数据集，<a class="ae kw" href="https://github.com/AcademiaSinicaNLPLab/sentiment_dataset/tree/master/data" rel="noopener ugc nofollow" target="_blank"> AcademiaSinicaNLPLab </a></li><li id="0f78" class="lq lr hh kc b kd lz kg ma kj mb kn mc kr md kv lv lw lx ly bi translated">TSNE，<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank"> skicit-learn </a></li></ul></div></div>    
</body>
</html>