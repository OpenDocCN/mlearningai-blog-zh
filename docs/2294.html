<html>
<head>
<title>Vision beyond classification: Tasks beyond classification: Task I: Object detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越分类的视觉:超越分类的任务:任务I:目标检测</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/vision-beyond-classification-task-i-object-detection-d2f32a5ea4ca?source=collection_archive---------2-----------------------#2022-04-09">https://medium.com/mlearning-ai/vision-beyond-classification-task-i-object-detection-d2f32a5ea4ca?source=collection_archive---------2-----------------------#2022-04-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bfa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2020年DeepMind <a class="ae jc" href="https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L4%20-%20UCLxDeepMind%20DL2020.pdf" rel="noopener ugc nofollow" target="_blank">系列讲座</a>第四课笔记</p><h2 id="c67a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">什么是物体检测？</h2><p id="c1ba" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">目标检测是一个分类和定位任务。它的目标是定位图像中带有边界框的对象及其相应的类。</p><p id="92e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">输入</strong>:带有一个或多个物体的图像。(RGB图像<code class="du kd ke kf kg b">H x W x 3</code>)</p><p id="7b68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">输出</strong>:针对图像中出现的所有物体(<em class="kh">图1 </em>)</p><ul class=""><li id="62b8" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated"><strong class="ig hi"> <em class="kh">分类标签</em> </strong>:分类标签的一键编码。比如:<code class="du kd ke kf kg b">0 0 0 1 0</code></li><li id="3ea5" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated"><strong class="ig hi"> <em class="kh">对象包围盒</em> </strong>:对于每个对象的位置，我们输出</li></ul><p id="3f75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kh"> (xᶜ，yᶜ，h，w) </em>其中<em class="kh"> (xᶜ，yᶜ) </em>为中心的坐标，<em class="kh"> (h，w) </em>为对应的高度和宽度。</p><figure class="kx ky kz la fd lb er es paragraph-image"><div class="er es kw"><img src="../Images/6fa6cada97904070de62d80b755f69b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*hz9OOCHyYN516FFt30lnfQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 1</strong>: The desired output for object detection</figcaption></figure><p id="a344" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如何准备数据集来训练对象检测？</strong></p><p id="ea9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们需要<strong class="ig hi"> <em class="kh"> N </em> </strong>个样本进行训练和测试。每个示例都包含一个带有对象列表的RGB图像，其中每个对象的坐标都有一个一键标签和一个边界框。</p><pre class="kx ky kz la fd li kg lj lk aw ll bi"><span id="1ee5" class="jd je hh kg b fi lm ln l lo lp">N_<em class="kh">train</em>, N_<em class="kh">test</em> samples<br/>{'image': p ∈ [0,1], <em class="kh">H</em> x <em class="kh">W</em> x 3,<br/> 'objects':<br/>  [<br/>   {'label': one_hot(N), 1 x <em class="kh">N</em>,<br/>    'bbox': <em class="kh">(xᶜ, yᶜ, h, w)</em> ∈ ℝ, 1 x 4},<br/>   {'label': one_hot(N), 1 x <em class="kh">N</em>,<br/>    'bbox': <em class="kh">(xᶜ, yᶜ, h, w)</em> ∈ ℝ, 1 x 4},<br/>   .<br/>   . <br/>   .<br/>  ]}</span></pre><p id="e497" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如何学会预测bbox坐标？</strong></p><p id="2455" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于这些包围盒的坐标是真实值，错误在分类中是不可量化的，我们需要使用回归来预测包围盒坐标。我们使用二次损失<em class="kh">(图2) </em>来反馈预测边界框的准确性。所以我们的目标是最小化样本的二次损失(均方误差)。</p><figure class="kx ky kz la fd lb er es paragraph-image"><div class="er es lq"><img src="../Images/cf1c3b9f24cc03481c4e0ae1f22e1c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*nnY15eRt7nht1res_8EhSw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 2</strong>: Quadratic loss</figcaption></figure><p id="4880" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中:</p><ul class=""><li id="e8e2" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated"><strong class="ig hi"> <em class="kh"> t </em> </strong>:地面真相</li><li id="d0f8" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated"><strong class="ig hi"> <em class="kh"> x </em> </strong>:预测</li></ul><p id="25ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，我们在一幅图像中有不止一个物体(<em class="kh">图2 </em>)。因此，我们使用两个步骤来识别我们应该与我们的预测进行比较的基本事实边界框。</p><ol class=""><li id="fb37" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb lr ko kp kq bi translated"><strong class="ig hi"> <em class="kh">分类</em> </strong>:我们先把输出值离散化成一个-hot标签来分类我们的预测属于哪个地面真值包围盒。</li><li id="028a" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb lr ko kp kq bi translated"><strong class="ig hi"> <em class="kh">回归</em> </strong>:一旦我们对地面真实边界框进行了分类，我们就可以通过回归来细化我们的预测值。</li></ol><h2 id="cfff" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">更快的R-CNN:</h2><p id="3642" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快的R-CNN </a> ( <em class="kh">图3 </em>)是由两个模块组成的两级物体探测器:</p><figure class="kx ky kz la fd lb er es paragraph-image"><div class="er es ls"><img src="../Images/397c53fe1f2c7f2222cd717828b62dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*D1pJRUiJ6_VKnjAVI1Ay5Q.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 3: </strong>Faster R-CNN: a single, unified network for object detection (<strong class="bd jf">Source:</strong> Figure from Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, Ren et al, 2016)</figcaption></figure><p id="8a2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kh"> 1。区域提议网络(</em></strong><em class="kh">RPN</em><strong class="ig hi"><em class="kh">):</em></strong>一个深度的、完全卷积的网络，它将图像(任何大小)作为输入，并预测一组矩形对象提议(候选边界框)，每个提议都有一个对象得分<em class="kh">。</em></p><p id="0f9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"><em class="kh">RPN是如何工作的？</em>T15】</strong></p><ul class=""><li id="947b" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated">离散bbox空间:<em class="kh"> (xᶜ，yᶜ，h，w)(图4) </em></li></ul><pre class="kx ky kz la fd li kg lj lk aw ll bi"><span id="4340" class="jd je hh kg b fi lm ln l lo lp">-&gt; anchor points for (<em class="kh">xᶜ, yᶜ): </em>to discretize the space of the center, choose an anchor point and distribute them uniformly over the image.<em class="kh"><br/>-&gt; </em>scales and ratios for<em class="kh"> (h, w): </em>choose candidate bbox with different scales and ratios.</span></pre><figure class="kx ky kz la fd lb er es paragraph-image"><div class="er es lt"><img src="../Images/eead60c6475ef5d8d21a26b5e1bec194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*dnLH-boyME10Oci3u-xIyw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 4</strong>: Discretizing bbox space</figcaption></figure><ul class=""><li id="7a75" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated">每个主播n个候选人:一般我们选择3个不同比例的3个不同尺度。</li><li id="dc2d" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated">预测每个bbox的对象性分数:我们训练一个分类器来预测bbox中是否有对象。</li><li id="8afa" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated">排序并保留前K名候选人</li></ul><p id="e43c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">②<em class="kh">。一个检测网络:</em> </strong>采用快速R-CNN(全连接层)使用来自RPN的建议区域作为分类和细化的输入。</p><blockquote class="lu lv lw"><p id="8d90" class="ie if kh ig b ih ii ij ik il im in io lx iq ir is ly iu iv iw lz iy iz ja jb ha bi translated"><strong class="ig hi">注:</strong> RPN和快速R-CNN独立训练，使用交替训练技术共享卷积层。</p></blockquote><h2 id="6c67" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">RetinaNet:</h2><p id="fe97" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><a class="ae jc" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"> RetinaNet </a> <em class="kh">(图5) </em>是由四个部件组成的一级物体探测器。</p><figure class="kx ky kz la fd lb er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es ma"><img src="../Images/8ee43c533a75d5c890993c41373172d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGYahG8zdL_71mU8ptLsFA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 5:</strong> The one-stage RetinaNet network architecture that uses a Feature Pyramid Network (FPN) backbone on top of a feedforward ResNet architecture.</figcaption></figure><p id="93ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kh"> 1。特征金字塔网络(FPN)主干:</em> </strong>用自上而下的路径和横向连接扩充标准卷积网络。因此，该网络从单分辨率输入图像中有效地构建了丰富的多尺度特征金字塔。</p><p id="5eb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kh"> 2。锚:</em> </strong>使用类似于RPN变体中的平移不变锚盒。每个锚被分配:</p><ul class=""><li id="583a" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated">分类目标的长度<strong class="ig hi"><em class="kh">K</em></strong>one-hot vector，其中<strong class="ig hi"> <em class="kh"> K </em> </strong>是对象类的数量。</li><li id="c726" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated">盒回归目标的四维向量。</li></ul><p id="4836" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">3<em class="kh">。分类子网:</em> </strong>一个小的全连接网络(FCN)，它为每个<strong class="ig hi"><em class="kh"/></strong>锚和<strong class="ig hi"> <em class="kh"> K </em> </strong>对象类预测每个空间位置上对象存在的概率。简而言之，它对主干网的输出执行卷积对象分类。</p><p id="bd7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kh"> 4。框回归子网:</em> </strong>另一个FCN，即<strong class="ig hi"> <em class="kh"> </em> </strong>将每个锚框的偏移量回归到附近的地面物体。简而言之，它执行卷积包围盒回归。</p><blockquote class="lu lv lw"><p id="81a2" class="ie if kh ig b ih ii ij ik il im in io lx iq ir is ly iu iv iw lz iy iz ja jb ha bi translated"><strong class="ig hi">注:</strong> RetinaNet是当前物体检测中最先进的模型。然而，这种单级检测器取得最佳结果并非基于网络设计的创新，而是由于其新颖的损耗:<strong class="ig hi"> <em class="hh">焦点损耗(FL) </em> </strong></p></blockquote><p id="6006" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是焦损失(FL)？</strong></p><p id="f157" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">焦点丢失(FL)旨在解决一阶段对象检测场景，其中在训练期间前景和背景类别之间存在极端不平衡<em class="kh">(图6)。</em>fl建立在用于二元分类的交叉熵(CE)之上，其中调制因子(<code class="du kd ke kf kg b">1-p<em class="kh">t</em></code> )ᵞ被添加到CE损失，可调聚焦参数γ <strong class="ig hi"> ≥ 0。</strong></p><figure class="kx ky kz la fd lb er es paragraph-image"><div class="er es mf"><img src="../Images/b8eea329dd977bb0350bb41ef96d1a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*ck4teqB57eNLXm5pAGgjvQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx"><strong class="bd jf">Figure 6</strong>: The Focal Loss</figcaption></figure><p id="af1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">FL用作分类子网输出的损耗。FL将训练集中在一组稀疏的硬例子上，并防止大量容易否定的例子在训练期间淹没检测机。</p><h2 id="734b" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">总结:</h2><p id="7b27" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">这篇文章探讨了分类之外的第一项任务:对象检测和旨在解决它的当前最先进的深度学习模型。</p><p id="6f3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具体而言，我们解决了:</p><ul class=""><li id="db80" class="ki kj hh ig b ih ii il im ip kk it kl ix km jb kn ko kp kq bi translated"><strong class="ig hi">物体检测</strong>:计算机视觉任务，定位图像中物体的存在，并用边界框指示它们的位置。</li><li id="a9e3" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated"><strong class="ig hi">快速R-CNN </strong>:基于区域提议网络(RPN)和快速R-CNN检测网络的两级检测器。</li><li id="51f2" class="ki kj hh ig b ih kr il ks ip kt it ku ix kv jb kn ko kp kq bi translated"><strong class="ig hi"> RetinaNet </strong>:一个一级检测器，使用焦点损失来解决训练过程中的班级不平衡问题。</li></ul><blockquote class="mg"><p id="2f18" class="mh mi hh bd mj mk ml mm mn mo mp jb dx translated"><a class="ae jc" rel="noopener" href="/@nghihuynh_37300/vision-beyond-classification-tasks-beyond-classification-task-ii-image-segmentation-5c5e81edf2b0">接下来是DeepMind深度学习系列第4讲的笔记:超越分类的视野:超越分类的任务:任务2:语义分割。</a></p></blockquote><div class="mq mr ms mt mu mv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj lc mv"/></div></div></a></div></div></div>    
</body>
</html>