<html>
<head>
<title>Building an end-to-end NLP application — A beginner friendly guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建端到端的NLP应用——初学者友好指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/building-an-end-to-end-nlp-application-an-overview-ef0221c4ab1f?source=collection_archive---------1-----------------------#2022-07-20">https://medium.com/mlearning-ai/building-an-end-to-end-nlp-application-an-overview-ef0221c4ab1f?source=collection_archive---------1-----------------------#2022-07-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9742c2f42ccc774e30f676d2bf3e0281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJ4x39copN5gVbSuLae3Tw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Credit: Alamy</figcaption></figure><p id="9a6e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">自然语言处理是语言学、计算机科学和机器学习的交叉领域。NLP的主要目标是让计算机像人类一样理解语言并产生反应。NLP的主要类别如下:</p><ul class=""><li id="fffe" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">语音识别</strong>处理口语的识别及其相应的文本翻译。</li><li id="96b4" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">自然语言理解</strong>其中计算机被训练理解人类语言。</li><li id="2206" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">自然语言生成</strong>，那里的计算机有望生成人类语言。</li></ul><p id="379e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">使用NLP可以构建各种各样的应用程序。在这篇博客中，我们将看看解决文本分类问题的一般步骤。所解决的问题是一个二元分类问题(特别是情感分析)，并让我们经历典型的机器学习(ML)生命周期的所有阶段。</p><ul class=""><li id="5826" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">问题:</strong>情感分析(监督)</li><li id="7f6d" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">使用的数据集:</strong>亚马逊产品评论</li><li id="2787" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">建模框架:</strong> <a class="ae kf" href="https://tensorflow.org" rel="noopener ugc nofollow" target="_blank">张量流</a>，<a class="ae kf" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">拥抱面</a>变形金刚</li><li id="14e7" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi"> Demo: </strong> <a class="ae kf" href="https://gradio.app" rel="noopener ugc nofollow" target="_blank"> Gradio </a></li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6051" class="kp kq hh kl b fi kr ks l kt ku"># Set the session to use GPU<br/>import tensorflow as tf<br/>import os<br/>num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))<br/>print("Num GPUs Available: ", num_gpus_available)<br/>assert num_gpus_available &gt; 0<br/><br/>sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))<br/>os.environ["CUDA_VISIBLE_DEVICES"]='0'</span><span id="52ec" class="kp kq hh kl b fi kv ks l kt ku"># Install necessary libraries<br/>!pip install gradio<br/>!pip install transformers<br/>!pip install tensorflow_datasets</span></pre><h2 id="85f7" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">机器学习工作流</h2><p id="785d" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">以下是构建任何典型的ML解决方案所涉及的阶段，</p><ul class=""><li id="250d" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">第一阶段:收集数据</li><li id="5169" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">第二阶段:</strong>探索数据</li><li id="37da" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">阶段2.5: </strong>选择型号</li><li id="d11a" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">第三阶段:准备数据</li><li id="7479" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">第四阶段:建立、训练和评估模型</li><li id="2122" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">阶段5: </strong>细化模型</li><li id="7a0f" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">阶段6: </strong>部署模型</li></ul><h2 id="c19e" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段1:收集数据</h2><p id="c73b" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">这是解决任何有监督的ML问题的最重要的一步。记住，模型和你的数据一样好。对于您试图解决的问题，有时，数据可能是可用的，但在大多数真实世界的场景中，数据并不容易获得。在这种情况下，收集数据时需要记住以下几点，</p><ul class=""><li id="afa2" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">检查是否有公共API可以获取数据。了解限制、定价、尝试次数等。API的。</li><li id="3f05" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">数据越多，模型就越好。</li><li id="6d11" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">确保每个目标类都有足够的样本。</li><li id="ebcb" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">样本应该充分涵盖你的问题的可能输入空间。</li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="1e9b" class="kp kq hh kl b fi kr ks l kt ku">import pandas as pd<br/>import numpy as np<br/>import tensorflow_datasets as tfds<br/>import matplotlib.pyplot as plt</span><span id="c97e" class="kp kq hh kl b fi kv ks l kt ku"># Load the tensorflow dataset<br/>dataset = tfds.load('amazon_us_reviews/Mobile_Electronics_v1_00', split='train', shuffle_files=True)</span></pre><h2 id="68b7" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段2:探索数据</h2><p id="922e" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">在尝试建立模型之前，必须了解手头的数据。了解数据的特征不仅有助于为我们试图解决的问题选择正确的建模技术，还有助于用高质量的数据构建更好的模型。</p><p id="f865" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">总是检查一些随机的数据样本，并获取一些关键指标，</p><ul class=""><li id="8e6b" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">可用样本总数</li><li id="c35c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">类别/目标的数量</li><li id="94bd" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">每类可用样本总数</li><li id="9cbf" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">每个样本的字数</li><li id="fbd4" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">词的频率分布</li><li id="69ff" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">样本长度分布</li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="5916" class="kp kq hh kl b fi kr ks l kt ku"># Convert the dataset to pandas dataframe<br/>dataset = tfds.as_dataframe(dataset)<br/>dataset.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="ab fe cl lu"><img src="../Images/88d003013dc17fa0e359bc8b0d3ac4d5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*yZAXRLbmft811E_OZhJw2g.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="e042" class="kp kq hh kl b fi kr ks l kt ku"># Pick the relevant columns of interest<br/>columns_of_interest = ['data/review_body', 'data/star_rating']<br/>data = dataset[columns_of_interest]<br/>data.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/ff04589d0d864aaf451456e53ee4e3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*HDAhLYflfudMoCA6Zgln4w.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="e0a8" class="kp kq hh kl b fi kr ks l kt ku"># Get the number of samples<br/>print("Total number of datapoints:", data.shape[0])</span></pre><p id="1977" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Total number of datapoints: 104975</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="b68c" class="kp kq hh kl b fi kr ks l kt ku"># Get the number of class (we will convert this the rating to sentiment in the next stage)<br/>print("Number of classes:", data['data/star_rating'].nunique())</span></pre><p id="8aaa" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Number of classes: 5</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6bf1" class="kp kq hh kl b fi kr ks l kt ku"># Get the number of samples per class<br/>print("Number of samples per class: \n" ,data['data/star_rating'].value_counts())</span></pre><p id="5f40" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Number of samples per class: <br/> 5 52255<br/>4 18088<br/>1 17587<br/>3 9734<br/>2 7311<br/>Name: data/star_rating, dtype: int64</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="258f" class="kp kq hh kl b fi kr ks l kt ku"># Get the number of words per sample<br/>print("Number of words per sample: " ,np.mean(data['data/review_body'].apply(lambda x: len(x.split()))))</span></pre><p id="8f7d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Number of words per sample: 63.97938556799238</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="64c8" class="kp kq hh kl b fi kr ks l kt ku"># Get the distribution of number of words per sample<br/>print("Distribution of number of words per sample:")<br/>pd.DataFrame([len(s) for s in data['data/review_body']]).value_counts()</span></pre><p id="707d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Distribution of number of words per sample:<br/>9 550<br/>114 534<br/>115 523<br/>113 522<br/>119 518<br/> ... <br/>2724 1<br/>2729 1<br/>2731 1<br/>2732 1<br/>16009 1<br/>Length: 3079, dtype: int64</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="98d3" class="kp kq hh kl b fi kr ks l kt ku"># Plot the distribution of of sample length<br/>def plot_sample_length_distribution(sample_texts):<br/>    """Plots the sample length distribution.<br/><br/>    # Arguments<br/>        samples_texts: list, sample texts.<br/>    """<br/><br/>    plt.hist([len(s) for s in sample_texts], 50)<br/>    plt.xlabel('Length of a sample')<br/>    plt.ylabel('Number of samples')<br/>    plt.title('Sample length distribution')<br/>    plt.show()<br/><br/>plot_sample_length_distribution(data['data/review_body'].tolist())</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="ab fe cl lu"><img src="../Images/e31de1e5fcc3df9dc6bd60fca897c785.png" data-original-src="https://miro.medium.com/v2/format:webp/1*pkeqmsoufzhBjgxzt87hgQ.png"/></div></figure><h2 id="ce44" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段2.5:选择模型</h2><p id="13c1" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">我们在前一阶段执行的数据探索有助于我们回答以下问题，</p><ul class=""><li id="78a4" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">选哪个型号？</strong> —如果数据较少，并且我们对每个类别都有足够的表示，那么使用简单的MLP或ML模型就足够了。如果您有足够的数据，并且该领域没有预先训练的模型，您可以从头开始训练序列模型。如果您在某个特定领域找到了预先训练好的模型，并希望将其应用到具有足够数据的类似领域数据中，请选择迁移学习。</li><li id="ffb0" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">我们如何高效地准备数据？</strong> —根据我们选择的模型，预处理的类型会发生变化。如果我们选择一个简单的MLP或ML模型，我们在适当的预处理后使用单词包执行矢量化。对于序列模型，我们可能需要保留数据的上下文，所以我们将使用word2vec、句子嵌入等嵌入。</li></ul><h2 id="35f4" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段3:准备数据</h2><p id="1b4c" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">数据应该被转换成模型能够理解的形式。数据准备包括两个步骤，</p><ol class=""><li id="193d" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq lz jx jy jz bi translated"><strong class="iv hi">特征工程:</strong>特征工程包括从文本数据中创建或提取特征，以便将其输入模型。在提取特征时需要完成两个步骤，</li></ol><ul class=""><li id="5de4" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">标记化:</strong>将文本转换成更小的单元，如单词(标记)或n元语法。基于唯一标记，将创建一个字典，该字典形成给定数据集的词汇表。</li><li id="43f8" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">矢量化:</strong>使用词汇表将文本数据转换成数字。用于向量化的一些常见技术是一键编码(基于标记的存在与否)、计数编码(基于标记出现的次数)和Tf-idf编码(基于分配给标记在文档中出现的频率的权重)。</li></ul><p id="8358" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><strong class="iv hi"> 2。特征选择:</strong>一旦创建了特征，我们可以通过一些统计测量使用特征重要性仔细选择相关的特征。在数量和质量方面使用正确的特性将有助于构建更好的模型。</p><h2 id="ca07" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">需要记住的要点:</h2><ul class=""><li id="4f54" class="jr js hh iv b iw lp ja lq je ma ji mb jm mc jq jw jx jy jz bi translated">始终确保数据被混洗和采样。</li><li id="ecce" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">将数据分为训练、测试和验证部分。</li><li id="0b96" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">将训练数据矢量化后保存矢量化工具，并使用它来转换测试数据。</li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="47d0" class="kp kq hh kl b fi kr ks l kt ku"># Take a sample of 50% of the dataset for the purpose of demonstration<br/>data = data.sample(frac=0.3, random_state=42)<br/>data.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es md"><img src="../Images/b316d4f083bc801f599be9cb3258cc4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*XKaztq-9juDiR7I5BsAF8g.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="971c" class="kp kq hh kl b fi kr ks l kt ku">print("Number of datapoints after sampling:", data.shape)</span></pre><p id="20b1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Number of datapoints after sampling: (31492, 2)</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="9ce9" class="kp kq hh kl b fi kr ks l kt ku"># Convert the star rating to sentiment<br/>data['sentiment'] = data['data/star_rating'].apply(lambda rating: "positive" if rating &gt;= 3 else "negative")<br/>data.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es me"><img src="../Images/1bdd079de871c1ee54b7dd39b122730a.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*QjKmtGf6eIsadHQc0mGslA.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="e1e7" class="kp kq hh kl b fi kr ks l kt ku"># One-hot encode the target label<br/>data['sentiment'] = data['sentiment'].map({'positive':1, 'negative':0})<br/><br/># Drop the rating column<br/>data.drop(columns=['data/star_rating'], inplace=True)<br/>data.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/2e196653967a483b206d80a149b0c864.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*kx6FOCOGHvehB7FhV8r1Gg.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="647a" class="kp kq hh kl b fi kr ks l kt ku"># Preprocess the reviews<br/>data.rename(columns={'data/review_body': 'review'}, inplace=True)<br/>data['review'] = data['review'].str.decode('utf-8')<br/>data['review'] = data["review"].apply(str.lower)<br/>data['review']<br/><br/>data.head()</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/be43660e7c02a780bcdb9eabae06e623.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*_nYb4kfRf8evbONtqMFCzw.png"/></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="b07d" class="kp kq hh kl b fi kr ks l kt ku"># Check the samples per class<br/>data['sentiment'].value_counts()</span></pre><p id="2a6a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">1 24067<br/>0 7425<br/>Name: sentiment, dtype: int64</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="13df" class="kp kq hh kl b fi kr ks l kt ku"># Balance out the classes for the purpose of demonstration<br/>positive = data[data['sentiment'] == 1].sample(n=7425, random_state=42)<br/>negative = data[data['sentiment'] == 0]<br/>train_data = pd.concat([positive, negative]).sample(frac=1).reset_index(drop=True)<br/>train_data['sentiment'].value_counts()</span></pre><p id="59d7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">0 7425<br/>1 7425<br/>Name: sentiment, dtype: int64</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="f310" class="kp kq hh kl b fi kr ks l kt ku"># Sample reviews and its corresponding labels<br/>reviews = train_data['review'].tolist()<br/>labels = train_data['sentiment'].tolist()<br/><br/>print("Sample reviews:")<br/>print(reviews[10:16])<br/>print("Corresponding labels:")<br/>print(labels[10:16])</span></pre><p id="c583" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Sample reviews:<br/>['does not work, waste of my money, what more can i say...requiring more words it ridiculous...four more words then and', "i ordered a black case as well as a pink. nicely packaged, but the black came very dirty and i noticed it was prone to get a lot of noticeable dust/dirt on it even if it's just sitting on my counter. the &amp;#34;flaps&amp;#34; covering the camera, headphone jack, and charging area are very weak and look like they could be easily broken. however, the case is water resistant and it is definitely shockproof. i drop tested this on concrete and my ipod was perfectly fine, so that's where the forth star comes in. my only real problem with this is the weak flaps and the dust, but the appearance can be fixed by just getting a lighter color.", 'great price, hits hard, clean sound. my only issue with it was when i removed one of the rca cables it pulled the grounding collar off the amp. no big deal, easy work around but i dont think that should have happened. i would buy this amp again.', 'nice', "i expected it to be bad, i mean it's 20$. the only reason i wouldn't buy it again is because it doesn't use 3.5mm jack.", 'this looked to be a nice skin but it in no way fits my ultrahd flip. it is huge, not fitted, sloppy and moves around, covers parts of lens. not something i would recommend to anyone.']<br/>Corresponding labels:<br/>[0, 1, 1, 1, 0, 0]</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="5914" class="kp kq hh kl b fi kr ks l kt ku"># Split the data into train, test and vvalidation split<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test  = train_test_split(reviews, labels, test_size=.2, random_state=42)</span><span id="0587" class="kp kq hh kl b fi kv ks l kt ku"># Number of train and test data<br/>print("Number of train data points:", len(X_train))<br/>print("Number of train labels:", len(y_train))<br/>print("Number of test data points:", len(X_test))<br/>print("Number of test labels:", len(y_test))</span></pre><p id="92e6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Number of train data points: 11880<br/>Number of train labels: 11880<br/>Number of test data points: 2970<br/>Number of test labels: 2970</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="c585" class="kp kq hh kl b fi kr ks l kt ku"># Load the tokenizer<br/>from transformers import DistilBertTokenizerFast<br/>MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'<br/>tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)</span><span id="815d" class="kp kq hh kl b fi kv ks l kt ku"># Sample tokenization<br/>print("Sentence:" , X_train[0])<br/>tokenizer(X_train[0], truncation=True, padding=True, max_length=256, return_tensors="tf")</span></pre><p id="45dc" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Sentence: unit sounds okay when it works but, unfortunately, it mutes itself every time and then cuts in and out whether on bluetooth or aux cable. this is regardless of which computer i connect it to, how close the speaker is to the unit, etc. noticing that other reviewers have the same problem. maybe some units are defective. if it didn't have that problem, i would have probably given it 5 stars. haven't begun the return process (will do later today), so can't comment on the customer service quality yet.<br/><br/>{'input_ids': &lt;tf.Tensor: shape=(1, 113), dtype=int32, numpy=<br/>array([[ 101, 3131, 4165, 3100, 2043, 2009, 2573, 2021, 1010,<br/> 6854, 1010, 2009, 20101, 2015, 2993, 2296, 2051, 1998,<br/> 2059, 7659, 1999, 1998, 2041, 3251, 2006, 2630, 19392,<br/> 2030, 19554, 5830, 1012, 2023, 2003, 7539, 1997, 2029,<br/> 3274, 1045, 7532, 2009, 2000, 1010, 2129, 2485, 1996,<br/> 5882, 2003, 2000, 1996, 3131, 1010, 4385, 1012, 15103,<br/> 2008, 2060, 15814, 2031, 1996, 2168, 3291, 1012, 2672,<br/> 2070, 3197, 2024, 28829, 1012, 2065, 2009, 2134, 1005,<br/> 1056, 2031, 2008, 3291, 1010, 1045, 2052, 2031, 2763,<br/> 2445, 2009, 1019, 3340, 1012, 4033, 1005, 1056, 5625,<br/> 1996, 2709, 2832, 1006, 2097, 2079, 2101, 2651, 1007,<br/> 1010, 2061, 2064, 1005, 1056, 7615, 2006, 1996, 8013,<br/> 2326, 3737, 2664, 1012, 102]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1, 113), dtype=int32, numpy=<br/>array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/> 1, 1, 1]], dtype=int32)&gt;}</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="a00b" class="kp kq hh kl b fi kr ks l kt ku"># Perform tokenization<br/>MAX_LENGTH=256<br/>train_encodings = tokenizer(X_train,<br/>                            truncation=True, <br/>                            padding=True,<br/>                            return_tensors="tf",<br/>                            max_length=MAX_LENGTH)<br/>test_encodings = tokenizer(X_test,<br/>                           truncation=True, <br/>                           padding=True,<br/>                           return_tensors="tf",<br/>                           max_length=MAX_LENGTH)</span><span id="6eb2" class="kp kq hh kl b fi kv ks l kt ku"># Convert the tokenizer to TF object<br/>train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),<br/>                                                    y_train))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),<br/>                                                   y_test))</span></pre><h2 id="df0c" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段4:构建、训练和评估模型</h2><p id="aff0" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">为了演示的目的，迁移学习被用于为手头的情感分析任务建立模型。我们将使用在SST-2(斯坦福情感树库v2)数据集上微调的预训练蒸馏模型。关于该模型的更多细节可以在这里找到:<a class="ae kf" href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/distil Bert-base-un cased-fine tuned-SST-2-English</a></p><p id="42a0" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在我们构建任何模型之前，需要考虑以下关键参数，</p><ul class=""><li id="39a9" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated"><strong class="iv hi">指标:</strong>我们如何衡量模型的性能？正确的衡量标准应该是什么？—这里可以使用精度。</li><li id="0c27" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">损失函数:</strong>是训练时学习到的目标函数。一个好的模型应该总是试图最小化损失函数。对于分类问题，交叉熵损失可以用作目标或损失函数。</li><li id="8a50" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated"><strong class="iv hi">优化器:</strong>根据损失函数的输出决定如何更新网络权重的函数。</li></ul><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="bb1d" class="kp kq hh kl b fi kr ks l kt ku"># Define the model - use pretrained distilbert model and specify the label<br/>from transformers import TFDistilBertForSequenceClassification<br/>model = TFDistilBertForSequenceClassification.from_pretrained(<br/>    pretrained_model_name_or_path=MODEL_NAME,<br/>    num_labels=2<br/>    )</span></pre><p id="fbf8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']<br/>- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).<br/>- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).<br/>Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_219']<br/>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="9edb" class="kp kq hh kl b fi kr ks l kt ku"># Specify the model optimizer<br/>optimizer = tf.keras.optimizers.Adam(<br/>            learning_rate=5e-5, <br/>        )</span><span id="ea92" class="kp kq hh kl b fi kv ks l kt ku"># Specify the loss function<br/>loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="712e" class="kp kq hh kl b fi kv ks l kt ku"># Compile the model<br/><br/>model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])</span><span id="fc57" class="kp kq hh kl b fi kv ks l kt ku"># Inspect the model<br/>model.summary()</span></pre><p id="6ac9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Model: "tf_distil_bert_for_sequence_classification_10"<br/>_________________________________________________________________<br/> Layer (type) Output Shape Param # <br/>=================================================================<br/> distilbert (TFDistilBertMai multiple 66362880 <br/> nLayer) <br/> pre_classifier (Dense) multiple 590592 <br/> classifier (Dense) multiple 1538 <br/> dropout_219 (Dropout) multiple 0 <br/>=================================================================<br/>Total params: 66,955,010<br/>Trainable params: 66,955,010<br/>Non-trainable params: 0<br/>_________________________________________________________________</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="bb50" class="kp kq hh kl b fi kr ks l kt ku"># Define hyperparameters<br/>BATCH_SIZE = 16<br/>NUM_EPOCHS = 2<br/><br/># Train the model<br/>model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),<br/>          epochs=NUM_EPOCHS,<br/>          batch_size=BATCH_SIZE,<br/>          validation_data=test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE)<br/>          )</span></pre><p id="2f50" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">Epoch 1/2<br/>743/743 [==============================] - 338s 444ms/step - loss: 0.2718 - accuracy: 0.8890 - val_loss: 0.2726 - val_accuracy: 0.8855<br/>Epoch 2/2<br/>743/743 [==============================] - 330s 445ms/step - loss: 0.1659 - accuracy: 0.9385 - val_loss: 0.2430 - val_accuracy: 0.9077<br/>&lt;keras.callbacks.History at 0x7fd95c251210&gt;</code></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="8957" class="kp kq hh kl b fi kr ks l kt ku"># Save the model<br/>model.save_pretrained("/tmp/sentiment_model")</span><span id="2509" class="kp kq hh kl b fi kv ks l kt ku"># Preprocess the unseen data<br/>unseen_data = data.sample(frac=0.05, random_state=42)<br/>unseen_reviews = unseen_data['review'].tolist()<br/><br/>unseen_encodings = tokenizer(unseen_reviews,<br/>                            padding=True,<br/>                            truncation=True,<br/>                            max_length=MAX_LENGTH,<br/>                            return_tensors="tf")<br/><br/>y_unseen = unseen_data['sentiment'].tolist()<br/><br/>unseen_encodings = tf.data.Dataset.from_tensor_slices((dict(unseen_encodings),<br/>                                                       y_unseen<br/>                                                      ))</span><span id="9554" class="kp kq hh kl b fi kv ks l kt ku"># Evaluate the model<br/>model.evaluate(unseen_encodings.shuffle(len(unseen_reviews))<br/>               .batch(BATCH_SIZE),<br/>               return_dict=True,<br/>               batch_size=BATCH_SIZE)</span></pre><p id="1f43" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><code class="du lw lx ly kl b">99/99 [==============================] - 14s 139ms/step - loss: 0.1880 - accuracy: 0.9371<br/>{'accuracy': 0.9371428489685059, 'loss': 0.18796877562999725}</code></p><h2 id="0860" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段5:优化模型</h2><p id="6289" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">一旦模型被构建和训练，结果有时可能不令人满意(我们可能无法从模型中获得预期的性能)。模型可能过拟合或欠拟合，这取决于几个因素。为了改进模型，我们可以进行超参数调整。一些超参数如下，</p><ul class=""><li id="14fd" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">模型中的层数(隐藏层)-层数太多会导致过度拟合，层数太少会导致拟合不足。</li><li id="de5b" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">每个图层的单位数-输入图层应与输入大小相匹配，隐藏图层中的单位数可根据您希望如何表示数据来决定。</li><li id="b3ce" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">辍学率——惩罚模型过度拟合。</li><li id="fa9c" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">学习率—网络收敛到最优权重值的速度。</li></ul><p id="2799" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">您可以尝试超参数的不同配置，看看哪种配置最适合您的模型。</p><h2 id="676a" class="kp kq hh bd kw kx ky kz la lb lc ld le je lf lg lh ji li lj lk jm ll lm ln lo bi translated">阶段6:部署模型</h2><p id="a316" class="pw-post-body-paragraph it iu hh iv b iw lp iy iz ja lq jc jd je lr jg jh ji ls jk jl jm lt jo jp jq ha bi translated">部署模型有几种选择。您还可以使用量化方法优化训练模型，以便将其部署在资源受限的环境中。一些可用的选项是，</p><ol class=""><li id="2170" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq lz jx jy jz bi translated"><a class="ae kf" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank">张量流发球</a></li><li id="79ce" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq lz jx jy jz bi translated"><a class="ae kf" href="https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html" rel="noopener ugc nofollow" target="_blank"> Pytorch闪电</a></li><li id="2563" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq lz jx jy jz bi translated">转换成<a class="ae kf" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>格式并选择你所选择的框架</li><li id="9d24" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq lz jx jy jz bi translated">使用任何REST API框架，如<a class="ae kf" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>或<a class="ae kf" href="https://dev.to/brandonwallace/deploy-flask-the-easy-way-with-gunicorn-and-nginx-jgc" rel="noopener ugc nofollow" target="_blank"> Flask with Gunicorn </a>对您的模型进行对接和部署</li></ol><p id="16f1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你的工作不会就此结束！！</p><ul class=""><li id="f93b" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">在生产过程中，您必须监控模型的任何性能下降和漂移(模型或数据)。定期检查。</li><li id="c67b" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">通过反馈在模型生产过程中收集更多数据。</li><li id="3f55" class="jr js hh iv b iw ka ja kb je kc ji kd jm ke jq jw jx jy jz bi translated">如果您发现模型正在退化，那么您将不得不使用从反馈中收集的数据来重新训练您的模型。</li></ul><p id="4c9a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了演示的目的，让我们看看如何使用notebook作为一个界面来使用Gradio获得预测。以下代码将在笔记本中启动一个Gradio应用程序。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="69c5" class="kp kq hh kl b fi kr ks l kt ku">import gradio as gr<br/><br/>def predict(sentence):<br/>    # Load model<br/>    loaded_model = TFDistilBertForSequenceClassification.from_pretrained("/tmp/sentiment_model")<br/><br/>    # Get the prediction<br/>    predict_input = tokenizer.encode(sentence,<br/>                                 truncation=True,<br/>                                 padding=True,<br/>                                 return_tensors="tf")<br/><br/>    tf_output = loaded_model.predict(predict_input)[0]<br/>    tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]<br/>    return ['negative', 'positive'][np.argmax(tf_prediction)]<br/><br/>demo = gr.Interface(fn=predict, inputs="text", outputs="text",live=True)<br/><br/>demo.launch()</span><span id="258e" class="kp kq hh kl b fi kv ks l kt ku">Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`<br/>Running on public URL: https://17805.gradio.app<br/><br/>This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)</span></pre><figure class="kg kh ki kj fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/3f1117b2e5499fc3907e8e72950976b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*AAQ9skE7uYagasV-cskCPA.png"/></div></figure><p id="6588" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">笔记本可以从<a class="ae kf" href="https://github.com/AbinayaM02/WiDS_Mysuru_2022/blob/main/building-an-end-to-end-nlp-application.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><ul class=""><li id="b50a" class="jr js hh iv b iw ix ja jb je jt ji ju jm jv jq jw jx jy jz bi translated">* <em class="mi">注:整个例子是参照</em> <a class="ae kf" href="https://developers.google.com/machine-learning/guides/text-classification" rel="noopener ugc nofollow" target="_blank"> <em class="mi">文本分类指南</em> </a> <em class="mi">构建的。请仔细阅读指南，了解更多关于文本分类的详细信息。祝阅读愉快，并请分享您的反馈！</em></li></ul><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na in mm"/></div></div></a></div></div></div>    
</body>
</html>