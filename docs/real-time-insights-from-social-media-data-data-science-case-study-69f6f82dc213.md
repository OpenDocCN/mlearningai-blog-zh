# æ¥è‡ªç¤¾äº¤åª’ä½“æ•°æ®çš„å®æ—¶æ´å¯Ÿâ€”æ•°æ®ç§‘å­¦æ¡ˆä¾‹ç ”ç©¶

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/real-time-insights-from-social-media-data-data-science-case-study-69f6f82dc213?source=collection_archive---------3----------------------->

![](img/9d1dd65692c8523712166b4ecc01db5e.png)

# åœ¨è¿™ç¯‡åšå®¢ä¸­:

*   æœ¬åœ°å’Œå…¨çƒæ€ç»´æ¨¡å¼
*   ç¾åŒ–è¾“å‡º
*   å¯»æ‰¾å…±åŒè¶‹åŠ¿
*   æ¢ç´¢çƒ­é—¨è¶‹åŠ¿
*   æ·±å…¥æŒ–æ˜
*   é¢‘ç‡åˆ†æ
*   å›´ç»•è¶‹åŠ¿çš„æ´»åŠ¨
*   ä¸€å¼ èƒ½è¯´å‡º 1000 ä¸ªå•è¯çš„æ¡Œå­
*   åˆ†æä½¿ç”¨çš„è¯­è¨€
*   æœ€åçš„æƒ³æ³•

# æœ¬åœ°å’Œå…¨çƒæ€ç»´æ¨¡å¼

è™½ç„¶æˆ‘ä»¬å¯èƒ½ä¸æ˜¯ twitter çš„ç²‰ä¸ï¼Œä½†æˆ‘ä»¬ä¸å¾—ä¸æ‰¿è®¤å®ƒå¯¹ä¸–ç•Œæœ‰ç€å·¨å¤§çš„å½±å“ã€‚Twitter æ•°æ®ä¸ä»…åœ¨æ´å¯ŸåŠ›æ–¹é¢æ˜¯å¤šå€çš„ï¼Œè€Œä¸” Twitter é£æš´å¯ç”¨äºè¿‘ä¹å®æ—¶çš„åˆ†æã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥äº†è§£ä¸–ç•Œå„åœ°å‡ºç°çš„æ€æƒ³å’Œæƒ…ç»ªçš„å¤§æµªæ½®ã€‚

å°±åƒä»»ä½•ä¸€ä¸ªå……æ»¡è´¢å¯Œçš„åœ°æ–¹ä¸€æ ·ï¼ŒTwitter æœ‰å®‰å…¨è­¦å«*é˜»æ­¢æˆ‘ä»¬é©¬ä¸Šæ‹¿åˆ°æ•°æ®ï¼Œâ›”ï¸éœ€è¦ä¸€äº›è®¤è¯æ­¥éª¤(çœŸçš„å¾ˆç®€å•)æ¥è°ƒç”¨ä»–ä»¬çš„ API è¿›è¡Œæ•°æ®æ”¶é›†ã€‚ç”±äºæˆ‘ä»¬ä»Šå¤©çš„ç›®æ ‡æ˜¯å­¦ä¹ ä»æ•°æ®ä¸­æå–è§è§£ï¼Œæˆ‘ä»¬å·²ç»è·å¾—äº†âœ…å®‰å…¨éƒ¨çš„ç»¿è‰²é€šè¡Œè¯ï¼Œæˆ‘ä»¬çš„æ•°æ®å·²ç»å¯ä»¥åœ¨æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­ä½¿ç”¨äº†ï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºæœ‰è¶£çš„éƒ¨åˆ†äº†ï¼ğŸ•µï¸â€â™€ï¸ğŸŒ*

Twitter æä¾›å…¨çƒå’Œæœ¬åœ°è¶‹åŠ¿ã€‚è®©æˆ‘ä»¬åŠ è½½å¹¶æ£€æŸ¥æŸ¥è¯¢æ—¶å…¨çƒå’Œç¾å›½çƒ­é—¨è¯é¢˜çš„æ•°æ®â€”â€”å¯¹ Twitter çš„ GET trends/place API è°ƒç”¨çš„ JSON å“åº”çš„å¿«ç…§ã€‚

**æ³¨æ„:** [è¿™é‡Œ](https://developer.twitter.com/en/docs/twitter-api/v1/trends/trends-for-location/api-reference/get-trends-place)æ˜¯è¿™æ¬¡é€šè¯çš„æ–‡æ¡£ï¼Œè¿™é‡Œ[æ˜¯ Twitter API çš„å®Œæ•´æ¦‚è¿°ã€‚](https://developer.twitter.com/en/docs/api-reference-index)

```
import json # Load WW_trends and US_trends data into the the given variables respectivelyWW_trends = json.loads(open('/content/WWTrends.json').read())US_trends = json.loads(open('/content/USTrends.json').read())
```

# ç¾åŒ–è¾“å‡º

æˆ‘ä»¬çš„æ•°æ®å¾ˆéš¾é˜…è¯»ï¼å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ© jason.dumps()æ–¹æ³•å°†å…¶æ ¼å¼åŒ–ä¸ºä¸€ä¸ªæ¼‚äº®çš„ JSON å­—ç¬¦ä¸²ã€‚

```
# Pretty-printing the results. First WW and then US trends.
print("WW trends:", WW_trends)print("\n", "US trends:", US_trends)
```

# å¯»æ‰¾å…±åŒè¶‹åŠ¿

ğŸ•µï¸â€â™€ï¸ä»æ¼‚äº®æ‰“å°çš„ç»“æœ(å‰ä¸€ä¸ªä»»åŠ¡çš„è¾“å‡º)ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°:

*   æˆ‘ä»¬æœ‰ä¸€ä¸ªè¶‹åŠ¿å¯¹è±¡æ•°ç»„ï¼Œå…¶ä¸­åŒ…å«:è¶‹åŠ¿ä¸»é¢˜çš„åç§°ã€å¯ç”¨äºåœ¨ Twitter-Search ä¸Šæœç´¢ä¸»é¢˜çš„æŸ¥è¯¢å‚æ•°ã€æœç´¢ URL å’Œè¿‡å» 24 å°æ—¶çš„æ¨æ–‡é‡(å¦‚æœå¯ç”¨)ã€‚(è¶‹åŠ¿æ¯ 5 åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ã€‚)
*   åœ¨æŸ¥è¯¢æ—¶é—´ ***#BeratKandiliã€#GoodFriday*** å’Œ***# we love the earth***æ˜¯è¶‹åŠ¿ WWã€‚
*   *â€œtweet _ volumeâ€*å‘Šè¯‰æˆ‘ä»¬*# welovethearth*æ˜¯ä¸‰è€…ä¸­æœ€å—æ¬¢è¿çš„ã€‚
*   ç»“æœæ²¡æœ‰æŒ‰ç…§*â€œtweet _ volumeâ€*æ’åºã€‚
*   æœ‰äº›è¶‹åŠ¿æ˜¯ç¾å›½ç‹¬æœ‰çš„ã€‚

æµè§ˆä¸¤ç»„è¶‹åŠ¿å¹¶æ‰¾å‡ºå…±åŒè¶‹åŠ¿æ˜¯å¾ˆå®¹æ˜“çš„ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸è¦åšâ€œæ‰‹å·¥â€å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python çš„ set æ•°æ®ç»“æ„æ¥å¯»æ‰¾å…±åŒçš„è¶‹åŠ¿â€”â€”æˆ‘ä»¬å¯ä»¥éå†ä¸¤ä¸ª trends å¯¹è±¡ï¼Œå°†åç§°åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆï¼Œå¹¶è°ƒç”¨ intersection æ–¹æ³•æ¥è·å¾—ä¸¤ä¸ªé›†åˆä¹‹é—´çš„å…±åŒåç§°ã€‚

```
# Extracting all the WW trend names from WW_trends
world_trends = set([trend['name'] for trend in WW_trends[0]['trends']])# Extracting all the US trend names from US_trends
us_trends = set([trend['name'] for trend in US_trends[0]['trends']])# Getting the intersection of the two sets of trends
common_trends = world_trends.intersection(us_trends)# Inspecting the data
print(world_trends, "\n")
print(us_trends, "\n")
print (len(common_trends), "common trends:", common_trends)
```

# æ¢ç´¢çƒ­é—¨è¶‹åŠ¿

ğŸ•µï¸â€â™€ï¸ä»äº¤é›†(æœ€åä¸€ä¸ªè¾“å‡º)æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ä¸¤ç»„è¶‹åŠ¿(æ¯ç»„å¤§å°ä¸º 50)ä¸­ï¼Œæˆ‘ä»¬æœ‰ 11 ä¸ªé‡å çš„ä¸»é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ‰ä¸€ä¸ªå¬èµ·æ¥éå¸¸æœ‰è¶£çš„å…±åŒè¶‹åŠ¿:***# we love the Earth***â€”â€”å¾ˆé«˜å…´çœ‹åˆ°*æ¨ç‰¹ç½‘å‹*ä¸€è‡´è°ˆè®ºçƒ­çˆ±åœ°çƒæ¯äº²ï¼ğŸ’š

***æ³¨æ„*** *:æˆ‘ä»¬æœ¬æ¥å¯ä»¥æ²¡æœ‰é‡å æˆ–è€…æœ‰æ›´é«˜çš„é‡å ï¼›å½“æˆ‘ä»¬æŸ¥è¯¢è¶‹åŠ¿æ—¶ï¼Œç¾å›½äººå¯èƒ½å·²ç»å¯¹åªä¸ä»–ä»¬ç›¸å…³çš„è¯é¢˜å¦‚ç«å¦‚è¼äº†ã€‚*

```
# Extracting all the WW trend names from WW_trends
world_trends = set([trend['name'] for trend in WW_trends[0]['trends']])# Extracting all the US trend names from US_trends
us_trends = set([trend['name'] for trend in US_trends[0]['trends']])# Getting the intersection of the two sets of trends
common_trends = world_trends.intersection(us_trends)# Inspecting the data
print(world_trends, "\n")
print(us_trends, "\n")
print (len(common_trends), "common trends:", common_trends)
```

![](img/d64b8d27b5fa09a2d5a6f14e7d3fedb1.png)

*Image Source:Official Music Video Cover:* [*https://welovetheearth.org/video/*](https://welovetheearth.org/video/)

æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæµè¡Œè¶‹åŠ¿ï¼Œ#æˆ‘ä»¬çˆ±åœ°çƒã€‚ç°åœ¨æ¥çœ‹çœ‹å®ƒå°–å«ç€è¦å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆæ•…äº‹ï¼
å¦‚æœæˆ‘ä»¬ç”¨è¿™ä¸ª hashtag ä½œä¸ºæŸ¥è¯¢å‚æ•°æŸ¥è¯¢ Twitter çš„æœç´¢ APIï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸ä¹‹ç›¸å…³çš„å®é™… tweetsã€‚æˆ‘ä»¬å°†æœç´¢ API çš„å“åº”å­˜å‚¨åœ¨ datasets æ–‡ä»¶å¤¹ä¸­ï¼Œåä¸º*â€˜welovethearth . JSONâ€™*ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬åŠ è½½è¿™ä¸ªæ•°æ®é›†ï¼Œå¹¶æ·±å…¥ç ”ç©¶è¿™ä¸€è¶‹åŠ¿ã€‚

```
# Loading the data
tweets = json.loads(open('/content/WeLoveTheEarth.json').read())# Inspecting some tweets
tweets[0:2]
```

# æ·±å…¥æŒ–æ˜

ğŸ•µï¸â€â™€ï¸æ‰“å°çš„å‰ä¸¤æ¡æ¨æ–‡è®©æˆ‘ä»¬æ„è¯†åˆ°ï¼Œä¸€æ¡æ¨æ–‡æ¯”æˆ‘ä»¬é€šå¸¸è®¤ä¸ºçš„æ¨æ–‡æœ‰æ›´å¤šçš„å†…å®¹â€”â€”ä¸ä»…ä»…æ˜¯ä¸€ç¯‡çŸ­æ–‡ï¼

ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬ä¸è¦è¢«ä¸€ä¸ª tweet å¯¹è±¡ä¸­çš„æ‰€æœ‰ä¿¡æ¯æ·¹æ²¡ï¼è®©æˆ‘ä»¬å…³æ³¨å‡ ä¸ªæœ‰è¶£çš„é¢†åŸŸï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½åœ¨é‚£é‡Œæ‰¾åˆ°éšè—çš„è§è§£ã€‚

```
# Extracting the text of all the tweets from the tweet object
texts = [tweet['text'] for tweet in tweets]# Extracting screen names of users tweeting about #WeLoveTheEarth
names = [user_mention['screen_name'] for tweet in tweets for user_mention in tweet['entities']['user_mentions']]# Extracting all the hashtags being used when talking about this topic
hashtags = [hashtag['text'] for tweet in tweets for hashtag in tweet['entities']['hashtags']]# Inspecting the first 10 results
print (json.dumps(texts[0:10], indent=1),"\n")
print (json.dumps(names[0:10], indent=1),"\n")
print (json.dumps(hashtags[0:10], indent=1),"\n")
```

# é¢‘ç‡åˆ†æ

ä»…ä»…ä»ğŸ•µï¸â€â™€ï¸æœ€åæå–çš„å‰å‡ ä¸ªç»“æœä¸­ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ¨æ–­å‡º:

*   æˆ‘ä»¬æ­£åœ¨è°ˆè®ºä¸€é¦–å…³äºçƒ­çˆ±åœ°çƒçš„æ­Œæ›²ã€‚
*   è®¸å¤šå¤§è‰ºæœ¯å®¶æ˜¯è¿™è‚¡ Twitter æµªæ½®èƒŒåçš„åŠ›é‡ï¼Œå°¤å…¶æ˜¯ Lil Dickyã€‚
*   è‰¾å¾·Â·å¸Œå…°æ˜¯æ­Œæ›²ä¸­å¯çˆ±çš„è€ƒæ‹‰â€”â€”â€œEdSheeranTheKoalaâ€æ ‡ç­¾ï¼ğŸ¨

è§‚å¯Ÿæ„Ÿå…´è¶£é¢†åŸŸçš„å‰ 10 é¡¹ç»™äº†æˆ‘ä»¬å¯¹æ•°æ®çš„æ„Ÿè§‰ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡åšä¸€ä¸ªç®€å•ä½†éå¸¸æœ‰ç”¨çš„ç»ƒä¹ â€”â€”è®¡ç®—é¢‘ç‡åˆ†å¸ƒâ€”â€”æ¥è¿›ä¸€æ­¥äº†è§£ã€‚ä»é¢‘ç‡å¼€å§‹é€šå¸¸æ˜¯ä¸€ä¸ªå¥½æ–¹æ³•ï¼›è¿™æœ‰åŠ©äºè·å¾—å¦‚ä½•è¿›ä¸€æ­¥å‘å±•çš„æƒ³æ³•ã€‚

```
# Importing modules
from collections import Counter# Counting occcurrences/ getting frequency dist of all names and hashtagsfor item in [names, hashtags]:
    c = Counter(item) # Inspecting the 10 most common items in c
    print (c.most_common(10), "\n")
```

# å›´ç»•è¶‹åŠ¿çš„æ´»åŠ¨

ğŸ•µï¸â€â™€ï¸æ ¹æ®æœ€è¿‘çš„é¢‘ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å»ºç«‹æˆ‘ä»¬çš„æ‰£é™¤:

*   æˆ‘ä»¬å¯ä»¥æ›´æœ‰æŠŠæ¡åœ°è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºåœ°çƒçš„éŸ³ä¹è§†é¢‘(æ ‡ç­¾ä¸ºâ€œEarthMusicVideoâ€)ï¼Œä½œè€…æ˜¯ Lil Dickyã€‚
*   è¿ªå¡æ™®é‡Œå¥¥ä¸æ˜¯éŸ³ä¹è‰ºæœ¯å®¶ï¼Œä½†ä»–ä¹Ÿå‚ä¸å…¶ä¸­*(åˆ©å¥¥æ˜¯ä¸€åç¯ä¿ä¸»ä¹‰è€…ï¼Œæ‰€ä»¥çœ‹åˆ°ä»–çš„åå­—å‡ºç°åœ¨è¿™é‡Œå¹¶ä¸å¥‡æ€ª)*ã€‚
*   æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´è§†é¢‘æ˜¯åœ¨æŸä¸ªæ˜ŸæœŸäº”å‘å¸ƒçš„ï¼›å¾ˆæœ‰å¯èƒ½æ˜¯ 4 æœˆ 19 æ—¥ã€‚

*æˆ‘ä»¬å·²ç»èƒ½å¤Ÿæå–å¦‚æ­¤å¤šçš„æ´è§ã€‚æŒºå‰å®³çš„å§ï¼Ÿï¼*

è®©æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†ææ•°æ®ï¼Œæ‰¾å‡ºæ¨æ–‡å‘¨å›´çš„æ´»åŠ¨æ¨¡å¼â€” **æ˜¯å¦æ‰€æœ‰çš„è½¬å‘éƒ½å‘ç”Ÿåœ¨ç‰¹å®šæ¨æ–‡å‘¨å›´ï¼Ÿ**

å¦‚æœä¸€æ¡æ¨æ–‡è¢«è½¬å‘ï¼Œé‚£ä¹ˆ*â€˜retweed _ statusâ€™*å­—æ®µä¼šç»™å‡ºè®¸å¤šå…³äºåŸå§‹æ¨æ–‡æœ¬èº«åŠå…¶ä½œè€…çš„æœ‰è¶£ç»†èŠ‚ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ†æ ***retweetcount*** å’Œ ***favoritecount*** å­—æ®µæ¥è¡¡é‡ä¸€æ¡æ¨æ–‡çš„å—æ¬¢è¿ç¨‹åº¦ã€‚ä½†æ˜¯è®©æˆ‘ä»¬ä¹Ÿæå–ä¸€ä¸‹æ¨ç‰¹ç²‰ä¸çš„æ•°é‡â€”â€”æˆ‘ä»¬æœ‰å¾ˆå¤šåäººï¼Œæ‰€ä»¥**æˆ‘ä»¬èƒ½çŸ¥é“ä»–ä»¬å¯¹#WeLoveTheEarth çš„å€¡å¯¼æ˜¯å¦å½±å“äº†å¾ˆå¤§ä¸€éƒ¨åˆ†ç²‰ä¸å—ï¼Ÿ**

***æ³¨****:retweet _ count ç»™å‡ºäº†åŸå§‹æ¨æ–‡è¢«è½¬å‘çš„æ€»æ¬¡æ•°ã€‚åœ¨æœ€åˆçš„æ¨æ–‡å’Œæ‰€æœ‰åç»­çš„è½¬å‘ä¸­åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚æ‘†å¼„ä¸€äº›æ ·æœ¬æ¨æ–‡å’Œå®˜æ–¹æ–‡æ¡£æ˜¯è®©ä½ äº†è§£ mnay é¢†åŸŸçš„æ–¹æ³•ã€‚*

```
retweets = [ (tweet['retweet_count'], tweet['retweeted_status']['favorite_count'], tweet['retweeted_status']['user']['followers_count'], tweet['retweeted_status']['user']['screen_name'], tweet['text']) for tweet in tweets if 'retweeted_status' in tweet]
```

# ä¸€å¼ èƒ½è¯´å‡º 1000 ä¸ªå•è¯çš„æ¡Œå­

è®©æˆ‘ä»¬è¿›ä¸€æ­¥æ“ä½œæ•°æ®ï¼Œå¹¶ä»¥æ›´å¥½ã€æ›´ä¸°å¯Œçš„æ–¹å¼å°†å…¶å¯è§†åŒ–â€” *â€œçœ‹èµ·æ¥å¾ˆé‡è¦ï¼â€*

```
# Importing modules
import matplotlib.pyplot as plt
import pandas as pd# Create a DataFrame and visualize the data in a pretty and insightful format
df = pd.DataFrame(retweets, columns['Retweets','Favorites','Followers','ScreenName','Text']).groupby(['ScreenName','Text','Followers']).sum().sort_values(by=['Followers'], ascending=False)df.style.background_gradient()
```

# åˆ†æä½¿ç”¨çš„è¯­è¨€

ğŸ•µï¸â€â™€ï¸æˆ‘ä»¬çš„è¡¨å‘Šè¯‰æˆ‘ä»¬:

*   Lil Dicky çš„ç²‰ä¸ååº”æœ€å¤§â€”â€”42.4%çš„ç²‰ä¸å–œæ¬¢ä»–çš„ç¬¬ä¸€æ¡æ¨æ–‡ã€‚
*   å³ä½¿åƒå‡¯è’‚Â·ä½©é‡Œå’Œè‰¾ä¼¦è¿™æ ·çš„åäººæœ‰å¾ˆå¤šæ¨ç‰¹ç²‰ä¸ï¼Œä»–ä»¬çš„ç²‰ä¸ä¹Ÿå‡ ä¹æ²¡æœ‰ååº”ï¼Œä¾‹å¦‚ï¼Œåªæœ‰ 0.0098%çš„å‡¯è’‚ç²‰ä¸å–œæ¬¢å¥¹çš„æ¨ç‰¹ã€‚
*   è™½ç„¶ Leo åœ¨è®¡æ•°æ–¹é¢è·å¾—äº†æœ€å¤šçš„å–œæ¬¢å’Œè½¬å‘ï¼Œä½†ä»–çš„ç¬¬ä¸€æ¡æ¨æ–‡åªæœ‰ 2.19%çš„ç²‰ä¸å–œæ¬¢ã€‚

ååº”çš„å·¨å¤§å·®å¼‚å¯ä»¥ç”¨è¿™æ˜¯ Lil Dicky çš„éŸ³ä¹è§†é¢‘æ¥è§£é‡Šã€‚é‡Œæ¬§ä»ç„¶æ¯”å‡¯è’‚æˆ–è‰¾ä¼¦æ›´å—å…³æ³¨ï¼Œå› ä¸ºä»–åœ¨è¿™ä¸ªé¡¹ç›®ä¸­æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚

æˆ‘ä»¬èƒ½åœ¨æ•°æ®ä¸­æ‰¾åˆ°ä¸€äº›æ›´æœ‰è¶£çš„æ¨¡å¼å—ï¼Ÿä»æ¨æ–‡çš„æ–‡æœ¬ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä¸åŒçš„è¯­è¨€ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¸ºè¯­è¨€åˆ›å»ºä¸€ä¸ªé¢‘ç‡åˆ†å¸ƒã€‚

```
# Extracting language for each tweet and appending it to the list of languages
tweets_languages = []
for tweet in tweets:
    tweets_languages.append(tweet['lang'])tweets_sources = []
for tweet in tweets:
    tweets_sources.append(tweet['source'])# Plotting the distribution of languages
%matplotlib inlineplt.hist(tweets_languages)
```

# å¯»æ‰¾æƒ³æ³•

ğŸ•µï¸â€â™€ï¸æœ€åçš„ç›´æ–¹å›¾å‘Šè¯‰æˆ‘ä»¬:

*   å¤§å¤šæ•°æ¨ç‰¹éƒ½æ˜¯è‹±æ–‡çš„ã€‚
*   æ³¢å…°äººã€æ„å¤§åˆ©äººå’Œè¥¿ç­ç‰™äººç´§éšå…¶åã€‚
*   æœ‰å¾ˆå¤šæ¨æ–‡ä½¿ç”¨äº†ä¸ Twitter å®Œå…¨ä¸åŒçš„è¯­è¨€ã€‚

ä¸ºä»€ä¹ˆè¿™ç±»ä¿¡æ¯æœ‰ç”¨ï¼Ÿå› ä¸ºå®ƒå¯ä»¥è®©æˆ‘ä»¬äº†è§£å¯¹è¿™ä¸ªè¯é¢˜æ„Ÿå…´è¶£çš„äººçš„â€œç±»åˆ«â€(èšç±»)ã€‚æˆ‘ä»¬è¿˜å¯ä»¥åˆ†ææ¨ç‰¹ç”¨æˆ·ä½¿ç”¨çš„è®¾å¤‡ç±»å‹ï¼Œ`tweet['source']`ï¼Œæ¥å›ç­”ç±»ä¼¼äº**â€œæ‹¥æœ‰è‹¹æœå’Œå®‰å“ç›¸æ¯”ï¼Œä¼šå½±å“äººä»¬å¯¹è¿™ä¸€è¶‹åŠ¿çš„å€¾å‘å—ï¼Ÿâ€**ã€‚æˆ‘ä¼šæŠŠå®ƒç•™ç»™ä½ ä½œä¸ºè¿›ä¸€æ­¥çš„ç»ƒä¹ ã€‚

![](img/7f375db1ba57e0b10c7270b8adcea6c8.png)

è¿™æ˜¯ä¸€æ¬¡å¤šä¹ˆä»¤äººå…´å¥‹çš„æ—…è¡Œå•Šï¼æˆ‘ä»¬å¼€å§‹å‡ ä¹ä¸€æ— æ‰€çŸ¥ï¼Œç°åœ¨æˆ‘ä»¬åœ¨è¿™é‡Œ..è§è§£ä¸°å¯Œã€‚

ä»åŸºäºä½ç½®çš„æ¯”è¾ƒåˆ°åˆ†ææ¨æ–‡å‘¨å›´çš„æ´»åŠ¨ï¼Œå†åˆ°ä»è¯­è¨€å’Œè®¾å¤‡ä¸­å¯»æ‰¾æ¨¡å¼ï¼Œæˆ‘ä»¬ä»Šå¤©å·²ç»è®¨è®ºäº†å¾ˆå¤šâ€”â€”è®©æˆ‘ä»¬ç»™è‡ªå·±ä¸€ä¸ªå—ä¹‹æ— æ„§çš„é¼“åŠ±å§ï¼ âœ‹

***ç¥å¥‡å…¬å¼=æ•°æ®+ Python +åˆ›é€ åŠ›+å¥½å¥‡å¿ƒ***

![](img/d6d62447f00115fa4aab1fcccfbbaccd.png)

```
plt.hist(tweets_sources)
```

![](img/92f601cc007dc5c4658fda5c362e92de.png)

data

ç¬”è®°æœ¬ GitHub é“¾æ¥:[https://GitHub . com/VD new/Real-Time-Insights-from-Social-media-Data-Data-Science-Case-Study](https://github.com/vdnew/Real-Time-Insights-from-Social-media-Data---Data-Science-Case-Study)

æ¨ç‰¹è´¦å·:

[https://twitter.com/datarishi](https://twitter.com/datarishi)

[](https://twitter.com/vishvdeep18) [## JavaScript ä¸å¯ç”¨ã€‚

### ç¼–è¾‘æè¿°

twitter.com](https://twitter.com/vishvdeep18) 

Linkedin:

 [## Vishvdeep Dasadiya -åŠ©ç†è½¯ä»¶å·¥ç¨‹å¸ˆ-åŸƒæ£®å“²| LinkedIn

### æŸ¥çœ‹ Vishvdeep Dasadiya åœ¨å…¨çƒæœ€å¤§çš„èŒä¸šç¤¾åŒº LinkedIn ä¸Šçš„ä¸ªäººèµ„æ–™ã€‚Vishvdeep åˆ—å‡ºäº† 3 ä¸ªèŒä½â€¦

www.linkedin.com](https://www.linkedin.com/in/vishvdeep-dasadiya-65770312b/) 

GitHub:

[https://github.com/vdnew/](https://github.com/vdnew/Real-Time-Insights-from-Social-media-Data---Data-Science-Case-Study)