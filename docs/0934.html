<html>
<head>
<title>Aggregating Data with PySpark on GCP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在GCP用PySpark聚合数据</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/aggregating-data-with-pyspark-on-gcp-aed527a1e8bc?source=collection_archive---------0-----------------------#2021-08-28">https://medium.com/mlearning-ai/aggregating-data-with-pyspark-on-gcp-aed527a1e8bc?source=collection_archive---------0-----------------------#2021-08-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/37007bc2f86e974499f90bc385a7ca15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDSmxes74rkDmJ-7qErfsg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@nananadolgo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Nana Smirnova</a> on <a class="ae it" href="https://unsplash.com/s/photos/boxes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ca61" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">👋🏼大家好！</p><p id="13e1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这篇文章是关于熟悉一些数据工程的基础工具。我们将从Kaggle(手动)获取IMBD评论数据集，将其上传到Hadoop集群，使用PySpark聚合数据，并将结果加载到Hive表中。</p><p id="1736" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个练习不仅是为了熟悉工具，也是为了学习数据生命周期。</p><p id="3b7f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于那些不熟悉上述技术的人，这里有一个简短的描述:</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/d3b024eab51acda326393d5ace44bbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BnKN3dyJ5JRy4XdtE_eGbw.png"/></div></div></figure><p id="cb89" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> Hadoop — </strong>一个允许我们在一个机器集群上处理(并行)和存储大型数据集的框架。处理框架称为MapReduce，存储称为Hadoop文件系统(HDFS)。我们将使用GCP的data proc——一种托管的Hadoop服务，而不是从头开始建立我们的集群。</p><p id="390a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> Spark </strong> —与Hadoop非常相似，Spark也是一个分布式数据处理框架(不包括存储)。这种处理发生在内存中，比Hadoop将数据从存储位置拉到计算位置的方法快得多。Spark提供了许多API其中之一是Python接口PySpark。</p><p id="08e3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> Hive </strong> —分布式数据仓库分析系统。Hive构建在Hadoop之上，将用于查询我们的PySpark作业的结果。</p><p id="bbe3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">🚨警告！！！GCP不是免费的，为了完成这个项目，你将支付几分钱(除非你有信用)。</p><h2 id="4c5d" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">Dataproc</h2><p id="4c85" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">让我们从前往Google云平台控制台并创建一个新项目<code class="du kx ky kz la b">imbd-agg</code>开始。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/3831b690e08e7a88c746a7e9cbd0d037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROXneJb6cW6V-UuPpfTDGA.png"/></div></div></figure><p id="ec2d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在创建集群之前，您必须启用Dataproc API(您可能必须首先启用billing)。所以抓紧了；这一步需要几分钟时间。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/ef5fc33b697bd67acc21aabd788ecf91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8B99k3WCnLioHYbQTcaOQ.png"/></div></div></figure><p id="7526" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在启用Dataproc API之后，继续创建集群，除了工作机之外，保持大部分默认值不变，在工作机上，我们将4个vCPUs更改为2个。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ld"><img src="../Images/674e3e6d35f10095363f8ed3bdd7815a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*MHkoqsTdkSI0fYifTQ1GzA.png"/></div></figure><p id="e73b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">那么我们到底创造了什么？</p><p id="30d6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Hadoop的架构是由主节点和工作节点组成的网络。在很高的层次上，主节点(特别是NameNode)负责协调由工作节点执行的HDFS功能。我们的集群由一个主机和两个工作机组成。</p><h2 id="8746" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">IMBD评论数据集</h2><p id="ccee" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">让我们暂时离开GCP，下载我们的数据集。</p><p id="6e8b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">前往<a class="ae it" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">Kaggle.com</a>，将电影评论下载到你的本地机器上。预计该数据集中有两列(评论和观点)和50K行。</p><p id="fb00" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的目标是使用Spark进行简单的聚合。我们将对数据进行分组，以找出正面和负面评论之间的计数。正面和负面评论的数量相当，因此我们预计每条评论的数量为25K。我希望这篇文章支持数据生命周期，而不是任何特定的工具，但是可以随意查看Spark的文档，了解更复杂的转换。</p><h2 id="ef9d" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">PySpark脚本</h2><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="a1b7" class="jx jy hh la b fi li lj l lk ll">""" main.py pyspark job to group and count positive and negative<br/>reviews """</span><span id="899b" class="jx jy hh la b fi lm lj l lk ll">from pyspark.sql import SparkSession</span><span id="322b" class="jx jy hh la b fi lm lj l lk ll">spark = SparkSession.builder.appName('sentiment').getOrCreate()</span><span id="46ff" class="jx jy hh la b fi lm lj l lk ll">reviews = spark \<br/>  .read \<br/>  .option('escape', '"') \<br/>  .csv('/user/jduran9987/data/imbd_dataset.csv', <em class="ln">header</em>=True)</span><span id="8be4" class="jx jy hh la b fi lm lj l lk ll">sentiment_breakdown = reviews \<br/>  .groupBy('sentiment') \<br/>  .count()</span><span id="6f6f" class="jx jy hh la b fi lm lj l lk ll">sentiment_breakdown.write.format('csv').save('sentiment_breakdown')</span></pre><p id="5974" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">🚨仅供参考:我们所有的工作都将在Dataproc集群中的主节点内完成，因此没有必要为这个项目设计一个文件夹结构。届时，上述脚本将被添加到我们的主节点中，并从那里执行。</p><p id="4307" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从脚本的顶部开始，我们导入SparkSession对象。这是Spark应用程序的入口点，用于(在我们的例子中)创建数据的抽象表示，称为dataframe。</p><p id="a5ea" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">创建数据帧时有许多可用的方法，但是对于这个简单的项目，我们只需要从我们的文件系统中读取一个csv文件(稍后我们将在HDFS创建这个路径)。我们还包括一个转义引号的选项，因为我们的一些评论包含逗号，这将真正扰乱Spark解析csv文件的方式。</p><p id="4502" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过按情感对<code class="du kx ky kz la b">reviews</code>数据帧进行分组并对值进行计数，创建新的数据帧<code class="du kx ky kz la b">sentiment_breakdown</code>。Spark提供了两种主要类型的操作——转换和操作。将数据分组是转换的一个例子，而执行计数是一个动作。每次我们转换数据时，我们可能会创建一个对数据进行分组的数据帧，并使用该结果创建另一个过滤数据的数据帧，这样就创建了一个新的不可变数据帧。实际上没有采取任何行动，相反，Spark为如何最好地执行您的转换创建了一个逻辑计划。只有当你调用一个动作(比如count)时，Spark才会执行你的代码并产生结果。</p><p id="6528" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，结果被写回我们的文件系统。</p><h2 id="6c4f" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">返回到Dataproc</h2><p id="743c" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">在“cluster details”页面中，导航到“VM instance”选项卡，然后单击SSH以进入主机。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/436ce8d608e749959c385b0b4af63099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUEbHoO9XZLR9BRdSDqFAg.png"/></div></div></figure><p id="14bd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">终端应该打开，在右上角有一个齿轮菜单，包含上传文件的选项。继续将main.py pyspark作业和IMBD评论文件上传到实例。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/f5bfd898acc4a9ff7e0098eddd15f5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJfu3KUS4w7qbSDPWVGOgQ.png"/></div></div></figure><p id="85fe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一旦我们的文件在我们的机器上，我们可以通过执行下面的命令在HDFS上创建一个用户目录。</p><p id="d59b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><code class="du kx ky kz la b">$ hadoop fs -mkdir /user/&lt;your-gcp-project-username&gt;</code></p><p id="c78c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在我的例子中，命令是/user/jduran9987，但是您必须在@符号前使用终端上显示的用户名。</p><p id="6491" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">完美！我们现在创建一个数据目录，然后使用<code class="du kx ky kz la b">put</code>命令将reviews数据集添加到HDFS。</p><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="3f5a" class="jx jy hh la b fi li lj l lk ll">$ hadoop fs -mkdir /user/&lt;your-gcp-project-username&gt;/data<br/>$ hadoop fs -put imbd_dataset.csv /user/&lt;your-gcp-project-username&gt;/data/</span></pre><p id="ce95" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在数据已经在HDFS了，是时候运行我们的PySpark作业了。</p><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="ea07" class="jx jy hh la b fi li lj l lk ll">$ spark-submit main.py</span></pre><p id="ee9c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以使用以下命令来验证结果。</p><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="a1cc" class="jx jy hh la b fi li lj l lk ll">$ hadoop fs -ls /user/&lt;your-gcp-project-username&gt;/sentiment_breakdown</span></pre><p id="8f53" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">成功！</p><h2 id="09f2" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">储备</h2><p id="f925" class="pw-post-body-paragraph iu iv hh iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">是时候创建一个Hive表来查询我们的结果了。我们只需在hive中键入，就可以在终端中访问Hive。</p><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="36dc" class="jx jy hh la b fi li lj l lk ll">$ hive</span></pre><p id="63b1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您现在应该能够看到<code class="du kx ky kz la b">&gt;hive</code>提示符。</p><p id="25a1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最后，让我们创建一个数据库、表，并使用以下命令查询我们的PySpark作业的结果。</p><pre class="jt ju jv jw fd le la lf lg aw lh bi"><span id="8c15" class="jx jy hh la b fi li lj l lk ll">&gt;hive create database if not exists sentiment;<br/>&gt;hive use sentiment;<br/>&gt;hive create table sentiment_breakdown<br/>  &gt; (sentiment string, review_count int)<br/>  &gt; row format delimited fields terminated by ','<br/>  &gt; location '/user/&lt;your-gcp-username&gt;/sentiment_breakdown';<br/>&gt;hive select * from sentiment_breakdown</span></pre><p id="05c4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这显示了Hive和HDFS之间的紧密关系。与其他仓库不同，我们不必将数据复制到表中。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/0ef20d118b8edfd1dc2f79c0c8df8add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M0X1nH03ZoqXAu7geREUvA.png"/></div></div></figure><h2 id="2bf4" class="jx jy hh bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">包裹</h2><ul class=""><li id="57b3" class="lr ls hh iw b ix ks jb kt jf lt jj lu jn lv jr lw lx ly lz bi translated">确保您删除了您的Dataproc集群，以避免一个大的账单。这个项目的资源成本几乎为零，但是您不想让您的集群继续运行。</li><li id="533c" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">这种设置不适合生产。目标是熟悉Hadoop、Spark、GCP和Hive。在现实世界中，使用类似Airflow(它有一个Dataproc操作符)的东西来提供集群，提交spark作业，并按计划删除集群。</li><li id="c935" class="lr ls hh iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">如果您是一名有抱负的数据工程师，并且被这些工具所淹没，不要担心。尝试在很高的水平上掌握这些概念，并在工作中学习细节。我从事这个行业已经有一段时间了，但仍然需要通过transformation command查找PySpark组🤷🏼‍♂️.我对记忆命令没有兴趣，你也不应该。</li></ul><p id="c187" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢阅读！</p></div></div>    
</body>
</html>