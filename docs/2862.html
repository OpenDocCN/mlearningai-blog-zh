<html>
<head>
<title>How Positional Embeddings work in Self-Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">位置嵌入如何在自我注意中起作用</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/how-positional-embeddings-work-in-self-attention-ef74e99b6316?source=collection_archive---------7-----------------------#2022-06-19">https://medium.com/mlearning-ai/how-positional-embeddings-work-in-self-attention-ef74e99b6316?source=collection_archive---------7-----------------------#2022-06-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/56c037eea75962688f2c2a1df9589e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sYENwlTpnOSVN7Pk"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image src: <a class="ae it" href="https://unsplash.com/photos/qodjMu0byZ8" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/qodjMu0byZ8</a></figcaption></figure><p id="98bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在语言中，单词的顺序和它们在句子中的位置很重要。如果单词被重新排序，整个句子的意思将会改变，或者在重新排序后句子可能变得没有意义。</p><p id="2539" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">递归神经网络具有处理序列排序的内置机制。但是，变形金刚不使用任何递归神经单元，如LSTM或GRU，因此将序列中的每个单词视为彼此独立。因此，增加了位置编码来保留关于句子中单词顺序的信息。</p><p id="c87b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">让我们开始吧！</em></p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="9915" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是位置编码？</h1><p id="9263" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">位置编码告诉转换器模型某个实体/单词在序列中的位置，以便为每个位置分配一个唯一的表示。虽然最简单的方法是使用索引值来表示位置，但这仍然会给长序列带来问题，因为索引的数量会变得很大。</p><p id="a614" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里，每个位置/索引被映射到一个向量。因此，位置编码层的输出是一个矩阵，其中矩阵中的每一行是序列中的编码字加上它的位置信息。</p><p id="b676" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下图显示了仅编码位置信息的矩阵示例。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ld"><img src="../Images/aa8a0bae019bd95224ab50fddae78421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iV9m-r7UlKjN8aKIXIMcGA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Positional encoding of sequence — “How are you”</figcaption></figure></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h2 id="cb3a" class="li kb hh bd kc lj lk ll kg lm ln lo kk jf lp lq ko jj lr ls ks jn lt lu kw lv bi translated">变压器中的位置编码层</h2><p id="7a9d" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">假设我们有一个长度为<strong class="iw hi"> L </strong>的输入序列，我们需要这个序列中物体的位置。位置编码由不同频率的正弦和余弦函数给出:</p><figure class="le lf lg lh fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/1f6ff5de6895491f7c40e639d0a8cffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*T7PWnHkaI5qjFGuBtx5MxQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">image src: <a class="ae it" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/text/tutorials/transformer</a></figcaption></figure><p id="872f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> d </strong>:输出嵌入空间的尺寸</p><p id="7db9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">位置</strong>:对象在输入序列中的位置，0 ≤位置≤ L/2</p><p id="c5a1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> i </strong>:用于映射到列索引0≤i &lt; d/2。I的单个值映射到正弦和余弦函数</p><p id="b872" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上面的表达式中，我们可以看到<strong class="iw hi">偶数位置对应正弦函数，奇数位置对应cos函数</strong>。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="abad" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">从头开始编码位置编码矩阵</h1><p id="1331" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">下面是使用NumPy实现位置编码的简短Python代码。代码经过了简化，以便更容易理解位置编码。</p><figure class="le lf lg lh fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><h2 id="5101" class="li kb hh bd kc lj lk ll kg lm ln lo kk jf lp lq ko jj lr ls ks jn lt lu kw lv bi translated">可视化位置矩阵</h2><p id="0f62" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">让我们在更大的值上可视化位置矩阵。我们将使用来自<code class="du lz ma mb mc b">matplotlib</code>库的Python的<code class="du lz ma mb mc b">matshow()</code>方法。如在原始论文中所做的，设置n=10，000，我们得到如下:</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/0c297b0dcb0503b375b0c506132212bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oq60E1kpIG4tMpWW48zYwA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Visualizing the Positional Matrix</figcaption></figure><p id="4177" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，位置编码层将单词嵌入与序列中每个标记的位置编码矩阵相加，作为下一层的输入。请注意，位置编码矩阵的维数应该与单词嵌入的维数相同。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="34d7" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">在Keras中编写自己的位置编码层</h1><h2 id="0288" class="li kb hh bd kc lj lk ll kg lm ln lo kk jf lp lq ko jj lr ls ks jn lt lu kw lv bi translated">导入部分:</h2><p id="5061" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">首先，让我们编写一节来导入所有需要的库。</p><figure class="le lf lg lh fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><h2 id="9990" class="li kb hh bd kc lj lk ll kg lm ln lo kk jf lp lq ko jj lr ls ks jn lt lu kw lv bi translated">符号化:</h2><p id="ff16" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">下面的代码片段使用Tokenizer对象将每个文本转换成整数序列(每个整数是字典中一个标记的索引)。</p><figure class="le lf lg lh fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><h2 id="7398" class="li kb hh bd kc lj lk ll kg lm ln lo kk jf lp lq ko jj lr ls ks jn lt lu kw lv bi translated">对Keras嵌入层进行子分类:</h2><p id="bb74" class="pw-post-body-paragraph iu iv hh iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">在实现transformer模型时，您必须编写自己的位置编码层。这个<a class="ae it" href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" rel="noopener ugc nofollow" target="_blank"> Keras例子</a>展示了如何子类化<code class="du lz ma mb mc b">Embedding</code>层来实现你自己的功能。</p><figure class="le lf lg lh fd ii"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">code src:<a class="ae it" href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/nlp/neural_machine_translation_with_transformer/</a></figcaption></figure></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="33ad" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考资料:</h1><ul class=""><li id="35a8" class="me mf hh iw b ix ky jb kz jf mg jj mh jn mi jr mj mk ml mm bi translated"><a class="ae it" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变压器</a></li><li id="7ef9" class="me mf hh iw b ix mn jb mo jf mp jj mq jn mr jr mj mk ml mm bi translated"><a class="ae it" href="http://vandergoten.ai/2018-09-18-attention-is-all-you-need/" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的——变压器</a></li></ul><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj in mv"/></div></div></a></div></div></div>    
</body>
</html>