# 来自变压器(BERT)的双向编码器表示的需求

> 原文：<https://medium.com/mlearning-ai/the-need-for-bidirectional-encoder-representations-from-transformers-bert-7d8702aab5eb?source=collection_archive---------4----------------------->

来自变压器的双向编码器表示(BERT)是一个用于处理 NLP 的免费开源机器学习框架。BERT 使用周围的文本来提供上下文，以便帮助计算机理解文本中歧义词的含义。在问答数据集的帮助下，BERT 框架可以在根据维基百科的文本进行预训练后进行调整。它的目标是产生一个语言模型。我们也可以说 BERT 是为 NLP 设计的变压器神经网络架构。

虽然它建立在深度学习技术的基础上，但它需要大量的处理能力才能正常工作。不要总是试图从零开始训练这些模型，建议使用可公开访问的预训练模型作为起点。

在 BERT 诞生之前，大多数模型只能单向处理文本。然而，BERT 通过处理一个单词的上下文或者一般来说从从左到右和从右到左两个方向处理文本改变了这个游戏。我们称之为双向的。BERT 使用 Transformer，这是一种学习文本中单词(或子单词)之间上下文关系的注意力机制。

所以现在你可以看出，BERT 的主要技术进步是将 Transformer 的双向训练(一种很受欢迎的注意力模型)应用于语言建模。相比之下，早期的研究从从左到右或者从左到右和从右到左相结合的训练角度来看待文本序列。这项研究的发现表明，双向训练的语言模型比单向语言模型更能理解语境和语言的流动。该论文的作者描述了一种称为掩蔽 LM (MLM)的独特方法，这种方法使双向训练在以前不可行的模型中成为可能。该模型的架构使得有效地理解句子中的单词和上下文成为可能。

现在你可能会问什么是**变形金刚**，嗯？

> Transformer 是一种深度学习模型，采用自我关注机制，对输入数据的每个部分的重要性进行不同的加权。

简而言之，transformer 包括两个独立的机制:解码器和编码器。编码器读取文本输入，解码器产生预测。

BERT 的主要技术进步是将 Transformer 的双向训练(一种很受欢迎的注意力模型)应用于语言建模。相比之下，早期的研究从从左到右或者从左到右和从右到左相结合的训练角度来看待文本序列。

研究表明，双向训练的语言模型比单向语言模型能更深刻地理解语境和语言流程。该论文的作者描述了一种称为掩蔽 LM (MLM)的独特方法，这种方法使双向训练在以前不可行的模型中成为可能。该模型的架构使得有效地理解句子中的单词和上下文成为可能。通过掩蔽 15%的记号来训练 BERT，目的是猜测它们。

# 为什么是伯特？

然而，Transformer 是第一个仅利用自我注意而不使用卷积或序列对齐 rnn 来生成其输入和输出表示的转导模型。这意味着这个模型使得成功的句子嵌入比以前更有可能。实际上，基于 RNN 的设计在学习输入和输出序列中的长程相关性时会有困难，并且在并行化方面也很有挑战性。BERT 是架构突破的结果，也是通过屏蔽一个或多个单词来使用这一概念训练网络的结果。

> 屏蔽语言模型(MLM):随机屏蔽掉输入中 15%的单词——用[屏蔽]标记替换它们——通过基于 BERT 注意力的编码器运行整个序列，然后根据序列中其他非屏蔽单词提供的上下文，仅预测屏蔽单词。
> 
> 伯特训练过程也使用下一句预测(NSP)。在训练期间，该模型将句子对作为输入，并学习预测第二个句子是否也是原文中的下一个句子。

伯特之所以是理想的选择，是因为这个模型同时接受了 MLM 和 NSP 的训练。这是为了最小化两种策略的组合损失函数。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)