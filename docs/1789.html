<html>
<head>
<title>In-Depth Review of Convolutional Neural Networks (CNN’s)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络(CNN)的深入回顾</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/convolutional-neural-networks-cnns-1ed0f1f8e176?source=collection_archive---------4-----------------------#2022-01-29">https://medium.com/mlearning-ai/convolutional-neural-networks-cnns-1ed0f1f8e176?source=collection_archive---------4-----------------------#2022-01-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/04efb0fd1a809e3b0f4a336e710ee597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pZ8gk7p3HI7Cr_HBDhCng.png"/></div></div></figure><p id="2f28" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在接下来的章节中，我将解释CNN的起源、结构和应用。</p><p id="2b10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在CNN的深度学习之前，计算机在玩AlphaGO等游戏方面比人类更好，但是，计算机无法处理图像和文本等非结构化数据。直到最近，我们才把计算机变成了超人机器，能够完成所有这些复杂的任务，比如图像和文本分类。</p><p id="7fc4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对我们人类来说，当我们看一张照片时，画面中的内容似乎很明显。假设有一个狗舔主人的图像。我们不能忽视画面中有一只狗的事实，观察这些事情已经成为我们的第二天性。直到最近，计算机才能够感知这些东西。</p><p id="fe32" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在卷积神经网络(CNN)出现了。对人类大脑视觉皮层的研究让我们发现了CNN。从那以后，细胞神经网络被广泛用于图像识别。如今，CNN正被用于各种各样的事情，如自动驾驶、图像和视频分类系统。这些不仅用于图形数据，而且在通过自然语言处理(NLP)计算文本数据方面也取得了巨大进步。</p><h1 id="60e4" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">CNN的历史</h1><p id="74d7" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">由于CNN的发展与我们对大脑视觉皮层的了解有关。我们首先需要了解我们的大脑是如何感知图像的。大卫·胡贝尔和托尔斯滕·威塞尔在1959年发现了我们体内简单和复杂细胞的工作方式。基于他们的研究，我们使用两种diﬀerent类型的细胞来识别视觉模式。简单的单元只能识别图像特定部分的特定方向的边缘和条。相比之下，复杂细胞不仅能做简单细胞做的事情，还能识别图像中任何位置的边缘和条纹。</p><p id="9219" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们面前有一幅图像。简单单元格只能识别右上角的水平条，而复杂单元格可以识别图像中所有位置的水平条。复杂细胞可以简单地看作是几个简单细胞的组合。我们的身体使用简单和复杂细胞之间的连贯理解来形成视觉系统。</p><p id="b68a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来是<em class="kq">祖母</em>或<em class="kq">诺斯替细胞的概念。</em>它只是指我们体内的一个假想细胞，只有在看到一个复杂的图像时才会被激活，比如一个人的祖母。因此，这些复杂的神经元中有几个是相互连接的，当它们看到祖母的复杂图像时就会被激活。</p><h1 id="5363" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">新克隆体</h1><p id="d474" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">受David Hubel和Torsten Wiesel工作的启发，Kunihiko Fukushima博士在20世纪80年代开发了一个人工神经网络。该网络模拟了我们体内简单和复杂细胞的工作方式。该网络并不对应于生物神经元，而是代表简单和复杂细胞的算法结构。福岛想到的主要想法是，新认知基因应该能够识别复杂的图像或模式。基于简单细胞对每个单独部分的识别，复杂细胞会识别整个图像或图案。</p><p id="820c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们想识别一只猫的形象。复合体应该能够检测到猫整体的存在，而简单细胞应该能够检测到单个部分，如爪子、尾巴、胡须等。</p><h1 id="40e5" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">网络</h1><p id="f815" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">福岛所做的工作为许多研究人员探索这一领域铺平了道路。第一个重大突破发生在20世纪90年代，Yann LeCun实现了一个基于卷积神经网络的现代应用。他的论文“<em class="kq">基于梯度的学习应用于文档识别</em>”成为当时最受欢迎的研究论文。它被用作任何开始学习CNN的人的指南。</p><p id="3a18" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在论文中，他描述了如何使用CNN训练手写数字数据集(MNIST)。他在福岛新克隆的基础上，利用人造细胞组装了许多复杂的功能。</p><p id="3cc5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这都是关于CNN的简史以及它们是如何形成的。在接下来的部分，我将解释一些与CNN相关的重要术语。</p><h1 id="a56a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">盘旋</h1><p id="0803" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">本质上，CNN的主要组件是卷积层。在数学术语中，卷积可以定义为一种涉及权重和输入相乘的运算。CNN的第一层是卷积层。与每个输入都完全连接的人工神经网络不同，CNN的第一层仅连接到其特定场的像素。同样，第二个卷积层与第一个卷积层的相应像素相连。因此，我们在第一层组装低级特征，然后将它们组合在一起，创建高级特征。真实世界的图像也具有类似的层次结构。这是CNN与真实世界的图像数据配合得如此之好的主要原因之一。</p><p id="a402" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">卷积层主要处理二维数据，因此权重和输入之间的乘法通常在2D权重阵列和输入阵列之间进行。这个2D权重阵列被称为<em class="kq">滤波器</em>或<em class="kq">内核。</em>必须保持滤波器的尺寸小于输入。因此，输入的大小会减小，以匹配滤波器的大小，两者之间的乘积称为点积。基本上，点积涉及元素级乘法，结果相加后以标量的形式给出最终输出。</p><p id="3437" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们倾向于保持滤波器的大小小于输入，因为它会导致相同的权重与输入数组相乘多次。当这种技术系统地应用于输入的每一个重叠部分时，它会变得更加强大。因为这里权重和输入数组之间的乘法发生了几次，所以输出是包含输入的滤波值的2D数组。这个输出2D阵列也被称为<em class="kq">特征图。</em>特征图中出现的每个过滤值随后通过一些非线性激活函数线ReLU传递。这部分过程与全连接层的情况类似。两个连续接收场之间的距离称为<em class="kq">步幅</em>。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/5eda14c306649c1547103f5578346238.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*7Po151lm3gNNBV6WnPOtAA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx"><em class="la">Fig: Convolution process</em></figcaption></figure><h1 id="009d" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">汇集层</h1><p id="d850" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">现在，我们已经了解了卷积层的工作原理，接下来我们将讨论CNN架构中的第二层，即池层。使用池层的主要目的是降低过度拟合的风险。这是通过减小卷积特征图的尺寸来实现的。这有助于减少计算量，节省内存，并且总体上减少了参数的数量。因此，池图层会对通过卷积图层获得的要素地图进行下采样。</p><p id="f87b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下采样指的是生成较低分辨率的输入图像的过程，该图像仍包含所有重要特征，但不处理原始输入的更精细细节。它们通过将要素组合成面片来总结要素地图中的要素。</p><p id="19ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该层涉及汇集操作，并且汇集操作的大小总是小于特征地图的大小。通常我们使用跨度为2个像素的池操作。这意味着我们实际上是将特征地图的大小减少了2倍。假设我们有一个大小为8×8(64像素)的特征地图，在应用池操作后，它将缩小到4×4(16像素)的大小。我们最常用的两种主要池化方法是<em class="kq">平均池化</em>和<em class="kq">最大池化。</em></p><p id="c493" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 1。</strong> <strong class="ir hi">平均池:</strong>在这个方法中，我们计算特征图中出现的每个补丁的平均值。</p><p id="138f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。</strong> <strong class="ir hi">最大池:</strong>在这种方法中，我们计算特征图中每个面片的最大值。</p><h1 id="ef6e" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">CNN的体系结构</h1><p id="1473" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">现在我们已经看到了diﬀerent层及其功能，让我们更广泛地看看卷积神经网络的整体架构。CNN本质上由两部分组成:</p><p id="8ec4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">1.输入层、卷积层和池层构成了CNN的特征提取部分。</p><p id="bc56" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.一个完全连接的图层，用于处理卷积过程生成的输出，以预测diﬀerent影像的类别。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/2eaaa23e6a3b9051fdd1c9d1b998823e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*7Vfy1IyRezivBaHmd3Qcrg.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx"><em class="la">Fig: Architecture of CNN</em></figcaption></figure><p id="778a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们已经解释了卷积层和池层的工作原理。现在让我们看看CNN架构的分类部分。</p><h1 id="4389" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">全连接层</h1><p id="2a4a" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">全连接层应该始终是卷积神经网络的最后一层。它接收卷积过程产生的输出作为输入。然后，它通过应用几个线性变换来变换输入，最后通过激活函数来预测图像的类别。</p><p id="db24" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">全连接层的输出是大小为N的向量的形式，其中N指定我们的问题中存在的类的数量。该输出向量的每个元素都与特定图像属于特定类别的概率相关联。</p><p id="81fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过简单地获得具有相应权重的每个输入的乘积，取这些乘积的总和，然后在其上应用激活函数，来计算概率。在这一层中，每个输入都与所有可能的输出相连接，因此我们使用术语“完全连接”。CNN在训练阶段通过使用反向传播来学习与输入相乘的权重。该层利用diﬀerent类型的激活函数，如ReLU、tanh、softmax函数等。</p><h1 id="9682" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">拒绝传统社会的人</h1><p id="a14d" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">通过将数据集的所有要素连接到完全连接的图层，我们增加了过度拟合的机会。这可能导致我们的模型在测试数据上的性能下降。</p><p id="de53" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了防止过度拟合，我们使用一种称为漏失层的东西。通过使用这种方法，大约30%的神经元被随机地从网络中删除。这通常发生在网络的训练阶段，导致网络规模缩小。</p><p id="21f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — -</p><p id="f687" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">至此，你应该明白什么是CNN以及它们是如何工作的了。我希望你喜欢它！</p><p id="a0e0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这篇文章摘自我的书<a class="ae lc" href="https://amzn.to/3KVCD6g" rel="noopener ugc nofollow" target="_blank"> <strong class="ir hi">《机器学习——综合方法》。</strong> </a></p><p id="0594" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要了解更多关于CNN和机器学习的一般知识，请查看我在亚马逊上的书:<a class="ae lc" href="https://amzn.to/3KVCD6g" rel="noopener ugc nofollow" target="_blank">https://amzn.to/3KVCD6g</a></p><p id="432e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请随时通过Linkedin 联系我。</p><p id="b784" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢你的阅读，我们下一集再见。</p><div class="ld le ez fb lf lg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hi fi z dy ll ea eb lm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">medium.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu in lg"/></div></div></a></div></div></div>    
</body>
</html>