<html>
<head>
<title>Common Loss Functions in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中常见的损失函数</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/common-loss-functions-in-deep-learning-6ef47d0c3230?source=collection_archive---------3-----------------------#2022-07-25">https://medium.com/mlearning-ai/common-loss-functions-in-deep-learning-6ef47d0c3230?source=collection_archive---------3-----------------------#2022-07-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/f67dfb3a1aff7374ab99f00262375adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2aZwtAxeZkF4JPGq"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@cdr6934?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chris Ried</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6fd7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">机器学习通过计算实际值和预测值之间的差异(误差)来学习，并旨在最小化这种计算。如果我们模型的预测偏离实际值太多，它往往会产生一个很大的数字。这个大的结果被优化函数迭代地最小化。然而，在这个故事中，我们将覆盖损失函数，而不是优化函数。</p><p id="7c72" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">计算实际值和预测值之间差异的方法是使用损失函数。术语“<strong class="iw hi">损失函数</strong>”，也称为“<strong class="iw hi">成本函数</strong>”或“<strong class="iw hi">误差函数</strong>”，试图评估我们的算法处理数据的好坏。有各种类型的损失函数。一般我们会根据正在处理的问题进行分类，分别是<strong class="iw hi">分类</strong>和<strong class="iw hi">回归</strong>问题。</p><h1 id="19c4" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">回归</strong></h1><h2 id="2377" class="kq jt hh bd ju kr ks kt jy ku kv kw kc jf kx ky kg jj kz la kk jn lb lc ko ld bi translated">-均方误差</h2><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/2cffbd3da048849d60400ef54049cdd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*OI7wD7XRIpxKpjfr3CzrQA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Mean Squared Error</figcaption></figure><p id="97a2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">均方误差是回归问题中常用的损失函数。它被定义为预测值和实际值之间的<strong class="iw hi">平方差</strong>的平均值。该值将产生正值，因为该公式取减法的平方。通过这些事情(平方)，与偏差较小的预测相比，远离实际值的预测会受到严重惩罚。</p><p id="0661" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="96e5" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">以一个<em class="ls">二次方程</em>的形式，这意味着没有局部极小值，我们只得到一个全局极小值。</li><li id="8d8d" class="lj lk hh iw b ix lt jb lu jf lv jj lw jn lx jr lo lp lq lr bi translated">惩罚犯大错误的模型，因为我们平方它们。</li></ul><p id="8434" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="718c" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">对异常值敏感或不稳健。这是由MSE惩罚大误差预测造成的，而异常值往往具有非常大或非常小的值。</li></ul><p id="7085" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于MSE具有这一缺点，所以当处理相当大且范围广泛数据时，它将受到影响。在这种情况下，我们可以使用<strong class="iw hi">均方对数误差(MSLE) </strong>。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/cdc7c42c385a3520eb1d9ad76e0e94e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*jvNqPo4SFU7Ihr-vnFuvyw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Mean Squared Logarithmic Error</figcaption></figure><p id="d8fe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">顾名思义，这样会先计算实际值和预测值的对数，再计算均方误差。</p><h2 id="bec9" class="kq jt hh bd ju kr ks kt jy ku kv kw kc jf kx ky kg jj kz la kk jn lb lc ko ld bi translated">-平均绝对误差</h2><p id="f129" class="pw-post-body-paragraph iu iv hh iw b ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">另一方面，MSE通过对数字求平方来产生正值，而平均绝对误差(MAE)通过对其求绝对值来产生正数。MAE计算预测值和实际值之间绝对差值的平均和。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es me"><img src="../Images/473b4d045d1a047469789c6867c35213.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*U5YrD9uUs59btGUo2aEoOQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Mean Absolute Error</figcaption></figure><p id="b91c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="8c00" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">与MSE相比，MAE对异常值更稳健。</li></ul><p id="5e82" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="e451" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">计算成本高，因为它比平方误差更复杂。</li><li id="d22d" class="lj lk hh iw b ix lt jb lu jf lv jj lw jn lx jr lo lp lq lr bi translated">可能存在局部最小值，因为它不是二次型。</li></ul><h2 id="7377" class="kq jt hh bd ju kr ks kt jy ku kv kw kc jf kx ky kg jj kz la kk jn lb lc ko ld bi translated">-胡伯损失</h2><p id="b1f3" class="pw-post-body-paragraph iu iv hh iw b ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">Huber损失是MSE和MAE的某种组合。它先取一个二次方程，然后把它变成一个线性方程。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/8bc3ecd393d43fcebc2786f9b1cf204b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*0yhxf7O5Xs-V1NMBOWAeiw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Huber Loss</figcaption></figure><p id="356e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">delta值相当于一个参数，用于考虑是使用二次方程还是线性方程。如果绝对值小于δ，将使用二次方程。否则，它将使用线性方程。换句话说，如果有异常值，绝对值将很可能大于delta，它将使用线性方程(MAE)。使用Hubber Loss的关键点是选择正确的delta值，因为它有助于确定异常值标准。</p><p id="7daf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">优点:</strong></p><ul class=""><li id="dba2" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">对异常值稳健</li><li id="2c73" class="lj lk hh iw b ix lt jb lu jf lv jj lw jn lx jr lo lp lq lr bi translated">如果我们设置正确的增量值，就不会有局部最优</li></ul><p id="33f3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">缺点:</strong></p><ul class=""><li id="9044" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">需要优化增量值，这是一个迭代过程</li></ul><h1 id="3910" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">分类</h1><h2 id="4766" class="kq jt hh bd ju kr ks kt jy ku kv kw kc jf kx ky kg jj kz la kk jn lb lc ko ld bi translated">-交叉熵</h2><p id="2973" class="pw-post-body-paragraph iu iv hh iw b ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">交叉熵是分类问题中最常用的损失函数。该值随着预测概率偏离实际标签而增加。换句话说，它测量两个概率分布之间的差异。</p><p id="362a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于<strong class="iw hi">二元分类问题</strong>，我们可以使用<strong class="iw hi">二元交叉熵</strong>。将预测的概率与实际标签(0或1)进行比较。当实际标签为0时，我们可以使用等式的后半部分，反之亦然。这种交叉熵损失基于与期望值的差异来惩罚概率。简单来说，有信心但错误的预测会受到惩罚。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/f2f1aea914f297bb5e03de1631c47267.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*Yf1yGGELtN6TQJnRJ5QNMA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Binary Cross Entropy</figcaption></figure><p id="eeaf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于<strong class="iw hi">多类分类问题</strong>，我们可以使用<strong class="iw hi">多类交叉熵</strong>损失，也称为<strong class="iw hi">分类交叉熵</strong>。我们需要确保标签是一个热编码的，这意味着真正的标签将表示为1，其余的表示为0。例如，如果我们有3类分类问题，其中第一个实例属于1类，第二个实例属于2类，第三个实例属于3类，实际目标(y)可以表示为<em class="ls"> y=[[1，0，0]，[0，1，0]，[0，0，1]]。</em></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/159aaca2805c8d54bcdcfd7b302752d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*FNOXCeNF5Eb1UiabZdzW_A.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Multi-Class Cross Entropy</figcaption></figure><p id="8c70" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">另一种处理多类分类问题的方法是使用<strong class="iw hi">稀疏多类交叉熵</strong>。该公式与多类交叉熵的公式相同，但是我们使用单个整数单元来表示标签，而不是一个热编码。从上一段的例子来看，实际目标(y)可以表示为<em class="ls">y =【1，2，3】</em>。</p><p id="a0c9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与MCCE相比，的优势在于计算效率更高，因为它不像热编码那样包含很多零。</p><p id="c175" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">的缺点</strong>是它不能表示一个<strong class="iw hi">多标签分类</strong>，其中每个实例可能属于不止一个类。对于这种情况，多类交叉熵将是首选。</p><h2 id="c5f4" class="kq jt hh bd ju kr ks kt jy ku kv kw kc jf kx ky kg jj kz la kk jn lb lc ko ld bi translated">-铰链损耗</h2><p id="0f78" class="pw-post-body-paragraph iu iv hh iw b ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">二进制分类的另一个损失函数，即<strong class="iw hi">铰链损失</strong>主要与SVM的软余量相关。该损失函数将分类边界的余量或距离合并到成本计算中。即使新的观察值被正确分类，如果决策边界的余量不够大，它们也会招致惩罚。铰链损耗增加。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/09fa1d10250a935b894b8d459965ff95.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*PFwgXIy2VNepaylcsiyONA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Hinge loss</figcaption></figure><p id="d2c2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意，这个损失函数使用(-1，1)标签。如果我们有一个(0，1)标签，我们可以在使用这个损失函数之前改变它。如果实际值和预测值有差异，我们将会损失惨重。</p><h1 id="65b9" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">摘要</h1><p id="debd" class="pw-post-body-paragraph iu iv hh iw b ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">我们已经介绍了深度学习中一些常用的损失函数。对于回归问题，MSE是常用的，但它遭受离群值。另一方面，与MSE相比，MAE对异常值更鲁棒，但是它的计算开销更大。Huber loss提供了一个可以处理这两个问题的解决方案，因为它是MSE和MAE的组合。</p><p id="a323" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于二叉分类问题，通常使用二叉交叉熵损失。对于多类分类问题，可以使用多类交叉熵损失。但是，如果我们为了计算成本而避免许多零值，我们可以使用稀疏多类交叉熵，它采用整数而不是一个热编码值。然而，如果我们有一个多标签分类问题，我们可以使用多类交叉熵，因为稀疏多类交叉熵不能处理它们。</p><p id="ba1c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">参考资料:</p><ul class=""><li id="e3bc" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated"><a class="ae it" href="https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">https://heart beat . comet . ml/5-regression-loss-functions-all-machine-learners-should-know-4fb 140 e 9 D4 b 0</a></li><li id="771a" class="lj lk hh iw b ix lt jb lu jf lv jj lw jn lx jr lo lp lq lr bi translated"><a class="ae it" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/loss _ functions . html #交叉熵</a></li><li id="c612" class="lj lk hh iw b ix lt jb lu jf lv jj lw jn lx jr lo lp lq lr bi translated"><a class="ae it" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras/losses</a></li></ul></div><div class="ab cl mj mk go ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ha hb hc hd he"><p id="8b8c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢阅读！</p><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">medium.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh in mt"/></div></div></a></div></div></div>    
</body>
</html>