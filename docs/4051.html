<html>
<head>
<title>The Gradient Boosting algorithm: the secret behind the XGBoost Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进算法:XGBoost库背后的秘密</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-gradient-boosting-algorithm-the-secret-behind-the-xgboost-library-efbeb8ebacf0?source=collection_archive---------2-----------------------#2022-12-01">https://medium.com/mlearning-ai/the-gradient-boosting-algorithm-the-secret-behind-the-xgboost-library-efbeb8ebacf0?source=collection_archive---------2-----------------------#2022-12-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/58684065d9fbeee65e33b9472c4cf980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bceUfY4cLU06Mar_0Hs4w.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/photos/dBI_My696Rk" rel="noopener ugc nofollow" target="_blank">Chris Liverani</a> (<a class="ae it" href="http://unsplash.com" rel="noopener ugc nofollow" target="_blank">unsplash.com</a>)</figcaption></figure><h1 id="539d" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">概观</h1><p id="720f" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">我四处寻找在<strong class="ju hi"> Kaggle </strong>竞赛中使用的最常见的数据科学库；原来<strong class="ju hi"> XGboost </strong>就是其中之一。这个库提供了一个有趣的模型形成过程，可以帮助你避免过度拟合(一个方差很大的模型)。</p><p id="087f" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi"> XGBoost，</strong>是<strong class="ju hi">极限梯度增强</strong>的简称，是陈天琦开发的ML库，目前非常流行。顾名思义，这个库的核心是一个叫做<strong class="ju hi">梯度提升</strong>的算法。这是一个有趣的算法，使用决策树来创建强回归模型或分类算法！</p><p id="5ddf" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">在本文中，我们将弄清楚这个算法是关于什么的。</p><h1 id="49e2" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">梯度推进算法</h1><p id="7294" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">基本上，它是一种机器学习算法，结合弱学习器来创建强预测模型。该模型分步骤工作，每一步将以前的模型与新的弱学习器(决策树)相结合，该弱学习器被调整为以残差为目标来建立预测模型。让我们看一个例子:</p><p id="323a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">假设我们有这样一个数据集:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/4827ac031750213ca5866e4b52b55e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1Dd5bw-dckmXdNJOhR6Aw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: Own Elaboration</figcaption></figure><p id="bd7b" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">我们的任务是创建一个适合这些数据的ML模型，所以让我们开始吧！</p><p id="ad01" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">梯度下降算法从一个“哑”模型开始。通常你可以使用一个常数值的函数(目标的平均值)。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="la lb l"/></div></figure><p id="fa83" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">看起来是这样的:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/26f6fd484bd890969d1c0035267a121b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k85H8rV6PS-oSGN9HTQ4hw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: Own Elaboration</figcaption></figure><p id="ef99" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">显然这不是我们想要的，所以我们给这个模型添加一个决策树，它的目标是<em class="ld"> F0 </em>的残差。让我们称这个决策树(弱学习者)为<em class="ld"> h0: </em></p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="la lb l"/></div></figure><p id="0354" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">如果我们将它们组合成一个<em class="ld"> F1 </em>模型，即<em class="ld"> F1 = F0 + h0 </em>，我们将得到如下结果:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/a0b4c06dd1735f3e40c10eba5d2ffbec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GcXQBPlfqjm2N1H_EJ_Vcw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: Own Elaboration</figcaption></figure><p id="cd4d" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">如果我们继续并基于<em class="ld"> F1 </em>和一个新的弱学习者<em class="ld"> h1 </em>创建一个<em class="ld"> F2 </em>模型会怎么样:</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="le lb l"/></div></figure><p id="b499" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">最后，我们可以为此设置一个通用公式:</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="la lb l"/></div></figure><p id="926f" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">如果我们基于F0设置公式，我们将得到:</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lf lb l"/></div></figure><p id="6898" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">有意思…但是让我们再做一件事！</p><p id="ea15" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">一般来说，构建预测模型时要处理的一个重要问题是<strong class="ju hi">过拟合</strong>。避免这种情况的一个方法是给弱学习者增加一个<strong class="ju hi">学习率</strong>。</p><p id="fe6c" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">为了创建一个良好的、非过度拟合的模型，模型<strong class="ju hi">应该</strong> <strong class="ju hi">而不是</strong> <strong class="ju hi">记住(记忆)</strong>整个训练集。所以一般公式是:</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lg lb l"/></div></figure><p id="cd7b" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">因此，如果我们计算10个循环<em class="ld"> (M=10) </em>，这个模型将是这样的:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/3f0773db2d181427015ebcbb64655a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*Ijag7D8WwMhTwBaqzeeSVg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: Own Elaboration</figcaption></figure><p id="3707" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">这就是梯度推进！</p><h1 id="e199" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">如何在Python中从头实现渐变提升</h1><p id="53c8" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">正如我提到的，这种算法是按步骤工作的(逐级方式)，所以您可能会猜测这是一个将重复相同过程的循环，因此算法将是这样的:</p><pre class="kw kx ky kz fd li lj lk bn ll lm bi"><span id="2c1b" class="ln iv hh lj b be lo lp l lq lr">def gradient_boosting_algorithm(times,learning_rate):<br/><br/>  F0 = y.mean()<br/>  Fm = F0<br/>  trees = []<br/>  row = 1<br/>  column = 1<br/>  #Creating an array where we can store each F function per step<br/>  F_array = []<br/>  F_array.append(np.full((data_size,1),F0))<br/><br/>  for i in range(times):<br/>    treeModel = DecisionTreeRegressor(max_depth = 1)<br/>    treeModel.fit(x,y-Fm)<br/>    Fm += learning_rate*treeModel.predict(x)<br/>    F_array.append(np.copy(Fm))<br/><br/>  return F_array</span></pre><p id="63a0" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">你可以这样绘制每个模型:</p><pre class="kw kx ky kz fd li lj lk bn ll lm bi"><span id="893b" class="ln iv hh lj b be lo lp l lq lr">M = 10<br/>F_array = gradient_boosting_algorithm(times = M, learning_rate = 0.5)<br/>N_columns = 2<br/>N_rows = int(M/N_columns)<br/>idx = 0<br/><br/>#Ploting each loop regression model (Fm)<br/>fig, ax = plt.subplots(nrows=N_rows, ncols=N_columns, figsize=(10,20))<br/>fig.tight_layout(pad=5.0)<br/>for i in range(N_rows):<br/>  for j in range(N_columns):<br/>    sns.scatterplot(data=dataset, x="x", y="y", color="blue", alpha=0.5,ax=ax[i][j])<br/>    ax[i][j].plot(x,F_array[idx],color="red")<br/>    ax[i][j].set_title(f"F model (m = {idx})")<br/>    idx+=1<br/><br/>plt.show()</span></pre><p id="15c5" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">我从<a class="ae it" href="https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch#Implementation" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hi"> Matt Bowers的文章</strong> </a> <strong class="ju hi">(全部功劳归于他)</strong>中学到了这个算法和代码，你也可以在我的<a class="ae it" href="https://github.com/luismirandad27/ds-gradient-boosting-algorithm-explanation" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hi"> GitHub </strong> </a>中查看如何模拟这个的执行，以及它如何执行Sklearn库中包含的GradientBoostingRegressor方法。</p></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><h1 id="4068" class="iu iv hh bd iw ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn md jp jq jr bi translated">结论</h1><p id="7883" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">梯度下降是XGBoost和LightGBM等许多流行库背后的算法。在使用这些模型之前，我认为了解它们在幕后做什么是很重要的。</p><p id="14ea" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">希望这有助于我在数据科学家之旅中找到它！</p><p id="b4d8" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">快乐学习！</strong></p><p id="7dc9" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">关注我:<a class="ae it" href="https://www.linkedin.com/in/lmirandad27/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae it" href="https://github.com/luismirandad27" rel="noopener ugc nofollow" target="_blank">GitHub</a>|<a class="ae it" href="https://luismiguelmiranda.com/" rel="noopener ugc nofollow" target="_blank">个人网站</a></p><p id="9372" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated"><strong class="ju hi">参考文献</strong></p><ul class=""><li id="6fb5" class="me mf hh ju b jv kq jz kr kd mg kh mh kl mi kp mj mk ml mm bi translated"><a class="ae it" href="https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch#Implementation" rel="noopener ugc nofollow" target="_blank">https://blog . matt powers . dev/gradient-boosting-machine-from-scratch #实现</a></li></ul><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hi fi z dy mv ea eb mw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne in mq"/></div></div></a></div></div></div>    
</body>
</html>