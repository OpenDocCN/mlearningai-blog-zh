<html>
<head>
<title>Eigendecomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征分解</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/eigendecomposition-2f04ce896e6f?source=collection_archive---------6-----------------------#2022-05-01">https://medium.com/mlearning-ai/eigendecomposition-2f04ce896e6f?source=collection_archive---------6-----------------------#2022-05-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bb38" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性代数中的概念是特征分解，它是一类称为谱方法的机器学习算法的基础，如主成分分析(PCA)和多维标度(MDS)。它使我们能够提取表征线性映射的有意义的信息。但是这到底是如何工作的，它如何帮助我们，这是我在这篇文章中想要回答的问题:)</p><p id="3181" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在阅读这篇文章之前，确保你熟悉线性代数中的这些概念:</p><ul class=""><li id="3121" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><a class="ae jl" href="https://en.wikipedia.org/wiki/Determinant" rel="noopener ugc nofollow" target="_blank">矩阵的行列式</a></li><li id="075f" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><a class="ae jl" href="https://en.wikipedia.org/wiki/Invertible_matrix" rel="noopener ugc nofollow" target="_blank">矩阵的逆矩阵</a></li><li id="63d4" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><a class="ae jl" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank">矩阵的特征向量和特征值</a></li><li id="6d23" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><a class="ae jl" href="https://en.wikipedia.org/wiki/Basis_(linear_algebra)" rel="noopener ugc nofollow" target="_blank">向量空间的基础</a></li></ul></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="1a3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们回忆一下对角矩阵这个术语。它是在所有非对角线元素上具有零值的矩阵。例如:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es jy"><img src="../Images/5156f5aabf59d10b12dba05d8653a0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*uRrBvO35LlP68ubG0xJzQg.png"/></div></figure><p id="2ac1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于对角矩阵有趣的事情是，它们允许快速计算行列式、幂和逆矩阵。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kg"><img src="../Images/b60535aab7e5e58a8f59e1f8ffe07681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*cJHAhKmVIxvA_5jprCg5Qg.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">calculating the power of a diagonal matrix</figcaption></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kl"><img src="../Images/80589ea92a727175eaa6d27d10e6e83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*feIe0h1jAdmS_ztDmLrKkw.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx">calculating the inverse of a diagonal matrix</figcaption></figure><p id="c4ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="km">备注</em>:如果对角线上的所有元素都是非零值，则对角矩阵的逆矩阵存在，因为否则，行列式将为零。</p><p id="a38d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于对角矩阵的适当特征，如果可能的话，我们感兴趣的是将矩阵分解成对角形式。这就是特征分解的由来。</p><p id="4f9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设<strong class="ig hi"><em class="km">A</em></strong>(n×n矩阵)具有非退化或不同的特征值:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kn"><img src="../Images/287796e19e20cd9979252e55b049eeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*OSI113evqUusYo9prkbylw.png"/></div></figure><p id="c694" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它们是矩阵D的对角元素:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ko"><img src="../Images/f840317a52af1e7ed8a3e9f08bac08c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*UVr7D9EN1nOjwxsA8El_AQ.png"/></div></figure><p id="c6e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">相应的特征向量表示为:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kn"><img src="../Images/eb9dc80646b6d44ae76e0d824a8377a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*aZiYSN0re6jOwK7lYRpqAA.png"/></div></figure><p id="32a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kp"><img src="../Images/5adef48845158fd4eda605ff0354cca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*fkUNeT0JXG2kMRFg66MatQ.png"/></div></figure><p id="2dac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这导致矩阵A的相似性分解:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kq"><img src="../Images/e220cdbe60821815a0cf8885518f84ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*rcibd1zDx6CgaD-xImjxKQ.png"/></div></figure><p id="77a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个方阵<strong class="ig hi"><em class="km"/></strong>的分解总是可能的，只要<strong class="ig hi"> <em class="km"> P </em> </strong>也是一个方阵。是因为如果<strong class="ig hi"> <em class="km"> P </em> </strong>不是一个<strong class="ig hi">方</strong>矩阵，它就没有一个<em class="km">逆</em>。这就是所谓的<strong class="ig hi">特征分解</strong>定理。此外，如果矩阵<strong class="ig hi"> <em class="km"> A </em> </strong>是对称的，则<strong class="ig hi"> <em class="km"> P </em> </strong>的列是正交向量。这直接来自于<strong class="ig hi">谱</strong>T42定理的说法:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kr"><img src="../Images/4acc6d4ab7f74ea529c365b81b1ce4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*yNyNWBgNXbTMYEh-fIsPeQ.png"/></div></figure><p id="84d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将矩阵变为对角形式是基变换的应用。新基由初始矩阵的特征向量组成。谱定理指出，在对称矩阵的情况下，我们总能找到这样一个基，它也是<strong class="ig hi">正交的</strong>。</p><p id="9a1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这个表格，我们就可以高效地计算出矩阵<strong class="ig hi">T3A</strong>的矩阵幂:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ks"><img src="../Images/4b22c8415a1f462abfdb2b248d14443c.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*sdO6b2huC7u_OKoM9QemFA.png"/></div></figure><p id="aaea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们已经看到，计算对角矩阵的幂是很容易的。计算矩阵的行列式<strong class="ig hi"><em class="km"/></strong>同理:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kt"><img src="../Images/60c09003a9b381fa45eb8b1c29d20145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*zr6bw_PfFkDn3EGwJhISyw.png"/></div></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h2 id="ff63" class="ku kv hh bd kw kx ky kz la lb lc ld le ip lf lg lh it li lj lk ix ll lm ln lo bi translated">特征分解背后的直觉</h2><p id="ed94" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">设A是某个线性映射相对于标准基的变换矩阵。P-1表示从标准基到特征基的基变化。矩阵D然后通过特征值对特征向量进行缩放。最后，P将向量转换回标准基坐标。下图对此进行了说明:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/b5a8becf2b7f0dc6f34956d03309f64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJMRZPKgfPyVFbEx6kjlvA.jpeg"/></div></div></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h2 id="95d4" class="ku kv hh bd kw kx ky kz la lb lc ld le ip lf lg lh it li lj lk ix ll lm ln lo bi translated">应用</h2><p id="aad1" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">矩阵分解是线性代数中的一个重要概念。它允许我们用不同的基来表示矩阵，并把它们分解成有意义的部分。</p><p id="bec3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如开头所述，分解包括统计数据分析的经典方法，例如:</p><ul class=""><li id="d2ad" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated"><strong class="ig hi">主成分分析</strong></li><li id="db73" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><strong class="ig hi">费希尔判别分析</strong></li><li id="dd07" class="jc jd hh ig b ih jm il jn ip jo it jp ix jq jb jh ji jj jk bi translated"><strong class="ig hi">多维标度(MDS) </strong></li></ul><p id="ee3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将在以后的文章中深入研究PCA。然而，重要的是要注意，特征分解仅用于方阵。可以应用于非方阵的更一般的矩阵分解方法是<strong class="ig hi">奇异值分解(SVD) </strong>，这将在我的下一篇文章<em class="km"/>中讨论，我也将使用SVD进行图像压缩，并展示它的一些应用。</p><p id="873b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望你清楚地理解特征分解是如何工作的，你将继续关注未来的帖子。</p><div class="lz ma ez fb mb mc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="md ab dw"><div class="me ab mf cl cj mg"><h2 class="bd hi fi z dy mh ea eb mi ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mj l"><h3 class="bd b fi z dy mh ea eb mi ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mk l"><p class="bd b fp z dy mh ea eb mi ed ef dx translated">medium.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq ke mc"/></div></div></a></div></div></div>    
</body>
</html>