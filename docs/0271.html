<html>
<head>
<title>Explain it to me like a 5-year-old: Introduction to LSTM and Attention Models — Part 2/2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">像一个5岁小孩一样解释给我听:LSTM和注意力模型介绍——第二部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/explain-it-to-me-like-a-5-year-old-introduction-to-lstm-and-attention-models-part-2-2-16482a58b30b?source=collection_archive---------1-----------------------#2021-03-14">https://medium.com/mlearning-ai/explain-it-to-me-like-a-5-year-old-introduction-to-lstm-and-attention-models-part-2-2-16482a58b30b?source=collection_archive---------1-----------------------#2021-03-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0aec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我是一个产品经理，有一些深度学习和数据科学的背景。深度学习令人生畏，我正试图让它尽可能直观。如果您认为下面的任何内容需要纠正，请随时联系我或发表评论-我欢迎反馈。</p><p id="c8b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我在这里的意图是让理解深度学习变得容易和有趣，而不是数学和全面。我没有详细解释每件事，但我相信在博客结束时，你将能够理解LSTM是如何工作的，并且你将永远不会忘记它！！！；)</p><p id="8687" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欢迎在LinkedIn 上与我联系！</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/2c188ce358a02714adb6a673d82e9b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bMOqAPdAd6iCsT-P"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Photo by <a class="ae jc" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Possessed Photography</a> on <a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="88f8" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">长短期记忆(LSTM)网络；</h1><p id="3ec6" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">正如我们之前在简单rnn的情况下看到的，如果rnn序列很大，它会导致梯度问题，主要是因为当信息从最后一个NN反向传播到第一个NN时，在它们的隐藏状态中可能会有一些数据丢失。因此，在处理大句子和记忆大句子的上下文时，这使得简单的RNNs不太有效。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ld"><img src="../Images/7d4930fe5670f123b703ae13c6bd323d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7gZjywFG3HOnL_g9tmYLQ.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.1 MIT 6.S191</figcaption></figure><p id="44cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，我们使用LSTM网络。它们只不过是简单神经网络的修改版本，但不是使用单个非线性函数和一个隐藏状态来完成所有工作(图3.1)，我们有多个非线性函数来帮助我们控制信息流和一个隐藏状态(图3.2)。这样，在反向传播时，隐藏状态就不必做所有的工作</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ld"><img src="../Images/88df7587cc26903c3297946053571206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6q3W0h8Sp2QdXKhuBSU6A.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.2 MIT 6.S191</figcaption></figure><p id="9b71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LSTM的信息是在盖茨的帮助下控制的。盖茨有选择地让信息通过。</p><h2 id="4967" class="le kb hh bd kc lf lg lh kg li lj lk kk ip ll lm ko it ln lo ks ix lp lq kw lr bi translated">LSTMs是如何工作的？</h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ls"><img src="../Images/8768fb2dcbc2f33535de51ee4a061c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ko7NeVyr6RSrLhZa79kt9A.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.3 MIT 6.S191</figcaption></figure><p id="b0a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这四个步骤主要体现了每个LSTM单元的工作原理:</p><ul class=""><li id="8d43" class="lt lu hh ig b ih ii il im ip lv it lw ix lx jb ly lz ma mb bi translated"><strong class="ig hi">忘了:</strong></li></ul><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ls"><img src="../Images/88fb26544ebb6aaa706a5eb9d9daf2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJ2el2rePGAqNHFBX0RhLg.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.4 MIT 6.S191</figcaption></figure><p id="3e30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个细胞从忘记所有与先前状态无关的信息开始。如图所示，先前的隐藏状态和当前输入通过sigmoid函数传递，该函数调节应保留多少内容和应保留多少内容(sigmoid输出0或1)，并将其存储在单元状态(c(t))</p><ul class=""><li id="f7d5" class="lt lu hh ig b ih ii il im ip lv it lw ix lx jb ly lz ma mb bi translated"><strong class="ig hi">店铺</strong>:</li></ul><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ls"><img src="../Images/72756605a42ce07fddfc5b3e374e8c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxH3YVknxeHzzC6DYM2tJA.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.5 MIT 6.S191</figcaption></figure><p id="191b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我们确定旧信息的哪一部分和新信息的哪一部分相关，并将其存储到单元状态中。</p><ul class=""><li id="0487" class="lt lu hh ig b ih ii il im ip lv it lw ix lx jb ly lz ma mb bi translated"><strong class="ig hi">更新</strong>:</li></ul><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ls"><img src="../Images/27c2db9838dafeb5e76e0ee72cb7adb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJyTGfRRVeHDU_TMM7T5sg.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.6 MIT 6.S191</figcaption></figure><p id="321b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LSTMs最关键的部分是它们为单元状态(c(t))及其隐藏状态保持不同的值，该隐藏状态通过这些门选择性地更新。</p><ul class=""><li id="2608" class="lt lu hh ig b ih ii il im ip lv it lw ix lx jb ly lz ma mb bi translated"><strong class="ig hi">输出</strong></li></ul><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ls"><img src="../Images/78c39931835a78224e6a2b3231a58f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxWNIH2QuzO9-IaAayY7-w.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.6 MIT 6.S191</figcaption></figure><p id="f8c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们有一个来自LSTM的输出，也叫做隐藏状态。输出门控制来自单元状态的多少编码信息应该最终通过其当前隐藏状态流到下一个单元</p><p id="2c66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="mc">关键要点:</em></strong><em class="mc">lstm通过维护单元状态和隐藏状态来控制每个单元状态内的信息流，这有助于它们解决消失梯度的问题。</em></p><p id="8b82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">lst ms中的反向传播:</strong></p><p id="55e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">几分钟前，我们观察到反向传播在简单RNN中是一个问题，因为存在梯度消失的风险，我们谈到了LSTMs能够帮助解决这个问题。但是怎么做呢？正如我们所看到的，在LSTM中有一个隐藏状态和一个细胞状态——反向传播只应用于细胞状态，它使用门控机制选择性地更新，因此我们有一个不间断的梯度流。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es md"><img src="../Images/9d572a39053e06beb8b6c868e019f654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVoqZW_Ilqurn8vlw6QbGg.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 3.7 MIT 6.S191</figcaption></figure><p id="6e91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="mc">直觉</em> </strong>:基本上你可以把隐藏状态看成城市道路，把单元格状态看成高速公路。所以，如果你想从纽约去波士顿，你是愿意使用需要9个小时才能到达波士顿的城市道路，还是使用4个小时就能到达波士顿的高速公路？因此LSTMs工作得更好，因为它们使用单元状态(高速公路)进行反向传播。</p><h1 id="64ec" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">注意力模型:</h1><p id="7b7b" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">RNNs的一个有趣的应用是在机器翻译中；把句子从一种语言翻译成另一种语言。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es me"><img src="../Images/5b5c3cbdf0d578784f65ac7ff8bbfe6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u33oN9K3kMTmfbvMHTjWYw.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 4.1 MIT 6.S191</figcaption></figure><p id="7878" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上图中，您可以看到RNNs如何用于机器翻译的架构。我们提供给RNN的输入使用编码器编码为状态向量，在执行计算之后，状态向量然后使用解码器解码为RNN的输出。</p><p id="1ba7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是使用RNNs进行机器翻译有几个问题:</p><ul class=""><li id="e6c1" class="lt lu hh ig b ih ii il im ip lv it lw ix lx jb ly lz ma mb bi translated"><strong class="ig hi">编码瓶颈:<br/></strong></li><li id="52a2" class="lt lu hh ig b ih mf il mg ip mh it mi ix mj jb ly lz ma mb bi translated"><strong class="ig hi">缓慢的顺序模型:<br/> </strong>由于这是一个顺序模型，因此没有机会进行并行计算，这导致模型的训练时间缓慢</li><li id="c2b7" class="lt lu hh ig b ih mf il mg ip mh it mi ix mj jb ly lz ma mb bi translated"><strong class="ig hi">不长记忆:<br/> </strong>我知道我们通过从简单的RNNs切换到LSTM解决了处理大句的问题，但当涉及到机器翻译时，lstm很难处理非常长的时间依赖性。</li></ul><p id="d85b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，我们如何处理大型文本序列和正文中的这种依赖性呢？立正！</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es mk"><img src="../Images/2a164b1512ea57c56e8ff82fbba5545c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6rq7sQ3EWGzqQ4gd0sbAg.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Fig 4.2 MIT 6.S191</figcaption></figure><p id="5412" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不是解码器只能访问编码器的最终状态，而是可以使用attentions来授权解码器访问原始句子中每个时间步长之后的状态，并且模型将在整个训练中学习这些向量的权重。神经网络中的注意机制提供了可学习的记忆访问。基本上，模型被训练成只关注那些对输出有很大影响的输入部分。</p><p id="a1f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="mc">直觉</em> </strong>:如果我问你“你叫什么名字？”你回复说“我的名字是Ameya”。我住在纽约，喜欢跳舞。我忘了说我姓尚巴格。”因此，模型将给予第一句和最后一句更多的权重(关注)，因为这是更重要的！</p></div><div class="ab cl jd je go jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="ha hb hc hd he"><p id="b956" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那都是乡亲们！</p><p id="9bec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请随意跳转到https://www.youtube.com/watch?v=qjrad0V0uJE<a class="ae jc" href="https://www.youtube.com/watch?v=qjrad0V0uJE&amp;ab_channel=AlexanderAmini" rel="noopener ugc nofollow" target="_blank">的艾娃讲座:)</a></p><p id="529d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一定要看看我的其他帖子，以获得更多关于金融和技术的知识。</p><p id="d38f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想让我写一篇关于深度学习的文章，请务必告诉我，我会尽力用更简单的术语解释它。另外，欢迎在评论区提问。会很乐意帮你的:)</p><p id="f2d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PS:我用的类比可能不是100%正确，但用一个更简单的类比很容易理解。</p><p id="707d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">学分:麻省理工学院开放课件</p></div></div>    
</body>
</html>