<html>
<head>
<title>Linear Regression : Part-2 (Gradient Descent)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:第二部分(梯度下降)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/linear-regression-part-2-gradient-descent-7d4ff9d81520?source=collection_archive---------3-----------------------#2022-04-26">https://medium.com/mlearning-ai/linear-regression-part-2-gradient-descent-7d4ff9d81520?source=collection_archive---------3-----------------------#2022-04-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9065" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">读者你好，希望你做得很好！！</p><p id="ce6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我之前的文章中，我写了关于使用OLS的线性回归。</p><p id="7ad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是它的链接:<a class="ae jc" rel="noopener" href="/@pradhyumn.16jics040/linear-regression-using-ols-481d6daf1f8">https://medium . com/@ pradhyumn . 16 jics 040/linear-regression-using-ols-481 D6 daf 1 f 8</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/27cffb9b25c5bb087eb5a06f7a5069fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6wEQZeeaX11kcHYkZ9GcIw.jpeg"/></div></figure><p id="2f1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数学就像一个由很多工具组成的盒子，其中一个就是著名的算法:<strong class="ig hi"><em class="jl"/></strong></p><h1 id="7c02" class="jm jn hh bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak"> 1。定义:- </strong></h1><p id="09df" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated"><strong class="ig hi">梯度</strong>:表示输出相对于输入的变化。Is是曲线在给定点的斜率(任意点x1处的dy/dx)。</p><p id="0935" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">下降</strong>:向下的意思。</p><p id="5d99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jl">梯度下降</em> </strong>是一种迭代优化算法，帮助我们找到函数的最小值。它在不同的领域有许多应用，但这里我们主要关注ML，因此该算法帮助我们最小化模型的成本函数。</p><p id="f191" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们继续之前，让我们先了解一下函数。</p><h1 id="1e23" class="jm jn hh bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak"> 2。功能:- </strong></h1><p id="5c86" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">函数是一组各有一个输出的输入之间的关系。</p><p id="3164" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jl">举例:</em></strong>T22】y = f(u，w)，y是输入u和w映射的输出</p><p id="6ec5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你一定听说过<strong class="ig hi"> <em class="jl">凸&amp;凹功能</em> </strong>。凹函数用在我们必须寻找函数最大值的地方。</p><p id="100a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但我们的重点是最小化，所以要应用梯度下降，函数需要在本质上是凸的，而且是可微的。</p><p id="d934" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jl">可微函数</em> </strong> <em class="jl">是那些对其定义域(所有可能输入的集合)中的每一点都有导数的函数。</em></p><p id="94bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">可微的例子:</strong> y = x + 2x - 6</p><p id="3f78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">不可微的例子:</strong> y = | x |</p><p id="f333" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jl">凸函数</em> </strong> <em class="jl">是二阶导数(f''(x))为&gt; 0的那些。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kp"><img src="../Images/af8d4e2ee4458bbd8662869b76d957af.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*-Ej7vL8nmq3qy3vZhQ1e0Q.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx"><em class="ku">Convex has one minima but non-convex has two</em></figcaption></figure><p id="9746" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们也可以将梯度下降应用于非凸函数，但问题是很难找到全局最小值，我们经常会停留在任何局部最小值，可能永远也找不到全局最小值。</p><h1 id="234a" class="jm jn hh bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">3.数学:-</h1><p id="71fa" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">到目前为止，我们知道我们必须最小化成本函数，线性回归的成本函数是均方误差(MSE ),数学上表示为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/82bc3f3f64c066e230d65c7b0fc1ef80.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*ECEP3bPrI1bKclMMU7v4jA.jpeg"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">MSE (Yi is actual and Y-hat is predicted value)</figcaption></figure><p id="c8ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简单起见，让我们取一个自变量X，所以我们最终得到这个方程:<strong class="ig hi"> Y = </strong> <strong class="ig hi"> β0 + β1*X，</strong> (β0 =截距，β1 = X的系数)</p><p id="8ef6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">梯度下降方程:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kw"><img src="../Images/53a53f22521ba2ec0b582163ee4bb88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*IsgZid5BCAO7zERulY0x4g.jpeg"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Gradient Descent Eq.</figcaption></figure><blockquote class="kx ky kz"><p id="a3ea" class="ie if jl ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">对于人类来说，通过查看图表，更容易说出我们应该向哪个方向移动，向下还是向上，以达到最小值，但机器将如何计算出。这就是导数帮助我们的地方，在每一点，我们计算导数，调整方程的参数，向负斜率的方向移动。</p></blockquote><p id="03a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一步:</strong>将方程(β0，β1)的参数初始化为0。</p><p id="72f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤2: </strong>选择一个学习率/步长，它不能太低，否则这个过程会比Internet Explorer慢，也不能太高，否则这个参数会像猴子一样从图中的一端跳到另一端。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/c541c8af3109c90eedc1c565da092c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VVY5kvlwpkuIa1yAcjbFQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Different step-size impact</figcaption></figure><p id="6685" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三步:</strong>现在计算∂Y/∂β0 &amp; ∂Y/∂β1.给出的y w r tβ0和β1的偏导数</p><p id="a163" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第四步:</strong>将这些值输入梯度下降方程，得到新值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/dfc6d72b7e41c0bf924155e51d76e9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*-WOV9KYjGV-PfFuxPQuysA.jpeg"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">New values of parameters</figcaption></figure><p id="d341" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤5: </strong>重复步骤3 &amp; 4，直到我们的代价函数值变得很低或者接近0。最后的<strong class="ig hi"> </strong> β0 &amp; β1的值是我们的优化值，我们在线性回归方程中使用这些值来生成预测。</p><p id="7c12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在多个独立变量的情况下，相同的过程适用于它们的所有系数，实际上我们使用矩阵(numpy)来实现这些步骤，这使得过程足够快。</p><p id="180e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">希望你从帖子中学到了。感谢阅读。祝您愉快！！</strong></p><p id="fcd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">快乐编码！！</strong></p><div class="lj lk ez fb ll lm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hi fi z dy lr ea eb ls ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">medium.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma jj lm"/></div></div></a></div></div></div>    
</body>
</html>