<html>
<head>
<title>Machine Learning Cheat Sheet for Data Scientist Interview : Regularization Frequently Asked Questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家面试的机器学习备忘单:规范化常见问题</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/machine-learning-cheat-sheet-for-data-scientist-interview-regularization-frequently-asked-e6007e89ae58?source=collection_archive---------2-----------------------#2022-02-04">https://medium.com/mlearning-ai/machine-learning-cheat-sheet-for-data-scientist-interview-regularization-frequently-asked-e6007e89ae58?source=collection_archive---------2-----------------------#2022-02-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="c285" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">这里总结了在数据科学家面试中经常被问到的ML问题。我已经尽可能保持备忘单的简洁。所有的内容都是必须记住的关键知识，并且不断更新。希望可以作为你DS面试前的准备指南！</p></blockquote><h1 id="22e2" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">基本解释</h1><p id="3d8d" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">向损失函数添加正则项。它缩小了每个参数，有助于解决过拟合问题。</p><h1 id="b052" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">p-范数定义</h1><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es km"><img src="../Images/4a170712d04af22ca249dc1758d9a92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*X9J1a6OeNRgZj1ignYSqgw.png"/></div></figure><p id="9536" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">当p = 1时:套索回归；当p = 2时:岭回归</p><h1 id="bef1" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">正则化成本函数</h1><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ku"><img src="../Images/862ace875d051aef22b6a1d1ad5d9c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*VpfOuEtwXwnBw4_1_9fhtw.png"/></div></figure><p id="00f7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">注:等价于在∑θ ⱼ <c where="" c="" is="" a="" constant.=""/></p><h1 id="0805" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">How does regularization shrink the parameter?</h1><p id="30d5" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">When updating θ using gradient descent, we can derive that the shrinkage term is less than 1, which means it shrinks the parameter a little bit for every update.</p><h1 id="9441" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">Choosing λ</h1><p id="a768" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">λ is the regularization parameter. It controls the trade-off between model complexity and model fitting.</p><ul class=""><li id="a28d" class="kv kw hh ik b il im ip iq kg kx ki ky kk kz jf la lb lc ld bi translated">If λ is too small: there is almost no regularization. It doesn’t shrink the parameter too much → prone to be overfitting</li><li id="7ec2" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">If λ is too large: it will shrink the parameters too much, all the parameters will be close to zero → prone to be underfitting</li><li id="f470" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">Usually we use cross-validation to choose the best λ.</li></ul><h1 id="118f" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">Ridge regression (L2 norm)</h1><p id="e11f" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated"><strong class="ik hi">损失函数</strong>的条件下最小化代价函数</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lj"><img src="../Images/7a1fd2019bac37b7b314fb1217db5b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*H32AlUYbOOEFMMwDFevolg.png"/></div></figure><p id="38ef" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">它通过变量权重的平方和来惩罚参数。注意:它不惩罚θ₀的偏见术语</p><p id="df12" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">岭回归的正规方程</strong></p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lk"><img src="../Images/9ae564900ddf2890de149791fbcae034.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*y0HnCfnOc5Ha15TNPDAPbw.png"/></div></figure><p id="4e6e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">增加一个λI项，帮助解决矩阵的可逆性。</p><h1 id="ed1c" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated"><strong class="ak">拉索回归(L1范数)</strong></h1><p id="6318" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated"><strong class="ik hi">损失函数</strong></p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ll"><img src="../Images/08752e0516d15b5a00e4c7a55eb1062a.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*vjHBFdZtWwMib0dnWJu7Tw.png"/></div></figure><p id="dc9b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">它通过变量权重的绝对值之和来惩罚参数。</p><h1 id="ca90" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">Lasso和岭回归的比较</h1><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lm"><img src="../Images/355291d84a95414673f71c8ea35af5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlUTZYJZWEaEYk17UpBs7A.png"/></div></div></figure><p id="5577" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">为什么Lasso能把系数缩小到正好为零？</strong></p><p id="9d67" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">我们可以用几何解释来解释它:</p><ul class=""><li id="abd9" class="kv kw hh ik b il im ip iq kg kx ki ky kk kz jf la lb lc ld bi translated">不同的正则项导致不同的约束区域形状。考虑具有两个变量的模型，那么对于约束区域，套索具有菱形形状，而脊具有圆形形状。</li><li id="281d" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">等高线代表具有不同θⱼ.值的损失函数值</li><li id="6d74" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">当损失函数的轮廓线与约束边界相切时，找到最优点。</li><li id="5839" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">对于套索回归，最优点很可能在拐角上，这意味着一些参数将为0。</li><li id="b73d" class="kv kw hh ik b il le ip lf kg lg ki lh kk li jf la lb lc ld bi translated">对于岭回归，最优点可以位于约束边界的任何地方，因此最优点不可能包含0值。</li></ul><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lr"><img src="../Images/2bd085ed1e834dcef91e7f309821e145.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*ieGv7bGGNg7hF2cWQ3czsw.png"/></div></figure><h1 id="6650" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">弹性网</h1><p id="b6e0" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">套索和山脊的混合，包括L1和L2正则化的加权组合。</p><div class="ls lt ez fb lu lv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lw ab dw"><div class="lx ab ly cl cj lz"><h2 class="bd hi fi z dy ma ea eb mb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mc l"><h3 class="bd b fi z dy ma ea eb mb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="md l"><p class="bd b fp z dy ma ea eb mb ed ef dx translated">medium.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj ks lv"/></div></div></a></div><p id="ba0d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><a class="ae mk" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ik hi">成为作家</strong> </a></p></div></div>    
</body>
</html>