<html>
<head>
<title>Unsupervised Learning — Let The Algorithm Find By Itself</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习——让算法自己去发现</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/unsupervised-learning-let-the-algorithm-find-by-itself-8ef0628dac9?source=collection_archive---------9-----------------------#2022-07-10">https://medium.com/mlearning-ai/unsupervised-learning-let-the-algorithm-find-by-itself-8ef0628dac9?source=collection_archive---------9-----------------------#2022-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="afca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我将讨论<strong class="ig hi">无监督学习</strong>的重要性及其实用性，并强调主导该领域的方法和算法。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/28342ea5b4fbb4a93b68f6df7a190eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7-GLmEDbbPfZh10g"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@mariashanina?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Maria</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="068d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">引入<strong class="ig hi">无监督</strong> <strong class="ig hi">学习</strong>总是比引入<strong class="ig hi">有监督学习</strong>更容易。实际上，它们都使用机器来训练基于算法的模型，然而，在监督学习的情况下，这些机器是被引导的。正如我在最近的一些文章中所说的那样(<a class="ae js" rel="noopener" href="/mlearning-ai/the-well-known-regression-what-is-it-6dbe5e0b2cd1">众所周知的回归</a>——它是什么？或<a class="ae js" rel="noopener" href="/mlearning-ai/understand-classification-problems-in-less-than-5-minutes-712244d259c8">在不到5分钟的时间内理解分类问题</a>)，训练模型执行监督学习任务在于给出机器示例(输入和输出变量)并告诉它从这些示例中学习。然后，将通过最小化成本/损失函数来优化模型，该函数最能描述预测中的误差。</p><p id="3c9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">无监督学习不使用任何目标，因为这不是那种机器学习的目的。相反，该模型将再次基于一种算法，试图理解输入变量，但没有<strong class="ig hi"/><strong class="ig hi">输出</strong>变量。它将基本上<strong class="ig hi">创建</strong> <strong class="ig hi">组</strong>，这些组由<strong class="ig hi">相似的</strong> <strong class="ig hi">观察</strong>组成。其想法是更好地理解数据集，并使用机器来识别观察结果之间的区别。</p><p id="3a75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们看看无监督学习的一些具体用例:</p><ul class=""><li id="10d1" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated"><strong class="ig hi">客户细分</strong>是应用无监督学习的经典用例之一。该模型将从描述客户特征的所有示例中学习，并创建收集相似客户的组(集群)。目标是更好地了解想在商店购买产品或服务的人，并为他们提供最好的体验，以便他们购买和消费。</li><li id="9979" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">异常</strong> <strong class="ig hi">检测</strong>是一个用例，其中的想法并不是真正找到聚类，而是找到倾向于与其他聚类隔离的观察。为了发现这些异常，该模型创建聚类，并将这些聚类中没有发现的太远的观测值声明为异常。最著名的用例是银行业中的欺诈检测。</li><li id="cf5b" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">降维</strong> <strong class="ig hi">降维</strong>，正如我在上一篇文章(<a class="ae js" rel="noopener" href="/mlearning-ai/dimensionality-reduction-fight-your-model-complexity-b80143b60b48">降维——对抗你的模型复杂性</a>)中谈到的，可以利用聚类，主成分分析(PCA)显然就是这种情况。</li><li id="96c3" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">数据</strong> <strong class="ig hi">分析</strong>也在使用无监督学习，因为数据可以在新的组中进行总结，然后用另一种视觉进行分析。</li></ul><p id="e866" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们清楚地看到了无监督学习是什么，让我们谈谈主要的方法和算法…</p><h1 id="e7cb" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">方法和算法</h1><p id="1927" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">存在不同的方法来分离数据集或识别可以对立的结构，让我们看看一些最重要的方法；</p><h2 id="e5aa" class="lk ki hh bd kj ll lm ln kn lo lp lq kr ip lr ls kv it lt lu kz ix lv lw ld lx bi translated">排他和重叠聚类的k-均值</h2><p id="e055" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">首先，<strong class="ig hi">聚类</strong>是一种倾向于在不同特征的不同组中收集数据观察值的技术。如果您在二维空间上表示数据集，您可以看到<strong class="ig hi">簇</strong>:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ly"><img src="../Images/eb8d2b585b3114d1d8760ef1f155768f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snccKSQGPXhs0Hev2f3iDA.jpeg"/></div></div></figure><p id="15bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">排他</strong>或<strong class="ig hi">非</strong> <strong class="ig hi">重叠</strong> <strong class="ig hi">聚类</strong>是一种倾向于将每个数据点与单个组相关联的聚类技术。下图是一个带有线性决策边界的<strong class="ig hi"> Voronoi图</strong>:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lz"><img src="../Images/4e2e3f2f24416f6868efc479c937bda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TUnw3jlMkZuCJq4sl5RrOA.jpeg"/></div></div></figure><p id="f70c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> K-Means </strong>算法是应用最广泛的无监督算法之一，因为它的性能和易于解释。事实上，K-Means旨在找到K个聚类的质心或中面，并将每个数据点分配给它们。这通常通过排他聚类来完成，也称为<strong class="ig hi">硬</strong> <strong class="ig hi">聚类</strong>，但是<strong class="ig hi">软</strong> <strong class="ig hi">聚类</strong>也是可能的(数据点将根据属于一个组的概率和分数的相似性被分成几个组)。</p><p id="39ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是如何工作的？</p><p id="91ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k-表示进行<strong class="ig hi"> 3步</strong>:</p><ol class=""><li id="196b" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb ma jz ka kb bi translated">质心的随机初始化</li><li id="f426" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb ma jz ka kb bi translated">将每个数据点分配到聚类中(基于质心)，并计算聚类中所有观察值与质心的平均距离。</li><li id="6bd1" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb ma jz ka kb bi translated">根据之前计算的平均距离更新质心作为新质心。</li></ol><p id="1a34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些步骤一直进行到形心不再移动为止。</p><p id="61f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">K-Means有<strong class="ig hi">一些限制</strong>:</p><ul class=""><li id="88a5" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">它需要多次迭代。</li><li id="9aae" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">知道潜在聚类的数量。</li><li id="022e" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">它不能很好地处理复杂的数据结构(非线性决策边界)。</li></ul><p id="1e84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您想尝试K-Means，<strong class="ig hi"> Scikit-Learn </strong>在其库中提供了一个实现:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/8ab744e7c5629982970eceb466a7ac01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bx12Q78grsLnVgon0XduIQ.png"/></div></div></figure><p id="faff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它有很多超参数，可以大大提高模型性能，找到更好的聚类(这里的doc是<a class="ae js" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">这里的</a>)。</p><h2 id="6d39" class="lk ki hh bd kj ll lm ln kn lo lp lq kr ip lr ls kv it lt lu kz ix lv lw ld lx bi translated">基于密度聚类的DBSCAN</h2><p id="9185" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated"><strong class="ig hi">基于密度的聚类</strong>假设聚类以其密度而非质心为特征。其思想是，聚类是由附近的数据观察形成的，而不是像K-Means那样由决策边界分隔。</p><p id="7f48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> DBSCAN </strong>(基于密度的带噪声应用聚类)是该方法背后的通用算法。</p><p id="faa8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是如何工作的？</p><ol class=""><li id="4bef" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb ma jz ka kb bi translated">计算各点之间的距离:<strong class="ig hi"> ε </strong>。这个距离是固定的，因此一个数据点可以基于ε在其周围有最大的观察值。</li><li id="7605" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb ma jz ka kb bi translated">如果一个数据点周围的观察计数在一个固定数以上，则意味着这个点是一个<strong class="ig hi">核心</strong> <strong class="ig hi">实例</strong>(被许多其他数据点包围)。与核心实例相关联的所有点形成唯一的集群。</li><li id="480c" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb ma jz ka kb bi translated">核心实例也可以是其他核心实例的数据点，因此，核心实例的所有数据点一起形成一个唯一的(更大的)集群。</li></ol><p id="1c94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有聚类被低密度区域分隔开，不在聚类中的数据点是异常值或噪声。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mc"><img src="../Images/60e6a1cd32bc0cecc8c4b3f14bd8bf71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IXx0XSf_RCfDgV78DgwfA.jpeg"/></div></div></figure><p id="07bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DBSCAN也可以在<strong class="ig hi"> Scikit-Learn </strong>中获得:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/dee90368d9c062af264201dca105db12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*83Y79T0Tbwm0v-zzo61OcA.png"/></div></div></figure><h2 id="ce1b" class="lk ki hh bd kj ll lm ln kn lo lp lq kr ip lr ls kv it lt lu kz ix lv lw ld lx bi translated">概率聚类的高斯混合模型</h2><p id="7e46" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated"><strong class="ig hi">概率</strong> <strong class="ig hi">聚类</strong>是另一种聚类和无监督学习的方法，它在基于组的基础上寻找数据点；</p><blockquote class="md"><p id="10f5" class="me mf hh bd mg mh mi mj mk ml mm jb dx translated">关于他们属于特定分布的<strong class="ak">可能性</strong>(<a class="ae js" href="https://www.ibm.com/cloud/learn/unsupervised-learning" rel="noopener ugc nofollow" target="_blank">无监督学习——IBM</a>)。</p></blockquote><p id="aa35" class="pw-post-body-paragraph ie if hh ig b ih mn ij ik il mo in io ip mp ir is it mq iv iw ix mr iz ja jb ha bi translated">概率聚类<strong class="ig hi">支持的主要算法，高斯混合模型</strong>，是一种假设数据点遵循各种高斯分布的技术。每个星团都呈椭圆形。</p><blockquote class="ms mt mu"><p id="c678" class="ie if mv ig b ih ii ij ik il im in io mw iq ir is mx iu iv iw my iy iz ja jb ha bi translated">GMM是一种无监督聚类技术，它基于使用期望最大化的概率密度估计来形成椭圆形聚类。伊娃·帕特尔</p></blockquote><p id="c2c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">高斯分布关于平均值对称，它们的形状由平均值和标准差描述:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mz"><img src="../Images/e2790b75f6da2da73f5935793afc23ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAPOXNHk_K-zZ4E95R6Sfw.jpeg"/></div></div></figure><p id="8d44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> GMMs </strong>和<strong class="ig hi"> K-Means </strong>之间的一个很大的区别是，GMMs给出了关于簇的大小、形状和方向的信息，这与K-Means的情况不同(不使用协方差矩阵)。</p><p id="f230" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了能够创建聚类，GMM可以使用期望最大化(EM ),该期望最大化基于初始化的权重给出数据点属于聚类的估计概率。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es na"><img src="../Images/603afc5bdc36321b6e0edd11ae91e02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHKJIBd7McP3DXaNstULLw.jpeg"/></div></div></figure><p id="c168" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当GMMs对所有分布采用单一协方差类型时，效果最好，速度最快。这意味着每个星团都有相同的形状和大小。</p><p id="ec74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与其他模型一样，高斯混合模型在<strong class="ig hi"> Scikit-Learn </strong>中可用:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/e7c56634ead1129113809319c7952cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RijF3at9fSNMmfYrkIv4Aw.png"/></div></div></figure><h2 id="f73f" class="lk ki hh bd kj ll lm ln kn lo lp lq kr ip lr ls kv it lt lu kz ix lv lw ld lx bi translated">无监督学习的其他方法</h2><p id="9e1c" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated">无监督学习还有其他技术，我们来看看。</p><ul class=""><li id="49f6" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated"><strong class="ig hi">层次聚类:</strong>通过聚集(凝聚聚类)或分割(分割聚类)找到聚类。第一种方法将每个数据点视为一个唯一的聚类，并根据它们之间的关系合并它们，最终得到更大的聚类。相反，分裂聚类从一个唯一的聚类(所有数据点)开始，并根据观察值之间的差异将其分开(就像决策树一样)。</li><li id="2a89" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated"><strong class="ig hi">关联规则:</strong>这种技术基本上是通过数据点的相似性(例如2个客户)来比较数据点，并承认对于接近的数据点，一个数据点的特征也表征另一个数据点的特征(特别用于推荐系统)。Apriori是关联规则的主要算法。</li></ul><h1 id="baa4" class="kh ki hh bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="b6e2" class="pw-post-body-paragraph ie if hh ig b ih lf ij ik il lg in io ip lh ir is it li iv iw ix lj iz ja jb ha bi translated"><strong class="ig hi">无监督学习</strong>是机器学习的一个分支，用于获得关于所有类型数据集的见解。它还可用于数据集缺少标注的半监督问题。<br/>存在许多方法，每种方法都有其优点和缺点。<strong class="ig hi">自动编码器</strong>是一种新的编码器，它正在彻底改变这个领域，因为它是基于神经网络的力量。</p><p id="2de4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你学习文章，希望你学到了一些关于无监督学习的趣事！</p><h2 id="d27d" class="lk ki hh bd kj ll lm ln kn lo lp lq kr ip lr ls kv it lt lu kz ix lv lw ld lx bi translated">资源</h2><div class="nb nc ez fb nd ne"><a href="https://www.ibm.com/cloud/learn/unsupervised-learning" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">什么是无监督学习？</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">了解无监督学习如何工作，以及如何使用它来探索和聚类数据</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">www.ibm.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jm ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://ryanwingate.com/intro-to-machine-learning/unsupervised/hierarchical-and-density-based-clustering/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">基于层次和密度的聚类</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">K-Means是一个伟大的算法，对于某些特定的情况下，依赖于距离质心的距离作为一个定义…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">ryanwingate.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns jm ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://reader.elsevier.com/reader/sd/pii/S1877050920309820?token=CF46A69AAD80C153FD85E5876FB3C32EC09F96C9D92F67A1EC86949F356D6A80F547CEF82063513C7EA0F234BA9C90CF&amp;originRegion=eu-west-1&amp;originCreation=20220710075052" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">集群云工作负载:K均值与高斯混合模型</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">由于大数据、物联网和业务数据分析等多样化的云工作负载而导致的日益增长的异构性，需要…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">reader.elsevier.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns jm ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">通过最近的一系列突破，深度学习推动了整个机器学习领域。现在，甚至…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">www.oreilly.com</p></div></div><div class="nn l"><div class="nv l np nq nr nn ns jm ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hi fi z dy nj ea eb nk ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">medium.com</p></div></div><div class="nn l"><div class="nw l np nq nr nn ns jm ne"/></div></div></a></div></div></div>    
</body>
</html>