<html>
<head>
<title>Best Day of My Life — Regression Analysis to Predict the Price of Diamonds</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我生命中最美好的一天——回归分析预测钻石价格</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/best-day-of-my-life-9eadaa14ef44?source=collection_archive---------1-----------------------#2022-09-09">https://medium.com/mlearning-ai/best-day-of-my-life-9eadaa14ef44?source=collection_archive---------1-----------------------#2022-09-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3bf8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希瑟·祖雷尔</p><p id="f4b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">2022–09–08</p><p id="2864" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我生命中最美好的一天是我的伴侣向我求婚的时候。他很好奇他给我买的钻石是不是多付了钱(他认为他买得很划算！)—所以我们来看看。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/73cea7eecfb743439148dfc69f337aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvmVkPaDzFiKrRLksgHDCQ.jpeg"/></div></div></figure><p id="1d39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将使用钻石数据集来训练一个回归模型，根据钻石的物理属性(包括大小/克拉、净度、颜色等)来预测钻石的价格。</p><p id="44c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一步是一些<strong class="ig hi">探索性的数据分析</strong>，看看我们是否需要在训练模型之前做些什么来清理数据。</p><p id="58e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有一些额外的分析，图表等。包含在<a class="ae js" href="https://github.com/Zeather709/best-day-of-my-life" rel="noopener ugc nofollow" target="_blank"> github </a>中的<code class="du jo jp jq jr b">diamonds.r</code>文件中，但没有出现在本文档中。</p><h1 id="23e5" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">准备和探索性数据分析</h1><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="5015" class="kv ju hh jr b fi kw kx l ky kz"># Setup<br/>library('tidyverse')<br/>library('GGally')<br/><br/># Import<br/>data(diamonds)<br/><br/># Exploratory <br/>summary(diamonds)</span><span id="64b3" class="kv ju hh jr b fi la kx l ky kz">ggpairs(diamonds)</span><span id="d3ea" class="kv ju hh jr b fi la kx l ky kz">##      carat               cut        color        clarity          depth      <br/>##  Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  <br/>##  1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  <br/>##  Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  <br/>##  Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  <br/>##  3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  <br/>##  Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00  <br/>##                                     J: 2808   (Other): 2531                  <br/>##      table           price             x                y         <br/>##  Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000  <br/>##  1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  <br/>##  Median :57.00   Median : 2401   Median : 5.700   Median : 5.710  <br/>##  Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735  <br/>##  3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  <br/>##  Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900  <br/>##                                                                   <br/>##        z         <br/>##  Min.   : 0.000  <br/>##  1st Qu.: 2.910  <br/>##  Median : 3.530  <br/>##  Mean   : 3.539  <br/>##  3rd Qu.: 4.040  <br/>##  Max.   :31.800  <br/>##</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/031d27c8912d2211421cb20738b8ca89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*EZTfr_JCkx00X6Zg4TDVbA.png"/></div></figure><p id="ec5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，x、y和z值对应于钻石的物理尺寸(以毫米为单位)——这些与克拉值密切相关，克拉值是衡量钻石重量的指标——这是有意义的——我们真的不需要将它们都包括在内。一些ML模型对包括多个彼此强烈相关的特征很敏感，这可能导致过度拟合，因此我们将在训练模型之前移除x、y和z变量。</p><h2 id="f502" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">目标变量的分布(美元价格)</h2><p id="1589" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">Q-Q图(或分位数-分位数图)用于快速、直观地识别单个变量的分布。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="fa9e" class="kv ju hh jr b fi kw kx l ky kz">qq_diamonds <strong class="jr hi">&lt;-</strong> qqnorm((diamonds<strong class="jr hi">$</strong>price),main<strong class="jr hi">=</strong>"Normal Q-Q Plot of Price");qqline((diamonds<strong class="jr hi">$</strong>price))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/f891b4e5908c7a071b03187aec561fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*NIX06Lo_jMTM06Xy.png"/></div></figure><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="9976" class="kv ju hh jr b fi kw kx l ky kz"><em class="lu"># Meh</em><br/><br/>qq_log_diamonds <strong class="jr hi">&lt;-</strong> qqnorm(<strong class="jr hi">log</strong>(diamonds<strong class="jr hi">$</strong>price),main<strong class="jr hi">=</strong>"Normal Q-Q Plot of log Price");qqline(<strong class="jr hi">log</strong>(diamonds<strong class="jr hi">$</strong>price))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/e03f12860beaea720c504af4a200493d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*9cLYFxqeQN--u3Ta.png"/></div></figure><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="3612" class="kv ju hh jr b fi kw kx l ky kz"><em class="lu"># Ooh this is a much better fit</em></span><span id="5967" class="kv ju hh jr b fi la kx l ky kz">hist_norm <strong class="jr hi">&lt;-</strong> ggplot(diamonds, aes(<strong class="jr hi">log</strong>(price)))  <strong class="jr hi">+</strong> <br/>  geom_histogram(aes(y <strong class="jr hi">=</strong> ..density..), colour <strong class="jr hi">=</strong> "black", fill <strong class="jr hi">=</strong> <br/>    'lightblue', bins <strong class="jr hi">=</strong> 50) <strong class="jr hi">+</strong> <br/>  stat_function(fun <strong class="jr hi">=</strong> dnorm, args <strong class="jr hi">=</strong> <strong class="jr hi">list</strong>(mean <strong class="jr hi">=</strong> <br/>    mean(<strong class="jr hi">log</strong>(diamonds<strong class="jr hi">$</strong>price)), sd <strong class="jr hi">=</strong> sd(<strong class="jr hi">log</strong>(diamonds<strong class="jr hi">$</strong>price))))<br/>hist_norm</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/86faae130ca8466a13d6e2f99b1254ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*cbB5ZZpjTd3cZMOi.png"/></div></figure><p id="03de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于Q-Q图和直方图，价格的对数似乎遵循双峰或多峰分布。让我们试试另外两块地。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="e9a9" class="kv ju hh jr b fi kw kx l ky kz">violin <strong class="jr hi">&lt;-</strong> ggplot(diamonds, aes(x <strong class="jr hi">=</strong> color, y <strong class="jr hi">=</strong> <strong class="jr hi">log</strong>(price), fill <strong class="jr hi">=</strong> <br/>  color))<br/>violin <strong class="jr hi">+</strong> <br/>  geom_violin() <strong class="jr hi">+</strong> <br/>  scale_y_log10() <strong class="jr hi">+</strong> <br/>  facet_grid(clarity <strong class="jr hi">~</strong> cut)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/6675198d94f9c3eeeb8844e03aec7346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*c_FTaR2xluz1mDv3.png"/></div></figure><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="477b" class="kv ju hh jr b fi kw kx l ky kz">carat <strong class="jr hi">&lt;-</strong> ggplot(data <strong class="jr hi">=</strong> diamonds, aes(x <strong class="jr hi">=</strong> carat, y <strong class="jr hi">=</strong> <strong class="jr hi">log</strong>(price), <br/>  colour <strong class="jr hi">=</strong> color)) <br/>carat <strong class="jr hi">+</strong> <br/>  stat_ecdf() <strong class="jr hi">+</strong> <br/>  facet_grid(clarity <strong class="jr hi">~</strong> cut)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/b268b179301dfb49234f58ed8ce8040c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*x5KN9QX-afmEiFhK.png"/></div></figure><p id="33a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">是的，这肯定看起来像一个多峰分布，分布中有多个峰值，对应于钻石的克拉数从0.99增加到1克拉、1.99增加到2克拉等。以及大约1/2克拉的较小跳跃。</p><h2 id="355c" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">数字变量的方差</h2><p id="b8be" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">我要检查的另一件事是数值变量的方差。如果任何变量的方差与其他变量相差&gt; = 1个数量级，我们将标准化这些值。如果一个变量的方差比其他变量的方差大得多，则可能会过度强调这些变量在训练模型中的重要性。</p><p id="fa67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jo jp jq jr b">carat</code>变量比<code class="du jo jp jq jr b">table</code>变量小&gt; 1个数量级(比<code class="du jo jp jq jr b">depth</code>小1 OOM)，所以我们将继续标准化表和深度。但是，这应该在数据集被拆分为训练数据集和测试数据集之后发生。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="afcd" class="kv ju hh jr b fi kw kx l ky kz">diamonds <strong class="jr hi">%&gt;%</strong> summarise_if(is.numeric, <strong class="jr hi">list</strong>(mean <strong class="jr hi">=</strong> mean, var <strong class="jr hi">=</strong> var)) <strong class="jr hi">%&gt;%</strong> t()</span><span id="d218" class="kv ju hh jr b fi la kx l ky kz">##                    [,1]<br/>## carat_mean 7.979397e-01<br/>## depth_mean 6.174940e+01<br/>## table_mean 5.745718e+01<br/>## price_mean 3.932800e+03<br/>## x_mean     5.731157e+00<br/>## y_mean     5.734526e+00<br/>## z_mean     3.538734e+00<br/>## carat_var  2.246867e-01<br/>## depth_var  2.052404e+00<br/>## table_var  4.992948e+00<br/>## price_var  1.591563e+07<br/>## x_var      1.258347e+00<br/>## y_var      1.304472e+00<br/>## z_var      4.980109e-01</span></pre><p id="8080" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><code class="du jo jp jq jr b">carat</code>变量比<code class="du jo jp jq jr b">table</code>变量小&gt; 1个数量级(比<code class="du jo jp jq jr b">depth</code>小1 OOM)，所以我们将继续标准化表格和深度。然而，这应该发生在数据集被分成训练和测试数据集之后。</p><p id="1e3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我想我已经为下一步准备了足够的信息…</p><h1 id="9b94" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">数据清理</h1><p id="27b7" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">我们将删除一些彼此密切相关的变量，留下一个变量来捕捉包含在其他3个变量中的信息。</p><p id="2a67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们要把价格转换成价格的对数。由于这个数据集来自2017年，我试图预测2021年购买的钻石的价值，我们还将调整通货膨胀率(约10.55%)。</p><p id="7903" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练模型之前，另一个重要的考虑是处理分类数据。通常，这些将被转换成“虚拟变量”或一个热编码。当类别没有自然的排序或顺序时，这是可行的。在这里，切割、透明度和颜色都有一个自然的顺序。例如，切割“好”的钻石比切割“一般”的钻石更好。如果您从r ( <code class="du jo jp jq jr b">data(diamonds)</code>)导入这些数据，那么这些变量将已经是具有正确顺序的因子。但是，如果您下载了该数据集的csv文件，则需要将它们从字符串转换为有序因子，因此我将在这里包含转换步骤(尽管它不应该改变我的数据集中的任何内容——尽管“color”变量的顺序似乎是相反的，因此我也将修复它)。注意:此功能中的级别从最差到最好分配。</p><p id="fc18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在进一步研究了台面和深度场之后，这些值是钻石平均直径的比值。表%影响钻石的光性能(即看起来有多闪亮)。深度%影响钻石的光彩和火候(我不是100%确定这意味着什么，但我们会看看它是否会影响价格)。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="992d" class="kv ju hh jr b fi kw kx l ky kz">diamonds <strong class="jr hi">&lt;-</strong> diamonds <strong class="jr hi">%&gt;%</strong><br/>  mutate(price <strong class="jr hi">=</strong> price <strong class="jr hi">*</strong> 1.1055) <strong class="jr hi">%&gt;%</strong><br/>  mutate(log_price <strong class="jr hi">=</strong> <strong class="jr hi">log</strong>(price)) <strong class="jr hi">%&gt;%</strong><br/>  select(<strong class="jr hi">-</strong>price, <strong class="jr hi">-</strong>x, <strong class="jr hi">-</strong>y, <strong class="jr hi">-</strong>z) <strong class="jr hi">%&gt;%</strong><br/>  mutate(cut <strong class="jr hi">=</strong> factor(cut, levels <strong class="jr hi">=</strong> <strong class="jr hi">c</strong>('Fair', 'Good', 'Very Good', 'Premium', 'Ideal'), ordered <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>),<br/>         color <strong class="jr hi">=</strong> factor(color, levels <strong class="jr hi">=</strong> <strong class="jr hi">c</strong>('J', 'I', 'H', 'G', 'F',        <br/>           'E', 'D'), ordered <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>),<br/>         clarity <strong class="jr hi">=</strong> factor(clarity, levels <strong class="jr hi">=</strong> <strong class="jr hi">c</strong>('I1', 'SI2', 'SI1', <br/>           'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'), ordered <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>))<br/><br/><em class="lu"># This should show that these three variables are now ordered factors.</em><br/>str(diamonds)</span><span id="277d" class="kv ju hh jr b fi la kx l ky kz"><em class="lu"># Here's a little trick to get R to output the order all possible factor levels, instead of just the first few:</em><br/><strong class="jr hi">min</strong>(diamonds<strong class="jr hi">$</strong>cut)<br/><strong class="jr hi">min</strong>(diamonds<strong class="jr hi">$</strong>color)<br/><strong class="jr hi">min</strong>(diamonds<strong class="jr hi">$</strong>clarity)</span><span id="16ff" class="kv ju hh jr b fi la kx l ky kz">## tibble [53,940 × 7] (S3: tbl_df/tbl/data.frame)<br/>##  $ carat    : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 <br/>##  $ cut      : Ord.factor w/ 5 levels "Fair"&lt;"Good"&lt;..: 5 4 2 4 2 <br/>##  $ color    : Ord.factor w/ 7 levels "J"&lt;"I"&lt;"H"&lt;"G"&lt;..: 6 6 6 2 <br/>##  $ clarity  : Ord.factor w/ 8 levels "I1"&lt;"SI2"&lt;"SI1"&lt;..: 2 3 5 4 <br/>##  $ depth    : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 <br/>##  $ table    : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...<br/>##  $ log_price: num [1:53940] 5.89 5.89 5.89 5.91 5.91 ...</span><span id="a3fc" class="kv ju hh jr b fi la kx l ky kz">## [1] Fair<br/>## Levels: Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal</span><span id="7b23" class="kv ju hh jr b fi la kx l ky kz">## [1] J<br/>## Levels: J &lt; I &lt; H &lt; G &lt; F &lt; E &lt; D</span><span id="5134" class="kv ju hh jr b fi la kx l ky kz">## [1] I1<br/>## Levels: I1 &lt; SI2 &lt; SI1 &lt; VS2 &lt; VS1 &lt; VVS2 &lt; VVS1 &lt; IF</span></pre><h2 id="4f77" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">看起来不错！</h2><h1 id="a25c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型准备和培训</h1><p id="a3d6" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">注:我们还对拆分数据集后的数据进行标准化(缩放)，以避免<strong class="ig hi">数据泄露</strong>。这意味着训练数据集的值正在影响测试数据集的值，因为它们的值用于标准化步骤。当对新数据运行模型时，这可能会影响模型的性能。</p><p id="d3fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然数据集已分为测试数据集和训练数据集，我们将训练几个不同的模型，测试它们的性能，并使用最好的一个来进行预测。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="36d2" class="kv ju hh jr b fi kw kx l ky kz"><em class="lu"># Prep</em><br/>library(caTools)<br/>library(tictoc)<br/><br/>set.seed(42)<br/><br/>tic.clearlog()<br/><br/>split <strong class="jr hi">&lt;-</strong> sample.split(diamonds<strong class="jr hi">$</strong>log_price, SplitRatio <strong class="jr hi">=</strong> 0.8)<br/>diamonds_train <strong class="jr hi">&lt;-</strong> subset(diamonds, split <strong class="jr hi">==</strong> <strong class="jr hi">TRUE</strong>)<br/>diamonds_test <strong class="jr hi">&lt;-</strong> subset(diamonds, split <strong class="jr hi">==</strong> <strong class="jr hi">FALSE</strong>)<br/><br/>diamonds_train <strong class="jr hi">&lt;-</strong> diamonds_train <strong class="jr hi">%&gt;%</strong> <br/>  mutate_at(<strong class="jr hi">c</strong>('table', 'depth'), <strong class="jr hi">~</strong>(scale(.) <strong class="jr hi">%&gt;%</strong> as.vector))<br/>diamonds_test <strong class="jr hi">&lt;-</strong> diamonds_test <strong class="jr hi">%&gt;%</strong> <br/>  mutate_at(<strong class="jr hi">c</strong>('table', 'depth'), <strong class="jr hi">~</strong>(scale(.) <strong class="jr hi">%&gt;%</strong> as.vector))  <br/><br/>glimpse(diamonds_test)</span><span id="ca0c" class="kv ju hh jr b fi la kx l ky kz"><em class="lu"># And let's check out the standardized variables:<br/></em>mean(diamonds_test<strong class="jr hi">$</strong>table)<br/>sd(diamonds_test<strong class="jr hi">$</strong>table)</span><span id="b8e3" class="kv ju hh jr b fi la kx l ky kz">## Rows: 9,706<br/>## Columns: 7<br/>## $ carat     &lt;dbl&gt; 0.30, 0.23, 0.30, 0.23, 0.23, 0.32, 0.32, 0.24, <br/>## $ cut       &lt;ord&gt; Good, Very Good, Very Good, Very Good, Very <br/>## $ color     &lt;ord&gt; I, H, J, F, E, H, H, F, I, E, H, F, E, H, G, G, <br/>## $ clarity   &lt;ord&gt; SI2, VS1, VS2, VS1, VS1, SI2, SI2, SI1, SI1, <br/>## $ depth     &lt;dbl&gt; 1.0417048, -0.5231586, 0.2932918, -0.5911962, <br/>## $ table     &lt;dbl&gt; -0.6422915, -0.1949679, -0.1949679, -0.1949679, <br/>## $ log_price &lt;dbl&gt; 5.961084, 5.966766, 5.978034, 5.978034, ...</span><span id="0959" class="kv ju hh jr b fi la kx l ky kz">## [1] 8.708704e-16</span><span id="3c93" class="kv ju hh jr b fi la kx l ky kz">## [1] 1</span><span id="a8c1" class="kv ju hh jr b fi la kx l ky kz"><em class="lu"># The other one (depth) look similar too, you can use the code in diamonds.r to check for yourself.</em></span></pre><p id="1d9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然数据集已分为测试数据集和训练数据集，我们将训练几个不同的模型，测试它们的性能，并使用最好的一个来进行预测。</p><h2 id="d64e" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">多元线性回归</h2><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="afd0" class="kv ju hh jr b fi kw kx l ky kz">tic('mlm')<br/>mlm <strong class="jr hi">&lt;-</strong> lm(log_price <strong class="jr hi">~</strong> ., diamonds_train)<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>summary(mlm)</span><span id="0e55" class="kv ju hh jr b fi la kx l ky kz">## <br/>## Call:<br/>## lm(formula = log_price ~ ., data = diamonds_train)<br/>## <br/>## Residuals:<br/>##     Min      1Q  Median      3Q     Max <br/>## -5.8529 -0.2258  0.0605  0.2531  1.5885 <br/>## <br/>## Coefficients:<br/>##              Estimate Std. Error  t value Pr(&gt;|t|)    <br/>## (Intercept)  6.042146   0.004654 1298.384  &lt; 2e-16 ***<br/>## carat        2.167970   0.003860  561.587  &lt; 2e-16 ***<br/>## cut.L        0.065157   0.007584    8.592  &lt; 2e-16 ***<br/>## cut.Q       -0.009315   0.006074   -1.534   0.1251    <br/>## cut.C        0.030696   0.005215    5.886 3.98e-09 ***<br/>## cut^4        0.006782   0.004174    1.625   0.1042    <br/>## color.L      0.510645   0.005811   87.878  &lt; 2e-16 ***<br/>## color.Q     -0.159645   0.005279  -30.240  &lt; 2e-16 ***<br/>## color.C      0.004640   0.004939    0.940   0.3475    <br/>## color^4      0.039363   0.004538    8.674  &lt; 2e-16 ***<br/>## color^5      0.022161   0.004290    5.165 2.41e-07 ***<br/>## color^6      0.004631   0.003903    1.187   0.2354    <br/>## clarity.L    0.768912   0.010191   75.449  &lt; 2e-16 ***<br/>## clarity.Q   -0.366598   0.009537  -38.441  &lt; 2e-16 ***<br/>## clarity.C    0.216408   0.008156   26.534  &lt; 2e-16 ***<br/>## clarity^4   -0.063964   0.006507   -9.830  &lt; 2e-16 ***<br/>## clarity^5    0.052507   0.005299    9.910  &lt; 2e-16 ***<br/>## clarity^6    0.006066   0.004606    1.317   0.1879    <br/>## clarity^7    0.007096   0.004069    1.744   0.0812 .  <br/>## depth       -0.001029   0.001921   -0.535   0.5923    <br/>## table        0.014223   0.002188    6.500 8.14e-11 ***<br/>## ---<br/>## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/>## <br/>## Residual standard error: 0.3442 on 44213 degrees of freedom<br/>## Multiple R-squared:  0.8884, Adjusted R-squared:  0.8884 <br/>## F-statistic: 1.76e+04 on 20 and 44213 DF,  p-value: &lt; 2.2e-16</span></pre><h2 id="97aa" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">多项式回归</h2><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="6f2a" class="kv ju hh jr b fi kw kx l ky kz">tic('poly')<br/>poly <strong class="jr hi">&lt;-</strong> lm(log_price <strong class="jr hi">~</strong> poly(carat,3) <strong class="jr hi">+</strong> color <strong class="jr hi">+</strong> cut <strong class="jr hi">+</strong> clarity <strong class="jr hi">+</strong> poly(table,3) <strong class="jr hi">+</strong> poly(depth,3), diamonds_train)<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>summary(poly)</span><span id="aac4" class="kv ju hh jr b fi la kx l ky kz">## <br/>## Call:<br/>## lm(formula = log_price ~ poly(carat, 3) + color + cut + clarity + <br/>##     poly(table, 3) + poly(depth, 3), data = diamonds_train)<br/>## <br/>## Residuals:<br/>##     Min      1Q  Median      3Q     Max <br/>## -3.2508 -0.0859 -0.0011  0.0872  1.9011 <br/>## <br/>## Coefficients:<br/>##                   Estimate Std. Error  t value Pr(&gt;|t|)    <br/>## (Intercept)       7.855146   0.001432 5486.562  &lt; 2e-16 ***<br/>## poly(carat, 3)1 224.223650   0.153651 1459.304  &lt; 2e-16 ***<br/>## poly(carat, 3)2 -65.007798   0.136049 -477.825  &lt; 2e-16 ***<br/>## poly(carat, 3)3  20.466149   0.135187  151.391  &lt; 2e-16 ***<br/>## color.L           0.441946   0.002259  195.623  &lt; 2e-16 ***<br/>## color.Q          -0.086252   0.002053  -42.007  &lt; 2e-16 ***<br/>## color.C           0.009905   0.001916    5.169 2.36e-07 ***<br/>## color^4           0.011069   0.001761    6.284 3.32e-10 ***<br/>## color^5           0.008375   0.001664    5.033 4.86e-07 ***<br/>## color^6          -0.001586   0.001514   -1.047 0.294889    <br/>## cut.L             0.091883   0.003636   25.273  &lt; 2e-16 ***<br/>## cut.Q            -0.009912   0.002696   -3.676 0.000237 ***<br/>## cut.C             0.012412   0.002088    5.944 2.80e-09 ***<br/>## cut^4            -0.002513   0.001625   -1.546 0.122172    <br/>## clarity.L         0.887564   0.003990  222.462  &lt; 2e-16 ***<br/>## clarity.Q        -0.244990   0.003728  -65.708  &lt; 2e-16 ***<br/>## clarity.C         0.141636   0.003181   44.523  &lt; 2e-16 ***<br/>## clarity^4        -0.062813   0.002532  -24.807  &lt; 2e-16 ***<br/>## clarity^5         0.029161   0.002058   14.167  &lt; 2e-16 ***<br/>## clarity^6        -0.003592   0.001787   -2.010 0.044466 *  <br/>## clarity^7         0.029769   0.001579   18.854  &lt; 2e-16 ***<br/>## poly(table, 3)1  -0.691116   0.179675   -3.846 0.000120 ***<br/>## poly(table, 3)2  -0.603061   0.140336   -4.297 1.73e-05 ***<br/>## poly(table, 3)3   0.591239   0.136248    4.339 1.43e-05 ***<br/>## poly(depth, 3)1  -1.288272   0.162940   -7.906 2.71e-15 ***<br/>## poly(depth, 3)2  -1.105807   0.168139   -6.577 4.86e-11 ***<br/>## poly(depth, 3)3  -0.057543   0.134870   -0.427 0.669631    <br/>## ---<br/>## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br/>## <br/>## Residual standard error: 0.1335 on 44207 degrees of freedom<br/>## Multiple R-squared:  0.9832, Adjusted R-squared:  0.9832 <br/>## F-statistic: 9.96e+04 on 26 and 44207 DF,  p-value: &lt; 2.2e-16</span></pre><p id="af21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">哇！多项式回归似乎更适合！</p><h2 id="ed51" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">支持向量回归机</h2><p id="15e4" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">SVR不依赖于基础因变量和自变量的分布。它也可用于通过<code class="du jo jp jq jr b">kernel = 'radial'</code>选项构建非线性模型。我认为是这样，因为线性模型是目前为止表现最差的。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="ce8a" class="kv ju hh jr b fi kw kx l ky kz">tic('svr')<br/>library(e1071)<br/>svr <strong class="jr hi">&lt;-</strong> svm(formula <strong class="jr hi">=</strong> log_price <strong class="jr hi">~</strong> .,<br/>                data <strong class="jr hi">=</strong> diamonds_train,<br/>                type <strong class="jr hi">=</strong> 'eps-regression',<br/>                kernel <strong class="jr hi">=</strong> 'radial')<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)</span></pre><h2 id="7ac0" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">决策树回归</h2><p id="bec3" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">决策树使用一组if-then-else决策规则。树越深，决策规则越复杂，模型越合适。训练DT模型有时会导致树过于复杂，不能很好地概括数据。这叫做过度拟合。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="17eb" class="kv ju hh jr b fi kw kx l ky kz">tic('tree')<br/>library(rpart)<br/>tree <strong class="jr hi">&lt;-</strong> rpart(formula <strong class="jr hi">=</strong> log_price <strong class="jr hi">~</strong> .,<br/>                  data <strong class="jr hi">=</strong> diamonds_train,<br/>                  method <strong class="jr hi">=</strong> 'anova',<br/>                  model <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>tree</span><span id="ab0a" class="kv ju hh jr b fi la kx l ky kz">## n= 44234 <br/>## <br/>## node), split, n, deviance, yval<br/>##       * denotes terminal node<br/>## <br/>##  1) root 44234 46934.3000 7.922656  <br/>##    2) carat&lt; 0.695 20143  4323.3280 6.963920  <br/>##      4) carat&lt; 0.455 13848  1212.0970 6.718800 *<br/>##      5) carat&gt;=0.455 6295   448.8476 7.503143 *<br/>##    3) carat&gt;=0.695 24091  8615.3050 8.724275  <br/>##      6) carat&lt; 0.995 7837   612.7115 8.100690 *<br/>##      7) carat&gt;=0.995 16254  3485.7290 9.024943  <br/>##       14) carat&lt; 1.385 10379  1099.8590 8.772585  <br/>##         28) clarity=I1,SI2,SI1 5582   243.9614 8.574533 *<br/>##         29) clarity=VS2,VS1,VVS2,VVS1,IF 4797   382.1680 9.003046 *<br/>##       15) carat&gt;=1.385 5875   557.1727 9.470768 *</span></pre><h2 id="2a96" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">随机森林回归</h2><p id="0b50" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">这使决策树模型更进了一步，并且使用许多决策树来做出比在该模型中使用任何单个决策树更好的预测。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="460d" class="kv ju hh jr b fi kw kx l ky kz">tic('rf')<br/>library(randomForest)<br/>rf <strong class="jr hi">&lt;-</strong> randomForest(log_price <strong class="jr hi">~</strong> .,<br/>                   data <strong class="jr hi">=</strong> diamonds_train,<br/>                   ntree <strong class="jr hi">=</strong> 500,<br/>                   importance <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>rf</span><span id="4805" class="kv ju hh jr b fi la kx l ky kz">## <br/>## Call:<br/>##  randomForest(formula = log_price ~ ., data = diamonds_train,      ntree = 500, importance = TRUE) <br/>##                Type of random forest: regression<br/>##                      Number of trees: 500<br/>## No. of variables tried at each split: 2<br/>## <br/>##           Mean of squared residuals: 0.01159442<br/>##                     % Var explained: 98.91</span></pre><h2 id="4dc3" class="kv ju hh bd jv lc ld le jz lf lg lh kd ip li lj kh it lk ll kl ix lm ln kp lo bi translated">XGBoost回归</h2><p id="faf7" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">XGBoost使用梯度增强决策树，是一个非常健壮的模型，在各种应用程序中表现非常好。它对影响其他一些模型性能的问题也不敏感，如多重共线性或数据规范化/标准化。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="dd98" class="kv ju hh jr b fi kw kx l ky kz">tic('xgb')<br/>library(xgboost)<br/>diamonds_train_xgb <strong class="jr hi">&lt;-</strong> diamonds_train <strong class="jr hi">%&gt;%</strong><br/>  mutate_if(is.factor, as.numeric)<br/>diamonds_test_xgb <strong class="jr hi">&lt;-</strong> diamonds_test <strong class="jr hi">%&gt;%</strong><br/>  mutate_if(is.factor, as.numeric)<br/><br/>xgb <strong class="jr hi">&lt;-</strong> xgboost(data <strong class="jr hi">=</strong> as.matrix(diamonds_train_xgb[-7]), label <strong class="jr hi">=</strong> diamonds_train_xgb<strong class="jr hi">$</strong>log_price, nrounds <strong class="jr hi">=</strong> 6166, verbose <strong class="jr hi">=</strong> 0)<br/><em class="lu"># the rmse stopped decreasing after 6166 rounds </em><br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)</span></pre><h1 id="c58b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型性能</h1><p id="1c59" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">现在，我们将使用我们训练的每个模型来预测测试数据集中钻石价格的对数，以确定哪个模型在它尚未看到的数据上表现最佳。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="2eef" class="kv ju hh jr b fi kw kx l ky kz"><em class="lu"># Make predictions and compare model performance</em><br/>tic('predict_all')<br/>mlm_pred <strong class="jr hi">&lt;-</strong> predict(mlm, diamonds_test)<br/>poly_pred <strong class="jr hi">&lt;-</strong> predict(poly, diamonds_test)<br/>svr_pred <strong class="jr hi">&lt;-</strong> predict(svr, diamonds_test)<br/>tree_pred <strong class="jr hi">&lt;-</strong> predict(tree, diamonds_test)<br/>rf_pred <strong class="jr hi">&lt;-</strong> predict(rf, diamonds_test)<br/>xgb_pred <strong class="jr hi">&lt;-</strong> predict(xgb, as.matrix(diamonds_test_xgb[-7]))<br/>toc(log <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>, quiet <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/><br/><em class="lu"># Calculate residuals (i.e. how different the predictions are from the log_price of the test data set)</em><br/>xgb_resid <strong class="jr hi">&lt;-</strong> diamonds_test_xgb<strong class="jr hi">$</strong>log_price <strong class="jr hi">-</strong> xgb_pred<br/>library(modelr)<br/>resid <strong class="jr hi">&lt;-</strong> diamonds_test <strong class="jr hi">%&gt;%</strong>  <br/>  spread_residuals(mlm, poly, svr, tree, rf) <strong class="jr hi">%&gt;%</strong><br/>  select(mlm, poly, svr, tree, rf) <strong class="jr hi">%&gt;%</strong><br/>  rename_with( <strong class="jr hi">~</strong> paste0(.x, '_resid')) <strong class="jr hi">%&gt;%</strong><br/>  cbind(xgb_resid)<br/><br/>predictions <strong class="jr hi">&lt;-</strong> diamonds_test <strong class="jr hi">%&gt;%</strong><br/>  select(log_price) <strong class="jr hi">%&gt;%</strong><br/>  cbind(mlm_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(poly_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(svr_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(tree_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(rf_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(xgb_pred) <strong class="jr hi">%&gt;%</strong><br/>  cbind(resid)           <em class="lu"># This will be useful for plotting later</em><br/><br/><em class="lu"># Calculate R-squared - this describes how much of the variability is explained by the model - the closer to 1, the better</em><br/><br/>mean_log_price <strong class="jr hi">&lt;-</strong> mean(diamonds_test<strong class="jr hi">$</strong>log_price)<br/>tss <strong class="jr hi">=</strong>  <strong class="jr hi">sum</strong>((diamonds_test_xgb<strong class="jr hi">$</strong>log_price <strong class="jr hi">-</strong> mean_log_price)<strong class="jr hi">^</strong>2 )<br/><br/>square <strong class="jr hi">&lt;-</strong> <strong class="jr hi">function</strong>(x) {x<strong class="jr hi">**</strong>2}<br/>r2 <strong class="jr hi">&lt;-</strong> <strong class="jr hi">function</strong>(x) {1 <strong class="jr hi">-</strong> x<strong class="jr hi">/</strong>tss}<br/><br/>r2_df <strong class="jr hi">&lt;-</strong> resid <strong class="jr hi">%&gt;%</strong><br/>  mutate_all(square) <strong class="jr hi">%&gt;%</strong><br/>  summarize_all(sum) <strong class="jr hi">%&gt;%</strong><br/>  mutate_all(r2) <strong class="jr hi">%&gt;%</strong><br/>  gather(key <strong class="jr hi">=</strong> 'model', value <strong class="jr hi">=</strong> 'r2') <strong class="jr hi">%&gt;%</strong><br/>  mutate(model <strong class="jr hi">=</strong> str_replace(model, '_resid', ''))<br/>r2_df</span><span id="6e59" class="kv ju hh jr b fi la kx l ky kz">##   model        r2<br/>## 1   mlm 0.8842696<br/>## 2  poly 0.9803430<br/>## 3   svr 0.9847216<br/>## 4  tree 0.9114766<br/>## 5    rf 0.9870050<br/>## 6   xgb 0.9842275</span></pre><h1 id="7b80" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">可视化模型的性能</h1><p id="c451" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">根据R值，随机森林模型表现最佳，这是一种衡量模型对数据集中的可变性的解释程度的方法，因此我们将主要关注这种可视化方法。它等于1-RMSE(均方根误差，描述所有预测与<code class="du jo jp jq jr b">y_test</code>数据集中真实值的差异)。</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="a9dd" class="kv ju hh jr b fi kw kx l ky kz">library(ggplot2)<br/>r2_plot <strong class="jr hi">&lt;-</strong> ggplot(r2_df, aes(x <strong class="jr hi">=</strong> model, y <strong class="jr hi">=</strong> r2, colour <strong class="jr hi">=</strong> model, fill <strong class="jr hi">=</strong> model)) <strong class="jr hi">+</strong> geom_bar(stat <strong class="jr hi">=</strong> 'identity')<br/>r2_plot <strong class="jr hi">+</strong> ggtitle('R-squared Values for each Model') <strong class="jr hi">+</strong> coord_cartesian(ylim <strong class="jr hi">=</strong> <strong class="jr hi">c</strong>(0.75, 1))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/0d52386a016a8f375eff4d2b6e2603c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*-IkcFdlEsKsVlux8.png"/></div></figure><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="bf20" class="kv ju hh jr b fi kw kx l ky kz">sample <strong class="jr hi">&lt;-</strong> predictions <strong class="jr hi">%&gt;%</strong><br/>  slice_sample(n <strong class="jr hi">=</strong> 1000) <br/>ggplot(sample, aes(x <strong class="jr hi">=</strong> <strong class="jr hi">exp</strong>(log_price), y <strong class="jr hi">=</strong> <strong class="jr hi">exp</strong>(rf_pred), <br/>    size <strong class="jr hi">=</strong> <strong class="jr hi">abs</strong>(rf_resid))) <strong class="jr hi">+</strong><br/>  geom_point(alpha <strong class="jr hi">=</strong> 0.1) <strong class="jr hi">+</strong> <br/>  labs(title <strong class="jr hi">=</strong> 'Predicted vs Actual Cost of Diamonds in USD', <br/>    x <strong class="jr hi">=</strong> 'Price', y <strong class="jr hi">=</strong> 'Predicted Price', size <strong class="jr hi">=</strong> 'Residuals')</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/a09cf2931992bd9693d15e7596531397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*mo7BBiVufImklmA6.png"/></div></figure><p id="51a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们尝试的所有模型中，<strong class="ig hi">随机森林</strong>模型表现最好。这并不奇怪，因为这是一种<strong class="ig hi">集成方法</strong>，这意味着它利用多个模型之间的一致性做出比任何模型单独做出的预测都更好的预测。XGBoost是集合方法的另一个例子，也表现得非常好。在第二个图中，大小与残差的绝对值成比例，这意味着预测值与真实值的差异有多大。</p><h1 id="215f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">特征重要性</h1><p id="87cf" class="pw-post-body-paragraph ie if hh ig b ih lp ij ik il lq in io ip lr ir is it ls iv iw ix lt iz ja jb ha bi translated">哪个(些)变量对预测钻石价格最重要？</p><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="1e4f" class="kv ju hh jr b fi kw kx l ky kz">varImpPlot(rf)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/3dbf399ff907a24c85462ec61e36935d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*oobFzK9rAj0looUR.png"/></div></figure><p id="1150" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x轴上的值表示如果模型中不包含该变量，预测误差会增加多少。正如所料，克拉(或大小)是最重要的变量。尽管表型和深度应该会影响钻石看起来有多闪亮，但它们实际上对价格没有那么大的影响。</p><h1 id="3ffe" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">训练模型和做预测花了多长时间？</h1><pre class="jd je jf jg fd kr jr ks kt aw ku bi"><span id="e534" class="kv ju hh jr b fi kw kx l ky kz"><em class="lu"># Training &amp; predicting times</em><br/>time_log <strong class="jr hi">&lt;-</strong> tic.log(format <strong class="jr hi">=</strong> <strong class="jr hi">TRUE</strong>)<br/>time_log</span><span id="2285" class="kv ju hh jr b fi la kx l ky kz">## [[1]]<br/>## [1] "mlm: 0.057 sec elapsed"<br/>## <br/>## [[2]]<br/>## [1] "poly: 0.154 sec elapsed"<br/>## <br/>## [[3]]<br/>## [1] "svr: 194.602 sec elapsed"<br/>## <br/>## [[4]]<br/>## [1] "tree: 0.248 sec elapsed"<br/>## <br/>## [[5]]<br/>## [1] "rf: 407.62 sec elapsed"<br/>## <br/>## [[6]]<br/>## [1] "xgb: 62.249 sec elapsed"<br/>## <br/>## [[7]]<br/>## [1] "predict_all: 8.219 sec elapsed"</span></pre><p id="88d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以表现最好的模特训练时间最长。这并不奇怪，因为他们实际上包括许多模型。请记住，这不是一个大型数据集。我为工作培训的大多数模型都需要花费大量的时间(我通常设置它们通宵运行)。这可以通过AWS中一个真正强大的机器甚至一个机器集群来加速。</p></div><div class="ab cl lv lw go lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ha hb hc hd he"><p id="5d09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lu">原载于</em><a class="ae js" href="https://zeather709.github.io/best-day-of-my-life/" rel="noopener ugc nofollow" target="_blank"><em class="lu">https://Zea ther 709 . github . io</em></a><em class="lu">。</em></p><p id="302a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">订阅:</strong><a class="ae js" href="https://zeather.medium.com/subscribe/" rel="noopener">https://zeather.medium.com/subscribe/</a></p><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt jm mf"/></div></div></a></div></div></div>    
</body>
</html>