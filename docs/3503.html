<html>
<head>
<title>Semi Supervised Learning — Making The Most of Noisy Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">半监督学习—充分利用噪声数据</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/semi-supervised-learning-making-the-most-of-noisy-data-5fbc711d2384?source=collection_archive---------2-----------------------#2022-09-12">https://medium.com/mlearning-ai/semi-supervised-learning-making-the-most-of-noisy-data-5fbc711d2384?source=collection_archive---------2-----------------------#2022-09-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d524" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我强调了<strong class="ig hi">半监督学习</strong>的有用性及其背后的不同技术，并进一步讨论了<strong class="ig hi">主动学习</strong>。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/d6c5ab5520741a40cf0eed9a43ce870b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zpgv52rNVxOfQucy"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@biancablah?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bianca Ackermann</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="33d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">半监督学习</strong>是一种机器学习，其中学习算法给定了少量的训练示例以及大量的未标记示例。半监督学习的目标是从训练数据中学习模型，并利用未标记数据提高模型的精度。</p><p id="403c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有许多方法可以执行半监督学习。一种常见的方法是使用无监督学习算法从未标记的数据中学习模型，然后使用所学习的模型来提高有监督学习算法的精度。另一种方法是使用<strong class="ig hi">强化学习算法</strong>从未标记的数据中学习策略，然后使用所学习的策略来提高监督学习算法的精度。</p><p id="48ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">半监督学习是一种很有前途的机器学习方法，因为它可以用很少的附加数据来提高模型的精度。</p><p id="2307" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们进一步探讨这个问题…</p><h1 id="0f54" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">为什么是半监督学习？</h1><p id="cd9d" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">现实世界的问题由许多缺少数据的情况组成，包括训练模型所需的标签。我们如何弥补这一点，并能够正确解决问题？</p><p id="db8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让个人或专家手动标记数据既<em class="kw">昂贵</em>又<em class="kw">耗时</em>。那么，解决方案是什么？</p><p id="c1a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">需要注意的是，根据范·恩格伦和霍斯的<a class="ae js" href="https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf" rel="noopener ugc nofollow" target="_blank">大调查</a><em class="kw">可知，大部分关于半监督学习的研究都集中在分类</em>上。因此，该领域应用的大多数方法都是针对分类问题的。这显然不是问题，因为现实世界中的场景充斥着分类问题。</p><blockquote class="kx ky kz"><p id="2f28" class="ie if kw ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">半监督分类方法试图利用未标记的数据点来构建一个学习者，该学习者的表现超过了仅使用标记数据时获得的学习者的表现(范·恩格伦和霍斯)。</p></blockquote><p id="1e45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们强调了定义当今大多数半监督算法的3个假设:</p><ul class=""><li id="fb93" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi">平滑度假设</strong>:一个输入上的两个相近样本x1和x2应该有相同的输出(y)。</li><li id="b053" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">低密度假设</strong>:类间的决策边界以输入空间中的低密度区域为特征。</li><li id="4ce6" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">流形假设</strong>:同一低维流形(低维子结构)上的数据点应该有相同的标签。</li></ul><p id="4192" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于这些假设，算法应该能够根据已经标记的数据点对未标记的数据点进行分类。然而，<strong class="ig hi">当且仅当不同的问题类别在标记的数据点</strong>中被很好地表示时，这种完美的场景才会出现:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lr"><img src="../Images/7f43b2f0d3d744807190b06dfa549fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3PJ3VuLUIpQZxTqRoBi3Q.jpeg"/></div></div></figure><p id="5761" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如你所看到的，由绿色三角形形成的类没有被充分代表，导致半监督学习模型不能正确区分类</p><p id="89a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，为了获得最准确和有效的模型，在标记和未标记数据之间划分数据集是很重要的。这并不总是可能的，因为数据集在一个类上缺少太多的点，我们将在本文后面看到克服这一点的方法。</p><p id="8850" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们来概述一下最著名的半监督算法/技术…</p><h1 id="cbed" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">半监督学习算法</h1><p id="b883" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">首先，这些算法有一个主要区别:</p><blockquote class="ls"><p id="3cbf" class="lt lu hh bd lv lw lx ly lz ma mb jb dx translated">归纳方法vs <strong class="ak">直推</strong>方法</p></blockquote><p id="18c7" class="pw-post-body-paragraph ie if hh ig b ih mc ij ik il md in io ip me ir is it mf iv iw ix mg iz ja jb ha bi translated">虽然<strong class="ig hi">归纳方法</strong>正在寻求建立一个分类模型，目的是从未标记的数据点获得预测，但<strong class="ig hi">直推方法</strong>正在直接进行预测，而不试图拥有一个分类器。</p><p id="66f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">归纳</strong>可与监督学习相关联，这与转导不同，因为它使用所有数据集(训练和测试)来预测标签。</p><p id="aed8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你对这两种方法的区别感兴趣，可以看看<a class="ae js" href="https://towardsdatascience.com/inductive-vs-transductive-learning-e608e786f7d" rel="noopener" target="_blank">Vijini mallawatarachchi</a><a class="ae js" href="https://vijini.medium.com/" rel="noopener">的这篇文章</a>。</p><p id="4dd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们深入研究半监督学习算法的4种主要方法…</p><h2 id="01c2" class="mh ju hh bd jv mi mj mk jz ml mm mn kd ip mo mp kh it mq mr kl ix ms mt kp mu bi translated">1 —包装方法<em class="mv">(归纳方法)</em></h2><p id="46b2" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated"><strong class="ig hi">包装器方法</strong>背后的想法是有一个<strong class="ig hi"> <em class="kw">训练步骤</em> </strong>，其中分类器从已标记的数据点中学习，还有一个<strong class="ig hi"> <em class="kw">伪标记步骤</em> </strong>，其中先前的分类器用于从未标记的数据中获得预测。然后，验证新标签(预测)的准确性，并将最准确的标签(基于置信度)添加到训练数据集中。重复这些步骤，直到模型达到最佳性能。</p><p id="396a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以区分两种包装方法:</p><ul class=""><li id="1b9e" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi">自我训练</strong>:包括在标记数据上训练任何类型的唯一分类器，并使用它来伪标记未标记的数据点。这是最简单的包装方法。</li><li id="7833" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">协同训练</strong>:对标记数据训练两个或多个分类器，并基于它们的预测，为下一次迭代将伪标记添加到训练数据点。主要的补充是不同类型的各种分类器应该具有更精确的信息，并且倾向于通过组合它们最有把握的预测来更好地概括。</li></ul><p id="6cf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一种包装器方法基于<strong class="ig hi">集成学习</strong> ( <strong class="ig hi">助推</strong>和<strong class="ig hi">装袋</strong>)，并通过让多分类器在来自标记数据点的随机样本上学习来扩展co训练。</p><h2 id="18cc" class="mh ju hh bd jv mi mj mk jz ml mm mn kd ip mo mp kh it mq mr kl ix ms mt kp mu bi translated"><strong class="ak"> 2 —无监督预处理(归纳方法)</strong></h2><p id="57c4" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这种类型的方法基本上是使用<strong class="ig hi">无监督技术</strong>和<strong class="ig hi">算法</strong>从所有数据中提取信息，以改进分类器的未来训练。它是无监督技术和有监督技术的混合。</p><p id="6cdb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为例子，我们发现<strong class="ig hi">特征提取</strong>或者甚至<strong class="ig hi">聚类</strong>在无监督学习阶段。</p><h2 id="638c" class="mh ju hh bd jv mi mj mk jz ml mm mn kd ip mo mp kh it mq mr kl ix ms mt kp mu bi translated"><strong class="ak"> 3 —固有半监督方法(归纳方法)</strong></h2><p id="bcd6" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这些方法的思想是基于一种假设，例如我们之前介绍的低密度假设，来推断一个可以分类数据点(标记的和未标记的)的函数。</p><p id="5dc3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里列出了三种主要的内在半监督方法:</p><ul class=""><li id="2c34" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi">最大间隔法(低密度分离)</strong>:基于前述<em class="kw">低密度假设</em>的算法，其中所有数据点，尤其是未标记的数据点允许确定低密度区域。因此，分类器能够根据其在输入空间中的位置来预测新的数据点。这些方法的一个例子是半监督SVM。</li><li id="fdd5" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">流形</strong>:数据点因其给出的信息而以其低维流形为特征的技术。因此，同一流形上的2个点应该有相同的标号，这就是<em class="kw">流形假设</em>。两种技术组成了流形方法:<strong class="ig hi">流形正则化</strong>和<strong class="ig hi">流形逼近</strong>。</li><li id="34ef" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">生成模型</strong>:试图理解数据是如何生成的方法。最广为人知的技术是<strong class="ig hi">混合模型</strong>和<strong class="ig hi">生成对抗网络</strong> (GAN)。</li></ul><h2 id="4eee" class="mh ju hh bd jv mi mj mk jz ml mm mn kd ip mo mp kh it mq mr kl ix ms mt kp mu bi translated"><strong class="ak"> 4 —基于图形的方法(直推式方法)</strong></h2><p id="9720" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这是唯一的直推类方法，因此，它不使用任何分类器来解决问题。</p><blockquote class="kx ky kz"><p id="7071" class="ie if kw ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">转导方法通常在所有数据点(包括已标记和未标记数据点)上定义一个图，用可能的加权边对数据点的成对相似性进行编码。(朱小金，<a class="ae js" href="https://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf" rel="noopener ugc nofollow" target="_blank">半监督图学习</a>)</p></blockquote><p id="202b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了评估算法，通过观察标记的数据是否被正确分类以及相似的数据点是否在正确的位置，来优化<strong class="ig hi">目标函数</strong>。</p><p id="f492" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一般分为三个步骤:<strong class="ig hi">图形创建</strong>、<strong class="ig hi">图形权重</strong> g和<strong class="ig hi">推理</strong>。如果你对这些方法的过程感兴趣，你可以在<em class="kw">范·恩格伦</em>和<em class="kw">霍克斯</em> <a class="ae js" href="https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的7.1部分找到很好的解释。</p><p id="eab3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经描述了一些最先进的半监督学习算法，让我们以讨论什么是<strong class="ig hi">主动学习</strong>来结束…</p></div><div class="ab cl mw mx go my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ha hb hc hd he"><h1 id="431a" class="jt ju hh bd jv jw nd jy jz ka ne kc kd ke nf kg kh ki ng kk kl km nh ko kp kq bi translated">主动学习</h1><p id="4e5f" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">主动学习是半监督学习的一种扩展，包括确定和选择<strong class="ig hi">高潜力的未标记数据</strong>以使模型更有效。一旦被识别，这些数据点被标记，分类器获得准确性。</p><p id="e3a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种技术与我们之前看到的方法相结合，最大限度地减少了昂贵和耗时的标签工作。</p><h2 id="008f" class="mh ju hh bd jv mi mj mk jz ml mm mn kd ip mo mp kh it mq mr kl ix ms mt kp mu bi translated">如何检测信息丰富的未标记数据点？</h2><p id="fcf1" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">可以注意到检测这些数据点的三种不同方法:</p><ul class=""><li id="65f0" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi">不确定性</strong>:标注模型对其预测最不自信的样本。</li><li id="47da" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">多样性</strong> / <strong class="ig hi">多样性</strong>:选择尽可能多样的样本，以最好地覆盖整个输入空间。</li><li id="0703" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">模型改进</strong>:选择能够提高模型性能(降低损失函数)的样本。</li></ul><p id="6be3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您对主动学习感兴趣，您可以从<em class="kw">毛刺沉淀</em>看一下<a class="ae js" href="https://burrsettles.com/pub/settles.activelearning.pdf" rel="noopener ugc nofollow" target="_blank">这篇伟大的论文</a>。</p><h1 id="3cd1" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="3a77" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated"><strong class="ig hi">半监督学习</strong>在一个数据缺失、数据嘈杂的世界里是必不可少的。能够最大限度地利用任何数据至关重要，即使它不是我们寻求解决的问题的标签。</p><p id="01d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">许多方法的存在使得这成为可能，例如<strong class="ig hi">包装方法</strong>、<strong class="ig hi">无监督预处理</strong>、<strong class="ig hi">本质上半</strong> - <strong class="ig hi">有监督方法</strong>或<strong class="ig hi">基于图的方法。</strong></p><p id="751b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于这个主题的一个有趣的开端是关于<strong class="ig hi">弱监督</strong>以及这如何挑战半监督学习算法。</p><p id="1570" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢它，并学到了很多东西！如果你对数据科学和机器学习感兴趣，可以在这里查看我的文章<a class="ae js" href="https://www.npogeant.com/#articles" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="26ca" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">资源:</h1><div class="ni nj ez fb nk nl"><a href="https://minds.wisconsin.edu/handle/1793/60444" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">半监督学习文献综述</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">本文回顾了一些关于半监督学习的文献。传统分类器需要标记数据…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">minds.wisconsin.edu</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz jm nl"/></div></div></a></div><div class="ni nj ez fb nk nl"><a href="https://link.springer.com/article/10.1007/s10994-019-05855-6" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">半监督学习——机器学习综述</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">半监督学习是机器学习的一个分支，它涉及使用有标签和无标签的数据进行分类</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">link.springer.com</p></div></div><div class="nu l"><div class="oa l nw nx ny nu nz jm nl"/></div></div></a></div><div class="ni nj ez fb nk nl"><a href="https://github.com/yassouali/awesome-semi-supervised-learning" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">GitHub-yassoua Li/awesome-半监督学习:😎一个最新的和策划的列表真棒…</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">令人敬畏的半监督学习资源的精选列表。灵感来自令人敬畏的深度视觉…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">github.com</p></div></div><div class="nu l"><div class="ob l nw nx ny nu nz jm nl"/></div></div></a></div><div class="ni nj ez fb nk nl"><a href="https://towardsdatascience.com/inductive-vs-transductive-learning-e608e786f7d" rel="noopener follow" target="_blank"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">归纳学习与传导学习</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">浅谈归纳式和直推式学习方法</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">towardsdatascience.com</p></div></div><div class="nu l"><div class="oc l nw nx ny nu nz jm nl"/></div></div></a></div><div class="ni nj ez fb nk nl"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hi fi z dy nq ea eb nr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">medium.com</p></div></div><div class="nu l"><div class="od l nw nx ny nu nz jm nl"/></div></div></a></div></div></div>    
</body>
</html>