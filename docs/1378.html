<html>
<head>
<title>Social Distancing using YOLOv3 — Object Detection — with source code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用YOLOv3的社交距离——对象检测——带源代码</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/social-distancing-using-yolov3-object-detection-with-source-code-fca11d9156ff?source=collection_archive---------4-----------------------#2021-12-04">https://medium.com/mlearning-ai/social-distancing-using-yolov3-object-detection-with-source-code-fca11d9156ff?source=collection_archive---------4-----------------------#2021-12-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="af53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以伙计们，伙计们，最期待的项目之一来了，<strong class="ig hi">使用YOLOv3和OpenCV </strong>的社交距离。</p><p id="bfc4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个项目中，我们要做的是，我们将检查在某个特定的地方，社会距离是否被遵循。</p><p id="dd26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在这里阅读全文并附带源代码—</strong><a class="ae jc" href="https://machinelearningprojects.net/social-distancing-using-yolov3/" rel="noopener ugc nofollow" target="_blank">https://machine learning projects . net/social-distanding-using-yolov 3/</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/1da75418b40611ee3799c0bbba96f4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*zq1YVQZWrst6TCY0.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">social distancing</figcaption></figure><h1 id="a98a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">让我们开始吧…</h1><h2 id="dffe" class="kn jq hh bd jr ko kp kq jv kr ks kt jz ip ku kv kd it kw kx kh ix ky kz kl la bi translated">社会距离项目代码…</h2><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="af95" class="kn jq hh lc b fi lg lh l li lj">import cv2<br/>import numpy as np<br/>import random<br/>import os<br/>from PIL import Image<br/>import time<br/><br/>net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")<br/>net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)<br/>net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)<br/><br/>distance_thres = 50<br/><br/>cap = cv2.VideoCapture('data/humans.mp4')<br/><br/>def dist(pt1,pt2):<br/>    try:<br/>        return ((pt1[0]-pt2[0])**2 + (pt1[1]-pt2[1])**2)**0.5<br/>    except:<br/>        return<br/><br/>layer_names = net.getLayerNames()<br/>output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]<br/>print('Output layers',output_layers)<br/><br/>_,frame = cap.read()<br/><br/>fourcc = cv2.VideoWriter_fourcc(*"MJPG")<br/>writer = cv2.VideoWriter('output.avi', fourcc, 30,(frame.shape[1], frame.shape[0]), True)<br/><br/><br/>ret = True<br/>while ret:<br/><br/>    ret, img = cap.read()<br/>    if ret:<br/>        height, width = img.shape[:2]<br/><br/>        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)<br/><br/>        net.setInput(blob)<br/>        outs = net.forward(output_layers)<br/><br/>        confidences = []<br/>        boxes = []<br/>        <br/>        for out in outs:<br/>            for detection in out:<br/>                scores = detection[5:]<br/>                class_id = np.argmax(scores)<br/>                if class_id!=0:<br/>                    continue<br/>                confidence = scores[class_id]<br/>                if confidence &gt; 0.3:<br/>                    center_x = int(detection[0] * width)<br/>                    center_y = int(detection[1] * height)<br/><br/>                    w = int(detection[2] * width)<br/>                    h = int(detection[3] * height)<br/>                    x = int(center_x - w / 2)<br/>                    y = int(center_y - h / 2)<br/><br/>                    boxes.append([x, y, w, h])<br/>                    confidences.append(float(confidence))<br/><br/>        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)<br/><br/>        persons = []<br/>        person_centres = []<br/>        violate = set()<br/><br/>        for i in range(len(boxes)):<br/>            if i in indexes:<br/>                x,y,w,h = boxes[i]<br/>                persons.append(boxes[i])<br/>                person_centres.append([x+w//2,y+h//2])<br/><br/>        for i in range(len(persons)):<br/>            for j in range(i+1,len(persons)):<br/>                if dist(person_centres[i],person_centres[j]) &lt;= distance_thres:<br/>                    violate.add(tuple(persons[i]))<br/>                    violate.add(tuple(persons[j]))<br/>        <br/>        v = 0<br/>        for (x,y,w,h) in persons:<br/>            if (x,y,w,h) in violate:<br/>                color = (0,0,255)<br/>                v+=1<br/>            else:<br/>                color = (0,255,0)<br/>            cv2.rectangle(img,(x,y),(x+w,y+h),color,2)<br/>            cv2.circle(img,(x+w//2,y+h//2),2,(0,0,255),2)<br/><br/>        cv2.putText(img,'No of Violations : '+str(v),(15,frame.shape[0]-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,126,255),2)<br/>        writer.write(img)<br/>        cv2.imshow("Image", img)<br/>    <br/>    if cv2.waitKey(1) == 27:<br/>        break<br/><br/>cap.release()<br/>cv2.destroyAllWindows()</span></pre><ul class=""><li id="c872" class="lk ll hh ig b ih ii il im ip lm it ln ix lo jb lp lq lr ls bi translated">第1–6行—导入所需的库。</li><li id="65d8" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第8–10行—读取Yolo文件并启用Cuda。</li><li id="5584" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第12行—将距离阈值设置为50像素。</li><li id="02c4" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第14行——实例化<a class="ae jc" href="https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html" rel="noopener ugc nofollow" target="_blank"> VideoCapture </a>对象，它将帮助我们从视频文件中读取帧。</li><li id="549b" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第16–20行—一个简单的距离函数，用于计算平面上两个坐标之间的距离。</li><li id="09ea" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第22行-获取网络中所有图层名称的列表。</li><li id="84b7" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第23行-获取输出图层。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/82fd961678c0c49a086302ad13d10a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*9ehAhXFiuYZ3EBhP.png"/></div></figure><ul class=""><li id="bdc5" class="lk ll hh ig b ih ii il im ip lm it ln ix lo jb lp lq lr ls bi translated">第26行——从视频中读取一帧，以获得它的高度和宽度。</li><li id="2680" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第28–29行—我们将使用<a class="ae jc" href="https://docs.opencv.org/3.4/dd/d9e/classcv_1_1VideoWriter.html" rel="noopener ugc nofollow" target="_blank"> VideoWriter </a>将我们的结果保存在视频输出中，如下所示。</li><li id="0eb0" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第32–33行—让我们开始循环。</li><li id="cbb4" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第35行—从输入视频开始读取。</li><li id="f896" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第36行——如果cam对象返回一些东西，那么ret将为真。</li><li id="a873" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第37行—提取图像的高度和宽度。</li><li id="6b84" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第38行—从图像中创建一个形状为416X416的斑点。</li><li id="504a" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第41行——使用<a class="ae jc" href="https://docs.opencv.org/4.5.2/d6/d0f/group__dnn.html#ga29f34df9376379a603acd8df581ac8d7" rel="noopener ugc nofollow" target="_blank"> cv2.dnn.blobFromImage </a>将该斑点作为网络的输入。</li><li id="507d" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第42行-从输出图层获取输出。</li><li id="ff2c" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第47行—遍历该帧的所有输出。</li><li id="be3a" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第48行—现在遍历所有检测。</li><li id="82f6" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第49行——检测阵列中有85个点。前四个索引用于盒子的坐标，从5到85的索引用于类别置信度。</li><li id="847f" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第50行—通过获取得分最高的元素的索引来获取类id。</li><li id="0c33" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第51–52行—如果class_id不为0(个人)，请继续。因为我们在这个用例中的主要目的只是检测人。</li><li id="bbe7" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第53行—获得信心得分。</li><li id="37fc" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第54行—如果置信度大于30%，则继续进行。</li><li id="aae1" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第55–56行-计算中心x和中心y点。</li><li id="97ce" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第58–61行-计算边界框的x，y，w，h。</li><li id="46c3" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第63行——将这个边界框添加到我们的框列表中。</li><li id="ad5c" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第64行——在机密列表中附加机密。</li><li id="3029" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第66行——这里我们使用<a class="ae jc" href="https://docs.opencv.org/4.5.2/d6/d0f/group__dnn.html#ga9d118d70a1659af729d01b10233213ee" rel="noopener ugc nofollow" target="_blank">cv2 . dnn . NMS box</a>执行边界框的非最大抑制。它将返回一个索引列表，包含我们必须考虑的那些索引的列表。</li><li id="077d" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第72–76行——现在遍历所有的框，只选择那些索引在索引列表中的框。并且仅在人员列表中附加这些相关的框。另外，在person_centres数组中追加框中心。</li><li id="9711" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第79–83行——现在遍历person_centres数组，找到所有违反50像素社交距离标准的person centers。我们将通过距离函数中的这些人中心点，并检查所有人之间的距离。我们将把这些违规者添加到违规数组中。</li><li id="4e08" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第85–93行——简单地在违反社交距离规范的人周围画一个红框，在没有违反的人周围画一个绿框。</li><li id="2f53" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第95行—在屏幕上显示违规次数。</li><li id="d8e2" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第96行—以视频形式保存输出。</li><li id="4402" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第97行—显示了输出。</li><li id="123b" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第99–100行—如果有人按ESC键，破解密码。</li><li id="690b" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">第102–103行—释放VideoCapture对象并销毁所有打开的窗口。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/1da75418b40611ee3799c0bbba96f4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*zq1YVQZWrst6TCY0.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">social distancing</figcaption></figure><p id="3cff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="lz">注意——虽然最好的方法是使用3D投影进行最佳预测，但为了简单起见，我简单地使用了欧几里德距离。</em>T5】</strong></p><p id="8c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="lz">探索更多机器学习、深度学习、计算机视觉、NLP、Flask项目访问我的博客。</em>T9】</strong></p><div class="ma mb ez fb mc md"><a href="https://machinelearningprojects.net/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">家庭机器学习项目</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">机器学习(ML)是对计算机算法的研究，它通过经验和使用自动改进</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">machinelearningprojects.net</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jj md"/></div></div></a></div><p id="902d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如需进一步的代码解释和源代码，请访问此处</strong> —</p><div class="ma mb ez fb mc md"><a href="https://machinelearningprojects.net/social-distancing-using-yolov3/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">社交距离使用YOLOv3 -对象检测-带源代码-最简单的解释-有趣…</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">所以伙计们，伙计们，这是最期待的项目之一，使用YOLOv3和OpenCV的社交距离。没有任何…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">machinelearningprojects.net</p></div></div><div class="mm l"><div class="ms l mo mp mq mm mr jj md"/></div></div></a></div><p id="508c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lz">这就是我写给这个博客的全部内容，感谢你的阅读，我希望你在阅读完这篇文章后，能有所收获，直到下一次👋… </em></p><div class="ma mb ez fb mc md"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">medium.com</p></div></div><div class="mm l"><div class="mt l mo mp mq mm mr jj md"/></div></div></a></div></div></div>    
</body>
</html>