<html>
<head>
<title>The heart of logistic regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归的核心</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-heart-of-logistic-regression-2bc2554dfebe?source=collection_archive---------2-----------------------#2021-04-12">https://medium.com/mlearning-ai/the-heart-of-logistic-regression-2bc2554dfebe?source=collection_archive---------2-----------------------#2021-04-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/73b17778001f2e5ea7034f2c1f91ecbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-SKFVIdQncfp-06HyQQn4Q.jpeg"/></div></div></figure><p id="dcd8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在本帖中，我们将使用与健康问题相关的数据集开发一个二元分类模型。它是从<a class="ae jn" href="https://www.kaggle.com/nareshbhat/health-care-data-set-on-heart-attack-possibility?select=heart.csv" rel="noopener ugc nofollow" target="_blank">kaggle.com</a>获得的，根据获得的14个特征，它与心脏病发作的可能性有关。</p><p id="2075" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将从探索已经干净的数据集(<em class="jo">没有空值</em>)开始，然后生成逻辑回归模型并评估其性能。</p><h1 id="2bff" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">逻辑回归</h1><p id="f937" class="pw-post-body-paragraph ip iq hh ir b is kn iu iv iw ko iy iz ja kp jc jd je kq jg jh ji kr jk jl jm ha bi translated">是一种回归分析，用于根据独立变量或预测变量预测分类变量(<em class="jo">一种可以包含有限数量类别的变量，通常为两个</em>)的结果。这对于将事件发生的概率建模为其他因素的函数非常有用。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/f6266764f45c35a69bd24312994de6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh01Era4F7ejyAlgoTCcqQ.png"/></div></div></figure><p id="444b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个过程与线性回归非常相似，只是响应变量是二项式的。结果是每个变量对所观察到的感兴趣事件的比值比的影响。</p><p id="889b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，当因变量本质上是二元时，使用逻辑回归(<em class="jo">或logit </em>)。例如，预测是否是垃圾邮件、是否患有covid、评论是否为正面评论等。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/25cc6835b43d656c360a9613126f1a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*dhPHcdtozxfuBsD5p-Xtlg.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx">Linear VS Logistic Regression</figcaption></figure><h1 id="b257" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">导入库和浏览数据集</h1><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="558e" class="lh jq hh ld b fi li lj l lk ll">import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>from sklearn import preprocessing<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import metrics<br/># create dataframe from csv file<br/>heart_data = pd.read_csv('./data-science/heart.csv')<br/>heart_data.head()</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/50818831891459cca9e33e6262571e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*62eH-VbwjGVmymbckuB4aw.png"/></div></div></figure><p id="741a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">属性信息:</p><p id="aca9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> 0)年龄— 1)性别— 2)胸痛类型(4个值)— 3)静息血压— 4)血清胆固醇(毫克/分升— 5)空腹血糖&gt; 120毫克/分升— 6)静息心电图。结果(值0，1，2) — 7)达到的最大心率— 8)运动诱发的心绞痛— 9)旧峰值— 10)运动峰值的斜率— 11)主要血管的数目(0—3)—12)thal:0 =正常；1 =固定缺陷；2 =可逆转的缺陷——13)目标:0=心脏病发作几率降低1=心脏病发作几率增加</em></p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/cbbb086c4d237a9533d73ebc5dc8b86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*dL2XRouYTmdxRNUtpBhaow.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx">heart_data.info()</figcaption></figure><p id="ba6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们开始在现有的数据上做一些探索。在下图中，我们有一个变量之间的热图关联图，包括我们的目标。接近0的值表示没有相关性，负值表示与负趋势(越接近-1越大)和正趋势相关，反之亦然。</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="b91e" class="lh jq hh ld b fi li lj l lk ll"># Correlation ranges from -1 to +1. <br/># Values closer to zero means there is no linear trend between the 2 variables. <br/># Close to 1, more positive correlation, # –1 indicates a perfect negative corr.</span><span id="f2af" class="lh jq hh ld b fi lo lj l lk ll">corr = heart_data.corr()<br/>plt.figure(figsize=(16,9))<br/>ax = sns.heatmap(<br/>    corr, <br/>    annot = True,     <br/>    vmin=-1, vmax=1, center=0,<br/>    cmap=sns.diverging_palette(10, 240, n=50),<br/>    square=True<br/>)<br/>ax.set_xticklabels(<br/>    ax.get_xticklabels(),<br/>    rotation=45,<br/>    horizontalalignment='right'<br/>)</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/b957917f4cf063cd14ccd92c138ebaa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByVLUdSvA3ji9f6DNbfyxg.png"/></div></div></figure><p id="9acc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在相关图中，我们看到特征fbs和chol与目标的相关性接近于0 ( <em class="jo">没有相关性</em>)，因此我们认为它们在我们的模型中不重要，我们将继续删除。我们实际上将创建一个新的数据帧(<em class="jo"> heart_part </em>)来保存原来的数据帧，以防我们在某个时候需要重用它</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="de40" class="lh jq hh ld b fi li lj l lk ll"># Drop 2 features with poor correlation with target<br/>heart_part=heart_data.drop(columns=['fbs', 'chol'])</span></pre><p id="fc13" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们想通过cp ( <em class="jo">胸痛类型</em>)更好地观察年龄和最大心率(<em class="jo"> thalach </em>)之间的-0.4 ( <em class="jo">负趋势</em>)的相关性:</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="d216" class="lh jq hh ld b fi li lj l lk ll"># Chest pain type (cp) group<br/># Age VS maximum heart rate (thalach) = -0.4 trend<br/>g = sns.FacetGrid(heart_data, col="cp")<br/>g.map(sns.scatterplot, "age", "thalach", alpha=.7)<br/>g.add_legend()</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/5abf21b4c0b18d41073d28622f2feea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6RT5X5QWbJaiVk8zb7D_A.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx">max rate heart VS age — by chest pain type</figcaption></figure><p id="c850" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们也可以用seaborn来形象化两个变量之间的关系，通过一条sigmoid线来分隔点。为此，我们将使用带逻辑=真参数的<a class="ae jn" href="https://seaborn.pydata.org/generated/seaborn.lmplot.html" rel="noopener ugc nofollow" target="_blank"> lmplot </a>方法。</p><p id="831e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在下图中，我们看到了目标(<em class="jo">0–1</em>)对特征thalach。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/086b0644033442bb09d42c22a5f6b0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*K1gYHzWX-EEZmm2ZUGuByg.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx">seaborn lmplot sigmoid function (target [0,1] VS max. rate heart)</figcaption></figure><h1 id="37fb" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">建立我们的逻辑回归模型</h1><p id="99a3" class="pw-post-body-paragraph ip iq hh ir b is kn iu iv iw ko iy iz ja kp jc jd je kq jg jh ji kr jk jl jm ha bi translated">我们将首先创建我们的训练和测试数据集，然后验证我们的训练数据中没有<strong class="ir hi">类不平衡</strong>问题</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="c6b4" class="lh jq hh ld b fi li lj l lk ll"># X features and Y labels<br/>X=heart_part.drop(columns=['target'])<br/>Y=heart_part['target']<br/># Build train and test datasets<br/>X_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=0)</span></pre><p id="bf2a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">阶层失衡问题</strong></p><p id="7509" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是分类模型中出现的一个问题，因为样本在类别中的分布是不平衡的，或者说是偏斜的。当一个或多个类别与其他类别相比在训练数据中具有非常低的样本比例时，会出现这种情况。</p><p id="6a92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">为什么要解决？</strong></p><p id="863f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来谈谈“<strong class="ir hi">准确性悖论</strong>”，这是一个自相矛盾的发现，即在预测分析中进行分类时，准确性不是预测模型的一个好指标。例如，如果我们对不平衡数据(<em class="jo">90%的样本来自第1类，只有10%来自第2类</em>)有90%的准确性，我们的模型将查看数据并决定它能做的最好的事情是预测数据是第1类！！在这种情况下，只测量准确性会导致我们在分类中处于误导的位置</p><p id="b6a5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">如何解决？</strong></p><p id="152d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">阶级不平衡并没有解决，它仍然是一个开放的问题，一旦确定，必须解决一些方法来缓解它，这将在未来的帖子中讨论。</p><p id="e298" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们检查类样本没有过度不平衡。</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="a68c" class="lh jq hh ld b fi li lj l lk ll">from collections import Counter<br/>Counter(y_train)</span></pre><p id="9aee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计数器({0: 105，1: 122})</p><p id="c5ad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">两个类中的训练数据非常接近，所以在这种情况下我们不应该担心类不平衡的问题。</p><p id="aac0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们必须对X数据进行标准化，使它们处于相同的值范围内，并促进模型的计算</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="5ed2" class="lh jq hh ld b fi li lj l lk ll"># Scale the X data<br/>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><p id="a55f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们创建我们的逻辑回归模型</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="03ad" class="lh jq hh ld b fi li lj l lk ll"># build logistic regression model, train and predict test data<br/>lr = LogisticRegression()<br/>model = lr.fit(X_train, y_train)<br/>lr_predict = lr.predict(X_test)</span></pre><p id="1580" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来衡量模型的性能</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="8708" class="lh jq hh ld b fi li lj l lk ll"># import data modeling metrics<br/>from sklearn.metrics import confusion_matrix, accuracy_score, classification_report<br/>lr_conf_matrix = confusion_matrix(y_test, lr_predict)<br/>lr_acc_score = accuracy_score(y_test, lr_predict)<br/>print("confusion matrix")<br/>print(lr_conf_matrix)<br/>print("\n")<br/>print("Accuracy of Logistic Regression:",lr_acc_score*100,'\n')<br/>print(classification_report(y_test,lr_predict))</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/aab84fa22f20398ec5af06a71a424aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*cFmelH4ciwqcWDNzDMfe0A.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx">classification_report from sklearn.metrics</figcaption></figure><p id="8a66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了更好的可视化我们的混淆矩阵，我们将使用Seaborn</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="7256" class="lh jq hh ld b fi li lj l lk ll">df_cm = pd.DataFrame(lr_conf_matrix, columns=np.unique(y_test), index = np.unique(y_test))<br/>df_cm.index.name = 'Real'<br/>df_cm.columns.name = 'Predicted'<br/>plt.figure(figsize = (10,7))<br/>sns.set(font_scale=1.2)<br/>sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 14})</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lt"><img src="../Images/c311c9cf93b4ce4fa6ef1fb867cb63ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ol6MH9_nbxNh8khQDKm-Aw.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx">seaborn heatmap confusion matrix</figcaption></figure><p id="f4a4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好的，但是所有这些指标意味着什么呢？如果你读过之前关于线性回归的帖子，你应该知道<strong class="ir hi">精度</strong>的值，在这个例子中是82.89%，还有其他的吗？</p><p id="1ecd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">二元分类的度量</strong></p><p id="c89f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">一)</strong>T22】混淆矩阵</p><p id="120c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是一个包含预测值和实际值的4种不同组合的表格。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lu"><img src="../Images/445770368913800052a6b189c96d6077.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*D3itfff3XUaKeNPJYtdmIQ.png"/></div></div></figure><p id="71e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> TN </strong>(真negat。):预测负面，这是真的</p><p id="5fbc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> FP </strong>(错误位置。/类型1错误):预测为正，但为假</p><p id="adb3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> FN </strong>(假阴性。/类型2错误):预测为负，这是假的</p><p id="0726" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> TP </strong>(真实位置。) :预测积极，这是真的</p><p id="697d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的例子中，例如，我们有39个TP和4个FN。这表明我们的模型预测实际43个样本中的39个样本适合心脏病发作，剩下4个样本未被识别为阳性(2型错误)。在这种情况下，如果我们预测患者会出现严重的健康问题，我们应该最小化最后一个值(FN)。</p><p id="e647" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">而该模型已经将33个样本中的24个识别为阴性，这使得我们有9个样本没有被正确地识别为阴性。虽然错误率较高，但不像前一个案例(FN)那样严重，已经确定了9个患者样本有心脏病发作的可能性，而实际上没有(FP)。</p><p id="d3e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> b) </strong> <strong class="ir hi">精度</strong> = TP / (TP+FP)</p><p id="8319" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">重要说明:如果我们必须避免假阳性，在我们的例子中，我们查看1级的值(<em class="jo">心脏病发作的可能性更大</em> ) = &gt; 39 / (39+9) = 0，8125</p><p id="45e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> c) </strong> <strong class="ir hi">召回</strong>(也叫<em class="jo">真阳性率</em>或<em class="jo">灵敏度</em> ) = TP / (TP+FN)</p><p id="adb2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们必须避免假阴性(这是我们的健康案例)=&gt; 39 / (39+4) = 0，9069，这一点很重要</p><p id="3ca5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">d)</strong><strong class="ir hi">F1-得分</strong> = 2。TP / (2。TP)+FN+FP)</p><p id="5c33" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将精度和召回率组合成一个值=&gt; 2.39 / (2.39)+4+9 = 0，8571</p><p id="7297" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">记住精确度和召回率之间的权衡是很重要的(<em class="jo">我们提高精确度，降低召回率，反之亦然</em></p><p id="3a71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简而言之有:</p><p id="e747" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">更多<strong class="ir hi">面向回忆的</strong>任务(<em class="jo">如果我们没有检测到积极的</em>结果会更高)</p><ul class=""><li id="7041" class="lv lw hh ir b is it iw ix ja lx je ly ji lz jm ma mb mc md bi translated"><em class="jo">疾病检测、法律信息的搜索和提取</em></li></ul><p id="ad11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">更多<strong class="ir hi">面向精度的</strong>任务(<em class="jo">如果我们没有检测到负面的结果</em>，后果会很严重)</p><ul class=""><li id="0f04" class="lv lw hh ir b is it iw ix ja lx je ly ji lz jm ma mb mc md bi translated"><em class="jo">搜索引擎排名，文档分类器</em></li></ul><p id="baaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> e) </strong> <strong class="ir hi">误报率</strong> ( <em class="jo">误报率，漏报率</em> ) = FP / (FP+TN)</p><p id="cea3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总结当实际结果为负时，预测为正类的频率。</p><p id="5ac1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> f) </strong> <strong class="ir hi"> ROC曲线</strong></p><p id="320a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接收器操作特性(<em class="jo"> ROC曲线</em>)总结了使用不同概率阈值的预测模型的真阳性率/召回率(<strong class="ir hi"> b </strong>)和假阳性率(<strong class="ir hi"> e </strong>)之间的权衡。当观察值在每一类之间平衡时，使用它是恰当的</p><p id="a0d5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> ROC/AUC曲线</strong></p><p id="5770" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，有足够的指标来思考，让我们回到评估我们的逻辑模型。我们已经说过，我们的两个类是相当平衡的，所以最好是查看采用ROC曲线值的模型的性能。嘿停在这里！，为什么可视化这个ROC曲线很重要？</p><ol class=""><li id="3f99" class="lv lw hh ir b is it iw ix ja lx je ly ji lz jm me mb mc md bi translated">不同模型的曲线一般可以直接比较，也可以针对不同的阈值进行比较。</li><li id="b703" class="lv lw hh ir b is mf iw mg ja mh je mi ji mj jm me mb mc md bi translated">曲线 ( <em class="jo"> AUC </em>)下的<strong class="ir hi">面积可以作为模型技巧的总结。</strong></li></ol><p id="49e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请记住，当我们预测一个二元结果时，它是正确的预测(真阳性)还是错误的预测(假阳性)。真否定和假否定也会产生同样的紧张。那么，什么是有技巧的模特呢？一个熟练的模型将分配一个更高的概率给一个随机选择的真正的积极事件，而不是一个平均的消极事件。熟练的模式通常用曲线表示，曲线向图的左上角倾斜。</p><p id="77b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将根据我们的数据生成roc-auc曲线图，为此，我们必须首先获得一个列表，其中包含我们的分类器模型对每个样本(行)做出的概率预测</p><pre class="kt ku kv kw fd lc ld le lf aw lg bi"><span id="c7a4" class="lh jq hh ld b fi li lj l lk ll"># obtain prediction for each row of X_test<br/>predictions = model.predict_proba(X_test)</span><span id="a3ae" class="lh jq hh ld b fi lo lj l lk ll"># build roc_curve values with y_test and probability <br/># predictions of 1 value class (<em class="jo">more chance heart attack</em>)<br/>fpr, recall, thresholds = roc_curve(y_test, predictions[:, 1])</span><span id="e3f1" class="lh jq hh ld b fi lo lj l lk ll"># compute Area Under the Roc Curve<br/>roc_auc = auc(fpr, recall)</span><span id="3dcd" class="lh jq hh ld b fi lo lj l lk ll"># Plot Roc-Auc<br/>plt.figure(figsize = (10,7))<br/>plt.title('Receiver Operating Characteristic')<br/>plt.plot(fpr,recall,'b',label='AUC = %0.2f' % roc_auc)<br/>plt.legend(loc='lower right')<br/>plt.plot([0,1], [0,1], 'r--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.0])<br/>plt.xlabel('Fall-out')<br/>plt.ylabel('Recall')<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mk"><img src="../Images/19ff1b2e7c7825bc7cb592fffac15471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSWK0zGALBkrPIYWxKfqFw.png"/></div></div></figure><p id="13b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">红色虚线将是一个没有技巧的模型，它不能区分类别，并在点(0.5，0.5)处表示，在每个阈值处，它由从图的左下方到右上方的对角线表示，并具有0.5的AUC。</p><p id="b2c0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一个能力完美的模型被表示在一个点(0，1)上。我们的ROC曲线用蓝线表示，优于随机的0.5 AUC ( <em class="jo">红色虚线</em>)。</p><p id="8ae5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">像往常一样，github链接附有完整的<a class="ae jn" href="https://github.com/jrercoli/logistic_regression_heart_attack" rel="noopener ugc nofollow" target="_blank">逻辑回归jupyter笔记本</a>，这样你可以自己验证代码。</p><p id="454d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">也请关注我的数据科学博客<a class="ae jn" href="https://rorjor.wixsite.com/empoweredatascience" rel="noopener ugc nofollow" target="_blank"> EmpowereDataScience </a></p><p id="2b64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，伙计们，现在是时候使用<a class="ae jn" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>通过逻辑回归模型项目生成您自己的二元分类，并推进您的数据科学职业生涯了，再见。</p><p id="6522" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的评论</p></div></div>    
</body>
</html>