<html>
<head>
<title>Imputing Missing Values with Machine Learning-Based Approaches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用基于机器学习的方法输入缺失值</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/imputing-missing-values-with-machine-learning-based-approaches-373219dba314?source=collection_archive---------2-----------------------#2022-03-20">https://medium.com/mlearning-ai/imputing-missing-values-with-machine-learning-based-approaches-373219dba314?source=collection_archive---------2-----------------------#2022-03-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2a18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我之前的博客文章中已经深入研究了数据准备(如果感兴趣，请查看<a class="ae jc" rel="noopener" href="/@sabrinaherbst/an-introduction-to-instance-selection-in-data-mining-7b2ac94fb07">数据挖掘中实例选择的介绍</a>)，我决定继续研究数据预处理中的另一个重要主题，即处理缺失值。</p><p id="6bf1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">特别是，我想深入研究基于最大似然法的方法，例如使用KNN或K均值聚类来估算缺失值。除非另有说明，相关信息摘自García等人[1]的第4章。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/334b0762ea78b7d8160a633599671ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPZcoAD9kERfEQxwlaPT-A.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">pixabay.com</figcaption></figure><h1 id="321b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">缺失价值的分类</h1><p id="4138" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">当考虑缺失值时，可以大致区分<strong class="ig hi">随机缺失(MAR) </strong>和<strong class="ig hi">非随机缺失(MNAR) </strong>变量。</p><p id="a85d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们考虑将输入变量<em class="kw"> X </em>分成观察部分和缺失部分。MAR的一个相当简单明了的观点是，缺失变量可以(但不一定)依赖于可用数据，但独立于其他缺失属性。</p><p id="4818" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">MAR的一个特例是<strong class="ig hi">完全随机缺失(MCAR) </strong>，其中缺失数据既不依赖于观测值，也不依赖于缺失数据。</p><p id="296b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，MNAR被定义为依赖于观察到的变量以及缺失的变量。请注意，以下方法依赖于MAR假设。</p><h1 id="544e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">使用ML技术估算数据</h1><p id="4af5" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">有许多不同的方法来处理缺失值。这些分类器的范围从相当简单的分类器(例如，删除缺失变量或根本不输入，这对于像朴素贝叶斯这样的ML模型是可能的)到复杂的统计分类器，依赖于找到/近似潜在的分布(例如，最大似然估计)。</p><p id="90cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用最大似然模型估算数据的一个优点是不需要明确地估计潜在的分布。熟悉ML模型工作的数据科学家将会看到，输入数据的过程与构建预测ML模型的过程非常相似。</p><h2 id="4210" class="kx ju hh bd jv ky kz la jz lb lc ld kd ip le lf kh it lg lh kl ix li lj kp lk bi translated">k-最近邻插补</h2><p id="c867" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">可能最直接的基于ML的数据插补策略之一是使用K近邻插补(KNNI)。顾名思义，它使用KNN模型来预测缺失值。</p><p id="29d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，重要的是:( 1)定义要考虑的多个邻居<em class="kw">k</em>,以及(2)选择最终预测值的策略。要考虑的相邻点数量取决于不同的数据集特征(例如，在处理小数据集时，我们将考虑较少数量的相邻点)，因此，必须针对每个数据集单独计算。</p><p id="48fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">预测最终值的策略通常与KNN分类器相同。因此，对于连续变量，将选择平均值，而对于名义属性，将进行多数投票。</p><p id="1501" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNNI的思想已经扩展到加权K近邻填充(WKNNI)。作为扩展，该算法考虑了相邻数据点到插补的相应数据点的距离(例如，最近的相邻数据点比第k个最近的相邻数据点“更”重要)[2]。</p><p id="51ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，欧几里得距离将被认为是一种距离度量。但是，请记住，当KNN计算数据点之间的距离时，变量的缩放会对分类器产生很大的影响。</p><h2 id="98a5" class="kx ju hh bd jv ky kz la jz lb lc ld kd ip le lf kh it lg lh kl ix li lj kp lk bi translated">k-均值聚类插补(KMI)</h2><p id="97a0" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在K-Means聚类中，输入数据被分成<em class="kw"> k </em>个聚类。然后，通过聚类中所有元素的平均值来计算每个聚类的聚类质心。目标是在同一聚类中有相似的数据点，从而最小化<em class="kw">聚类内相异度</em>(聚类中的数据点和质心之间的差异总和)。</p><p id="979a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将数据集分为两部分，即一部分包含所有完整的数据点，另一部分包含具有缺失值的样本。当考虑一组完整的数据点时，KMI首先声明<em class="kw"> k </em>元素为质心。</p><p id="79cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">之后，修改聚类以减少聚类内的不相似性，直到达到某个不相似性阈值或者不再进行改变。</p><p id="66f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，添加具有缺失值的元素的剩余数据分区，并将每个数据点分配给一个聚类。为了找到样本的插补值，可以选择任意最近邻算法。我们将同一簇中的所有元素定义为最近邻。</p><p id="d1dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种方法也被扩展到模糊K均值聚类，其中一个节点可以同时属于(至少部分属于)多个聚类。欲了解更多信息，请参考李等人[3]。</p><h2 id="e9c7" class="kx ju hh bd jv ky kz la jz lb lc ld kd ip le lf kh it lg lh kl ix li lj kp lk bi translated">其他方法</h2><p id="a776" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">还有许多其他基于最大似然法的数据插补方法，例如下面列出的方法。如果感兴趣，请参考各自的论文。</p><ul class=""><li id="69bf" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb lq lr ls lt bi translated">奇异值分解插补[2]</li><li id="8fda" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">事件覆盖[4]</li><li id="99d6" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">局部最小二乘插补[5]</li></ul><p id="cf86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据插补领域的研究在不断发展，新论文的发表非常频繁，提出了对现有技术的改进。虽然不可能在一篇简短的博客文章中涵盖所有内容，但我希望这篇文章仍能让您对将来如何处理丢失的数据有所了解。</p></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="e4cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1] García等<a class="ae jc" href="https://link.springer.com/book/10.1007/978-3-319-10247-4" rel="noopener ugc nofollow" target="_blank">数据挖掘中的数据预处理</a>。<em class="kw">智能系统参考图书馆。</em> 2015年。</p><p id="5566" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] Troyanskaya等<a class="ae jc" href="https://doi.org/10.1093/bioinformatics/17.6.520" rel="noopener ugc nofollow" target="_blank">DNA微阵列的缺失值估计方法</a>。<em class="kw">生物信息学。17/6: 520–525.2001年。</em></p><p id="ea3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]李等.【面向缺失数据填补:模糊K-均值聚类方法研究】。<em class="kw">粗糙集与当前计算趋势。2004年。</em></p><p id="7196" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4] Wong等<a class="ae jc" href="http://doi.org/10.1109/tpami.1987.4767986" rel="noopener ugc nofollow" target="_blank">从不完全混合模式数据中综合统计知识。</a> <em class="kw"> IEEE传输模式分析机器智能。9/6:796–805</em>。1987.</p><p id="4114" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5] Kim等<a class="ae jc" href="https://doi.org/10.1093/bioinformatics/bth499" rel="noopener ugc nofollow" target="_blank">DNA微阵列基因表达数据的缺失值估计:局部最小二乘插补</a>。<em class="kw">生物信息学。</em><em class="kw">21/2:187–198</em>。2005.</p><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx jn mj"/></div></div></a></div></div></div>    
</body>
</html>