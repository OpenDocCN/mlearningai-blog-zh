<html>
<head>
<title>Explaining/Interpreting Machine Learning models using Visual Analytics — A Background.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用视觉分析解释/诠释机器学习模型-背景。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/explaining-interpreting-machine-learning-models-using-visual-analytics-a-background-db132c471cea?source=collection_archive---------1-----------------------#2021-08-11">https://medium.com/mlearning-ai/explaining-interpreting-machine-learning-models-using-visual-analytics-a-background-db132c471cea?source=collection_archive---------1-----------------------#2021-08-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="873e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习在当今世界已经变得无处不在，我们对算法所做决策的依赖也在增加。此外，随着现实生活中问题的复杂程度激增，ML从业者一直在寻求开发更好的算法来拟合曲线。这也让我们想到一些常见的问题-</p><p id="0448" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“我们如何理解这些复杂模型的运作？”</p><p id="9f87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">“我们如何验证模型做出的决策？”</em></p><p id="da7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着最大似然算法应用的增加，对透明和可解释模型的需求变得至关重要。这对于涉及人类的决策尤为重要。这一领域的一个常见例子是COMPAS(替代制裁的矫正罪犯管理概况)，这是一个判断刑事被告成为惯犯可能性的软件。该工具因其带有偏见的种族决定和未能解释这些决定而受到广泛批评。这种类型的场景质疑算法中的“信任”,并且可能是不道德的。</p><p id="488e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">那么这些问题的解决方案是什么呢？</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/78ff04f29d1e39dbe5e2732cbd00b565.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/0*wWXYutVvMB9f65R2.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Ref: A. Adadi and M. Berrada, “Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),” in IEEE Access, vol. 6, pp. 52138–52160, 2018, doi: 10.1109/ACCESS.2018.2870052.</figcaption></figure><p id="f802" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的云一词总结了处理上述问题的研究领域和相关术语。最近，从主要研究渠道获得牵引力的领域是<strong class="ig hi">可解释的人工智能(XAI) </strong>和<strong class="ig hi">可解释的人工智能</strong>。虽然这两个词经常被用作同义词，但它们之间有着精确的区别。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jp"><img src="../Images/32bce4f6b72dab51fe35c83781c1c710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*og5dGYlP0Op2n3_3.jpeg"/></div></div></figure><h2 id="35d7" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated"><strong class="ak">讲解策略:</strong></h2><p id="8c79" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">在对现有可解释策略的文献调查中，<em class="jc"> Adadi等人</em>。将这些方法主要分为三种类型，我个人认为这些策略涵盖了解释的所有主要方面。他们是-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ku"><img src="../Images/32d7b54a05667a087606982d6084361b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2CoFa6iLFZE0LNpY.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Explainability strategies</figcaption></figure><ul class=""><li id="1a8f" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">基于复杂性的</strong>:模型的复杂性往往与其解释成正比。所以，这种方法的想法是制造简单和更容易解释的模型。一个典型的<em class="jc">例子</em>可以是将深度神经网络提炼到决策树中，因为决策树非常容易理解(查找模型代理以获得详细信息)。</li><li id="1654" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">基于作用域的</strong>:解释一个模型可以从两个作用域——全局&amp;局部。</li></ul><p id="a05b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> —全局:</em>目标解释整个模型逻辑，帮助人类理解模型的内部行为。<em class="jc">示例</em>:在决策过程中使用了哪些特征？模型学到的知识是什么？</p><p id="2092" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> —本地:</em>解释特定的实例及其结果预测。例:为什么一个人被拒绝贷款？</p><ul class=""><li id="01b0" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">基于模型的</strong>:触及模型的本质。</li></ul><p id="9d1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> —不可知:</em>适用于所有模型，不管其性质如何。</p><p id="208a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc"> —特定:</em>限于特定型号。<em class="jc">举例</em>:解释CNN的技巧。</p><h2 id="8eba" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated"><strong class="ak">用于解释的视觉分析&amp;解释ML模型:</strong></h2><p id="ac14" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">很长一段时间以来，视觉隐喻已经被认为能够解决解释ML算法的问题。自然地，人类擅长使用可视化技术理解数据，同样的原理也适用于解释ML模型。近年来，许多视觉分析系统被开发出来，以理解如下所示的三个重要方面:</p><p id="84cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解释模型的内部工作原理。</p><ul class=""><li id="536f" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><em class="jc">示例</em>—CNN的最后一个隐藏层激活在下面的训练后被可视化。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/2920265a8b357366510401c8e8133c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*_kZt1BHnTUsscieR.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Ref: P. E. Rauber, S. G. Fadel, A. X. Falcão and A. C. Telea, “Visualizing the Hidden Activity of Artificial Neural Networks,” in IEEE Transactions on Visualization and Computer Graphics, vol. 23, no. 1, pp. 101–110, Jan. 2017, doi: 10.1109/TVCG.2016.2598838.</figcaption></figure><p id="57fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从模型中提取信息，即事后可解释性。</p><ul class=""><li id="99b6" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><em class="jc">示例</em> —视觉分析系统<em class="jc"> RuleMatrix </em>从神经网络中提取规则，并解释分类器的决策。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es lk"><img src="../Images/0dea25a53e29749f32d119bd06f39b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bkIXLLGSKKrL48zC.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Ref: Y. Ming, H. Qu and E. Bertini, “RuleMatrix: Visualizing and Understanding Classifiers with Rules,” in IEEE Transactions on Visualization and Computer Graphics, vol. 25, no. 1, pp. 342–352, Jan. 2019, doi: 10.1109/TVCG.2018.2864812.</figcaption></figure><p id="c817" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">启用性能诊断以构建更精确的模型。</p><ul class=""><li id="c7c5" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><em class="jc">示例</em> —可视化分析系统ReVACNN，通过在训练过程中动态添加/删除节点和层来实现实时模型导向，以构建更好的CNN模型。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ll"><img src="../Images/30887759871b7b35f7513f9eb317edc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7wS47Cg94R-qsnhK.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Ref: Chung, Sunghyo, Park, Cheonbok, Suh, Sangho, Kang, Kyeongpil, Choo, Jaegul, and Kwon, Bum Chul in<em class="lm"> </em>Future of interactive learning machines workshop at the 30th annual conference on neural information processing systems (NIPS) 2016</figcaption></figure><h2 id="0827" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated"><strong class="ak">explability的目标用户是谁？</strong></h2><p id="3a31" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">模型可解释性的观点因用户而异。例如:为ML模型开发人员构建一个工具可能不会对那些有ML基础知识的人有太大的影响。通常，目标用户分为以下三种类型:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ln"><img src="../Images/bf06587dfe77f6046284da216d1b6941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yCrHNyU-7nHe426m.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Orig ref: Hohman FM, Kahng M, Pienta R, Chau DH. Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. IEEE Trans Vis Comput Graph. 2018 Jun . doi:10.1109/tvcg.2018.2843369. PMID: 29993551; PMCID: PMC6703958.</figcaption></figure><p id="9605" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我们介绍了模型可解释性背后的背景，以及可视化分析如何在这个过程中作为一个补充工具。在接下来的几周内，我们将深入研究该领域的各种研究，以了解模型的可解释性/解释。我们还将看看各种可用的开源工具，以帮助我们更好地利用机器学习算法。</p><h2 id="db9d" class="ju jv hh bd jw jx jy jz ka kb kc kd ke ip kf kg kh it ki kj kk ix kl km kn ko bi translated">一些我真正喜欢的解释ML模型的工具:</h2><ul class=""><li id="8f54" class="kv kw hh ig b ih kp il kq ip lo it lp ix lq jb la lb lc ld bi translated"><a class="ae lr" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">嵌入投影仪</a> —大规模嵌入的解释。</li><li id="3a3f" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><a class="ae lr" href="https://tensorboard.dev/#get-started" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a> —理解、调试和优化基于TensorFlow的模型。</li><li id="218e" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><a class="ae lr" href="https://playground.tensorflow.org/#activation=relu&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2,2&amp;seed=0.32054&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" rel="noopener ugc nofollow" target="_blank"> TensorFlow Playground </a> —可视化分析工具，无需编程即可探索简单的神经网络。</li><li id="2573" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><a class="ae lr" href="https://teachablemachine.withgoogle.com/" rel="noopener ugc nofollow" target="_blank">可教机器</a>——教授图像、音频分类和姿态估计的基础知识。</li></ul><p id="bff4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想分享一些模型可解释性/可解释性工具？在—<a class="ae lr" href="https://www.linkedin.com/in/dipankar-mazumdar/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>&amp;<a class="ae lr" href="https://twitter.com/Dipankartnt" rel="noopener ugc nofollow" target="_blank">Twitter</a>上与我联系。</p></div></div>    
</body>
</html>