<html>
<head>
<title>The forest of amazon reviews, part 2 (an NLP story)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊森林评论，第2部分(NLP的故事)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-forest-of-the-amazon-reviews-part-2-an-nlp-story-e7441439caf5?source=collection_archive---------4-----------------------#2021-05-17">https://medium.com/mlearning-ai/the-forest-of-the-amazon-reviews-part-2-an-nlp-story-e7441439caf5?source=collection_archive---------4-----------------------#2021-05-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/454abe63060a9c048092fc8436634b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_IWbKRZXzKhJNxzCh37pCA.jpeg"/></div></div></figure><p id="65f8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">嗨，朋友们，这是基于亚马逊评论的NLP分析的第二部分，这里我们将描述我们将应用于我们的数据的机器学习阶段，特别是随机森林模型。</p><p id="cb8c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<a class="ae jn" href="https://rorjor.medium.com/the-forest-of-the-amazon-reviews-part-1-an-nlp-story-fa98efa892b8" rel="noopener">第一部分</a>中，我们获得了数据，清理了数据，并在对评论进行标记化和应用矢量器之后构建了一个单词包。</p><p id="bc7d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们从解释什么是随机森林开始，最初我会说它是决策树的集合，那么什么是决策树呢？</p><h1 id="9ee9" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">决策图表</h1><p id="8971" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">是一种监督学习算法，主要用于分类问题。根据输入变量中最显著的差异，将数据分成两个或多个同类集合。决策树确定最重要的变量及其提供最佳同质群体集的值。</p><p id="6719" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">评估所有输入变量和所有可能的分割点，并选择结果最佳的一个。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kr"><img src="../Images/13d31d5191e3b27a52dfc5047057bb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S10T4ah3_JqdQ-eY6Hau0Q.png"/></div></div></figure><p id="d4c4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一张图胜过千言万语，好吗？</p><p id="4eb8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">优势:</p><ul class=""><li id="aafb" class="kw kx hh ir b is it iw ix ja ky je kz ji la jm lb lc ld le bi translated">易于解释和可视化模型结果</li><li id="a5c1" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">自动特征选择(不受无关特征的影响)</li><li id="0916" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">不需要标准化/缩放数据</li><li id="0b79" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">缺少的值对结果几乎没有影响</li></ul><p id="8cc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">缺点:</p><ul class=""><li id="33d4" class="kw kx hh ir b is it iw ix ja ky je kz ji la jm lb lc ld le bi translated">倾向于过度拟合</li><li id="46dc" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">对数据变化敏感(结果可能会发生巨大变化)</li><li id="0292" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">训练的好时机</li></ul><p id="867c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然已经明白了是决策树，那就去我们的森林吧:)</p><h1 id="2a0e" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">随机森林</h1><p id="87a2" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">创建决策树时出现的一个问题是，如果我们给它足够的深度，树倾向于“记忆”解决方案，而不是概括学习= &gt;过度拟合:(避免这种情况的解决方案是创建许多树并一起工作= &gt;森林:)</p><p id="6067" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一个<strong class="ir hi">随机森林</strong>是决策树结合<strong class="ir hi">打包</strong> ( <a class="ae jn" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">引导聚合</a>)的集合体。该模型获取训练数据，并给每棵树一个特征子集，分配数据的大小是相同的，尽管一些特征在每个子集中随机重复。我们还可以改变分配给每棵树的样本数量(<em class="lk">引导样本</em>)。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ll"><img src="../Images/642a3c2e482f7672b42229330a907e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VY3lEFysaQ0nnV_zkxyU-w.png"/></div></div></figure><p id="d5b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于分类问题，通常使用<strong class="ir hi">软投票</strong>来组合决策树的结果，这意味着给予结果更多的重要性，其中树更安全。</p><p id="6828" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">优势:</p><ul class=""><li id="8c95" class="kw kx hh ir b is it iw ix ja ky je kz ji la jm lb lc ld le bi translated">更好地概括，显著降低DT过拟合</li><li id="f8d7" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">不平衡数据上的良好性能</li><li id="4b6b" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">可以处理大型数据集(许多要素和许多样本)</li><li id="5b7a" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">缺少的值对结果几乎没有影响</li><li id="81e1" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">它被广泛用于提取数据集最重要的特征</li></ul><p id="bea7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">缺点:</p><ul class=""><li id="b395" class="kw kx hh ir b is it iw ix ja ky je kz ji la jm lb lc ld le bi translated">功能需要有预测能力</li><li id="48e4" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">它看起来像一个黑盒子，得到的结果很难解释</li></ul><p id="7b13" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简单介绍了随机森林模型之后，让我们继续讨论分类问题。</p><h1 id="9461" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">构建训练和测试数据</h1><p id="b4be" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">在<a class="ae jn" href="https://rorjor.medium.com/the-forest-of-the-amazon-reviews-part-1-an-nlp-story-fa98efa892b8" rel="noopener">第1部分</a>中，我们以一包单词的形式获得了我们的特征，并用它生成了一个熊猫数据帧(<em class="lk"> X_features_df </em>)，我们还有自己的标签(<em class="lk"> y_amz_rev_part </em>)。</p><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="e8a9" class="lr jp hh ln b fi ls lt l lu lv"># Split train and test <br/>X_train, X_test, y_train, y_test = train_test_split(X_features_df, y_amz_rev_part, test_size=0.2)</span><span id="0167" class="lr jp hh ln b fi lw lt l lu lv"># Class balance ??<br/>print('Clase 0-Bad review: {} - Clase 1-Good review: {}'.format(y_train.count(0), y_train.count(1)))</span></pre><p id="a047" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们检查了我们班级的平衡，没有不平衡的问题:</p><p id="c087" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lk">0级-差评:5054-1级-好评:4946 </em></p><h1 id="5117" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">建立射频模型，拟合和预测</h1><p id="d881" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">RF有大量用于调谐的超参数，我们将解释3个最重要的参数，我们将在稍后的练习中使用它们。</p><ul class=""><li id="1a17" class="kw kx hh ir b is it iw ix ja ky je kz ji la jm lb lc ld le bi translated">n_estimators:森林中树木的数量。默认值=100</li><li id="19b6" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">max_depth:树的最大深度。如果没有(默认值)，则节点会一直扩展，直到所有叶子都是纯的，或者直到所有叶子包含的样本都少于min_samples_split样本。</li><li id="ca24" class="kw kx hh ir b is lf iw lg ja lh je li ji lj jm lb lc ld le bi translated">n_jobs:并行运行的作业数量(默认情况下None=1)。输入-1以使用所有处理器。</li></ul><p id="570b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你在<a class="ae jn" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier" rel="noopener ugc nofollow" target="_blank"> sklearn </a>里有详细的列表。</p><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="386b" class="lr jp hh ln b fi ls lt l lu lv"># Now, build a RandomForest with parameters, and fit the model<br/>rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)<br/>rf_model = rf.fit(X_train, y_train)</span></pre><p id="492d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们说过，RF的优势之一是利用它获得最重要的特性:</p><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="3b8d" class="lr jp hh ln b fi ls lt l lu lv"># view the 10 more important features from the model. <br/># zip and sort (reverse)<br/>sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]</span></pre><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/addffdbd46cc186f4bd7fe54177a3540.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*Kmf3KecNLj7Wkpa6MV79Lw.png"/></div></figure><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="6df4" class="lr jp hh ln b fi ls lt l lu lv"># Predict with X_test and obtain score metrics<br/>from sklearn.metrics import precision_recall_fscore_support as score<br/><br/>y_pred = rf_model.predict(X_test)<br/>precision, recall, fscore, support = score(y_test, y_pred, average='binary')<br/>print('Precision: {} / Recall: {} / Fscore: {} / Accuracy: {}'.format(<br/> round(precision,3), round(recall,3), round(fscore,3), round((y_pred==y_test).sum() / len(y_pred),3)))</span></pre><p id="861d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">精确度:0.792 /召回率:0.803 / Fscore: 0.797 /准确度:0.796</p><h1 id="4d6a" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">努力取得更好的成绩</h1><p id="7c02" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">让我们尝试改变我们认为最重要的模型的超参数(<em class="lk">调整</em>),以尝试提高我们的预测水平。为此，我们将使用sklearn提供的<a class="ae jn" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=grid%20search#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>方法，我们将所有数据传递给它，因为该方法适合并预测，让我们开始编码:</p><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="89d6" class="lr jp hh ln b fi ls lt l lu lv">from sklearn.model_selection import GridSearchCV<br/><br/># using GridSearch with 9 combinations<br/>rf = RandomForestClassifier()<br/><br/># hyperparameters setting<br/>param = {'n_estimators': [10, 100, 150],<br/> 'max_depth': [10, 30, 60]}<br/><br/>gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)<br/>gs_fit = gs.fit(X_features_df, y_amz_rev_part)</span></pre><p id="b301" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了查看结果，我们将创建一个由grid search(<em class="lk">' mean _ test _ score '</em>)提供的列排序的dataframe:</p><pre class="ks kt ku kv fd lm ln lo lp aw lq bi"><span id="4d6f" class="lr jp hh ln b fi ls lt l lu lv">pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]</span></pre><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/7ac2b1aa7dda8dce25b1bbdd4de0b447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fnK7vfg0bYQokgTpoqGbvg.png"/></div></div></figure><p id="1bfa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们看到了前5名，我们必须使用<strong class="ir hi">60深度</strong>的<strong class="ir hi"> 150估计量</strong>来获得我们模型的超参数的最佳组合。我们将用这些值重新生成RF:)</p><p id="76e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑到预测值可能较低，因为我们使用了原始数据集(<em class="lk">400万条评论</em>)的非常少的数据(<em class="lk">1.2万条</em>)。</p><p id="60bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，现在是时候让你用你的jupyter笔记本做你自己的探索了，探索这个迷人的AI主题的所有边缘。</p><p id="a025" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">像往常一样，github链接附有完整的<a class="ae jn" href="https://github.com/jrercoli/rf_amazon_reviews_part2" rel="noopener ugc nofollow" target="_blank">情绪分析jupyter nb </a>以便您可以自己验证代码。也请关注我的数据科学博客<a class="ae jn" href="https://rorjor.wixsite.com/empoweredatascience" rel="noopener ugc nofollow" target="_blank">赋能数据科学</a></p><p id="6524" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的评论和/或您的喜欢；)</p></div></div>    
</body>
</html>