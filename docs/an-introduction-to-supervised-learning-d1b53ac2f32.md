# 监督学习导论

> 原文：<https://medium.com/mlearning-ai/an-introduction-to-supervised-learning-d1b53ac2f32?source=collection_archive---------4----------------------->

监督学习是现代世界中最广泛使用的机器学习形式。在我的[上一篇文章](/@fenneccharles/how-do-machines-learn-c7fbe396f752)中，我们简要地谈到了什么是监督学习，我们如何在现实生活中使用它，以及它对什么样的项目最有用。在这一篇中，我们将深入研究各种类型的监督学习，以及每种学习的基本原理。虽然我们将深入研究，但每种类型的监督学习都足够复杂，我们将等待在涵盖每种类型的单独文章中深入研究数学和代码，并进入您可以在家跟随的项目。

## *我们知道监督学习使用带标签的数据来训练分类数据或预测结果的算法，但有哪些类型的监督学习呢？*

*监督学习可以分为两类问题，每类问题由几种不同用途的算法组成:*

> 分类

算法用于将数据分配到特定的类别中。它试图根据数据中的模式来推断数据点应该被分配到哪里。在现实生活中，分类的一个例子就是在你要洗的衣服中分类浅色和深色。机器学习中分类算法的一些例子是最近邻、线性分类器、决策树和随机森林。

> 回归

一种算法用来理解你的因变量(你试图预测的变量)和自变量(你怀疑有影响的变量)之间的关系。这种类型的机器学习通常用于预测商业、房价、股票市场等领域的未来。当我们在日常生活中做各种事情时，我们使用回归背后的概念，例如，当你正在学习烹饪新的东西时，每次你尝试烹饪它时，你可能会改变过程中的一个步骤，看看它是否会改善你的结果。这些变化，比如你做饭的时间，温度等等。，是你食谱中的自变量；而食物有多好吃是你的因变量。就像你可能想要的某种质地、味道、气味等等。，在你的食物中，回归问题也可以有多个因变量。不过，为了介绍回归，我们将重点预测单个因变量。

# 监督学习用的是什么算法？

*通常在 R、Python、Tableau、Excel 和 C 等程序中计算，让我们深入了解一下使用了哪些算法。我们不可能在一篇博客中涵盖所有的内容，但是我们会涉及到最常用的。*

## 分类算法:

> 朴素贝叶斯

这指的是基于应用[贝叶斯定理](https://en.wikipedia.org/wiki/Bayes%27_theorem)的一族[概率分类器](https://en.wikipedia.org/wiki/Probabilistic_classification)，其具有特征之间独立性的强假设(意味着特征不严重相关)。这些算法尤其适用于大型数据集，因为它们具有难以置信的可扩展性。

用简单的英语来说，贝叶斯定理的等式可以写成:

后验=先验*可能性/证据

后验概率表示将我们的新信息考虑在内的结果的概率。假设数据来自特定场景，后验概率等于我们之前对概率的信念乘以观察到的数据的概率。

这是高斯、多项式和伯努利模型所基于的定理，这些模型使用贝叶斯定理和其他假设，如标准分布来获得更好的分类。

如果你不太熟悉统计学，不要担心，当我们深入研究这些算法并做一个展示它们如何工作的项目时，你会更好地理解它们背后的统计学是如何使它们工作的。[如果你不想等，速成班有一个很棒的统计学系列，它会解释你需要知道的一切，不需要代码！](https://thecrashcourse.com/courses/statistics)

> 梯度下降

![](img/de1a29cfdf0d643776b04fa4ea8ab1d6.png)

Photo by [Flo Maderebner](https://www.pexels.com/@fmaderebner?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/two-man-hiking-on-snow-mountain-869258/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)

梯度下降也指一族算法。这些优化算法使用迭代方法在由负梯度定义的最陡下降方向上移动。

你可以把这想象成把一个徒步旅行者蒙上眼睛放在山顶上，并要求他们找到下山的路。他们会摸索着寻找最陡的斜坡，并利用他们以前在山上的位置来预测下一步的最佳位置。

在机器学习中，山是损失(对一个糟糕预测的惩罚)，步骤是每次迭代，你的学习率是每一步有多大。在每一次迭代中，如果损失变得更低，只要你的学习速度允许，你就继续朝那个方向前进。这意味着不用测试每一个点，你可以在优化下山路线时只测试几个点。有一些事情需要注意，比如把你的学习率设置得太高，直接跳过了你的目标最小损失，或者设置得太低，陷入了一个小陷阱，永远不会降到更低的水平。当我们深入研究如何使用梯度下降来编写和工作时，我们将讨论如何避免这些陷阱。

> k-最近邻(KNN)分类

![](img/478127be94ea74b119808cde68e48604.png)

[Antti Ajanki AnAj submission to Wikipedia](https://commons.wikimedia.org/wiki/File:KnnClassification.svg)

KNN 是一种非参数算法(这意味着它有助于解决不遵循规定模型(如线性或标准分布)的数据)，该算法根据数据点之间的距离来测量数据点的相似性，并使用该算法来确定将数据点放入哪个组。它可用于回归或分类。

K 最近邻中的 K 是数据集中要使用的最近样本的数量。简而言之，k-NN 测量您尝试分类的点与其最近邻点(k)的距离，并使用它与这些邻点的接近程度来决定新点的归属位置。

在上面的例子中，内圆的 k 值为 3，因为这个圆包含了最近的三个点。如果我们在我们的例子中使用三个 k，它将与红色三角形分组，因为附近的红色三角形比蓝色正方形多。我们示例中的下一个圆将代表 k 值为 5 的圆，它包括另外两个蓝色正方形，k 值为 5 的圆，我们的新点可以与蓝色正方形分组，因为现在附近的三角形比红色三角形多。

因为它使用您的要素来计算点之间的距离，所以不同比例的要素可能会影响 k-NN 算法，最好在使用 k-NN 算法之前对数据进行归一化。

> 决策树(分类树)

![](img/223358eb3b5cd279a9a54c9b57a3ee11.png)

[By Gilgoldm — Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=90405437)

决策树是一种使用树状结构的算法，其中每个主干(或内部节点)代表对属性的“测试”，每个分支代表测试的结果，每个叶子(或节点)代表一个标签(在计算所有属性后做出的决定)。从根到叶的路径代表分类规则。

传统上，决策树是手工制作的，但它们对于机器学习中的优化非常有效。决策树既可以用于分类，也可以用于回归，这两种技术之间几乎没有区别，还有几种集成方法可以将两者结合起来。

在进入机器学习之前，你可能已经在生活中使用过决策树，因为它们像标准的流程图一样工作，例如在社交媒体上传播的决策树，由 BuzzFeed 等公司创建。

![](img/43a792adebc801bb5c1e26d0ff6c945b.png)

在机器学习或我们的日常生活中使用这样的算法，就像其他优化算法一样，可以帮助我们缩短时间，让您走更短的路径来得出结论，并在此过程中需要做更少的测试。

> 随机森林

![](img/967633e86aaa08e59de2436f17ac00a6.png)

By [Venkata Jagannath](https://community.tibco.com/wiki/random-forest-template-tibco-spotfirer-wiki-page), [CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=68995764)

随机森林是一种用于回归、分类和其他机器学习任务的集成学习方法。集成方法使用多个机器学习算法来获得更好的预测性能。随机森林集合由多个决策树组成。

随机森林通过在训练时构建大量决策树来运行。对于随机森林分类任务，森林的输出是树最常预测的类别。随机森林往往优于单独的决策树，因为决策树容易过度拟合其数据，这使得预测新数据变得困难。

![](img/49ef67d991ecef9c6e9d78b707771827.png)

[https://blog.knockknockstuff.com/funny-flowcharts-consequential-dilemmas/](https://blog.knockknockstuff.com/funny-flowcharts-consequential-dilemmas/)

使用随机森林进行预测就像在不同时间记录你的性格流程图，并选择最频繁出现的一个作为最有可能的结果。如果你第一次得到的是“你不是养猫女士”，但接下来的四次是“你是养猫女士”，那么下一次就更有可能预测到“你是养猫女士”。

> 支持向量机(SVM)

![](img/d8fdcb4a8a8f837ef24bed1d55ee02c1.png)

[By Larhmam — Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=73710028)

使用支持向量机(SVM)算法时，每个要分类的对象都是 n 维空间中的一个点，该点的坐标就是您的特征。SVM 算法的工作原理是创建一个超平面(2d 中的一条线，或 3d 中的一个平面),该超平面将你想要分类的类别分开。可以有多个平面，但 SVM 将试图找到一个最好的分类与最大的差距(线和点之间的空间)。

在我们上面的例子中，一种简单的想象方式是，我们试图预测数据集中的生物是狗还是猫。我们的 Y 轴代表身高，X 轴代表体重。虽然一些狗在大小和重量上接近猫，但我们可以在两者之间画出一个非常明显的超平面，当我们找到最佳边界时，我们的超平面可以用来预测动物是狗还是猫，这取决于它落在超平面的哪一边。

SVM 使用了一个凸优化问题，该问题最大化了它的裕度，并具有约束条件，表明超平面的哪一侧对你的训练数据是正确的。[凸优化问题是目标函数是凸函数，可行集是凸集的优化问题。这背后的数学知识将会构成它的博客文章，但是现在，你可以使用几个库来实现 SVM 算法，而不一定需要理解它们背后的数学知识，只要你理解一般的概念。](https://en.wikipedia.org/wiki/Convex_optimization)

点离线越远，你的类别就越清晰，你的利润就越大。一般来说，在机器学习中，我们希望我们的类别尽可能地不同，因为你的类别越不同，你就越有信心说任何被预测在该类别中的点应该在那里。

> 逻辑回归

与线性回归类似，逻辑回归是对数据进行直线拟合以做出预测。在逻辑回归中，您预测某件事可能是真还是假，而不是使用连续数据。

这里，逻辑回归不是用一条线来拟合数据，而是使用一个从 0 到 1 的 S 形[‘逻辑函数’](https://en.wikipedia.org/wiki/Logistic_function)，给你一个点是真还是假的概率。

![](img/4d8291ee2d2842489d2e91bc35027d73.png)

[By Qef (talk) — Created from scratch with gnuplot, Public Domain](https://commons.wikimedia.org/w/index.php?curid=4310325)

例如，假设你试图预测一个孩子在 10 岁时的身高是否会长高。使用您的训练数据，逻辑函数将生成一条类似上面的线。这里，Y 轴是身高的概率，范围在 0 到 1 之间。X 轴是身高，所以上面这条线预示着一个身高与平均值的标准差为 0 的孩子有 50%的概率长高，而一个身高与平均值的标准差为 2 的孩子有 80%的概率长高。

在逻辑回归中，我们使用这些概率来进行分类，因此，一个有 80%的概率长高的孩子通常会被归类为高个子。这些模型可以变得更加复杂，使用更多的特征来进行预测。

对于我们的示例，这可能意味着添加由父代高度组成的新要素，然后使用所有 3 个高度来进行预测。就像线性回归一样，逻辑回归可以处理离散数据(如星座、流派、性格类型等标签)或连续数据(如身高、体重、日期等标签)。我们可以使用技术来计算我们的每个特征对最终结果的影响，以及我们的任何特征是否相关。

逻辑回归使用最大似然估计进行预测，最大似然估计是在给定一些观察数据的情况下预测分布参数的方法。本质上，该算法预测数据中每个点的概率，并通过它绘制一条线，然后移动该线，直到找到在给定数据的情况下出现概率最高的线。[如果你想在我深入研究逻辑回归之前了解更多关于最大似然的知识，可以看看 StatQuest 的视频，Josh 做了一项令人难以置信的工作，用非常容易理解的方式分解了数学背后的概念。](https://www.youtube.com/watch?v=XepXtl9YKwc)

> 神经网络

![](img/8e00bf547c85dc5f63d7f0c80da07edf.png)

[By User:Wiso — from en:Image:Neural network example.svg, vetorialization of en:Image:Neural network example.png, Public Domain](https://commons.wikimedia.org/w/index.php?curid=5084582)

神经网络是另一种集成算法，它试图通过模拟人脑的过程来识别数据中的潜在模式。

神经网络由我们称为神经元的层组成，第一层神经元是我们的输入层，它只是接收我们的输入。比方说，我们像弗兰克·罗森布拉特一样，试图教会计算机识别三角形。给定由 28×28 像素组成的图像，我们的神经网络将从由每个单独像素组成的输入层开始，这意味着我们从 784 个输入神经元开始。

每个输入像素将通过通道(我们图像中的箭头)到达下一层，这些通道具有指定的权重(和可能的偏差)，当它们在网络中移动时会改变它们。一旦它们进入隐藏层，除了权重和偏差之外，它们还会遇到激活函数，这些函数决定了我们网络中的每个单个神经元是否会被激活。激活的神经元通过通道向下一层传输数据，在遇到另一个激活函数之前，获得另一轮权重和偏差。这可以持续一段时间，多个激活函数做不同的工作来帮助我们的整体模型。这称为正向传播。

然后，当模型的输出层做出预测时，我们的模型会评估它相对于其标签的表现，以及哪些权重和偏差最有帮助，向后工作以撤销每一层所做的更改，以确定过程的哪一部分改变了我们的结果。这个回顾我们的数据以确定我们新的权重和偏差的过程被称为反向传播。

然后，我们的模型再次尝试这些新的权重和偏差，在每次迭代中学习。

在现实生活中，你的视网膜将这些像素作为视觉信息，然后它们被转化为电信号，在你的大脑中计算和传输信息。你大脑的不同部分会激活并帮助你在你正在看的东西和你之前看到并为你贴上标签的东西之间建立联系，类似于我们的神经网络，你会评估你是对还是错，并确定你是否需要对导致你误入歧途的内部想法做出任何改变。你的大脑是如何工作的一个很好的例子是像下面这样的图像，它显示了两个非常不同但看起来相似的东西。你的大脑可能不会立即识别任何给定照片属于哪个类别，但当你花一点时间在内部向前传播，并反向传播你接收到的信息时，你开始对哪张照片属于哪个类别做出更好的预测。

![](img/ee05867971d09511fc79e15bfd4fb713.png)

## 回归算法:

> 线性回归

![](img/f53795f5587182dd96a0f46ddbfa40b7.png)

[By Oleg Alexandrov — self-made with MATLAB, Public Domain](https://commons.wikimedia.org/w/index.php?curid=4099808)

当您有线性数据时，线性回归可以是帮助进行预测的一个很好的算法。线性回归采用直线的简单公式(y = mx+b ),并使用您的要素对其进行修改，以使您的数据尽可能地符合最小误差。在线性回归中，我们使用平方误差，它是从每个点到直线的所有平方距离的总和。误差平方最小的线是我们的最佳拟合线。使用这条线，我们可以对我们的独立变量进行预测，计算每个特征对我们的结果的影响程度，并计算 P 值以确定任何给定的特征是否具有统计显著性。

我们如何使用这个等式的一个例子是预测你在一篇流行的媒体文章上会有多少掌声。用简单的英语改写我们的等式，你期望的掌声数(y)等于你期望每个评论有多少掌声(m)乘以实际有多少评论(x)加上没有掌声的热门文章的预期评论数(b，也称为 y 截距)。这可以像上面的图一样可视化，其中 y 是你期望的掌声数，x 是评论数，穿过它们的线是它离每个点最近的线。

线性回归对非线性数据不是很有效，所以一定要事先画出你的数据，并确保在尝试这样做之前，它有一个可以用直线拟合的形状。它还可能在训练时对它认为特别重要的特征赋予很大的权重，这有时会导致在较小的数据集中过度拟合。

由于线性回归的局限性，对它进行了一些修改，使它在某些情况下做得更好。我将在下面讨论其中的一些，但是考虑到有多少，我肯定不会涉及到所有的问题。希望有了本文和以后关于回归的文章中提供的信息，您将有足够的背景知识来轻松理解任何新的回归概念。

自然，OLS(普通最小二乘法)线性回归只跟踪一个因变量(我们试图预测或理解的变量)，使用一个自变量(我们认为有影响的变量)。在机器学习中，我们通常使用多个自变量，将它们组合在一起形成*多元线性回归。*

> 决策图表

![](img/eba09dcb8c8780df48b3bbc3ac4d1a75.png)

Example of a Regression Tree from [StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)

回归树是一种决策树，其中每个叶子代表一个连续的数值，而不是真或假的概率。就像分类树一样，每个分支或初始节点都提出了一个真或假的问题，并且可以由分类数据和连续数据组成。

这对于非线性数据尤其有用，比如搅打奶油需要多长时间。太短的话，鲜奶油不够硬，不能保持它的形状，太长的话，你最终会变成黄油。另一个很好的例子是药物剂量，就像我们在上面的图片中看到的，过高或过低的剂量都不如中等剂量有效，回归树可以帮助我们快速有效地找出这一点。

> 支持向量回归

![](img/ae9d6247d4b5793eb8a408aa730a1a32.png)

[Example of Support Vector from AI Online Course](https://www.aionlinecourse.com/tutorial/machine-learning/support-vector-regression)

支持向量回归是 SVM 的对应物，适用于回归问题。SVR 承认数据中的非线性，并且仍然提供熟练的预测。在分类(SVM)中，我们的超平面是我们两个类别之间的分隔线，但是在回归(SVR)中，我们的超平面是我们用来预测连续输出的最佳拟合线。此外，SVR 试图在预定义或阈值误差值内拟合最佳拟合线，而其他线性回归模型试图最小化预测值和实际值之间的误差。

这是目前在机器学习中最不常用的回归形式之一，尽管它在高维数据中具有广泛的功能和易用性，这可能是因为它在大型数据集上工作得不是特别好，不提供对其输出的概率解释，并且可能很难理解。稍后我会深入探讨这个问题，但如果你想同时了解更多，我建议你看看这篇博文。

> 套索回归

![](img/ee61a80171de34cac4dba2c63ba5a4e3.png)

[By Nicoguaro — Own work, CC BY 4.0](https://commons.wikimedia.org/w/index.php?curid=58258966)

缩写“LASSO”代表 **L** 东**A**b**S**h**S**选举 **O** 操作员。套索回归是对使用收缩的线性回归的修改。收缩是指数据值向中心点收缩，就像平均值一样。套索回归惩罚权重(系数)的绝对值之和，也称为 L1 惩罚。这导致权重的绝对值普遍降低，因此我们不太可能因为几个非常重的权重而过度拟合，并且随着我们的许多权重被降低到零，我们可以去掉许多权重并简化我们的模型。这意味着套索回归特别适合处理数据中的高度多重共线性。

上图左边是套索回归，右边是岭回归。

> 里脊回归

岭回归是线性回归的一种改进，它对权重(系数)的平方和进行惩罚，也称为 L2 惩罚。这往往会使权重的绝对值变小，并对异常值造成更大的不利影响，从而使权重的分布更加均匀。

与 Lasso 回归一样，岭回归也是一种收缩估计量，从理论上讲，它可以产生更接近“真实”总体参数的新估计量。与套索回归(L1 正则化)不同，岭回归(L2 正则化)将*而不是*产生简化的模型。这是因为所有权重都以相同的因子收缩，这不会消除任何因子，从而允许您的模型保持复杂，同时有助于控制多重共线性。

> 随机森林

随机森林回归是一种监督学习算法，它使用集成学习算法来组合来自多个机器学习算法的预测，以做出比单个模型更准确的预测，在这种情况下，多个算法是决策树。

随机森林分类算法之间的唯一真正区别是您希望从中获得的输出，随机森林分类器处理具有离散标签的数据(如您是否有可能获得贷款批准、进入学校等)，而随机森林回归处理具有数字或连续输出的数据(房价、股票价格等)。

> 高斯过程回归

高斯过程回归是一种基于由先验协方差(贝叶斯统计)控制的高斯过程的插值方法。在适当的先验假设下，高斯过程回归可以在未采样位置给出最佳线性无偏预测。GPR 不是计算特定函数参数的概率分布，而是计算符合数据的所有容许函数的概率分布。

> 稳健回归(RANSAC)

稳健的回归方法被设计成不会受到基础数据生成过程违反假设的过度影响。

应该考虑稳健估计的一个例子是，当非常怀疑异方差时，即如果模型的所有随机变量不具有相同的有限方差。[异方差的存在是回归分析和方差分析中的一个主要问题，因为它使假设建模误差都具有相同方差的显著性统计检验无效。](https://en.wikipedia.org/wiki/Heteroscedasticity)

使用稳健估计的另一种常见情况是当数据包含异常值时。在存在异常值的情况下，OLS 估计是低效的，并且可能是有偏差的。

> k 近邻(KNN)回归

正如我们之前了解到的，KNN 是一种非参数算法，它使用数据集中各点之间的距离来确定要将数据放入的组。

KNN 回归使用与 KNN 分类相同的算法，但是 KNN 回归试图通过使用局部平均值来预测输出变量的值，而 KNN 分类试图通过计算局部概率来预测输出变量所属的组

## 监督学习的好处是什么？

*   监督学习可以直观地理解机器是如何学习的。
*   你可以在培训前搞清楚到底有多少节课。
*   您可以通过一种具有完美决策边界的方式来训练分类器，以准确区分不同的类。
*   整个训练完成后，不一定需要把训练数据保存在内存里。相反，您可以将决策边界保留为数学公式。

## 有哪些挑战？

*   需要大量的数据，并且由于[维数灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)，计算量和时间都很大。
*   数据集中的人为错误会产生很大的影响，因为许多形式的监督学习对异常值很敏感。
*   不能自己学习，不像无监督学习。

虽然这篇文章有点晦涩，但我希望你已经开始理解这些算法是如何工作的，并以一种更容易理解的方式深入研究它们。接下来，我们将深入研究无监督和半监督学习背后的算法。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)