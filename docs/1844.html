<html>
<head>
<title>Machine Learning Cheat Sheet for Data Scientist Interview : Linear Regression Frequently Asked Questions Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家面试的机器学习备忘单:线性回归常见问题第1部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/machine-learning-cheat-sheet-for-data-scientist-interview-linear-regression-frequently-asked-998d99920993?source=collection_archive---------4-----------------------#2022-02-03">https://medium.com/mlearning-ai/machine-learning-cheat-sheet-for-data-scientist-interview-linear-regression-frequently-asked-998d99920993?source=collection_archive---------4-----------------------#2022-02-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="755c" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">这里总结了在数据科学家面试中经常被问到的ML问题。我已经尽可能保持备忘单的简洁。所有的内容都是必须记住的关键知识。希望可以作为你DS面试前的准备指南！</p></blockquote><h1 id="93ec" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated"><strong class="ak">损失函数</strong></h1><p id="e324" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">定义:衡量预测结果和实际结果之间的差距。</p><p id="7277" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">常见的损失函数类型</strong></p><p id="3cbe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">线性回归:平方损失(误差平方和)</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es km"><img src="../Images/798111346cb81b9ebdeee99f5faab22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*9BMNhPfFMA_u8r32TcF-DQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">sum of squared errors’s formula</figcaption></figure><p id="f4d1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">分类(如逻辑回归、决策树):对数损失/交叉熵损失</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ky"><img src="../Images/b68ff8659ca3bed7bddf912aa917b8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*eEPhu7OQhBdefU8t6y6I3w.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">cross-entropy loss’s formula</figcaption></figure><p id="4a37" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">软边缘SVM:铰链损失</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es kz"><img src="../Images/9c43203d04fb8db23911aa268dc36ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*vVwx4chfKRSA2JAhk3bFbQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">hinge loss’s formula</figcaption></figure><h1 id="d529" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">梯度下降</h1><ul class=""><li id="dd8e" class="la lb hh ik b il ke ip kf kg lc ki ld kk le jf lf lg lh li bi translated">优化目标函数(最小化损失函数)。</li><li id="016e" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">寻找局部最优，只有当目标函数是凸的时才能找到全局最优。</li></ul><p id="9452" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">步骤:</p><p id="8dcd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">1.从初步猜测开始。</p><p id="bd6f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">2.每次使用梯度的负方向(偏导数)减少一点目标函数。</p><p id="c0fd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">3.停止直到收敛到局部最优。</p><p id="6b5d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">学习速率</strong>:控制我们在梯度下降中的步长。</p><ul class=""><li id="2c7d" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">如果学习率太小:收敛慢</li><li id="758c" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">如果学习率太大:超过最优值，无法收敛</li><li id="4b79" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">选择自适应学习速率:在接近局部最小值时采取较小的步长</li><li id="64b2" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated"><strong class="ik hi">同时</strong>更新所有参数</li></ul><p id="fa32" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">注意:线性回归的成本函数总是一个<strong class="ik hi">凸函数</strong>——总是有一个最小值。</p><p id="ebb7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">梯度下降、随机梯度下降和小批量梯度下降</strong></p><p id="92b3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">梯度下降:在一次迭代中使用所有数据点，并计算其梯度以更新参数。</p><ul class=""><li id="b06d" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">特别适用于凸函数</li></ul><p id="1f9c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">随机梯度下降:在一次迭代中使用一个随机数据点</p><ul class=""><li id="cf4b" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">收敛更快</li><li id="b517" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">降低陷入局部最优的可能性</li></ul><p id="79fe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">小批量GD:在一次迭代中使用小批量的数据点</p><ul class=""><li id="73e9" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">比SGD更有效，因为我们可以使用矢量化实现</li></ul><p id="42e0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">梯度下降的特征缩放</strong></p><ul class=""><li id="4ed3" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">特征缩放加速了梯度下降法的收敛。</li><li id="601a" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">几何解释:梯度下降方向垂直于等高线。当我们在梯度下降之前不实施特征缩放时，轮廓线的形状是椭圆形的。它不一定指向最优，因此导致收敛较慢。</li></ul><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lr"><img src="../Images/c186d47abb3994c1a7d6448f9d2bebcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*3NDul_LHs4H6GIzuYgeDIA.png"/></div></figure><h1 id="2fce" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">正规方程</h1><p id="e337" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">线性回归的封闭形式解(矩阵形式)</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ls"><img src="../Images/942dbcfe00bf23839d75e493330fb2c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*8OO0CY341VjFn9NIU2Ut3A.png"/></div></figure><h1 id="2ec9" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">梯度下降与正常方程</h1><ul class=""><li id="df6c" class="la lb hh ik b il ke ip kf kg lc ki ld kk le jf lf lg lh li bi translated">当样本大小n非常大时，梯度下降效果很好。</li><li id="4e6f" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">正规方程需要计算(XᵀX)⁻，时间复杂度为O(n))。当矩阵不可逆时会出现一个问题。如果不可逆→没有唯一解，用梯度下降代替。</li><li id="c651" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">不可逆性的原因:共线性。要素的数量大于数据点的数量(改用正则化)</li></ul><h1 id="97bf" class="jg jh hh bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">逻辑回归</h1><p id="bd4e" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is kg kh iv iw ki kj iz ja kk kl jd je jf ha bi translated">广义线性模型。目标变量是一个二元变量。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lt"><img src="../Images/9aa6eb8a832b30a4a6c093e5dc64915d.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*QSJtj6vZ2lF_ft4kicny4g.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">f(x) is the estimated probability of y=1 given x</figcaption></figure><p id="76e7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">s形函数</strong></p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lu"><img src="../Images/c0ce4467512e77377db136683e3261d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*n02plOyx6ByRaggEGwKbBw.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">The function crosses 0.5 at the origin and then flattens out, between 0 and 1</figcaption></figure><p id="bcb1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">系数解释</strong></p><p id="65a3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">当某个变量增加一个单位，而其他变量保持不变时，出现结果的概率对数的预期变化。</p><p id="e595" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">损失函数</strong></p><p id="5464" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">如果我们使用传统的平方损失，由于增加了sigmoid函数，它是非凸函数。因此我们可能找不到全局最优解。</p><p id="737f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">二元逻辑回归的损失函数(凸函数)</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lv"><img src="../Images/b1f0e96a70bef38dc4c3a8f249ec9b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*KW-AgaoNPyJAMvOipaz6ag.png"/></div></figure><ul class=""><li id="b45e" class="la lb hh ik b il im ip iq kg lo ki lp kk lq jf lf lg lh li bi translated">它可以使用最大似然估计(MLE)来导出。</li><li id="0745" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">没有封闭形式的解。我们需要使用梯度下降法来寻找最优解。</li><li id="8e58" class="la lb hh ik b il lj ip lk kg ll ki lm kk ln jf lf lg lh li bi translated">梯度下降中的高级优化:BFGS算法，通常比传统的梯度下降更快。</li></ul><p id="2bee" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated"><strong class="ik hi">多类分类问题</strong></p><p id="dcb8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kg iu iv iw ki iy iz ja kk jc jd je jf ha bi translated">使用一个对所有分类，建立多个二元分类模型。</p><div class="lw lx ez fb ly lz"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ma ab dw"><div class="mb ab mc cl cj md"><h2 class="bd hi fi z dy me ea eb mf ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mg l"><h3 class="bd b fi z dy me ea eb mf ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mh l"><p class="bd b fp z dy me ea eb mf ed ef dx translated">medium.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div></div></div>    
</body>
</html>