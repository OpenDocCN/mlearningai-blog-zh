<html>
<head>
<title>Resistance, Accommodation and the Threat of A.I. Algocracy by John Danaher</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">约翰·丹纳赫的《抵抗、适应和人工智能统治的威胁》</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/resistance-accommodation-and-the-threat-of-a-i-algocracy-by-john-danaher-2abf9fed85bf?source=collection_archive---------0-----------------------#2021-01-08">https://medium.com/mlearning-ai/resistance-accommodation-and-the-threat-of-a-i-algocracy-by-john-danaher-2abf9fed85bf?source=collection_archive---------0-----------------------#2021-01-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/c577c756f7052d1be61cb58a250b48d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/0*jXOs7Ic5EbSyO-dR.jpg"/></div></figure><p id="1839" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这篇文章中:'<em class="jj">算法统治的威胁:现实、抵抗和适应'，</em>约翰·丹纳赫讨论了<em class="jj">算法统治的问题。</em>算法治理指的是不断增加的<em class="jj">算法化</em>，即在解决行政问题和修改人类决策中不断增加算法的使用。Danaher担心法律和政治形式与算法系统的这种融合，除了隐私和个人数据的问题，会产生所谓的“算法统治的威胁”，这引起了一些严重的道德问题。在更详细地定义了威胁之后，丹纳赫提出了两种可能的解决方案:<em class="jj">【抵抗】</em>和<em class="jj">【适应】。</em></p><p id="fd55" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从交易股票和揭露逃税，到参与科学发现和寻找约会对象，随着数据革命和物联网的发展，算法决策现在已经成为我们环境中的一个自然元素。这种趋势只会增长。在人类生活受到影响的地方依赖自动化在道德上可以接受吗？鉴于算法对我们大多数人来说要么是看不见的，要么是不可理解的，我们怎么能相信它们的合法性，即使(或许更是如此)我们完全相信它们的效率？</p><p id="b12e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">丹纳赫借鉴了大卫·埃斯特伦德的工作和他的知识统治的概念来阐述算法统治的问题。根据Danaher，j .:算法政治是<em class="jj">“一种基于算法的系统构建和约束人类参与和理解公共决策的机会的情况”</em> (2016)。一个恰当的例子是，在决策中，为了整体效率而牺牲了人的能动性。</p><p id="3ec3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从一开始就需要做一个重要的区分。丹纳赫并不是在谈论一个潜在的接管整个人类文明的人工智能已经失控。这不是另一篇关于未来由机器统治的反乌托邦社会的危险的论文。他指的是当前社会领域算法官僚化的一个具体问题。</p><p id="3647" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">也就是说，算法政治不同于其他形式的约束系统。市场是一个根据价格运动来约束人类行为的系统，典型的官僚体制是一个通过法律形式来约束人类行为的系统，而算法政治是一个通过计算机算法来约束人类的系统。这些系统经常交叉和重叠。他们之间并不总是有明确的界限。</p><p id="b5d8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">有许多类型的算法系统。Danaher选择关注他认为最具侵入性的类型:用于预测性和描述性数据挖掘实践的算法。Danaher借鉴了Zarsky对数据挖掘的定义:“<em class="jj">在数据中识别有效、新颖、潜在有用且最终可理解的模式的非平凡过程”</em> (Zarsky 2011，291)。大数据用于监控、预测，有时甚至煽动和控制人类行为。</p><p id="d981" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在算法系统的使用中，有不同层次的自动化，类似地，也有不同层次的人类参与。Danaher用军用无人机的例子来说明一个相关的伦理问题。</p><p id="ae14" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">有三种不同类型的机器人武器系统:</p><p id="53ee" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">人在回路武器</strong>:没有人的同意，机器不会执行命令。</p><p id="a1a6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">人在回路武器</strong>:机器可以决定并执行暴力攻击，但人类可以随时拦截并超越系统。</p><p id="e916" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">人类之外的武器:机器被放任自流。它可以选择、决定和执行，而不需要人工干预。</p><p id="0db2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">同样的区别适用于所有算法系统，包括数据挖掘软件。一些数据挖掘系统比其他系统更容易被人为修改和覆盖。这些被称为<em class="jj">可解释</em>和<em class="jj">不可解释</em>系统。</p><p id="b761" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如前所述，算法统治的威胁可以归结为关于算法治理和算法系统的两类问题:</p><p id="d311" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">隐藏问题</strong>和<strong class="in hi">不透明问题</strong>。第一个问题应该由<a class="ae jk" href="https://developers.google.com/admob/android/eu-consent#:~:text=Under%20the%20Google%20EU%20User%20Consent%20Policy%2C%20you,personal%20data%20%28such%20as%20AdID%29%20to%20serve%20ads." rel="noopener ugc nofollow" target="_blank">谷歌欧盟用户同意政策</a>解决，至少对欧洲国家是这样。事实上，隐藏的问题在某种程度上得到了解决，但只是被一个新的问题所取代，我称之为<strong class="in hi"><em class="jj"/></strong>强制同意的问题，在这种情况下，我们别无选择，只能同意隐私政策，因为我们的雇主或教育机构要求我们这样做。</p><p id="1f12" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第二个问题，不透明的问题，表明即使我们知道“我们的数据是如何使用的”，我们也无法在技术层面上理解它。让我们对其潜在的影响束手无策。事实证明，对不透明性的关注与解释独裁统治的威胁特别相关。强调这种关注的重要性的原因之一是，它不属于现成的数据类别-隐私和个人信息问题，意思是；更多的机构可以逃脱未能解决它。</p><p id="e2a9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果决策过程太不透明，他们就不再具有对我们的生活行使权力所需的合法性。什么使得决策程序合法？工具主义认为，如果决策过程达到了预期的结果，那么它就是合法的。这是一种对决策过程持结果论的方式。<em class="jj">程序主义者认为透明度比效率更重要。换句话说，有一个低效的算法要比只有少数人能理解的决策过程好得多。即最好有一个有缺陷的系统，而不是一个强制性的系统。</em></p><p id="45a7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">丹纳赫喜欢“混合”方法，这意味着他的目标是两全其美。这确实是非常直观的，如果不是非常明显的话；如果我们极度赞同这两个阵营中的任何一个，那么整个决策的想法就会崩溃。一个决策系统简单易懂，但却不能做出有效的决策并履行其职能，这有什么意义呢？另一方面，一个让大多数人无依无靠的完美高效的系统将是纳粹德国的现代替代方案。极端工具主义的一个不太夸张的版本是我们已经提到过的<em class="jj">知识论</em>的概念，它是由大卫·埃斯特伦德首先提出的。"<em class="jj">这些系统有利于一小部分认知精英，而不是更广泛的公众"</em> (Danaher，J. 2016) <em class="jj">。</em></p><p id="6a58" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这种工具主义的危险在于，它听起来很有说服力。这与精英主义的现代形式或者我们可以称之为权力的<strong class="in hi"> <em class="jj">专业化</em> </strong>非常吻合。人们往往对决策的不透明相当宽容，只要他们“知道”负责的人不是泛泛的官僚或政客，而是“有能力的专家”。工具主义者观点的核心始终保持不变:合法性是由结果赋予的。</p><p id="da3a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，知识统治意味着由精英专业人士的特权子群体垄断决策权。另一方面，算法政治是集中权力的一个类似例子，但在这种情况下，认知精英是一个人工智能。引用Danaher的话:<em class="jj">“因此，当我谈论独裁统治的威胁时，我指的是这种有利于独裁统治系统的认知所产生的威胁”</em> (Danaher，J. 2016)。</p><p id="1a48" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在很大程度上，合法性问题可以归结为不透明性问题，而不透明性又是人类参与算法决策的问题。系统越复杂，它就变得越不人性化或越难解释，最终导致算法统治。</p><p id="c5a4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">创建更加透明、简单和可解释的系统的问题既是技术性的，也是体制性的。企业和政府权力基本上是通过制度化的保密来运作的。交易秘密和政治秘密受到算法加密的保护。他们不希望有人侵入他们的数据。人类自由的问题再一次与算法的政治经济紧密交织在一起。与丹纳赫的观点不同，我认为数据挖掘软件只是经济治理的最后一句话；人类生活的算法化和身体的温顺、有利可图和易于管理。</p><p id="833d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这个问题的技术方面在于数据挖掘系统需要消化的数据集的庞大规模。需要处理的数据量永远不可能仅靠人类的努力来管理，这就是为什么我们必须依靠机器学习来完成这项工作。但是，将人类生活、自然和整个生物圈转变为可管理数据的常设储备的需要，甚至连丹纳赫似乎都认为是理所当然的。同样，这些都是由人为创造的需求导致的伪问题。这种需求是我们通过各种现有的自由经济整体系统所固有的管理技术而被训练出来的。这里指的是消费主义。</p><p id="02b8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们应该抵制独裁统治的威胁吗？怎么会？一种选择是破坏制度，通过革命实践解放自己。丹纳赫强烈反对。毫不奇怪，出于同样的原因，他“未能”解决挖掘大型数据集问题的根源——资本的逻辑。在Danaher看来，算法决策的好处超过了权衡<em class="jj">至少在某种程度上直接抵制是错误的做法</em>。它会废除太多。比如可持续能源管理，它主要由算法技术控制。</p><p id="bffb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">丹纳赫的亲算法的论点并不严格以结果为导向和工具。就支持算法治理的程序性论点而言，Danaher揭示了在算法系统中隐含偏见的不可能性。一个例子包括作为反恐措施的貌相做法。一种算法更有可能保持客观，而不会陷入民族或种族偏见。同样，我想在丹纳赫的观点和我自己的观点之间制造一些距离。恐怖主义的想法在很大程度上是对美国扩张主义帝国议程的强大意识形态补充。在确保德里达所说的<em class="jj">有组织的恐怖</em>的安全方面，它起着不可或缺的作用。因此，一旦偏见问题得到解决，貌相问题就没有解决(后者显然使问题变得更糟)，事实上，貌相是否是一项正当的警察措施，它是否揭露或实际上是构建和发明恐怖主义，才是真正的问题。再说一次，算法治理只是本质上治理问题的另一种表达方式。</p><p id="e23a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对独裁统治威胁的第二个“解决方案”是<em class="jj">和解。</em>简单地说，适应意味着部分算法决策。每个数据驱动的流程都需要考虑到人工干预和控制。“<em class="jj">这将避免不透明的问题，同时仍允许我们获得算法系统的好处。”</em> (Danaher，J. 2016) Danaher继续为我们提供四种可能的算法容纳方式。</p><ol class=""><li id="4aa9" class="jl jm hh in b io ip is it iw jn ja jo je jp ji jq jr js jt bi translated"><strong class="in hi">“坚持人工审核算法”</strong></li></ol><p id="179d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第一个解决方案位于抵抗和适应之间的边界。它依赖于确保对个人生活有影响的每一个数据驱动的过程都应该受到人类的密切监视和控制。欧盟已经在<em class="jj">数据保护指令下实施了该法律的一个版本:</em></p><blockquote class="ju"><p id="5f98" class="jv jw hh bd jx jy jz ka kb kc kd ji dx translated">15.1 —成员国应赋予每个人不受制于某项决定的权利，该决定对其产生法律效力或对其产生重大影响，且该决定完全基于旨在评估与其相关的某些个人方面(如其工作表现、信誉、可靠性、行为等)的自动化数据处理。</p></blockquote><p id="4600" class="pw-post-body-paragraph il im hh in b io ke iq ir is kf iu iv iw kg iy iz ja kh jc jd je ki jg jh ji ha bi translated">这里的问题是，该法律涉及欧盟的适当居民，这在人权方面造成了一个盲点，国际学生和移民不在该法律的保护范围内，导致他们受到不断的监视和经济局的算法排斥。</p><p id="2419" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">更有甚者，Danaher认为<em class="jj">人工审查</em>的解决方案是无用的，因为它没有解决真正的问题。的确如此，错误的解决方案只会将决策权从公正的算法转移到部分专家手中，将我们带回到知识统治的问题上。</p><p id="ea19" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 2。“人类的认知增强”</strong></p><p id="f989" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">根据Danaher的说法，人类的提高可以用来改善公共决策。同样的技术能被用来遏制独裁统治的威胁吗？让我们引用Danaher的话来定义<em class="jj">认知增强:“任何旨在改善或增加人类用于获取知识的能力的生物医学干预，包括理论和实践/道德”</em> (Danaher 2013，88)。鉴于平等获得这些技术的问题可以解决，这将阻止知识精英的崛起。但是还不清楚平等机会是否有可能。</p><p id="07d9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">通过认知增强进行调节的论点失败了，因为算法的计算能力与增强或不增强的人类潜力之间存在差异。人类能力的提高不可能达到数据挖掘系统的计算水平。事实上，平等获得人类进步可以消除人类精英和普通大众之间的差异，但这只能解决上位治理的问题。</p><p id="4e16" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是丹纳赫的解决方案。如果人工增强与人工审查(第一种解决方案)结合使用，那么认知能力的算法危险将得到解决。信息权力将不再从算法转移到精英阶层，取而代之的是，它将被增强的公民所掌握。</p><p id="dc80" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里显而易见的问题是，我们似乎没有合适的增强技术来完成这一壮举。第二，即使有了现成的技术，我们是否有资源让所有人都可以公开使用，这一点也不清楚。</p><p id="c67e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 3。“拥抱监控技术”</strong></p><p id="5b62" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">监视是一种颠倒监视中固有的权力关系的艺术。这是“自下而上进行监视”的艺术。可以这么说，调查测量员。"<em class="jj">如果大数据算法的问题是经济和政治精英对我们活动的持续监控和监视，那么解决方案就是将监控技术重新转向那些经济和政治精英"</em> (Danaher，J. 2016)。监控技术加上隐私法应提供适当的资源，以增强公共决策的权力和合法性，因为监控和监控技术都依赖于算法。</p><p id="1b95" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">根据丹纳赫的说法，这也行不通。与监视技术的单纯数据收集设备不同，算法政治意味着一种特殊的、以目标为导向的数据收集，它解释了权力关系中的认知不对称。监督无法弥补这一点。尤其是考虑到复杂性是在软件层面而不是在人的层面引入的这一事实。没有直接的人类对抗可以解决这个问题。</p><p id="7c79" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> 4。“与算法结成个人伙伴关系”</strong></p><p id="0bb9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第四个住宿解决方案应该解决这一复杂性。Danaher解释了两种可能的方法，以合法的决策程序的方式将人类与算法结合起来。第一个是<em class="jj">非整合</em>，这意味着在人类有机体和算法系统之间没有直接的生物整合。第二个是<em class="jj">综合</em>，意味着算法和人类之间的直接统一。前者显然在近期更可行，后者更多的是对未来可能性的铺垫。</p><p id="688c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">非一体化解决方案只是对sousveillance解决方案的轻微补充。在数据监控软件之上，非集成解决方案还在流程中添加了数据挖掘算法。一个个性化的人工智能作为对抗现状的一种形式。这实际上就像拥有了自己的私人人工智能助手，它可以帮助你理解和理解影响你生活的其他算法过程。这个想法是，这是授权，因为你不再需要为了理解正在发生的事情而遵从认知精英。有一个完整的运动致力于人机联盟，称为<em class="jj">“量化自我”</em>运动。参见<em class="jj"> Thompson，2013 </em>了解更多关于量化自我运动的信息。</p><p id="a270" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">非整合方法的问题在于，算法数据挖掘软件的实施只有在大数据环境下才有意义。但是围绕个人的数据量并不庞大，因此它无法解决生物信息鸿沟的问题。更不用说更明显的威胁，通过与算法技术形成上述联盟，我们可能事实上通过参与或成为算法统治的被动接受者来加速算法统治的到来。我们可能确切地知道我们的数据是如何被使用和收集的，而无需实际参与决策过程。</p><p id="aa0f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">综合方法的问题是一个简单的事实，即它是高度投机性的。我们甚至不知道我们是否能够拥有这种技术，更不用说确保所有人平等获得和安全。另一个更微妙的问题涉及到人类的能动性。一个拥有科技人工制品的直接生物联盟能够保护人类的能动性，更不用说赋予它权力了吗？谁知道在一个人能够在这个整合水平上有意义地行动之前，他还必须满足什么条件？无数的问题浮出水面。</p><p id="8bae" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">参考</strong></p><ol class=""><li id="ae08" class="jl jm hh in b io ip is it iw jn ja jo je jp ji jq jr js jt bi translated">Danaher，J. (2016年)。算法统治的威胁:现实、抵抗和适应。<em class="jj">哲学&amp;技术</em>，<em class="jj"> 29 </em> (3)，245–268。</li><li id="4406" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">j .德里达(2003年)。真实的和象征性的自杀:与雅克·德里达的对话。恐怖时期的哲学:与尤尔根·哈贝马斯和雅克·德里达的对话。</li><li id="ee5f" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">埃斯特隆德博士(1993年)。让民主的真相变得安全。在d .科普，j .汉普顿和j .罗默(编辑。)，民主的理念。剑桥:剑桥大学出版社。</li><li id="571d" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">埃斯特隆德博士(2003年)。为什么不是知识统治？在内奥米·雷肖特科(编辑)的《欲望、身份和存在:纪念彭纳随笔》中。学术印刷和出版</li><li id="4797" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">埃斯特隆德博士(2008年)。民主权威。普林斯顿:普林斯顿大学出版社。</li><li id="ba61" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">c .汤普森(2013年)。比你想象的更聪明:技术如何让我们的思想变得更好。伦敦:威廉·柯林斯。</li><li id="37c5" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">茨韦塔纳·扎尔斯基(2011)。政府数据挖掘及其替代方案。宾州法律评论，116，285页。</li><li id="e59c" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">茨韦塔纳·扎尔斯基(2012)。自动预测:感知、法律和政策。美国计算机学会的通讯，15(9)，33-35。</li><li id="2f90" class="jl jm hh in b io kj is kk iw kl ja km je kn ji jq jr js jt bi translated">茨韦塔纳·扎尔斯基(2013)。透明预测。伊利诺伊大学法律评论，41504年。</li></ol></div></div>    
</body>
</html>