<html>
<head>
<title>All About Activation Functions In Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/all-about-activation-functions-in-neural-networks-f2a1889f59f1?source=collection_archive---------2-----------------------#2021-03-02">https://medium.com/mlearning-ai/all-about-activation-functions-in-neural-networks-f2a1889f59f1?source=collection_archive---------2-----------------------#2021-03-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/43fd281440048bad03e149efc5d42071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*9ruSPXn0nADznzPe.jpeg"/></div></figure><p id="2c88" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi jj translated">神经网络激活功能是深度学习的重要组成部分。激活函数决定了深度学习模型的输出，其准确性，以及训练模型的计算效率——这可以成就或摧毁一个大规模的神经网络。激活函数对神经网络的收敛能力和收敛速度也有很大影响，或者在某些情况下，激活函数可能会首先阻止神经网络收敛。</p><h1 id="5dec" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">一个简单的人工神经元:</h1><p id="427d" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">深度学习模型通常由许多分层堆叠的神经元组成。为了简单起见，让我们考虑单个神经元。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/048c119bb718f6345fed22497fbe386a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*YwZ8NnvJBNN1WXry"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Single-layer Perceptron</figcaption></figure><p id="4fc8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">神经元执行的操作基本上包括线性的乘法和加法操作，并产生输出。在这之后，一个激活函数被应用于产生神经元的最终输出。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es le"><img src="../Images/96780d49f17e1c9fed9910e52fb92473.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/0*o9kf9sedlBtZ_Yme"/></div></figure><p id="b141" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果不应用激活函数，上面的函数就像一个线性函数，将输入映射到输出。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/6c14f95491a413c314b5582617b80d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/0*w-fl_PEFhObrAgIm"/></div></figure><p id="f5c6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这使得神经元只能近似线性函数。因此，模型无法识别数据中的复杂模式。</p><h1 id="e35e" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">为什么需要激活功能？</strong></h1><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lg"><img src="../Images/ecb63b70b9775e4afe27d76a15443f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*7OhEUjeX9Iq7W-a3.jpeg"/></div></figure><p id="a0cb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为了使神经网络逼近非线性或复杂函数，必须有一种方法将非线性属性添加到结果的计算中。</p><p id="043a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在神经网络中，称为输入的数字数据点被输入到输入层的神经元中。每个神经元都有一个权重，输入数乘以权重就得到神经元的输出，输出传递到下一层。</p><p id="73ca" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">激活函数是馈送当前神经元的输入和去往下一层的输出之间的数学“门”。它可以像阶跃函数一样简单，根据规则或阈值打开或关闭神经元输出。或者它可以是将输入信号映射成神经网络运行所需的输出信号的变换。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/a5cba0e7cc86c7e90ed07e998bf51358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AsCuasIivJoxArIk"/></div></div></figure><p id="254d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">神经网络越来越多地使用非线性激活函数，这可以帮助网络学习复杂的数据，计算和学习几乎任何代表问题的函数，并提供准确的预测。</p><h1 id="b248" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">A.任何非线性函数都可以用作激活函数吗？</h1><p id="41ad" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">不，在函数可以被认为是深度学习模型的良好候选之前，它应该具有以下属性:</p><ol class=""><li id="b742" class="lm ln hh in b io ip is it iw lo ja lp je lq ji lr ls lt lu bi translated"><strong class="in hi">非线性</strong>:需要在模型中引入非线性。</li><li id="659c" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji lr ls lt lu bi translated"><strong class="in hi">单调</strong>:要么完全不增，要么完全不减的函数。</li><li id="fde3" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji lr ls lt lu bi translated"><strong class="in hi">可微分</strong>:深度学习算法通过一种叫做<a class="ae ma" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>的算法来更新它们的权重。当使用的激活函数是可微的时，该算法可以工作。即它的导数是可以计算的。</li></ol><h1 id="5262" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">B.激活功能的理想特征:</h1><h1 id="4e27" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 1。消失渐变问题:</strong></h1><p id="ee01" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">当我们将神经网络训练得非常深(成百上千层)时，就会出现这个问题。使用过程梯度下降来训练神经网络。梯度下降包括反向传播步骤，该步骤基本上是获得权重变化的链式法则，以减少每个时期后的损失。考虑一下，一个4层神经网络由4个神经元组成，分别用于<strong class="in hi">输入层</strong>，4个神经元用于<strong class="in hi">隐藏层，</strong>和1个神经元用于<strong class="in hi">输出层</strong>。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/9936bbb2f1b12fa17a2fc823dfe745b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UvSqTK0PSSYm3A1L"/></div><figcaption class="la lb et er es lc ld bd b be z dx">4- layer Neural Network</figcaption></figure><h2 id="5e4d" class="mc jt hh bd ju md me mf jy mg mh mi kc iw mj mk kg ja ml mm kk je mn mo ko mp bi translated">输入层</h2><p id="208d" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated"><strong class="in hi">紫色</strong>的神经元代表输入数据。这些可以像标量一样简单，也可以像向量或多维矩阵一样复杂。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/0995fa75a09efbb8a8782e0ef5ef8a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/0*8wi8I61Qv8c--UZA"/></div></figure><h2 id="97e8" class="mc jt hh bd ju md me mf jy mg mh mi kc iw mj mk kg ja ml mm kk je mn mo ko mp bi translated">隐藏层</h2><p id="c442" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">使用<em class="mr"> z^l </em> —层<em class="mr"> l </em>中的加权输入和<em class="mr"> a^l </em> —层<em class="mr"> l </em>中的激活，计算隐藏神经元的最终值，用<strong class="in hi">绿色</strong>、<strong class="in hi"> </strong>表示。对于第2层和第3层，等式为:</p><ul class=""><li id="39aa" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><em class="mr"> l = 2 </em></li></ul><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/7d50dac97edb806d939304ab7c257b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/0*iOm54PjWWKj2P1bj"/></div></figure><ul class=""><li id="a020" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><em class="mr"> l = 3 </em></li></ul><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/67a5b9525cf9dd364f643bab5b86063a.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/0*_G6ZGqYS9_eFPxak"/></div></figure><p id="8ae6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="mr"> W </em>和<em class="mr"> W </em>是层2和层3中的权重，而b和b是这些层中的偏差。</p><p id="9956" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">同时反向传播和计算梯度以更新权重<em class="mr"> W，</em></p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mv"><img src="../Images/ceddf30eca0a1732d9789a24b575bfce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QJQfyDcFhKrukLgi"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx">Backpropagation and Gradient calculation</figcaption></figure><p id="e8c7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">权重<em class="mr"> (w_22) </em>连接<em class="mr"> (a_2) </em>和<em class="mr"> (z_2) </em>，因此计算梯度需要通过<em class="mr">(z _ 2)</em><em class="mr">(a _ 2)</em>应用链式法则</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/8cf41e13d75faf2b597b9711c1ec0fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8QyxzWt_43DMpR4a"/></div></figure><p id="665b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们将多个梯度(导数)相乘，然后相乘后我们可能会得到非常小的值。</p><p id="c0d5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">换句话说，由于网络的深度和激活将值移至零，它们的梯度趋于消失。这被称为<strong class="in hi">消失渐变问题</strong>。所以我们希望我们的激活函数不会将梯度移向零。</p><h1 id="1ef8" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 2。以零为中心:</strong></h1><p id="6bec" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">激活函数的输出应该在零处对称，使得梯度不会移动到特定方向。</p><h1 id="5098" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> <em class="mw"> 3。</em>计算费用</strong>:</h1><p id="1058" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">在每一层之后应用激活函数，并且在深度网络中需要计算数百万次。因此，它们的计算成本应该很低。</p><h1 id="7ba4" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">C.什么是神经网络激活函数？</h1><p id="0ed7" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">激活函数是决定神经网络输出的数学方程。该函数附属于网络中的每个神经元，并基于每个神经元的输入是否与模型的预测相关来确定是否应该激活(“激发”)该函数。激活函数也有助于将每个神经元的输出标准化到1和0之间或-1和1之间的范围。</p><p id="2796" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">激活函数的另一个方面是，它们必须在计算上高效，因为它们是针对每个数据样本跨数千甚至数百万个神经元计算的。现代神经网络使用一种称为反向传播的技术来训练模型，这增加了激活函数及其导数函数的计算负担。</p><h1 id="24cc" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">D.激活功能的类型</h1><h1 id="c405" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">1.二元阶跃函数:</h1><p id="13d3" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">二元阶跃函数是基于阈值的激活函数。如果输入值高于或低于某个阈值，神经元就会被激活，并向下一层发送完全相同的信号。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/215a32ced96412779caf115b8e1ec340.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/0*qS9RqYMZ6cVTy0cU"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Binary Step Function Plot</figcaption></figure><p id="d33d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">阶跃函数的问题在于它不允许多值输出，例如，它不支持将输入分类到几个类别中的一个。</p><h1 id="947e" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.线性激活函数:</h1><p id="c0f0" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">线性激活函数采取以下形式:A = cx</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es my"><img src="../Images/6cd31d59067881cb2dec97b4728e859c.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/0*B0DPJ1nsRupY8SOl"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Linear Function Plot</figcaption></figure><p id="2132" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它获取输入，乘以每个神经元的权重，并创建与输入成比例的输出信号。在某种意义上，线性函数比阶跃函数更好，因为它允许多种输出，而不仅仅是是和不是。</p><p id="f572" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">然而，线性激活函数有两个主要问题:</p><p id="f48f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> a .无法使用</strong> <a class="ae ma" href="https://missinglink.ai/guides/neural-network-concepts/backpropagation-neural-networks-process-examples-code-minus-math/" rel="noopener ugc nofollow" target="_blank">反向传播</a>(梯度下降)来训练模型——函数的导数是一个常数，与输入x无关。因此无法回过头来了解输入神经元中的哪些权重可以提供更好的预测。</p><p id="37d0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> b .神经网络的所有层都坍缩成一个</strong> —采用线性激活函数，无论神经网络有多少层，最后一层都将是第一层的线性函数(因为线性函数的线性组合仍然是线性函数)。所以一个线性激活函数把神经网络变成一层。</p><p id="b55d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">具有线性激活函数的神经网络仅仅是线性回归模型。它处理复杂多变的输入数据参数的能力有限。</p><h1 id="f64f" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">E.非线性激活函数</h1><p id="dd79" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">现代神经网络模型使用非线性激活函数。它们允许模型在网络的输入和输出之间创建复杂的映射，这对于学习和建模复杂的数据是必不可少的，例如图像、视频、音频和非线性或高维的数据集。</p><p id="9925" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">只要激活函数是非线性的，几乎任何可以想象的过程都可以表示为神经网络中的函数计算。</p><p id="f244" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">非线性函数解决了线性激活函数的问题:</p><ol class=""><li id="d1d4" class="lm ln hh in b io ip is it iw lo ja lp je lq ji lr ls lt lu bi translated">它们允许反向传播，因为它们有一个与输入相关的导数函数。</li><li id="ab68" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji lr ls lt lu bi translated">它们允许多层神经元的“堆叠”来创建深度神经网络。需要多个隐藏的神经元层来学习高精度的复杂数据集。</li></ol><h1 id="d81c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">常见的非线性激活函数及如何选择激活函数</h1><h1 id="9777" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 1。乙状结肠/逻辑:</strong></h1><p id="c509" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">逻辑函数在逻辑回归中用作挤压函数，用于挤压有限范围[0，1]内的异常值点。</p><p id="9b30" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">s形函数:f(x)=1/(1+exp(-x)</p><p id="0aa5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">sigmoid <strong class="in hi">的导数:</strong> df(x)=f(x)*(1-f(x))</p><p id="2b55" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">代号:</strong></p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/b4238d3d14796fc5a992d539b0dd3e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*Xqtz4RBow8OGHLmJ"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Sigmoid Function and It’s Derivative</figcaption></figure><p id="4643" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优势:</strong></p><ul class=""><li id="9674" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated">平滑渐变，防止输出值“跳跃”。</li><li id="a29b" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">输出值介于0和1之间，使每个神经元的输出正常化。</li><li id="2b2c" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">清晰预测-对于大于2或小于-2的X，倾向于将Y值(预测)带到曲线边缘，非常接近1或0。这可以实现清晰的预测。</li></ul><p id="8d62" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点:</strong></p><ul class=""><li id="fede" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated">消失梯度-对于非常高或非常低的X值，预测几乎没有变化，从而导致消失梯度问题。这可能导致网络拒绝进一步学习，或者太慢而不能达到准确的预测。</li><li id="585b" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">输出不在零中心。</li><li id="f7da" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">计算开销很大。</li></ul><p id="bad1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">我们什么时候用乙状结肠:</strong></p><p id="1016" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(I)如果想要0到1之间的输出值，则仅在输出层神经元使用sigmoid。</p><p id="720a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">(ii)在做二元分类问题时，使用sigmoid</p><p id="07dc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">否则乙状结肠不是优选的。</p><h1 id="5085" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.正切/双曲正切:</h1><p id="1ca2" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">双曲正切函数是另一种可能的函数，可用作神经网络层间的非线性激活函数。它实际上与乙状结肠激活功能有一些共同之处。他们两个看起来非常相似。但是，sigmoid函数会将输入值映射到0和1之间，而Tanh会将值映射到-1和1之间。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/a4e29eb1cc7749701513db2327613cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/0*vmtNfM5n3kBoPXKq"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Plot for Sigmoid vs. tanh</figcaption></figure><p id="b37f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">像sigmoid函数一样，tanh函数的一个有趣的性质是导数可以用函数本身来表示。下面是双曲正切函数的实际公式以及计算其导数的公式</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/048ab51b9f164c3c0b40350e0f98416d.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*lW54_O5u2nTwCv4_"/></div><figcaption class="la lb et er es lc ld bd b be z dx">tanh function and it's derivative</figcaption></figure><p id="acd3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">代码:</strong></p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/a369e6e2dd917a62eedc6c236ab5bf30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/0*NNYO-4tkSaOL3qNB"/></div><figcaption class="la lb et er es lc ld bd b be z dx">tanh function and its derivative plot</figcaption></figure><p id="ffc7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点</strong></p><ul class=""><li id="edc0" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi">以零为中心的</strong> —更容易对具有强负值、中性值和强正值的输入进行建模。通常用于神经网络的隐藏层，因为其值位于<strong class="in hi"> -1到1 </strong>之间，因此隐藏层的平均值为0或非常接近于0，因此通过使平均值接近于0来帮助<em class="mr">将数据</em>居中。这使得下一层的学习更加容易。</li><li id="b212" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">否则就像Sigmoid函数。</li></ul><p id="8486" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点</strong></p><ul class=""><li id="1a9e" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated">比如Sigmoid函数</li></ul><h1 id="a74a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">3.整流线性单位</h1><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/50e5f8b6acd0b25109b6a46531dfe1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*KMNgs0N6uiHyIALK.png"/></div></figure><p id="86d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">ReLU在计算上比tanh和sigmoid便宜，因为它涉及更简单的数学运算。一次只有几个神经元被激活，使网络变得稀疏，从而使其高效且易于计算。</p><ul class=""><li id="1505" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi">方程:</strong> f(x) = a =max(0，x)。如果x为正，它给出输出x，否则给出0。</li><li id="8211" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated"><strong class="in hi">导数:</strong>f '(x)= { 1；如果z &gt; 0，0；如果z &lt;为0，如果z=0，则未定义</li><li id="0dc9" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated"><strong class="in hi">取值范围:- </strong> [0，inf</li></ul><p id="59a4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它在一定程度上避免和纠正了<strong class="in hi">消失梯度</strong>问题。现在几乎所有的深度学习模型都用<strong class="in hi"> ReLU </strong>。</p><p id="23fb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">代码:</strong></p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es ng"><img src="../Images/cd7e1f145fdb7d9982f28ebaf0d54ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*iyor6JfrUYHxbqxl"/></div><figcaption class="la lb et er es lc ld bd b be z dx">ReLU function and it’s Derivative Plot</figcaption></figure><p id="65ad" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优势</strong></p><ul class=""><li id="c61f" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi">计算效率高</strong> —允许网络快速收敛</li><li id="dad3" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated"><strong class="in hi">非线性— </strong>尽管ReLU看起来像线性函数，但它有一个导数函数，并允许反向传播</li></ul><p id="8199" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">不利于濒死ReLU问题</strong> —当输入趋近于零或为负时，函数的梯度变为零，网络无法进行反向传播，无法学习。</p><h1 id="aaf8" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">4.泄漏ReLU:</h1><p id="0046" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">为了解决ReLU中的渐变问题，引入了另一个修改，称为<strong class="in hi"> <em class="mr"> Leaky ReLU。</em> </strong></p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/3252688c85c3ded0d727561ca196d964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9l0QvR55sNGeLi7e"/></div><figcaption class="la lb et er es lc ld bd b be z dx">The difference in ReLU (left) and Leaky ReLU (right)</figcaption></figure><p id="83e6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">泄漏有助于增加ReLU功能的范围。通常a的值是0.01左右。当α不为0.01时，称为随机化ReLU。</p><p id="8faa" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因此，泄漏ReLU的<strong class="in hi">范围</strong>为(-无穷大到无穷大)。</p><p id="ebbd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">代码</strong>:</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es ng"><img src="../Images/8e782f53d4520c895d99842b74876db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*HT8GezYhuLoCNf92"/></div><figcaption class="la lb et er es lc ld bd b be z dx">Leaky ReLU function and it’s derivative plot</figcaption></figure><p id="9adf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优点</strong></p><ul class=""><li id="6053" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi">防止死ReLU问题</strong>—ReLU的这种变化在负区域有一个小的正斜率，因此它确实支持反向传播，即使对于负输入值也是如此</li><li id="552b" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">否则像ReLU</li></ul><p id="2319" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">缺点</strong></p><ul class=""><li id="911e" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi">结果不一致</strong> — leaky ReLU不能为负输入值提供一致的预测。</li></ul><h1 id="6401" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">5.Softmax:</h1><p id="42c0" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">softmax函数是一个激活函数，它将数字转化为总和为1的概率。softmax函数输出一个向量，该向量表示结果列表的概率分布。它也是深度学习分类任务中使用的核心元素。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/8f53a39d4e1301ceeef491234f44c3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/0*Q5YRv8DMi3vF62i8"/></div></figure><p id="7aa4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Softmax函数将logit[2.0，1.0，0.1]转换为概率[0.7，0.2，0.1]，概率总和为1。Logits是神经网络最后一层输出的原始分数。在激活发生之前。为了理解softmax函数，我们必须看看第(n-1)层的输出。</p><p id="556c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">优势</strong></p><ul class=""><li id="0e6f" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated">当我们有多个类时，使用softmax函数。</li><li id="aa5c" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">这对于找出具有最大值的类是有用的。概率。</li><li id="2577" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">Softmax函数理想地用在输出层，我们实际上试图获得概率来定义每个输入的类。</li><li id="cb04" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">范围从0到1。</li></ul><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ni"><img src="../Images/63f7f09915ca4a1a15ced820840bd7e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cE0YX7DmxBPP2MXk.png"/></div></div></figure><h1 id="4f90" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">F.使用哪个激活功能？</h1><p id="a9f3" class="pw-post-body-paragraph il im hh in b io kq iq ir is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji ha bi translated">所以到目前为止，我们已经看到了不同类型的激活函数，它们的优点和缺点。现在问题来了——我应该为我的神经网络使用哪个激活函数？</p><p id="83e0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">推荐一个适用于所有用例的激活函数是非常困难的。有许多考虑因素——计算导数有多困难(如果它是可微的话！)，你选择的AF的网络收敛速度有多快，有多光滑，是否满足泛逼近定理的条件，是否保持归一化等等。你可能关心也可能不关心这些。</p><ul class=""><li id="acfd" class="lm ln hh in b io ip is it iw lo ja lp je lq ji ms ls lt lu bi translated"><strong class="in hi"> Sigmoid函数</strong>及其组合一般在<strong class="in hi">二元分类问题</strong>的情况下效果更好。</li><li id="3422" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">由于梯度消失的问题，Sigmoid和tanh函数有时会被避免。</li><li id="f206" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">由于死神经元问题，Tanh在大多数情况下被避免。</li><li id="5328" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated"><strong class="in hi"> ReLU </strong>激活功能被广泛使用，并且是<strong class="in hi">默认选择</strong>，因为它产生更好的结果。</li><li id="5c3f" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">如果我们在网络中遇到死亡神经元的情况，那么leaky ReLU函数是最好的选择。</li><li id="4d9d" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated"><strong class="in hi"> ReLU函数</strong>是否应该<strong class="in hi">仅</strong>用于<strong class="in hi">隐藏层。</strong></li><li id="9353" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">在出现<strong class="in hi">回归问题时，<strong class="in hi">输出层</strong>可以是<strong class="in hi">线性</strong>激活函数。</strong></li><li id="e1f9" class="lm ln hh in b io lv is lw iw lx ja ly je lz ji ms ls lt lu bi translated">对于多类分类<strong class="in hi"> Softmax </strong>激活功能是最可取的</li></ul><p id="f297" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我希望这篇文章的目的是了解激活函数，为什么，什么时候，以及对于给定的问题陈述使用哪一个。</p><p id="46af" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">快乐学习！！！</p><h1 id="38fc" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">参考:</h1><ol class=""><li id="2ffd" class="lm ln hh in b io kq is kr iw nj ja nk je nl ji lr ls lt lu bi translated"><a class="ae ma" href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/" rel="noopener ugc nofollow" target="_blank">https://missing link . ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/</a></li></ol><p id="7891" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.<a class="ae ma" href="https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253#:~:text=Simply%20put%2C%20an%20activation%20function,fired%20to%20the%20next%20neuron." rel="noopener" target="_blank">https://towards data science . com/everything-you-need-to-know-on-activation-functions-in-deep-learning-models-84ba 9 f 82 c 253 #:~:text = Simply % 20 put % 2C % 20 an % 20 activation % 20 function，fired % 20 to % 20 the % 20 next % 20 neuron。</a></p></div></div>    
</body>
</html>