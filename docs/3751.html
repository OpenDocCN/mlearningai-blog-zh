<html>
<head>
<title>Playground for Stable Diffusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稳定扩散的游乐场</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/playground-for-stable-diffusion-d6954e8477a4?source=collection_archive---------0-----------------------#2022-10-16">https://medium.com/mlearning-ai/playground-for-stable-diffusion-d6954e8477a4?source=collection_archive---------0-----------------------#2022-10-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="54eb" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">使用稳定扩散模型生成图像和视频的简单指南</p></blockquote><p id="3562" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">嗨，让我们来看一篇关于稳定扩散和使用的简短的资料性阅读。这篇文章是为对计算机视觉和深度学习感兴趣，需要为他们的工作创建图像资产，或者对应用扩散模型构建他们的应用感兴趣的内容创建者和开发者写的。这篇文章不会深入稳定扩散背后的数学、建筑和研究。如果您有兴趣了解详细信息，链接附在参考资料部分。</p><h2 id="99c0" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">介绍📜</h2><p id="969e" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">什么是“扩散”？</p><p id="871f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">根据维基百科，“扩散是任何东西(例如，原子、离子、分子、能量)通常从一个浓度较高的区域到一个浓度较低的区域的净运动。”</p><p id="172f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">类似于定义，在前向扩散过程中，扩散模型逐渐将噪声应用于图像，直到图像变成完全的噪声。这实质上是扩散图像中的像素。在反向扩散过程中，噪声图像在相同的步骤中被去噪，直到数据被恢复。因为这是一个连续的过程，所以发生<a class="ae kj" href="https://developers.google.com/machine-learning/gan/problems" rel="noopener ugc nofollow" target="_blank">模式崩溃</a>(GANs的问题)的可能性较小。</p><h2 id="6ba5" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">潜在扩散模型🧙‍♂️</h2><p id="213f" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">潜在扩散模型由GAN结构、扩散模型和变压器模型构成。大多数扩散模型使用U-Net架构来保持图像的维度。通常，扩散模型应用像素空间中的扩散，但是稳定扩散模型应用潜在空间中的扩散。因此，术语“潜在扩散模型(LDM)”。使用转换器(编码器和解码器)完成像素空间到潜在空间的转换。与以前的方法相比，这种方法的内存效率高，并且还能生成非常详细的图像。</p><h2 id="e060" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">使用🏃‍♂️</h2><p id="5d59" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">我们将使用Github库<a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground" rel="noopener ugc nofollow" target="_blank">稳定扩散操场</a>来实现稳定扩散模型的不同应用模式。</p><p id="0e37" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">在撰写本文时，该存储库支持5种模式，<br/> 1。文本到图像<br/> 2。图像到图像<br/> 3。修补<br/> 4。梦想<br/> 5。有生命的</p><p id="9e1e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">让我们详细了解一下每个模式及其用法。</p><h2 id="3d60" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">代码要求</h2><p id="6b51" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">遵循存储库中的<a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground/blob/main/README.md" rel="noopener ugc nofollow" target="_blank"> readme </a>代码需求部分并设置环境。这是本文后面几节的基本部分。</p><h2 id="069b" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">拥抱脸访问令牌</h2><p id="a0c9" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated"><a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground" rel="noopener ugc nofollow" target="_blank">稳定扩散游乐场</a> codebase使用拥抱脸下载模型并使用它们的API。要使用hugging face hub，用户必须创建一个访问令牌来验证其身份。为此，在huggingface.co的<a class="ae kj" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">创建一个账户。然后，进入设置- &gt;访问令牌。创建具有读取权限的访问令牌。</a></p><p id="294a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">创建的访问令牌将由应用程序的模式使用。当提示输入访问令牌时，从“设置”-&gt;“访问令牌”中复制并使用它。请不要公开共享访问令牌。</p><h2 id="25f4" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">文本到图像模式</h2><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es kk"><img src="../Images/9983c6dd7e2ea1d6c2faf2cc7a97063e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Y-ocrrDfMF1pBjNc3-2VSQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Text to Image result</figcaption></figure><p id="9047" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">给定输入提示，该模式将基于提示描述生成图像。运行以下命令启动文本到图像模式的应用程序，</p><pre class="kl km kn ko fd kw kx ky kz aw la bi"><span id="c525" class="jj jk hh kx b fi lb lc l ld le">python run.py --mode txt2img --device gpu --save</span></pre><p id="86d5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">遵循命令行界面提供拥抱脸用户访问令牌，输入提示和要生成的图像的分辨率。</p><h2 id="b4ba" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">图像到图像模式</h2><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/4fd21ccaaabee4bfdac5f1af00b585bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_OqWxIZXivMS7vE-kvLiA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Image to Image result. Left is original input image. Right is generated image based on prompt and input image.</figcaption></figure><p id="ef48" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">给定输入提示和图像，该模式将根据提示描述修改输入图像。运行以下命令启动图像到图像模式的应用程序，</p><pre class="kl km kn ko fd kw kx ky kz aw la bi"><span id="35e5" class="jj jk hh kx b fi lb lc l ld le">python run.py --mode img2img --device gpu --save</span></pre><p id="868e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">遵循命令行界面提供拥抱脸用户访问令牌，输入提示，输入图像和强度。“强度”将接受[0，1]范围内的值，其中0表示与初始输入图像相比没有变化，1表示基于输入提示与初始输入图像相比完全变化。</p><h2 id="084e" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">修复模式</h2><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lk"><img src="../Images/49ab89ee4564e40d2c6a2b86ba89bd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykw3eLfvsrqP1KymqkQocA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Inpaint result. Left is initial input image. Center is mask image. Right is generated image based on prompt, initial image and mask image.</figcaption></figure><p id="a208" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">给定一个输入提示、一个初始输入图像和一个掩码图像，该模式将只根据掩码图像指定区域的提示描述修改输入图像。运行下面的命令来启动修复模式的应用程序，</p><pre class="kl km kn ko fd kw kx ky kz aw la bi"><span id="0bf7" class="jj jk hh kx b fi lb lc l ld le">python run.py --mode inpaint --device gpu --save</span></pre><p id="c642" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">遵循命令行界面提供拥抱脸用户访问令牌，输入提示，输入图像，面具图像和力量。“强度”将接受[0，1]范围内的值，其中0表示与初始输入图像相比没有变化，1表示基于输入提示与初始输入图像相比完全变化。</p><h2 id="db72" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">梦想模式</h2><p id="f5db" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">给定输入提示，该模式将根据提示描述生成视频。运行下面的命令来启动梦想模式的应用程序，</p><pre class="kl km kn ko fd kw kx ky kz aw la bi"><span id="7eca" class="jj jk hh kx b fi lb lc l ld le">python run.py --mode dream --device gpu --save --num &lt;number of frames&gt;</span></pre><p id="866a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><number of="" frames="">表示生成的视频所需的帧数。</number></p><p id="8012" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">遵循命令行界面提供拥抱脸用户访问令牌，输入提示和分辨率的视频生成。</p><p id="2f5c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这种模式的工作方式是在固定数量的步长上对潜在值进行球面插值，并将潜在值作为初始输入潜在值提供给模型。这在生成的帧中产生小的变化，保持帧之间的一致性。因此，视频感觉像是由模型想象出来的梦幻效果。</p><h2 id="e965" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">动画模式</h2><p id="ca4c" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">这种模式与其他模式的不同之处在于它的用途和体系结构。动画模式支持基于输入提示的2D和3D视频生成。这种模式也支持视频作为输入，并根据输入提示将视频转换成一种风格。运行以下命令启动动画模式的应用程序，</p><pre class="kl km kn ko fd kw kx ky kz aw la bi"><span id="5dcf" class="jj jk hh kx b fi lb lc l ld le">python run.py --mode animate --device gpu --save</span></pre><p id="8d67" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">遵循命令行界面提供拥抱脸用户访问令牌。需要在<a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground/blob/main/animation_mode/config.py" rel="noopener ugc nofollow" target="_blank"> animation_mode/config.py </a>文件中设置模式的提示和配置。浏览<a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground/tree/main/animation_mode" rel="noopener ugc nofollow" target="_blank">自述文件</a>以更好地理解配置及其用法。</p><h2 id="de0c" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">演示视频</h2><figure class="kl km kn ko fd kp"><div class="bz dy l di"><div class="ll lm l"/></div></figure><h2 id="d81a" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">结论</h2><p id="bf18" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">潜在扩散模型在图像生成方面向前迈进了一步，生成了具有极端细节的高分辨率图像，同时还保留了图像的语义结构。</p><p id="2541" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground" rel="noopener ugc nofollow" target="_blank">稳定扩散操场</a>应用是扩散模型领域突破性研究和发展的结果。感谢令人惊叹的创作者和开发者开源了论文、项目和模型，供每个人实验、用于各种应用并改进之前的作品。</p><h2 id="86fe" class="jj jk hh bd jl jm jn jo jp jq jr js jt jg ju jv jw jh jx jy jz ji ka kb kc kd bi translated">参考</h2><p id="4c3d" class="pw-post-body-paragraph ih ii hh ik b il ke in io ip kf ir is jg kg iv iw jh kh iz ja ji ki jd je jf ha bi translated">[1]罗宾·龙巴赫，安德烈亚斯·布拉特曼，张秀坤·洛伦茨，帕特里克·埃塞尔，比约恩·奥姆，“利用潜在扩散模型的高分辨率图像合成”，arXiv:2112.10752，2021</p><p id="e463" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">[2]布拉特曼等人。艾尔。，潜在扩散模型，<a class="ae kj" href="https://github.com/CompVis/latent-diffusion" rel="noopener ugc nofollow" target="_blank">https://github.com/CompVis/latent-diffusion</a>，2022</p><p id="43b5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">[3] Logeswaran Sivakumar，稳定扩散游乐场，<a class="ae kj" href="https://github.com/Logeswaran123/Stable-Diffusion-Playground" rel="noopener ugc nofollow" target="_blank">https://github . com/Logeswaran 123/Stable-Diffusion-Playground</a>，2022</p><p id="1b97" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">[4]抱脸稳定扩散，<a class="ae kj" href="https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion" rel="noopener ugc nofollow" target="_blank">https://github . com/Hugging Face/diffusers/tree/main/src/diffusers/pipelines/Stable _ Diffusion</a>，2022</p><div class="ln lo ez fb lp lq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hi fi z dy lv ea eb lw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">medium.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me kq lq"/></div></div></a></div></div></div>    
</body>
</html>