<html>
<head>
<title>Multi-Process Service (MPS) of NVIDIA GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NVIDIA GPUs的多进程服务(MPS)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/multi-process-service-mps-of-nvidia-gpus-2d8683df273f?source=collection_archive---------2-----------------------#2022-07-27">https://medium.com/mlearning-ai/multi-process-service-mps-of-nvidia-gpus-2d8683df273f?source=collection_archive---------2-----------------------#2022-07-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="20de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇全面的文章收集了理解NVIDIA GPUs的多进程服务(MPS)能力所需的所有信息。这篇文章就像一个时间线故事，详细阐述了理解MPS如何工作所需的概念。从CPU和GPU的区别入手，阐述了GPU的利用不足问题；之后，它深入研究CUDA流，讨论Hyper-Q，最后到达MPS。如果读者想直接阅读MPS，他们可以向下滚动到MPS部分或使用Nvidia文档，如参考资料部分所示。</p><h1 id="970a" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">介绍</h1><p id="1027" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">与中央处理单元(CPU)相比，图形处理单元(GPU)为计算中具有并行特性的应用提供了显著更高的性能增益。然而，开发顺序程序比开发并行程序容易得多，因为它不需要并行思维和解决问题的技能。另一方面，并行程序的设计和调试通常需要更多的时间和脑力。然而，这是在软件层面获得加速的代价。此外，与CPU相比，GPU拥有更多内核。考虑以下CPU和GPU规格差异的示例。A100 GPU有大约7000个简单内核，而AMD EPYC 7742只有64个复杂内核。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/e0249be46b28cac8c4a84189d9295615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhlYAqG3sqVae-8HQf8Lug.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">CPU and GPU specification difference [<a class="ae kv" href="https://www.nvidia.com/en-us/data-center/a100/" rel="noopener ugc nofollow" target="_blank">NVIDIA A100</a>][<a class="ae kv" href="https://www.amd.com/en/products/cpu/amd-epyc-7742" rel="noopener ugc nofollow" target="_blank">AMD EPYC 7742</a>]</figcaption></figure><p id="a9a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了它们在架构方面的不同之处。CPU采用大型缓存层次系统，这使它们成为面向延迟的处理器，而GPU是面向吞吐量的。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kw"><img src="../Images/058338a77cde4c4bd5fcc07f4ec4e621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Kg2E-ndQUKOf1YC1.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Image credit [<a class="ae kv" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" rel="noopener ugc nofollow" target="_blank">CUDA c programming guide</a>]</figcaption></figure><p id="23c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，GPU压缩了程序必须利用的许多硬件资源。然而，大多数时候，开发或拥有使用这些资源的程序是不可能的。这个问题就是众所周知的“利用不足”问题。解决方案是在同一个GPU上同时运行几个应用程序，以提高利用率。然而，与CPU不同，GPU缺乏细粒度的共享机制。此外，GPU没有虚拟内存。此外，由于要移动的数据量通常非常大，因此上下文切换会带来非常昂贵的开销。因此，NVIDIA first在2013年2月推出了Hyper-Q技术，使几个CPU线程能够在单个GPU上启动工作。</p><h1 id="5748" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">NVIDIA Hyper-Q技术</h1><p id="e0de" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">Hyper-Q支持多个CPU线程在单个GPU上启动内核，从而提高了GPU利用率，并<strong class="ig hi">减少了CPU空闲时间</strong>。Hyper-Q还消除了错误的依赖关系，以提高GPU利用率。为了深入研究Hyper-Q如何工作，首先，我们需要回顾CUDA流并理解它们是如何工作的，这是GPU的主要处理方式。NVIDIA在Fermi (2010)之后推出了采用开普勒架构的Hyper-Q(2012)。</p><h1 id="3a70" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">CUDA流</h1><p id="de6a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">为了简化学习过程，请记住，我们通常使用不同的流进行重叠数据传输(数据传输发生在CPU内存和GPU内存之间)和计算。CUDA流<strong class="ig hi"> </strong>是在GPU上按照CPU代码启动的顺序执行的内核队列。但是，来自不同流的内核可以交错。</p><blockquote class="kx ky kz"><p id="a5a3" class="ie if la ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated"><strong class="ig hi">注意，流式多处理器是一个不同于流的概念。</strong></p></blockquote><p id="a15c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CUDA中所有的GPU操作(<strong class="ig hi">内核和数据传输</strong>)都是在一个流中运行的。当内核中没有指定流时，使用默认流(或空流)。以下句子摘自NVIDIA博客，描述默认流[ <a class="ae kv" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank"> ref </a> ]。</p><blockquote class="kx ky kz"><p id="10f5" class="ie if la ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated">在设备上的任何流中的所有先前发布的操作<em class="hh">完成之前，默认流中的任何操作都不会开始，并且默认流中的操作必须在任何其他操作(设备上的任何流中的)开始之前完成。</em></p></blockquote><p id="5972" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<strong class="ig hi"> CUDA 7 </strong>和更新的版本中，每个主机线程可以有单独的默认流。考虑下面的CUDA C++例子[ <a class="ae kv" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">我使用了NVIDIA博客文章ref </a>中的这个例子。指令(1)将数据从系统的主内存复制到GPU的内存中。当复制完成时，指令2启动一个内核，它有一个由N个线程组成的块。这些线程在GPU核上以并行方式执行。然后完成内核执行后，开始从GPU内存复制数据。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="35ab" class="lj jd hh lf b be lk ll l lm ln">cudaMemcpy(gpu_array, sys_array, numBytes, cudaMemcpyHostToDevice);   // (1)<br/><br/>increment&lt;&lt;&lt;1, N&gt;&gt;(gpu_array);                                        // (2)<br/><br/>cudaMemcpy(sys_array, gpu_array, numBytes, cudaMemcpyDeviceToHost);   // (3)</span></pre><p id="cb86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">GPU不知道CPU(主机)端发生了什么。在下面的代码片段中，当指令2在设备上启动时，指令3开始在CPU上执行。当内核执行在GPU上完成时，指令4将完成数据移动。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="5f3d" class="lj jd hh lf b be lk ll l lm ln">cudaMemcpy(gpu_array, sys_array, numBytes, cudaMemcpyHostToDevice);   // (1)<br/><br/>increment&lt;&lt;&lt;1, N&gt;&gt;(gpu_array);                                        // (2)<br/><br/>aCpuFunction(b);                                                      // (3)<br/><br/>cudaMemcpy(sys_array, gpu_array, numBytes, cudaMemcpyDeviceToHost);   // (4)</span></pre><p id="6a43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在创建了一个非默认的流之后，我们可以指定我们希望内核在哪个流上启动，如下所示:</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="c6bd" class="lj jd hh lf b be lk ll l lm ln">cudaStream stream1;<br/>cudaError_t result;<br/>result = cudaStreamCreate(&amp;stream1); // creating a non-default stream<br/><br/>increment&lt;&lt;&lt;1, N, 0, stream1&gt;&gt;&gt;(gpu_array);<br/>result = cudaMemcpyAsync(gpu_array, sys_array, N, cudaMemcpyHostToDevice, steam1);<br/><br/>result = cudaStreamDestroy(stream1); // destroying the previously created stream</span></pre><p id="4d97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="la">cudamemacpyasync</em>用于非默认流，使CPU计算与GPU计算重叠。<em class="la">cudamemacpyasync</em>发出复制操作后将控制权返回给主机线程。</p><p id="5daf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下来自NVIDIA博客的示例清楚地显示了如何明智地使用流，因为更多的并行性而导致更高的性能。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="a8fd" class="lj jd hh lf b be lk ll l lm ln">const int block_size = 256, number_of_streams = 4;<br/>const int n = 4 * 1024 * block_size * number_of_streams;<br/>const int stream_size = n / number_of_streams;<br/>const int stream_bytes = stream_size * sizeof(float);<br/>const int bytes = n * sizeof(float);<br/><br/>// Version 1<br/><br/>for (int i = 0; i &lt; number_of_streams; ++i) {<br/>  int offset = i * stream_size;<br/>  cudaMemcpyAsync(&amp;gpu_array[offset], &amp;sys_array[offset], stream_bytes, cudaMemcpyHostToDevice, stream[i]);<br/>  kernel&lt;&lt;&lt; stream_size/ block_size, block_size, 0, stream[i]&gt;&gt;&gt;(gpu_array, offset);<br/>  cudaMemcpyAsync(&amp;sys_array[offset], &amp;gpu_array[offset], stream_bytes, cudaMemcpyDeviceToHost, stream[i]);<br/>}</span></pre><pre class="lo le lf lg bn lh li bi"><span id="1ad2" class="lj jd hh lf b be lk ll l lm ln">// Version 2<br/><br/>for (int i = 0; i &lt; number_of_streams; ++i) {<br/>  int offset = i * stream_size;<br/>  cudaMemcpyAsync(&amp;gpu_array[offset], &amp;sys_array[offset], stream_bytes,cudamemcpyHostToDevice, cudamemcpyHostToDevice, stream[i]);<br/>}<br/><br/>for (int i = 0; i &lt; number_of_streams; ++i) {<br/>  int offset = i * stream_size;<br/>  kernel&lt;&lt;&lt;stream_size/ block_size, block_size, 0, stream[i]&gt;&gt;&gt;(gpu_array, offset);<br/>}<br/><br/>for (int i = 0; i &lt; number_of_streams; ++i) {<br/>  int offset = i * stream_size;<br/>  cudaMemcpyAsync(&amp;gpu_array[offset], &amp;sys_array[offset], stream_bytes,cudamemcpyDeviceToHost, cudamemcpyDeviceToHost, stream[i]);<br/>}</span></pre><p id="7aec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面显示了在GPU设备上执行单副本和内核引擎对它们进行排队的结果。顺序版本是当使用<em class="la">cudamemacpy</em>代替<em class="la">cudamemacpyasync</em>时。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lp"><img src="../Images/86773a5218b76bb7e7af60301dd183b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yV1Vboj-FLiEv5zlzGqLhA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [NVIDIA blog post: <a class="ae kv" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">How to Overlap Data Transfers in CUDA C/C++</a>]</figcaption></figure><p id="ec6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面显示了在具有两个副本和一个内核执行流的GPU上执行的结果。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lq"><img src="../Images/48aeebd510135c94b652034d0a28fc71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVY8UaeTPGkrFsqZI0N57Q.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [NVIDIA blog post: <a class="ae kv" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">How to Overlap Data Transfers in CUDA C/C++</a>]</figcaption></figure><p id="d24b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当多个内核在不同的非默认流中在GPU上启动时，GPU调度程序会尝试启用这些内核的并发执行。GPU延迟通常在每次内核完成后出现的完成信号，这是内存复制操作的开始标志。因此，虽然在我们的异步代码的第二个版本中，主机到设备的传输和内核执行之间有重叠，但是内核执行和设备到主机的传输之间没有重叠。</p><p id="942f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，异步操作会在设备完成请求的作业之前将控制权返回给主机线程。因此CPU可以发送更多的作业，引擎将它们排队等待执行。这些命令是:</p><ul class=""><li id="c6f7" class="lr ls hh ig b ih ii il im ip lt it lu ix lv jb lw lx ly lz bi translated"><strong class="ig hi">内核启动</strong></li><li id="b325" class="lr ls hh ig b ih ma il mb ip mc it md ix me jb lw lx ly lz bi translated"><strong class="ig hi">存储器在两个地址之间复制到同一个设备存储器</strong></li><li id="6575" class="lr ls hh ig b ih ma il mb ip mc it md ix me jb lw lx ly lz bi translated"><strong class="ig hi">从主机到设备的64 KB或更小内存块的内存拷贝</strong></li><li id="f3e9" class="lr ls hh ig b ih ma il mb ip mc it md ix me jb lw lx ly lz bi translated"><strong class="ig hi">内存复制由带有</strong> <code class="du mf mg mh lf b"><strong class="ig hi">Async</strong></code> <strong class="ig hi">后缀</strong>的函数执行</li><li id="df02" class="lr ls hh ig b ih ma il mb ip mc it md ix me jb lw lx ly lz bi translated"><strong class="ig hi">记忆设置功能调用</strong></li></ul><p id="8a61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要在CUDA 7和更高版本中启用每线程默认流，您可以在包含CUDA头文件(<code class="du mf mg mh lf b"><em class="la">cuda.h</em></code>或<code class="du mf mg mh lf b"><em class="la">cuda_runtime.h</em></code>)之前，使用<code class="du mf mg mh lf b"><em class="la">nvcc</em></code>命令行选项<code class="du mf mg mh lf b"><em class="la">--default-stream per-thread</em></code>或<code class="du mf mg mh lf b"><em class="la">#define</em></code>编译<code class="du mf mg mh lf b"><em class="la">CUDA_API_PER_THREAD_DEFAULT_STREAM</em></code>预处理宏。</p><p id="2b2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值得注意的是:当代码由<code class="du mf mg mh lf b">nvcc</code>编译时，您不能使用<code class="du mf mg mh lf b"><em class="la">#define CUDA_API_PER_THREAD_DEFAULT_STREAM</em></code>在. cu文件中启用这种行为，因为<code class="du mf mg mh lf b">nvcc</code>在翻译单元的顶部隐式地包含了<code class="du mf mg mh lf b">cuda_runtime.h</code>。</p><h2 id="6935" class="mi jd hh bd je mj mk ml ji mm mn mo jm ip mp mq jq it mr ms ju ix mt mu jy mv bi translated">多流示例</h2><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="b1a2" class="lj jd hh lf b be lk ll l lm ln">const int N = 1 &lt;&lt; 20;<br/><br/>__global__ void kernel(float *x, int n)<br/>{<br/>    int tid = threadIdx.x + blockIdx.x * blockDim.x;<br/>    for (int i = tid; i &lt; n; i += blockDim.x * gridDim.x) {<br/>        x[i] = sqrt(pow(3.14159,i));<br/>    }<br/>}<br/><br/>int main()<br/>{<br/>    const int num_streams = 8;<br/><br/>    cudaStream_t streams[num_streams];<br/>    float *data[num_streams];<br/><br/>    for (int i = 0; i &lt; num_streams; i++) {<br/>        cudaStreamCreate(&amp;streams[i]);<br/> <br/>        cudaMalloc(&amp;data[i], N * sizeof(float));<br/>        <br/>        // launch one worker kernel per stream<br/>        kernel&lt;&lt;&lt;1, 64, 0, streams[i]&gt;&gt;&gt;(data[i], N);<br/><br/>        // launch a dummy kernel on the default stream<br/>        kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(0, 0);<br/>    }<br/><br/>    cudaDeviceReset();<br/><br/>    return 0;<br/>}</span></pre><p id="5541" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果使用标准nvcc命令编译代码，如下所示:</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="cb5c" class="lj jd hh lf b be lk ll l lm ln">$ nvcc ./stream_test.cu -o stream_legacy</span></pre><p id="68fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为未启用每线程默认流，所以设备通常会执行。分析结果将显示:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es mw"><img src="../Images/f3ed3dc6938455470ca626475181b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*pTEvhGwYVG11vT27NUKg5w.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/" rel="noopener ugc nofollow" target="_blank">NVIDIA blog, “GPU Pro Tip: CUDA 7 Streams Simplify Concurrency”</a>]</figcaption></figure><p id="1280" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，使用以下命令:</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="ea27" class="lj jd hh lf b be lk ll l lm ln">$ nvcc --default-stream per-thread ./stream_test.cu -o stream_per-thread</span></pre><p id="c4d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它将执行如下相同的程序:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es mw"><img src="../Images/ba47de65c19f78dac5c328adaf5158a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*j9pl9ZC1Z59fV1liYrwmVA.png"/></div></figure><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es mw"><img src="../Images/f3ed3dc6938455470ca626475181b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*pTEvhGwYVG11vT27NUKg5w.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/" rel="noopener ugc nofollow" target="_blank">NVIDIA blog, “GPU Pro Tip: CUDA 7 Streams Simplify Concurrency”</a>]</figcaption></figure><p id="34ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">多线程可以在设备上启动内核，借助CUDA 7中引入的每线程默认流，这些内核可以并行化。如果您想查看代码并进行更多调查，请点击此处。</p><h1 id="15ff" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">超Q技术</h1><p id="7484" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在Hyper-Q和Kepler (2012)之前，不同的线程可以在不同的流上提交任务(CUDA 7+)。工作分配器过去常常在检查所有的依赖关系都得到满足之后，从管道的负责人那里接管工作，并在可用的SMs上开展工作。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mx"><img src="../Images/16fc8dd58c2ab51afaaaf22097a6d9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHVu6iXXmE-yQhOigefvRA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">Hyper-Q Example by NVIDIA</a>]</figcaption></figure><p id="cdc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于费米架构的单管道(一个执行引擎，记住streams中的例子)，这种深度优先的启动序列将导致错误的依赖关系。结果，硬件只能确定它可以同时执行阴影线对。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es my"><img src="../Images/4fac031f04f7e73ce85276f4ae21fd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rj5Qh8RdD83bQ_M8FI2CHQ.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">Hyper-Q Example by NVIDIA</a>]</figcaption></figure><p id="8060" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着开普勒架构和Hyper-Q，网格管理单元(<strong class="ig hi"> GMU </strong>)被引入。GMU创建多个硬件工作队列来<strong class="ig hi">减少</strong>或消除虚假依赖。</p><blockquote class="kx ky kz"><p id="6111" class="ie if la ig b ih ii ij ik il im in io lb iq ir is lc iu iv iw ld iy iz ja jb ha bi translated">SMX代表下一代流式多处理器！</p></blockquote><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es mz"><img src="../Images/72c9c85c39ada495470bd97bc3c5d85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*AgCoH9LwJx-VFBetLBVvsg.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">Hyper-Q Example by NVIDIA</a>]</figcaption></figure><p id="24b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面的例子显示了在没有和有Hyper-Q的设备上代码片段的执行是如何不同的。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="4e81" class="lj jd hh lf b be lk ll l lm ln">for (int i = 0; i &lt; number_of_streams; i++) {<br/>kernel_A &lt;&lt;&lt;1, 1, 0, streams[i]&gt;&gt;&gt;(&amp;gpu_array[2 * i], time_clocks);<br/>kernel_B &lt;&lt;&lt;1, 1, 0, streams[i]&gt;&gt;&gt;(&amp;gpu_array[2 * i + 1], time_clocks);<br/>}</span></pre><p id="b680" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不带Hyper-Q:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es na"><img src="../Images/4eaccff3c51ddff02a17aa5ad7806f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZYIXUFszxbhYWUX2vN8ug.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">Hyper-Q Example by NVIDIA</a>]</figcaption></figure><p id="1bfd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">借助Hyper-Q消除虚假依赖:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es nb"><img src="../Images/ef38b41551ceafb043d15388fa244a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*ojm2Zv4kXvQ5y8zUd2OkQw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">image credit [<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">Hyper-Q Example by NVIDIA</a>]</figcaption></figure><h1 id="afa2" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">主生产计划</h1><p id="5c70" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated"><strong class="ig hi">它是CUDA API </strong>的一个替代和二进制兼容的实现。MPS支持协作式多进程CUDA应用，通常是MPI作业，以利用基于开普勒或更新架构的NVIDIA GPUs上的Hyper-Q功能。<strong class="ig hi"> Hyper-Q使得在一个GPU上同时处理CUDA内核成为可能，当单个应用程序未充分利用GPU计算能力时，这有利于提高性能。</strong></p><p id="1693" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与Pascal架构相比，基于Volta架构的MPS增加了新功能。在Volta中，QoS受到尊重，因此对GPU的配置有限制。此外，所有MPS客户端都有GPU内存地址空间。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nc"><img src="../Images/6b9c1a42de73bb438bb56bd9e7075597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPBOWYpc46ACEvTDuwBguw.png"/></div></div></figure><p id="a5b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">综上所述，MPS提高了GPU利用率，减少了GPU上的上下文存储和切换。</p><h2 id="2e27" class="mi jd hh bd je mj mk ml ji mm mn mo jm ip mp mq jq it mr ms ju ix mt mu jy mv bi translated">何时以及如何使用MPS</h2><p id="12de" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">当每个应用程序的工作不能使GPU饱和时，使用MPS是有益的。每个网格包含少量块的应用程序无法充分利用GPU。使用MPS时，GPU要设置为<strong class="ig hi">EXCLUSIVE _ PROCESS</strong>compute模式，保证只有一台MPS服务器使用GPU才有一个仲裁点。</p><p id="6bac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于使用MPS，考虑你有两个不同的源代码，并想在一个GPU上同时执行它们。首先，必须更改GPU的计算模式，然后启动MPS服务器。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="a8d3" class="lj jd hh lf b be lk ll l lm ln">$ nvidia-smi -i 0 -c EXCLUSIVE_PROCESS<br/>$ nvidia-cuda-mps-control -d</span></pre><p id="e77b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，应用程序可以按如下方式启动:</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="34e5" class="lj jd hh lf b be lk ll l lm ln">$ ./app1 &amp;<br/>$ ./app2 &amp;<br/>$ ./app3 &amp;</span></pre><p id="3d34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要关闭MPS服务器:</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="f564" class="lj jd hh lf b be lk ll l lm ln">$ echo quit | nvidia-cuda-mps-control<br/>$ nvidia-smi -i 0 -c DEFAULT</span></pre><p id="1780" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意:</p><ul class=""><li id="f2ca" class="lr ls hh ig b ih ii il im ip lt it lu ix lv jb lw lx ly lz bi translated">一个系统中只有一个用户可以拥有活动的MPS服务器。</li><li id="dce2" class="lr ls hh ig b ih ma il mb ip mc it md ix me jb lw lx ly lz bi translated">独占模式限制适用于MPS服务器，而不是MPS客户端。</li></ul><p id="93da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面给出了一个使用MPS的脚本示例。请注意，要将GPU设置为独占执行模式，我们必须拥有root权限。</p><pre class="kg kh ki kj fd le lf lg bn lh li bi"><span id="7143" class="lj jd hh lf b be lk ll l lm ln">mkdir /tmp/mps_0<br/>mkdir /tmp/mps_log_0<br/><br/>export CUDA_VISIBLE_DEVICES=0<br/>export CUDA_MPS_PIPE_DIRECTORY=/tmp/mps_0<br/>export CUDA_MPS_LOG_DIRECTORY=/tmp/mps_log_0<br/><br/>nvidia-smi -i 0 -c EXCLUSIVE_PROCESS<br/><br/>nvidia-cuda-mps-control -d<br/><br/># Launching two applications on the GPU index = 0<br/>application1 &amp;<br/>application2 &amp;</span></pre><h1 id="7a1b" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">参考</h1><p id="0abf" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">[1]<a class="ae kv" href="https://developer.download.nvidia.com/compute/DevZone/C/html_x64/6_Advanced/simpleHyperQ/doc/HyperQ.pdf" rel="noopener ugc nofollow" target="_blank">https://developer . download . NVIDIA . com/compute/DevZone/C/html _ x64/6 _ Advanced/simple hyperq/doc/hyperq . pdf</a></p><p id="49d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]<a class="ae kv" href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">https://developer . NVIDIA . com/blog/how-optimize-data-transfers-cuda-cc/</a></p><p id="1166" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]<a class="ae kv" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">https://developer . NVIDIA . com/blog/how-overlap-data-transfers-cuda-cc/</a></p><p id="4ae3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]<a class="ae kv" href="https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/" rel="noopener ugc nofollow" target="_blank">https://developer . NVIDIA . com/blog/GPU-pro-tip-cuda-7-streams-simplify-concurrency/</a></p><p id="34a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5]<a class="ae kv" href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf" rel="noopener ugc nofollow" target="_blank">https://docs . NVIDIA . com/deploy/pdf/CUDA _ Multi _ Process _ Service _ overview . pdf</a></p><p id="e2aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6]<a class="ae kv" href="https://stackoverflow.com/questions/34709749/how-do-i-use-nvidia-multi-process-service-mps-to-run-multiple-non-mpi-cuda-app" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/34709749/how-do-I-use-NVIDIA-multi-process-service-MPs-to-run-multiple-non-MPI-cuda-app</a></p><p id="b9bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7]<a class="ae kv" href="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf" rel="noopener ugc nofollow" target="_blank">https://www . olcf . ornl . gov/WP-content/uploads/2021/06/MPS _ ORNL _ 2021 08 17 . pdf</a></p><div class="nd ne ez fb nf ng"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">medium.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu kp ng"/></div></div></a></div></div></div>    
</body>
</html>