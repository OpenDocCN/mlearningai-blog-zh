<html>
<head>
<title>Transformers oversimplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器过于简单</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/transformers-oversimplified-3410407ad4aa?source=collection_archive---------2-----------------------#2022-01-07">https://medium.com/mlearning-ai/transformers-oversimplified-3410407ad4aa?source=collection_archive---------2-----------------------#2022-01-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="310c" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">理解狂怒的变形金刚网络架构</h2></div><p id="ff40" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi js translated">这些年来，eep学习一直在发展。这是它享有盛名的一个重要原因。深度学习实践高度强调使用<strong class="iy hi">大桶参数</strong>来提取关于我们正在处理的数据集的有用信息。有了一大组参数，分类/检测某样东西就变得<strong class="iy hi">容易了，因为我们有更多的数据可以清楚地识别。</strong></p><p id="bce3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">到目前为止，在深度学习的旅程中，特别是在自然语言处理中，一个值得注意的里程碑是<strong class="iy hi">语言模型</strong>的引入，它极大地提高了完成各种NLP任务的准确性和效率。</p><p id="076f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">序列-序列模型</strong>是基于<strong class="iy hi">编码器-解码器</strong>机制的模型，它接受一系列输入并返回一系列输出作为结果。考虑图像标题，对于给定的图像，我们创建一个标题。在这种情况下，seq-seq模型将<strong class="iy hi">图像像素向量(序列)</strong>作为输入，并将<strong class="iy hi">字幕逐字(序列)</strong>作为输出返回。</p><p id="7cf9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一些促进此类模型训练的重要DL算法包括<strong class="iy hi">递归神经网络、长短期记忆和门控递归单元</strong>。但随着时间的推移，这些算法的使用逐渐减少，因为随着数据集大小的增加，这些算法的复杂性和一些缺点会严重影响性能。一些重要的缺点包括<strong class="iy hi">更长的训练时间，消失梯度</strong>问题，当我们为大数据集进一步训练模型时，丢失了关于旧数据的信息，算法的复杂性等等。</p><h1 id="65a1" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">你需要的只是关注</h1><p id="959b" class="pw-post-body-paragraph iw ix hh iy b iz kt ii jb jc ku il je jf kv jh ji jj kw jl jm jn kx jp jq jr ha bi translated">在语言模型训练方面取代所有上述算法的一个突破性概念是<strong class="iy hi">基于多头注意力的转换器架构</strong>。谷歌在2017年的论文<strong class="iy hi">“注意力是你所需要的一切”中首次介绍了transformer架构。它变得如此受欢迎的主要原因是因为它的架构引入了并行化。变形金刚利用高度<strong class="iy hi">强大的TPUs </strong>和<strong class="iy hi">并行训练</strong>导致<strong class="iy hi">减少训练时间</strong>。</strong></p><p id="e59f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">transformer架构看起来更接近于此。</p><figure class="ky kz la lb fd lc"><div class="bz dy l di"><div class="ld le l"/></div></figure><p id="310d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">开个玩笑，但是如果能看到这样一个变形金刚组合在一起的视觉效果，那就太棒了。但事实上，它有很酷的架构。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/b2f1412ca221a082051d8a0ad4ab4265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MiYwcWai2oCyUjOu.png"/></div></div></figure><p id="501d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有趣的是，即使如此抽象，整个架构也不适合你的屏幕。但是，在每一层的罩盖下仍然隐藏着太多的东西。但是在这篇文章中，我们并没有深入到本质。我们只想知道每一层以及它在概述中的作用。</p><p id="53b7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">转换器有一个用于seq-seq模型的编码器-解码器模型，输入在左边，输出在右边。它内部使用了<strong class="iy hi">注意力机制</strong>，已经成为语言模型的<em class="lm">最高算法。</em></p><p id="cd62" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[注:如果你不想看完整个细节，我建议你跳到<a class="ae ln" rel="noopener" href="/@logeshvarl/transformers-oversimplified-3410407ad4aa#:~:text=the%20decoder%20layer.-,The%20overall%20picture,-Now%20let%E2%80%99s%20take"> <strong class="iy hi">【整体画面】</strong> </a> <strong class="iy hi"> </strong>部分，它简单地给出了所有的信息。]</p><p id="8610" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，当我解释每一层时，我们将使用语言翻译任务示例，该示例包含简单句“我是学生”及其法语翻译形式“Je suis étudiant”。</p><h2 id="8595" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">嵌入层</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mc"><img src="../Images/2af47a528e775d7079988c8fa49222bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*BKO6YD8LdvFGLlOT_cLkQQ.jpeg"/></div></figure><p id="c053" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">输入嵌入是变压器编码器和解码器端的第一步。这些<strong class="iy hi">机器</strong> <strong class="iy hi">无法理解任何语言的文字</strong>。它只吃数字。因此，我们得到了输入/输出中每个单词的<strong class="iy hi">嵌入，这在像<a class="ae ln" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi"><em class="lm">GloVe</em></strong></a><strong class="iy hi"><em class="lm">这样的地方很容易得到。</em> </strong>对于该嵌入值，我们添加该单词在句子<strong class="iy hi">中的<strong class="iy hi">位置信息</strong>(基于奇数或偶数位置出现的不同值)以给出上下文</strong>语义信息。</strong></p><h1 id="6e31" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">多头注意力</h1><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es md"><img src="../Images/fb0303c2afd7f65b015ad21a5448a748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0YUmRkDAkujSEgtWWsG5Q.jpeg"/></div></div></figure><p id="7fb0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">多头关注层由嵌入在一起的多个自我关注层组成。注意力层的主要目的是收集关于句子中每个单词与其他单词的<strong class="iy hi">相关性的信息，并帮助轻松理解意思</strong>。上面的插图描述了我们句子中的每个单词是如何依赖其他单词来表达意思的。但是让机器理解这种依赖性和相关性并不容易。</p><p id="167a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这就是注意力进入画面的地方。在我们的注意力层中，我们取三个输入向量，即,<strong class="iy hi">查询(Q)、键(K)和值(V)。</strong>把查询想象成你在浏览器上搜索的东西，浏览器有一组页面要匹配，这些页面就是键，我们得到的结果就是值。同样，对于句子<strong class="iy hi"> (Q) </strong>中的给定单词，对于其中的其他单词<strong class="iy hi"> (K) </strong>，我们得到它<strong class="iy hi"> (V) </strong>对其他单词的相关性和依赖性。对于Q、K和v，使用<strong class="iy hi">不同的权重矩阵</strong>进行多次这种自我关注过程，从而形成多头关注层。</p><p id="0c36" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是注意力层的一百英尺抽象。作为多头注意力层的结果，我们得到多个注意力矩阵。</p><p id="57b6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在该架构中，我们可以看到解码器中还有另外两个关注层。</p><h2 id="f6ae" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">掩蔽的多头注意力</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es me"><img src="../Images/e7aa3f5475c06e556eb8f017b3e04572.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*RT6wpUQsr_OLBLdbZAxIxw.jpeg"/></div></figure><p id="0619" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是我们解码器端的第一层关注点。但为什么是* <strong class="iy hi">掩蔽注意力* </strong>？</p><p id="1927" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因为，在输出的情况下，如果当前单词可以访问它后面的所有单词，那么<strong class="iy hi">它不会学到</strong>任何东西。它会直接建议输出这个单词。但是通过屏蔽，我们隐藏了当前单词后面的单词。因此，它将有空间<strong class="iy hi">预测到目前为止哪个单词对给定的单词和句子有意义</strong>。它已经有了当前单词的嵌入和位置信息，所以<strong class="iy hi">我们让它使用之前已经看到的所有单词有意义</strong>使用<strong class="iy hi"> Q，K和V </strong>向量，并找出最可能的下一个单词。</p><h2 id="55e1" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">编码器-解码器注意</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mf"><img src="../Images/19e06b4127a3eadbe1147e9e246e8d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*-ElLIU3xQwJeJeHcOn2rFQ.jpeg"/></div></figure><p id="f0b9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">解码器端的下一个多头关注层从编码器端获取<strong class="iy hi">两个输入(K，V ),从解码器</strong>的前一个关注层获取<strong class="iy hi">另一个输入(Q)。现在，它可以访问来自输入和输出</strong>的<strong class="iy hi">注意值。基于来自输入和输出的当前注意力信息，它现在将进行两种语言之间的<strong class="iy hi">交互</strong>并且<strong class="iy hi">学习输入句子中的每个单词与输出句子</strong>之间的关系。</strong></p><h2 id="83f7" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">残留层</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mg"><img src="../Images/45b005819581b0bd68b846e2482eccac.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*cbEB6WEvgoHBfnq9KVlBqA.jpeg"/></div></figure><p id="e123" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这些关注层将返回一组关注矩阵，该矩阵将与实际输入相加，并将执行<strong class="iy hi">层/批次标准化</strong>。这种标准化有助于<strong class="iy hi">平滑损失表面，因此在使用较大的学习率时很容易优化</strong>。</p><h2 id="532a" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">前馈层</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mh"><img src="../Images/aa0f8a3e1f433960d9e90e8b8d4721e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*FwV_kttK9eeol-t0c1PfJA.jpeg"/></div></figure><p id="b1bc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在编码器块中，前馈网络是一个简单的神经网络，它获取平均的注意力值，然后<strong class="iy hi">将它们转换成更容易被下一层</strong>消化的形式。它可以是顶部的另一个编码器层，也可以传递到解码器端的编码器-解码器关注层。</p><p id="5e81" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，在解码器模块中，我们有另一个前馈网络来完成相同的工作，并将转换后的注意力值传递给顶部的下一个解码器层或线性层。</p><p id="cd87" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">神奇的时刻</strong>就发生在这一层。由于<strong class="iy hi">每个单词都可以通过</strong>神经网络<strong class="iy hi">独立传递</strong>其关注值，我们在此引入<strong class="iy hi">甜蜜并行化</strong>。</p><p id="3e9f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正因为如此，我们可以同时传递输入句子中的所有单词，编码器可以并行处理所有单词并给出编码器输出。</p><h2 id="6edb" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">输出线性图层和softmax概率</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mi"><img src="../Images/81a7aeb92f7cdc44ef3bd45fcea4d341.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*GgGMerbjBqC13fni2iyl6g.jpeg"/></div></figure><p id="0803" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在所有的解码器端处理完成后，我们有了带有线性层和softmax层的后处理层。线性层用于<strong class="iy hi">将来自神经网络的注意力值</strong>展平为输出语言中所有单词的<strong class="iy hi">大小。在这之后，我们应用softmax，找到所有单词的概率，从中我们得到最可能的单词。这只是预测下一个可能的字</strong>作为解码器层输出的<strong class="iy hi">概率。</strong></p><h1 id="15e0" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated"><strong class="ak">整体画面</strong></h1><p id="c697" class="pw-post-body-paragraph iw ix hh iy b iz kt ii jb jc ku il je jf kv jh ji jj kw jl jm jn kx jp jq jr ha bi translated">现在让我们快速看一下整个过程。</p><h2 id="f490" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">编码器</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mj"><img src="../Images/6295944b7da654dcce969db67b716576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7cNjV84S-RyVz5PbpvSpZQ.jpeg"/></div></div></figure><p id="d817" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们从输入的句子中取出每个单词，并行地传递给它们<strong class="iy hi">。</strong>我们把<strong class="iy hi"> </strong>这个词的<strong class="iy hi">嵌入</strong>和<strong class="iy hi"> </strong>我们加上<strong class="iy hi">的位置</strong>信息给<strong class="iy hi">上下文</strong>。然后我们有多头注意力层，它学习<strong class="iy hi">与其他单词的相关性</strong>，从而产生<strong class="iy hi">多个注意力向量</strong>。这些然后被平均，并且一个<strong class="iy hi">标准化</strong>层被应用来减轻优化。这些又被传递到<strong class="iy hi">前馈网络，该网络将这些值转换为</strong>顶部的下一个编码器或编码器-解码器关注层可读的维度。</p><h2 id="6cdd" class="lo kc hh bd kd lp lq lr kh ls lt lu kl jf lv lw kn jj lx ly kp jn lz ma kr mb bi translated">解码器</h2><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mk"><img src="../Images/1fb0bf88c634b9d52a196276208bdd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nF4F3zp7popDEQbnN4FJ3Q.jpeg"/></div></div></figure><p id="e11f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们有一个类似的单词嵌入和添加上下文的预处理步骤。然后，我们有一个<strong class="iy hi">掩蔽注意力</strong>层，它在输出句子的<strong class="iy hi">当前单词和它已经看到的所有先前单词</strong>中学习注意力，并且不允许接下来的单词。然后一层<strong class="iy hi">归一化</strong>完成。现在，我们将编码器层对于<strong class="iy hi">键的输出，值</strong>向量与解码器关注值一起作为<strong class="iy hi">查询</strong>到下一个关注层。现在，输入和输出语言之间的实际交互<strong class="iy hi">发生了，这导致算法的<strong class="iy hi">更好地理解语言翻译的</strong>。</strong></p><p id="ad14" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然后，我们有另一个前馈网络，它将转换后的输出传递给一个线性层，使注意力值变平。然后使用<strong class="iy hi"> softmax层来获得输出语言中所有单词的下一次出现的概率</strong>。从这里开始，概率最高的<strong class="iy hi">单词将是输出的</strong>。</p><h1 id="a9f3" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">编码器和解码器的堆叠</h1><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ml"><img src="../Images/ceef04541b4de89f4d625ff5750c6fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*I9kMRRxQFzqocypa.jpg"/></div></figure><p id="d7d7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">将编码器和解码器堆叠起来也是有效的，因为它导致<strong class="iy hi">更好地学习任务的</strong>并且<strong class="iy hi">增强算法的预测能力</strong>。在实际的论文中，谷歌已经堆叠了<strong class="iy hi"> 6个</strong>编码器和解码器。但也要确保，它不会过度，使培训过程昂贵。</p><h1 id="1086" class="kb kc hh bd kd ke kf kg kh ki kj kk kl in km io kn iq ko ir kp it kq iu kr ks bi translated">结论</h1><p id="2bb1" class="pw-post-body-paragraph iw ix hh iy b iz kt ii jb jc ku il je jf kv jh ji jj kw jl jm jn kx jp jq jr ha bi translated">自从谷歌推出《变形金刚》的那一天起，它就一直是自然语言处理领域的革命性产品。它被用于各种语言模型的开发，包括受到高度赞扬的<strong class="iy hi"> BERT、GPT2和GPT3 </strong>，它们在所有语言任务中都优于以前的模型。了解基础架构肯定会让你在游戏中保持领先。</p><p id="383d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢你阅读这篇文章！我希望这篇文章能让你对变形金刚的整体架构有所了解。</p><p id="c48f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请随意分享你的想法/评论。让我们一起让编码变得有趣！</p><figure class="ky kz la lb fd lc er es paragraph-image"><a href="https://faun.to/bP1m5"><div class="er es mm"><img src="../Images/8f497a40fc9423af91397c9e53e5ff35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCiLLad3dvZLwBa-B5cAVQ.png"/></div></a></figure><p id="3652" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">加入牧神:<a class="ae ln" href="https://faun.to/i9Pt9" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">网站</strong> </a> <strong class="iy hi"> </strong>💻<strong class="iy hi">|</strong>|<a class="ae ln" href="https://faun.dev/podcast" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">播客</strong></a><strong class="iy hi"/>🎙️<strong class="iy hi">|</strong><a class="ae ln" href="https://twitter.com/joinfaun" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">推特</strong> </a> <strong class="iy hi"> </strong>🐦<strong class="iy hi"> | </strong> <a class="ae ln" href="https://www.facebook.com/faun.dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">脸书</strong> </a> <strong class="iy hi"> </strong>👥<strong class="iy hi">|</strong><a class="ae ln" href="https://instagram.com/fauncommunity/" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">insta gram</strong></a><strong class="iy hi"/>📷| <a class="ae ln" href="https://www.facebook.com/groups/364904580892967/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi"> Facebook群</strong></a><strong class="iy hi"/>🗣️<strong class="iy hi">|</strong><a class="ae ln" href="https://www.linkedin.com/company/faundev" rel="noopener ugc nofollow" target="_blank"><strong class="iy hi">LinkedIn群</strong> </a> <strong class="iy hi"> </strong>💬<strong class="iy hi"> | </strong> <a class="ae ln" href="https://faun.dev/chat" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">松弛</strong> </a>📱<strong class="iy hi"> | </strong> <a class="ae ln" href="https://thechief.io" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">云原生</strong> <strong class="iy hi">新闻</strong> </a> <strong class="iy hi"> </strong>📰<strong class="iy hi"> | </strong> <a class="ae ln" href="https://linktr.ee/faun.dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">更有</strong> </a> <strong class="iy hi">。 </strong></p><p id="3062" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果这篇文章有帮助，请点击拍手👏按钮几下，以示你对作者的支持👇</p><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hi fi z dy mv ea eb mw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne lk mq"/></div></div></a></div></div></div>    
</body>
</html>