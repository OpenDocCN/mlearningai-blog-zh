<html>
<head>
<title>StoryDALL-E: When AI starts Storytelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当人工智能开始讲故事时</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/storydall-e-8d60b6577f5a?source=collection_archive---------4-----------------------#2022-09-29">https://medium.com/mlearning-ai/storydall-e-8d60b6577f5a?source=collection_archive---------4-----------------------#2022-09-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="be34" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">浅谈DALL-E的新故事可视化版本</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/3a2aa911f635d062219cfe63a3414661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5YlTmrreaPU_mz3yYt01A.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by Author (generated by AI)</figcaption></figure><p id="19bf" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi ki translated">无论是技术人员还是非技术人员，我们在互联网上的任何地方都能看到人们痴迷于新的文本到图像的模式。从今年4月的DALL-E 2开始，涨势已经从谷歌的Imagen上升到现在的开源模型，如StableDiffusion和MidJourney，后者实际上没有限制。你想到了什么，然后立刻创造出来。你猜怎么着？现在，这些艺术品可以在Playform这样的平台上购买和创作。</p><p id="d962" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">随着所有新的研究都朝着这个方向发展，如果另一个令人兴奋的事情从下面出现了呢？是的，现在AI也会讲故事了！这是StoryDALL-E在2022年9月的论文中提出的- <a class="ae kr" href="https://paperswithcode.com/paper/storydall-e-adapting-pretrained-text-to-image" rel="noopener ugc nofollow" target="_blank"> <em class="ks"> StoryDALL-E:为故事延续</em> </a>调整预训练的文本到图像转换器，这是一个巨大的潜力。它能够通过从初始帧复制相关元素来连续生成图像，这是图像生成中的一个突破。</p><h2 id="1346" class="kt ku hh bd kv kw kx ky kz la lb lc ld jv le lf lg jz lh li lj kd lk ll lm ln bi translated">StoryDALL-E:有什么新消息？</h2><p id="92ee" class="pw-post-body-paragraph jm jn hh jo b jp lo ii jr js lp il ju jv lq jx jy jz lr kb kc kd ls kf kg kh ha bi translated">正如我们所知，在文本到图像合成方面正在进行激烈的研究，这导致了许多复杂的GAN架构和性能升级，但迄今为止在故事可视化方面几乎没有进展。StoryDALL-E可能是同类产品中第一个基于文本转录和初始输入图像产生类似故事的序列图像。</p><p id="aa07" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">StoryDALL-E的基本工作原理是将地面真实场景和文本转录作为输入，并从中生成后续场景。所发生的是，当故事生成时，模型从源帧中提取视觉元素，如人物、物体和场景，并将它们复制到后续帧中。它也可以创造全新的角色或看不见的元素，但目前为止效果还令人满意。</p><p id="cc0f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi">建筑</strong></p><p id="33ce" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">在深入研究之前，我们需要大致了解DALL-E是如何工作的。<em class="ks"> DALL-E是一个解码器专用的稀疏变换器</em>。120亿参数架构主要使用64个注意层工作。简单来说，DALL-E是Open-AI的GPT-3的扩展。GPT 3号接收文本，并以文本形式作出回应，而另一方面，DALL-E号以图像形式作出回应。它主要由两部分组成:将文本转换为潜在空间的转换器部分和将潜在空间转换为最终图像的VAE部分。一种文字到像素的映射。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lt"><img src="../Images/fb4c770ffee94a2a85c54e232d89beda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DDfX62fkn7_hsQAAl-aoBQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">StoryDALL-E architecture for prompt tuning setting. (Image strictly from the original paper)</figcaption></figure><p id="8a6a" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">看起来StoryDALL-E是一个创造性的微调DALL-E架构本身，但有一些额外的实现。在这里，他们采用了一个预训练的DALL-E，带有插件，使其能够根据源帧有条件地生成图像序列。以下是插件:</p><p id="2ac6" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi"> 1。全球故事编码器</strong>:原文称之为“<em class="ks">一个基于自我关注的编码器</em>”。Global Story Encoder没有采用顺序处理的方法，而是依靠自我关注模块从单词嵌入中生成故事嵌入。它根据输入框架和转录提供预期故事的上下文。</p><p id="8c90" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi"> 2。改装</strong>:在微调过的DALL-E架构中，交叉关注块被插入64个自关注模块，在原文中称为“<em class="ks">改装</em>”。这使得模型能够在从源图像生成的后续图像中创建新的看不见的元素。</p><p id="b515" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><strong class="jo hi">数据集&amp;结果</strong></p><p id="7b1a" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">StoryDALL-E已经在两个数据集上进行训练- <a class="ae kr" href="https://paperswithcode.com/sota/story-visualization-on-pororo" rel="noopener ugc nofollow" target="_blank"> PororoSV </a>和<a class="ae kr" href="http://flintstones-dataset-dev-rev.s3-website-us-west-2.amazonaws.com/structure/" rel="noopener ugc nofollow" target="_blank"> FlintStonesSV </a>。本文还提出了一个新的数据集来评估模型的泛化能力，<a class="ae kr" href="https://paperswithcode.com/dataset/didemo" rel="noopener ugc nofollow" target="_blank"> DiDeMo </a>。与包含后续帧中重复出现的字符的前两个数据集相比，这个新数据集有助于判断其创建全新字符的能力。</p><p id="8f1f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这里有一些直接来自<a class="ae kr" href="https://arxiv.org/pdf/2209.06192v1.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>的精彩结果。该演示目前不可用，因此新图像将在公开发布后立即包含在内。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lu"><img src="../Images/fcb9835f045e94fde7a1d7d1289ebb45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*rJ-YhJNg4Ke_lSQDgE38kw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">(Image strictly from the original paper implementation)</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lv"><img src="../Images/dab9f426d3ec58d47dddb5601af28621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*ITsi4hDnDeGdcRkWpiT9rQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">(Image strictly from the original paper implementation)</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/bb5e1023a6dfdd3b6d55b3f90c6680ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U9HLlb3-fStni8yDZ1xcUA.png"/></div></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lx"><img src="../Images/5a62fb6efa44eb054ed8af2dc7752e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wx0kj2hz5gHJKHR6RrehcQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">(Image strictly from the original paper implementation)</figcaption></figure></div><div class="ab cl ly lz go ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ha hb hc hd he"><h2 id="a502" class="kt ku hh bd kv kw kx ky kz la lb lc ld jv le lf lg jz lh li lj kd lk ll lm ln bi translated"><strong class="ak">结论</strong></h2><p id="2e57" class="pw-post-body-paragraph jm jn hh jo b jp lo ii jr js lp il ju jv lq jx jy jz lr kb kc kd ls kf kg kh ha bi translated">随着图像生成领域的研究越来越多，我们听到了许多令人兴奋的消息。现在人工智能可以创造一个完整的短篇故事，你一定会欣喜若狂。当考虑在故事情节中创造全新角色的能力时，StoryDALL-E还有很多要走的路，但随着它开始的更多研究，毫无疑问我们可以在不久的将来看到杰作。别忘了查看StoryDALL-E的原始论文和官方GitHub <a class="ae kr" href="https://github.com/adymaharana/storydalle" rel="noopener ugc nofollow" target="_blank">回购</a></p><p id="a68f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">本文中的所有数字和数据都来自参考文献中的原始论文。</p><h2 id="1b60" class="kt ku hh bd kv kw kx ky kz la lb lc ld jv le lf lg jz lh li lj kd lk ll lm ln bi translated">参考</h2><p id="64f1" class="pw-post-body-paragraph jm jn hh jo b jp lo ii jr js lp il ju jv lq jx jy jz lr kb kc kd ls kf kg kh ha bi translated">[1].Adyasha Maharana、Darryl Hannan和Mohit Bansal。<a class="ae kr" href="https://arxiv.org/pdf/2209.06192v1.pdf" rel="noopener ugc nofollow" target="_blank"> StoryDALL-E:为故事延续调整预训练的文本到图像转换器</a> (2022)。arXiv预印本arXiv:2209.06192</p><p id="b575" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">[2].Lisa Anne Hendricks，Oliver Wang，Eli Shechtman，Josef Sivic，Trevor Darrell，Bryan Russell，加州大学伯克利分校，Adobe Research，INRIA。<a class="ae kr" href="https://arxiv.org/pdf/1708.01641v1.pdf" rel="noopener ugc nofollow" target="_blank">用自然语言定位视频中的瞬间</a> (2017)。arXiv预印本arXiv:1708.01641</p><p id="f539" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">[3].Aditya Ramesh，Mikhail Pavlov，Gabriel Goh，Scott Gray，Chelsea Voss，Alec拉德福德，陈唐山，Ilya Sutskever。<a class="ae kr" href="https://arxiv.org/pdf/2102.12092" rel="noopener ugc nofollow" target="_blank">零镜头文本到图像生成</a> (2021)。arXiv预印本arXiv:2102.12092</p><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw jg mi"/></div></div></a></div></div></div>    
</body>
</html>