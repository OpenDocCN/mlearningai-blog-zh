# ICRA 论文摘要:“VIODE:应对动态环境中视觉惯性里程计挑战的模拟数据集”

> 原文：<https://medium.com/mlearning-ai/icra-paper-summary-viode-a-simulated-dataset-to-address-the-challenges-of-visual-inertial-2be2b98c906c?source=collection_archive---------4----------------------->

在过去的十年里，人们对计算机视觉的兴趣和新颖研究急剧增加，尤其是当它应用于自主机器人时。今年当然也不例外。研究视觉引导机器人的研究人员正专注于使视觉感知网络更加鲁棒、快速和准确。

作为 IEEE 机器人和自动化学会旗舰会议[机器人和自动化国际会议(ICRA)](https://www.ieee-icra.org/index.aspx) 的一部分，ICRA 将四篇论文提名为 2021 年机器人视觉类别最佳论文奖的决赛论文。在这篇文章中，我总结了一篇决赛论文，它提出了一个数据集来改进混合静态&动态环境中的 VIO 算法。复杂(恶劣、动态和混乱)环境中的视觉导航是一个活跃的研究领域，我对此很感兴趣，所以我决定更深入地研究这篇论文。

# 总结:

*作者:Minoda Koji，Fabian Wüest，Valentin Schilling，Dario Floreano，Takehisa Yairi*

本文旨在解决在现有数据集上定量评估现有 VIO(视觉惯性里程计)方法鲁棒性的相关挑战，这些方法无法有效模拟动态环境。他们的研究很新颖，因为像城市地区这样的动态环境对 VIO 算法的良好表现提出了挑战。他们提出的数据集在广泛的环境和动态水平上测试了 VIO 算法。

他们在研究中做出了三大贡献:

*   提出了模拟视觉惯性里程计数据集(VIODE)，其中他们在空间中一致地添加四个级别的动态对象，以测试视觉里程计(VO)和 VIO 算法的性能。
*   通过系统地添加动态对象，实验演示了两种最先进的 VIO 算法([VINS-莫诺](https://github.com/HKUST-Aerial-Robotics/VINS-Mono)和[罗维奥](https://github.com/ethz-asl/rovio))的退化。基于准确性、鲁棒性和计算效率，他们在现有的单目 VIO 算法中选择了 VINS-单声道和 ROVIO 作为两种性能最好的方法。
*   证明在 VIO 算法中加入语义信息可以减轻 VINS-莫诺和罗维奥的退化。

**相关工作:**据他们所知，没有公开可用的数据集可以在动态场景中测试 VIO 算法的性能。大多数记录在无人机或手持设备上的数据集包含很少的动态对象。模拟室外环境的数据集也不足以挑战 VIO 算法的性能，因为它们不够动态。在 VIODE 创建之前，具有最动态场景的数据集 ADVIO 也不包含动态级别的定量信息。然而，VIODE 对动态级别进行了清晰的分层，并对其进行了量化定义。

**关于 VIODE 数据集:** VIODE 是用 [AirSim](https://microsoft.github.io/AirSim/) 创建的，在 VIO 应用中模拟四旋翼飞行器(运载工具)上的运动。数据集包含来自三种环境的数据:

*   parking_lot:从室内停车场生成的序列。
*   city_day:白天的现代城市环境，包括树木和高楼。
*   city_night:和 city_day 一样的城市环境，但是在晚上。

他们通过收集 IMU 和地面真实 6-DoF 姿态以及捕捉图像和分割掩模来生成数据集。在收集 IMU 数据和姿态时，他们为每个环境生成四个轨迹。他们在每个环境中使用立体摄像机捕捉 RGB 图像和地图四次，以生成环境和动态级别的不同组合的序列。在论文中，他们还讨论了传感器设置和校准的细节，包括 IMU 和立体摄像机的采样速率。

**定义和评估动态水平:**有四种不同的动态水平:无、低、中和高。每一层都由越来越多的移动物体组成。他们决定，为了公平地评估 VIO 在动态环境中的表现，他们比较序列，其中唯一的区别是动态水平。虚幻引擎 4 中提供的具有不同纹理和大小的六种类型的汽车被用作动态对象，因为它们是常见的动态对象。对于四个动态级别中的每一个，环境都有静态和动态对象(除了只有静态对象的“无”)。动态级别是可量化的，不仅仅是动态对象的数量。该指标由两个动态比率定义:

*   基于像素的动态速率:基于地面真实实例分割的动态对象占据 FOV 的程度。它不包含来自周围物体的速度信息。
*   基于光流(OF-based)的动态速率:说明基于像素的速率中丢失的速度信息。

**在 VIODE 上评估 VIO 方法:**他们在 VIODE 数据集上应用了两种最先进的 VIO 算法。为了分析它们在数据集上的性能，使用了许多参数:

*   绝对轨迹误差(ATE):直接测量地面真实轨迹和估计轨迹的点之间的差异。
*   相对姿态误差(RPE):通过计算时间戳对之间的相对运动误差来确定子轨迹的局部精度。
*   退化率:他们引入的一个度量，即比率 r_d = ate_high/ATE_none，其中 ATE_high 和 ATE_none 分别是高序列和低序列的 ATE。这个度量是重要的，因为它证明了与静态环境相比，给定 VIO 算法在高度动态环境中的鲁棒性。

他们评估 VINS-莫诺和 ROVIO 十次，因为算法的性能是不确定的。他们发现，随着动态级别的增加，两种算法的性能都越来越差。ROVIO 和 VINS 单声道的降解率始终高于 5.0。在这些环境中，城市白天降解率最高，城市夜晚降解率最低。他们进一步证明，只有当 FOV 中存在动态对象时，才会出现错误的估计。这一点在两种动态心率类型和 VINS-莫诺估计的 RPE 之间的对应性上是明显的。他们的结果支持 VIO 性能下降是由于动态水平的假设。结果还表明，他们的数据集在动态环境中稳健地测试了 VIO 性能。

**使用 VINS 掩码提高 VIO 性能:**在表明他们可以使用数据集降低动态环境中的 VIO 性能后，他们提出了一种提高性能的方法。他们在 VINS 单声道的基础上开发了 VINS 掩码，只是它使用语义信息来提高性能。在 VINS 掩码中，他们在特征提取之前屏蔽掉动态对象区域。在对 VIODE 上的 VINS 掩码进行评估后，他们发现性能完全独立于动态电平，并且 VINS 掩码可以抑制由动态电平变化引起的漂移。

**未来方向:**一个必要的下一步是通过使用[语义分割](https://nanonets.com/blog/semantic-image-segmentation-2020/)而不是地面真实分割来评估 VINS 掩码。另一个未来的步骤是区分静态和动态对象，以更鲁棒地评估动态环境中的 VIO。最后，他们希望通过向数据集引入更多传感器并使用真实的 IMU 和轨迹数据而不是模拟数据来改进 VIODE 数据集。

**进一步讨论:**他们的研究很新颖，基于他们的实验，他们展示了 VIODE 数据集如何在动态环境中暴露 VIO 算法的漏洞。暴露这些漏洞对于机器人在特定环境中导航至关重要。作为一名密歇根州的司机，这篇论文真的很吸引我(我会爱上那些密歇根州的坑坑洼洼！).机器人应该能够在各种条件下工作，包括恶劣的天气、恶劣的路况以及静态和动态物体混合的环境。我有兴趣了解这个数据集如何应用于其他应用，包括未来航天器的深空探索、家用机器人和医院机器人。将这个数据集扩展到包括危险对象，以暴露恶劣条件下的算法漏洞，这也是很酷的。这些算法可能会结合用于坑洞和湿地检测的[YOLO](https://ieeexplore.ieee.org/document/9432186)。

# 更多关于 ICRA 的信息

ICRA 是 IEEE 机器人自动化学会的首要年度会议。机器人、自动化和社会影响是会议的焦点，会议汇集了顶级研究人员和公司来分享他们的进展。会议由研讨会、论文会议和机器人挑战组成。第 39 届 ICRA 2022 大会将于 5 月 23 日至 27 日在宾夕法尼亚州费城举行。如果你对 ICRA 2022 感兴趣，看看他们的[网站](https://www.icra2022.org/)！

# 引文

原文：<https://arxiv.org/pdf/2102.05965v1.pdf>

ICRA 2021(视频和论文):[https://robo hub . org/IEEE-icra-2021-有视频和论文的奖项/](https://robohub.org/ieee-icra-2021-awards-with-videos-and-papers/)

—

感谢阅读！我是一名软件工程师，在迪士尼流媒体公司为[迪士尼捆绑包](https://www.disneyplus.com/welcome/disney-hulu-espn-bundle?cid=DSS-Search-Google-71700000059616273-&s_kwcid=AL!8468!3!541984743040!b!!g!!%2Bdisney%20%2Bbundle&gclid=CjwKCAjwyvaJBhBpEiwA8d38vKmDgRQnVnbt1FMRYhg5GFHcdHPNP0fzo9X1010fSKZUe9PB4pc3-RoCIykQAvD_BwE&gclsrc=aw.ds)的定制操作系统工作。我拥有密歇根大学的计算机工程学士学位。我对嵌入式系统和自治也有浓厚的兴趣。如果你想了解更多关于我的信息，请访问我的网站 [ishakbhatt.dev](https://ishakbhatt.dev/) ！