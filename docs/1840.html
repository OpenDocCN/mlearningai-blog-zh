<html>
<head>
<title>Vision Transformers from Scratch (PyTorch): A step-by-step guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉变形金刚从零开始(PyTorch):一步一步的指南</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c?source=collection_archive---------0-----------------------#2022-02-03">https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c?source=collection_archive---------0-----------------------#2022-02-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d898" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">视觉变形金刚(ViT)，自Dosovitskiy等人推出以来。艾尔。<a class="ae jc" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">参考文献</a>2020年，在计算机视觉领域占据主导地位，首先在图像分类方面获得最先进的性能，随后在其他任务中也是如此。</p><p id="058e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，与其他架构不同，它们有点难以理解，尤其是如果您还不熟悉自然语言处理(NLP)中使用的转换器模型的话。</p><p id="f28a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您对计算机视觉(CV)感兴趣，但仍然不熟悉ViT模型，请不要担心！我也是！</p><p id="59be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这段简短的文字中，我将向您展示我是如何从头开始实现我的第一个ViT的(使用PyTorch)，并且我将指导您完成一些调试，这将帮助您更好地可视化ViT中到底发生了什么。</p><p id="ee46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然这篇文章是专门针对ViT的，但是你在这里会发现一些概念，例如多头自我注意(MSA)块，它们在人工智能的各个子领域中都存在，并且当前非常相关，例如CV、NLP等</p><h1 id="d7cc" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">定义任务</h1><p id="51d2" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">因为目标只是学习更多关于ViT架构的知识，所以明智的做法是选择一个简单且众所周知的任务和数据集。在我们的例子中，任务是由伟大的<strong class="ig hi"> LeCun等人对流行的MNIST数据集进行图像分类。艾尔。</strong><a class="ae jc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">参考</a>。</p><p id="b0e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你还不知道，MNIST是一个手写数字([0–9])的数据集，全部包含在28x28二进制像素图像中。这个任务对于今天的算法来说是微不足道的，所以我们可以预期一个正确的实现将会执行得很好。</p><p id="f4c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从进口开始:</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="b2f3" class="kp je hh kl b be kq kr l ks kt">import numpy as np<br/><br/>from tqdm import tqdm, trange<br/><br/>import torch<br/>import torch.nn as nn<br/>from torch.optim import Adam<br/>from torch.nn import CrossEntropyLoss<br/>from torch.utils.data import DataLoader<br/><br/>from torchvision.transforms import ToTensor<br/>from torchvision.datasets.mnist import MNIST<br/><br/>np.random.seed(0)<br/>torch.manual_seed(0)</span></pre><p id="273d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们创建一个<strong class="ig hi">主函数</strong>，它准备MNIST数据集，实例化一个模型，并为其训练5个时期。之后，在测试集上测量损耗和精度。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="4303" class="kp je hh kl b be kq kr l ks kt">def main():<br/>    # Loading data<br/>    transform = ToTensor()<br/><br/>    train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)<br/>    test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)<br/><br/>    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)<br/>    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)<br/><br/>    # Defining model and training options<br/>    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>    print("Using device: ", device, f"({torch.cuda.get_device_name(device)})" if torch.cuda.is_available() else "")<br/>    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)<br/>    N_EPOCHS = 5<br/>    LR = 0.005<br/><br/>    # Training loop<br/>    optimizer = Adam(model.parameters(), lr=LR)<br/>    criterion = CrossEntropyLoss()<br/>    for epoch in trange(N_EPOCHS, desc="Training"):<br/>        train_loss = 0.0<br/>        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1} in training", leave=False):<br/>            x, y = batch<br/>            x, y = x.to(device), y.to(device)<br/>            y_hat = model(x)<br/>            loss = criterion(y_hat, y)<br/><br/>            train_loss += loss.detach().cpu().item() / len(train_loader)<br/><br/>            optimizer.zero_grad()<br/>            loss.backward()<br/>            optimizer.step()<br/><br/>        print(f"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}")<br/><br/>    # Test loop<br/>    with torch.no_grad():<br/>        correct, total = 0, 0<br/>        test_loss = 0.0<br/>        for batch in tqdm(test_loader, desc="Testing"):<br/>            x, y = batch<br/>            x, y = x.to(device), y.to(device)<br/>            y_hat = model(x)<br/>            loss = criterion(y_hat, y)<br/>            test_loss += loss.detach().cpu().item() / len(test_loader)<br/><br/>            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()<br/>            total += len(x)<br/>        print(f"Test loss: {test_loss:.2f}")<br/>        print(f"Test accuracy: {correct / total * 100:.2f}%")</span></pre><p id="2a2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了这个模板，从现在开始，我们可以只关注模型(ViT)，它必须用形状(<strong class="ig hi"> N </strong> x 1 x 28 x 28)对图像进行分类。</p><p id="0041" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们从定义一个空的<em class="ku"> nn开始。模块</em>。然后，我们将逐步填充这个类。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="b9b7" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>  def __init__(self):<br/>    # Super constructor<br/>    super(MyViT, self).__init__()<br/><br/>  def forward(self, images):<br/>    pass</span></pre><h1 id="0966" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">前进传球</h1><p id="2c22" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">由于Pytorch和大多数DL框架都提供了<em class="ku">自动签名的</em>计算，我们只关心实现ViT模型的向前传递。因为我们已经定义了模型的优化器，所以框架将负责反向传播梯度和训练模型的参数。</p><p id="6121" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当实现一个新的模型时，我喜欢在某个标签上保存一个架构的图片。这是我们从<strong class="ig hi"> Bazi et得到的ViT的参考图片。铝</strong> (2021)[ <a class="ae jc" href="https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification" rel="noopener ugc nofollow" target="_blank">参考</a>:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kv"><img src="../Images/3f9f0e3248e7dd9154e0f4b4ee5a8827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">The architecture of the ViT with specific details on the transformer encoder and the MSA block. Keep this picture in mind. Picture from <a class="ae jc" href="https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification" rel="noopener ugc nofollow" target="_blank">Bazi et. al.</a></figcaption></figure><p id="bb9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过该图，我们看到输入图像(a)被“切割”成大小相等的子图像。</p><p id="c2cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个这样的子图像经历线性嵌入。从那时起，每个子图像只是一个一维向量。</p><p id="63a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后将位置嵌入添加到这些向量(记号)中。位置嵌入允许网络知道每个子图像最初在图像中的位置。没有这些信息，网络将无法知道每个这样的图像将被放置在哪里，从而导致潜在的错误预测！</p><p id="f80b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，这些令牌与特殊的分类令牌一起被传递到变换器编码器块，每个编码器块包括:层标准化(LN ),随后是多头自关注(MSA)和残差连接。然后是第二个LN，一个多层感知器(MLP)，再次是一个剩余连接。这些积木是背靠背连接的。</p><p id="22ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，分类MLP块仅用于特殊分类标记上的最终分类，该特殊分类标记在该过程结束时具有关于图像的全局信息。</p><p id="5760" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们按照<strong class="ig hi">的6个主要步骤来构建ViT。</strong></p><h2 id="39bd" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤1:修补和线性映射</h2><p id="be9e" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">transformer编码器的开发考虑到了序列数据，例如英语句子。然而，图像不是序列。它只是，嗯…一个图像…那么我们如何对一个图像“排序”？我们把它分解成多个子图，把每个子图映射成一个向量！</p><p id="b705" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们通过简单地将大小为(N，C，H，W)(在我们的例子中为(N，1，28，28))的输入整形为大小为(N，#面片，面片维数)的输入来实现，其中面片的维数被相应地调整。</p><p id="533b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个例子中，我们将每个(1，28，28)分成<strong class="ig hi"> 7x7个小块</strong>(因此，每个大小为4x4)。也就是说，我们将从单个图像中获得7×7 = 49个子图像。</p><p id="7e76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，我们将输入(N，1，28，28)整形为:</p><blockquote class="lv"><p id="c4cb" class="lw lx hh bd ly lz ma mb mc md me jb dx translated"><em class="mf"> (N，PxP，HxC/P x WxC/P) = (N，7x7，4x4) = (N，49，16) </em></p></blockquote><p id="4a12" class="pw-post-body-paragraph ie if hh ig b ih mg ij ik il mh in io ip mi ir is it mj iv iw ix mk iz ja jb ha bi translated">请注意，虽然每个小块都是大小为1x4x4的图片，但我们将其展平为16维向量。此外，在这种情况下，我们只有一个单一的颜色通道。如果我们有多个颜色通道，这些通道也会被展平到矢量中。</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es ml"><img src="../Images/8e9405e6a9975e637f6e8f4bc9d56f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*CFbOxEuvo-Pgq7ETIrt0Eg.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Raffiguration of how an image is split into patches. The 1x28x28 image is split into 49 (7x7) patches, each of size 16 (4x4x1)</figcaption></figure><p id="9561" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们修改我们的<strong class="ig hi"> MyViT </strong>类，只实现修补。我们创建一个从头开始操作的方法。请注意，这是一种执行操作的低效方式，但是对于学习核心概念来说，代码是直观的。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="de84" class="kp je hh kl b be kq kr l ks kt">def patchify(images, n_patches):<br/>    n, c, h, w = images.shape<br/><br/>    assert h == w, "Patchify method is implemented for square images only"<br/><br/>    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)<br/>    patch_size = h // n_patches<br/><br/>    for idx, image in enumerate(images):<br/>        for i in range(n_patches):<br/>            for j in range(n_patches):<br/>                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]<br/>                patches[idx, i * n_patches + j] = patch.flatten()<br/>    return patches</span></pre><pre class="mm kk kl km bn kn ko bi"><span id="f964" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>  def __init__(self, chw=(1, 28, 28), n_patches=7):<br/>    # Super constructor<br/>    super(MyViT, self).__init__()<br/><br/>    # Attributes<br/>    self.chw = chw # (C, H, W)<br/>    self.n_patches = n_patches<br/><br/>    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/><br/>  def forward(self, images):<br/>    patches = patchify(images, self.n_patches)<br/>    return patches</span></pre><p id="63bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类构造函数现在让类知道我们的输入图像的大小(通道数、高度和宽度)。注意，在这个实现中，<em class="ku"> n_patches </em>变量是我们将在宽度和高度上找到的面片数(在我们的例子中是7，因为我们将图像分成7x7个面片)。</p><p id="b9ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以用一个简单的主程序来测试我们的类的功能:</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="3721" class="kp je hh kl b be kq kr l ks kt">if __name__ == '__main__':<br/>  # Current model<br/>  model = MyViT(<br/>    chw=(1, 28, 28),<br/>    n_patches=7<br/>  )<br/><br/>  x = torch.randn(7, 1, 28, 28) # Dummy images<br/>  print(model(x).shape) # torch.Size([7, 49, 16])</span></pre><p id="79ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们有了展平的面片，我们可以通过线性映射来映射它们。虽然每个小块是4×4 = 16维向量，但是线性映射可以映射到任何任意大小的向量。因此，我们给我们的类构造函数添加了一个参数，称为<em class="ku"> hidden_d </em>表示“隐藏维度”。</p><p id="4732" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个例子中，我们将使用8个隐藏维度，但是原则上，任何数字都可以放在这里。因此，我们将把每个16维的面片映射到一个8维的面片。</p><p id="639c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们简单地创建一个<em class="ku">神经网络。线性</em>层，并在我们的正向函数中调用它。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="1b0c" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>  def __init__(self, chw=(1, 28, 28), n_patches=7):<br/>    # Super constructor<br/>    super(MyViT, self).__init__()<br/><br/>    # Attributes<br/>    self.chw = chw # (C, H, W)<br/>    self.n_patches = n_patches<br/><br/>    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)<br/><br/>    # 1) Linear mapper<br/>    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])<br/>    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)<br/><br/>  def forward(self, images):<br/>    patches = patchify(images, self.n_patches)<br/>    tokens = self.linear_mapper(patches)<br/>    return tokens</span></pre><p id="3ddd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，我们通过(16，8)线性映射器(或矩阵)运行(N，49，16)张量。线性运算只发生在最后一个维度上。</p><h2 id="cb65" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤2:添加分类令牌</h2><p id="1aa7" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">如果您仔细观察架构图，您会注意到还有一个“<em class="ku"> v_class </em>”令牌被传递给了Transformer编码器。这是什么？</p><p id="7915" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简单地说，这是我们添加到模型中的一个特殊令牌，它的作用是捕获关于其他令牌的信息。这将在MSA块中发生(稍后)。当关于所有其他标记的信息将出现在这里时，我们将能够仅使用这个特殊的标记来对图像进行分类。特殊令牌的初始值(馈送给变换器编码器的那个)是需要学习的模型的参数。</p><p id="4d93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个很酷的变形金刚概念！如果我们想做另一个下游任务，我们只需要为另一个下游任务添加另一个特殊的令牌(例如，将一个数字分类为高于5或低于5)和一个接受这个新令牌作为输入的分类器。很聪明，对吧？</p><p id="8d74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以向我们的模型添加一个参数，并将我们的(N，49，8)令牌张量转换为(N，50，8)张量(我们向每个序列添加特殊令牌)。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="c91c" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>  def __init__(self, chw=(1, 28, 28), n_patches=7):<br/>    # Super constructor<br/>    super(MyViT, self).__init__()<br/><br/>    # Attributes<br/>    self.chw = chw # (C, H, W)<br/>    self.n_patches = n_patches<br/><br/>    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)<br/><br/>    # 1) Linear mapper<br/>    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])<br/>    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)<br/><br/>    # 2) Learnable classifiation token<br/>    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))<br/><br/>  def forward(self, images):<br/>    patches = patchify(images, self.n_patches)<br/>    tokens = self.linear_mapper(patches)<br/><br/>    # Adding classification token to the tokens<br/>    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])<br/>    return tokens</span></pre><p id="288e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，分类标记作为每个序列的第一个标记。当我们随后检索分类令牌以提供给最终的MLP时，记住这一点很重要。</p><h2 id="b8cd" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤3:位置编码</h2><p id="430e" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">正如预期的那样，位置编码允许模型理解每个补片在原始图像中的位置。虽然理论上有可能学习这样的位置嵌入，但是Vaswani等人以前的工作。艾尔。【<a class="ae jc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">参考文献</a>建议我们可以只把正弦和余弦波相加。</p><p id="5bff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具体来说，位置编码将低频值添加到第一维度，将高频值添加到后一维度。</p><p id="11f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每个序列中，对于令牌<em class="ku"> i </em>,我们向其<em class="ku">第j个</em>坐标添加以下值:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es mn"><img src="../Images/ef81de628b5880f367482e1d7bdb2ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpRYHE0XjVkxRVKFrWkzuw.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Value to be added to the i-th tensor in its j-th coordinate. <a class="ae jc" href="https://blogs.oracle.com/ai-and-datascience/post/multi-head-self-attention-in-nlp" rel="noopener ugc nofollow" target="_blank">Image source</a>.</figcaption></figure><p id="3fd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种位置嵌入是序列中元素数量和每个元素维数的函数。因此，它总是一个二维张量或“矩形”。</p><p id="adba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一个简单的函数，在给定记号的数量和每个记号的维数的情况下，输出一个矩阵，其中每个坐标(I，j)是要添加到维度j中的记号I的值。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="9c7e" class="kp je hh kl b be kq kr l ks kt">def get_positional_embeddings(sequence_length, d):<br/>    result = torch.ones(sequence_length, d)<br/>    for i in range(sequence_length):<br/>        for j in range(d):<br/>            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))<br/>    return result<br/><br/>if __name__ == "__main__":<br/>  import matplotlib.pyplot as plt<br/><br/>  plt.imshow(get_positional_embeddings(100, 300), cmap="hot", interpolation="nearest")<br/>  plt.show()</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es mo"><img src="../Images/8d109c34551aec340e5a852727793e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*erwsFgn3I-FGzUKOIeQSAw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Heatmap of Positional embeddings for one hundred 300-dimensional samples. Samples are on the y-axis, whereas the dimensions are on the x-axis. Darker regions show higher values.</figcaption></figure><p id="1cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从我们绘制的热图中，我们看到所有的“水平线”都互不相同，因此可以区分样品。</p><p id="e296" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在线性映射和添加类标记之后，我们现在可以将这种位置编码添加到我们的模型中。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="003a" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>  def __init__(self, chw=(1, 28, 28), n_patches=7):<br/>    # Super constructor<br/>    super(MyViT, self).__init__()<br/><br/>    # Attributes<br/>    self.chw = chw # (C, H, W)<br/>    self.n_patches = n_patches<br/><br/>    assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)<br/><br/>    # 1) Linear mapper<br/>    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])<br/>    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)<br/><br/>    # 2) Learnable classifiation token<br/>    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))<br/><br/>    # 3) Positional embedding<br/>    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))<br/>    self.pos_embed.requires_grad = False<br/><br/>  def forward(self, images):<br/>    patches = patchify(images, self.n_patches)<br/>    tokens = self.linear_mapper(patches)<br/><br/>    # Adding classification token to the tokens<br/>    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])<br/><br/>    # Adding positional embedding<br/>    pos_embed = self.pos_embed.repeat(n, 1, 1)<br/>    out = tokens + pos_embed<br/>    return out</span></pre><p id="fa88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将位置嵌入定义为模型的一个参数(我们不会通过将其requires_grad设置为False来更新它)。注意，在前向方法中，由于记号的大小为(N，50，8)，我们必须重复N次(50，8)位置编码矩阵。</p><h2 id="4982" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤4:编码器模块(第1/2部分)</h2><p id="a15b" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">这可能是最难的一步。编码器模块将我们的当前张量[N，S，D]作为输入，并输出相同维数的张量。</p><p id="cb28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编码器模块的第一部分对我们的令牌应用层标准化，然后是多头自我关注，最后添加一个残差连接。</p><p id="1754" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">图层归一化</strong></p><p id="ec30" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图层归一化是一个常用的模块，它在给定一个输入的情况下，减去其平均值，然后除以标准差。</p><p id="f917" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，我们通常对(N，d)输入应用层归一化，其中d是维度。幸运的是，图层规范化模块也可以推广到多个维度，请看:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es mp"><img src="../Images/59614d0b8f462de843a99ddcf2d06b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*7bLT6Fwt59CLYbuVY_hbXQ.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">nn.LayerNorm can be applied in multiple dimensions. We can normalize fifty 8-dimensional vectors, but we can also normalize sixteen by fifty 8-dimensional vectors.</figcaption></figure><p id="0ec7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图层规范化仅适用于最后一个尺寸。因此，我们可以使我们的每个50×8矩阵(代表单个序列)的均值为0，标准差为1。在我们通过LN运行我们的(N，50，8)张量之后，我们仍然得到相同的维数。</p><p id="29f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">多头自我关注</strong></p><p id="f243" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在需要实现架构图的子图<em class="ku"> c </em>。那里发生了什么事？</p><p id="27ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简而言之:对于单个图像，我们希望基于与其他图像的相似性度量来更新每个图像块。我们通过将每个面片(在我们的例子中现在是一个8维向量)线性映射到3个不同的向量来做到这一点:<strong class="ig hi"> q </strong>、<strong class="ig hi"> k </strong>和<strong class="ig hi"> v </strong>(查询、键、值)。</p><p id="7950" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，对于单个补丁，我们将计算其<strong class="ig hi"> q </strong>向量与所有<strong class="ig hi"> k </strong>向量之间的点积，除以这些向量的维数的平方根(sqrt(8))，softmax这些所谓的<em class="ku">注意力线索</em>，最后将每个注意力线索乘以与不同的<strong class="ig hi"> k </strong>向量相关联的<strong class="ig hi"> v </strong>向量，并求和。</p><p id="a7aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以这种方式，每个小块采用基于其与其他小块的相似性(在线性映射到<strong class="ig hi"> q </strong>、<strong class="ig hi"> k </strong>和<strong class="ig hi"> v </strong>之后)的新值。然而，这整个过程是在我们当前8维面片的<strong class="ig hi"> H </strong>子向量上执行<strong class="ig hi"> H </strong>次，其中<strong class="ig hi"> H </strong>是<strong class="ig hi">头的数量。</strong>如果你对注意力和多头注意力机制不熟悉，我建议你阅读<a class="ae jc" href="https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">这个由<a class="ae jc" href="https://data-science-blog.com/blog/author/yasuto/" rel="noopener ugc nofollow" target="_blank"> Yasuto Tamura </a>写的</a>好帖子。</p><p id="2daf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦获得所有结果，它们就被连接在一起。最后，结果通过一个线性层(为了更好的测量)。</p><p id="1a4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意力背后的直观想法是，它允许对输入之间的关系进行建模。使“0”成为零的不是单个像素值，而是它们如何相互关联。</p><p id="8cbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于执行了相当多的计算，因此有必要为MSA创建一个新类:</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="e7e9" class="kp je hh kl b be kq kr l ks kt">class MyMSA(nn.Module):<br/>    def __init__(self, d, n_heads=2):<br/>        super(MyMSA, self).__init__()<br/>        self.d = d<br/>        self.n_heads = n_heads<br/><br/>        assert d % n_heads == 0, f"Can't divide dimension {d} into {n_heads} heads"<br/><br/>        d_head = int(d / n_heads)<br/>        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])<br/>        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])<br/>        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])<br/>        self.d_head = d_head<br/>        self.softmax = nn.Softmax(dim=-1)<br/><br/>    def forward(self, sequences):<br/>        # Sequences has shape (N, seq_length, token_dim)<br/>        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)<br/>        # And come back to    (N, seq_length, item_dim)  (through concatenation)<br/>        result = []<br/>        for sequence in sequences:<br/>            seq_result = []<br/>            for head in range(self.n_heads):<br/>                q_mapping = self.q_mappings[head]<br/>                k_mapping = self.k_mappings[head]<br/>                v_mapping = self.v_mappings[head]<br/><br/>                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]<br/>                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)<br/><br/>                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))<br/>                seq_result.append(attention @ v)<br/>            result.append(torch.hstack(seq_result))<br/>        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])</span></pre><p id="9c4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意，对于每个头部，我们创建不同的Q、K和V映射函数(在我们的例子中是大小为4x4的方阵)。</p><p id="9e39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们的输入将是大小为(N，50，8)的序列，并且我们只使用2个头，我们将在某个点有一个(N，50，2，4)张量，使用一个<em class="ku"> nn。线性(4，4) </em>模上，然后回来，串联后，得到一个(N，50，8)张量。</p><p id="0179" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还要注意，使用循环并不是计算多头自我关注的最有效方式，但它使代码更清晰，便于学习。</p><p id="9150" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">残留连接</strong></p><p id="f97f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">剩余连接只是将原始输入添加到一些计算的结果中。这直观地允许网络变得更强大，同时还保留了模型可以近似的一组可能的函数。</p><p id="fe97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将添加一个剩余连接，它将把我们原来的(N，50，8)张量添加到LN和MSA之后获得的(N，50，8)张量中。是时候创建transformer encoder block类了，它将是MyViT类的一个组件:</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="3d74" class="kp je hh kl b be kq kr l ks kt">class MyViTBlock(nn.Module):<br/>    def __init__(self, hidden_d, n_heads, mlp_ratio=4):<br/>        super(MyViTBlock, self).__init__()<br/>        self.hidden_d = hidden_d<br/>        self.n_heads = n_heads<br/><br/>        self.norm1 = nn.LayerNorm(hidden_d)<br/>        self.mhsa = MyMSA(hidden_d, n_heads)<br/><br/>    def forward(self, x):<br/>        out = x + self.mhsa(self.norm1(x))<br/>        return out</span></pre><p id="52b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">唷，那是相当多的工作！但我保证这是最难的部分。从现在开始，一切都在走下坡路。</p><p id="9040" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这种自我关注机制，类标记(N个序列中每个序列的第一个标记)现在有了关于所有其他标记的信息！</p><h2 id="b050" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤5:编码器模块(第2/2部分)</h2><p id="5f7c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">所有留给变换器编码器的只是我们已经拥有的和我们通过另一个LN和MLP传递当前张量后得到的之间的简单剩余连接。MLP由两个图层组成，其中隐藏图层通常是其四倍大(这是一个参数)</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="e40e" class="kp je hh kl b be kq kr l ks kt">class MyViTBlock(nn.Module):<br/>    def __init__(self, hidden_d, n_heads, mlp_ratio=4):<br/>        super(MyViTBlock, self).__init__()<br/>        self.hidden_d = hidden_d<br/>        self.n_heads = n_heads<br/><br/>        self.norm1 = nn.LayerNorm(hidden_d)<br/>        self.mhsa = MyMSA(hidden_d, n_heads)<br/>        self.norm2 = nn.LayerNorm(hidden_d)<br/>        self.mlp = nn.Sequential(<br/>            nn.Linear(hidden_d, mlp_ratio * hidden_d),<br/>            nn.GELU(),<br/>            nn.Linear(mlp_ratio * hidden_d, hidden_d)<br/>        )<br/><br/>    def forward(self, x):<br/>        out = x + self.mhsa(self.norm1(x))<br/>        out = out + self.mlp(self.norm2(out))<br/>        return out</span></pre><p id="7bcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们确实可以看到，编码器模块输出相同维数的张量:</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="0020" class="kp je hh kl b be kq kr l ks kt">if __name__ == '__main__':<br/>  model = MyVitBlock(hidden_d=8, n_heads=2)<br/><br/>  x = torch.randn(7, 50, 8)  # Dummy sequences<br/>  print(model(x).shape)      # torch.Size([7, 50, 8])</span></pre><p id="21d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然编码器模块已经准备好了，我们只需要将它插入到我们更大的ViT模型中，它负责在变压器模块之前进行修补，并在之后进行分类。</p><p id="d1bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以有任意数量的变压器模块。在这个例子中，为了简单起见，我将只使用2。我们还添加了一个参数，以了解每个编码器模块将使用多少头。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="69ae" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):<br/>        # Super constructor<br/>        super(MyViT, self).__init__()<br/>        <br/>        # Attributes<br/>        self.chw = chw # ( C , H , W )<br/>        self.n_patches = n_patches<br/>        self.n_blocks = n_blocks<br/>        self.n_heads = n_heads<br/>        self.hidden_d = hidden_d<br/>        <br/>        # Input and patches sizes<br/>        assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>        assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)<br/><br/>        # 1) Linear mapper<br/>        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])<br/>        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)<br/>        <br/>        # 2) Learnable classification token<br/>        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))<br/>        <br/>        # 3) Positional embedding<br/>        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)<br/>        <br/>        # 4) Transformer encoder blocks<br/>        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])<br/><br/>    def forward(self, images):<br/>        # Dividing images into patches<br/>        n, c, h, w = images.shape<br/>        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)<br/>        <br/>        # Running linear layer tokenization<br/>        # Map the vector corresponding to each patch to the hidden size dimension<br/>        tokens = self.linear_mapper(patches)<br/>        <br/>        # Adding classification token to the tokens<br/>        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)<br/>        <br/>        # Adding positional embedding<br/>        out = tokens + self.positional_embeddings.repeat(n, 1, 1)<br/>        <br/>        # Transformer Blocks<br/>        for block in self.blocks:<br/>            out = block(out)<br/>            <br/>        return out</span></pre><p id="810d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样，如果我们通过我们的模型运行一个随机的(7，1，28，28)张量，我们仍然得到一个(7，50，8)张量。</p><h2 id="6895" class="lh je hh bd jf li lj lk jj ll lm ln jn ip lo lp jr it lq lr jv ix ls lt jz lu bi translated">步骤6:MLP分类</h2><p id="d9d3" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">最后，我们可以从我们的N个序列中提取分类标记(第一个标记)，并使用每个标记获得N个分类。</p><p id="2d63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们决定每个记号是一个8维向量，并且由于我们有10个可能的数字，我们可以将分类MLP实现为一个简单的8×10矩阵，用SoftMax函数激活。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="5f82" class="kp je hh kl b be kq kr l ks kt">class MyViT(nn.Module):<br/>    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):<br/>        # Super constructor<br/>        super(MyViT, self).__init__()<br/>        <br/>        # Attributes<br/>        self.chw = chw # ( C , H , W )<br/>        self.n_patches = n_patches<br/>        self.n_blocks = n_blocks<br/>        self.n_heads = n_heads<br/>        self.hidden_d = hidden_d<br/>        <br/>        # Input and patches sizes<br/>        assert chw[1] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>        assert chw[2] % n_patches == 0, "Input shape not entirely divisible by number of patches"<br/>        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)<br/><br/>        # 1) Linear mapper<br/>        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])<br/>        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)<br/>        <br/>        # 2) Learnable classification token<br/>        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))<br/>        <br/>        # 3) Positional embedding<br/>        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)<br/>        <br/>        # 4) Transformer encoder blocks<br/>        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])<br/>        <br/>        # 5) Classification MLPk<br/>        self.mlp = nn.Sequential(<br/>            nn.Linear(self.hidden_d, out_d),<br/>            nn.Softmax(dim=-1)<br/>        )<br/><br/>    def forward(self, images):<br/>        # Dividing images into patches<br/>        n, c, h, w = images.shape<br/>        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)<br/>        <br/>        # Running linear layer tokenization<br/>        # Map the vector corresponding to each patch to the hidden size dimension<br/>        tokens = self.linear_mapper(patches)<br/>        <br/>        # Adding classification token to the tokens<br/>        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)<br/>        <br/>        # Adding positional embedding<br/>        out = tokens + self.positional_embeddings.repeat(n, 1, 1)<br/>        <br/>        # Transformer Blocks<br/>        for block in self.blocks:<br/>            out = block(out)<br/>            <br/>        # Getting the classification token only<br/>        out = out[:, 0]<br/>        <br/>        return self.mlp(out) # Map to output dimension, output category distribution</span></pre><p id="87d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们模型的输出现在是一个(N，10)张量。万岁，我们完成了！</p><h1 id="bdba" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结果</h1><p id="5191" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我们修改了主程序中唯一一行之前没有定义的代码。</p><pre class="kg kh ki kj fd kk kl km bn kn ko bi"><span id="1058" class="kp je hh kl b be kq kr l ks kt">model = MyVit((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)</span></pre><p id="65ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在只需要运行训练和测试循环，看看我们的模型表现如何。如果您已经手动设置了torch seed(设置为0)，您应该打印出以下内容:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es mq"><img src="../Images/02110057745bed8e0eb3f5e64fe70b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd6tOT0VZ8PAIHCLc7irbA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Training losses, test loss, and test accuracy obtained.</figcaption></figure><p id="4f62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就是这样！我们现在已经从头开始创建了一个ViT。我们的模型在仅仅5个历元和很少的参数下就达到了80%的准确率。</p><p id="4011" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在下面的<a class="ae jc" href="https://github.com/BrianPulfer/PapersReimplementations/blob/master/vit/vit_torch.py" rel="noopener ugc nofollow" target="_blank">链接</a>找到完整的脚本。考虑鼓掌👏如果你觉得这个故事有用，让我知道你是否认为有什么不清楚的地方！</p><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hi fi z dy mz ea eb na ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni lb mu"/></div></div></a></div></div></div>    
</body>
</html>