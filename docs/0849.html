<html>
<head>
<title>LSTM Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM网络公司</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/lstm-networks-75d44ac8280f?source=collection_archive---------2-----------------------#2021-08-03">https://medium.com/mlearning-ai/lstm-networks-75d44ac8280f?source=collection_archive---------2-----------------------#2021-08-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/73b6462219bbd1c85aa726bc2fc18cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbzpEg77lJr5r_ZKgXfALw.png"/></div></div></figure><p id="7cca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本文讨论了传统的RNNs的问题，如消失和爆炸梯度，并提出了一个简单的解决方案的形式长短期记忆(LSTM)。长短期记忆(LSTM)是递归神经网络(RNN)架构的一种更复杂的变体，创建该架构是为了比普通的rnn更精确地描述时间序列及其长期关系。</p><p id="068f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一个基本的LSTM细胞的内部设计，修改包括到LSTM的架构，以及lstm的一些应用，是在很大的需求之间的亮点。它还比较和对比了LSTMs和GRUs。本文最后列出了LSTM网络的缺点，并简要概述了即将出现的基于注意力的模型，这些模型正在现实世界的应用中迅速取代lstm。</p><h1 id="6644" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">简介:</strong></h1><p id="02fc" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">LSTM网络是一种递归神经网络(RNN ),被开发用于处理RNNs失败的情况。就RNNs而言，它们是对当前输入起作用的网络，同时考虑先前的输出(反馈)并将它们保持在存储器中一小段时间(短期存储器)。最常见的应用是在语音处理、非马尔可夫控制和音乐创作等领域。然而，rnn有几个缺点。首先，它无法长时间保留数据。为了预测当前的输出，通常需要求助于很久以前保存的信息。另一方面，rnn完全不能处理这样的“长期依赖”</p><p id="2595" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第二，对于应该保留哪些方面的内容以及应该遗忘多少内容，没有更精细的控制。在网络的回溯训练阶段出现的爆炸和消失梯度(稍后描述)是RNNs的另一个问题。因此，长短期记忆(LSTM)的概念被引入。它是以这样一种方式构建的，即几乎完全消除了消失梯度问题，但训练模型保持不变。LSTMs用于桥接某些应用中的长时间延迟，它们还可以处理噪声、分布式表示和连续数据。与隐马尔可夫模型(HMM)一样，LSTMs不需要从一开始就保留有限数量的状态。学习率、输入和输出偏差以及学习率只是LSTMs提供的几个参数。</p><p id="720d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，不需要精确的修改。使用LSTMs，更新每个权重的难度降低到O(1)，与通过时间的反向传播(BPTT)相当，这是一个好处。</p><h1 id="eaef" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">爆炸和消失渐变:</strong></h1><p id="0312" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">网络训练过程的基本目标是当训练数据通过它传递时，减少输出中注意到的损失量(在误差或成本方面)。我们计算某一组权重的梯度或损失，然后适当地改变权重，继续下去，直到我们得到一组损失最小的理想权重。回溯是一个用来描述这个过程的术语。</p><p id="e57e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">偶尔，梯度很小，几乎看不出来。值得注意的是，层的渐变会受到后续层中特定组件的影响。如果这些分量很小(小于1)，获得的梯度将更小。规模效应就是这个术语。当这个梯度乘以学习率(0.1到0.001之间的一个很小的数字)时，结果是一个更小的数字。</p><p id="9ce5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，权重的变化很小，导致输出几乎与之前相同。类似地，如果梯度由于大的分量值而非常大，则权重被改变为不是最佳的值。爆发渐变的问题就是所谓的。神经网络单元以这样的方式重建，即缩放因子固定为1，以避免这种缩放影响。在增加了多个门控单元后，该单元被命名为LSTM。</p><h1 id="9b86" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">架构:</strong></h1><p id="899d" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">RNN和LSTM设计的主要区别是LSTM的埋层是一个门控单元或门控细胞。它由四层组成，这四层相互作用，生成单元的输出以及单元的状态。然后，这两个项目被传递到下一个隐藏层。LSTMs包含三个逻辑sigmoid门和一个tanh层，这与rnn不同，rnn只有一个tanh神经网络层。</p><p id="9e02" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">开发门是为了限制通过单元传输的数据量。他们计算出数据的哪些部分将被下一个单元需要，哪些将被丢弃。结果通常在0–1范围内，0表示“拒绝全部”，1表示“包括全部”</p><h1 id="a5c4" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">变异:</strong></h1><p id="02c3" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">随着LSTM的日益流行，对传统的LSTM体系结构进行了一些修改，以简化单元的内部设计，从而使它们更有效地运行并最小化计算复杂度。Gers和Schmidhuber引入了窥视孔连接，使栅极层可以随时了解单元的状况。一些LSTMs使用了一个链接的输入和遗忘门，而不是两个不同的门，这使得它们可以同时做出两个选择。另一项改进是引入了门控循环单元(GRU)，减少了门的数量，从而降低了设计的复杂性。它混合了单元状态和隐藏状态，以及一个结合了遗忘门和输入门的更新门。</p><h1 id="216a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> GRUs Vs LSTMs </strong></h1><p id="6e90" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">尽管gru与LSTMs非常相似，但它们从未如此流行。但是GRUs到底是什么？门控循环单位(GRU)是门控循环单位的缩写。顾名思义，Cho建议的循环单元包括一个门控方法，以高效和自适应地捕捉跨多个时间尺度的关系。他们有一个重置和更新门。前者负责决定哪部分知识将被继承，而后者负责决定在两个连续的循环单元之间有多少信息将被遗忘。</p><p id="a2cd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">GRUs的另一个值得注意的特征是，它们不以任何方式保留单元状态，因此它们不能控制下一个单元暴露于多少内存内容。另一方面，LSTMs控制进入细胞的新鲜信息的数量。另一方面，当计算新的候选激活时，GRU控制来自先前激活的信息流，但是不控制被添加的候选激活的数量(该控制通过更新门被绑定)。</p></div></div>    
</body>
</html>