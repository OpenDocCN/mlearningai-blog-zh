<html>
<head>
<title>Review — Mantini’s VISAPP’19: Generative Reference Model and Deep Learned Features (Camera Tampering Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— Mantini的VISAPP'19:生成参考模型和深度学习功能(相机篡改检测)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/review-mantinis-visapp-19-generative-reference-model-and-deep-learned-features-camera-f608371c9854?source=collection_archive---------5-----------------------#2021-02-27">https://medium.com/mlearning-ai/review-mantinis-visapp-19-generative-reference-model-and-deep-learned-features-camera-f608371c9854?source=collection_archive---------5-----------------------#2021-02-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="bdd7" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">利用生成式对抗网络(<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">干</a>)和连体网络进行摄像头篡改检测</h2></div><p id="366b" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>这个故事中，回顾了休斯顿大学的<strong class="iz hi">利用生成参考模型和深度学习特征</strong>进行的摄像头篡改检测。在本文中:</p><ul class=""><li id="aa68" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">通过使用<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>，<strong class="iz hi">生成模型</strong>被用于<strong class="iz hi">从监视摄像机学习正常操作条件下的图像分布</strong>。</li><li id="bea0" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated"><strong class="iz hi">训练暹罗网络</strong>将图像转换到特征空间，从而<strong class="iz hi">最大化生成图像和篡改图像之间的距离</strong>。基于该距离，监视摄像机图像被分类为正常或被篡改。</li></ul><p id="27f7" class="pw-post-body-paragraph ix iy hh iz b ja jb ii jc jd je il jf jg jh ji jj jk jl jm jn jo jp jq jr js ha bi translated">这是<strong class="iz hi"> 2019 VISAPP </strong>上的一篇论文。(<a class="kq kr ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----f608371c9854--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="daf6" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">概述</h1><ol class=""><li id="789b" class="kc kd hh iz b ja lr jd ls jg lt jk lu jo lv js lw ki kj kk bi translated"><strong class="iz hi">拟议框架</strong></li><li id="f1e2" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated"><strong class="iz hi">发电机作为参考型号</strong></li><li id="f17d" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated"><strong class="iz hi">作为特征提取器的连体网络</strong></li><li id="9064" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated"><strong class="iz hi">培训策略</strong></li><li id="5f65" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated"><strong class="iz hi">实验结果</strong></li></ol></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="9311" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">1.拟议框架</h1><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es lx"><img src="../Images/b208b730f828c7e764de66b9ee0ebee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*lZpV18FCc047Lif4IcWf_A.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Proposed framework for camera tampering detection</strong></figcaption></figure><ul class=""><li id="6b78" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">提议的框架包括</li></ul><ol class=""><li id="9401" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js lw ki kj kk bi translated">去卷积神经网络(生成器)。</li><li id="1ee7" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated">一对具有共享权重的卷积神经网络(CNN)(暹罗网络)。</li><li id="4781" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js lw ki kj kk bi translated">一个完全连接的神经网络。</li></ol><ul class=""><li id="102d" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated"><strong class="iz hi">生成器</strong>将随机数向量作为输入。它<strong class="iz hi">生成一个代表正常工作状态下监控摄像机的图像。</strong></li><li id="87fd" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">在时间t来自摄像机的图像以及生成的图像被用作<strong class="iz hi">共享权重</strong>的一对CNN的输入。该阶段充当<strong class="iz hi">用于生成和测试图像的特征提取器。</strong></li><li id="eb19" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">变换特征之间的距离被输入到<strong class="iz hi">全连接神经网络</strong>。输出是一个后验值<strong class="iz hi">，估计给定两个输入之间的距离时选择类别<em class="mj"> C </em>的概率。</strong></li></ul></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="d78a" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated"><strong class="ak"> 2。发电机作为参考型号</strong></h1><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es mk"><img src="../Images/6a9e7a77b37a2e1e588194ef49b981a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*2pKiIvPOmd6l3WuVuk8WnA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Generator as a Reference Model</strong></figcaption></figure><ul class=""><li id="859f" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated"><strong class="iz hi">生成器<em class="mj"> G </em>旨在创建视觉上类似于训练示例的图像<em class="mj"> x </em> *。</strong></li><li id="eb7c" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated"><strong class="iz hi">鉴别器<em class="mj"> D </em>旨在将生成的图像<em class="mj"> x </em> *与原始训练图像<em class="mj"> x </em> </strong>区分开来。</li><li id="6382" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">假设<em class="mj"> D </em>给一个图像分配一个高的分数，如果它是原始图像，如果它是生成的，则分配一个低的分数。</li></ul><h2 id="e90b" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">2.1.发电机</h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es mz"><img src="../Images/dd89f08f5d81dad45e0eb8a84273f0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zllW1y1h1OugZJFhxmqyZQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">The Generator: Network Architecture</strong></figcaption></figure><ul class=""><li id="a060" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">输入是一个大小为16×16×256的随机数矩阵，数字通过三个2D上采样层。</li><li id="87de" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">每个上采样层之后是2D卷积层。</li><li id="4892" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated"><a class="ae iw" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">使用批次定额</a>和ReLU。</li><li id="87ad" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">输出是大小为127×127×3的矩阵。</li><li id="c806" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">生成器的目标是最大化<em class="mj">D</em>(<em class="mj">G</em>(<em class="mj">y</em>)(或者最小化1-<em class="mj">D</em>(<em class="mj">G</em>(<em class="mj">y</em>))。发电机被训练以优化以下功能:</li></ul><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es ne"><img src="../Images/98ffec51c21f80c2f89e7f6a27207f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*d-cs2nRZ0FkBRRNBDYEQzA.png"/></div></figure><ul class=""><li id="1f8c" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">其中<em class="mj"> V </em>是损失函数。</li></ul><h2 id="0a41" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">2.2.鉴别器</h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es nf"><img src="../Images/f1c8218983cb6c5cd16a590a12de782e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc6ldmxHqs83a4xEn36ozg.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">The Discriminator: Network Architecture</strong></figcaption></figure><ul class=""><li id="95a9" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">鉴别器由四个2D卷积层组成。</li><li id="012f" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated"><a class="ae iw" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">使用批次定额</a>和漏ReLU。</li><li id="136a" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">25% <a class="ae iw" href="https://sh-tsang.medium.com/paper-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-image-classification-a74b369b4b8e" rel="noopener">下降</a>用于每次激活。</li><li id="8c51" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">鉴别器优化了以下功能:</li></ul><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es ng"><img src="../Images/d08428389a6adb3150855c46de4fe833.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*EXJjzbwzNPj0WfoOBB5jew.png"/></div></figure><ul class=""><li id="9da0" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">发生器和鉴频器的组合损耗函数为:</li></ul><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es nh"><img src="../Images/82cfe919fb21dbc9d265756ad1c07092.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*sYd2aW004qb2yRpM_nfxGg.png"/></div></figure><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es ni"><img src="../Images/33fbd8bfb9ee1b63355383f4afcb828a.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*XeccUYtaV3b6gz89kXesAQ.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">a) generated image (daytime), b) original image (daytime), c) generated image (nighttime), and d) original image (nighttime).</strong></figcaption></figure><ul class=""><li id="3af5" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">上图为<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a> ((a)、(c))生成的图像，并与原始图像((b)、(d))进行对比。</li><li id="d6b1" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">这两组图像分别代表白天和黑夜。</li><li id="0129" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">对数缩放应用于夜间图像。</li></ul></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="69e5" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">3.作为特征提取器的暹罗网络</h1><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es nj"><img src="../Images/d9f41100d899d2f10970a9ed3c06f03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tTnMIK8l9hDkIHiDs_oSw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Siamese Network: Network Architecture</strong></figcaption></figure><ul class=""><li id="47b4" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">生成器合成参考图像(<em class="mj"> x* </em>)。</li><li id="7f9b" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">使用用于检测篡改的距离测量将来自摄像机的图像(<em class="mj"> x </em>)与合成图像(<em class="mj"> x </em> *)进行比较。</li></ul><blockquote class="nk nl nm"><p id="dc04" class="ix iy mj iz b ja jb ii jc jd je il jf nn jh ji jj no jl jm jn np jp jq jr js ha bi translated">在连体网络中，<em class="hh"> x </em>和<em class="hh"> x </em> *被变换到另一个特征空间中，这样如果<em class="hh"> x </em>被篡改，变换后的特征<em class="hh"> x </em>和<em class="hh"> x </em> *之间的距离最大；和最小值，如果<em class="hh"> x </em>正常。</p></blockquote><ul class=""><li id="a61c" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">基本CNN由两个卷积层组成，每个卷积层之后是ReLU激活和2D最大池层。</li><li id="1f4c" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">该网络由共享同一组权重的两个并行卷积网络组成。</li><li id="8f61" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">距离向量作为输入被提供给一个全连接层，其后是一个<a class="ae iw" href="https://sh-tsang.medium.com/paper-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-image-classification-a74b369b4b8e" rel="noopener">断开</a>和ReLU激活层。</li><li id="708c" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">最后通过另一个完全连接的层，其输出使用softmax激活被映射到四个类的后验值。</li><li id="8759" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">这四个等级代表相机的正常、被覆盖、散焦和移动状态。</li></ul></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="d1cc" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">4.培训策略</h1><ul class=""><li id="64da" class="kc kd hh iz b ja lr jd ls jg lt jk lu jo lv js kh ki kj kk bi translated">生成对抗网络和暹罗网络是分开训练的。</li><li id="f213" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">这些数据被分成两个集群，用于日夜训练个人<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>和暹罗网络。</li><li id="38d8" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">GAN和Siamese网络分别经过5个和10个时期的训练。</li></ul><h2 id="2c32" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">4.1.训练<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a></h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es nq"><img src="../Images/03c665aebf74697a140af0447d16df16.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*mauXsaovUIFAHEaQ50M3CQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">(a) Day (b) Light (c,e) K=0, (d,f) K=1</strong></figcaption></figure><ul class=""><li id="0444" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">使用K-means基于它们的颜色特征将训练数据分割成多个聚类。(<em class="mj"> K </em> =2，也许是昼光图像)</li><li id="445a" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">测试时，根据图像到聚类的距离选择合适的<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>。</li><li id="9978" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">捕获的正常图像用于训练<a class="ae iw" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>。</li></ul><h2 id="192a" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">4.2.培训暹罗网络</h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es nr"><img src="../Images/72b4f96f3b1bbb9a9ee5858196f5db85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*caeEUpFusVNaHTIYM-zxZw.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Synthesis Procedure</strong></figcaption></figure><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es ns"><img src="../Images/ad2c70ce35cea6c0b08182b49652da71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*IF-fUONT_3_53kajXqbVfw.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">a) Original, b) Covered, c) Defocused, and d) Moved images.</strong></figcaption></figure><ul class=""><li id="4858" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">训练暹罗网络需要四类数据。</li><li id="55cd" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">空间平移、空间平滑和像素复制操作分别用于合成移动的、散焦的和覆盖的篡改。</li><li id="3427" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">包含四个类的均匀分布的合成数据被用于训练暹罗网络。</li><li id="55b5" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">(合成程序应与<a class="ae iw" href="https://sh-tsang.medium.com/review-uhctd-a-comprehensive-dataset-for-camera-tampering-detection-camera-tampering-detection-f2a132eb7aca" rel="noopener"> UHCTD </a>相同或相似。请随时访问<a class="ae iw" href="https://sh-tsang.medium.com/review-uhctd-a-comprehensive-dataset-for-camera-tampering-detection-camera-tampering-detection-f2a132eb7aca" rel="noopener"> UHCTD </a>。)</li></ul></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h1 id="d850" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated"><strong class="ak"> 5。实验结果</strong></h1><h2 id="0dab" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">5.1.性能赋值</h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="er es nt"><img src="../Images/bd1d43e9844464351d740c2abb0e733a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2p-BSsE7pkIQgGbl7A67qw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Performance comparison, TP — true positives, FP — false positives, TN — true negatives, FN — false negatives, TPR — true positive rate, FPR, false positive rate, and Acc — accuracy</strong></figcaption></figure><ul class=""><li id="0865" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">提出2 :一个简单的时间分析机制来抑制虚假的错误警报。它通过从先前的<em class="mj"> t </em> - <em class="mj"> n </em>实例中获取类别预测的模式，在时间<em class="mj"> t </em>检测篡改。</li></ul><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es nu"><img src="../Images/cc70c8f7a2af0b0bb61c17870f57fd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*aL8ZyMAz0G9OYcxo9z_yAg.png"/></div></figure><ul class=""><li id="6fd7" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">其中<em class="mj"> n </em> =3。</li><li id="1625" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">检测三类篡改的性能通过将数据集分为三组来量化，每组包含正常图像和仅一种篡改(覆盖、散焦和移动)。</li><li id="049d" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">结果显示<strong class="iz hi">提出的方法2表现最好，总体准确率为95% </strong>，其次是<strong class="iz hi">提出的</strong>方法，准确率为<strong class="iz hi"> 91% </strong>，曼蒂尼和沙阿(2017)的准确率为85%，李氏(2014)的准确率为25%。</li><li id="839e" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">lee(2014)无法以97%的误报率应对场景的复杂性。</li><li id="27d9" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">Mantini和Shah(2017)的方法能够比其他篡改者更好地检测散焦图像。</li><li id="7329" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">与提出的系统相比，Mantini和Shah(2017)显示了覆盖和散焦图像的更高准确性，并且产生了更少的假阳性。</li><li id="03bb" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated"><strong class="iz hi"> Proposed2方法在准确率和假阳性率方面优于Mantini和Shah(2017)的方法。</strong></li></ul><blockquote class="nk nl nm"><p id="31c4" class="ix iy mj iz b ja jb ii jc jd je il jf nn jh ji jj no jl jm jn np jp jq jr js ha bi translated">所提出的方法具有很高的检测篡改的能力，它分别检测到99%、98%和99%的覆盖、散焦和移动篡改，而Proposed2检测到93%、91%和91%，Mantini和Shah(2017)检测到67%、86%和13%。</p><p id="475b" class="ix iy mj iz b ja jb ii jc jd je il jf nn jh ji jj no jl jm jn np jp jq jr js ha bi translated">时间分析降低了假阳性，但是影响了系统检测篡改的能力。</p></blockquote><h2 id="331a" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">5.2.混淆矩阵</h2><figure class="ly lz ma mb fd mc er es paragraph-image"><div class="er es nv"><img src="../Images/aab5db3bea1e4d911be873ed153043b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*S4s_saeHTRCEXEtriJvDdA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx"><strong class="bd lb">Confusion matrix of Proposed System</strong></figcaption></figure><ul class=""><li id="d715" class="kc kd hh iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">在三种篡改类别和正常图像之间存在明显的混淆。这些对应于假警报。</li><li id="9d2a" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">假阴性是最小的。百分之五的移动图像被分类为被覆盖，百分之二的散焦图像被分类为移动。</li></ul><h2 id="b8a3" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">5.3.不足之处</h2><ul class=""><li id="e734" class="kc kd hh iz b ja lr jd ls jg lt jk lu jo lv js kh ki kj kk bi translated">提议的系统<strong class="iz hi">需要一个大数据集来进行训练。</strong></li><li id="d3cb" class="kc kd hh iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">所提出的方法没有正式引入在线机制来更新训练模型。因此，系统在<strong class="iz hi">极端天气</strong>条件下的性能是<strong class="iz hi">不可预测的</strong>。</li></ul></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><blockquote class="nk nl nm"><p id="3538" class="ix iy mj iz b ja jb ii jc jd je il jf nn jh ji jj no jl jm jn np jp jq jr js ha bi translated">在未来，作者希望探索使用<strong class="iz hi">场景独立特征</strong>的系统的性能，并验证学习是否可以转移到各种场景。</p></blockquote></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h2 id="c361" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">参考</h2><p id="b571" class="pw-post-body-paragraph ix iy hh iz b ja lr ii jc jd ls il jf jg nw ji jj jk nx jm jn jo ny jq jr js ha bi translated">【2019 VISAPP】【曼蒂尼的VISAPP ' 19】<br/><a class="ae iw" href="https://www.scitepress.org/Link.aspx?doi=10.5220/0007392100850095" rel="noopener ugc nofollow" target="_blank">利用生成式参考模型和深度学习特征的摄像头篡改检测</a></p><h2 id="56af" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated">摄像机篡改检测</h2><p id="ef66" class="pw-post-body-paragraph ix iy hh iz b ja lr ii jc jd ls il jf jg nw ji jj jk nx jm jn jo ny jq jr js ha bi translated"><strong class="iz hi"> 2016 </strong> [ <a class="ae iw" href="https://sh-tsang.medium.com/review-dongs-icdsp-16-morphological-analysis-and-deep-learning-camera-anomaly-tampering-c685c4ddd3c7" rel="noopener">董的IC DSP ' 16</a>]<strong class="iz hi">2019</strong>[<a class="ae iw" href="https://sh-tsang.medium.com/review-video-frame-interpolation-using-convlstm-camera-tampering-detection-5b07dec0fb52" rel="noopener">VFI-ConvLSTM</a>][<a class="ae iw" href="https://sh-tsang.medium.com/review-uhctd-a-comprehensive-dataset-for-camera-tampering-detection-camera-tampering-detection-f2a132eb7aca" rel="noopener">UHCTD</a>][<a class="ae iw" href="https://sh-tsang.medium.com/review-mantinis-visapp-19-generative-reference-model-and-deep-learned-features-camera-f608371c9854" rel="noopener">曼蒂尼的VISAPP'19 </a> ]</p><h2 id="3319" class="ml la hh bd lb mm mn mo lf mp mq mr lj jg ms mt ll jk mu mv ln jo mw mx lp my bi translated"><a class="ae iw" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他论文阅读</a></h2></div></div>    
</body>
</html>