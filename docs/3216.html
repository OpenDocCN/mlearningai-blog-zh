<html>
<head>
<title>Axial Self-Attention in CNN — Efficient and Elegant</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN中的轴向自我关注——高效而优雅</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/axial-self-attention-in-cnn-efficient-and-elegant-85d8ce2ca8eb?source=collection_archive---------0-----------------------#2022-08-05">https://medium.com/mlearning-ai/axial-self-attention-in-cnn-efficient-and-elegant-85d8ce2ca8eb?source=collection_archive---------0-----------------------#2022-08-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/dc7cc818390325f4295bbdd7d3f394ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VKxKKmChgoAoRVEX"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@atlemo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Atle Mo</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="90d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意:这篇文章试图通过忽略一些细节，用最少的精力让你直观地理解这个主题。</p><h1 id="58ac" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">目录</h1><p id="17ed" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">🔥<a class="ae it" href="#97a2" rel="noopener ugc nofollow">CNN</a>中的自我关注<br/>🔥<a class="ae it" href="#16cf" rel="noopener ugc nofollow">轴向注意块</a>T7】🔥<a class="ae it" href="#1a2a" rel="noopener ugc nofollow">总结</a> <br/> ∘ <a class="ae it" href="#3a6a" rel="noopener ugc nofollow">奖金:官方PyTorch实施</a> <br/> <a class="ae it" href="#8340" rel="noopener ugc nofollow">引用</a></p></div><div class="ab cl kw kx go ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ha hb hc hd he"><h1 id="97a2" class="jt ju hh bd jv jw ld jy jz ka le kc kd ke lf kg kh ki lg kk kl km lh ko kp kq bi translated">🔥CNN中的自我关注</h1><p id="f6dc" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">卷积神经网络(<strong class="iw hi">CNN</strong>)非常适合利用局部特征。然而，他们通常<strong class="iw hi">不能探索长范围的特征</strong>。然而，<strong class="iw hi">自我关注</strong>机制<strong class="iw hi">无法探索局部特征</strong>却非常适合探索长期特征。因此，自我关注模块被广泛地<strong class="iw hi">与</strong>CNN模块相结合，以探索<strong class="iw hi">短距离和长距离信息。</strong></p><figure class="lj lk ll lm fd ii er es paragraph-image"><div class="er es li"><img src="../Images/be0ea2f1f97d438bc9244d02e5fdc19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*BIGG2smPKv3pMSxWg1GJ0Q.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Fig. 1: Standard self-attention module in CNN [H. Wang et al.] “Circled X” denotes matrix multiplication, and “Circled +” denotes element-wise sum.</figcaption></figure><p id="ee55" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">最近，已经提出了纯粹的<strong class="iw hi">堆叠</strong>自我注意模块以探索长范围特征，并且将局部约束应用于自我注意模块以探索短范围特征，并且显示了有希望的结果。</p><p id="0fbc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">然而，将局部约束应用于自我注意<strong class="iw hi">会限制感受野</strong>，这在图像分割任务中是不可接受的，因为它需要高分辨率输入。</p><p id="01fb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">❗️Therefore，提出了轴向注意，它允许<strong class="iw hi">高效的计算和丰富的感受域</strong>，解决了普通自注意模块的缺点。</p></div><div class="ab cl kw kx go ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ha hb hc hd he"><h1 id="16cf" class="jt ju hh bd jv jw ld jy jz ka le kc kd ke lf kg kh ki lg kk kl km lh ko kp kq bi translated">🔥轴向注意阻滞</h1><p id="55c8" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">用一句话概括</p><blockquote class="ln lo lp"><p id="8778" class="iu iv js iw b ix iy iz ja jb jc jd je lq jg jh ji lr jk jl jm ls jo jp jq jr ha bi translated"><strong class="iw hi">轴向注意将2D注意分解为沿高度轴和宽度轴的两个1D注意。</strong></p></blockquote><p id="1232" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">具体来说，轴向注意不是像大多数自我注意块那样首先展平多维输入，而是一次沿着特征图的单个轴应用自我注意<strong class="iw hi">，然后<strong class="iw hi">组合多个轴的注意图</strong>以实现全局感受野。这样做也可能导致输出维度与输入维度相同。</strong></p><p id="d4a6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，它比传统的自我注意模块在计算上更有效，但仍能保持全局感受野。</p><figure class="lj lk ll lm fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lt"><img src="../Images/83446afb9f7131011551966fbbab2eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0ROMiRIh84uV3ImG-i3FA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Fig. 2: Types of Axial Attention layers used in the Axial Transformer. <strong class="bd jv">Blue is the receptive field of the output red. </strong>By combining row-wise and column-wise, and potentially channel-wise, self-attention, the output could retain the original input dimension.</figcaption></figure><p id="f985" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在轴向变压器中，轴向注意层有<strong class="iw hi">三种主要类型</strong>(<em class="js">两种类型及其各自的子类型如图3 </em>所示)，即<strong class="iw hi">行向注意、列向注意、</strong>和<strong class="iw hi">通道向注意。</strong></p><p id="c269" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实际上，如上所述，它们经常被堆叠在一起以获得一个全局感受野。</p><figure class="lj lk ll lm fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/cf135dcf578c5c025a1e1756969bc873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*coMOxOWX89dp4fqkATx3dQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Fig. 3: Axial self-attention module illustrated in the CCNet paper [Z. Huang et al.].</figcaption></figure><p id="7736" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">轴向自我关注模块的流水线如图3所示。<strong class="iw hi"> H </strong>是特征图。<strong class="iw hi"> A </strong>是轴向关注块。<strong class="iw hi">H’</strong>是施加轴向注意后的特征图。<strong class="iw hi">亲和</strong>操作是一种特殊类型的转化(<em class="js">不是三言两语能解释清楚的，不知道的请随意看</em> <a class="ae it" href="https://www.youtube.com/watch?v=E3Phj6J287o" rel="noopener ugc nofollow" target="_blank"> <em class="js">本视频</em> </a>)。<strong class="iw hi">聚合</strong>就是简单的元素求和。</p></div><div class="ab cl kw kx go ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ha hb hc hd he"><h1 id="1a2a" class="jt ju hh bd jv jw ld jy jz ka le kc kd ke lf kg kh ki lg kk kl km lh ko kp kq bi translated">🔥摘要</h1><p id="f2b4" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">通过提出轴向注意，基于注意的CNN现在可以更有效地处理图像，同时仍然保留全局感受野，这解决了传统自我注意模块的问题。通过应用轴向注意力，出现了某些架构，例如CCNet、轴向变压器和轴向深度实验室。他们都表现出了卓越的性能和当时接近最先进的结果(<em class="js">然而，这些在本文中不会展开</em>)。</p><h2 id="0d9a" class="lv ju hh bd jv lw lx ly jz lz ma mb kd jf mc md kh jj me mf kl jn mg mh kp mi bi translated">谢谢你！❤️ </h2><h2 id="2079" class="lv ju hh bd jv lw lx ly jz lz ma mb kd jf mc md kh jj me mf kl jn mg mh kp mi bi translated">如果你觉得这篇文章对你有帮助，请考虑给我们投赞成票！❤️ </h2><h2 id="3a6a" class="lv ju hh bd jv lw lx ly jz lz ma mb kd jf mc md kh jj me mf kl jn mg mh kp mi bi translated">奖励:PyTorch官方实现</h2><div class="mj mk ez fb ml mm"><a href="https://github.com/lucidrains/axial-attention" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">GitHub-luci drains/Axial-attention:实现轴向注意力-关注…</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">Pytorch中轴向注意的实现。处理多维数据的简单而强大的技术…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">github.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na in mm"/></div></div></a></div><h1 id="8340" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">引用</h1><p id="a0cc" class="pw-post-body-paragraph iu iv hh iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr ha bi translated">[1] <a class="ae it" href="https://arxiv.org/pdf/1912.12180.pdf" rel="noopener ugc nofollow" target="_blank">多维变形金刚中的轴向注意力</a></p><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="nb l mx my mz mv na in mm"/></div></div></a></div></div></div>    
</body>
</html>