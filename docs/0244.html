<html>
<head>
<title>Self Attention in Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络中的自注意</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/self-attention-in-convolutional-neural-networks-172d947afc00?source=collection_archive---------0-----------------------#2021-03-09">https://medium.com/mlearning-ai/self-attention-in-convolutional-neural-networks-172d947afc00?source=collection_archive---------0-----------------------#2021-03-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2f7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我最近在一个网络中加入了自我关注，我训练它来<a class="ae jc" href="https://apps.apple.com/us/app/wall-color-ai/id1440854963" rel="noopener ugc nofollow" target="_blank">检测墙壁</a>，它提高了墙壁分割的骰子分数。我写这篇短文来总结CNN的自我关注。我写下这些笔记，主要是为了让我可以回过头来回忆我所做的事情，但我希望你也会觉得有用。</p><h1 id="467f" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">为什么自我关注</h1><p id="504f" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">自我注意在<a class="ae jc" href="https://arxiv.org/pdf/1805.08318.pdf" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中有描述。它增加了CNN的感受野，而没有增加与非常大的核大小相关的计算成本。</p><h1 id="d3a3" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">它是如何工作的</h1><ul class=""><li id="1d24" class="kg kh hh ig b ih kb il kc ip ki it kj ix kk jb kl km kn ko bi translated">重塑先前隐藏图层中的要素，以便:</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es kp"><img src="../Images/dedfe0486faf01e0d806a1e1ae60c206.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*K0JirfsIif8v-dj8BlUtKg.png"/></div></figure><p id="2825" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，<em class="kx"> C </em>是通道的数量，<em class="kx"> N </em>是所有其他维度的乘积(我们将在后面看到代码)</p><ul class=""><li id="f980" class="kg kh hh ig b ih ii il im ip ky it kz ix la jb kl km kn ko bi translated">对<em class="kx"> x </em>执行1x1卷积，以获得<em class="kx">，f，g和h。</em>这将使通道数从<em class="kx"> C </em>变为<em class="kx"> C*: </em></li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es lb"><img src="../Images/706d0e40df4e4d4c956aaab6db8e035f.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*qN_pvdODL0lYaV4m1WtrUA.png"/></div></figure><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es lc"><img src="../Images/1343605c3da26276a06fb3e1e2afa72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*8dzEP4yeGQzx25t03u5Tgw.png"/></div></figure><ul class=""><li id="b1b2" class="kg kh hh ig b ih ii il im ip ky it kz ix la jb kl km kn ko bi translated">计算<em class="kx"> f(x) </em>和<em class="kx">g(x)</em>中像素位置之间的一系列softmax权重</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es ld"><img src="../Images/8464dff1ec20223eb3c352650573d586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*WsnlW2B9j35PfpV0BNadaA.png"/></div></figure><p id="366a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些权重被称为“注意力图”，本质上是量化图像中像素<em class="kx"> j </em>相对于像素<em class="kx"> i. </em>的“重要性”。由于这些权重(β)是在特征集的整个高度和宽度上计算的，感受野不再局限于小核的大小。</p><ul class=""><li id="c89a" class="kg kh hh ig b ih ii il im ip ky it kz ix la jb kl km kn ko bi translated">将自我关注层的输出计算为:</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es le"><img src="../Images/4c7033f33adb323c87bfc50331de5fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*pNDJLV28uQ-7pGuwvb8Tbg.png"/></div></figure><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es lf"><img src="../Images/2ba623ff6cfafeb139c37e83a4494381.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*wBe9yNFl45NNGANtPX3ftw.png"/></div></figure><p id="91d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，<em class="kx"> v </em>是另一个1x1卷积的输出。请注意，输出的通道数与自我关注层的输入特征数相同。</p><p id="42bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是论文中的一张图，它将这些操作可视化</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lg"><img src="../Images/8192d62521e410bc93dcec3c3e409f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LUE23j9u__ch3b8nTR5_wA.png"/></div></div></figure><p id="3f46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，我们设置:C* = C/8。</p><ul class=""><li id="dce8" class="kg kh hh ig b ih ii il im ip ky it kz ix la jb kl km kn ko bi translated">最后一步，我们将输入特征<em class="kx"> x、</em>添加到输出的加权版本中(伽马是另一个可学习的标量参数):</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es ll"><img src="../Images/7cf7e6cd070b94b9606de41d7903f5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*qan4befYR5zCS9lbrshByw.png"/></div></figure><h1 id="aabf" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">Pytorch实现</h1><p id="81aa" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">以下简短高效的实现来自<a class="ae jc" href="http://fast.ai" rel="noopener ugc nofollow" target="_blank"> Fast.ai </a></p><figure class="kq kr ks kt fd ku"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="617e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第4行:定义三个1x1的conv层来创建，<em class="kx"> f(x)，g(x)，h(x) </em>。这些通常被称为查询、键和值(见第14行)</p><p id="7ba1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第13行:整形为一个大小为<em class="kx"> C x N </em>的张量。</p><p id="43f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第15行:按照上面的定义计算softmax注意力权重(“bmm”是torch中的批量矩阵乘法)。</p><p id="4cb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第17行:恢复要素的原始形状</p><p id="1e17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种实现与论文中描述的算法有些不同(但等效)，因为它将1x1卷积<em class="kx"> v(x) </em>和<em class="kx"> h(x) </em>组合在一起，并调用is <em class="kx"> h(x) </em>或“值”。组合的1x1 conv层具有C个输入通道和C个输出通道。这种实现等效于本文中的算法，因为学习两个背对背的1x1 conv层等效于学习一个兼容大小的conv层。</p><h1 id="635d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">样本结果</h1><p id="d8d9" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我在UNet架构中使用了自我关注层，替换了UNet模块中的conv层。自我关注层的引入提高了分割墙壁的骰子点数。下面是“<a class="ae jc" href="https://apps.apple.com/us/app/wall-color-ai/id1440854963#?platform=iphone" rel="noopener ugc nofollow" target="_blank">墙色AI </a>”应用的一个例子:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lo"><img src="../Images/6092b838134444cc813fdfcced264482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TYszxBeVnY76EntdGDR-jw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Source: <a class="ae jc" href="https://apps.apple.com/us/app/wall-color-ai/id1440854963#?platform=iphone" rel="noopener ugc nofollow" target="_blank">Wall Color AI App</a></figcaption></figure><div class="lt lu ez fb lv lw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lx ab dw"><div class="ly ab lz cl cj ma"><h2 class="bd hi fi z dy mb ea eb mc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="md l"><h3 class="bd b fi z dy mb ea eb mc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="me l"><p class="bd b fp z dy mb ea eb mc ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kv lw"/></div></div></a></div><p id="17c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">成为ML写手</a></p></div></div>    
</body>
</html>