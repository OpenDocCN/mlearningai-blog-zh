<html>
<head>
<title>Answer Questions by Popular Quotes with Transformers: BERT Language Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用变形金刚的流行语录回答问题:伯特语言模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/answer-questions-by-popular-quotes-with-transformers-bert-language-model-fe832032ab2e?source=collection_archive---------5-----------------------#2022-02-21">https://medium.com/mlearning-ai/answer-questions-by-popular-quotes-with-transformers-bert-language-model-fe832032ab2e?source=collection_archive---------5-----------------------#2022-02-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b7587ade2be92a1f25b7637c69bdc7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUVXbYDAGGlgRaupVEsHtw.jpeg"/></div></div></figure><blockquote class="ip iq ir"><p id="274b" class="is it iu iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如需完整解决方案，请访问我的GitHub</p></blockquote><p id="4d99" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">你想从著名思想家那里获得哲学问题的答案吗？</p><p id="311b" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">让我们用<a class="ae jr" href="https://en.wikipedia.org/wiki/BERT_(language_model" rel="noopener ugc nofollow" target="_blank"> BERT </a> architecture for NLP在有影响力的作者的名言中寻找答案吧！</p><p id="7ffd" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">为此，我们将:</p><ol class=""><li id="b01a" class="jv jw hh iv b iw ix ja jb js jx jt jy ju jz jq ka kb kc kd bi translated">使用Google Colab中的TPU有效地训练模型</li><li id="849b" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq ka kb kc kd bi translated">微调伯特的具体任务:“检查一个句子是否包含一个问题的答案”</li><li id="5913" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq ka kb kc kd bi translated">应用这个调整后的模型来查找可能包含所选问题答案的报价</li></ol><h1 id="c480" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">伯特是什么？</h1><blockquote class="ip iq ir"><p id="4ed1" class="is it iu iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><a class="ae jr" href="https://en.wikipedia.org/wiki/BERT_(language_model" rel="noopener ugc nofollow" target="_blank"> BERT </a>的核心是一个transformer语言模型，具有可变数量的编码器层和自关注头。该架构与Vaswani等人(2017)的原始变压器实施“几乎相同”。</p><p id="dc78" class="is it iu iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">BERT接受了两项任务的预训练:语言建模(15%的标记被屏蔽，BERT被训练从上下文中预测它们)和下一句预测(BERT被训练预测所选择的下一句是否可能或是否给出第一句)。作为训练过程的结果，BERT学习单词的上下文嵌入。在计算代价昂贵的预训练之后，BERT可以在较小的数据集上用较少的资源进行微调，以优化其在特定任务上的性能。</p></blockquote><h1 id="f207" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">设置TPU</h1><p id="1d51" class="pw-post-body-paragraph is it hh iv b iw lh iy iz ja li jc jd js lj jg jh jt lk jk jl ju ll jo jp jq ha bi translated">在Google Colab中，可以使用TPU来代替GPU。选择Runtime -&gt; Change runtime type，验证是否选择了TPU，并连接到TPU工作进程:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="4819" class="lv kk hh lr b fi lw lx l ly lz"><strong class="lr hi">if</strong> os<strong class="lr hi">.</strong>environ['COLAB_TPU_ADDR']:<br/>  cluster_resolver <strong class="lr hi">=</strong> tf<strong class="lr hi">.</strong>distribute<strong class="lr hi">.</strong>cluster_resolver<strong class="lr hi">.</strong>TPUClusterResolver(tpu<strong class="lr hi">=</strong>'')<br/>  tf<strong class="lr hi">.</strong>config<strong class="lr hi">.</strong>experimental_connect_to_cluster(cluster_resolver)<br/>  tf<strong class="lr hi">.</strong>tpu<strong class="lr hi">.</strong>experimental<strong class="lr hi">.</strong>initialize_tpu_system(cluster_resolver)<br/>  strategy <strong class="lr hi">=</strong> tf<strong class="lr hi">.</strong>distribute<strong class="lr hi">.</strong>TPUStrategy(cluster_resolver)<br/>  print('Using TPU')<br/><strong class="lr hi">elif</strong> tf<strong class="lr hi">.</strong>config<strong class="lr hi">.</strong>list_physical_devices('GPU'):<br/>  strategy <strong class="lr hi">=</strong> tf<strong class="lr hi">.</strong>distribute<strong class="lr hi">.</strong>MirroredStrategy()<br/>  print('Using GPU')<br/><strong class="lr hi">else</strong>:<br/>  <strong class="lr hi">raise</strong> ValueError('Running on CPU is not recommended.')</span></pre><h1 id="8ab9" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">调整伯特模型</h1><p id="e5ae" class="pw-post-body-paragraph is it hh iv b iw lh iy iz ja li jc jd js lj jg jh jt lk jk jl ju ll jo jp jq ha bi translated">要将BERT应用到我们的任务中，我们需要执行以下步骤:</p><ol class=""><li id="a338" class="jv jw hh iv b iw ix ja jb js jx jt jy ju jz jq ka kb kc kd bi translated">以适合BERT的方式准备文本输入</li><li id="2265" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq ka kb kc kd bi translated">通过BERT编码器处理输入</li><li id="58a5" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq ka kb kc kd bi translated">训练我们的神经网络使用编码输入来定义一个句子是否包含答案</li></ol><p id="9a1f" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">首先，让我们使用BERT预处理层对输入进行预处理，以备后用:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="4cd6" class="lv kk hh lr b fi lw lx l ly lz"><em class="iu"># Preprocessing for encoder</em><br/>tfhub_handle_preprocess <strong class="lr hi">=</strong> 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'</span></pre><p id="1fb5" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">我们的预处理模型有两个输入，如下所示:</p><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/0298d66e5973eef8c32cd310d6f92799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDkVvfEbujTklBkCyrA65A.png"/></div></div></figure><p id="6637" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">为了进行预测，我们需要使用BERT编码器，然后使用密集层来预测每个问题-句子对的类别标签。BERT编码器经过预训练，下载方式与预处理层相同:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="9ba8" class="lv kk hh lr b fi lw lx l ly lz"><em class="iu"># BERT model - encoder</em><br/>tfhub_handle_encoder <strong class="lr hi">=</strong> 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'</span></pre><p id="39aa" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">为了微调这个架构，让我们使用一个<a class="ae jr" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">粘合数据集</a> : <a class="ae jr" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> QNLI </a>(问答自然语言推理)。任务是确定上下文句子是否包含问题的答案。请参见以下数据集中的示例:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="290e" class="lv kk hh lr b fi lw lx l ly lz">sample row 1<br/>b'The period of time from 1200 to 1000 BCE is known as what?'<br/>b'In the Iron Age'<br/>label: 0 (entailment)<br/><br/>sample row 4<br/>b"What is Nigeria's local vehicle manufacturer?"<br/>b'In 2013, Nigeria introduced a policy regarding import duty on vehicles to encourage local manufacturing companies in the country.'<br/>label: 1 (not_entailment)</span></pre><p id="a363" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">为3个时期训练我们的模型:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="9dc3" class="lv kk hh lr b fi lw lx l ly lz">Epoch 1/3<br/>3273/3273 [==============================] - 362s 88ms/step - <br/>loss: 0.4016 - accuracy: 0.8314 - val_loss: 0.2576 - val_accuracy: 0.9079<br/>Epoch 2/3<br/>3273/3273 [==============================] - 272s 83ms/step - <br/>loss: 0.2434 - accuracy: 0.9204 - val_loss: 0.2582 - val_accuracy: 0.9142<br/>Epoch 3/3<br/>3273/3273 [==============================] - 270s 82ms/step - <br/>loss: 0.1539 - accuracy: 0.9555 - val_loss: 0.3484 - val_accuracy: 0.9142</span></pre><h1 id="eeb9" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">用名言作为回答</h1><p id="b864" class="pw-post-body-paragraph is it hh iv b iw lh iy iz ja li jc jd js lj jg jh jt lk jk jl ju ll jo jp jq ha bi translated">为了找到哲学问题的答案，我们可以引用有影响力的思想家的名言。此处提供报价列表<a class="ae jr" href="https://github.com/akhiltak/inspirational-quotes/raw/master/Quotes.csv" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="e6e2" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">给定一个问题，我们可能会找到更有可能包含答案的引语:</p><ul class=""><li id="4e29" class="jv jw hh iv b iw ix ja jb js jx jt jy ju jz jq mb kb kc kd bi translated">将一个问题与每个报价配对，并使用我们训练有素的模型来确定该报价包含答案的可能性</li><li id="090a" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq mb kb kc kd bi translated">对所有报价重复该过程，并选择得分最高的报价</li></ul><p id="f75c" class="pw-post-body-paragraph is it hh iv b iw ix iy iz ja jb jc jd js jf jg jh jt jj jk jl ju jn jo jp jq ha bi translated">由于我们有许多可能的报价(30k+唯一报价),我们可以选择每次使用一个较小的随机子样本来加快处理过程。</p><h1 id="3b7c" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">例子</h1><p id="0cde" class="pw-post-body-paragraph is it hh iv b iw lh iy iz ja li jc jd js lj jg jh jt lk jk jl ju ll jo jp jq ha bi translated">让我们在几个问题上试试吧！</p><h2 id="43ac" class="lv kk hh bd kl mc md me kp mf mg mh kt js mi mj kx jt mk ml lb ju mm mn lf mo bi translated">怎么开心？</h2><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mp"><img src="../Images/d7605cc804de71ca5d13e72f4d7e8835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6eKC62OF_s6cjUaWmOT5qw.png"/></div></div></figure><h2 id="fa81" class="lv kk hh bd kl mc md me kp mf mg mh kt js mi mj kx jt mk ml lb ju mm mn lf mo bi translated">人为什么会感到悲伤？</h2><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/dbc414be14daf581bc12f8f824a79a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kO-HLQA_WdRX6ZKgDgEYjA.png"/></div></div></figure><h2 id="730d" class="lv kk hh bd kl mc md me kp mf mg mh kt js mi mj kx jt mk ml lb ju mm mn lf mo bi translated">怎么挣钱？</h2><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mq"><img src="../Images/8728acb0fbf8cfb17e89a894e1994785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mu8MuyE16cEXzzQbU9tRg.png"/></div></div></figure><h2 id="8a22" class="lv kk hh bd kl mc md me kp mf mg mh kt js mi mj kx jt mk ml lb ju mm mn lf mo bi translated">如何变得成功？</h2><figure class="lm ln lo lp fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/dc2219eb09e5a419cdeaa9987b83d6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwdNKwH0KMSjo7mAH1d1JA.png"/></div></div></figure><h1 id="3ef7" class="kj kk hh bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">参考</h1><ol class=""><li id="73d1" class="jv jw hh iv b iw lh ja li js ms jt mt ju mu jq ka kb kc kd bi translated">伯特微调检查一个句子是否包含问题的答案:【https://www.tensorflow.org/text/tutorials/bert_glue<a class="ae jr" href="https://www.tensorflow.org/text/tutorials/bert_glue" rel="noopener ugc nofollow" target="_blank"/></li><li id="3617" class="jv jw hh iv b iw ke ja kf js kg jt kh ju ki jq ka kb kc kd bi translated">名言:<a class="ae jr" href="https://github.com/akhiltak/inspirational-quotes/blob/master/Quotes.csv" rel="noopener ugc nofollow" target="_blank">https://github . com/akhiltak/inspirational-quotes/blob/master/quotes . CSV</a></li></ol><div class="mv mw ez fb mx my"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hi fi z dy nd ea eb ne ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">medium.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm in my"/></div></div></a></div></div></div>    
</body>
</html>