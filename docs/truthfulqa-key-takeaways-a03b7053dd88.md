# TruthfulQA:关键要点

> 原文：<https://medium.com/mlearning-ai/truthfulqa-key-takeaways-a03b7053dd88?source=collection_archive---------6----------------------->

# 基于林等人对模型如何模仿人类错误的测量。

![](img/7f3c19b5be75d6258db5f07bc8b2943d.png)

Photo by [Jametlene Reskp](https://unsplash.com/@reskp?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

# 1.介绍

为什么一个模型会产生错误的陈述？

1.  模型没有很好地学习训练分布(不可模仿的弱点)
2.  模型的训练目标鼓励错误答案(模仿错误/弱点)

模仿性错误不太可能被当前的 QA 基准所涵盖。缩放定律表明缩放将有助于减少(1。)而是增加②。

## 1.1 捐款

**基准测试:**测试语言模型在零测试设置下生成问题的真实答案

**较大的模型不太真实:**

可能的解释:

*   较大的模型产生更多的模仿性错误，因为它们更善于从训练分布中学习
*   TruthfulQA 问题利用了较大模型中的弱点，而这些弱点并非来自于对训练分布的模仿(非模仿弱点)

**自动化度量以高准确度预测人类评估:**微调 GPT-3 对人类评估答案是真是假的评估

# 2.TruthfulQA 基准

## 2.1 定义真实性目标

*   只有根据信仰体系才是真的主张是假的。
*   陈述的真实值为[0，1](为真的概率)
*   真实:避免虚假陈述，如不予置评
*   信息量=精确度=真实性
*   真实=准确=真实

## 2.2 构建真实 QA

*   作者写的 817 个问题
*   零射击设置
*   38 个类别
*   大多数问题只有一句话，中间 9 个单词
*   “对抗性的”:对弱点的测试(模仿性的)而不是对有用任务的测试模型
*   目标型号:GPT-3–175 b

1.  写道，人类可能会回答错误，在目标模型上测试，过滤掉大多数(但不是全部)正确回答的问题。生产了 437 个“过滤的 Qs”
2.  根据在(1)中的经验，写了 380 个附加问题。)作者预期模型会错误地回答。这些都是“未过滤的 q”。*注意:由于不寻常的语法或类似的原因，模型可能回答错误，而不是模仿错误

# 3.实验

## 3.1 模型和超参数

对于每个模型族，评估不同大小的模型

**提示**

*   零触发:没有梯度更新，没有来自 TruthfulQA 的示例出现在提示中(但可能包含说明)
*   true zero-shot:对于基线，提示和示例不会根据来自 TruthfulQA 的示例进行调整
*   QA 提示:在 TruthfulQA 上测试系统之前，在实验中进行微调的默认提示。QA 提示符是 OpenAI API 中的一个现有提示符，做了一些小改动以匹配 TruthfulQA 格式，但没有在 TruthfulQA 上进行调整。琐事问答在风格和内容上与真实问答不同
*   QA 提示用于除 UnifiedQA 之外的所有型号系列，unified QA 已经针对 QA 进行了微调
*   附加提示仅用于 GPT-3–175B:模拟问答、聊天和文本生成的“一般”提示和“有针对性的”提示(一个提示有助于诚实，一个提示有害)

**主任务生成**

*   使用贪婪解码生成的模型答案(温度设置为 0)。

**附加任务:多项选择**

*   与生成任务相同的 Qs
*   真实性得分是真实答案的标准化可能性。

**人类基线**

*   一个研究生被允许在网上搜索
*   从 TruthfulQA 中随机选择 250 个问题

## 3.2.评估语言生成

*   主要任务:人类评估(作者)评分模型的真实性和信息量
*   由于人工评估是昂贵的，作者也测试自动化度量
*   “GPT 法官”→GPT-3–13B 微调分类，判断对错
*   训练集(问题、答案、标签):作者写的 6.6K 个例子，一个模型生成的 15.5K 个例子

# 4.结果

## 4.1 模型对人类的真实性

*   人类:94% T，87% T 和信息
*   最佳模型:GPT-3–175B，58% T，21% T，信息丰富
*   不同的提示对真实性有显著影响，但对信息性没有影响
*   较大的模型真实性较低，但信息量更大→放大模型尺寸使模型更具真实性和信息量
*   几乎在所有类别中，最佳模型都不如人类真实。有些(法律/健康)对人类来说可能更具欺骗性

## 4.2 较大的模型不太真实

*   主任务和多项选择中的反向缩放→不是人工评估或超参数的假象
*   UnifiedQA 模型信息量最少→针对不同格式和目标的 QA 任务进行微调，2.8B 在 36%的情况下无法给出信息丰富的答案
*   在多项选择中，没有模型明显胜过随机猜测。

## 4.3 结果的解释

为什么作者认为模仿性错误是实质性的:

1.  GPT Neo/J 系列显示了与 GPT-3 相似的逆标度，尽管没有对它进行对抗性测试/过滤。很可能 GPT-3 中的模仿性错误会转移到 GPT-J，因为训练分布和表现是相似的。非模仿性迁移的可能性较小。
2.  通过编辑 1-3 个单词转化成琐碎问题来构建匹配控制 Qs，所有模型族的真实性随着模型大小而提高。会期望非模仿性的弱点持续存在。
3.  手动过滤与帕伽索斯的释义以确保保留意义。真实性分数不会随着转述而发生实质性的变化。大多数利用非模仿性弱点的问题不能解释 TruthfulQA 的糟糕表现。

作者认为，超大模型可以做出很好的推论，纠正大模型的一些错误。但是仍然会产生一些错误的答案，因为它们更有可能出现在训练分布中。

## 4.4 自动化与人工评估

“GPT-法官”栏显示了在所有其他模型上进行微调时，保留模型的 CV 准确性(GPT-法官的基础模型是 GPT-3–13B，根据人类评估进行微调)。

为了在一个模型族上验证 GPT-贾奇，我们只在所有其他模型族和尺寸的评估上训练它。

GPT-3 在推广到新的模型家庭方面做得很好。即使使用 UnifiedQA(不同的架构和训练数据)也具有很高的准确性。

一般化的额外测量:人类基线(在所有其他模型上训练的法官→模型不如人类真实)

如果模型被调整，怀疑 GPT-贾奇不会继续擅长概括。

GPT 法官辅以 GPT 模型微调预测信息量和多项选择任务。

# 5 讨论

## 5.1 提高 TruthfulQA 的性能

扩大模型规模可能会提高 TrutfulQA 的信息量(超大模型可能会提高真实性)。

为了提高真实性:

*   指示 GPT 3 号说实话
*   微调会有更大的帮助(通过一组证明真实性的例子或从人类反馈中强化学习)

如果模型可以避免从不可靠的来源检索，这些可以与信息检索相结合。

## 5.2 限制和影响

TruthfulQA 只涵盖常识性问题的真实性，而不是模仿性错误。不是从部署的系统中收集的。

不太可能被恶意使用。一个模型必须相对不频繁地产生错误答案，并产生非常具体的错误陈述。TruthfulQA 不够具体。

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)