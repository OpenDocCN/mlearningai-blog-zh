<html>
<head>
<title>Images from the Convolutional World</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自卷积世界的图像</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/images-from-the-convolutional-world-596b4aa6cdae?source=collection_archive---------1-----------------------#2021-08-31">https://medium.com/mlearning-ai/images-from-the-convolutional-world-596b4aa6cdae?source=collection_archive---------1-----------------------#2021-08-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ce4b03c186dc87d35b9438a9b4cfd0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8l4ksfo98aeNmETYJX_Jeg.jpeg"/></div></div></figure><p id="5d23" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的<a class="ae jn" rel="noopener" href="/mlearning-ai/neural-networks-from-the-beginnings-470fbeab0fc8">上一篇文章</a>的基础上，我们将神经网络向前推进了一步，用一种叫做卷积的过程对图像进行分类。</p><p id="9a9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> CNN </strong> ( <em class="jo">卷积神经网络</em>)是一种人工神经网络，它模仿人眼对其各层进行处理，使其能够识别物体并“看见”。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/4984c4296b6c3011487a8694ed69adc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*ZShPBrqMmmqi_B49BoEZEg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">General CNN</figcaption></figure><p id="5028" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">CNN 包含几个具有层次结构的专门隐藏层，例如，第一层可以检测线条、曲线和边缘，然后进行专门处理，直到它们到达更深的层，识别复杂的形状，如面部或人体的一部分。</p><p id="4cfe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这次旅行中，首先我们必须了解如何使用这种新类型的输入数据:</p><h1 id="1938" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">形象</h1><p id="9291" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">数字图像有3个属性:以像素为单位的宽度和高度(分辨率)，以及称为通道的RGB(红、绿、蓝)格式的颜色。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/f22a481e1542a5655cb6bbb2368f43e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*eET6z-QkPmhDYnIDfedGsw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">An RGB image</figcaption></figure><p id="1940" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的例子中，我们将使用一个名为CIFAR-10的微小图像数据集，它很好地服务于我们的学习目的。它由60，000个32×32彩色(RGB)图像组成，用一个整数标记，对应10类中的一类(<em class="jo">飞机(0)、汽车(1)、鸟(2)、猫(3)、鹿(4)、狗(5)、青蛙(6)、马(7)、船(8)和卡车(9) </em>)。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/e8782e08f5e977c1836f135827a73e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjMi1lMedD3VII-fwxUKMg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">Cifar10 dataset classes</figcaption></figure><p id="3dad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将继续在我们的项目中使用Pytorch，所以我们将从您的torchvision库中导入这个数据集。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="71f8" class="li jz hh le b fi lj lk l ll lm">from torchvision import datasets <br/>data_path = './dlpytorch/' <br/>cifar10 = datasets.CIFAR10(data_path, train=True, download=True) cifar10_val = datasets.CIFAR10(data_path, train=False,              download=True) <br/>len(cifar10)</span></pre><p id="835b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将下载参数设置为True，表示<em class="jo">数据路径</em>，将<em class="jo">训练</em>设置为True，表示我们对训练集感兴趣。在值的情况下，我们的<em class="jo">训练</em>将为假以获得验证数据。最后，我们验证了所获得的数据集的大小:50，000张图像</p><p id="aa28" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，数据集的每个图像都是RGB PIL图像的一个实例，让我们来可视化其中的一些图像:</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/7fde1b52365ff72d96a09c933b880f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*CrpuhJIFw-GBjkFAnTxdyQ.png"/></div></figure><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="813b" class="li jz hh le b fi lj lk l ll lm"><br/># plot ex. image <br/>img, label = cifar10[79] <br/>plt.imshow(img) <br/>plt.show()<br/></span><span id="5a08" class="li jz hh le b fi lo lk l ll lm">print(f”Label:{label} — Class:{class_names[label]}”)</span></pre><p id="15fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">标签:1 —类别:汽车</em></p><p id="9398" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们需要将我们的图像转换成张量，然后才能用Pytorch做任何事情</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="ee89" class="li jz hh le b fi lj lk l ll lm">from torchvision import transforms<br/>to_tensor = transforms.ToTensor()<br/>img_t = to_tensor(img)<br/>img_t.shape</span></pre><p id="c256" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">火炬。尺寸([3，32，32]) </em></p><p id="9c20" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这样，<em class="jo"> </em>图像就变成了一个3 ( <em class="jo"> RGB通道</em> ) × 32 × 32的张量。好了，让我们使用数据集变换参数将所有图像数据集转移到Pytorch张量:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="4c79" class="li jz hh le b fi lj lk l ll lm">tensor_c10 = datasets.CIFAR10(data_path, train=True, download=False,                                                <br/>                             transform=transforms.ToTensor())<br/>                             <br/># load the same image and view the the tensor shape and type<br/>img_t, _ = tensor_c10[79]<br/>img_t.shape, img_t.dtype</span></pre><p id="0603" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">(火炬。尺寸([3，32，32])，torch.float32) </em></p><p id="64b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">ToTensor变换将数据转换为每个通道的32位浮点，将值从0.0缩小到1.0，同时记住，我们必须改变轴的顺序，以便使用matplotlib ( <em class="jo">从RGB通道-H-W到H-W-RGB通道</em>)重新查看我们的图像。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="df38" class="li jz hh le b fi lj lk l ll lm"># Permute columns and visualize image again<br/>plt.imshow(img_t.permute(1, 2, 0))<br/>plt.show()</span></pre><p id="68bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">归一化图像数据</strong></p><p id="d60b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是因为通过选择在0加或减1 ( <em class="jo">或2 </em>)附近线性的激活函数，我们将数据保持在相同的范围内，因此神经元更可能具有非零梯度，因此它们将更早地学习。此外，将每个信道归一化为具有相同的分布将确保信道信息可以通过使用相同学习速率的梯度下降来混合和更新。</p><p id="7ba3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，我们必须计算数据集上每个通道的<em class="jo">平均值</em>和<em class="jo">标准偏差，并应用以下变换:</em></p><p id="4e3f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">v _ norm[c]=(v[c]-mean[c])/stdev[c]</p><p id="059f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们为CIFAR-10训练集计算它们:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="8323" class="li jz hh le b fi lj lk l ll lm"># Normalizing data<br/># 1. stack the dataset tensors along an extra dimension<br/>imgs = torch.stack([img_t for img_t, _ in tensor_c10], dim=3)<br/>imgs.shape</span></pre><p id="5758" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">火炬。尺寸([3，32，32，50000]) </em></p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="ee00" class="li jz hh le b fi lj lk l ll lm"># 2. compute mean per channel<br/>imgs.view(3, -1).mean(dim=1)</span></pre><p id="6b1a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">张量([0.4914，0.4822，0.4465]) </em></p><p id="4a62" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">view(3，-1)保留3个通道，并将所有剩余的维度合并为一个，因此我们的3 × 32 × 32图像被转换为3 × 1，024向量，然后对每个通道的1，024个元素取平均值。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="33e2" class="li jz hh le b fi lj lk l ll lm"># 3. compute std deviation<br/>imgs.view(3, -1).std(dim=1)</span></pre><p id="fa4d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">张量([0.2470，0.2435，0.2616]) </em></p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="301e" class="li jz hh le b fi lj lk l ll lm"># 4 normalize the data <br/>normalized_c10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.Compose([<br/>                       transforms.ToTensor(),<br/>                       transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616)) ]))</span></pre><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/8b425368e301d33bbab22b7d3562eba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*QdcLKQGTk3Ysh9u8uYH9Eg.png"/></div></figure><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="d093" class="li jz hh le b fi lj lk l ll lm"># view normalized image </span><span id="e281" class="li jz hh le b fi lo lk l ll lm">img_t, _ = normalized_c10[79] </span><span id="d1a3" class="li jz hh le b fi lo lk l ll lm">plt.imshow(img_t.permute(1, 2, 0)) </span><span id="fac8" class="li jz hh le b fi lo lk l ll lm">plt.show()</span></pre><h1 id="85a3" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">从完全连接到卷积神经网络</h1><p id="100f" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">假设我们要构建一个全连通的NN ( <em class="jo">如</em> <a class="ae jn" rel="noopener" href="/mlearning-ai/neural-networks-from-the-beginnings-470fbeab0fc8"> <em class="jo">上一篇</em> </a>中所解释的)，这次我们将使其更深，为此我们将添加一个新的隐藏层。我们的输入层将有3072个值(<em class="jo"> 3 * 32 * 32 </em>)，并将为第一个隐藏层产生1024个值的输出，为第二个隐藏层产生512个值的输出。隐藏层，输出层为128，将输出10个值(每个类的概率)</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="922a" class="li jz hh le b fi lj lk l ll lm"># fully connected NN<br/>fc_model = nn.Sequential(<br/>          nn.Linear(3072, 1024),<br/>          nn.Tanh(),<br/>          nn.Linear(1024, 512),<br/>          nn.Tanh(),<br/>          nn.Linear(512, 128),<br/>          nn.Tanh(),<br/>          nn.Linear(128, 10))</span></pre><p id="3b0c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看看我们的网络中会有多少参数(<em class="jo">只是权重，还有待添加偏差)</em>:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="f538" class="li jz hh le b fi lj lk l ll lm"># how many parameters have our fully connected NN ?<br/>param_list = [p.numel()<br/>              for p in fc_model.parameters()]</span><span id="fe29" class="li jz hh le b fi lo lk l ll lm">sum(param_list), param_list</span></pre><p id="fd12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> (3738506，[3145728，1024，524288，512，65536，128，1280，10]) </em></p><p id="d396" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">哇！！<strong class="ir hi"> 3.738.506 </strong>参数，为什么这么多？</p><p id="b7f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">记住一个线性层计算出<strong class="ir hi"> y = weight * x + bias </strong>，如果x的长度为3072，y的长度必须为1024，那么权重张量的大小需要为1024×3072，bias的大小必须为1024。因此，第一层有1，024 * 3，072 + 1，024 = 3，146，752个参数。</p><p id="d0a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这告诉我们，我们的神经网络在像素方面没有很好的缩放，想象一下，如果我们有1024x1024 RGB图像，只有310万个输入值，超过30亿个参数，会发生什么！！</p><p id="c1f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">盘旋营救</strong></p><p id="1ac1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们想要<strong class="ir hi">识别与物体相对应的图案</strong>，例如一条路线上的汽车，我们可能需要查看附近的像素是如何排列的，我们对彼此远离的像素如何组合起来不太感兴趣，因此重要特征的组合往往是彼此在一起的像素。如果我们想在图像中检测我们的福特汽车，角落里是否有树或云并不重要。</p><p id="5483" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了数学地解释这一点，我们可以计算一个像素与其紧邻像素的加权和，而不是图像中所有其他像素的加权和。这将等同于构建<em class="jo">权重矩阵</em>，每个输出特征和输出像素位置一个，其中超过距中心像素一定距离的所有权重将为零。</p><p id="480f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为这些局部模式对输出有影响，不管它们在图像中的位置如何，我们必须实现<strong class="ir hi">不变平移</strong>。</p><p id="cf3f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">幸运的是，我们在图像转换中有一个线性的、局部的和不变的操作:</p><h1 id="fbe3" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">卷积</h1><p id="d1d6" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">我们可以将2D图像的卷积定义为权重矩阵的点积:<strong class="ir hi">内核</strong>在输入处具有每个邻域，生成新的输出矩阵。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es jp"><img src="../Images/21fbfb34823b1fe0216d44355d2a560a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*uyVLq2agBdDnDnmZ7ldt9Q.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">convolutional kernel</figcaption></figure><p id="9c04" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该内核将在输入图像中从左到右、从上到下移动，就好像一个<strong class="ir hi">补丁</strong>被放到图像上。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lq"><img src="../Images/087f74325e86f14fd8821bb5e979dc68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45acv9Cj5PyA6O_dZZ0-3Q.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">kernel moves over image</figcaption></figure><p id="ef45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总的来说，优点是:</p><ul class=""><li id="3e7e" class="lr ls hh ir b is it iw ix ja lt je lu ji lv jm lw lx ly lz bi translated">社区中的本地操作</li><li id="27a4" class="lr ls hh ir b is ma iw mb ja mc je md ji me jm lw lx ly lz bi translated">翻译不变性</li><li id="1c53" class="lr ls hh ir b is ma iw mb ja mc je md ji me jm lw lx ly lz bi translated">参数少得多的模型</li></ul><p id="8bd0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">内核(<em class="jo">也叫卷积矩阵</em>)一般都是方形的小(<em class="jo"> 3x3，5x5 </em>)，通常用随机值初始化。当然，选择内核大小需要权衡，我们将在后面讨论。</p><p id="00b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们开始看一些代码，Pytorch提供了1，2，3维的卷积:nn。时间序列的Conv1d，nn。用于图像和神经网络的Conv2d。用于体积/视频的Conv3d。我们将为图像创建一个2d卷积:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="b351" class="li jz hh le b fi lj lk l ll lm"># create Conv2d<br/>conv = nn.Conv2d(3, 16, kernel_size=3)<br/>conv</span></pre><p id="efae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> Conv2d(3，16，kernel_size=(3，3)，stride=(1，1)) </em></p><p id="a953" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">参数:3是输入特征的数量(<em class="jo">或本例中的通道</em>)，16是输出通道，这是一个任意的数字，但是通道越多，网络检测不同类型特征的能力就越大，kernel_size是3 ( <em class="jo"> Pytorch假设3x3 </em>)。最后，默认情况下，步幅为1，这是内核在图像中滑动时的步长。</p><p id="f993" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，在这种情况下，我们的权重张量为:16 (out_ch) x 3 (in_ch) x 3 x 3，偏差大小为16。验证:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="ede0" class="li jz hh le b fi lj lk l ll lm">conv.weight.shape, conv.bias.shape</span></pre><p id="e3a6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">(火炬。大小([16，3，3，3])，火炬。尺寸([16])</em></p><p id="b4b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好，对我们的示例图像应用卷积:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="91e8" class="li jz hh le b fi lj lk l ll lm"># Apply convolution<br/>output = conv(img_t.unsqueeze(0))<br/>img_t.unsqueeze(0).shape, output.shape</span></pre><p id="ca6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">(火炬。大小([1，3，32，32])，火炬。尺寸([1，16，30，30])</em></p><p id="07d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">unsqueeze向输出添加一个新的dim 0，因为Conv2d期望一个形式为<strong class="ir hi"> B(atch) </strong> <strong class="ir hi"> × C × H × W </strong>的张量作为输入，在这种情况下，批处理只有1个图像。</p><p id="458d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">和显示卷积图像:</p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/ccdddb14fd412d257b516ae8902a5b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*XYPJ7UxiLR_RAwVJNU7_kg.png"/></div></figure><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="b170" class="li jz hh le b fi lj lk l ll lm">plt.imshow(output[0, 0].detach()) </span><span id="f9be" class="li jz hh le b fi lo lk l ll lm">plt.show()</span></pre><p id="bf6d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，输出的形状是30x30，而不是32x32，因此卷积后，我们在每个维度上都丢失了两个像素。为了解决这个问题，Pytorch为我们提供了<strong class="ir hi">填充</strong>图像的可能性，方法是在边界周围创建零值的鬼像素。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="377b" class="li jz hh le b fi lj lk l ll lm">conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)</span></pre><p id="ac8d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在假设我们希望我们的内核执行一个<em class="jo">边缘检测</em>。我们如何给这个新内核分配权重呢？</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="d53a" class="li jz hh le b fi lj lk l ll lm"># create new conv to perform edge detection<br/>conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)</span><span id="eef8" class="li jz hh le b fi lo lk l ll lm"># disable gradient calculation and set the weights<br/>with torch.no_grad():<br/>    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],<br/>                                   [-1.0, 0.0, 1.0],<br/>                                   [-1.0, 0.0, 1.0]])<br/>    conv.bias.zero_()</span></pre><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/4b6a9414316e05eac944e614f609da37.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*xVDHIyKF26HbVkz9kzbBBQ.png"/></div></figure><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="3b65" class="li jz hh le b fi lj lk l ll lm">output = conv(img_t.unsqueeze(0)) </span><span id="1876" class="li jz hh le b fi lo lk l ll lm">plt.imshow(output[0, 0].detach()) </span><span id="f120" class="li jz hh le b fi lo lk l ll lm">plt.show()</span></pre><p id="b7cc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过这种方式，我们可以构建许多更精细的过滤器。CNN的工作是在连续的层中估计一组滤波器组的核心，这些层将把一个多通道图像转换成另一个多通道图像，其中不同的通道对应于不同的特征。</p><p id="e8f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将有一个输出通道x内核(像一个通道的平均，另一个通道的垂直边缘，等等。)</p><p id="8cfa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">内核大小权衡</strong></p><p id="a6a5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">较小的内核会给你很多细节，但它会导致你过度拟合，而且计算量很大。</p><p id="37ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">较大的内核会丢失很多细节，并可能导致欠拟合，但计算时间更快，内存使用更少。</p><p id="4f67" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，你应该调整你的模型，找到最佳尺寸。使用奇数内核是很常见的，最常用的是3x3和5x5。</p><h1 id="4d3b" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">向下采样</h1><p id="7273" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">下采样或汇集旨在基于某些数学运算(如平均或最大汇集)来减少图像的空间维度。结合卷积和下采样可以帮助我们识别更大的结构。</p><p id="fd21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，将图像缩放一半相当于将4个相邻像素(位置)作为输入，产生一个像素作为输出。图像的这种下采样可以通过应用:</p><ul class=""><li id="2480" class="lr ls hh ir b is it iw ix ja lt je lu ji lv jm lw lx ly lz bi translated">平均池:4个像素的平均值，这是第一种方法，但现在已经废弃不用了</li><li id="4719" class="lr ls hh ir b is ma iw mb ja mc je md ji me jm lw lx ly lz bi translated">最大池化:取四个像素中的最大值，目前是最常见的</li></ul><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mh"><img src="../Images/5c44b9bcb8d20d6ff7324fb525226fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9kQaofGIPqRlFj3T4ozQA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">Max Pooling example</figcaption></figure><p id="bbb6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">缩减像素采样有助于捕捉渲染图像的基本结构特征，而不会过分关注细节，并且通常充当噪声抑制剂。</p><p id="137c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">组合卷积和池化的优势</strong></p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mi"><img src="../Images/f551f709071e76ffd4584379e86c8208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rRclGw64I_VWq1NRcYnbg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">Combine convolutions and pooling</figcaption></figure><p id="74a0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的例子中，第一组核在小邻域和低级特征中操作，而第二组核有效地在较大邻域中操作，产生由先前特征组成的特征。</p><p id="a01d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种组合使CNN能够观看非常复杂的场景。</p><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mj"><img src="../Images/6e7a72da78c7d790ae8e109f9289eb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lY-EhljTb46QieuVj4G_sg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">A CNN with convolution and max pooling</figcaption></figure><p id="22f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">特征映射</strong></p><figure class="jq jr js jt fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/bfdc7eb05fa4406637aa517ba600ff99.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*pwqlFFfX-4GgGU-C.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Feature mapping</figcaption></figure><p id="c769" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特征地图是应用于前一层的一个过滤器的输出。</p><p id="7426" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果我们有16个核的第一卷积，我们将有16个输出矩阵(<em class="jo">特征映射</em>)</p><p id="d086" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> CNN编码时间！</strong></p><p id="39e7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好了，现在是时候用卷积和池来重建我们的神经网络，然后检查我们是否有一些可接受的参数，以便训练比完全连接的神经网络更快，计算成本更低。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="db85" class="li jz hh le b fi lj lk l ll lm"># CNN combining convolutions and pooling<br/>model = nn.Sequential(<br/>    nn.Conv2d(3, 16, kernel_size=3, padding=1),<br/>    nn.Tanh(),<br/>    nn.MaxPool2d(2),<br/>    nn.Conv2d(16, 8, kernel_size=3, padding=1),<br/>    nn.Tanh(),<br/>    nn.MaxPool2d(2),<br/>    nn.Linear(8 * 8 * 8, 32),<br/>    nn.Tanh(),<br/>    nn.Linear(32, 10))</span></pre><p id="3c05" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第一个卷积采用3个通道到16个通道，因此它生成16个独立的特征，用于区分图像的低级特征，然后我们应用Tanh激活函数，最后将16通道32 × 32图像汇集成16通道16×16图像(<em class="jo"> MaxPool2d </em>)。</p><p id="2fa9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对第二卷积、Tanh和Pool进行相同的处理，最后我们将8通道8×8图像传递到线性模块，并将32个元素输出到最终线性模块，最终线性模块输出10个元素(<em class="jo"> 10个概率，Cifar10数据集中每类图像一个</em>)。</p><p id="faf1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们获得了该网络需要与完全连接的网络进行比较的参数数量:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="7a91" class="li jz hh le b fi lj lk l ll lm">numel_list = [p.numel() for p in model.parameters()]<br/>sum(numel_list), numel_list</span></pre><p id="8414" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo"> (18354，[432，16，1152，8，16384，32，320，10]) </em></p><p id="ca14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">18.354</strong>vs<em class="jo"/><strong class="ir hi">3 . 738 . 506</strong><em class="jo"/>真是大裁员！！</p><p id="1183" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们试图将图像应用到我们的模型来进行预测，它将会给出一个错误。这是因为在最后一次卷积后，我们必须将8通道* 8*8图像重新整形为512 1D矢量。</p><p id="45c0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但遗憾的是，当我们使用nn时，我们没有任何显式的每个模块的输出可见性。Pytorch中的Sequential，所以我们必须子类化nn。模块:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="b727" class="li jz hh le b fi lj lk l ll lm"># The solution: make our own nn.module subclass<br/>class Net(nn.Module):<br/>  def __init__(self):<br/>    super().__init__()<br/>    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)<br/>    self.act1 = nn.Tanh()<br/>    self.pool1 = nn.MaxPool2d(2)<br/>    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)<br/>    self.act2 = nn.Tanh()<br/>    self.pool2 = nn.MaxPool2d(2)<br/>    self.fc1 = nn.Linear(8 * 8 * 8, 32)<br/>    self.act3 = nn.Tanh()<br/>    self.fc2 = nn.Linear(32, 10)</span><span id="7501" class="li jz hh le b fi lo lk l ll lm">  # That takes the inputs to the module and returns the output<br/>  def forward(self, x):<br/>    out = self.pool1(self.act1(self.conv1(x)))<br/>    out = self.pool2(self.act2(self.conv2(out)))<br/>    out = out.view(-1, 8 * 8 * 8)  # the reshape to 1D 512 elements<br/>    out = self.act3(self.fc1(out))<br/>    out = self.fc2(out)<br/>    return out</span></pre><p id="50f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将做一个新的重构，因为一些模块像nn。Tanh和nn。MaxPool2d没有参数，没有必要在新的子类中注册它们。为此，Pytorch有一个函数API(<em class="jo">torch . nn . functional</em>)，我们将使用它来执行这个任务。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="674c" class="li jz hh le b fi lj lk l ll lm"># Refactor the Net subclass to use funct. Api for Activat.funct. and Pooling</span><span id="bbc6" class="li jz hh le b fi lo lk l ll lm">import torch.nn.functional as F<br/>class Net(nn.Module):<br/>    def __init__(self):<br/>      super().__init__()<br/>      self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)<br/>      self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)<br/>      self.fc1 = nn.Linear(8 * 8 * 8, 32)<br/>      self.fc2 = nn.Linear(32, 10)</span><span id="d1fe" class="li jz hh le b fi lo lk l ll lm">    def forward(self, x):<br/>      out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)<br/>      out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)<br/>      out = out.view(-1, 8 * 8 * 8)<br/>      out = torch.tanh(self.fc1(out))<br/>      out = self.fc2(out)<br/>      return out</span></pre><p id="cef4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将图像应用于网络模型:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="dc4b" class="li jz hh le b fi lj lk l ll lm"># we obtain 10 probabilities, 1 per class in the Cifar10 Dataset<br/>model = Net()<br/>model(img_t.unsqueeze(0))</span></pre><p id="1a11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">张量([[ 0.1109，0.1352，0.1018，-0.0142，-0.1172，0.0679，0.0349，0.1117，0.0222，0.1687]]，grad _ fn =&lt;AddmmBackward&gt;)</em></p><p id="c5d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">在CNN分类模型中使用的损失函数:Softmax交叉熵损失</strong></p><p id="db13" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们之前关于神经网络的帖子中，我们使用MSE作为损失函数，但这里我们面临一个分类问题，所以我们应该使用一个函数，更好地将输出值解释为概率。也就是说，数组的每个值必须在0-1之间，并且每个样本的向量总和必须为1。</p><p id="1680" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Softmax交叉熵通过产生比相同输入的MSE更陡的梯度来利用这些特性。它有两个组成部分:</p><ul class=""><li id="ecb1" class="lr ls hh ir b is it iw ix ja lt je lu ji lv jm lw lx ly lz bi translated">a) <strong class="ir hi"> Softmax函数</strong>:相对于其他值，强烈放大最大值，迫使NN在它认为正确的预测上不那么中立。例如:</li></ul><p id="a6a7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">normalize(np.array([10，6，4]) ==&gt; array([0.5，0.3，0.2])</p><p id="d09d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> softmax </strong> (np.array([10，6，4]) == &gt; array([0.84，0.11，0.04])</p><ul class=""><li id="354e" class="lr ls hh ir b is it iw ix ja lt je lu ji lv jm lw lx ly lz bi translated">b) <strong class="ir hi">交叉熵损失</strong>:对于区间[0，1]来说，罚分比MSE高得多，并且当预测(p)和目标(y)之间的差接近1时，罚分变得更陡地接近∞。</li></ul><figure class="jq jr js jt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ml"><img src="../Images/28c4887b531c1b43fbe311d93a9b3922.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*YdVRszdfTGzc7AmVqXsb9g.png"/></div></div></figure><p id="4ddb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">示例:</p><p id="32da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">y = 0时交叉熵损失与均方误差的关系</p><h1 id="3732" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">训练卷积神经网络模型</h1><p id="a15e" class="pw-post-body-paragraph ip iq hh ir b is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm ha bi translated">好了，现在我们必须训练我们的模型，让我们开始工作。为了能够更快地执行我们的训练，我们将使用来自Cifar10的精简数据集，其中只有3个类别的图像，而不是最初的10个(<em class="jo">飞机、汽车和船只</em>)称为Cifar3:)。你可以在github repo里看到代码。</p><p id="5d09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们创建了一种在n个时期的循环中训练网络的方法。我们将使用Pytorch提供的数据加载器向网络提供成批的图像(在我们的示例中是64)。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="1759" class="li jz hh le b fi lj lk l ll lm"># Training the Net CNN<br/>import datetime</span><span id="e34b" class="li jz hh le b fi lo lk l ll lm">def training_net(n_epochs, optimizer, model, loss_fn, train_loader):<br/>    for epoch in range(1, n_epochs + 1):<br/>        loss_train = 0.0</span><span id="4753" class="li jz hh le b fi lo lk l ll lm">        # The DataLoader (train_loader) create batches of 64 imgs (batch_size)<br/>        for imgs, labels in train_loader:<br/>            outputs = model(imgs)<br/>            # compute the loss to minimize<br/>            loss = loss_fn(outputs, labels)<br/>            # put 0 the grads of the last round, compute new grads and update model<br/>            optimizer.zero_grad()<br/>            loss.backward()<br/>            optimizer.step()<br/>            # sums the losses we saw over the epoch<br/>            # item() to transform loss to a python number<br/>            loss_train += loss.item()</span><span id="024a" class="li jz hh le b fi lo lk l ll lm">        if epoch == 1 or epoch % 10 == 0:<br/>            print('{} Epoch {}, Training loss {}'.format(<br/>            datetime.datetime.now(), epoch,<br/>            loss_train / len(train_loader)))</span></pre><p id="3321" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们将创建一个CNN模型的实例，一个SGD优化器，一个交叉熵损失，并将它们传递给训练网络。</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="dd7e" class="li jz hh le b fi lj lk l ll lm">train_loader = torch.utils.data.DataLoader(cifar3, batch_size=64,<br/>                shuffle=True)<br/>model = Net() <br/>optimizer = optim.SGD(model.parameters(), lr=1e-2) <br/>loss_fn = nn.CrossEntropyLoss() <br/>training_net(<br/>              n_epochs = 20,<br/>              optimizer = optimizer,<br/>              model = model,<br/>              loss_fn = loss_fn,<br/>              train_loader = train_loader,<br/>              )</span></pre><p id="da6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:35:11.218508第一纪元，训练损耗1.23238880456</em></p><p id="69f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:36:10.194867纪元10，训练损耗0.46648594766</em></p><p id="cfb5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:37:15.662480纪元20，训练损耗0.3668626133877</em></p><p id="0083" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:38:21.163712第30纪元，训练损耗0.3332703081618</em></p><p id="f477" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi">…</p><p id="7a32" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:44:56.936097纪元90，训练损耗0.185609463060157</em></p><p id="4efd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">2021–08–27 15:46:02.644376纪元100，训练损耗0.16131676103206</em></p><p id="2ba8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们将根据验证数据来衡量模型的准确性:</p><pre class="jq jr js jt fd ld le lf lg aw lh bi"><span id="8707" class="li jz hh le b fi lj lk l ll lm"># build the data loaders for train and validation<br/>train_loader = torch.utils.data.DataLoader(cifar3,     <br/>                     batch_size=64,shuffle=False)<br/>val_loader = torch.utils.data.DataLoader(cifar3_val, <br/>                     batch_size=64,shuffle=False)</span><span id="b6ab" class="li jz hh le b fi lo lk l ll lm"># measure the accuracy using the validation dataset<br/>def validate_net(model, train_loader, val_loader):<br/>    for name, loader in [("train", train_loader), ("val", val_loader)]:<br/>        correct = 0<br/>        total = 0<br/>        # don't want grads here<br/>        with torch.no_grad():<br/>          for imgs, labels in loader:<br/>              outputs = model(imgs)<br/>              # gives the index of the highest value of outputs<br/>              _, predicted = torch.max(outputs, dim=1)<br/>              # increased total with batch size<br/>              total += labels.shape[0]<br/>              # Comparing the predicted class that had the maximum probability with the correct label. The sum gives the number of items are agree<br/>              correct += int((predicted == labels).sum())<br/>        print(f"Accuracy {name}: {correct / total}")</span><span id="dd67" class="li jz hh le b fi lo lk l ll lm">validate_net(model, train_loader, val_loader)</span></pre><p id="63b7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">精度训练:0.9295 </em></p><p id="109c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">精度值:0.8713 </em></p><p id="45d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好吧，伙计们，这个帖子里的信息太多了。在下一个视频中，我们将会谈到:</p><p id="765c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如何存储和检索我们的神经网络的训练参数，调整我们的网络对抗过度拟合的方法，以及在GPU上运行我们的训练。</p><p id="a777" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">像往常一样，github与完整的<a class="ae jn" href="https://github.com/jrercoli/nn_convolutional_world" rel="noopener ugc nofollow" target="_blank">神经网络jupyter nb </a>的链接被附上，这样你就可以自己验证代码。</p><p id="805c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的评论。</p></div></div>    
</body>
</html>