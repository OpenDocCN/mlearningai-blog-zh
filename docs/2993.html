<html>
<head>
<title>CLIP : Represent your images with text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">剪辑:用文本表现你的图像</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/clip-represent-your-images-with-text-e93dc106ac15?source=collection_archive---------2-----------------------#2022-07-06">https://medium.com/mlearning-ai/clip-represent-your-images-with-text-e93dc106ac15?source=collection_archive---------2-----------------------#2022-07-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/2cc0c6dfecd3a2b266230f837cb7df91.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*EDC69DgFEtlt-EtXTPG4tQ.png"/></div><figcaption class="il im et er es in io bd b be z dx">An image of daisy correctly labelled by CLIP</figcaption></figure><h1 id="b1f4" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="9407" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">CLIP(对比语言图像预训练)是由OpenAI制造的具有零触发能力的多模态神经网络。多模态神经网络能够解释模态，模态是指发生或经历的事情。</p><p id="5cf1" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">如今，典型的视觉模型是劳动密集型的，因为它们需要训练大量的参数。这里出现的另一个问题是，你训练的视觉模型是专门为特定数据集训练的。因此，它将在其他数据集上表现不佳。特定的数据集需要特定的模型。这种解决方案不太适合现实世界的问题，这就是为什么许多研究人员一直致力于零尝试学习。</p><p id="7ac7" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">零镜头学习意味着你要概括一个模型，使它能够在许多不同种类的标签上表现良好。这些标签可能是看不见的标签，因为我们的零射击模型没有在该标签的数据上训练。</p><h1 id="bcde" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">资料组</h1><p id="6bc9" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">为了使CLIP成为一个零镜头分类器，它已经在4亿张带有成对文本描述的图像上进行了训练(ImageNet只包含128万张图像！).剪辑的数据是网上搜集的。数据集中有32，678个随机采样的文本片段。这些片段基本上描述了图像，因此有了多模态神经网络这个词。如果数据集中的类不是标题格式，则它们会被转换为标题。例如:</p><ol class=""><li id="3f8b" class="kq kr hh jp b jq kl ju km jy ks kc kt kg ku kk kv kw kx ky bi translated">狗→一张狗的照片</li><li id="5845" class="kq kr hh jp b jq kz ju la jy lb kc lc kg ld kk kv kw kx ky bi translated">苹果→苹果的照片</li><li id="e619" class="kq kr hh jp b jq kz ju la jy lb kc lc kg ld kk kv kw kx ky bi translated">对象→一张{对象}的照片</li></ol><p id="1ca6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">CLIP执行的任务是用文本标记图像。当目标类别是短语而不是单个单词时，CLIP将给出更好的预测，因为数据是从互联网上丢弃的，并且丢弃的图像具有描述它的短语而不仅仅是单个单词。OpenAI的另一个著名模型，称为DALL-E，则相反。它根据文本描述生成图像。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es le"><img src="../Images/4ae48bf58ccc720f5c2cb4f719e9927b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pps0XHpF3DOxRd-36-ZiMQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">CLIP (courtesy: OpenAI)</figcaption></figure><h1 id="6f95" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">CLIP神经网络</h1><p id="f059" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">CLIP执行以下操作:</p><p id="4d9a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">图像→图像编码器(IE) →编码图像(N个图像的N个表示，其中每个表示是一个向量)</p><p id="114c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">文本→文本编码器(TE) →编码文本(M个文本片段的M个表示，其中每个表示是一个向量)</p><p id="01eb" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">然后，我们询问模型，对于图像“X ”, N个文本中哪个文本是合适的。这就是为什么它被称为对比目标。对比学习是基于你可以对比/区分相似和不相似事物的直觉。我们将以这样的方式训练它，即图像最接近与其对应的文本标签，而不是任何其他文本。</p><p id="76a6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">该模型通过最小化正确图像-文本对(N个真实对)之间的余弦距离，同时最大化不正确对(N -N)之间的余弦距离来训练。当两个向量之间的余弦距离减小时，两个向量之间的相似性增加。CLIP就是这样被训练出来的。</p><p id="78b6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">两个编码器都是变压器。图像编码器是ResNet50或视觉转换器。Vision Transformer的计算效率是标准ResNet的3倍。表现最好的剪辑模型在256GPUs上训练了2周。</p></div><div class="ab cl ln lo go lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ha hb hc hd he"><h1 id="ad38" class="ip iq hh bd ir is lu iu iv iw lv iy iz ja lw jc jd je lx jg jh ji ly jk jl jm bi translated">零射击分类</h1><p id="3d6c" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在包括ImageNet在内的16个数据集上，Zero-Shot CLIP在ResNet50上的性能优于线性探针。仔细想想，零拍剪辑是个很美的东西。它已经在如此大的数据(从互联网上收集的)上接受了训练，以至于它的性能超过了许多最先进的卷积神经网络，并且它记住，它可能从未见过它正在测试的数据。</p><p id="8532" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">当在不同的ImageNet数据集上使用零镜头剪辑时，性能差异非常小，但是当我们使用专门为ImageNet训练的分类器，但在ImageNet的其他版本上使用时，性能会下降很多。这向我们展示了CLIP的灵活性，因为它可以用作许多数据集的通用解决方案。</p><p id="fc40" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">要使用零镜头剪辑，您只需将目标标签作为标题传递到模型中，它将为您提供预测。它在大多数大型图像数据集上运行良好，但有时，它可能不会执行得那么好。对于某些数据集，您会更喜欢经典的卷积神经网络或转换器，而不是CLIP。</p><p id="137d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">但是你可以使用线性探头提高零杆剪辑的得分。在带有线性探测的剪辑中，您可以更改分类图层，以便它更好地适应您想要预测的数据。</p><h1 id="6e27" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">多模态神经元的存在性</h1><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/81384d71ff5a93c3d5b27ce6a1d62d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*RxjKom_VDcP6QtBbdmpg4g.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Halle Berry Neuron</figcaption></figure><p id="c329" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">CLIP最有趣的发现之一是“蜘蛛侠”神经元的存在。像生物学上的“哈莉·贝瑞”神经元一样，在clip中存在一个“蜘蛛侠”神经元，它对一个蜘蛛的图像、一个文本蜘蛛的图像和著名的超级英雄蜘蛛侠做出反应。</p><p id="3698" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">这一发现向我们展示了神经网络和我们的生物神经系统之间的重要联系。研究人员进一步发现，CLIP的最高层将图像组织为思想的松散语义集合，这为模型的多功能性提供了简单的解释。</p><h1 id="5eac" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">局限性和结论</h1><p id="00bc" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">就像我说的，CLIP是灵活和通用的，但它在某些数据集上表现不佳。这种任务的一些例子是计算图像中对象的数量、分类花、飞机的变体、汽车模型等。它在MNIST这个非常基本的深度学习任务上表现不佳。CLIP在不需要真实文本的情况下检测仇恨迷因方面也很有竞争力。</p><p id="187a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">CLIP是一个非常棒的神经网络，一定会让你开心不已。它也被用于DALL-E，这是另一个令人惊叹的网络，但是的，这需要进一步的研究，我相信有一天，通用人工智能将在更多数据和更多创新的帮助下存在。</p></div><div class="ab cl ln lo go lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ha hb hc hd he"><h1 id="f892" class="ip iq hh bd ir is lu iu iv iw lv iy iz ja lw jc jd je lx jg jh ji ly jk jl jm bi translated">参考</h1><p id="0c1b" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">[1]亚历克·拉德福德，琼·金旭，克里斯·哈拉西，阿迪蒂亚·拉梅什，加布里埃尔·高，桑迪尼·阿加瓦尔，吉里什·萨斯特里，阿曼达·阿斯克尔，帕梅拉·米什金，杰克·克拉克，格雷琴·克鲁格，伊利亚·苏茨基弗，<a class="ae ma" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型(2021) </a>，夹研究论文</p><p id="3a53" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">[2] Gabriel Goh，Chelsea Voss，Daniela Amodei，Shan Carter，Michael Petrov，Justin王杰，Nick Cammarata，Chris Olah，<a class="ae ma" href="https://openai.com/blog/multimodal-neurons/" rel="noopener ugc nofollow" target="_blank">人工神经网络中的多模态神经元(2021) </a>，CLIP Multi-Modal Neurons博客</p><p id="5be5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">如果你喜欢这篇文章，请鼓掌！</p><div class="mb mc ez fb md me"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hi fi z dy mj ea eb mk ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">medium.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ij me"/></div></div></a></div></div></div>    
</body>
</html>