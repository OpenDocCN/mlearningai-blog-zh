<html>
<head>
<title>Hierarchical Clustering and Density-Based Spatial Clustering of Applications with Noise (DBSCAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">含噪声应用的层次聚类和基于密度的空间聚类(DBSCAN)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/hierarchical-clustering-and-density-based-spatial-clustering-of-applications-with-noise-dbscan-b8d903095532?source=collection_archive---------1-----------------------#2021-02-04">https://medium.com/mlearning-ai/hierarchical-clustering-and-density-based-spatial-clustering-of-applications-with-noise-dbscan-b8d903095532?source=collection_archive---------1-----------------------#2021-02-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d1c9c3ad03cc32c249496aedbf8ed0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jeRF_omdmxXGQ74z.png"/></div></div></figure><p id="75be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">简介</strong></p><p id="3563" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">分层聚类是另一种无监督的机器学习算法，用于将未标记的数据集分组到一个聚类中，也称为分层聚类分析或HCA。</p><p id="d7f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在该算法中，我们以树的形式开发聚类的层次结构，并且这种树形结构被称为树状图。有时，K-means聚类和层次聚类的结果可能看起来很相似，但它们都有所不同，这取决于它们的工作方式。因为不需要像在K-Means算法中那样预先确定聚类的数量。</p><p id="3d43" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">分层聚类技术有两种方法:</p><ul class=""><li id="aa34" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">凝聚</strong>:凝聚是一种自下而上的方法，算法从将所有数据点作为单个聚类开始，并将其合并，直到剩下一个聚类。</li><li id="9677" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">除法</strong>:除法算法与凝聚算法相反，因为它是自顶向下的方法。</li></ul><p id="fe56" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">凝聚层次聚类</strong></p><p id="6835" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">凝聚层次聚类算法是HCA的一个流行的例子。为了将数据集分组到聚类中，它遵循自底向上的方法。这意味着，该算法在开始时将每个数据集视为单个聚类，然后开始将最近的一对聚类组合在一起。这样做，直到所有聚类合并成包含所有数据集的单个聚类。</p><p id="f549" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种聚类层次以树状图的形式表示。</p><p id="9d33" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">凝聚式层次聚类如何工作？</strong></p><p id="6a1c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">步骤1:将每个数据点创建为单个集群。假设有N个数据点，那么聚类数也将是N。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/0fb75b884d8a6483cb50e58268035da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*4G5_gnqvbQG-3uIs.png"/></div></figure><p id="b370" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">步骤2:取两个最接近的数据点或聚类，并将它们合并成一个聚类。因此，现在将有N-1个集群。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/87b84eb8849d94b2eef016122b943fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*FYqKWbcbJVrDK_ld.png"/></div></figure><p id="afb7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第三步:同样，取两个最接近的聚类，将它们合并在一起，形成一个聚类。将会有N-2个集群。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/90b07b13c82562ccd178e1c68db9ae78.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*u1ylh_-4LFgGkBae.png"/></div></figure><p id="1651" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">步骤4:重复步骤3，直到只剩下一个集群。因此，我们将得到以下集群。考虑下面的图片:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/784f06dfc3f91a42a067faebf4c10063.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*JYzjDJoTttnuRFej.png"/></div></figure><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/0e879de55880006e38610988824463dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*Aq1VbETuIQzzC4DV.png"/></div></figure><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/218ccfe7f5b848d5b8bc271b1a876fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*bvJl3ZUC7J4d1Pz0.png"/></div></figure><p id="12c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第5步:一旦所有的集群被组合成一个大的集群，开发树状图来根据问题划分集群。</p><p id="0c33" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">测量两个集群之间的距离</strong></p><p id="a59b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们已经看到的，两个聚类之间的最近距离对于分层聚类是至关重要的。计算两个聚类之间的距离有多种方法，这些方法决定了聚类的规则。这些措施被称为联系方法。下面给出了一些流行的连接方法:</p><p id="4cfc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">单连锁:</strong>是聚类最近点之间的最短距离。考虑下图:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/9b98a01000b16730083be748f7cd2b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*sDk_-DuphXHjtu6_.png"/></div></figure><p id="ae86" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">完全连锁:</strong>是两个不同集群的两点之间的最远距离。这是一种流行的连锁方法，因为它比单连锁形成更紧密的簇。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/4ab8d82bcdf3d1784f82c88c9a9e4d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*sfCVYYDPMU8j-D2E.png"/></div></figure><p id="253a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">平均连锁:</strong>是将每对数据集之间的距离相加，然后除以数据集总数，计算两个聚类之间的平均距离的连锁方法。也是最受欢迎的联动方式之一。</p><p id="b5dc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">质心连接:</strong>这是一种连接方法，在该方法中，计算簇的质心之间的距离。考虑下图:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kb"><img src="../Images/1079e8cbb714f9fed8f11c694f55e199.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*FQYqSQMawdkpxPA5.png"/></div></figure><p id="49a8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">层次聚类中树状图的绘制</strong></p><p id="2fad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">树状图是一种树状结构，主要用于将HC算法执行的每个步骤存储为内存。在树状图中，Y轴显示数据点之间的欧几里得距离，x轴显示给定数据集的所有数据点。</p><p id="803d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">可以使用下图解释树状图的工作原理:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kg"><img src="../Images/acb784f142e5a8dd3f971c54470621ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*endFhY1ifX0zIs9X.png"/></div></div></figure><p id="3df3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，左边部分显示了如何在聚集聚类中创建聚类，右边部分显示了相应的树状图。</p><ul class=""><li id="b2ca" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">正如我们上面所讨论的，首先，数据点P2和P3组合在一起并形成一个集群，相应地创建了一个树形图，它用一个矩形连接P2和P3。高度是根据数据点之间的欧几里德距离决定的。</li><li id="1e81" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">在下一步中，P5和P6形成一个聚类，并创建相应的树状图。这比以前的更高，因为五常和P6之间的欧几里得距离比P2和P3之间的距离稍大一些。</li><li id="be89" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">同样，创建了两个新的系统树图，将P1、P2和P3组合在一个系统树图中，将P4、P5和P6组合在另一个系统树图中。</li><li id="0f01" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">最后，创建最终的树状图，将所有的数据点组合在一起。</li></ul><p id="984e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们开始在Google collab中使用python实现。</p><p id="f4bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">导入库</strong></p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="67b5" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">import</strong> pandas <strong class="ki hi">as</strong> pd<br/><strong class="ki hi">import</strong> numpy <strong class="ki hi">as</strong> np<br/><strong class="ki hi">from</strong> google.colab <strong class="ki hi">import</strong> files<br/>uploaded = files.upload()</span></pre><p id="38b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">加载数据集</strong></p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="636a" class="km kn hh ki b fi ko kp l kq kr">import io<br/>train_data = pd.read_csv(io.StringIO(uploaded['Mall_Customers.csv'].decode('utf-8')))</span></pre><p id="2c85" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">显示数据集中的前5条记录</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ks"><img src="../Images/8736f35d72501f56cca3459cb2619a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*khd0XueOjWt26y8E.png"/></div></figure><p id="9153" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">确定X值</strong></p><p id="ea0e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算年收入和支出分数。</p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="68b7" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">X</strong>= train_data.iloc[:,3:].values<br/><strong class="ki hi">X</strong></span></pre><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/4569acc635b52c69554fd2e81d0800f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/0*okju2Xq1CAj_cTwS.png"/></div></figure><p id="eb38" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">使用树状图寻找最佳聚类数</strong></p><p id="df7e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们将使用我们的模型的树状图来找到最佳的聚类数。为此，我们将使用scipy库，因为它提供了一个函数，可以直接返回我们代码的树状图。</p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="6d7b" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">from</strong> matplotlib <strong class="ki hi">import</strong> pyplot <strong class="ki hi">as</strong> mtp<br/><strong class="ki hi">import</strong> scipy.cluster.hierarchy <strong class="ki hi">as</strong> shc  <br/>dendro = shc.dendrogram(shc.linkage(X, method="ward"))  <br/>mtp.title("Dendrogram Plot")  <br/>mtp.ylabel("Euclidean Distances")  <br/>mtp.xlabel("Customers")  <br/>mtp.show()</span></pre><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ku"><img src="../Images/41cd63ac3ac5c9c9fa69c0ff98206f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*otryilBR8l8mMOZj.png"/></div></figure><p id="22de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用这个树状图，我们现在将为我们的模型确定最佳的集群数量。为此，我们将找到不切断任何水平杆的最大垂直距离。因此，分类的最佳数量将是5，我们将在下一步中使用相同的方法来训练模型。</p><p id="e8bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">应用层次聚类模型</strong></p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="bb57" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">from</strong> sklearn.cluster <strong class="ki hi">import</strong> AgglomerativeClustering  <br/>hc= AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  <br/>y_pred= hc.fit_predict(X)  <br/>y_pred</span></pre><p id="ec5c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">聚类采用以下参数，</p><ul class=""><li id="e825" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">n_clusters=5:它定义了集群的数量，我们在这里取5是因为它是最优的集群数量。</li><li id="6a6b" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">affinity='euclidean ':这是一个用于计算关联的度量。</li><li id="c093" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">linkage='ward ':它定义了链接标准，这里我们使用了“ward”链接。这种方法是我们已经用来创建树状图的流行的链接方法。它减少了每个集群中的差异。</li></ul><p id="2482" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">可视化</strong></p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="38c0" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">mtp</strong>.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], s = 100, c = 'blue', label = 'Cluster 1')  <br/><strong class="ki hi">mtp</strong>.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], s = 100, c = 'green', label = 'Cluster 2')  <br/><strong class="ki hi">mtp</strong>.scatter(X[y_pred == 2, 0], X[y_pred == 2, 1], s = 100, c = 'red', label = 'Cluster 3')  <br/><strong class="ki hi">mtp</strong>.scatter(X[y_pred == 3, 0], X[y_pred == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')  <br/><strong class="ki hi">mtp</strong>.scatter(X[y_pred == 4, 0], X[y_pred == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')  <br/><strong class="ki hi">mtp</strong>.title('Clusters of customers')  <br/><strong class="ki hi">mtp</strong>.xlabel('Annual Income (k$)')  <br/><strong class="ki hi">mtp</strong>.ylabel('Spending Score (1-100)')  <br/><strong class="ki hi">mtp</strong>.legend()  <br/><strong class="ki hi">mtp</strong>.show()</span></pre><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/911c003cc44f9fc80a46aa8b965ca2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*D6rKYkLjk7obRVuL.png"/></div></figure><p id="c15d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> DBSCAN集群</strong></p><p id="5951" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基本上，所有聚类方法都使用相同的方法，即首先我们计算相似性，然后我们使用它将数据点聚类成组或批次。在这里，我们将重点介绍基于密度的应用空间聚类与噪声(DBSCAN)聚类方法。</p><p id="2924" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基于密度的聚类指的是无监督学习方法，其基于数据空间中的聚类是具有高点密度的连续区域，通过具有低点密度的连续区域与其他此类聚类分离的思想，来识别数据中的独特组/聚类。</p><p id="b626" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">含噪声应用的基于密度的空间聚类(DBSCAN)是基于密度的聚类的基本算法。它可以从包含噪声和离群点的大量数据中发现不同形状和大小的簇。</p><p id="afd0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">算法参数</strong></p><p id="25f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">DBSCAN算法使用两个参数:</p><ul class=""><li id="0dea" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">minPts:一个区域被认为密集时聚集在一起的最小点数(阈值)。</li><li id="84d5" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">eps (ε):一种距离度量，用于定位任意点邻域中的点。</li></ul><p id="dd50" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们探索称为密度可达性和密度连通性的两个概念，就可以理解这些参数。</p><p id="e99d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">可达性</strong>根据密度建立了一个点，如果该点位于另一个点的特定距离(eps)内，则该点可以从另一个点到达。</p><p id="a67b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">另一方面，连通性</strong>涉及基于传递性的链接方法，以确定点是否位于特定的簇中。例如，如果p- &gt; r- &gt; s- &gt; t- &gt; q，则p点和q点可以连接，其中a- &gt; b表示b在a的邻域中。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/03c85e1c68adeb39d90c18e44b84b144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/0*zXh67LJ7V-it3o9H.png"/></div></figure><p id="14aa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">DBSCAN聚类完成后有三种类型的点:</p><ul class=""><li id="f0af" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">核心——这是一个距离自身n以内至少有m个点的点。</li><li id="9771" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">边界-这是一个在距离n处至少有一个核心点的点。</li><li id="ebae" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">噪音——这是一个既不是核心也不是边界的点。并且它在距离自身n的范围内具有少于m个点。</li></ul><p id="785c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">它是如何工作的？</strong></p><ul class=""><li id="be82" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">选择任意一个数据点p作为你的第一个点。</li><li id="ee08" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">将p标记为已访问。</li><li id="b991" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">提取其邻域中存在的所有点(从该点到eps的距离)，并将其称为集合nb</li><li id="194f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">如果nb &gt;= minPts，则<br/> a .将p视为一个新群的第一个点<br/> b .将eps距离内的所有点(nb的成员)视为该群中的其他点。对nb中的所有点重复步骤b</li><li id="9c26" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">否则将p标记为噪声</li><li id="2309" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">重复步骤1-5，直到整个数据集被标记，即聚类完成。</li></ul><p id="4d1b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在执行算法之后，我们应该理想地将数据集分成多个聚类，并且将一些点标记为不属于任何聚类的噪声。</p><p id="afa8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">例子</strong></p><p id="cdf4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些观点可以用形象化来更好地解释。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/2886ddf0806acbbca78d6c1bdf6d1a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*8wNb7SYYdRe5Zu56.png"/></div></figure><p id="9014" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，minPts是4。红色点是核心点，因为在其周围区域内至少有4个eps半径的点。该区域在图中用圆圈表示。黄色点是边界点，因为它们可以从核心点到达，并且在其邻域内的点少于4个。可到达意味着在核心点的周围区域。点B和C在其邻域(即半径为eps的周围区域)内有两个点(包括点本身)。最后，N是一个离群值，因为它不是一个核心点，不能从核心点到达。</p><p id="31c5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">用Python实现</strong></p><p id="3712" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导入库</p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="6efd" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">Copy</strong><strong class="ki hi">import</strong> numpy <strong class="ki hi">as</strong> np<br/><strong class="ki hi">import</strong> matplotlib.pyplot <strong class="ki hi">as</strong> plt<br/><strong class="ki hi">from</strong> sklearn <strong class="ki hi">import</strong> metrics<br/><strong class="ki hi">from</strong> sklearn.datasets <strong class="ki hi">import</strong> make_blobs<br/><strong class="ki hi">from</strong> sklearn.preprocessing <strong class="ki hi">import</strong> StandardScaler<br/><strong class="ki hi">from</strong> sklearn.cluster <strong class="ki hi">import</strong> DBSCAN</span></pre><p id="43f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">应用DBSCAN算法</strong></p><pre class="kc kd ke kf fd kh ki kj kk aw kl bi"><span id="0f32" class="km kn hh ki b fi ko kp l kq kr"><strong class="ki hi">Copy</strong><strong class="ki hi">X</strong>, y = make_blobs(n_samples=500,n_features=2,<br/><strong class="ki hi">centers</strong>=4, cluster_std=1,<br/><strong class="ki hi">center_box</strong>=(-10.0, 10.0),<br/><strong class="ki hi">shuffle</strong>=True, random_state=1)<br/><strong class="ki hi">X</strong> = StandardScaler().fit_transform(X)<br/><strong class="ki hi">y_pred</strong> = DBSCAN(eps=0.3, min_samples=30).fit_predict(X)<br/><strong class="ki hi">plt</strong>.scatter(X[:,0], X[:,1], c=y_pred)<br/><strong class="ki hi">print</strong>('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))<br/><strong class="ki hi">print</strong>('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))</span></pre><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/31f441def2d3682ece3da60226eaff8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*IMyI1FqTZc7fnx6a.png"/></div></figure><p id="4a01" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">显然，如预期的那样检测到4个簇。根据DBSCAN模型，黑点是异常值或噪声。由于被分析的数据集是由我们生成的，并且我们知道关于它的基本事实，我们可以使用类似于<strong class="ir hi">同质性</strong>分数(检查每个聚类是否只有一个类的成员)和<strong class="ir hi">完整性</strong>分数(检查一个类的所有成员是否都被分配到同一个聚类)的度量。</p><p id="1b51" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">希望您对层次聚类和DBSCAN有更好的了解。</p><p id="b3aa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">感谢您的阅读，希望您喜欢！</p><p id="dbe7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">参考</p><p id="6aec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kz" href="https://www.youtube.com/watch?v=C3r7tGRe2eI" rel="noopener ugc nofollow" target="_blank">数据库扫描</a></p></div></div>    
</body>
</html>