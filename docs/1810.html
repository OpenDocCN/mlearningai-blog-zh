<html>
<head>
<title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">辍学:防止神经网络过度拟合的简单方法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-e95572aad1df?source=collection_archive---------10-----------------------#2022-01-31">https://medium.com/mlearning-ai/dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-e95572aad1df?source=collection_archive---------10-----------------------#2022-01-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="c134" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">辍学，过度适应，如何解决</h2></div><p id="199a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">深度学习的世界不仅仅是密集的层。有几十种层可以添加到模型中。(尝试浏览<a class="ae js" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/" rel="noopener ugc nofollow" target="_blank"> Keras文档</a>获取样本！)有些就像致密层，定义神经元之间的连接，而有些则可以进行预处理或其他类型的转换。</p><p id="4134" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将了解一种特殊的层，不包含任何神经元本身，但它添加了一些功能，有时可以以各种方式使模型受益。现代建筑中常用。</p><h1 id="b042" class="jt ju hh bd jv jw jx jy jz ka kb kc kd in ke io kf iq kg ir kh it ki iu kj kk bi translated">拒绝传统社会的人</h1><p id="a073" class="pw-post-body-paragraph iw ix hh iy b iz kl ii jb jc km il je jf kn jh ji jj ko jl jm jn kp jp jq jr ha bi translated">第一个是“脱层”，它可以帮助纠正过度拟合。</p><p id="2d48" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在上一课中，我们讨论了网络学习训练数据中的虚假模式是如何导致过度拟合的。为了识别这些虚假模式，网络通常依赖于非常特定的权重组合，这是一种权重“共谋”。如此具体，他们往往是脆弱的:删除一个和阴谋土崩瓦解。</p><p id="26e8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这就是<strong class="iy hi">辍学</strong>背后的想法。为了打破这些阴谋，我们在训练的每一步随机地<em class="kq">丢弃</em>一层输入单元的一部分，使得网络更难学习训练数据中的那些虚假模式。相反，它必须寻找广泛的、一般的模式，其权重模式往往更稳健。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/13c07a02e56f067eda8e1919e0043d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/0*sTulvHwJk7XzmVuW.gif"/></div></figure><p id="ac40" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你也可以把辍学看作是创建一种网络组合。预测将不再由一个大网络做出，而是由一个由较小网络组成的委员会做出。委员会中的个人往往会犯不同类型的错误，但同时又是正确的，这使得委员会作为一个整体比任何个人都要好。(如果你熟悉作为决策树集合的随机森林，这是相同的想法。)</p><h1 id="5561" class="jt ju hh bd jv jw jx jy jz ka kb kc kd in ke io kf iq kg ir kh it ki iu kj kk bi translated">添加辍学</h1><p id="9302" class="pw-post-body-paragraph iw ix hh iy b iz kl ii jb jc km il je jf kn jh ji jj ko jl jm jn kp jp jq jr ha bi translated">在Keras中，退出率参数<code class="du kz la lb lc b">rate</code>定义了关闭输入单元的百分比。将<code class="du kz la lb lc b">Dropout</code>图层放在要应用漏白的图层之前:</p><pre class="ks kt ku kv fd ld lc le lf aw lg bi"><span id="4bc9" class="lh ju hh lc b fi li lj l lk ll">keras.Sequential([<br/>    # ...<br/>    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer<br/>    layers.Dense(16),<br/>    # ...<br/>])</span></pre><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md kx lp"/></div></div></a></div></div></div>    
</body>
</html>