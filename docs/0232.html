<html>
<head>
<title>Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/regularization-e7b7d5104eb1?source=collection_archive---------5-----------------------#2021-03-07">https://medium.com/mlearning-ai/regularization-e7b7d5104eb1?source=collection_archive---------5-----------------------#2021-03-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="b680" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">“机器学习中使用的许多策略都是为了减少测试错误而明确设计的，可能是以增加训练错误为代价。这些策略统称为正规化。”— Goodfellow等人。</p></blockquote><p id="4464" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">在神经网络中，通过选择正确的参数集(权重和偏差)来获得通用模型是减少过拟合影响的关键因素。我们如何获得这样的参数？。答案是<strong class="ik hi">正规化</strong>。正规化可以分为两个阶段。</p><ul class=""><li id="cb61" class="jj jk hh ik b il im ip iq jg jl jh jm ji jn jf jo jp jq jr bi translated"><em class="ij">隐式正则化方法</em>:数据扩充和提前停止</li><li id="9c08" class="jj jk hh ik b il js ip jt jg ju jh jv ji jw jf jo jp jq jr bi translated"><em class="ij">显式正规化方法</em>:辍学</li></ul><p id="e17b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">有各种类型的正则化技术，如L1正则化、L2正则化(通常称为“权重衰减”)和弹性网，它们通过更新损失函数本身、添加附加参数来约束模型的容量来使用。</p><h1 id="5920" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">为什么我们需要正规化？</h1><p id="ce52" class="pw-post-body-paragraph ih ii hh ik b il kv in io ip kw ir is jg kx iv iw jh ky iz ja ji kz jd je jf ha bi translated">正则化有助于我们控制我们的模型容量，确保我们的模型能够更好地对未经训练的数据点进行(正确的)分类，我们称之为泛化能力。如果我们不应用正则化，我们的分类器很容易变得过于复杂并过度适应我们的训练数据，在这种情况下，我们将失去对我们的测试数据(以及测试集以外的数据点，如野外的新图像)进行归纳的能力。</p><p id="fdf1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">然而，过多的正规化可能是一件坏事。我们可能会面临拟合不足的风险，在这种情况下，我们的模型在训练数据上表现不佳，并且无法对输入数据和输出类标签之间的关系进行建模。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es la"><img src="../Images/45c29deda306aa4de68115b10bd091cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*8zQWv2mPFkS0eN40pJH4wQ.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jz">blue line:</strong> overfit, <strong class="bd jz">orange line: </strong>Underfit</figcaption></figure><p id="b58b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">正则化的目标是获得这些类型的“格林函数”,它们很好地适应我们的训练数据，但避免过度适应我们的训练数据(蓝色)或未能对底层关系建模(橙色)。</p><h1 id="200c" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">损失和重量更新，包括正规化</h1><p id="6b34" class="pw-post-body-paragraph ih ii hh ik b il kv in io ip kw ir is jg kx iv iw jh ky iz ja ji kz jd je jf ha bi translated">整个训练集的损失可以写成:</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lm"><img src="../Images/4261e8691b7fdfca0049dba0f693f691.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*oxbbX-fxVpSNQd9bZI9U7g.png"/></div></figure><p id="cc12" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">现在，假设我们已经获得了一个权重矩阵W，使得我们训练集中的每个数据点都被正确分类，这意味着我们的损失L = 0。太棒了，我们获得了100%的准确性，但让我问你一个关于这个权重矩阵的问题，它是唯一的吗？或者，换句话说，是否有更好的W选择来提高我们模型的泛化能力并减少过度拟合？</p><p id="1812" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">如果有这样的W，我们怎么知道？我们怎样才能把这种惩罚纳入我们的损失函数呢？答案是定义一个正则化惩罚，一个作用于权重矩阵的函数。正则化罚函数通常写成函数R(W)。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es ln"><img src="../Images/3990bea99d5ff1a56f0206a55bbeb468.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*RdruCBbsoW2E4oA5K6bGZA.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx">L2 Regularization</figcaption></figure><p id="f609" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">为了减轻各种维度对输出分类的影响，我们应用正则化，从而寻求考虑所有维度而不是少数具有大值的维度的W值。在实践中，你可能会发现正则化会稍微损害你的训练精度，但实际上增加了你的测试精度。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lo"><img src="../Images/a298e15a4e5886ed9a9776f30b8715ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*uJptb0IY3mbWOnsYLLm_0A.png"/></div></figure><p id="68a3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">第二项是新的，这是我们的正则化惩罚。\lambda变量是一个超参数，它控制我们正在应用的正则化的数量或强度。在实践中，学习率\alpha和正则项\lambda都是您将花费最多时间调整的超参数。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es lp"><img src="../Images/6aee8d99ef334a13c245780a863766e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*gFRSp_z-rE0saFt1fvOesA.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx">weight updating during backpropagation</figcaption></figure><p id="2105" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里，我们将负线性项添加到我们的梯度(即梯度下降)，惩罚大权重，最终目标是使我们的模型更容易推广。</p><p id="b4c6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">参考:</strong></p><p id="7839" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">Adrian Rosebrock用Python实现计算机视觉的深度学习(入门包)</p></div></div>    
</body>
</html>