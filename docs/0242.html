<html>
<head>
<title>Activation functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/activation-functions-17f8e7d5fcf8?source=collection_archive---------7-----------------------#2021-03-08">https://medium.com/mlearning-ai/activation-functions-17f8e7d5fcf8?source=collection_archive---------7-----------------------#2021-03-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d839" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活函数是附属于网络中每个神经元的数学函数，并确定它是否应该被激活。通常，在每一层中，神经元使用权重和偏差对输入执行线性变换:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/ac12ae7a52eb9b3846bf0855c40b27ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*dFW-2DdJluSl4yT8"/></div></figure><p id="b5a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，将激活函数应用于上述结果:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jk"><img src="../Images/ce7955486865062cfa1c851d03fa2f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/0*Gu_mlAq3xbY-SG7j"/></div></figure><p id="e1c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当前层的输出成为下一层的输入。这个过程与网络中的所有隐藏层一起重复。这种信息的前向运动称为<strong class="ig hi"> <em class="jl">前向传播</em> </strong>。</p><p id="c796" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">激活函数有一些重要的意义:</p><ul class=""><li id="cd25" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">它在神经网络中引入非线性变换，使网络具有学习和执行更复杂任务的能力。想象没有激活函数的神经网络，因此使用权重作为偏差对输入只有线性变换。在这种情况下，神经网络作为线性回归模型工作，那么它的功能将会减弱，并且不能执行复杂的任务。</li><li id="ae30" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">此外，一些激活函数也有助于将每个神经元的输出标准化到一个新的范围，在0和1之间，或在-1和1之间。</li></ul><p id="96ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将发现一些流行的激活功能:</p><h1 id="9942" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> 1。乙状结肠功能(逻辑激活功能)</strong></h1><p id="8473" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">该功能定义如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/7ced7725fe2e01d7bd498e01746ac372.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/0*AoVzwcWa5BO0mfNR"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="0f62" class="lj kb hh lf b fi lk ll l lm ln">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="d95f" class="lj kb hh lf b fi lo ll l lm ln">x = np.arange(-6,6, 0.01)</span><span id="a0e8" class="lj kb hh lf b fi lo ll l lm ln">def plot(func, ylim):<br/>  plt.plot(x, func(x), c = 'r', lw = 3)<br/>  plt.xticks(fontsize = 14)<br/>  plt.yticks(fontsize = 14)<br/>  plt.axhline(c = 'k', lw = 1)<br/>  plt.axvline(c = 'k', lw = 1)<br/>  plt.ylim(ylim)<br/>  plt.box(on = None)<br/>  plt.grid(alpha = 0.4, ls = '-.')</span></pre><p id="3f6e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">乙状结肠功能的可视化:</p><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="9a37" class="lj kb hh lf b fi lk ll l lm ln">def sigmoid(x):<br/>  return 1/ (1 + np.exp(-x))</span><span id="f78d" class="lj kb hh lf b fi lo ll l lm ln">plot(sigmoid, ylim = (-0.25, 1.25))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/aa1ef18ceebe8014228718c187eed84a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Vje_kBln7XS7XVDq09KPFQ.png"/></div></figure><ul class=""><li id="2420" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">由于输出值介于0和1之间，因此该函数尤其适用于需要预测输出概率的模型。</li><li id="0d2a" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">该函数可微分，其导数由下式给出:</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/3cf70b204795305f57aa8c2e7e2c30f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/1*AqzGpTco2NiuNy8fIWVqRw.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="ea58" class="lj kb hh lf b fi lk ll l lm ln">def derivative_sigmoid(x):<br/>  return sigmoid(x)*(1-sigmoid(x))</span><span id="2def" class="lj kb hh lf b fi lo ll l lm ln">plot(derivative_sigmoid, ylim=(-0.02, 0.35))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/f11129831fed8820a5b8c4296715593b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*CuKJOdYwWvaR8dJX2z_j0Q.png"/></div></figure><p id="6fb1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="2c88" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">输出在范围0和1内标准化</li><li id="4680" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">平滑渐变</li></ul><p id="902e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="2699" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">消失渐变:根据上图，当x &gt; 6或x &lt; -6. The vanishing gradient may stop the neural network from further training.</li><li id="4e96" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">The outputs are not centered.</li><li id="9b8a" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">The computational complexity is large.</li></ul><h1 id="c588" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">2. Tanh function</h1><p id="b7a8" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">Tanh function is defined as:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lr"><img src="../Images/301dc9c404b3f3eada50e6fd74c77ed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/1*97bIi74_vAcV70SEEQIlvA.gif"/></div></figure><p id="b1b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">This function is very similar to the sigmoid function. However, it is symmetric around the origin. The outputs of this function are centered and range between -1 and 1.</p><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="66b0" class="lj kb hh lf b fi lk ll l lm ln">def tanh(x):<br/>  return 2*sigmoid(2*x) - 1</span><span id="3b3d" class="lj kb hh lf b fi lo ll l lm ln">plot(tanh, ylim = (-1.4, 1.4))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/055c1d70751fb1928d391ae0318f0aac.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ueGYryCx4t7xWbJeUEMcuA.png"/></div></figure><p id="0e53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">tanh(x) is a monotone, continuous and differentiable function. Its derivation is determined by:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ls"><img src="../Images/dc17534cfe1fbccced414d5a4f0b66b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/1*62-t0mif__jzPIKaetarkw.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="2db9" class="lj kb hh lf b fi lk ll l lm ln">def derivative_tanh(x):<br/>  return 1 - (tanh(x))**2</span><span id="bda8" class="lj kb hh lf b fi lo ll l lm ln">plot(derivative_tanh, ylim = (-0.2, 1.2))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/62569a338d06a0e0954fab670c566e6e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*P7nyREj8hFyzmBqL2PTt6A.png"/></div></figure><p id="8fd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Similar to the sigmoid function, the derivation of this function is closed to zero when x &gt; 3或x &lt; -3. Hence, that also leads to vanishing gradient phenomena during training neural networks.</p><h1 id="ca96" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">3. Rectified Linear Unit (ReLU) function</h1><p id="6bf8" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">This function has been used widely in the deep learning domain. Its formula is defined by:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lt"><img src="../Images/d886659529292eaa672f58e94fff43c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/1*b2jPSGUNeSN9aSiahF6NZw.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="0a9d" class="lj kb hh lf b fi lk ll l lm ln">relu = np.vectorize(lambda x: x if x &gt; 0 else 0, otypes=[np.float])</span><span id="2cf4" class="lj kb hh lf b fi lo ll l lm ln">plot(relu, ylim=(-0.3, 1.5))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/751790b6917ca81ed600b56ef278ebfa.png" data-original-src="https://miro.medium.com/v2/format:webp/1*u7Ddf_79AMaNoty_SDqeFg.png"/></div></figure><p id="f703" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">This function is continuous, monotone in ℝ, and differentiable for all x ≠ 0.</p><p id="efd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">The derivation of the ReLU function is determined by:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/9732122087508753b6e85fa91cdfd0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/1*VdxwdbERVzwzIcCWxs8MKA.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="0eff" class="lj kb hh lf b fi lk ll l lm ln">derivative_relu = np.vectorize(lambda x:1 if x &gt; 0 else 0, otypes=[np.float])</span><span id="ad3d" class="lj kb hh lf b fi lo ll l lm ln">plot(derivative_relu, ylim = (-0.5, 1.5))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/e6981c0960bfd6702fb2a9f63a5a3661.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DK1Qu-nuvO-cjUHNRI57xA.png"/></div></figure><p id="25d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">时，渐变值接近0优点:</strong></p><ul class=""><li id="8215" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">与sigmoid和tanh函数相比，此函数的计算效率更高。因此，使用该功能可以加快训练时间。</li></ul><p id="0b4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="78b6" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">输出既不是标准化的，也不是居中的。</li><li id="9f93" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">这个函数在原点不可微</li><li id="8de9" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">由于ReLU只激活正信号，而去激活所有负信号，因此它不能为负输入值提供一致的预测。</li><li id="c637" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">当输入接近零或负值时，导数值为零。因此，在反向传播期间，一些神经元的权重和偏差没有被更新。</li></ul><h1 id="3ae3" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">4.泄漏ReLU函数</h1><p id="b274" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">这个函数是ReLU函数的改进版本，克服了ReLU的一些缺点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/10ac69f632d8ad4f7f9abbc4c91c01b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/1*GWyQGHJfwkiZ9EP0YjOBSw.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="21f1" class="lj kb hh lf b fi lk ll l lm ln">a = 0.01<br/>leaky_relu = np.vectorize(lambda x: x if x &gt; 0 else a*x, otypes= [np.float])</span><span id="1617" class="lj kb hh lf b fi lo ll l lm ln">plot(leaky_relu, ylim = (-0.5, 1.5))</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/30dfd30b8dbf6faf202579c38f9b769c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*wXnefGlU--8iFvXuraxOxg.png"/></div></figure><p id="7c22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这种情况下，所有负值x都被ax代替，其中a ∈ (0，1)。这允许我们不断更新所有神经元的权重和偏差。</p><p id="064c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优势:</strong></p><ul class=""><li id="2016" class="jm jn hh ig b ih ii il im ip jo it jp ix jq jb jr js jt ju bi translated">这个函数很简单，其计算复杂度仍然小于sigmoid和tanh函数。</li><li id="a607" class="jm jn hh ig b ih jv il jw ip jx it jy ix jz jb jr js jt ju bi translated">不断更新所有神经元的权重和偏差，因此它确实支持反向传播。</li></ul><p id="0857" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong>虽然它保持了负向神经元的活跃，但是这些神经元被重新调节成了一个很小的值。换句话说，对应于这些负向神经元的信号显著减少。因此，漏ReLU不能为负输入值提供一致的预测。</p><h1 id="adb4" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">5.Softmax函数</h1><p id="cfac" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">softmax函数由下式定义:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/9ab8415ac1f4e99c5ca878b583fa13bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/1*WF38DfVJCZe6xwYj7lgImg.gif"/></div></figure><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="9c0b" class="lj kb hh lf b fi lk ll l lm ln">def softmax(x):<br/>  z = np.exp(x)<br/>  return z/z.sum()</span><span id="3191" class="lj kb hh lf b fi lo ll l lm ln">softmax([0.4, 2, 5])</span><span id="933e" class="lj kb hh lf b fi lo ll l lm ln">&gt;&gt;&gt; array([0.00948431, 0.04697607, 0.94353962])</span></pre><p id="7251" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">softmax函数的导数:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/769989a00fb3c6adda496f533e680c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*o9qXNIhKAGPqlFKN-Ar7jg.png"/></div></figure><p id="102f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当j ≠ i时:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/100dce3e586f9f919ee21a48316247e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*G5h9UI5ZRQ_00Kk6lFtxpQ.png"/></div></figure><p id="82c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来表示:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/38a03e3b61a7d5c6abc1c5555a341475.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*mBxT0_kJcw9KqK5UclrE0w.png"/></div></figure><p id="0428" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，softmax函数的导数可以重写为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/a67674a07c818d62e3b766b518e5e682.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*cGHeD0tHqH8eUsewDWp4gw.png"/></div></figure><p id="e413" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">softmax函数返回介于0和1之间的输出值，这些输出的总和等于1。因此，此函数对于多类分类非常有用，我们的目标是预测数据点属于特定类的概率。</p></div><div class="ab cl mb mc go md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ha hb hc hd he"><p id="7ca2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong>我们发现了一些在构建卷积神经网络时经常出现的激活函数。希望这篇帖子对你有帮助。</p><p id="bb40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你有任何问题，请随意写在评论区。</p><p id="7f0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢谢大家！</p><p id="ef96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jl">我的博客页面:</em></strong><a class="ae mi" href="https://lekhuyen.medium.com/" rel="noopener">https://lekhuyen.medium.com/</a></p></div></div>    
</body>
</html>