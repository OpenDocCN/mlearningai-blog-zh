<html>
<head>
<title>Understanding TF-IDF in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解NLP中的TF-IDF</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/understanding-tf-idf-in-nlp-8a2931c017e3?source=collection_archive---------0-----------------------#2022-01-21">https://medium.com/mlearning-ai/understanding-tf-idf-in-nlp-8a2931c017e3?source=collection_archive---------0-----------------------#2022-01-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="adfe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章收集了一些关于为什么TF-IDF作为一种文本矢量化方法有效的结构化问题，首先是为什么首先需要它的原始问题。</p><h2 id="e2d6" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么我们需要文本矢量化？</h2><ul class=""><li id="702a" class="jx jy hh ig b ih jz il ka ip kb it kc ix kd jb ke kf kg kh bi translated">第一，计算机只理解数字。在最低层(字节码)输入到计算机的任何输入都是一系列二进制数，这意味着为了能够解释有声语言的文本，我们需要用数字来表示它们。</li><li id="df55" class="jx jy hh ig b ih ki il kj ip kk it kl ix km jb ke kf kg kh bi translated">第二，文本的向量表示允许我们利用线性代数的能力进行不同的运算(<em class="kn">点/叉积、平面、垂线、法线等</em>)。这有助于探索性数据分析(EDA)、几何直觉、问题的可视化(1维、2维和3维)以及将线性代数方法外推至更高维度(&gt; 3维)。</li></ul><h2 id="49d9" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">将文本转换成矢量的规则是什么？</h2><p id="8526" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">如英语中的三个单词<em class="kn"> (w1，w2，w3) </em>，分别有了矢量表示法<em class="kn"> (v1，v2，v3) </em>。那么，简单地说，这个规则就是:</p><p id="a37a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kn">“如果(w1，w2)在语义上比(w1，w3)更多的</em><a class="ae kr" href="https://dictionary.cambridge.org/us/dictionary/english/semantically" rel="noopener ugc nofollow" target="_blank"><em class="kn"/></a><em class="kn">，那么(v1，v2)向量之间的距离应该小于(v1，v3)向量之间的距离”</em></p><p id="6c64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，词语(<em class="kn">好的</em>、<em class="kn">伟大的</em>)在意义上比(<em class="kn">好的</em>、<em class="kn">坏的</em>)关系密切，所以<a class="ae kr" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"> <em class="kn">欧几里得</em> </a> <em class="kn">距离</em><em class="kn">(</em><strong class="ig hi"><em class="kn">V</em></strong><em class="kn">(好的)</em><strong class="ig hi"><em class="kn">V</em></strong>【T44)</p><h2 id="979a" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么相似的词一定要几何接近？</h2><p id="38ae" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">在训练机器学习模型时，该模型试图学习和定义等式(或条件)来对不同的组进行分类。如果相似的点在几何上接近，那么在不同的组之间分类就更容易。下图通过正面、负面和基于性别/性别中性词的示例，帮助您可视化不同数据标签之间的决策界限。</p><div class="ks kt ku kv fd ab cb"><figure class="kw kx ky kz la lb lc paragraph-image"><img src="../Images/f32e9fa0f50d01de9230d3791a4c6922.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*x6V6ukmywSh8Mtdzn1-jkg.png"/></figure><figure class="kw kx lf kz la lb lc paragraph-image"><img src="../Images/c28b500d1856e3edbf35d7d312b333b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*15Ntn4ooiXPJ6dOVQ2xybg.png"/><figcaption class="lg lh et er es li lj bd b be z dx lk di ll lm">Grouping of similar words leads to easier decision boundaries</figcaption></figure></div><h1 id="deb8" class="ln jd hh bd je lo lp lq ji lr ls lt jm lu lv lw jp lx ly lz js ma mb mc jv md bi translated">TF-IDF</h1><p id="4552" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">TF-IDF是对文档中的单词进行文本矢量化的臭名昭著的方法之一。</p><blockquote class="me mf mg"><p id="8e97" class="ie if kn ig b ih ii ij ik il im in io mh iq ir is mi iu iv iw mj iy iz ja jb ha bi translated">文档:代表单个数据点的一组单词、文本或句子<br/>语料库:文档的集合</p></blockquote><p id="3064" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> TF </strong>代表<strong class="ig hi"> <em class="kn">词频</em> </strong>，代表该词在文档中出现的概率。</p><figure class="ks kt ku kv fd kx er es paragraph-image"><div class="er es mk"><img src="../Images/7fcfc486f3c43a21f3d4299908847f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/1*Yznju3RBOYhP5tAY7Uxspw.gif"/></div></figure><p id="871f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似于上面的定义，我们可以定义<em class="kn">文档频率</em>，它</p><figure class="ks kt ku kv fd kx er es paragraph-image"><div class="er es ml"><img src="../Images/f3d31844cacaa14f50cf7b0b6fe48698.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/1*tDz356TRMOZkmBC8yJ5ivQ.gif"/></div></figure><p id="b3e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">表示在语料库的文档中找到该单词的概率。</p><p id="b360" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> IDF </strong>代表<strong class="ig hi"> <em class="kn">逆文档频率</em> </strong>。这代表<em class="kn">文档频率</em>的<strong class="ig hi">逆</strong>，除了文档频率逆的变化——<strong class="ig hi">对数</strong>变换。</p><figure class="ks kt ku kv fd kx er es paragraph-image"><div class="er es mm"><img src="../Images/2d6ab99951c0d83c3055081acbb9d76c.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/1*bGp1pXvaRdlKtgU0ywda0Q.gif"/></div></figure><p id="bbcd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">IDF为我们提供了一个衡量单词在整个语料库(或数据集)中的重要性的方法。换句话说，在语料库的不同文档中很少出现的单词被赋予更高的重要性。</p><p id="7704" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，该等式使用文档频率倒数的对数变换。这就引出了下一个问题。</p><h2 id="3f1d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么我们使用对数标度来计算IDF？</h2><p id="c055" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">我们可以试着从理论和实践两个方面进行推理。</p><p id="fef7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一种<strong class="ig hi">理论上的</strong>解释，一种语言(如<em class="kn">英语</em>)中词的出现频率遵循<a class="ae kr" href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank">齐夫定律</a>，表述为<em class="kn">给定一些</em> <a class="ae kr" href="https://en.wikipedia.org/wiki/Text_corpus" rel="noopener ugc nofollow" target="_blank"> <em class="kn">语料库</em></a><em class="kn"/><a class="ae kr" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank"><em class="kn">自然语言</em> </a> <em class="kn">的话语，任何词的出现频率都与</em> <a class="ae kr" href="https://en.wikipedia.org/wiki/Inversely_proportional" rel="noopener ugc nofollow" target="_blank"> <em class="kn">成反比</em></a></p><figure class="ks kt ku kv fd kx er es paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="er es mn"><img src="../Images/35e04515ae44f03ed0078aaff6d3b2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fPCCzbdA7uaBHUo9.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Source: <a class="ae kr" href="https://phys.org/news/2017-08-unzipping-zipf-law-solution-century-old.html" rel="noopener ugc nofollow" target="_blank">phys.org</a> — Early frequency distributions for the words in English</figcaption></figure><p id="a4d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于英语的这一特性，词频的概率分布遵循<a class="ae kr" href="https://en.wikipedia.org/wiki/Power_law" rel="noopener ugc nofollow" target="_blank"> <em class="kn">幂律</em> </a>分布。现在，许多机器学习算法在变量的<em class="kn">高斯</em>分布下表现更好。为了将数据从<em class="kn">幂律</em>分布转换为<em class="kn">高斯</em>分布，我们可以使用具有<strong class="ig hi">对数</strong>性质的<a class="ae kr" href="https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation" rel="noopener ugc nofollow" target="_blank"> <em class="kn"> Box-Cox </em> </a>变换。这可能解释了为什么在研究论文<a class="ae kr" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.8343" rel="noopener ugc nofollow" target="_blank">中建议对IDF进行对数变换。</a></p><p id="d95c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其次，一个更<strong class="ig hi">实用的</strong>直觉是缩小IDF的范围，使其与TF相对可比，这样当这两个数在TF-IDF中相乘时，产生的结果值具有两者的可比贡献。</p><h2 id="7c7d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">为什么TF和IDF的射程一定要相对比较？</h2><p id="3561" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">想想英语中的单词<em class="kn">、</em>和<em class="kn">勤劳的</em>。我们知道后者有更多的意义，必须被认为是重要的。语料库中单词<em class="kn">‘the’</em>的IDF(没有<em class="kn"> log </em>变换)可以是1。<em class="kn">如何</em>？</p><p id="9d3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好吧，考虑一个有1000个文档的语料库。如果单词'<em class="kn"/>'出现在所有<em class="kn"> 1000 </em>文档中，那么根据IDF的定义(没有<em class="kn"> log </em>变换)就是<em class="kn"> 1000/1000 = 1 </em>。</p><p id="fd1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，因为'<em class="kn">勤劳的</em>'是一个很少出现的单词，IDF(没有<em class="kn">日志</em>转换)可能是<em class="kn"> 1000 </em>。<em class="kn">如何</em>？</p><p id="80eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑与<em class="kn"> 1000 </em>文档相同的语料库。如果单词'<em class="kn">勤劳的</em>'只出现在<strong class="ig hi">一个</strong>文档中，那么根据IDF的定义(没有<em class="kn">日志</em>变换)就是<em class="kn"> 1000/1 = 1000 </em>。</p><p id="7407" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">IDF的值的范围(没有<em class="kn"> log </em>变换)现在是<em class="kn">【1，1000】</em>——由于很少出现单词。然而，通过以<em class="kn">为基数</em>为10的对数变换(为简单起见)，该范围缩小到<em class="kn">【1，3】</em>。</p><p id="8b55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们将TF和IDF相乘(不使用<em class="kn"> log </em>变换)，那么这些值现在会迅速爆炸(由于IDF的巨大范围)。需要抑制该产品中IDF的影响。</p><p id="c3c3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了补偿巨大的范围并抑制这种影响，我们对IDF使用对数变换，使得TF和IDF在TF-IDF的数值中的贡献是可比较的，并且在相对范围内。</p><h2 id="ad43" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">计算出TF-IDF值后接下来会发生什么？</h2><p id="eff8" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">对于每个单词，我们都有一个TF-IDF值。在文档的向量表示中，与每个单词相关的维度值保存数字TF-IDF值。这个向量用于训练、测试和评估模型。</p><h2 id="5189" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">TF-IDF的缺点是什么？</h2><p id="7b95" class="pw-post-body-paragraph ie if hh ig b ih jz ij ik il ka in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">首先，我们可以清楚地看到，可以有两个不同的词，具有相同的TF-IDF值。因为，这个度量只考虑了这个词在文档和语料库中出现的次数。</p><p id="fd57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二，它在具有相似含义的单词在语料库中出现相似次数的假设下工作。或者，换句话说，我们也可以说，单词的语义是通过这种方法来解释的。比如:<em class="kn">好吃</em>和<em class="kn">好吃</em>是两个意思相同的词。但是，我们不能保证这些词的TF-IDF值是相对的和相似的。</p><p id="4f4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，这些缺点可以通过使用2013年提出的算法<a class="ae kr" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="kn"> Word2Vec </em> </strong> </a>来克服——这被证明是一种最先进的文本矢量化技术。将这一技巧留到下一篇文章中。</p><p id="5c77" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">希望这有所帮助:)</p><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ld mv"/></div></div></a></div></div></div>    
</body>
</html>