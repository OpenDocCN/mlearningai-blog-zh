<html>
<head>
<title>Optimizers in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的优化器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0?source=collection_archive---------0-----------------------#2021-03-27">https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0?source=collection_archive---------0-----------------------#2021-03-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="0988" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">什么是优化器？</h1><p id="94a4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">优化器</strong>是用于最小化误差函数(<em class="ka">损失函数</em>)或最大化生产效率的算法或方法。优化器是数学函数，取决于模型的可学习参数，即权重&amp;偏差。优化器帮助知道如何改变神经网络的权重和学习速率来减少损失。</p><p id="e2aa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">这篇文章将带你了解优化器和一些流行的方法。</p><h1 id="25af" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">优化器的类型</h1><p id="d762" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">让我们来了解不同类型的优化器，以及它们是如何将损失函数最小化的。</p><h1 id="b47f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak">梯度下降</strong></h1><p id="c363" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">梯度下降是一种基于凸函数的优化算法，它反复调整凸函数的参数，以使给定函数最小化到其局部最小值。梯度下降<strong class="je hi"> </strong>通过在与最陡上升相反的方向上移动来迭代地减少损失函数。寻找最小值依赖于损失函数的导数。使用整个训练集的数据来计算成本函数对参数的梯度，这需要大量的存储器并且减慢了过程。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/a2b0021bdd5b13aa65971faeceb44b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*NjuyXhO7mASiYZuxPH63oQ.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Gradient Descent</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/f80b6c57db751137295779f8f6517617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I47BK0AUgESr4M3hlvUwnQ.png"/></div></div></figure><p id="871f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">梯度下降的优点</strong></p><ol class=""><li id="62b1" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">容易理解</li><li id="ca10" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">易于实施</li></ol><p id="d3aa" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">梯度下降的缺点</strong></p><ol class=""><li id="28b6" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">因为这种方法在一次更新中计算整个数据集的梯度，所以计算非常慢。</li><li id="429b" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">它需要很大的内存，并且计算量很大。</li></ol><h1 id="a26d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">学习率</h1><p id="c454" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">梯度下降进入局部最小值方向的步长有多大/多小由学习速率决定，学习速率计算出我们向最优权重移动的快慢。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ll"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Learning Rate</figcaption></figure><h1 id="c2fd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">随机梯度下降</h1><p id="b8d7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是梯度下降的一个变种。它逐个更新模型参数。如果模型有10K数据集，SGD将更新模型参数10k次。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/0e51e762593fc549487cdb9d99424cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*RGfDmTb1yhzGLclpnWK8tA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Stochastic Gradient Descent</figcaption></figure><p id="de01" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">随机梯度下降的优势</strong></p><ol class=""><li id="742f" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">模型参数的频繁更新</li><li id="cea4" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">需要更少的内存。</li><li id="0f55" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">允许使用大型数据集，因为一次只需更新一个示例。</li></ol><p id="1601" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">随机梯度下降的缺点</strong></p><ol class=""><li id="53f6" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">这种频繁还会导致噪声梯度，从而导致误差增加而不是减少。</li><li id="d44b" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">高方差。</li><li id="8494" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">频繁更新的计算成本很高。</li></ol><h1 id="268a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">小批量梯度下降</h1><p id="4e22" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">它结合了SGD和批量梯度下降的概念。它只是将训练数据集分成小批，并对每一批执行更新。这在随机梯度下降的鲁棒性和批量梯度下降的效率之间建立了平衡。它可以减小参数更新时的方差，收敛更稳定。它将数据集分成随机选择的50到256个样本。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ln"><img src="../Images/76bd2dcb5e057f6a4c0ea2b313beae75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*DT91N-b0OCW7RQQ_7gtEiA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Mini Batch Gradient Descent</figcaption></figure><p id="21d2" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">小批量梯度下降的优势:</strong></p><ol class=""><li id="12bb" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">它导致更稳定的收敛。</li><li id="c2d6" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">更高效的梯度计算。</li><li id="5b2e" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">需要更少的内存。</li></ol><p id="718f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">小批量梯度下降的缺点</strong></p><ol class=""><li id="cf99" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">小批量梯度下降不能保证良好的收敛性，</li><li id="0c10" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">如果学习率太小，收敛速度会很慢。如果太大，损失函数会在最小值处振荡甚至偏离。</li></ol><h1 id="d4d2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">带动量的SGD</h1><p id="5212" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">带动量的SGD</strong>是一种在常规随机梯度下降中加入动量项的随机优化方法。动量模拟物体在运动时的惯性，即在更新时一定程度上保留前一次更新的方向，而用当前的更新梯度来微调最终的更新方向。这样就可以在一定程度上增加稳定性，让自己学得更快，也有能力摆脱局部优化。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lo"><img src="../Images/fabdbd4454ff8850c8b448d349c3ab34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5lNKxAHLPYNc6-Zs4Vscw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">SGD with Momentum</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/4de8760da2fdedf595f411f6cb9a29a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNiy4Uhfil_fTT95KstNIg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Momentum Formula</figcaption></figure><p id="2f67" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">带动量的SGD的优势</strong></p><ol class=""><li id="b2cc" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">动量有助于降低噪音。</li><li id="48f2" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">指数加权平均用于平滑曲线。</li></ol><p id="4b51" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">带动量的SGD的缺点</strong></p><ol class=""><li id="8009" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">增加了额外的超参数。</li></ol><h1 id="7843" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated"><strong class="ak"> AdaGrad(自适应梯度下降)</strong></h1><p id="363e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在我们之前讨论的所有算法中，学习率保持不变。AdaGrad背后的直觉是，我们可以基于不同的迭代对每个隐藏层的每个神经元使用不同的学习速率。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/902cdd2340c55db244ede8c33c147af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wls0cHKicrnvT8woi5Syw.png"/></div></div></figure><p id="082f" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">AdaGrad的优势</strong></p><ol class=""><li id="097e" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">学习率随着迭代自适应地变化。</li><li id="aec4" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">它也能够训练稀疏数据。</li></ol><p id="be69" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">阿达格拉德的缺点</strong></p><ol class=""><li id="feb3" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">如果神经网络很深，学习率变得非常小，这将导致死神经元问题。</li></ol><h1 id="b9d3" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">均方根传播</h1><p id="1a75" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">RMS-Prop是Adagrad的特殊版本，其中学习速率是梯度的指数平均值，而不是梯度平方的累积和。RMS-Prop基本上结合了动量和AdaGrad。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/7888af87601f349e292325227fc9873e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-FnGQ6nm7Hipbn6rNxuPQ.png"/></div></div></figure><p id="2471" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">RMS-Prop的优势</strong></p><ol class=""><li id="d2c8" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">在RMS-Prop中，学习率会自动调整，并为每个参数选择不同的学习率。</li></ol><p id="4ffe" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">RMS-Prop的缺点</strong></p><ol class=""><li id="69c6" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">缓慢的学习</li></ol><h1 id="9e5c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">阿达德尔塔</h1><p id="b875" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Adadelta是Adagrad的扩展，它也试图减少Adagrad的攻击性，单调地降低学习率，并消除学习率衰减的问题。在Adadelta中，我们不需要设置默认的学习速率，因为我们采用先前时间步长的运行平均值与当前梯度的比值。</p><p id="16cc" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">阿达德尔塔的优势</strong></p><ol class=""><li id="37d7" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">AdaDelta的主要优点是我们不需要设置默认的学习速率。</li></ol><p id="a32d" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">阿达德尔塔的缺点</strong></p><ol class=""><li id="7575" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">计算成本高</li></ol><h1 id="9353" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Adam(自适应矩估计)</h1><p id="59e3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Adam optimizer是最流行和最著名的梯度下降优化算法之一。这是一种为每个参数计算自适应学习率的方法。它存储过去梯度的衰减平均值(类似于动量)和过去平方梯度的衰减平均值(类似于RMS-Prop和Adadelta)。因此，它结合了两种方法的优点。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lp"><img src="../Images/99f9d81d85ebc6a9777ad91dbba7287d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qm07YVd-EGf2Ib7C_myDIw.jpeg"/></div></div></figure><p id="9319" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><strong class="je hi">亚当的优点</strong></p><ol class=""><li id="055b" class="kx ky hh je b jf kb jj kc jn kz jr la jv lb jz lc ld le lf bi translated">易于实施</li><li id="1519" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">计算效率高。</li><li id="db4c" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lc ld le lf bi translated">内存需求小。</li></ol><h1 id="ea0a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">比较</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/25bb3cf8e7c92e753c2f8368baf920dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*XVFmo9NxLnwDr3SxzKy-rA.gif"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Optimizers Comparison</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/5ced5286be46c059022fec7161e0f4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*SjtKOauOXFVjWRR7iCtHiA.gif"/></div><figcaption class="ks kt et er es ku kv bd b be z dx"><strong class="bd ig">Optimization on saddle point</strong></figcaption></figure><h1 id="1983" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">如何选择优化器？</h1><ul class=""><li id="cda1" class="kx ky hh je b jf jg jj jk jn lr jr ls jv lt jz lu ld le lf bi translated">如果数据稀疏，使用自适用的方法，即Adagrad、Adadelta、RMSprop、Adam。</li><li id="c06b" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lu ld le lf bi translated">RMSprop，Adadelta，Adam在很多情况下都有类似的效果。</li><li id="e17f" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lu ld le lf bi translated">Adam只是在RMSprop的基础上增加了偏差修正和动量，</li><li id="7d26" class="kx ky hh je b jf lg jj lh jn li jr lj jv lk jz lu ld le lf bi translated">随着梯度变得稀疏，Adam的性能将优于RMSprop。</li></ul><p id="706d" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated">我希望这篇文章能够帮助你学习和理解这些概念。</p><div class="lv lw ez fb lx ly"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hi fi z dy md ea eb me ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm kq ly"/></div></div></a></div><p id="25cf" class="pw-post-body-paragraph jc jd hh je b jf kb jh ji jj kc jl jm jn kd jp jq jr ke jt ju jv kf jx jy jz ha bi translated"><a class="ae mn" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">成为ML作家</a></p></div></div>    
</body>
</html>