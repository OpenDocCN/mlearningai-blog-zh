<html>
<head>
<title>Topic Modeling with Latent Dirichlet Allocation (LDA) on the Tweets Mentioning Elon Musk: Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用潜在狄利克雷分配(LDA)对提及埃隆马斯克的推文进行主题建模:第一部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/topic-modelling-with-lda-on-the-tweets-mentioning-elon-musk-687076a2c86b?source=collection_archive---------2-----------------------#2022-01-19">https://medium.com/mlearning-ai/topic-modelling-with-lda-on-the-tweets-mentioning-elon-musk-687076a2c86b?source=collection_archive---------2-----------------------#2022-01-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="2fda" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">发现推文中的潜在话题，了解人们在Twitter上谈论埃隆·马斯克</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/c14b0c9f9213a2c931630296d489bf7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*jjhkii_JmvCEazFAzQIdGA.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">LDA Visualization</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="6edf" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">自然语言处理是当今的热门话题，主题建模是NLP技术之一，组织从中受益匪浅。</p><p id="6958" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在本文中，我将尝试解释主题建模背后的思想，并将其应用于使用<a class="ae kc" href="https://developer.twitter.com/apitools/api?endpoint=%2F2%2Ftweets%2Fsearch%2Frecent&amp;method=get" rel="noopener ugc nofollow" target="_blank"> Twitter API </a>提取的Twitter数据。我将分两部分写这篇文章。在第1部分中，我将解释像主题建模和LDA这样的概念。我还会进行大量的数据清理和处理。最后，我将实现LDA算法并可视化结果。这里，数据集包含了关于埃隆·马斯克的推文。通过对数据应用主题建模，我们将能够深入了解人们在Twitter上谈论埃隆·马斯克的内容。</p><p id="365f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在本文结束时，您将理解潜在的Dirichlet分配(LDA)算法，并且能够使用Python库实现LDA。还会学到一些很多时候被忽略的数据准备步骤。最后，您将能够在一个非常酷的仪表板上可视化由LDA模型识别的主题，就像您在上面看到的那样。我们开始吧！！</p><p id="92d8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">话题造型</strong></p><p id="4587" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">主题建模是一种无监督的学习技术，用于在多个主题的帮助下表示文本文档。它不需要文档的预定义标签列表，而是分析文本数据来确定一组文档的簇词(主题)。</p><p id="07c9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">假设你经营一家初创企业，为你的客户提供B2C服务。您希望了解客户对您的服务的评价，以便做出明智的决策。你可以用主题建模算法来分析它们，而不是手动浏览大量的反馈，试图推断出哪些文本在谈论你感兴趣的主题。</p><p id="c59d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">有几种方法可以进行主题建模，但是在这篇文章中，我们将介绍潜在的狄利克雷分配(LDA)算法，这可能是最流行的主题建模方法。</p><p id="3774" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">潜在狄利克雷分配(LDA) </strong></p><p id="78cd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">为了理解LDA算法，让我们看看这个例子。</p><p id="ea78" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">假设您有以下文档。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es kd"><img src="../Images/602382879929426726782853f0895568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*Lb2Ee1EoNQSSDzXEgZA9ag.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Table by Author</figcaption></figure><p id="9c57" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">LDA会发现这些句子包含的潜在主题。给定这些句子，LDA可能会生成如下内容:</p><p id="4ea8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">文件1和文件2: </strong> 100%题目A</p><p id="849b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">文件3和4: </strong> 100%题目B</p><p id="f188" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">文档5: </strong> 50%话题A，50%话题B</p><p id="540f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">其中<strong class="ik hi">话题A: </strong> 30%苹果，10%橘子，10%牛奶，10%馅饼，10%市场…(人们会将话题A理解为关于食物)以及</p><p id="e104" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">话题B: </strong> 30%市场，10%文具，10%买入，…。(有人会将话题B理解为关于食物)</p><p id="4230" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">因此，LDA将文档表示为主题的混合，将每个主题表示为单词的混合。</p><p id="1363" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">了解了LDA算法的输出之后，让我们看看它是如何学习生成这些输出的。</p><p id="7d71" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">假设你有n个文档，你选择了固定数量的m个主题来识别。现在，您想使用LDA来了解每个文档的主题分布和每个主题的单词分布。LDA使用<strong class="ik hi"> Gibbs抽样</strong>学习这些分布。接下来的步骤是:</p><p id="6797" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">遍历每个文档，将文档中的每个单词随机分配给m个主题中的一个。</p><p id="ab82" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">如果我们仔细观察，我们会发现这种随机分配既给出了所有n个文档的主题分布，也给出了所有m个主题的单词分布。最初，这些分布不会很精确。</p><p id="8865" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">为了改善每个文档d的分布，遍历d中的每个单词w……对于每个主题t，计算两个概率:</p><p id="0bd4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi"> 1。</strong> <strong class="ik hi"> p(主题t |文档d) </strong> =文档d中当前分配给主题t的字数/d中的总字数</p><p id="c64a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi"> 2。p(word w | topic t) </strong> =主题t的赋值在所有来自这个单词w的文档中所占的比例。</p><p id="6a6f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">计算完以上两个概率后，再给w重新分配一个新的题目，这里我们选择概率为<strong class="ik hi"> <em class="ij"> p的题目t(题目t |文档d) * p(单词w |题目t) </em> </strong>。请注意，对于每个单词，我们将获得一个概率向量，该向量将解释该单词属于每个主题的可能性。</p><p id="1526" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在重复上一步很多次后，你最终会达到一个大致稳定的状态，你的任务完成得相当好。因此，使用这些分配来估计每个文档的主题混合(通过统计分配给该文档中每个主题的单词的比例)和与每个主题相关的单词(通过统计分配给每个主题的单词的总体比例)。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="e4b2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">实施</strong></p><p id="c37a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">现在，让我们看看如何实现LDA。如前所述，我将使用谈论埃隆·马斯克的推文作为数据。但是在将LDA算法应用到我们的数据之前，我将做一些预处理。实际上，很多！！</p><p id="d6b8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">进口:</em></p><p id="d345" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">我将使用熊猫、re、缩写和spaCy进行数据清理和预处理。我将为LDA和pyLDAvis生成一个非常酷的交互式仪表板gensim。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="e6e6" class="kj kk hh kf b fi kl km l kn ko">import pandas as pd<br/>import unicodedata<br/>import re<br/>import contractions<br/>import string</span><span id="4f8b" class="kj kk hh kf b fi kp km l kn ko">#Gensim<br/>import gensim<br/>import gensim.corpora as corpora<br/>from gensim.utils import simple_preprocess<br/>from gensim.models import CoherenceModel</span><span id="11c6" class="kj kk hh kf b fi kp km l kn ko">#spacy<br/>import spacy<br/>from nltk.corpus import stopwords</span><span id="0e42" class="kj kk hh kf b fi kp km l kn ko">#vis<br/>import pyLDAvis<br/>import pyLDAvis.gensim_models</span></pre><p id="5a89" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">数据加载:</em></p><p id="e1f7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">我已经使用<a class="ae kc" href="https://developer.twitter.com/apitools/api?endpoint=%2F2%2Ftweets%2Fsearch%2Frecent&amp;method=get" rel="noopener ugc nofollow" target="_blank"> Twitter API </a>获取了数据，因此我将在这里直接导入数据。你可以从<a class="ae kc" href="https://drive.google.com/file/d/1UmzzWlO-PaQbHg3Ggmj7N21WW0gQSYdH/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据，或者你可以使用Twitter API获取你选择的数据。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="e3ad" class="kj kk hh kf b fi kl km l kn ko">def load_data(path):<br/>    return pd.read_csv(path)</span><span id="9468" class="kj kk hh kf b fi kp km l kn ko">tweets_df=load_data(path)</span></pre><p id="9cc2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在我的例子中，名为“<em class="ij"> Tweets </em>的列包含文本数据，即Tweets。我将对这个专栏进行数据清理和处理。</p><p id="d18a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">数据清理和处理:</em></p><p id="c0e0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">小写转换:</strong>首先，我们将整个文本数据转换成小写。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="3aff" class="kj kk hh kf b fi kl km l kn ko">def to_lowercase(text):<br/>    return text.lower()</span><span id="12e6" class="kj kk hh kf b fi kp km l kn ko">#converting every row of the column into lower case <br/>tweets_df.Tweets=tweets_df.Tweets.apply(to_lowercase)</span></pre><p id="3b45" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">规范重音字符:</strong>有时，人们会使用像<em class="ij"> é、φ等重音字符。</em>表示发音时对特定字母的强调。在某些情况下，重音符号还可以澄清单词的意思，如果没有重音符号，意思可能会有所不同。虽然重音符号的使用非常有限，但是将这些字符转换成标准的ASCII字符是一个很好的做法。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="7d1c" class="kj kk hh kf b fi kl km l kn ko">def standardize_accented_chars(text):<br/> return unicodedata.normalize(‘NFKD’, text).encode(‘ascii’, ‘ignore’).decode(‘utf-8’, ‘ignore’)</span><span id="0eb5" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(standardize_accented_chars('Sómě words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.'))</span><span id="00d6" class="kj kk hh kf b fi kp km l kn ko">#standardizing accented characters for every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(standardize_accented_chars)</span></pre><p id="9713" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="2237" class="kj kk hh kf b fi kl km l kn ko">Some words such as resume, cafe, pretest, divorce, coordinate, expose, latte.</span></pre><p id="3308" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">删除URL:</strong>很多时候，人们在推文中使用URL来给他们的推文添加额外的信息。让我们使用下面提到的代码来计算数据集中包含URL的tweets的百分比。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="0c5a" class="kj kk hh kf b fi kl km l kn ko">def get_number_of_urls(documents):<br/>    print("{:.2f}% of documents contain urls".format(sum<br/>(documents.apply(lambda x:x.find('http'))&gt;0)/len<br/>(documents)*100))</span><span id="1fd2" class="kj kk hh kf b fi kp km l kn ko"># Passing the 'Tweets' column of the dataframe as the argument<br/>print(get_number_of_urls(tweets_df.Tweets)) </span></pre><p id="e5c8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="4802" class="kj kk hh kf b fi kl km l kn ko">57.38% of documents contain urls</span></pre><p id="9c69" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">57.38% !！！这是相当多的数据。URL不提供任何关于文档主题的信息。我们将使用下面提到的函数删除文本中的URL</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="7dab" class="kj kk hh kf b fi kl km l kn ko">def remove_url(text):<br/> return re.sub(r’https?:\S*’, ‘’, text)</span><span id="8395" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(remove_url('using <a class="ae kc" href="https://www.google.com/" rel="noopener ugc nofollow" target="_blank">https://www.google.com/</a> as an example'))</span><span id="c9df" class="kj kk hh kf b fi kp km l kn ko">#removing urls from every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(remove_url)</span></pre><p id="d5a4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="ba19" class="kj kk hh kf b fi kl km l kn ko">'using as an example'</span></pre><p id="4f90" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">扩展缩写:</strong>缩写是单词或音节的缩短版本。它们是通过从单词中去掉一个或多个字母而产生的。通常情况下，多个单词组合在一起构成一个缩略词。例如，<em class="ij"> I will就承包成I will，do not成don。我们不希望我们的模型以不同的方式考虑我会和我会。因此，我们将使用下面提到的代码将每个收缩转换为其扩展形式。</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="e65c" class="kj kk hh kf b fi kl km l kn ko">def expand_contractions(text):<br/>    expanded_words = [] <br/>    for word in text.split():<br/>       expanded_words.append(contractions.fix(word)) <br/>    return ‘ '.join(expanded_words)</span><span id="0fa3" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(expand_contractions("Don't is same as do not"))</span><span id="65b0" class="kj kk hh kf b fi kp km l kn ko">#expanding contractions for every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(expand_contractions)</span></pre><p id="a995" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="a4b1" class="kj kk hh kf b fi kl km l kn ko">Do not is same as do not</span></pre><p id="c78a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">删除提及和标签:</strong>经过快速分析，我发现在这个例子中，提及和标签对主题建模的相关信息没有太大贡献，因为它们不遵循任何模式，即它们在推文中被随机使用。使用下面提到的功能删除提及和标签。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="3556" class="kj kk hh kf b fi kl km l kn ko">def remove_mentions_and_tags(text):<br/>    text = re.sub(r’@\S*’, ‘’, text)<br/>    return re.sub(r’#\S*’, ‘’, text)</span><span id="3796" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(remove_mentions_and_tags('Some random @abc and #def'))</span><span id="5ea1" class="kj kk hh kf b fi kp km l kn ko">#removing mentions and tags from every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(remove_tags)</span></pre><p id="08cf" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="51cc" class="kj kk hh kf b fi kl km l kn ko"><em class="ij">Some random and</em></span></pre><p id="4c4c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">仅保留字母:</strong>标点符号、数字和特殊字符，如'<em class="ij"> $、%等。</em>不提供任何信息。因此，我们将只保留字母，并使用下面提到的函数删除文本中的所有内容。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="b0c5" class="kj kk hh kf b fi kl km l kn ko">def keep_only_alphabet(text):<br/>    return re.sub(r’[^a-z]’, ‘ ‘, text)</span><span id="b01f" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(keep_only_alphabet('Just a bit more $$processing required.Just a bit!!!'))</span><span id="65c2" class="kj kk hh kf b fi kp km l kn ko">#for all the rows<br/>tweets_df.Tweets=tweets_df.Tweets.apply(keep_only_alphabet)</span></pre><p id="8938" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="385e" class="kj kk hh kf b fi kl km l kn ko">'Just a bit more   processing required Just a bit   '</span></pre><p id="5202" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">去掉停用词(默认+自定义)去掉短词:</strong>停用词像<em class="ij">我，am，me等，</em>不添加任何有助于建模的信息。保持它们增加了噪声并增加了我们的单词向量的维数，这严重影响了计算成本和模型精度。因此，我们想删除它们。</p><p id="a6d9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">此外，在大多数推文中，人们已经用<em class="ij">【伊隆】</em>和<em class="ij">【马斯克】来</em>指代伊隆·马斯克。但是，因为我们知道我们正在对谈论埃隆·马斯克的推文进行建模，所以在推文中保留他的名字会增加不必要的信息。</p><p id="6bf1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">最后，我们将删除包含少于3个字母的单词。虽然当我们删除停用词时，其中一些会被删除，但一些会留下来，这可能是由于打字风格(使用俚语，如GM表示早上好)或由于打字错误而引入的。这些词不能概括所有样本，因此我们将其删除。</p><p id="4cea" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在下面提到的函数中，我使用spacy库来移除停用词。自定义停用词可以作为参数传递。</p><p id="192d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">在代码中，在初始化spaCy对象时，我使用了<em class="ij"> disable=["parser "、" ner "、" lemmatizer"] </em>来避免不必要的计算。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="d116" class="kj kk hh kf b fi kl km l kn ko">def remove_stopwords(text,nlp,custom_stop_words=None,<br/>remove_small_tokens=True,min_len=2):</span><span id="2c0d" class="kj kk hh kf b fi kp km l kn ko">    # if custom stop words are provided, then add them to default stop words list<br/>    if custom_stop_words:<br/>        nlp.Defaults.stop_words |= custom_stop_words<br/>    <br/>    filtered_sentence =[] <br/>    doc=nlp(text)<br/>    for token in doc:<br/>        <br/>        if token.is_stop == False: <br/>            <br/>            # if small tokens have to be removed, then select only those which are longer than the min_len <br/>            if remove_small_tokens:<br/>                if len(token.text)&gt;min_len:<br/>                    filtered_sentence.append(token.text)<br/>            else:<br/>                filtered_sentence.append(token.text)</span><span id="1a38" class="kj kk hh kf b fi kp km l kn ko">    # if after the stop word removal, words are still left in the sentence, then return the sentence as a string else return null <br/>    return “ “.join(filtered_sentence) if len(filtered_sentence)&gt;0 else None</span><span id="24c2" class="kj kk hh kf b fi kp km l kn ko">#creating a spaCy object. <br/>nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])</span><span id="2a60" class="kj kk hh kf b fi kp km l kn ko">#removing stop-words and short words from every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(lambda x:remove_stopwords(x,nlp,{"elon","musk",}))</span></pre><p id="a137" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">词汇化:</strong>词汇化生成单词的词根。它利用词汇和词形分析来生成单词的词根形式。我们将使用spaCy库来执行词汇化。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="df27" class="kj kk hh kf b fi kl km l kn ko">def lemmatize(text, nlp):<br/>   doc = nlp(text)<br/>   lemmatized_text = []<br/>   for token in doc:<br/>   lemmatized_text.append(token.lemma_)<br/>   return “ “.join(lemmatized_text)</span><span id="4044" class="kj kk hh kf b fi kp km l kn ko">#testing the function on a single sample for explaination<br/>print(lemmatize('Reading NLP blog is fun.' ,nlp ))</span><span id="d27a" class="kj kk hh kf b fi kp km l kn ko">#Performing lemmatization on every row<br/>tweets_df.Tweets=tweets_df.Tweets.apply(lambda x:lemmatize(x,nlp))</span></pre><p id="fee6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="da58" class="kj kk hh kf b fi kl km l kn ko">'Read NLP blog is fun.'</span></pre><p id="50a8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">请注意，原来句子中的<em class="ij">读数</em>变成了<em class="ij">读数。</em>这就是词汇化的作用。将每个单词转换为其词根形式，以便模型可以将单词的词形变化形式和词根形式视为相同。</p><p id="e66b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">生成文档矩阵和字典:</strong></p><p id="c561" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">LDA主题模型算法需要文档单词矩阵和字典作为主要输入。</p><p id="62a6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">一个<strong class="ik hi">文档-术语矩阵</strong>是一个数学矩阵，描述术语在文档集合中出现的频率。在文档-术语矩阵中，行对应于集合中的文档，列对应于术语。</p><p id="62f2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">一个<strong class="ik hi">字典</strong>是文档中存在的所有唯一标记的集合。</p><p id="59ad" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">为了生成文档-术语矩阵和字典，首先我们将使用下面提到的函数将句子转换成记号。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="2b47" class="kj kk hh kf b fi kl km l kn ko">def generate_tokens(tweet):<br/>    words=[]<br/>    for word in tweet.split(‘ ‘):<br/>    # using the if condition because we introduced extra spaces during text cleaning<br/>    if word!=’’:<br/>       words.append(word)<br/>    return words</span><span id="d12f" class="kj kk hh kf b fi kp km l kn ko">#storing the generated tokens in a new column named 'words'<br/>tweets_df['tokens']=tweets_df.Tweets.apply(generate_tokens)</span></pre><p id="b3d9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">然后我们创建一个字典和一个语料库对象，这是LDA模型需要的文档矩阵作为主要输入。使用下面提到的代码来生成唯一令牌的字典。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="1d9e" class="kj kk hh kf b fi kl km l kn ko">def create_dictionary(words):<br/>    return corpora.Dictionary(words)</span><span id="8e09" class="kj kk hh kf b fi kp km l kn ko">#passing the dataframe column having tokens as the argument<br/>id2word=create_dictionary(tweets_df.tokens)<br/>print(id2word)</span></pre><p id="8a94" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="3d75" class="kj kk hh kf b fi kl km l kn ko">Dictionary(19692 unique tokens: ['break', 'collaboration', 'influential', 'internet', 'material']...)</span></pre><p id="33e5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">使用下面提到的函数创建文档矩阵。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="e4f3" class="kj kk hh kf b fi kl km l kn ko">def create_document_matrix(tokens,id2word):<br/>    corpus = []<br/>    for text in tokens:<br/>       corpus.append(id2word.doc2bow(text))<br/> return corpus</span><span id="c489" class="kj kk hh kf b fi kp km l kn ko">#passing the dataframe column having tokens and dictionary<br/>corpus=create_document_matrix(tweets_df.tokens,id2word)<br/>print(tweets_df.tokens[0])<br/>print(corpus[0])</span></pre><p id="2505" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="8ff8" class="kj kk hh kf b fi kl km l kn ko">['collaboration','ridiculous','break','internet','material','influential','people','visionarie', 'unique', 'personality','people',<br/> 'special']</span><span id="1831" class="kj kk hh kf b fi kp km l kn ko">[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]</span></pre><p id="131a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">实施LDA: </strong></p><p id="6214" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">我们将为LDA使用gensim库。为了生成基本模型，我使用了<em class="ij"> num_topics=10。</em>我们可以稍后执行超参数调整，并达到最佳主题数量。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="de01" class="kj kk hh kf b fi kl km l kn ko">lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,<br/> id2word=id2word,<br/> num_topics=10,<br/> random_state=100,<br/> )</span></pre><p id="a121" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">生成LDA主题:</strong></p><p id="adcc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">我们将迭代LDA模型确定的主题，获得每个主题中的热门词。我们将使用下面提到的函数将每个主题的<em class="ij">前10个</em>单词存储在一个数据帧中。<em class="ij"> top_n_words </em>参数控制我们想要为每个主题存储的热门单词的数量</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="638f" class="kj kk hh kf b fi kl km l kn ko">def get_lda_topics(model, num_topics, top_n_words):<br/>     word_dict = {}<br/>     for i in range(num_topics):<br/>         word_dict[‘Topic # ‘ + ‘{:02d}’.format(i+1)] = [i[0] for i in model.show_topic(i, topn = top_n_words)];<br/> <br/>     return pd.DataFrame(word_dict)</span><span id="2c03" class="kj kk hh kf b fi kp km l kn ko">get_lda_topics(lda_model,10,10)</span></pre><p id="1f86" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><em class="ij">输出:</em></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kq"><img src="../Images/548cbd8c5033cc6b5e7e6e61e22e1c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g53MjJq04OVR9bxu0dscHQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Table by Author</figcaption></figure><p id="6e1d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated"><strong class="ik hi">可视化主题:</strong></p><p id="af07" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">可视化由我们的模型识别的主题有助于超参数调整。我们将使用pyLDAvis库来可视化结果。下面提到的代码将生成一个非常酷的显示结果的仪表板。</p><pre class="jh ji jj jk fd ke kf kg kh aw ki bi"><span id="9d00" class="kj kk hh kf b fi kl km l kn ko">pyLDAvis.enable_notebook()<br/>vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=”mmds”, R=30)<br/>vis</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/c14b0c9f9213a2c931630296d489bf7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*jjhkii_JmvCEazFAzQIdGA.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">GIF by Author</figcaption></figure><p id="9455" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jz iu iv iw ka iy iz ja kb jc jd je jf ha bi translated">瞧啊。！您已经成功地在高度预处理的文本数据上实现了LDA。但是等等，超参数调优和推断结果呢？此外，LDA在短文本上与其他方法相比如何？不要担心，我将在本文的第2部分讨论这个问题。我希望这有所帮助。第2部分见。谢谢！！！</p><div class="kv kw ez fb kx ky"><a rel="noopener follow" target="_blank" href="/data-driven-fiction/how-to-submit-5e0808dce313"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">如何提交？</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">数据驱动的小说</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">medium.com</p></div></div><div class="lh l"><div class="li l lj lk ll lh lm jm ky"/></div></div></a></div></div></div>    
</body>
</html>