<html>
<head>
<title>Convolutional networks, recurrent neural networks and transfomers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积网络、递归神经网络和变换器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/convolutional-networks-recurrent-neural-networks-and-transfomers-ee6a63ddb57f?source=collection_archive---------5-----------------------#2021-12-27">https://medium.com/mlearning-ai/convolutional-networks-recurrent-neural-networks-and-transfomers-ee6a63ddb57f?source=collection_archive---------5-----------------------#2021-12-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e16a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我总结Sergey Levine教授举办的CS182讲座系列文章的第二篇，所有的荣誉都归于他。所有图片均取自他的讲座。这些是第一条、<a class="ae jc" rel="noopener" href="/mlearning-ai/detailed-basics-of-deep-learning-part-three-b930c40cf7f8">、第三条</a>和<a class="ae jc" href="https://samuelebolotta.medium.com/7a88e847f62e" rel="noopener">第四条。</a></p><h2 id="952a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><a class="ae jc" href="https://www.youtube.com/watch?v=jNW1Hi7Yi4c&amp;list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&amp;index=17" rel="noopener ugc nofollow" target="_blank">卷积网络</a></h2><p id="c11c" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">假设你有一张可爱小狗的照片。你想对一张小狗、猫、河马或长颈鹿的图片进行分类。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/0148f658236853cfeaa41e73dda3e6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*vGYG1ZPsJmKMcPbzyGCb_Q.png"/></div></figure><p id="72e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以训练一个完全连接的网络来解决这个问题:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kl"><img src="../Images/89496bcff938265989e5e4b42190484d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*39xw8P27QF0v98xfEvvOSQ.png"/></div></figure><p id="c738" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，至少有两个问题:</p><p id="832a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 1计算困难</strong></p><p id="16f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们观察这样一个网络中的第一个线性层，我们会看到它读入一个图像并产生第一个激活向量。该线性层中的权重数量将等于第一隐藏层中的激活数量z(1)乘以整个图像中的数值数量。对于图像中的每个像素和每个颜色通道，我们需要与输出数量相等的多个权重。</p><p id="252b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设:</p><ul class=""><li id="ec99" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">这个图像是128x128x3 (49.152个数字)</li><li id="d771" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">我们的第一个隐藏层有64个维度，这实际上是一个非常小的隐藏层</li></ul><p id="b776" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">即使这样，这个第一权重向量中的参数总数将是49.152的64倍，超过300万:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es la"><img src="../Images/ccbb29d3515b2262612a504b616ac6bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*FvEMCXm2r61v3KItJ9Y7kQ.png"/></div></figure><p id="112d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这只是这个微小的64维隐藏层的第一层。</p><p id="608f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2对换档不稳定</strong></p><p id="0fdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果小狗的照片向左或向右移动哪怕一个像素，它在网络上看起来就会完全不同。</p><p id="699b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">一个想法</strong></p><p id="745d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在某种意义上，我们关心的许多视觉特征都是局部的。如果我们想识别这是一张小狗的照片，首先我们要提取边缘:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lb"><img src="../Images/efcfaedd28f1611248084052db2a4d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*_9NE1FgYvGL5bXd6ZZbg9A.png"/></div></figure><p id="3226" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们会提取一些局部区域，比如耳朵和鼻子:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lc"><img src="../Images/accf0026c55057f63fe694b00d5a6e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*hAq_dmd9ue_aPWwPVO4YEg.png"/></div></figure><blockquote class="ld le lf"><p id="81de" class="ie if lg ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">关于这些属性，有趣的是它们都是局部特征:要确定某个特定位置是否有边缘，你只需要查看附近的像素。 </p></blockquote><p id="d0c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以利用这一观察来设计一种神经网络，这种网络可以通过首先进行局部运算，然后仅在信息量已经减少到更易于管理的水平时才执行全局运算，而不需要更多的参数。</p><p id="1ad9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">卷积</strong></p><p id="940c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将构建一个小型边缘检测器，并将其应用于图像中的每个区域，因为您识别图像边缘的方式不会因您观察的位置而改变:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lk"><img src="../Images/ed3a44ad55921e64f859ee83e8768db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*YzbtWZSQqtz0_BjYnuGiTw.png"/></div></figure><ul class=""><li id="8f9a" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">我们的小检测器将被称为过滤器(或内核)。</li><li id="bda6" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">它将具有与输入图像相同的深度(在这种情况下是3个颜色通道)。</li><li id="2426" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">由于有64个特征，我们将为每个3x3x3区域堆叠64个过滤器。这里我只画了四个:</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ll"><img src="../Images/285f915ae908968e8e248147d45fdfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*e33L7BoIfaRz8kT2MncAYQ.png"/></div></figure><ul class=""><li id="793e" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">对于输入图像中的每个位置，我们有64个过滤器，这为每个3x3x3区域产生一个长度为64的小向量。</li><li id="348a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">我们将把这个3x3x3 (27个数字)过滤器应用于图像中的每个小块，并计算64个特征。现在，64乘以27只是1728，而不是300万。随着卷积滤波器沿图层的输入矩阵滑动，卷积运算会生成一个特征图，该图反过来会影响下一图层的输入。</li><li id="e096" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">之后，我们对它们中的每一个应用非线性，就像我们在常规神经网络中所做的那样</li><li id="a84f" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">我们现在得到另一个盒子，它的深度是64，宽度和高度也是128。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lm"><img src="../Images/1010ce0cca04e1613225ff61a6ff9e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*-YscRgumFlO6ztP3mEpUeg.jpeg"/></div></figure><p id="3435" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们已经将128x128x3的图像转换成了128x128x64的激活图。好像每个像素都变成了64个特征的向量。过滤器是什么样子的？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ln"><img src="../Images/266138d512b4617730cb3c0ef287535a.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*tyR6ZuzHR4_yAdFQzHBvlQ.png"/></div></figure><p id="ed47" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以想象将这些滤镜中的每一个通过图像，对于相应的滤镜，激活将是该特征存在的程度。</p><p id="430f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另外，卷积神经网络的一个优点是，网络通过自动学习来学习优化滤波器，因此它们独立于先验知识和人工干预。</p><p id="e1c9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积利用了三个可以帮助改进机器学习系统的重要思想:稀疏交互、参数共享和等变表示。</p><ol class=""><li id="fc2b" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">传统的神经网络层使用参数矩阵的矩阵乘法，其中单独的参数描述每个输入单元和每个输出单元之间的相互作用。这意味着每个输出单元都与每个输入单元相互作用。然而，卷积网络通常具有稀疏交互(也称为稀疏连通性或稀疏权重)。这是通过使内核小于输入来实现的。例如，在处理图像时，输入图像可能有数千或数百万个像素，但我们可以检测到小的、有意义的特征，如核仅占用数十或数百个像素的边缘。</li><li id="4ec9" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">参数共享是指在一个模型中对多个函数使用相同的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次。它被乘以输入的一个元素，然后不再被重新访问。作为参数共享的同义词，可以说网络具有绑定的权重，因为应用于一个输入的权重值与应用于其他地方的权重值绑定在一起。在卷积神经网络中，在输入的每个位置使用核的每个成员(可能除了一些边界像素，取决于关于边界的设计决策)。卷积运算使用的参数共享意味着我们只学习一组参数，而不是为每个位置学习一组单独的参数。</li><li id="6597" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">在卷积的情况下，参数共享的特定形式会使图层具有一种称为平移等方差的属性。说一个函数是等变的意味着如果输入改变，输出也以同样的方式改变。</li></ol><p id="0dbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是这里有一个问题:我们没有把东西做得更小。我们减少了参数的数量，但我们的激活仍然是这些巨大的地图，它们具有与原始图像相同的分辨率和潜在的更多深度，因为我们可能想要比原始图像中的颜色通道更多的功能。</p><p id="c05d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">联营</strong></p><p id="a221" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了降低图层的分辨率，我们将每个2x2x64的面片转换为1x1x64，方法是对每个区域的每个通道进行最大激活。为什么是麦克斯？直观上，如果该图中的激活表示该特征存在的程度，那么我们通过取最大激活来评估该区域中该特征的存在是有意义的。这使得它对小的平移变化也是鲁棒的:如果图像移动很小的量，每个区域中的最大值可能会保持不变。</p><p id="fafc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">合并后，您可以执行另一个卷积，应用另一个非线性，然后执行另一个合并。一般来说，卷积网络的典型层包括三个阶段。</p><ol class=""><li id="364d" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">在第一阶段，该层并行执行若干卷积，以产生一组线性激活。</li><li id="3da8" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">在第二阶段，每个线性激活通过非线性激活函数运行，例如修正的线性激活函数。这个阶段有时被称为探测器阶段。</li><li id="62ce" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">在第三阶段，我们使用一个池函数来进一步修改层的输出</li></ol><p id="5ab0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">卷积和合并</strong></p><p id="7906" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回想一下先验概率分布的概念。这是一个模型参数的概率分布，在我们看到任何数据之前，它编码了我们对什么模型是合理的信念。先验可以被认为是弱的或强的，这取决于先验中概率密度的集中程度。弱先验是具有高熵的先验分布，例如具有高方差的高斯分布。这种先验允许数据或多或少地自由移动参数。强先验具有非常低的熵，例如具有低方差的高斯分布。这种先验在确定参数的最终位置时起着更积极的作用。一个无限强的先验将一些参数的概率设置为零，并表示这些参数值是完全禁止的，无论数据对这些值的支持程度如何。我们可以把卷积网络想象成类似于全连通网络，但其权重具有无限强的先验。这个无限强的先验表明，一个隐藏单元的权重必须与其相邻单元的权重相同，但在空间上有所偏移。先验还指出，权重必须为零，除非在分配给该隐藏单元的小的、空间连续的感受场中。总的来说，我们可以认为卷积的使用在层的参数上引入了无限强的先验概率分布。这个先验知识表明该层应该学习的函数只包含局部交互，并且与翻译是等变的。同样，池的使用是一个无限强的先验，每个单元对于小的平移应该是不变的。</p><p id="a9da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CNN看起来像什么？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lp"><img src="../Images/425130fa48e44521f52834d8aa0e655a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*F3_G28DEh20Cn-3wL58Q6Q.png"/></div></figure><p id="07bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，这是LeNet，一个用于手写数字识别的网络。</p><ul class=""><li id="e276" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">它接受32x32个手写字符作为输入</li><li id="0426" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">C1卷积层:28x28x6 (28而不是32，因为边缘(我们将在下面讨论)和6，因为它有6个特征)</li><li id="37ab" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">非线性</li><li id="4289" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">S2池(二次采样):2x2池，将28x28的地图转换为14x14的地图。</li><li id="8648" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">C3卷积层:10×10×16(10而不是14是因为边缘，16是因为它有16个特征)</li><li id="c423" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">非线性</li><li id="ffba" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">S4池(二次采样):2x2池，将10x10的地图变成5x5的地图。</li><li id="c656" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">5x5x16足够小，现在您可以将这些激活展平到一个大矢量中，并将其放入一个标准的全连接线性层中</li></ul><p id="b815" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">实现卷积层</strong></p><p id="ab2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将需要N维数组，通常作为“张量”的同义词使用。张量是矩阵的高维推广。</p><p id="cb19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，输入图像可以是3D阵列；过滤器可以是4D；激活可以是3D的。这类似于我们在标准神经网络中看到的权重和激活，其中我们有2D权重矩阵和1D激活。</p><p id="4961" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我有一个激活a(1)，我要把它变成z(2):</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lq"><img src="../Images/bc6a4777e60f6a6af893a936cce3d3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*qYAJSdHikxGm3Uz3aLN7aQ.png"/></div></figure><p id="fb91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的滤波器由W张量指定:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lr"><img src="../Images/f68e235a1bc3752ab200a73801744492.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*1kEOPoHV1859Ei9RD2b4ig.png"/></div></figure><p id="1fbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积运算将使用此过滤器，将其放置在输入地图的每个位置，并生成相应的地图。让我们把它写成z(2)中沿纵轴的位置I和沿横轴的位置j的等式:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ls"><img src="../Images/18f0732ff7aa945ae034cc22a01522c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*mh9qR8CBm3e-t6P0K1W1dA.png"/></div></figure><p id="b2d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将W(2)乘以输入激活图中的一个点；我们将通过以(I，j)为中心来获得该点，然后根据l和m向左或向右移动。这是一种说法，在卷积中，每个位置都有一个小的线性层，您可以滑动它。</p><p id="ba5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这样做之后，不要忘记应用你的非线性:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lt"><img src="../Images/931aa571c0302a229023217f4cd9c99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*NElH3gW4I_FFn-g8dRaQNg.png"/></div></div></figure><p id="1f4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">填充和边缘</strong></p><p id="09a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你的内核向左移动两步，离开了图像的末端，会怎么样呢？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ly"><img src="../Images/6118a4a621c7d852a143a8a080ad0b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*e_Nn-5qSX2mHLsQr_djE6w.png"/></div></figure><p id="2986" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你有两个选择:</p><p id="de66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 1切掉边缘</strong></p><p id="4698" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在某些指数变为负值或大于图像宽度时，不允许对过滤器进行评估</p><p id="ae8f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果输入为32x32x32，滤波器为5x5x6，则输出为28x28x6。</p><p id="a4d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以这么想。我们可以想象过滤器有一个半径，我们可以通过将它的高度减去一除以二来计算:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lz"><img src="../Images/8aec72f94f3ac07a25cc52a1a2435629.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*LOKjxx6-XPGzs-DhQoOPjg.png"/></div></figure><p id="dbee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们的例子中，半径因此是2。如果半径超出了图像的末端，那么它就是无效的——这就是为什么我们在每一侧(底部、顶部、左侧、右侧)切掉一些与半径相等的位置。32x32就这样变成了28x28。</p><p id="90d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，当你像这样切掉边缘时，我们的激活会随着每一层而收缩。有些人不喜欢这样，尤其是当这些激活图在你的网络末端变得非常小的时候。</p><p id="9e32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2补零</strong></p><p id="c055" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用零填充，您可以在角和边上评估过滤器:任何超出图像或激活图末端的点都将被替换为零:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ly"><img src="../Images/f60123657437520b5f07d47be307c392.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*HFHZ95m_r72ObL-3SY7oaw.png"/></div></figure><p id="da48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">记住先减去图像平均值是很重要的，因为你放入的这些零将低于原始图像中的任何激活。通常，你取平均像素强度，然后从每个位置的像素强度中减去它，这样图像就在中间了。</p><p id="0441" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">零填充的一个优点是它在某种意义上更简单:激活映射的大小保持不变，您不必担心事情变得更小。</p><p id="3b0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">交错卷积</strong></p><p id="3a2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所说，每一层的标准结构包括三个步骤:</p><p id="05cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1卷积<br/> 2激活功能<br/> 3汇集</p><p id="6bd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，第一步在计算上可能非常昂贵，因为它不需要很多参数，但你要重复矩阵乘法很多很多次。</p><p id="da21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一个让它便宜一点的想法:如果你跳过一些位置呢？您可以跳过一个称为步幅的量，而不是在输入图像的每个点评估过滤器。</p><p id="6994" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是，步长卷积不同于池化，因为在池化中，你确实要计算每个位置的激活，然后取最大值，而在步长卷积中，你要一起跳过一些位置。</p><p id="3282" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">卷积神经网络的例子</strong></p><p id="a56b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> AlexNet </strong>是一个经典的中等深度卷积神经网络:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ma"><img src="../Images/18693e597d0a570a390052b47d41bdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*cO4ekyk2CBqy-D0eyAhDCQ.png"/></div></figure><p id="ae81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">之所以以这种方式绘制，是因为网络是在两个GPU上运行的，并且被手动分成两部分。这对网络的功能没有任何影响。</p><ul class=""><li id="f970" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">输入是224x224x3的图像</li><li id="059a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">第一个卷积层是11x11x96</li><li id="1c6d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">这些过滤器的应用步长为4。你将把图像的大小减少大约四分之一，减去由于边上的填充而产生的边缘效果；这为我们提供了一个55x55x96的激活图</li><li id="bba3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">ReLU激活</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mb"><img src="../Images/3d3029aa13713bb850ccd03aa22dd4bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*ri-TfxWVc0p6M6jQk_8a2g.png"/></div></figure><ul class=""><li id="951d" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">3x3最大池层，步幅为2。您将减少到27x27x96</li><li id="7717" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">标准化层。这已经不再广泛使用了。</li></ul><p id="7701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">突击测验:第一个卷积层有多少个参数？参数的数量仅取决于滤波器，滤波器为11x11x96。</p><p id="5841" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，每个卷积层中W矩阵的大小为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lr"><img src="../Images/f68e235a1bc3752ab200a73801744492.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*1kEOPoHV1859Ei9RD2b4ig.png"/></div></figure><p id="594c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以你最后得到11x11x3x96，总计34.848个参数。我们遗漏了偏置向量中的参数数量，它只取决于输出的数量(96)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mc"><img src="../Images/64f61ca27218be7c2f7e3d0ae1b67727.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*c-_mqrUpllhwdm98-z1J6g.png"/></div></figure><p id="08a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些是相当多的参数，但它仍然比我们拥有一个完全连接的层所需的参数低得多。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es md"><img src="../Images/a42108c2946c0862e2f0f2e647e97c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*0jMwEN2lgQlMDTocOvS0kQ.jpeg"/></div></figure><p id="93ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们有完全连接的层:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es me"><img src="../Images/708b83a5cf598f9ad84590ab660ad87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*0gErWPV_cLpxBCYR2rGL8w.png"/></div></figure><p id="e5d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以看到一些模式:</p><ul class=""><li id="fcf9" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">池层减少了激活的规模</li><li id="9c5b" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">随着你越深入，特征的数量往往会增加:我们试图用更深层次的低分辨率激活图来总结图像中的所有信息，这有助于我们表现更抽象的特征。</li><li id="971b" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">一旦事情变得足够小，我们就将它们展平，并通过几个完全连接的层来产生一个答案</li></ul><p id="a85e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些图案很有价值。但是AlexNet已经不再被使用了，现在处理视觉输入的最好的网络也变得更加深入了。</p><p id="f3bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> VGG </strong>的建造试图追求标准化和模块化，当我们必须建立非常深的网络时，这真的很有帮助。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mf"><img src="../Images/051b51ef21e4fcd268f1c900b11459a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*kAIQpMA3xBSfQ5U6a4MG1Q.png"/></div></figure><ul class=""><li id="2ba9" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">输入是一个224x224x3的图像，我们用步长1和零填充卷积来处理它:分辨率没有降低。这些是巨大的激活图，但是过滤器非常小(3x3x64)。</li><li id="38c1" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">非线性</li><li id="df2b" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">2x2最大池层，以降低分辨率。</li><li id="acb9" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">两层卷积</li><li id="7d55" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">非线性</li><li id="e538" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">2x2最大池层</li><li id="2da4" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">三层卷积</li><li id="8965" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">2x2最大池层</li><li id="b2f1" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">三层卷积</li></ul><p id="c310" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi">…</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mg"><img src="../Images/a89c0084d6168c62e0a51b3ebe9b2804.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*FZhUZIU7pIRJmXskS9CG1g.png"/></div></figure><p id="2bb4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在这里看到图案:我们有两到三层完全不会降低分辨率的回旋层，中间穿插着将分辨率降低一半的汇集层。每次我们将分辨率降低一半，我们通常会增加过滤器的数量，在某些时候，它足够小，我们可以构建我们的第一个完全连接的层。</p><p id="efcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一些观察结果:</p><ul class=""><li id="5465" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">更多的层意味着更多的处理，这就是为什么我们看到这些重复的块</li><li id="72a8" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">大部分内存实际上是在开始时使用的，因为在开始时我们有这些巨大的激活图</li><li id="18fe" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">参数的数量在完全连接的层中是最大的，其中我们将激活展平成一个巨大的向量</li></ul><p id="bc68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ResNet最初有152层，但从那时起，人们就用类似的想法来扩展网络。</p><p id="7cce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我将展示一个高度简化的18层原型的图表:完整的152层版本与此类似，只是所有内容都被拉伸了很多。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mh"><img src="../Images/33adede1b2acd2afb202f44d7c605bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*R4UCKPpv__xjFcCcs1PlGg.png"/></div></figure><ul class=""><li id="6ec7" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">我们从一个7x7卷积开始，这样可以降低分辨率。然后，我们有这些重复的3x3卷积块，它们之间的分辨率降低；随着时间的推移，滤波器的数量会增加两倍，直到达到512，然后保持不变。</li><li id="3142" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">我们有许多层的通用块，中间穿插一些池操作。</li><li id="910d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">该模型的另一个显著特征是，我们不是在末端有一个巨大的全连接层，而是简单地取上一个卷积响应图，并对每个位置的特征向量进行平均</li><li id="86a3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后我们有一个线性层直接进入softmax</li></ul><p id="8d0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们来看看为什么ResNet可以用这么大的层数来训练。ResNet论文的作者进行了一些实验，研究增加层数的影响。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mi"><img src="../Images/8723945adb718debb9e9a528c5e9a80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmPmC7xe8H0RO9o5WxyMAw.jpeg"/></div></div></figure><p id="882a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在左边，你可以看到他们对一个标准卷积网络的实验，你可以把层数从20层增加到32层，从44层增加到56层。你可以看到，错误率随着层数的增加而增加。ResNet的人发现，通过对神经网络架构的修改，你可以逆转这一趋势，并看到准确性随着层数的增加而增加。</p><p id="2aae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">主旨是什么？</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mj"><img src="../Images/89cf82f76d6a1f382d1245d1c0bd34ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGhizyp2O4sE_Rm8OpYUfA.png"/></div></div></figure><p id="13fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在左边的规则网络中，你有你的权重层，它可以是卷积或完全连接的，你可以串联应用它们。另一方面，对于剩余网络，你取每组两个卷积，把第一个卷积的输入加到第二个卷积的输出上。</p><p id="e845" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，如果在规则网络中，你考虑每组两个卷积计算某个函数h(x)，在剩余网络中，该组两个卷积将计算h(x)，其中h(x)等于规则网络中发生的情况加上X。X绕过这些卷积，并在最后被相加。直觉告诉我们，你计算的是x的变化，而不是一个全新的x。</p><p id="1cb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了理解这种直觉，我们必须重新考虑为什么训练非常深的网络如此困难。链式法则看起来就像一堆雅可比的乘法运算；对于任意深度的网络，损失对第一层权矩阵的导数将是大量雅可比矩阵的乘积。然后最后你乘以这个数dL / dz(n):</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mk"><img src="../Images/449ddc4a75181fcd29c45c4529c74bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*qzvIu05tmvO5jzkIMhuD2w.png"/></div></figure><p id="f9ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些雅可比矩阵可以是各种不同的东西。非线性的导数，线性层的导数，卷积的导数。如果我们简化一下，考虑一下标量，将许多数字相乘的问题是有两种可能的结果:如果大多数数字小于1，我们将得到0，而如果大多数数字大于1，我们将得到无穷大。有一小部分情况下答案更有趣，这一小部分情况是当所有的数字都接近1时:只有当它们都接近1时，你才能从许多标量相乘中得到合理的答案。</p><p id="5e06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对了，这就是为什么我们更喜欢ReLUs而不是sigmoids整流线性单位的导数是输入是否为正的指标，这意味着这些导数的大部分仅为1。</p><p id="29d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于矩阵来说，接近1并不意味着每一项都是1，而是意味着矩阵的特征值需要接近1。</p><p id="cd98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们回头看看我们的图像，残差层dH/dx由dF/dx加上恒等式给出，因为您只是添加了x，只要卷积层中的权重不太大，您可能希望这个dF/dx也不会太大。在这一点上，这个总和将非常接近于恒等式:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ml"><img src="../Images/e058378b6e28feeba73ea3bbeec0df05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*IHr1n-QZN4zYMRi6mUtaAw.png"/></div></div></figure><h2 id="5d17" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><a class="ae jc" href="https://www.youtube.com/watch?v=0dNAhN4ypFc&amp;list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&amp;index=20" rel="noopener ugc nofollow" target="_blank">让神经网络训练</a></h2><p id="da6f" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">请记住，神经网络优化景观是有问题的，神经网络是混乱的。他们需要很多技巧来训练好，知道这些技巧对于好的结果非常重要，就像理解所有理论上的细微差别一样重要。我们将讨论:</p><ul class=""><li id="f133" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">标准化输入和输出</li><li id="c31d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">标准化激活(批量标准化)</li><li id="388a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">初始化权重矩阵和偏差向量</li><li id="9338" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">渐变剪辑</li><li id="f974" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">超参数优化的最佳实践</li><li id="f8e9" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">辍学学生</li></ul><p id="a62d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">标准化输入和输出</strong></p><p id="4444" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设你有这样一个神经网络:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mm"><img src="../Images/ad3fa1218d853f8a086554ef48b2da47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*3D2-9hAPzIa6Phjc-aJkQw.png"/></div></figure><p id="7c3a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它接受2D输入，我们的一些数据点如下所示:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mn"><img src="../Images/6eabd06e48e36acd16559c62e225f5c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*szWCR_CCc1vqnyv3yfKFKQ.png"/></div></figure><p id="a58a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，一切看起来都很正常。X1和x2各不相同，这是一个标准设置，你会期望一切正常。但是如果我们看看这个场景:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mo"><img src="../Images/2629287a0f520dd404476f34f3647498.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*XC-ZHqWI6O2iTh3PRqj-7Q.png"/></div></figure><p id="2c50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一个维度与第二个维度的尺度非常不同。它的数值更大，范围更大。这些数据点代表的实际信息可能完全相同，如果您重新调整右侧数据点的条形，您可能会得到与左侧分布完全相同的结果。由此产生的梯度条件较差，网络将倾向于在传递到较小的数字之前关注较大的数字，即使它们对于预测问题不一定更重要。</p><p id="4c5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">标准化是我们解决这个问题的方法:我们转换我们的输入，使它们趋向于均值为零，标准差为一。如果你想让平均值为零，你要做的就是从x的每个维度上减去平均值:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mp"><img src="../Images/1af8f4f9e6b2299f9a801e452c80d6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*tIqnZlJheY3umvEsJoOUiQ.png"/></div></div></figure><p id="09c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x的期望值是平均值，通过对数据集中的所有x取平均值来估算。</p><p id="5aa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你也想让标准差为1，那么你做同样的事情，然后除以数据集中x的标准差:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mq"><img src="../Images/dfbb8bae5b72930efdf55d42ebc48d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*nNF4FilQqDZBt6m80G6dpA.png"/></div></figure><p id="2a46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你正在解决一个回归问题，对于输出来说这样做也是一个好主意。</p><p id="3731" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">标准化激活(批量标准化)</strong></p><p id="a950" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使激活标准化，我们将使用小批量来估计平均值和标准差。不是通过对数据集中每个数据点的激活进行平均来估计平均值，这将是非常昂贵的，我们将通过对该批中所有数据点的激活进行平均来估计平均值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mp"><img src="../Images/3230e712a5212650d6905ad96c2d5aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*7sl8L3cfvwXDA0rTcpMdkQ.png"/></div></figure><p id="343e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们对标准差做同样的事情。我们不是计算整个数据集的标准偏差，而是只计算批次的标准偏差。在实际实现中，我们还会有两个额外的激活参数(γ和β)。在我们减去平均值并除以标准差后，我们的激活将在一个特定的范围内，但我们可能不希望它进入下一层。相反，我们可能想对它们进行变换，将它们乘以一个标度，并加上一个偏差。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mr"><img src="../Images/04a8dfb07e157a6d13e960ef5c04092b.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*l-Y8nm3bLKSu4T1lKLIlLg.png"/></div></figure><p id="ed5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以将批处理规范化视为一个层，它根据当前批处理中进入批处理规范层的所有激活计算平均值和标准差，然后具有参数gamma和beta。</p><p id="77f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一些实用的细节:</p><ul class=""><li id="f3c2" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">如果你使用批量范数，你通常可以使用更大的学习率，因为它会使你的导数更好地进行调节，并在某种程度上防止这些真正笨拙地缩放的客观景观，其中一些维度比其他维度具有更大的梯度。</li><li id="a3a2" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">具有批量范数的模型可以训练得更快，并且通常需要较少的正则化</li></ul><p id="beff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">初始化权重矩阵和偏置向量</strong></p><p id="5041" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们不会尝试初始化我们的权重，以便它们接近一个好的解决方案，我们只是要初始化它们，以便在初始化时，神经网络的导数是高质量的，这意味着它们指向一个最优的方向。</p><p id="f1ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于小型网络，一个非常简单的选择是将权重初始化为一些小的随机数；例如，我们可以选择平均值为0、标准差为0.0001的正态分布。但随着你的网络越来越深，这将很快给你不好的答案，因为如果激活为零，那么梯度也为零。如果梯度为零，那么你没有任何进展:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ms"><img src="../Images/df960ccfa80efcbb387bbfafa32bc7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*knVBMJZMPf6J-eV99v89Uw.png"/></div></figure><p id="ba7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，这相当于一个非常糟糕的平台期。</p><p id="2bd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了避免这个问题，我们使用Xavier初始化。Xavier初始化的目标是初始化权重，使得激活的方差在每一层上都相同。这种恒定的变化有助于防止渐变爆炸或消失。我不会解释数学证明的所有细节，因为这不是本文的目的。下面是粗略的描述。</p><p id="3f71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们将在某个线性层初始化我们的权重，以便根据均值为0且方差为σw平方的高斯分布对我们的权重进行采样</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mt"><img src="../Images/d35af7d81206a555439b581dc6aa4d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*dygG4lI2DpzBBIHDeh2l_A.png"/></div></figure><p id="9503" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们应该把方差设为多少？</p><ul class=""><li id="ce41" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">偏差将被初始化为零，这是z中每个条目的公式</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mu"><img src="../Images/3f1774ada95bae98b092eba6283ed06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*kRKpAVkjofdK3N4ZxHsy8A.png"/></div></figure><ul class=""><li id="d0f3" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">我们标准化x，因此激活是高斯的。</li></ul><p id="c410" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些z的大小是多少？如果平均值都是零，那么z的大小就是它们的标准差。如果我们写出z的方差，因为z是零均值，这就是z平方的期望值:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mv"><img src="../Images/c4b2d31d8241bdb5d0bbb96cd10a606d.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*83c52HFZS_8iVGB0u_2m9Q.png"/></div></figure><p id="6dd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你得到平方和的期望值，并得到平方和中的每一项。这简化为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mw"><img src="../Images/1e57dbb37f71a58cfd55f45fdd21529d.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*Xx5XcdZTdOKvMlso9ys-Ow.png"/></div></figure><p id="5e62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">的维数乘以σw的平方乘以σa的平方。</p><p id="1d4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们走上这条道路是因为我们担心我们对权重初始化的选择会增加或减少激活的幅度。如果前一层激活的差异为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mx"><img src="../Images/e98b35441c4e9e39e25f6ef146bf9ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*TZ8FoHWtD1wztMQXu4qHuQ.png"/></div></figure><p id="7c79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下一层有大量的激活:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mw"><img src="../Images/1e57dbb37f71a58cfd55f45fdd21529d.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*Xx5XcdZTdOKvMlso9ys-Ow.png"/></div></figure><p id="1de1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们知道:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es my"><img src="../Images/1e5128e3d722386636f59375da815fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*s31JpYUfIk-oZAhhnXT82g.png"/></div></figure><p id="d899" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们希望激活的幅度在一层又一层之间保持不变，我们需要以某种方式确保Da sigma w的平方大约为1。我们可以选择σw的平方为1/Da:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mz"><img src="../Images/58fd3fa68ba35e593844144c2f423222.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*xGgezfzghWdJ4DwURbkpTQ.png"/></div></figure><p id="3304" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">基本原理是，我们需要标准偏差为1除以输入向量维数的平方根。</strong></p><p id="c792" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们忽略了一个细节:我们忘记了非线性。在我们应用这个线性层之后，我们将对z应用一些非线性函数，这个非线性函数将改变它们的大小。一种非常常用的非线性函数是整流线性单元。更深层次的网络几乎总是使用ReLUs，因为它们比sigmoids表现得更好。然而，问题是ReLU会将我们的许多激活清零；事实上，如果我们的激活是正态分布的，并且平均值为零，那么ReLU会将其中的一半激活归零。</p><p id="8d82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们一半的激活将被删除，这确实会减少我们的差异。为了避免这种情况，我们将方差增大两倍，或者等效地将标准差平方根增大两倍:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es na"><img src="../Images/a5244eeadc367c407606d9548ca111a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/1*i_dJUruMGfk0aM251GdgYw.png"/></div></figure><p id="0971" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个更小的细节:偏见。之前，我们说过我们会把所有的偏置向量初始化为零。但是再一次，ReLU会杀了他们中的一半。通常，尤其是当人们不使用半因子时，将偏差初始化为一些小的正常数是很常见的，比如0.1，因为死单位。</p><p id="a619" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">高级初始化</strong></p><p id="718d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我所描述的基本初始化对于大多数问题都很有效。然而，我们现在将讨论一种更高级的初始化形式，这既是因为有时它工作得很好，也是因为它有助于理解网络初始化发生了什么。</p><p id="aa4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">损失相对于某一层的权重的导数由许多许多矩阵的乘积给出，特别是该层和损失之间的所有雅可比矩阵。不用担心这些矩阵是什么，我们可以把它写成一个大产品。如果你把许多东西相乘，而所有的东西都小于1，那么乘积大约为零；另一方面，如果它大于1，它将是无穷大。只有当所有这些雅各比都接近一个时，你才能得到合理的答案:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nb"><img src="../Images/850b487b21c209f55894ab63a96f244d.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*FVXuIxaPfIV1fgV7rGGZFg.png"/></div></figure><p id="6607" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于一个接近1的矩阵，它的特征值必须接近1。</p><p id="3fc9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以把任何一个雅可比矩阵写成三个矩阵的乘积(例如，用奇异值分解):</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nc"><img src="../Images/3abf5b5850c610894d2b9ebc88e1f249.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*7kTXjVLWpN6H8KwqrAWkoA.png"/></div></figure><ul class=""><li id="748d" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">u和V是比例保持变换；它们是正交基。这意味着当你把一个向量乘以矩阵U或V时，它不会改变向量的长度，它只会以不同的方式旋转向量。</li><li id="c0f1" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">λ是对角的，λ的对角元素是雅可比矩阵的特征值。粗略地说，lambda捕捉到了这个矩阵的所有缩放，U和V捕捉到了所有旋转</li></ul><p id="35ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们可以强迫lambda在每个维度上都有一个尺度，那就足以确保这个雅可比的大乘积不会产生巨大或微小的数字。请记住，线性层的雅可比矩阵只是其权重矩阵的转置矩阵，因此，如果我们希望这些雅可比矩阵的特征值约为1，我们必须获取权重矩阵，并对其执行类似奇异值分解的操作，然后以某种方式强制λ成为单位矩阵:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nd"><img src="../Images/f443b26efccb23fc8263293736cf20c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UasbzhpVaeIvjdvbFii_g.png"/></div></div></figure><p id="df99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以进行一些任意的初始化，对其进行奇异值分解，强制对角线一致，然后将它放回一起。强迫对角线相等，意味着放弃它，把最终的矩阵构造成U乘以v。</p><p id="42e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">渐变裁剪</strong></p><p id="bef0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有时，可能会发生这样的情况，你选择最陡下降的方向，一切都很好，你认为你正在取得良好的进展，然后你得到一些怪物梯度，完全打乱了你的网络:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ne"><img src="../Images/778991b7a83e95390e5b43348aecd7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*gvjMXEUcPnUjoH0dWkqXDw.png"/></div></figure><p id="c195" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">渐变剪辑通常是修补这个问题的一种合理的权宜之计。有两种方法可以裁剪渐变。</p><ul class=""><li id="6c78" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">一种方法是你可以裁剪渐变中的每一个条目，使它不大于某个常数c，也不小于负c。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nf"><img src="../Images/b83e284a623aa49b2dd671f0e6630075.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*lb4cucppFcz0yR0scBovEA.png"/></div></figure><ul class=""><li id="a0cb" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">你可以做的另一件事是，你可以剪切梯度的范数:你保持方向，只剪切长度。</li></ul><p id="7e2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要手动选择c，您可以运行几个时期的训练，看看健康的梯度幅度是什么样的。这是万不得已的措施。</p><p id="9336" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">合群和辍学</strong></p><p id="697c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将从讨论如何让神经网络更好地优化，转向讨论如何让它们更好地推广。</p><p id="112c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们在进行机器学习时，我们担心我们的模型是否会犯错误，以及大概如何让它少犯错误。神经网络将会有很多参数，因此会有很高的方差。这里有一个有趣的想法:如果我们有多个高方差学习者，我们训练许多神经网络而不是一个，也许这些多个神经网络都会同意正确的答案，但他们会不同意错误的答案。</p><p id="4c78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是另一种说法，即错误的方式比正确的方式多得多。这些网络有望独立出错，这意味着当一个模型出错时，另一个也可能出错，但方式不同。这是合奏背后的基本直觉。</p><p id="2131" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们正在解决一个回归问题，假设这条绿色曲线代表真实函数，而蓝色点是我的训练集:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ng"><img src="../Images/6b65225276f72fc352a6574e5616763f.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*zPUHqHKpXJdOK3LLxbKASw.png"/></div></figure><p id="857b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我训练了一个大型神经网络模型，它的解用这条橙线表示，这条橙线在我的训练数据附近很好，但在远离训练数据的地方就很差。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nh"><img src="../Images/77697920543c4f6500daf47f7669cc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*TGRe4piCS6vKL-oAWSSEzA.png"/></div></figure><p id="5b18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我可以用完全不同的初始化训练另一个模型，得到另一个解。这两个模型在训练集上都很好，在测试集中都很差。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ni"><img src="../Images/3752aaf9e0ce39afc173011ac5d85602.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*attmCzaL1We-PARe1VKOIQ.png"/></div></figure><p id="3732" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我再训练一个。如果我看看这三个模型的平均值，可能会相当不错。他们都会不同意，但总的来说，他们的错误都集中在正确的答案上。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mz"><img src="../Images/dcb75e55e8b8554d6648d836fd795bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*icxq-l7w6GJmbH9ZQFwWXg.png"/></div></figure><p id="71af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，这张图有点理想化，但这是基本的直觉。</p><p id="fbdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我接下来要描述的是构建自举系综的理论方法。方差被量化为我们从对某个数据集D的训练中获得的模型与来自该训练分布的所有可能数据集的所有模型的平均值之间的差值的期望值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ng"><img src="../Images/f3fb573aced0103e11fc6eab0db7654c.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*qdiYC2QODKUU_8Ttrb-Xfw.png"/></div></div></figure><p id="9cb2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果方差很大，你会认为你的误差很大。但是如果我们可以估计f bar，那么我们应该可以消除很多方差。我们能不能以某种方式得到m个不同的训练，每组训练一个不同的模型，然后把这些模型平均起来？我们预计，如果所有这些不同的模型都不同意，并在测试点上犯不同的错误，那么他们的平均值将被迫同意接近正确的答案。</p><p id="142e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是我们从哪里得到这些数据集呢？我们能不能从一个数据集中拼凑出多个独立的数据集，而不必处理更少的数据？</p><ul class=""><li id="9fdd" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">一个简单而天真的方法是将一个大数据集分割成m个不重叠的部分，但这是一种浪费，因为这样你就可以在更少量的数据上训练每个模型</li><li id="5451" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">也许我们不需要这m个数据集是不重叠的，我们可以用某种方式从一个大型数据集构建独立的数据集，这些独立的日期与原始日期的大小相同</li></ul><p id="7720" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是可能的，这被称为替换重采样。我们将对原始数据集中的数据点进行重新采样。假设我们的原始数据集有三个点(当然，实际上我们会有更多)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mt"><img src="../Images/10bf62d599626798a60a5274f3f14a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*xTDMZNey3hosytMvIMBEVw.png"/></div></div></figure><p id="ca89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们正在构建第一个重采样数据集；我们掷出一个三面骰子，得到这个:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lq"><img src="../Images/d493409761a3c37f438695487d8c488f.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*H5wzOj-ry85SBjMwufsx9w.png"/></div></figure><p id="e776" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">话说回来:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nj"><img src="../Images/aa8f7fb1e781cf0477e64fb641344dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*HfCHvj8dWVTr_FlsWBH4wA.png"/></div></figure><p id="76ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们从原始训练集中对新数据点i.i.d .进行采样，它们也将是原始训练分布中的i.i.d .样本。因为它们是独立采样的，所以它们最终是来自同一个原始分布的独立样本。这些数据集最终会变得独立。</p><p id="0400" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些模型之间的区别在于它们有不同的参数向量，因为它们是在不同但密切相关的数据集上独立训练的。一旦我们有了这些不同的模型，那么在对新的测试点进行分类时，我们必须以某种方式将它们的预测结合起来。</p><ul class=""><li id="35f7" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">原则上的方法是将它们的概率平均在一起，所以你向前运行这些模型，从它们的软最大值中得到概率，然后你将这些概率平均在一起得到集合的预测</li><li id="734d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">一个稍微简单但仍然有效的方法是简单地进行多数表决。我们也可以让他们预测他们认为正确的答案是什么，然后输出对应于网络多数票的答案，而不是看这些网络的实际概率</li></ul><p id="ec04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在实践中，当我们深入学习时，我们很少使用这种带替换的重采样方法，因为有一种更简单的方法非常常用，而且似乎也同样有效。使用替换进行重采样的目的是为了在不同的模型之间获得足够的可变性，所以直观地说，我们不希望所有的模型最终都得到相同的错误解。使用替换对数据进行重采样是一种区分数据的方法，但是我们通常用于训练深层网络的方法已经具有很大的随机性(不同的随机初始化、随机小批量洗牌、随机梯度下降)。由于所有这些随机性的来源，没有两次训练运行，即使是对于相同的精确模型和相同的精确数据，看起来也会是相同的。在实践中，我们甚至不用生成这些单独的数据集，也能从集成中获得同样的好处。如果我们只采用m种不同的模型，并在完全相同的训练集上对它们进行训练，我们就会得到足够不同的模型。</p><p id="56ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从深层网络中构建集成的实际方法是，在同一训练集上简单地训练m个不同的模型，然后对它们的概率取平均值，或者进行多数表决:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nk"><img src="../Images/ab9585a13f1fe8342bcdd91e83990220.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*JOjdEZla3Ts33ETY3E1UxA.png"/></div></figure><p id="b69a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">更快的合奏</strong></p><p id="760a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些网络可能非常大，培训成本也很高。但是请记住，深度学习是关于表示的，最后一层通常是特定于任务的，并且试图理解表示。这意味着我们可以尝试只集成最后几层。我们可以训练多个不同的网络，但让它们共享执行特征提取的主干，然后为每个头部设置一组不同的完全连接的层。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nl"><img src="../Images/95b81e6fdc7b7988bafd73a0268da0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*ecmNas8YhPubNmao-WsAFA.png"/></div></figure><p id="4970" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这并不像真正的组装那样好，但是训练起来要快得多，设置起来也更容易</p><p id="4867" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通常，合奏团越大效果越好，但是制作大型合奏团确实很昂贵，尤其是如果你在训练独立的模特。这种计算很容易并行化，但它仍然会让您付出代价。那么，我们能不能只用一个网络组成一个庞大的团队？</p><p id="1970" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是所谓的辍学技术试图做的。脱落背后的想法是，你要利用你的神经网络，你要随机杀死一些激活——你会进入每一个隐藏层，并以一定的概率，你会删除一些单位设置他们为零。</p><p id="03b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这和合奏有什么关系？你可以把这个杀死一些激活的过程想象成在旧网络之外构建一个不同的网络；每组激活对应于一个不同的网络，因此通过选择不同的随机激活组来删除，您正在构建新的网络。但是这些网络没有单独的权重，它们实际上共享所有的权重，它们只是具有不同的架构。</p><p id="d382" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们实现的方式是:在每次向前传递期间，每次我们到达某一层时，我们将通过删除一半来计算该层的激活:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nm"><img src="../Images/9182d9bc59315bc7c9351ddb98b3b7b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*uLpYZxUwDIBoUL5IIrYKbA.png"/></div></figure><p id="001f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">向后传球的工作方式完全相同。梯度是与a的外积，因此如果激活被设置为零，则其导数也将被设置为零。实际上，当您这样做时，就从网络中删除了该激活。</p><p id="0d9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">dropout工作原理背后的直觉是，它迫使神经网络建立冗余的表示，并处理每个特征都有可能被删除的事实——因此它不能仅基于单个特征进行预测。它迫使网络变得健壮和更加可靠。当你以这种方式训练时，你会得到一些看起来非常非常庞大的集合，因为每一个特征删除的组合都可以被认为是一个不同的模型。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nn"><img src="../Images/f06ce9e7880d609ec9731af812376706.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*NbQAoNhG2vcOiC3NMhijRA.png"/></div></figure><p id="83ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">测试时会发生什么？</p><p id="6076" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练时，一半的维度被强制为0。在测试时，我们不使用任何丢弃，所以权重和激活之间的乘积是两倍大。因此，我们将砝码除以2，以进行补偿，并使其回到原始刻度。</p><p id="f259" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">超参数</strong></p><p id="8041" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了我们讨论过的所有这些技巧，我们最终会得到很多超参数。其中一些会影响优化:</p><ul class=""><li id="7375" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">学习率</li><li id="7210" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">动力</li><li id="a072" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">初始化</li><li id="3b5c" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">批量标准化</li></ul><p id="32cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中一些影响概括:</p><ul class=""><li id="a08e" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">组装</li><li id="f09e" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">拒绝传统社会的人</li><li id="0322" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">体系结构</li></ul><p id="8163" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们如何选择所有这些参数？要做的一件重要的事情是识别哪种类型的参数应该影响什么。</p><p id="9303" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，坏的优化超参数会在训练过程的早期出现。因此，我们可以调整它们，开始训练网络，观察训练误差，看看它在最初几个时期是如何演变的。其他超参数会影响验证误差。当调整超参数时，从非常广泛的混合参数开始，只训练一小会儿，然后看看基础训练进展如何，这通常是一个好主意。然后你再次调整你的超参数，缩小它们的范围，然后重复。</p><p id="0c7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在网格搜索中，参数被详尽地定义和搜索。在随机搜索中，不会尝试所有指定的参数。与穷举搜索相比，这种方法更有优势，因为您可以选择该搜索所需的最大尝试次数。</p><h2 id="8c52" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><a class="ae jc" href="https://www.youtube.com/watch?v=PyZvbaC5oQY&amp;list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&amp;index=30" rel="noopener ugc nofollow" target="_blank">递归神经网络</a></h2><p id="21e8" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><strong class="ig hi">如果我们有一个可变大小的输入会怎么样？</strong></p><p id="8b5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">序列输入可以是可变长度的。例如，它们可能是英语句子(第一个序列有四个元素，第二个序列有三个元素，第三个序列有五个元素):</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nb"><img src="../Images/712ffa794f513c242391430233e3f4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*-tP6xKHNeK2JL2KfxmmcDw.png"/></div></figure><p id="37ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这可能包括:</p><ul class=""><li id="dd99" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">语序→情感分类问题:你网上有一篇评论，你想猜猜这是正面评论还是负面评论。</li><li id="3e7e" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">声音序列→从声音中识别音素的问题</li><li id="4e5d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">图像序列→视频中的活动分类问题</li></ul><p id="1f70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您需要一个能够:</p><ul class=""><li id="ef46" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">适应多个输入和多个不同数量的输入</li><li id="c4fe" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">共享功能</li></ul><p id="3c61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">一个想法</strong></p><p id="a015" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">层数变得与输入数相同。换句话说，每一层都有单独的输入:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es no"><img src="../Images/69baa86e8e34d2b92c999ccbcd135be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MsJmtun9KfDFDNAujVZuRQ.png"/></div></div></figure><p id="1b0e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你有一个长度为4的序列，那么你就有四层。如果你有一个长度为3的序列，那么你会有三层。如果你有一个长度为5的序列，那么你将有五层。</p><p id="b74d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每一层，我们现在有两个输入:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lc"><img src="../Images/8159cf1b91f72305168c503fd0a5c92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*zNVNpqfGqw-obwsx5cGwsg.png"/></div></figure><ul class=""><li id="d65c" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">前一层的激活</li><li id="db29" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">新的输入x</li></ul><p id="66e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们对此应用常规线性运算:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es np"><img src="../Images/0538219616dec4d23a30caf684832b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*TZDk_SymQyDfgrDa-Qt1MA.png"/></div></figure><p id="9bc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像往常一样，我们应用非线性:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nq"><img src="../Images/68fc1b6bf48312472e99368077c6f165.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*A_cH3UJ905kxAGGLYuHnFw.png"/></div></figure><p id="191a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这非常像标准的线性层；唯一的区别是，现在您将新输入连接到先前的层激活，然后通过线性和非线性操作传递它们，而不是将线性层应用到先前的层激活。</p><p id="d4f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">缺失的层会发生什么？如果你有一个长度为5的序列，那么你会有五层，而如果你有一个长度为3的序列，那么你会有三层。</p><p id="1e6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一个有趣的技巧:如果我们有一个较短的序列，我们就假设在第一次输入之前的层(该层不存在)的激活都是零:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nr"><img src="../Images/25814ed08ac850a312fc94677d8923e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*g8iGGEeGKwBE79_my6_KZw.png"/></div></figure><p id="e348" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，不管序列有多长，我们都会这样做；即使对于最长的序列，我们仍然有一个虚拟的前一层，其中激活都是零。执行我们的常规操作时:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ns"><img src="../Images/93905f6b9d08fd5970015470af9b13ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*fxM3gMYsgJtj4zODtgHFyg.png"/></div></figure><p id="263d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第一层，我们将把输入连接成一个大的零向量</p><p id="4021" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种设计的一个大问题是我们在每一层仍然有不同的权重矩阵。真正长的序列将最终需要许多这样的权重矩阵。然而，最后几层将被训练用于所有序列，而仅用于最长序列的第一层可能很少被训练。</p><p id="f4b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了将这种设计变成成熟的递归神经网络，我们需要做的一个额外的修改是共享权重矩阵。</p><p id="7a61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这意味着所有这些层的W(l)都是相同的</p><ul class=""><li id="59df" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">你如何评价这个网络的一切都不会改变</li><li id="b357" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">在测试时，一切都是完全一样的。我们只需将虚拟的第一层初始化为零，然后继续这些操作:</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/5b58cacf7a1bede13ffb5a38c7482f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*HWFgBZXBMKOGWvDD9Ws5Zg.png"/></div></figure><ul class=""><li id="f699" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">这只影响训练。在训练期间，我们必须强制矩阵W(l)对于所有层都是相同的:</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nt"><img src="../Images/fe17e035cff9df5960b071c88d724c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*yX3j8oLPQR4auVo-Par6eA.png"/></div></figure><p id="563d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">非常长的序列可能需要非常早期层的权重(我们只在非常长的序列中看到)，但现在这些权重与后面层中使用的权重相同，并且它们已经被训练得相当好。</p><p id="cfc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们强迫权重矩阵在每一层都完全相同，我们就可以有任意多的层</p><p id="13b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RNN的基本设计:</p><ul class=""><li id="9d26" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">深度等于序列长度的非常深的网络</li><li id="8a89" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">每一层都获得不同的输入:它将前一层的激活与该时间步的输入连接起来，通过线性层，然后通过非线性层</li><li id="0538" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">第一层从虚拟的前一层获得零作为输入</li><li id="bb0e" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">权重矩阵在所有层共享，这意味着我们不需要数量等于最长序列长度的权重矩阵</li></ul><p id="0f74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们如何培养rnn？</strong></p><p id="a719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简而言之，我们要稍微修改一下反向传播。</p><p id="1292" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们运行反向传播时，基本设计是相同的。首先，我们向前运行一遍来计算每一步的所有a和z。然后，对于向后传球，我们以同样的方式进行。我们将delta初始化为最终损耗相对于最后一层输出的导数，然后对于每个线性层和每个ReLU，我们计算相对于其参数的导数和相对于其输入(即前一层)的导数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nu"><img src="../Images/7e7a2ca491ae11c79cb07456bd96ab1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*_fyDZZ91U7RAWw8TkbtavA.jpeg"/></div></figure><p id="edbd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题是现在所有这些层的参数都是相同的。这意味着第4层的θf与第3层的θf是相同的变量。如果我们要在下一层运行这个算法:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nv"><img src="../Images/5b9d4d2349a1bbee53488c73c42cce42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*kGEkfPiPbvEBSiF4HPQwqA.jpeg"/></div></figure><p id="3545" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从右边开始，我们将计算δ，即损耗相对于第四层输出的导数，然后在第四层，我们将计算损耗相对于权重和偏差的导数。然后，我们将计算新的增量，我们将回到第3层，在第3层，我们将用第3层w和b的梯度覆盖梯度。因此，从字面上看，层l-1的梯度将覆盖层l的梯度-我们不希望这样。<strong class="ig hi">我们希望损失对w的导数和损失对b的导数能够解释w和b对每一层的影响，而不仅仅是第一层。</strong></p><p id="3774" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解决这个问题非常非常简单。不用把导数“df / dθf”设为“df/dθf”x“δ”，只需要把它加到导数的值上:</p><p id="26fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你要这样做:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nw"><img src="../Images/42596048928d4672d0cb636cb183dc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*FU-VOF1R7ltpH_Pp6WT-Gw.png"/></div></figure><p id="643d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">而不是这个:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nx"><img src="../Images/dfd2c2f7004483589d1bd424f71f4953.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*_OZvemdw6gi3qRjayZiyOA.png"/></div></figure><p id="0b69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在反向传播的开始，你初始化你所有的梯度，所有的导数为零，然后在反向传播的每一步，对于每一层，你把“df/dθf”x“δ”加到导数的当前值上。</p><p id="67ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们有可变大小的输出会怎样？</p><p id="439f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，我们可能想要为图像生成文本标题，预测未来视频帧的序列或生成音频序列。以前，我们在每一层都有一个输入；现在，我们将在每一层都有一个输出。</p><p id="5246" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个迫在眉睫的问题是，这些输出中的每一个都有自己的损失。在每一步，就像前面一样，我们将执行这些操作:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ny"><img src="../Images/192aa5d0832d9e92c167af7bb1d31754.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*khTCELv42FiuX-4qFA3ZzQ.png"/></div></figure><p id="902a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，该层将有一些函数应用于a (l ),这是某种读出函数，有时也称为解码器。f的参数将在所有时间步中共享，就像线性层的参数一样。我们在每个y hat(l)上都有损失，我们的总损失就是每一步损失的总和。这是一个非常简单的合并损失的方法。</p><p id="6d9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的问题是，在这一点上，如何使用反向传播还不完全清楚，因为常规的神经网络是一个链，从下一层获得delta，然后反向传播到上一层。现在，我们处在一个神经网络有分支的情况下。在每一个时间步，我们会从该步的损失中得到一个增量，并从下一步得到另一个信号。然后，我们需要生成一个返回到上一步的增量。</p><p id="b9fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们要画出这个过程的计算图。</p><p id="f386" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输入x进入线性层，然后进入非线性层，然后是我们的第一个读出函数f1，它可以是一个线性层，后跟一个softmax，进入第一个时间步长的损耗:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nz"><img src="../Images/efc825f920b1bfd89dd0c05686e88d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*UYQZuz4iM24Roy_MVdbBdg.png"/></div></figure><p id="cf94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">适马1也进入第二线性层。所以，它产生一个值，比如说a1，这个a1被两个下游函数使用:f1和lin2。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oa"><img src="../Images/9007cc23caaf5e6b45a6bf85d38b865f.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*MIneWJM9b5ZAbaYVovH06g.png"/></div></figure><p id="656a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，lin2进入sigma 2，同样的机制。所有时间步长的损失相加，这就是最终产生最终损失值的原因。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ob"><img src="../Images/4b3b7abdd7b34d35cf2f2c16cbec7b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*tzSWFMEQnFTB5yZ60zhMdg.png"/></div></figure><p id="88df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以把每一层想象成一个函数f，它以xf为输入，以yf为输出。我们总是从最后一个函数开始反向传播，其中初始增量只有一个。这是表示常规神经网络反向传播的图形方式:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oc"><img src="../Images/2f9883f876b775ad43226bef9d4351a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*cokkdap1nAqCe562y3kWFg.jpeg"/></div></figure><p id="8277" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">常规网络的算法只是一个循环，从最后一层开始向后进行，在每一层都要计算这些δ“YF ”,并用它们来计算δ“xf”和“dL/dθ”。</p><p id="2f74" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将对此进行一点概括。我们必须处理的问题是，我们有一些节点，其输出进入多个节点。如果你看一下我上面画的计算图，你会注意到sigma1的输出进入f1，也进入lin2。在反向传播过程中，会有一个增量来自其中一个输出，另一个增量来自另一个输出。我们所做的就是把它们加在一起，然后把它们代入同一个算法。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es od"><img src="../Images/ebd59d7aa470e23da6187b24827cb4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*SKvo45LtJneyx3WehoexKw.jpeg"/></div></figure><p id="5fde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这有时也被称为“反向模式自动微分”</p><p id="8980" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果每一步都有多个输入和多个输出会怎么样？</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oe"><img src="../Images/5b7c0b671b90f10314720e4f96854f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*qv1QKOugAlgTn7nXXhCn6w.png"/></div></figure><p id="d411" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们只是把之前看到的两个概念结合起来。在每一步，我们都:</p><ul class=""><li id="06a0" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">将前一层的激活与新输入连接起来</li><li id="1f15" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">应用线性层</li><li id="e4ea" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">应用非线性层</li><li id="6c2a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">有一些读出功能(可以只是一个线性层和一个softmax)来产生输出y hat在这一步</li></ul><p id="91b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">是什么让注册护士难以训练？</strong></p><p id="41a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最基本的rnn是非常深的网络</p><p id="6566" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们必须重新考虑为什么训练非常深的网络如此困难。<a class="ae jc" rel="noopener" href="/mlearning-ai/backpropagation-6a2575f09537">链式法则</a>看起来就像一堆雅可比的乘法运算；对于任意深度的网络，损失对第一层权矩阵的导数将是大量雅可比矩阵的乘积。然后最后你乘以这个数dL / dz(n):</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es of"><img src="../Images/e0a4eed11b1f7449a1f0b4045ef07f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*bG2s-1JilZCN6wuS"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mk"><img src="../Images/e9221e06b9be726985770111b979ca27.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/0*TR9Idwr00CnwCRcx.png"/></div></figure><p id="b2eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些雅可比矩阵可以是各种不同的东西。非线性的导数，线性层的导数，卷积的导数。如果我们简化一下，考虑一下标量，将许多数字相乘的问题是有两种可能的结果:如果大多数数字小于1，我们将得到0(消失梯度)，而如果大多数数字大于1，我们将得到无穷大(爆炸梯度)。有一小部分情况下答案更有趣，这一小部分情况是当所有的数字都接近1时:只有当它们都接近1时，你才能从许多标量相乘中得到合理的答案。顺便说一下，这是我们我们更喜欢ReLUs而不是sigmoids整流线性单位的导数是输入是否为正的指标，这意味着这些导数的大部分仅为1。</p><p id="d4f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于矩阵来说，接近1并不意味着每一项都是1，而是意味着矩阵的特征值需要接近1。爆炸渐变并不太难处理，因为我们总是可以剪辑我们的渐变，以防止它们爆炸；RNNs的最大挑战来自消失梯度。一个直观的解释是，如果你的梯度消失，来自后面步骤的梯度信号永远不会到达前面的步骤:当这种情况发生时，你的神经网络变得无法保持记忆</p><p id="9555" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">通过RNN促进更好的梯度流动</strong></p><p id="dfb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每一层，我们都要将新的输入连接到先前的层激活，然后通过线性和非线性操作传递它们。我们称之为“RNN动力学”。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es og"><img src="../Images/3543b26da601d334eeb507637d4a0b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*EiR-iZsj8j-F6Gi3LyMyTw.png"/></div></figure><p id="7839" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们谈论消失梯度时，我们关心的特定导数是RNN动力学的雅可比:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oh"><img src="../Images/dd9e6e2e865778652ffb778ded4f3f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/format:webp/1*x7enK3rO0UxH47YbdOYM4w.png"/></div></figure><p id="93e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是q相对于先前激活的导数。当然，我们不只是想强迫导数总是有接近1的特征值(或者接近恒等式)，因为有时候我们确实想忘记一些事情，有时候我们想用各种有趣的方式来变换它们。我们只希望在真正想回忆的时候，它能接近身份。</p><p id="5908" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">换句话说，不要只是强迫他们是身份，而是想出一些设计，让网络可以决定它想要记住一些东西；当它决定要记忆时，那么这个导数就应该接近恒等式。</p><p id="91a0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直觉告诉我们，对于每个单元，我们都有一个小小的神经回路来决定是记住还是忘记。如果你正在回忆，你只是照原样复制先前的激活，不要改变它；如果你忘记了，那你就用别的东西覆盖它</p><p id="1b84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有一个细胞状态，然后乘以一个介于0和1之间的数f(t)。这被称为遗忘门，因为如果它被设置为0，你会忘记你所拥有的，否则你会记住:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mr"><img src="../Images/feaa0e0c892348756e2e11ff9ab4803c.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*F6HsWdgIJD0EaUnOWUU51A.png"/></div></figure><p id="b638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们给它加上一个数字g(t)。</p><ul class=""><li id="e583" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">如果f(t)接近零，则单元状态由g(t)代替</li><li id="07b7" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">如果f(t)接近1，g(t)接近0，则单元状态保持不变</li><li id="db6a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">如果f(t)接近1，g(t)不为零，那么单元状态以相加的方式被修改，这成为我们新的单元状态a(t)</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oc"><img src="../Images/dc8eacf2a6dbecc81512e306ff166afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*Ao0RsLRI1huZLG4YxWn-Cg.png"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/5862798fea069995fd13059a0a7851d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*Mjsv8eVRGxC4pVlmd7sThg.png"/></div></figure><p id="3931" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从哪里得到这些f(t)和g(t)？</p><p id="153e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> LSTM细胞</strong></p><p id="c6a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们决定是记住还是忘记的方法是基于另一个时间信号h (t-1)。H (t-1)与a (t-1)一起作为前一时间步的输出。</p><p id="cb98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们在这个时间步，x(t)有我们的输入。我们将做和以前一样的事情:我们将x(t)和h (t-1)连接起来，并对它们应用线性和非线性层。这个线性层将产生比我们之前的输出大四倍的输出:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oi"><img src="../Images/c49dab2e690bb18b1e4e4f2524577e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*UxSiwV84UeErp4knus2EJQ.png"/></div></figure><p id="ef6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这四排中的每一排都有不同的功能。其中一个是我们之前看到的遗忘门f(t ),另一个是我们之前添加到产品中的g(t)。</p><p id="0487" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.我们取f(t)并让它通过一个sigmoid，它把它放在0的范围内，这就成了遗忘门。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oj"><img src="../Images/0c530f516624eb051ca14713913ee36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*d2YmvDRv8acnHUdciCcc5g.jpeg"/></div></figure><p id="744e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.我们取i (t ),并让它通过一个乙状结肠。这被称为输入门，它控制对单元状态的修改</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ok"><img src="../Images/cc905b67c313b8e3ec148f236f53cf69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*go5xXvHQ_YWy-t8RKooSgQ.jpeg"/></div></figure><p id="313f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.我们取g (t ),让它通过一些非线性。可能是tanh或者ReLU</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ol"><img src="../Images/8e33df7449580ad06eb4e0e225a161e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*d1mN8DmGvhVtkGkncZNrLg.jpeg"/></div></figure><p id="64cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">g (t)逐点乘以i (t ),然后将乘积加到遗忘门后的单元状态。直觉上，i (t)决定了你是否想要修改单元状态，而g (t)决定了修改是什么。这非常好，因为它允许网络独立地选择是否修改，并且单独地选择如何修改</p><p id="234e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.o (t)被称为输出门，它也通过一个sigmoid，并控制下一个h (t)。在遗忘门和g (t)相加之后，我们获取单元状态a (t ),并使其通过非线性。然后，我们逐点乘以o (t ),这就变成了新的h (t)</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es om"><img src="../Images/e1bcc3b8b6ded997b42626fd111a2866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*BHBARhZySsCmI5BzcIjsxQ.jpeg"/></div></figure><p id="19eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们想要在那个时间步长上的读出功能/解码器，这个新的h (t)也是我们使用的。</p><p id="1470" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里发生的是:我们试图保留a (t)作为线性信号的角色。所有的非线性都被应用到h上，因此a (t)上的梯度将会非常简单</p><ul class=""><li id="b943" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">没有影响a (t)的非线性函数使得它的导数表现得非常好</li><li id="0898" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">同时，通过g (t)的非线性修正和h (t)的非线性读出，我们仍然保留了非线性的好处</li></ul><p id="dbdb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么这些LSTM细胞工作得更好？最简单的原因是a (t)在每一步都以非常简单的方式被修改</p><p id="6a05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以认为a(t)是长期记忆，而h(t)是短期记忆——它一直在变化，并执行复杂的非线性处理。</p><p id="1c64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">实用笔记</strong></p><ul class=""><li id="e490" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">rnn几乎总是在每一步都有输入和输出</li><li id="e9d5" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">像第一部分中的那些天真的rnn几乎从不工作；在实践中，如果你想要一个RNN，你可能会使用类似LSTM细胞，即使它需要更多的超参数调整比标准的全连接或卷积网络</li><li id="7dda" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">对于处理序列，也有一些rnn的替代方法，在实践中可以更好地工作(时间卷积和变换)</li><li id="9495" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">有一些LSTM的变体稍微简单一点并且工作得一样好(门控循环单元)</li></ul><p id="2f8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自回归模型和结构化预测</strong></p><p id="450b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在实践中实际使用的大多数rnn都有多个输入和多个输出，因为大多数需要多个输出的问题在这些输出之间有很强的依赖性。</p><p id="16a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这类问题有时被称为结构化预测，因为你预测的东西是有结构的，而不是像标签那样的东西。一个经典的例子是文本生成。不管文本的输出是否是正确的答案，单词和文本之间的关系决定了它是否是有效的文本。</p><p id="87fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设您有一个由三个句子组成的训练集:</p><ul class=""><li id="c326" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">“我思故我在”</li><li id="b273" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">“我喜欢机器学习”</li><li id="f27a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">“我不只是一个神经网络”</li></ul><p id="3c2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">暂且说你不关心泛化:你只是想让你的模型记住这三句话。我们将单词视为分类变量；每个时间步都是不同的单词，它只是n个可能单词中的一个。</p><p id="b03a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你的神经网络会把单词“我”作为输入。然后我们让它完成句子，它可以用三种不同的方式来完成:“想”，“喜欢”，“我”。然后我们有一个softmax分布，将这些词转化为概率:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es mw"><img src="../Images/93af777db8c9e3af15d1b140864ea42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*Z4yZhJOXLrzGAvhV_wSyOw.png"/></div></div></figure><p id="e331" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将从这个softmax分布中随机抽样，选择第二个单词。现在网络正在完成句子，说“我想”。然后我们继续下一个时间步骤；我们有一个线性层，使用以前的激活来计算新的激活，然后我们有第二个时间步的输出，它再次通过softmax分布。</p><p id="fe9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题是网络不知道哪个词是从那个softmax中随机抽样的:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es on"><img src="../Images/016d7eaf3b17274c507b6c05ecf23356.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*T7H5X3rca49wHDP70Uo1hA.png"/></div></div></figure><p id="3100" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">网络认为它产生了30%“认为”，30%“喜欢”和40%“am”的输出。问题是你试图独立地对这些词进行取样。在第二个时间步，你可能会随机得到“机器”，然后在第三个时间步，你可能会随机得到“只是”。这一代人毫无意义，尽管网络在学习这种分布方面做得很好。我们因此得到一个无意义的输出，这是因为在这个任务中，单词之间的协方差和得到正确单词的概率一样重要。</p><p id="dfb9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是为什么接收可变长度输入和可变长度输出的网络如此重要。当我们运行这个网络来完成一个句子时，我们将从第一个时间步骤的softmax中采样，然后我们将在第二个时间步骤中输入我们的样本。这基本上会告诉网络我们采样了什么。现在网络知道它不只是预测任何一个句子中的第三个词:它是预测前两个词是“我认为”的句子中的第三个词。因此，它会正确地预测，如果前两个词是“我认为”，第三个词可能会是“因此”。</p><p id="f455" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关键思想是过去的产出应该影响未来的产出，而让过去的产出影响未来的产出的方法是把昨天的产出当作今天的投入。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es oo"><img src="../Images/ea2945c10129d4861f80cf251a2e72f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*XR49nv2p8PYmNFdPXapYWw.png"/></div></figure><p id="55db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种基本设计用于所有必须输出结构化序列的rnn，如图像字幕模型或视频预测模型。</p><p id="cfce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自回归模型</strong></p><p id="d0fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练它的最简单的方法是将输入设置为整个训练序列，将地面真实输出设置为相同的序列，但是偏移一个步长。在训练过程中，你不是要求神经网络只是虚构一个句子，而是要求完成一个句子。你是说:在时间步长t，取时间步长1到t的所有单词，在时间步长t+1输出单词。</p><p id="529a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你的x将会是句子中的所有记号，你的基本事实输出将会是所有向后移动一位的记号。最后一个将被替换为停止令牌。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es op"><img src="../Images/5f03fe5d8e88dd4c5db8c14c5a442131.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*HXIOcsEzrhnGOeBTZrGM4w.png"/></div></figure><p id="cfa4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本上这个网络被教导，如果它看到“我”，它应该输出“思考”；如果它看到“想”，就应该输出“所以”等等。</p><p id="bbb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">分配转移</strong></p><p id="7dfb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是有问题的。我展示的基本设计将会很好地工作，但是如果你用它来训练非常长序列的非常复杂的网络，你可能会开始看到一些问题，这是由于分布的变化。</p><p id="de09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设你有一个和以前一样的例子，但是网络犯了一些小错误:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es oq"><img src="../Images/360d9f40ea5f20109bb184668f7e485e.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*kPxuHgVTv-Ei2hpQ0cLiEQ.png"/></div></div></figure><p id="4080" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么会这样？可能是训练的不太好吧。假设你运气不好，抽取了10%的单词。问题是，现在你要用它作为下一个时间步的输入。由于只是一些随机的令牌，网络从来没有见过，要彻底搞混了。在下一个时间步，它会输出一些完全疯狂的东西，然后它不会产生任何合理的东西:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es or"><img src="../Images/d601d0fb908648aa7090d7f4375df406.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*QUbWmYeiXcYTHJsD29zBAg.png"/></div></div></figure><p id="cf89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是训练和测试之间的差异:网络总是将真实序列视为输入，但在测试时，它将一个(可能不正确的)预测作为输入。这称为分布转移，因为输入分布从训练期间的真实字符串转移到测试时的合成字符串。</p><p id="ebda" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">预定抽样</strong></p><p id="f487" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本的想法是，在训练过程中，我们会在每一步做出随机的决定。在一定的概率下，我们将给出网络在该时间步的真实输入，用1减去该概率，我们将给出它以前的输出作为输入。至关重要的是，我们没有通过这条边进行区分，所以网络不知道它正在将自己的输出作为输入。</p><p id="30bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以认为这是以某种概率用网络自己的前一时间步的输出来随机替换真实输入。这真的很有帮助，因为现在在训练期间，网络将自己的输出视为输入，因此，当它看到测试时间时，它不会完全混淆。</p><p id="d79b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于用模型自己以前的输出替换真实输入，我们要如何选择这个概率？直觉上，我们希望在开始时主要输入真实的输入，然后一旦网络变得相当好，我们希望主要输入模型自己的预测，以减轻这种分布变化。因此，我们为这种概率制定了一个时间表:使用真实输入的概率一开始非常高，然后逐渐下降。由于使用模型输出的概率是1减1，所以随着时间的推移，你大部分时间都是在用自己以前的输出来训练网络</p><p id="562f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">rnn的一大优势是它们提供了极大的灵活性:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es os"><img src="../Images/956771fec430629ce29b0e54d41f7a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKgPYHHOhUfL98G1HDrAwg.png"/></div></div></figure><p id="c9b3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">常规前馈网络具有一对一的映射，而RNNs可以有一个输入到多个输出，多个输入到一个输出，多个输入到多个输出，等等。</p><p id="b5ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">实施细节</strong></p><p id="27ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们经常用RNNs做的一件事是，我们在模型中间的某个地方有一个RNN主干，然后我们使用一个非递归编码器(一个常规的卷积网络，然后馈入RNN)。您也可以使用某种复杂的解码器，将RNN或LSTM电池的输出送入多个层，然后产生输出:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ot"><img src="../Images/d25de41899ce4ba369a9adbf2f2d17d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUpnnvoykMu03CLZQenXdQ.png"/></div></div></figure><p id="c631" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您也可以有多个RNN层，在特定的时间步长将RNN的输出输入到另一个RNN层，然后最终输入到softmax。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ou"><img src="../Images/41f7a6bf9fcf8a9ca798b541b044ae0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nUB51FnF8PG5jyF4c9YBg.png"/></div></div></figure><p id="9813" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RNNs可以使用的另一个技巧是双向模型。例如，在语音识别中，您的输入是声音序列，您希望产生相应的单词。大多数情况下，特定时间步的单词取决于该时间步的声音，但有时声音的其余部分会影响您对单词的理解。因此，如果不看整个话语，在特定时间步的单词可能很难猜测。双向模型是一个RNN，其中首先有一个向前运行的递归连接层，然后在其上有另一个向后运行的层。这意味着你拥有过去和未来的信息。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es ov"><img src="../Images/8008ee4675215d5b78607f5c6f7d61c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUWoLz5l4kTxDfka8ieQ4Q.png"/></div></div></figure><h1 id="0524" class="ow je hh bd jf ox oy oz jj pa pb pc jn pd pe pf jq pg ph pi jt pj pk pl jw pm bi translated">序列到序列</h1><p id="ccf9" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">我们将讨论如何使用递归神经网络来解决一些有趣的问题；我们将关注如何训练和利用序列对序列模型。</p><p id="257c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">递归神经网络非常灵活，可用于解决各种不同类型的序列处理问题:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es pn"><img src="../Images/ea98a7696125b196bb72d5b0e83b65e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbVaBmcmBjCk9v_UY0xh0A.png"/></div></div></figure><p id="cb0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将关注多对多问题。在此之前，让我们讨论如何建立一个基本的神经语言模型，该模型为表示文本的序列分配概率。语言模型对于我们将要讨论的很多内容来说是一个非常重要的概念，因为它们不仅可以分配概率，还可以经常生成文本。</p><p id="5a97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个生成短语的神经网络模型的例子。为此的训练数据将是自然语言句子的大集合。这里我只展示了三个:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es po"><img src="../Images/f80534b18cabe2174e859acd76d9787c.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*22XWZxDfwAJCTAlAsclZag.png"/></div></figure><p id="7533" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们是如何表现的？这里有几个选择，我们稍后会更深入地讨论，但一个非常简单的选择是将每个句子标记化，这意味着每个单词都成为一个单独的时间步长，然后以某种方式对该单词进行编码。</p><p id="a17b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对单词进行编码的一个非常简单的方法是一个热矢量。one hot vector只是一个长度等于字典中可能单词数量的向量，并且该向量中的每个元素都是零，除了对应于单词索引的元素，该元素被设置为1。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pp"><img src="../Images/08c9cb5f5da272621654c1b8495a5b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*8tc1dkJG2XL2DUhYBULoCg.png"/></div></figure><p id="87ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有更复杂的方式来表示单词。一个例子是单词嵌入，这是单词的连续向量值表示，旨在反映它们的语义相似性，以便表示相似事物的单词在欧几里德距离方面更接近。</p><p id="3e9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们假设单词是一个热门向量，这意味着我们的自然语言训练数据只是由这些热门向量的序列组成</p><p id="93b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">几个细节</strong></p><ul class=""><li id="cbd7" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">我们需要理解模型何时完成输出一个句子。举个例子，如果你给模型“我”，你想让它用“我思故我在”来完成这个句子，你怎么知道应该停在“m”呢？“我认为因此我是河马”也是一个有效的句子。因此，我们将在所有句子的末尾，在训练数据中包含一个特殊的标记。这是序列标记的结尾，有时也称为句尾标记。它不代表任何实际的词，它只是代表模型已经完成的事实。</li><li id="36d1" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">我们需要以某种方式启动这个模型。对于第二个时间步长，它将使用第一个时间步长的输出作为输入。但是我们在第一个时间步用什么呢？我们可以简单地计算每个句子开始一个单词的频率，从中随机抽取样本，然后作为第一步输入到模型中。一个稍微好一点的解决方案，不需要我们有一个特殊的组件，就是引入一个句子开始标记。因此，该模型将学习它应该输出一个随机单词，其概率与实际开始一个句子的概率成比例。</li></ul><p id="2be3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这些开始标记和句子结束标记，我们知道如何让rnn生成完全随机的句子。但是，如果我们想让RNN完成一个句子，例如“我认为”，这样做根本不需要改变训练程序。我们仍然一次放一个代币进去。你输入“开始”，你让它做一个预测，但是你不从那个预测中取样；取而代之的是，你直接输入下一个你正在调节的词，也就是“我”。然后，你让它做一些预测，你忽略它，然后你输入“思考”，做一些预测，然后你实际上从中取样，输入到第四步。为了使你的一代适应一个特定的起始片段，你所要做的就是强迫最初的几个输入与那个片段一致，而不管网络输出的是什么。</p><p id="c06d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">条件语言模型</p><p id="3280" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我们讨论了无条件语言模型；现在，我要谈谈我们如何建立条件语言模型。对于这些条件语言模型，文本是模型的输出，而不是输入。你要让这个模型以一些输入为条件，这些输入会告诉它你想要它生成什么样的文本。例如，您可以想象一个图像字幕的条件语言模型:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pq"><img src="../Images/1acbf1c07bbb9132ad1e25acdb4740af.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*YRS693wPfsHqF6wTRTClPg.png"/></div></figure><p id="0580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以有某种编码器模型，通过卷积网络读入条件信息(小狗的图片)，这将产生一个代表RNN初始状态的向量。这个矢量a0是RNN的初始状态。这整个事情将被端到端地训练CNN和RNN将被一起训练以产生正确的文本。</p><p id="ea48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总而言之，这是一个大型神经网络，由几个卷积层组成，可能是一些完全连接的层，其中一层进入RNN。我们把这个网络的RNN部分称为RNN解码器，因为它的工作是把a0中包含的信息解码成英文文本；我们称卷积部分为CNN编码器，因为它的工作是将输入编码成a0:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pr"><img src="../Images/1a4a18b8dc2e4cece51db4ed364be72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*F7028jUHKS-IYz9uk1M1VQ.png"/></div></figure><p id="9eca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的训练数据将包含图片和标签。</p><p id="3f08" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，如果您想将法语翻译成英语，您可以让一个RNN读入法语文本，并为另一个生成英语文本的RNN生成初始隐藏状态激活。现在，你有了RNN编码器，而不是CNN编码器，无论法语句子的最后一步发生了什么，都会直接进入英语句子的第一步。</p><p id="f2b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有一些细节可以让它真正发挥作用。第一，有时人们以相反的输入顺序阅读，从句尾开始，然后倒着读。这样更好的原因是，英语句子的开头可能与法语句子的开头关系更大，而不是与法语句子的结尾关系更大。通过颠倒输入句子，第一部分排在最后，这使它最接近输出句子的第一部分。</p><p id="c17f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">序列对序列模型</strong></p><p id="c418" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们有两个不同的rnn用于输入和输出；编码器和解码器具有不同的权重，并且编码器为解码器产生初始隐藏状态激活。这是一个更现实的例子:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ps"><img src="../Images/47284d016dbd9b07aa5284c000385670.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*YZCXQQ3Y0zlmVh4t9BnYoQ.png"/></div></figure><p id="79d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1我们反向读入输入句子和<br/> 2我们添加了多层；多个LSTMs(两个到五个之间)堆叠在彼此之上<br/> 3在成对的序列上首尾相连地训练网<br/> 4序列可以具有不同的长度</p><p id="b0d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">执行解码</strong></p><p id="357e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，从这些rnn中得到一个句子的明显方法是一次做一步，然后在第一步我们必须输出一些东西，我们得到一个softmax分布。这意味着我们得到一个长度等于可能单词数的向量，你对它应用一个softmax，然后把它变成一个概率向量。这是解码句子中第一个单词的概率分布。在这一点上，显而易见的事情可能是选择概率最大的单词，然后在下一个时间步骤中输入所选择的单词。这边走:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pt"><img src="../Images/d22676520d9ee5c82573aafd1645682e.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*Quxf_zujRlVc4nBcqDqoXA.png"/></div></figure><p id="a9bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是请注意，网络犯了一个小错误。在第一个时间步骤中，它有几个可能的单词可供选择(当然，在简化的场景中)，正确的单词应该是“a”，但单词“one”也有类似的概率，只是稍微大一点，因为法语单词“un”可能表示两个单词。这似乎是一个合理的选择，但它最终会打乱其余的解码。它为第二个选择了“小狗”这个词，但是“一只小狗很可爱”不是一个正确的英语句子。所以，网络选择“是”，这产生了一个有效的英语句子…但不是正确的翻译。</p><p id="f2aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，哪里出了问题？问题是我们承诺在看到其余的解码之前选择“一”。第一个错误并不是那么不合理，却导致了一个完全不合理的结果。如果我们没有在第一个时间步选择“一”，而是选择了“一”，那么我们将在以后得到一个相当合理的世代。通过选择“a ”,在第三个时间点上“小狗”的概率会更高。</p><p id="babe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，概率取决于整个输入序列和我们为输出序列生成的所有前面的单词:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pu"><img src="../Images/ca1e0a860151c32fd1777777e1f4ff91.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*ZJUHqlcNgB7-Gy0hTElB2g.png"/></div></figure><p id="13e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想法是<strong class="ig hi">我们想要最大化所有概率</strong>的乘积，这允许我们从我们的解码器得到最可能的输出序列。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pv"><img src="../Images/f4afb36b2a7e59a3ee9d79227b592c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*h0_RsL3I1oEOK6387wbKvw.png"/></div></figure><p id="38c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于处理大量概率的乘积很烦人，我们更喜欢在对数空间中表达它们，在对数空间中乘积变成总和:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pw"><img src="../Images/6f5b041dd0fc625e137e432cbfa670ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*h1ei5u8umNXzz3Wit9zbrw.png"/></div></figure><p id="0aa2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了最大化所有概率的乘积，我们不应该只是贪婪地选择第一步中最高的概率。这意味着，我们可能会在早期选择一个概率稍低的单词，以便在后来获得一个概率高得多的单词:贪婪地选择最有可能的单词并不总是最佳的。</p><p id="a3a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有多少种可能的解码？对于m个单词，有m个长度为t的^ T序列。如果我们在每一步只有四个记号，原则上我们可以在第一时间步选择这四个记号中的任何一个，在第二步选择这四个记号中的任何一个，在第三步选择这四个记号中的任何一个，依此类推。当然，这些序列中的大多数概率很低，但原则上它们中的任何一个都可能是最优的。从根本上说，解码是一种树形搜索，其中转换的成本是其对数概率的负值(因为在树形搜索中，您在进行的过程中对成本进行求和),并且搜索路径的数量呈指数增长。我们可以使用任何树搜索算法来解决这个问题，但在这种情况下精确搜索是非常昂贵的，因为这个问题没有一个很好的结构，使广度优先搜索和深度优先搜索这样的事情能够很好地工作。幸运的是，这个问题的结构使得一些简单的近似搜索方法非常有效。</p><p id="a361" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于我们对序列解码的了解，基本的直觉是，虽然在第一步选择概率最高的单词可能不是最佳的，但选择概率非常低的单词也不太可能导致好的结果——这在一般的搜索中并不总是正确的。所以，我们不能贪婪，但我们可以<strong class="ig hi">有点</strong>贪婪。这种算法被称为波束搜索，它不是在每一步贪婪地解码最佳字，而是存储迄今为止的k个最佳序列，然后更新它们中的每一个。将k设置为1的特殊情况只是正常解码，因为您总是选择最有可能的单词。</p><p id="b7fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个k = 2的波束搜索示例。我们要把法语中的一个句子翻译成英语，我们特意选择了一个有点不平凡的翻译，意思是没有完全对等的英语单词“entarté”。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nu"><img src="../Images/ec93a5b8ec419141f9707e0af2158b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*KOR_CoFvJ44fVLJ0d7r1ew.png"/></div></figure><p id="dafc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每一步，我们追踪两个最有可能的假设。我们从“开始”令牌开始，并产生softmax分布——我们向前运行我们的RNN一步，在第一个单词上产生softmax分布，并跟踪两个最可能的假设。我们得到“他”和“我”。现在，我们将根据单词“他”向前移动RNN，然后我们将根据单词“我”分别向前移动RNN。对于这两个向前的步骤，我们将在第二个字上总共有四个不同的softmax分布。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es px"><img src="../Images/8606e4064863db745f8b90f969e87d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*f18K4uIfufN_CvLVaUk8dg.png"/></div></figure><p id="79f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将计算当我们用“他”时，前两个单词的对数概率和，以及当我们用“我”时，前两个单词的对数概率和。我们现在有四个分数，我们取最好的两个:“我是”和“他打中了”。我们将继续进一步拓展这两个领域:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es py"><img src="../Images/3a3f8ea9addb7b9ad10bd76f239f3845.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*5AeIU6qUv9OAWfMnvv-oyg.png"/></div></figure><p id="f59a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">再一次，和以前一样。我们什么时候停止解码？很简单，如果我们的一个评分假设以END token结尾，我们停止扩展它并保存在那里。我们将继续下去，直到某个截止长度，或者直到你有一些预定义的完整序列数。最后，我们选择哪个序列？</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pz"><img src="../Images/43d5cf25820da881d4b3405a6f8d0e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*FRi47ldH7VNgsaWK9HPoOg.png"/></div></figure><p id="da43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，这些分数中的每一个都是每一步的对数概率的总和，但每句话中的步骤数是不同的。句子越长，我们加起来的负数越多，这意味着一般来说，较长的句子倾向于具有较低的对数概率:为了避免短而差的翻译比长而好的翻译更经常被选择，我们实际上将分数定义为总对数概率除以步骤数。</p><p id="95ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总之，这就是光束搜索，它在大多数情况下都能很好地解码最有可能的句子。事实上，很小的k值通常也能很好地工作，所以5到10是非常典型的，但是甚至低至2的数字有时也能很好地工作。</p><p id="b0ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意</strong></p><p id="0666" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我想简单介绍一下序列对序列模型的注意概念。这源于这样一个事实，即在序列到序列模型中，我们有一个瓶颈问题:所有关于源序列的信息都必须包含在解码开始时的激活中。解码器不知道任何关于源序列的信息，除了编码器将什么放入第一个隐藏状态。当输入序列很长时，这可能是一个大问题。</p><p id="4dd7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们允许解码器在解码时更多地查看输入序列，也许我们可以稍微放松一下。如果您的输入是一个完整的段落，并且编码器对其主旨进行了编码，我们希望解码器到达该点并引用回输入序列，以便提取出它感兴趣的特定标记。这将大大降低瓶颈的重要性，并且处理非常长的序列会容易得多。</p><p id="5b79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本想法是:</p><ol class=""><li id="c3ba" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">编码时，我们将让编码器RNN的每一步产生一小段信息，描述在那个时间步出现的内容。这条信息对我们来说没有语义上的意义，它只是一个小小的激活向量，在孤立的情况下没有任何意义。它将被称为“关键”，并将作为学习过程的一部分来学习。</li><li id="b37a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">解码时，我们将让解码器RNN的每一步输出一个长度相同的小向量，我们称之为“查询”。直观地说，它代表了我们在这个时间步需要的信息类型。</li><li id="b29d" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">我们将查询向量与每个关键字进行比较，以找到最相似的关键字，这将告诉我们输入中的哪个时间步长与解码过程中的这个时间步长最相关。然后，我们将把这些信息发送给解码器。</li></ol><p id="48db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对这可能做什么的一些粗略的直觉是，一个键可能编码句子中正在发生的一些概念，像“这个词是句子的主语”；如果查询编码了一些概念，如“我需要知道句子的主题”，那么前面提到的键将被选中，因为它与查询最相似。然后，我们采取相应的时间步骤，获取其隐藏状态，并将其输入解码器。</p><p id="fb44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">重要的是，我们不需要手动指定这些东西。整个网络仍将进行端到端的训练，键和查询的实际内容将由神经网络确定——就像卷积网络中的过滤器具有某种直观的意义，但不是手动指定的。</p><p id="07af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实际上，我将使用字母h来表示编码器的隐藏状态。我将用字母s来表示解码器的隐藏状态。对于密钥，我将使用字母k；每个时间步都有一个独立的关键点，关键点是我们应用于RNN隐藏状态的函数。例如，它可以只是一个线性层，后面跟着一个非线性层，并且在实践中，函数k通常只是一个线性变换。对于这个查询，我将使用字母q来表示它，用下标l代替t。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es pr"><img src="../Images/17906ff801be3480f5b5d23f6920a386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*NHtQ0ER8WwqfFgVh-yU6iw.png"/></div></figure><p id="21a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关注点积“e”是一个分数，表示一个关键字与一个查询的相似程度。它将由kt和ql之间的点积给出，我们想要在点积最大的时间步长t中取出隐藏状态ht。</p><p id="23dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为此，我们将创建一个向量alpha(l)，通过关注点乘积的softmax获得:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qa"><img src="../Images/675cab9ca8cfff9ee0fad0bb8d33f813.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*QJdgC5GZ7o0YewFmhyMrYQ.png"/></div></figure><p id="4a54" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意力点积越大，这个softmax就越接近arg max。然后，如果我们想要发送大致对应于最大点积的密钥，我们将在加权和中使用alpha:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mt"><img src="../Images/544bc841159b30ff7014282cf2813976.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*Iad6xmvCy1g4oK0CmaFJ3A.png"/></div></figure><p id="d76d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直观上，除了具有最大点积的时间步长之外，这些α对于所有时间步长t都是小数值；因此，这个加权和将由对应于点积的arg max的激活来支配。</p><p id="555a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">直觉上，我们正在尝试提取时间步长t的编码器RNN状态，对应于键和查询之间最大的点积。我们称之为关注的原因是网络试图关注输入中最相关的部分。总而言之，对于特定的查询qt:</p><ol class=""><li id="0940" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">从编码器的隐藏状态“h”中学习密钥，例如利用线性层</li><li id="ad66" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">注意力分数“e”是通过给该查询的关键字打点来产生的</li><li id="d7e7" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">我们对所有的点积“e”应用软最大值，我们称结果为“alphas”</li><li id="8656" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">最大的alpha被发送到解码器</li></ol><p id="24ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将alpha发送给解码器是什么意思？解码器可以以不同的方式使用阿尔法。</p><ul class=""><li id="65e7" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">把它交给产生输出的函数，这样输出就变成了对应于最高注意分数的解码器状态和编码器状态的函数</li><li id="75ab" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">如果您使用的是多层或堆叠RNN，请将其提供给下一个RNN图层</li><li id="d319" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">将其交给下一个解码器步骤，以便注意力向量与例如新单词一起作为附加输入</li></ul><p id="80d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注意变量</strong></p><ul class=""><li id="27a1" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">一个简单的选择是使用k和q作为恒等函数。在这种情况下，时间步长t处的关键字只是ht，而时间步长t处的查询只是sl。当你解码时，你的注意力分数是输入隐藏状态和解码器隐藏状态的点积。这很容易实现，但是表达能力有限</li><li id="ef8b" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">另一个非常简单的选择是使用线性乘法注意力，其中键和查询只是隐藏状态的线性函数。这是非常合理的，因为RNN隐藏态已经通过应用一个非线性函数形成了。因此，这并没有大大削弱我们的表达能力</li><li id="b70f" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">最后一个变体稍微复杂一些。它仍然有键和查询，但最后，当你构造你的注意力向量时，你不是构造编码器隐藏状态的加权组合，而是构造由一些学习函数V转换的那些隐藏状态的加权组合。V在这里代表“值”,解释是在编码期间你产生键-值对，而在解码期间你找到k和q之间最大乘积的时间步长，并取其值。这可以增加一点灵活性。</li></ul><p id="3c4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">总结</strong></p><ul class=""><li id="ba53" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">每个编码器步长t产生一个密钥kt</li><li id="efef" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">每个解码器步骤l产生一个查询ql</li><li id="3701" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">解码器被发送对应于乘积kt和ql的最大值的编码器激活ht</li><li id="e733" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">注意力是非常强大的，因为所有解码器步骤最终都与所有编码器步骤相关联</li><li id="3279" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">梯度表现得更好，因为沿着注意力路线相乘的雅可比数是O(1)。这对于非常长的序列变得非常重要</li></ul><h2 id="87d5" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><a class="ae jc" href="https://www.youtube.com/watch?v=VDnEnlYzHOU&amp;list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&amp;index=36" rel="noopener ugc nofollow" target="_blank">变形金刚</a></h2><p id="20ef" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">之前，我已经谈过我们如何建立序列对序列模型，并通过使用注意机制来提高它们处理长范围依赖性的能力。</p><p id="35ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，我们将讨论另一类处理序列的模型，它不使用循环连接，而是完全依赖于注意力。我们将建立一类叫做变形金刚的模型。</p><p id="7cea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基本的问题是:我们能在没有任何明显复发的情况下摆脱注意力连接吗？原则上，注意力可以从输入字符串中获取你想要的任何信息，我们的RNN可以转化为一个纯粹基于注意力的模型。但是有一些警告:</p><p id="df9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如果我们天真地使用基本的注意力机制来做这件事，我们可以访问编码器的隐藏状态，但不能访问解码器的先前状态。</strong></p><p id="647e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，我们使用自我关注。我们将获取序列中的所有时间步长，并对每个时间步长进行编码。这就像一个前馈模型。H1是x1的某个函数；h2是x2的某个函数；h3是x3的某个函数。权重在所有时间步长上是共享的，因为函数在每种情况下都是相同的。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qb"><img src="../Images/af0e35f836cc10f2f24a3727ecaa2b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*ZKrtMuWsJxWyuUtWydDq2g.png"/></div></figure><p id="44a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将为每个时间步长产生一个值，其中vt是ht的线性函数:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qc"><img src="../Images/549e0e3d1221eff30d8daef507a05bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*1lEQLDKQsOW9cBDTU-gxnQ.png"/></div></figure><p id="c5de" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还将输出一个密钥，其中密钥是ht的某个线性函数:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es qd"><img src="../Images/23244d59cfed8d96dacf6091b3270f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*9WNvXy1C8inAn_Gfd9cxwQ.png"/></div></div></figure><p id="2f79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，每个时间步长也会输出一个查询；就好像我们把编码器和解码器放在了一起。到目前为止，我们已经得到了一个键、一个值和一个查询，它们都是ht的线性函数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es qe"><img src="../Images/bb868e7820dbec2eede72de9f28792f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*5ODt_hu7GiQmKyJ6gq7-vQ.png"/></div></div></figure><p id="f874" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们将完全按照我们之前对注意力所做的那样去做，除了现在编码器和解码器之间没有区别。本质上，每一步都可以索引到其他每一步，包括它自己。</p><ol class=""><li id="60f1" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">我们将计算每个qt和每个kt之间的点积，这将给出我们的注意力分数</li><li id="dde3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">我们将通过一个softmax来传递它们，我们将得到alpha值</li><li id="fdcc" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">然后，我们取每一步的值，我们用阿尔法加权，这就给了我们注意力</li></ol><p id="f991" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种自我关注机制在每一个时间点都产生了一个关注向量。您可以将此视为一个跨时间步长集成信息的层。我们可以越来越多地重复这个过程:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qf"><img src="../Images/ec5999ad63eedfdea1cd888c8e9798dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*ebPJU2-jRlTaSjyt7i1tSA.png"/></div></figure><p id="d846" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将它转化为可行模型的实际过程需要一些额外的步骤。自我关注的基本概念可以用来开发一种强大的序列模型，称为转换器，但要使它真正工作，我们需要开发一些额外的组件，并解决自我关注的一些基本限制。</p><p id="6fc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是我们需要注意的事项:</p><ul class=""><li id="5a54" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated"><strong class="ig hi">位置编码</strong>解决了自我关注的一个问题，即自我关注没有任何接近和时间的概念。所有这些x都是完全并行处理的，不考虑它们的顺序。如果我们交换他们的顺序，自我注意层会产生完全相同的答案。当处理像自然语言这样的数据时，这可能是一个大问题，因为单词的顺序确实很重要</li><li id="359b" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated"><strong class="ig hi">多头关注</strong>允许查询每层多个位置；在每一层，我们不只是在查询和键中有一个值，而是可以有多个值——这样我们就可以进行更复杂的操作。粗略地说，您可以将(键、值、查询)元组视为CNN中的过滤器；实际上，你永远不会构建一个每层只有一个过滤器的CNN，同样，你也不想构建一个每层只有一个注意力头的纯自我关注模型</li><li id="e6b3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated"><strong class="ig hi">增加非线性</strong>，因为我们到目前为止讨论的模型完全是线性的。</li><li id="e4fe" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated"><strong class="ig hi">屏蔽解码</strong>防止对未来的注意力查找。现在，注意力机制不区分过去和未来。当你试图用自我关注来解码一个语言模型时，这可能是一件非常糟糕的事情</li></ul><h1 id="09cd" class="ow je hh bd jf ox oy oz jj pa pb pc jn pd pe pf jq pg ph pi jt pj pk pl jw pm bi translated">从自我关注到变形金刚</h1><p id="3176" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><strong class="ig hi">位置编码</strong></p><p id="2d0e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们看到一个句子时，我们所看到的既包括这个句子中的单词，也包括这些单词出现的顺序。例如，如果我们有这样一句话“他用馅饼砸了我”，那么天真的自我关注看到的只是一袋单词。如果你要排列单词，你会得到完全相同的注意力向量——而循环模型一次看一个单词，它会记住之前看到的单词。一般来说，单词在句子中的位置携带着重要的信息，当我们使用自我注意模型时，我们非常希望保留这些信息。</p><p id="0c07" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这个问题，我们将在开始时向表示中添加一些信息，表明它在序列中的位置。位置编码意味着第一个ht将是xt和t的函数:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qg"><img src="../Images/9d53bc2cdf0479ef6bf0fad33b092204.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*uoRUzkvYItXzMaystgzkXQ.png"/></div></figure><p id="cbde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这将保持记号在序列中的顺序，并允许自我注意利用它。简单地将t附加到输入x上并不理想，因为绝对位置没有相对位置重要。如果我给你两个句子(“我每天都遛狗”和“我每天都遛狗”)，重要的信息是单词dog相对于单词walk的索引，而不是单词dog本身的索引。</p><p id="ca22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以提出一种位置编码，它更关注相对位置而不是绝对位置。我们希望以这样一种方式来表示位置，即具有相似相对位置的标记具有相似的位置编码。例如，我们可以附加时间步长的频率，而不是附加实际的时间步长。位置编码可以是与xt的嵌入长度相同的向量；向量中的每一项都是应用于时间步长t除以某个频率的正弦或余弦:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qh"><img src="../Images/1999d8d7cf14e3a4706d7affcf83d368.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*tST5pM4zzpJNzXEOiAgqjw.png"/></div></figure><p id="dac8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">早期的条目有很高的频率，因为它是t除以某个小数字；在向量的末端，它们有很低的频率，因为它被t除以一个很大的数，比如10.000。如果你要画一张这些位置编码的图片，它们应该是这样的:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qi"><img src="../Images/45d73c14071590f26766fd7103428774.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*HYezuON1FxpJ5lXagK_63A.png"/></div></figure><p id="2c77" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的每一行都是这个位置编码向量的不同维度，水平方向上的每一点都是序列中的索引。这种频域编码对于获得相对位置非常好。</p><p id="c644" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，我们可以有更复杂的位置编码。例如，我们可以学习它。我们可以学习我上面展示的整个矩阵，使矩阵中的条目成为我们模型的可学习参数</p><p id="dede" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每个位置我们都有一个x，对于每个输入序列都是不同的；在每个位置，我们也有一个p，对于每个序列都是一样的，但它是学习来的，所以在每个时间步都是不同的p，但一个序列的p1与另一个序列的p1完全相同，所以它只是一个学习常数。这比正弦-余弦编码更灵活，在某种意义上可能更优，因为我们最终学习了最适合我们序列模型的位置编码。缺点是有点复杂，你需要手动选择最大序列长度</p><p id="1206" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们如何将这些位置编码整合到我们的自我关注模型中？</p><p id="7ec7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在每个时间步，我们都有输入xt和位置编码pt。转换器通常首先嵌入输入，然后将位置编码添加到嵌入中。所以编码就是xt的嵌入加上位置编码pt。这是一个任意的选择，你可以用任何其他方式组合它们。</p><p id="a6da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">多头关注</strong></p><p id="ce8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们现在完全依赖注意力，所以允许自我注意力机制整合来自多个时间点的信息可能是可取的。例如，如果在第二个位置，你需要句子的动词和主语来组合它们，那么让一个注意机制来提供主语，另一个注意机制来提供动词会很酷。</p><p id="b98b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实现这一点的方法是，在每个步骤中可以有多个键、查询和值:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qj"><img src="../Images/d73748d43db70b03a9ec37ffcd199427.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*lF6kYzwB-d8-xQH9-bC2Nw.png"/></div></figure><p id="7272" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不同的颜色轮廓代表不同的头部。这些头部中的每一个都产生一个注意力分数，并且为每个头部独立计算权重。最后把三个头叠加起来就形成了一个完整的注意力向量。也许其中一个头拉出一个主语，一个拉出一个形容词，一个拉出一个动词等等。</p><p id="e3c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用每个自我关注层的多个头部来建立自我关注是一个非常好的主意——通常，八个头部左右的东西对大模特来说似乎很好</p><p id="ca41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">增加非线性</strong></p><p id="78e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，自我关注在值上完全是线性的，因为我们的键、我们的查询和我们的值是ht的线性函数。每个自我关注层都有一套不同的权重。然后，我们的alphas通过应用一个softmax来计算，这是一个非线性操作，但是实际的注意力是通过那些softmax分数加权的值的线性组合来获得的。所以，注意力在v字母中是线性的，而v字母在h字母中是线性的。这意味着每个自我关注层都是前一层的线性变换，具有非线性权重，这不是很有表现力。</p><p id="8171" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">增加非线性的一个非常简单的方法是简单地用某种非线性层交替自我关注:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es qk"><img src="../Images/58cd40fd74e89257dc3b84b53671d0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UaerpEykRcsINx2l_CM8w.png"/></div></div></figure><p id="bd7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种位置式的非线性功能处理信息，而自我注意层是记忆提取层。</p><p id="de4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">蒙面解码</strong></p><p id="3cac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自我关注的问题在于它不区分过去和未来；相反，您希望第一个时间步骤仅使用第一个步骤的信息来生成输出，然后您希望将该输出用作第二个步骤的输入，第二个步骤可以处理该信息(包括第一个步骤的信息)，然后生成第二个步骤的输出，依此类推。</p><p id="3fbd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题是我们有一个循环依赖。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ql"><img src="../Images/c1e803617d86301c066b9384369a5501.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*xFDTCrEQg5pIMomD5JV7tw.png"/></div></figure><p id="e4d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编码时，步骤1的自我注意可以查看步骤2和3的值，该值基于步骤2和3的输入。在测试时，步骤2和3的输入将基于步骤1的输出，这需要知道步骤2和3的输入。</p><p id="7776" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以用一个非常简单的方法来解决这个问题，这个方法叫做掩蔽注意力。你必须允许自我关注过去，但不能关注未来:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qm"><img src="../Images/c0e42398caba2c5f30db95d0fa07a0d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*djjMplwLs3egFg7k67bioA.png"/></div></figure><p id="16ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从数学上来说，要计算关注度分数，只有当关键字出现在查询之前时，才需要像以前一样做，否则就将关注度分数设置为负无穷大。这样，当你把它放入softmax时，e的负无穷大是零，未来时间步长的权重将永远是零，永远不会被使用。在代码中，处理无穷大很烦人，所以你直接把指数设置为0。</p><p id="2da3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">总结</strong></p><p id="8fd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以实现一个实际的序列模型，完全基于自我关注，如果你结合我们描述的四个修改，序列模型将会工作。</p><ul class=""><li id="b047" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">你必须用非线性位置前馈网络来交替自我关注层，以获得非线性变换</li><li id="0ad6" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">您必须对输入或输入嵌入使用位置编码，以使模型知道序列中标记之间的相对位置</li><li id="8ba1" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">你必须使用多头注意力，这样你就可以在每个自我注意力层整合来自多个不同时间点的信息</li><li id="2ca7" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">如果你想用模型来解码，你必须使用掩蔽注意力来防止峰值进入未来。</li></ul><p id="7d8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">变形金刚</strong></p><p id="03d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将结合目前为止所学的内容，浏览经典变形金刚模型。</p><p id="f413" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用连续自注意和位置非线性来处理序列的模型设计通常被称为变换器，因为它们使用这些自电位机制在每层变换一个序列和另一个序列。</p><p id="4d63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">序列到序列RNN模型具有RNN编码器和RNN解码器。这些通常可以具有多层。为了把它变成一个变换器，我们将把编码器和解码器替换成连续的自关注层，交替进行位置非线性变换。</p><p id="23a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编码器</strong>将获得x的序列，具有相应的位置编码p1、p2和p3。在每一个位置，我们将对这些进行编码，然后我们将它们传递到自我关注层。然后，我们将获得多头电位的输出，并将其传递到位置式非线性网络中。这样重复n次。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qn"><img src="../Images/f5943db17fd5913a40be8a9f96f25603.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*oDItKXKFbe8hXeq3LJFTPA.png"/></div></figure><p id="7bd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在解码器</strong>中，我们还将对输出步骤及其相应的位置编码进行位置嵌入。我们将使用掩蔽注意力，这意味着较早的时间步骤不会看到后面的时间步骤。然后，我们将有一个位置式非线性网络。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qo"><img src="../Images/219742376d7aa6b2a253d0e1f41bc50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*rsJfa510WIJxC-e07amOzA.png"/></div></figure><p id="6b00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们会有交叉注意，它使用所有这些编码器产生的隐藏向量。这非常类似于我们在上一讲中描述的经典注意力，因此，交叉注意力将查看编码器中的时间步长，而不是查看相同序列中的其他时间步长的自我注意力:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qp"><img src="../Images/839044e04fc933e3d2f3229650bf1de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*Nz4af-4SanJ5tHCj0Irl8A.png"/></div></figure><p id="e72b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">经过交叉注意，我们有了另一个位置式非线性网络。这意味着每个解码器层有两个注意步骤:它有一个掩蔽的自我注意、一个非线性函数、交叉注意、另一个非线性函数，然后进入下一个块。这些块也重复n次，因此解码器块的内容是编码器块的两倍</p><p id="7345" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，在n个这样的块之后，我们在每个位置应用一个softmax并读出输出:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qq"><img src="../Images/5360d0d1b683f1c63a7ae7618b7a2d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*BvJgnNXEOipK3S98JlKPag.png"/></div></figure><p id="0b29" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们更仔细地看看交叉注意机制，它有点微妙。</p><p id="d2ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用带有上标l的ht来表示编码器中l层的隐藏状态，使用sl上标l来表示解码器中的隐藏状态。这些状态是由位置非线性网络产生的。在交叉注意中，我们有:</p><ul class=""><li id="e9d6" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">在步骤t，通过在解码器层l将矩阵Wql应用于位置式非线性网络的输出而获得的查询</li><li id="7d91" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">在步骤t，通过将矩阵wk1应用于编码器层l的位置式非线性网络的输出而获得的密钥</li><li id="2e13" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">在步骤t，通过将矩阵wv1应用于编码器层l的位置式非线性网络的输出而获得的值</li></ul><p id="8055" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们计算每个t和l之间的注意力得分，应用softmax，然后计算我们的交叉注意力。</p><p id="6632" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现实中，交叉注意力也是多头的。在展示真正的变形金刚模型之前，还有一个小细节:图层规范化。</p><p id="713e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">层规范化与注意力模型没有任何关系，但事实证明它对转换器很重要，因为很难对它使用常规的批量规范化。主要思想是批处理规范化非常有用，但是由于几个原因很难将其用于序列模型:</p><ul class=""><li id="3d34" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">序列的长度是不同的，这使得在批处理之间进行标准化有点困难</li><li id="abb2" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">序列可能很长，所以我们有时只有很小的一批</li></ul><p id="5bf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用的一个简单的解决方案是用所谓的层规范化来代替批规范化，它类似于批规范化，但跨层中的不同激活，而不是跨批中的不同样本。在标准批处理规范中，在每一层，你都有一堆激活向量，每个输入一个；批次标准计算该批次中所有点的平均值和标准差。然后，当您使用批量归一化对它们进行变换时，一切都是针对每个元素的，对于每个元素，您减去平均值的对应元素，除以标准差的对应元素，乘以gamma的对应元素(这是学习到的比例)，然后加上beta的对应元素:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qr"><img src="../Images/3c83bdda33bfc43246bac3a12857e50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*SQ2Et9EUY83dlYS3s9LOwQ.png"/></div></figure><p id="2072" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Layer norm将是一个更传统的层，它只使用单个数据点中的信息，而不是计算批中不同向量的平均值，它通过平均每个维度的激活来计算单个数字:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qs"><img src="../Images/44203328443bf227561ab9ce9b8010a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*vb7Hy97gKqFEfC3BUsXRBA.png"/></div></figure><p id="0ddb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">将所有这些放在一起</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es qt"><img src="../Images/12cdf35754fc30ea35b3589a09b51e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*3mg3JmhG8TtX0OLdfjT1yg.png"/></div></figure><p id="589c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这张图片摘自介绍变形金刚的论文，名为“注意力是你所需要的”。本文提出的是使用连续的自我注意和位置非线性函数来实现序列到序列模型——它们应用于机器翻译。</p><p id="a5c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">编码器</strong></p><ul class=""><li id="7995" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">我们从输入序列开始，分别在输入序列中嵌入每个标记，并在嵌入中添加相应的位置编码</li><li id="8dec" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后我们有了第一个多头自我关注层</li><li id="9eaf" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">你把所有人的注意力连接起来，以获得每个位置的阿尔法值</li><li id="efee" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后你就有了这个叫“Add &amp; Norm”的东西，它是一个残差连接后面跟着一个层范数。原因是，他们希望每一层都是一个修改，从剩余连接中获得良好的渐变效果</li><li id="d518" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后你有一个位置非线性函数。特别是，它由一个线性层、一个ReLU和另一个线性层组成。</li><li id="b6f2" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后是另一个“添加&amp;规范”</li></ul><p id="914b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">他们有n个这样的区块。每个块是多头自关注、加法和范数、位置非线性、加法和范数、将其传递到下一个块、多头自关注、加法和范数、位置非线性、加法和范数、将其传递到下一个块等等。</p><p id="0508" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那是编码器。一旦你一直向前运行编码器，你将会计算出其中的六个块。编码器的产品是这些键和值</p><p id="ebbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">解码器</strong></p><ul class=""><li id="fded" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">解码器具有相同的位置编码业务，每个编码器模块都有相应的解码器模块，但解码器模块稍微复杂一些。现在，多头注意力被屏蔽，只查看过去的时间步骤，因为它将用于生成序列。</li><li id="8374" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">添加定额</li><li id="38b2" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">有一种交叉注意力会根据自我注意力的输出产生疑问。这些查询用于从编码器中查找键和值。</li><li id="17bb" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">再次，有一个添加和规范。这个加法&amp; norm将把交叉注意和自我注意的结果加在一起。</li><li id="d903" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后，您有一个位置式非线性函数，它使用与编码器相同的架构，但权重不同</li><li id="d42a" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后你有另一个add &amp; norm</li><li id="54f3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">然后你把它们传给下一个街区</li><li id="2572" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">在最后一个块的末尾，每个位置都有一个线性层，后跟一个softmax，它输出该位置令牌的概率分布</li></ul><p id="4301" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解码器以掩蔽的注意力一次解码一个位置</p><p id="6aed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么是变形金刚？</strong></p><p id="2a3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">变形金刚的一些利与弊:</p><ul class=""><li id="4f76" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb kr ks kt ku bi translated">注意力计算在技术上是O的n的平方，尽管这并不像看起来那样是个大问题，因为大部分计算时间不是花在执行这些点积上，而是花在网络的所有其他部分上。实际上，就计算成本而言，变压器往往比rnn便宜得多</li><li id="c11e" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">它们实现起来有些复杂，涉及到各种看似怪异的决策，比如位置编码，这使得它们的使用有些棘手。可能需要相当多的超参数调整才能让变形金刚训练良好</li><li id="81e9" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">它们具有更好的长距离连接:输出中的每个位置都通过长度为1的跳跃连接到每个其他输出位置和每个其他输入位置。从计算上来说，在GPU等并行硬件上实现转换器可以让它们运行得更快</li><li id="29e3" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb kr ks kt ku bi translated">实际上，你可以把变形金刚做得比堆叠的rnn深得多。</li></ul><p id="7191" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总的来说，transformer的好处似乎大大超过了它的缺点，在许多实际的序列到序列处理案例中，transformer比RNNs和LSTMs工作得更好。</p><p id="d6dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lg">随时给我留言或:</em></p><ol class=""><li id="f7db" class="km kn hh ig b ih ii il im ip ko it kp ix kq jb lo ks kt ku bi translated">通过<strong class="ig hi"> </strong> <a class="ae jc" href="https://www.linkedin.com/in/samuele-bolotta-841b16160/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae jc" href="https://twitter.com/SamBolotta" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我</li><li id="1477" class="km kn hh ig b ih kv il kw ip kx it ky ix kz jb lo ks kt ku bi translated">跟着我上<a class="ae jc" rel="noopener" href="/@samuelebolotta">媒</a></li></ol><h1 id="1a58" class="ow je hh bd jf ox oy oz jj pa pb pc jn pd pe pf jq pg ph pi jt pj pk pl jw pm bi translated">参考</h1><p id="a4e4" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><a class="ae jc" href="https://www.youtube.com/watch?v=lKRatcD9hEg&amp;list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A&amp;index=14" rel="noopener ugc nofollow" target="_blank"> CS 182:第五讲</a></p><p id="9d2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有图片均取自2021年CS182的莱文。</p><div class="qu qv ez fb qw qx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="qy ab dw"><div class="qz ab ra cl cj rb"><h2 class="bd hi fi z dy rc ea eb rd ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="re l"><h3 class="bd b fi z dy rc ea eb rd ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="rf l"><p class="bd b fp z dy rc ea eb rd ed ef dx translated">medium.com</p></div></div><div class="rg l"><div class="rh l ri rj rk rg rl kj qx"/></div></div></a></div></div></div>    
</body>
</html>