<html>
<head>
<title>Customer personality analysis II: Cluster analysis and customer ranking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">客户个性分析II:聚类分析和客户排名</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/customer-personality-analysis-ii-cluster-analysis-and-customer-ranking-bdad75000b33?source=collection_archive---------4-----------------------#2022-05-27">https://medium.com/mlearning-ai/customer-personality-analysis-ii-cluster-analysis-and-customer-ranking-bdad75000b33?source=collection_archive---------4-----------------------#2022-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8842" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习是一个广泛的研究领域，可以大致分为几个子域，如监督学习、非监督学习、强化学习等等。大多数相关书籍都将大部分内容致力于监督学习，因为这是一个很好理解的领域。监督学习技术通常具有明确的目标(目标函数或成本函数)以及系统的方法来评估训练模型的<em class="jc">实时</em>性能(例如准确度、召回率、精确度等)。).另一方面，无监督学习指的是在没有任何目标或响应变量的情况下揭示数据模式的方法。无监督学习往往更主观，大多数时候是探索性数据分析的一部分。</p><p id="41ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为无监督学习的一个分支，聚类分析(CA)(也称为聚类或数据分割)旨在发现数据中隐藏的子组或片段，使得同一个子组(聚类)中的数据点与分配给其他聚类的数据点相比，彼此更加相似和密切相关。数据对之间的相似性/不相似性/接近性的概念是CA算法的组成部分，通常通过<strong class="ig hi">距离</strong>来测量。两个实例之间的距离越高，相似性度量越低。在我们的例子中，CA被用来将客户分成几个不同的同类组，目的是为每个客户组定制不同的营销活动和广告。</p><p id="a0d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在简要讨论了集群的一般概念之后，让我们看看如何利用CA来执行客户细分。最后，提出了一种多准则决策分析方法——理想解相似优先排序法(TOPSIS ),用于对属于特定类别的客户进行排序。在进入代码之前，善意的提醒:本文的第一部分可以在<a class="ae jd" href="https://jq0112358.medium.com/exploratory-customer-personality-analysis-i-data-visualization-51386c678f92" rel="noopener">这里</a>找到。</p><h1 id="4886" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">加载库包</h1><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="50c9" class="kl jf hh kh b fi km kn l ko kp"><strong class="kh hi">library</strong>(tidyverse)<br/><strong class="kh hi">library</strong>(e1071)   <em class="jc"># for skewness calculation</em><br/><strong class="kh hi">require</strong>(cluster)  <em class="jc"># gower distance matrix and pam algorithm<br/></em><strong class="kh hi">library</strong>(fpc)      <em class="jc"># cluster evaluation metrics</em><br/><strong class="kh hi">library</strong>(mdw)      <em class="jc"># entropy-based feature weights</em><br/><strong class="kh hi">library</strong>(MCDA)      <em class="jc"># TOPSIS</em><br/><strong class="kh hi">library</strong>(FactoMineR)  <em class="jc"># Factor analysis for mixed data (FAMD)</em><br/><strong class="kh hi">library</strong>(factoextra)  # Visualization of factor analysis<br/><strong class="kh hi">library</strong>(knitr)       <em class="jc"># For beatiful table display</em><br/><strong class="kh hi">library</strong>(car)         <em class="jc"># For interactive 3D scatter plot<br/></em><strong class="kh hi">library</strong>(kableExtra)  # Custom html table styles</span></pre><h1 id="f013" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">导入以前保存的数据</h1><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="6f7a" class="kl jf hh kh b fi km kn l ko kp"><em class="jc"># set the working directory</em><br/><strong class="kh hi">setwd</strong>("~/ml projects/customer segmentation")<br/>cust_data = <strong class="kh hi">readRDS</strong>("preprocessed_cust_data.rds")</span></pre><h1 id="434f" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">聚类分析</h1><h2 id="8370" class="kl jf hh bd jg kq kr ks jk kt ku kv jo ip kw kx js it ky kz jw ix la lb ka lc bi translated">利用高尔距离度量计算相异矩阵</h2><p id="ebca" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">高尔距离测量两个数据点的相异度，这两个数据点具有混合的数值和分类数据。我不会在这里谈论高尔距离的公式，但可以在这里找到高尔距离计算的简单示例<a class="ae jd" href="https://jamesmccaffrey.wordpress.com/2020/04/21/example-of-calculating-the-gower-distance/#:~:text=The%20Gower%20distance%20is%20a,numeric%20distance%20between%20data%20items." rel="noopener ugc nofollow" target="_blank"/>。</p><p id="dbb7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于混合数据类型，<code class="du li lj lk kh b">cluster</code>包中的函数<code class="du li lj lk kh b">daisy()</code>可用于计算观察值之间的成对相异度。<code class="du li lj lk kh b">type</code>参数中的<code class="du li lj lk kh b">daisy</code>函数用于指定输入<code class="du li lj lk kh b">cust_data_without_ID</code>中的变量类型。关于<code class="du li lj lk kh b">daisy</code>功能的更多信息可从控制台上的<code class="du li lj lk kh b">help()</code>功能或输入命令<code class="du li lj lk kh b">?daisy</code>获得。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="7782" class="kl jf hh kh b fi km kn l ko kp">idx_col_retain = !(<strong class="kh hi">colnames</strong>(cust_data) %in% <strong class="kh hi">c</strong>("ID", "min_num_household", "tot_AcceptedCmp"))<br/>cust_data_without_ID = cust_data[,idx_col_retain]<br/><br/><em class="jc"># Boolean attributes</em><br/>seq_binary = <strong class="kh hi">c</strong>("Complain", "Response", "Accepted")<br/>idx_binary = <strong class="kh hi">sapply</strong>(seq_binary, <br/>                    <strong class="kh hi">function</strong>(x) <strong class="kh hi">grep</strong>(x, <strong class="kh hi">colnames</strong>(cust_data_without_ID)))<br/>idx_binary = <strong class="kh hi">unlist</strong>(idx_binary)<br/><br/><em class="jc"># continuos attributes</em><br/>cont_features_patterns = <strong class="kh hi">c</strong>("Mnt", "Num", "Income","Recency", "age", <br/>                           "days_enroll") <br/>idx_cont = <strong class="kh hi">sapply</strong>(cont_features_patterns, <br/>                  <strong class="kh hi">function</strong>(x) <strong class="kh hi">grep</strong>(x, <strong class="kh hi">colnames</strong>(cust_data_without_ID)))<br/>idx_cont = <strong class="kh hi">unlist</strong>(idx_cont)<br/><br/>skewness_col = <strong class="kh hi">apply</strong>(cust_data_without_ID[, idx_cont], 2, skewness)<br/>idx_logtrans = idx_cont[<strong class="kh hi">which</strong>(<strong class="kh hi">abs</strong>(skewness_col)&gt;1)]<br/><br/><em class="jc"># Ordinal attributes</em><br/><br/>dissimilarity_matrix = <strong class="kh hi">daisy</strong>(cust_data_without_ID, metric = "gower", <br/>                             type = <strong class="kh hi">list</strong>(ordratio=<strong class="kh hi">grep</strong>("home", <strong class="kh hi">colnames</strong>(cust_data)),<br/>                                         asymm = idx_binary,<br/>                                         logratio = idx_logtrans))</span></pre><h2 id="65b5" class="kl jf hh bd jg kq kr ks jk kt ku kv jo ip kw kx js it ky kz jw ix la lb ka lc bi translated">K-medoid聚类(PAM算法)</h2><p id="aef1" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">有各种聚类方法，如基于划分、基于层次、基于密度和基于模型。</p><p id="ca6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与流行的k-means聚类类似，K-medoid聚类是一种分区聚类方法，其中数据点被分成组，即所谓的聚类。超参数，集群数量由用户选择。然而，k-medoid通常比k-means聚类更健壮，对异常值更不敏感。为什么？答案有两个方面:</p><ul class=""><li id="2a21" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb lq lr ls lt bi translated">k-medoid选择medoids(实际数据点)作为质心(即聚类的中心，而k-means选择平均值作为质心。这类似于中位数和平均数之间的类比，中位数比平均数更稳健，因为中位数的分解值为0.5，而平均数的分解点为1/n。您可以将分解值视为对异常值的抵抗。它越高，统计测量就越稳健。</li><li id="cdfe" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">如果我们观察如下所示的两种聚类方法的成本函数，我们会注意到k-means的成本函数实际上在聚类平方和内，而k-medoid的距离函数d(x，z)是任意的。L2范数代价函数会受到异常值的影响。</li></ul><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es lz"><img src="../Images/79497e3c2f7e2aaf051d66dfc96004e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*JJykyaKlfuNrts3eVw6ZYQ.png"/></div></figure><p id="6da7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本研究中，k-medoid聚类选择了基于medoids的划分(PAM)算法。该算法与k-means算法非常相似:1)初始化:随机选择k个数据点作为中间点，<em class="jc"> m </em> 2)将每个非中间点，<em class="jc"> o </em>分配给最近的中间点，3)对于每个<em class="jc">m</em>o和<em class="jc"> o，</em>swap<em class="jc">m</em>o和<em class="jc"> o，重复步骤2至3，直到代价函数停止下降。</em></p><p id="d3b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">选择最佳聚类数，k </strong></p><p id="1b12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在决定了使用哪种聚类方法之后，现在的问题是我们如何度量为不同的k生成的聚类的“良好性”?答案是通过集群验证指数(内部或外部)。</p><ol class=""><li id="f02a" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb md lr ls lt bi translated">内部:聚类中的典型目标函数将获得高的类内相似性(紧密性)和低的类间相似性(分离性)的目标形式化。相似性是两个数据实例相似程度的数值度量，通常通过距离度量(欧几里德距离)来量化。两个数据点之间的距离越大，相似度越低。</li><li id="2533" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb md lr ls lt bi translated">外部:应该有基本事实或黄金标准来评估聚类结果与基本事实标签的匹配程度。</li></ol><p id="f5e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于数据不带有任何注释标签，因此，我们只剩下内部评估指标。为了选择最佳k，我将使用在<code class="du li lj lk kh b">fpc</code>包中找到的内部聚类评估指标，例如<em class="jc">在聚类平方和(肘方法)</em>、<em class="jc">平均轮廓宽度</em>、<em class="jc">卡林斯基-哈拉巴斯指数</em>和<em class="jc">邓恩指数。</em></p><p id="6cf5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最佳k可以从平方和对k的曲线的拐点(平方和开始稳定下降的点)选择；对于平均轮廓宽度、CH指数和Dunn指数的情况，我们试图最大化验证指数，因此在实验的k值范围内，当这些指数最大时，k值最佳。我没有在这里显示公式，但为了清楚起见，关键是平均轮廓宽度、CH指数和Dunn指数是通过考虑数据点之间的成对距离得出的标量值。对于希望深入了解的感兴趣的读者来说，这些指数的公式列在这篇<a class="ae jd" href="http://datamining.rutgers.edu/publication/internalmeasures.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="94a1" class="kl jf hh kh b fi km kn l ko kp"><em class="jc"># Possible number of clusters are assumed to be in the range of 2 to 8. </em><br/>k_array = <strong class="kh hi">seq</strong>(from = 2, to = 8, by=1)<br/>cluster_eval_df = <strong class="kh hi">data.frame</strong>(<strong class="kh hi">matrix</strong>(, nrow = <strong class="kh hi">length</strong>(k_array), ncol = 4))<br/><strong class="kh hi">colnames</strong>(cluster_eval_df) = <strong class="kh hi">c</strong>("silhouette", "CH_index", "Dunn_index", "wc_SOS")<br/>cluster_eval_df$k = k_array<br/><br/><strong class="kh hi">for</strong> (i <strong class="kh hi">in</strong> (1:<strong class="kh hi">length</strong>(k_array))){<br/>  <strong class="kh hi">set.seed</strong>(i+100)<br/>  kmedoid = <strong class="kh hi">pam</strong>(dissimilarity_matrix, k = k_array[i], diss = TRUE,<br/>                nstart = 10)<br/>  <em class="jc"># set diss to TRUE, and set the number of random start as 10.</em><br/>  clust_stat = <strong class="kh hi">cluster.stats</strong>(dissimilarity_matrix, <br/>                             clustering = kmedoid$clustering)<br/>  cluster_eval_df[i,"silhouette"] = clust_stat$avg.silwidth<br/>  cluster_eval_df[i, "CH_index"] = clust_stat$ch<br/>  cluster_eval_df[i, "Dunn_index"] = clust_stat$dunn<br/>  <em class="jc"># Add in the within cluster sum of squares</em><br/>  cluster_eval_df[i, "wc_SOS"] = clust_stat$within.cluster.ss<br/>}<br/><br/><em class="jc"># Line plot for all evaluation metrics (internal cluster evaluation)</em><br/>cluster_eval_df %&gt;% <br/>  <strong class="kh hi">pivot_longer</strong>(!k, names_to = "cluster_eval", values_to = "value") %&gt;% <br/>  <strong class="kh hi">ggplot</strong>(<strong class="kh hi">aes</strong>(x = k, y = value)) +<br/>  <strong class="kh hi">geom_line</strong>() + <strong class="kh hi">geom_point</strong>() + <strong class="kh hi">facet_grid</strong>(rows = <strong class="kh hi">vars</strong>(cluster_eval), <br/>                                          scales = "free_y")</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es me"><img src="../Images/3ff0fd2ff83f3b2d2c66032d57a19296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h53KBIh5tJyIGxjwRqflqQ.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx">Line plots of multiple cluster evaluation metrics for different number of clusters, k. The best k is 3, as suggested by CH index and Dunn index. The possible number of clusters are assumed to be within 2 to 8.</figcaption></figure><p id="2187" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们通过将k参数作为3传递来运行PAM算法。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="6666" class="kl jf hh kh b fi km kn l ko kp"><em class="jc"># find the best_k from stackoverflow using the mode function found on</em><br/><em class="jc"># https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode?page=1&amp;tab=scoredesc#tab-top</em><br/>mode = <strong class="kh hi">function</strong>(x){<br/>  ux = <strong class="kh hi">unique</strong>(x)<br/>  ux[<strong class="kh hi">which.max</strong>(<strong class="kh hi">tabulate</strong>(<strong class="kh hi">match</strong>(x, ux)))]<br/>}<br/>best_k = <strong class="kh hi">mode</strong>(<strong class="kh hi">c</strong>(<strong class="kh hi">which.max</strong>(cluster_eval_df$silhouette), <br/>                <strong class="kh hi">which.max</strong>(cluster_eval_df$CH_index),<br/>                <strong class="kh hi">which.max</strong>(cluster_eval_df$Dunn_index))) + 1</span><span id="ae27" class="kl jf hh kh b fi mn kn l ko kp"><strong class="kh hi">set.seed</strong>(100 + best_k - 1)  # for reproducibility<br/>kmedoid = <strong class="kh hi">pam</strong>(dissimilarity_matrix, k = best_k, diss = TRUE, nstart = 10)   # set nstart as 10 for cluster stability<br/><em class="jc"># Save the cluster label in the dataframe. Change it to factor to facilitate data wrangling.</em><br/>cust_data$cluster_label = <strong class="kh hi">as.factor</strong>(kmedoid$clustering)</span></pre><h2 id="fcc0" class="kl jf hh bd jg kq kr ks jk kt ku kv jo ip kw kx js it ky kz jw ix la lb ka lc bi translated">不同客户群特征的统计摘要</h2><ol class=""><li id="5b53" class="ll lm hh ig b ih ld il le ip mo it mp ix mq jb md lr ls lt bi translated">每个集群中的客户数量。</li></ol><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="c0a5" class="kl jf hh kh b fi km kn l ko kp"><em class="jc">#1: Number of observations (customers)</em><br/>cust_data %&gt;% <strong class="kh hi">group_by</strong>(cluster_label) %&gt;% <br/>  <strong class="kh hi">summarise</strong>(n = <strong class="kh hi">n</strong>()) %&gt;% <strong class="kh hi">kable</strong>(caption = "Number of observations in each cluster")</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mr"><img src="../Images/68278bece45e0d272fd5f133165ce0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SohMDf0-b4Y8kWtKFxmrSQ.png"/></div></div></figure><p id="68ff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.每个聚类下<em class="jc">数字特征</em>的平均值</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="5a74" class="kl jf hh kh b fi km kn l ko kp"><em class="jc">#2: Average of numerical features for each cluster</em><br/>summary_cont_features_per_cluster = <br/>  cust_data %&gt;% <strong class="kh hi">group_by</strong>(cluster_label) %&gt;% <br/>  <strong class="kh hi">summarise_if</strong>(is.numeric, mean) %&gt;% <strong class="kh hi">select</strong>(-ID)<br/>summary_cont_features_per_cluster</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es ms"><img src="../Images/2734b33b9c984d0b76938b9650947ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*5znmWQNjukOoYsrlDVz4lQ.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx">Screenshot of R console output. The full table can be viewed on <a class="ae jd" href="http://rpubs.com/JQ_programmer_92/907418" rel="noopener ugc nofollow" target="_blank">RPubs</a>.</figcaption></figure><p id="c54d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.每个聚类的<em class="jc">分类特征</em>的分布(比例)。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="af62" class="kl jf hh kh b fi km kn l ko kp"><em class="jc">#3: Distribution of categorical features for each cluster</em><br/>cate_features_names = <strong class="kh hi">names</strong>(<strong class="kh hi">Filter</strong>(is.factor, cust_data))<br/><em class="jc"># Change the second argument of group_by() and third argument aes() to desired categorical feature names: Education, Marital_status, Kidhome, Teenhome.</em><br/>cust_data %&gt;% <strong class="kh hi">select</strong>(<strong class="kh hi">one_of</strong>(cate_features_names)) %&gt;% <br/>  <strong class="kh hi">group_by</strong>(cluster_label, Teenhome) %&gt;% <strong class="kh hi">summarise</strong>(n = <strong class="kh hi">n</strong>()) %&gt;% <br/>  <strong class="kh hi">mutate</strong>(prop = (n/<strong class="kh hi">sum</strong>(n))) %&gt;% <br/>  <strong class="kh hi">ggplot</strong>(<strong class="kh hi">aes</strong>(x = cluster_label, y = prop, fill = Teenhome)) +<br/>  <strong class="kh hi">geom_bar</strong>(stat = "identity", position = <strong class="kh hi">position_dodge</strong>()) +<br/>  <strong class="kh hi">geom_text</strong>(<strong class="kh hi">aes</strong>(label = <strong class="kh hi">round</strong>(prop, 2)), vjust = 1.5, size = 2,<br/>            position = <strong class="kh hi">position_dodge</strong>(0.9)) +<br/>  <strong class="kh hi">theme_minimal</strong>()</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mt"><img src="../Images/3ff627d5f597aa74b8b6985d881f4ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tI4VcT8C3bAkMoKju_5IdA.png"/></div></div></figure><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es mu"><img src="../Images/999ed3b9eab966507600180d0f228e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*kQ8WCc7RSjHroc3OLw8dmQ.png"/></div></figure><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mv"><img src="../Images/239957008c38cf2ba1a28e03ea83b977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*90a1eF6BLxreBYBKyRChTQ.png"/></div></div></figure><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es mw"><img src="../Images/bfb078dddca1206ed3570f3228196ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*WnrnvCQiYwc0DjrraQin7w.png"/></div></figure><p id="7663" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.每个聚类的派生属性(例如，最小家庭成员数)的分布。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="621c" class="kl jf hh kh b fi km kn l ko kp">cust_data$min_num_household = <strong class="kh hi">factor</strong>(cust_data$min_num_household, ordered = TRUE)<br/>cust_data %&gt;% <strong class="kh hi">group_by</strong>(cluster_label, min_num_household) %&gt;% <strong class="kh hi">summarise</strong>(n = <strong class="kh hi">n</strong>()) %&gt;% <br/>  <strong class="kh hi">mutate</strong>(prop = (n/<strong class="kh hi">sum</strong>(n))) %&gt;% <br/>  <strong class="kh hi">ggplot</strong>(<strong class="kh hi">aes</strong>(x = cluster_label, y = prop, fill = min_num_household)) +<br/>  <strong class="kh hi">geom_bar</strong>(stat = "identity", position = <strong class="kh hi">position_dodge</strong>()) +<br/>  <strong class="kh hi">geom_text</strong>(<strong class="kh hi">aes</strong>(label = <strong class="kh hi">round</strong>(prop, 2)), vjust = 1.5, size = 2,<br/>            position = <strong class="kh hi">position_dodge</strong>(0.9)) +<br/>  <strong class="kh hi">theme_minimal</strong>()</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/b08dfc12eab74a6bf44ccba3c145e740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mq9bjqCTQJ0bLY52qw3QUw.png"/></div></div></figure><p id="652c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">5.为每个分类列出二进制(布尔型)变量。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="efdf" class="kl jf hh kh b fi km kn l ko kp">table1 = <br/>  cust_data %&gt;% <br/>  <strong class="kh hi">group_by</strong>(cluster_label, Complain) %&gt;% <strong class="kh hi">summarise</strong>(n = <strong class="kh hi">n</strong>()) %&gt;% <br/>  <strong class="kh hi">mutate</strong>(prop = (n/<strong class="kh hi">sum</strong>(n))) %&gt;% <strong class="kh hi">select</strong>(-n)<br/><strong class="kh hi">colnames</strong>(table1)[2] = "Binary_outcomes"<br/><br/><em class="jc">#pattern = c("Complain", "Response", "Accept", "cluster")</em><br/><em class="jc">#idx_select = lapply(pattern, function (x) grep(x, colnames(cust_data)))</em><br/><em class="jc">#idx = unlist(idx_select)</em><br/>cust_data_1 = cust_data %&gt;% <br/>  <strong class="kh hi">select</strong>(<strong class="kh hi">contains</strong>("Response") | <strong class="kh hi">contains</strong>("Accept") | <strong class="kh hi">contains</strong>("cluster"))<br/><br/>n_col = <strong class="kh hi">ncol</strong>(cust_data_1)<br/><strong class="kh hi">for</strong> (i <strong class="kh hi">in</strong> (1:(n_col-2))){<br/>  table2 = cust_data_1 %&gt;%  <strong class="kh hi">select</strong>(<strong class="kh hi">c</strong>(n_col, <strong class="kh hi">all_of</strong>(i))) %&gt;% <br/>    <strong class="kh hi">group_by_all</strong>() %&gt;% <br/>    <strong class="kh hi">summarise</strong>(n = <strong class="kh hi">n</strong>()) %&gt;% <br/>    <strong class="kh hi">mutate</strong>(prop = (n/<strong class="kh hi">sum</strong>(n))) %&gt;% <br/>    <strong class="kh hi">select</strong>(-n)<br/>  <strong class="kh hi">colnames</strong>(table2)[2] = "Binary_outcomes"<br/>  table1 = table1 %&gt;% <br/>    <strong class="kh hi">left_join</strong>(table2, by = <strong class="kh hi">c</strong>("cluster_label" = "cluster_label", <br/>                             "Binary_outcomes" = "Binary_outcomes"))<br/>}<br/>oldname = <strong class="kh hi">colnames</strong>(table1)[3:<strong class="kh hi">length</strong>(<strong class="kh hi">colnames</strong>(table1))]<br/>newname = <strong class="kh hi">c</strong>("Complain", <br/>            <strong class="kh hi">colnames</strong>(cust_data_1)[-<strong class="kh hi">length</strong>(<strong class="kh hi">colnames</strong>(cust_data_1))])<br/><em class="jc"># Rename</em><br/><strong class="kh hi">for</strong> (i <strong class="kh hi">in</strong> (1:<strong class="kh hi">length</strong>(newname))){<br/>  <strong class="kh hi">names</strong>(table1)[<strong class="kh hi">names</strong>(table1) == oldname[i]] = newname[i]<br/>}<br/>table1 %&gt;% as_tibble()</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es my"><img src="../Images/5045cdd479bdf64e9729b2d7ca341997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*nns_M_8cay_mI-MgNRbROA.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx">Screenshot of R console output. The full table can be viewed on <a class="ae jd" href="http://rpubs.com/JQ_programmer_92/907418" rel="noopener ugc nofollow" target="_blank">RPubs</a>.</figcaption></figure><p id="6db8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上述统计概要中推断出的每个聚类的特征被列表为below⁴:</p><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mz"><img src="../Images/e9da3e13c217f6f929724f79406b4181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPXbWdTDU1BRR-wHa8nEHg.png"/></div></div></figure><h2 id="bff3" class="kl jf hh bd jg kq kr ks jk kt ku kv jo ip kw kx js it ky kz jw ix la lb ka lc bi translated">混合数据的主成分分析</h2><p id="535b" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">主成分分析主要用于降维，以便我们可以在低维空间中可视化数据。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="d076" class="kl jf hh kh b fi km kn l ko kp">res.famd = <strong class="kh hi">FAMD</strong>(cust_data_without_ID, graph = F)<br/><br/>eig.value = <strong class="kh hi">get_eigenvalue</strong>(res.famd)<br/><strong class="kh hi">head</strong>(eig.value)<br/><strong class="kh hi">fviz_screeplot</strong>(res.famd)</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es na"><img src="../Images/866c27f972438bec0652da0452fdffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUOIbHb2_Y5zX95ocGJknw.png"/></div></div></figure><figure class="kc kd ke kf fd ma er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es nb"><img src="../Images/cd5199acbe47387006ee9c030dc8993c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dzw2CduA0lsN9lb897GBlg.png"/></div></div></figure><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="4b40" class="kl jf hh kh b fi km kn l ko kp"><em class="jc"># predict to get the transformed feature space and plot on 3 dimensional scatter plot</em><br/>transformed_data = <strong class="kh hi">predict</strong>(res.famd, cust_data_without_ID)</span><span id="5915" class="kl jf hh kh b fi mn kn l ko kp"><strong class="kh hi">library</strong>(rgl)<br/><strong class="kh hi">scatter3d</strong>(x = transformed_data$coord[,1], y = transformed_data$coord[,2],<br/>          z = transformed_data$coord[,3], <br/>          groups = cust_data$cluster_label, grid = FALSE, <br/>          surface = FALSE, ellipsoid = TRUE, <br/>          surface.col = <strong class="kh hi">c</strong>("#80FF00", "#009999", "#FF007F"))</span></pre><p id="d121" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果运行上述代码块，Rstudio中将出现一个弹出窗口，您可以自由旋转交互式3D散点图，并从不同角度查看该图。如果您以html格式打开R markdown代码，请确保您的浏览器启用了WebGL。关于如何在不同浏览器上启用WebGL，可以参考这篇<a class="ae jd" href="https://www.picmonkey.com/help/errors-and-troubleshooting/crashes-and-performance/how-to-enable-webgl#:~:text=Microsoft%20Edge-,Enable%20WebGL%20status,so%20the%20change%20takes%20effect." rel="noopener ugc nofollow" target="_blank">文章</a>。</p><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es nc"><img src="../Images/925ee8d3d8b88426acff506f0ce95028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*_1pSEQax1ZIJAq4jVrJ1ew.png"/></div></figure><p id="5f35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，散点图(以转换维度表示的数据)仅代表原始数据变化的大约30%。因此，应谨慎使用。此外，3个集群之间有高度的重叠。</p><h1 id="63c1" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">特定聚类(客户群)的客户排名</h1><p id="5781" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">根据帕累托法则(又称80-20法则)，五分之一(20%)的客户贡献了公司收入的80%。因此，识别最有价值的客户对于公司分配更多资源来培养与这些客户的密切关系至关重要。</p><h2 id="da22" class="kl jf hh bd jg kq kr ks jk kt ku kv jo ip kw kx js it ky kz jw ix la lb ka lc bi translated">相似优先排序技术</h2><p id="67e6" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">TOPSIS是多准则决策分析(MCDA)工具之一，用于根据多个准则(选择的特征集)对备选方案(客户)进行排序或选择。从直观上讲，与正理想解距离短、与负理想解距离大的备选方案应授予高分。你可以在这个<a class="ae jd" href="https://en.wikipedia.org/wiki/TOPSIS#:~:text=The%20Technique%20for%20Order%20of,Lai%20and%20Liu%20in%201993." rel="noopener ugc nofollow" target="_blank">维基页面</a>上找到详细的TOPSIS算法描述，所以我在这里就跳过细节了。实施TOPSIS的底线是:</p><ul class=""><li id="305b" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb lq lr ls lt bi translated">您可以设置要素权重，使某些属性优先于其他属性。如果你是一个专家，知道每个特征的重要性，你可以自己设置特征权重。本实验采用熵权法。主要思想是分散程度越高，可以提取的信息就越多，因此该特定特征将被赋予更高的权重。</li><li id="4f29" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">输出分数范围从0到1，接近1的值是最佳选择。</li></ul><p id="0971" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我们有兴趣对第一个集群中的客户进行排序，</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="4ebc" class="kl jf hh kh b fi km kn l ko kp"><em class="jc"># Calculate feature weights for continuous and categorical features for first cluster</em><br/>i = 1<br/>data_analysis_cont = cust_data %&gt;% <strong class="kh hi">filter</strong>(cluster_label==i) %&gt;% dplyr::<strong class="kh hi">select</strong>(<strong class="kh hi">starts_with</strong>(<strong class="kh hi">c</strong>("Mnt", "Num")))<br/>data_analysis_cate = cust_data %&gt;% <strong class="kh hi">filter</strong>(cluster_label==i) %&gt;% <br/>  dplyr::<strong class="kh hi">select</strong>(<strong class="kh hi">starts_with</strong>(<strong class="kh hi">c</strong>("Accepted", "Response")))<br/><br/>h_cont = <strong class="kh hi">get.bw</strong>(<strong class="kh hi">scale</strong>(data_analysis_cont), bw = "nrd", nb = 'na')<br/>w_cont = <strong class="kh hi">entropy.weight</strong>(<strong class="kh hi">scale</strong>(data_analysis_cont), h = h_cont)<br/>w_cate = <strong class="kh hi">entropy.weight</strong>(data_analysis_cate, h='na')</span></pre><p id="0acf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面的代码片段可能需要一些时间来执行。</p><pre class="kc kd ke kf fd kg kh ki kj aw kk bi"><span id="94a5" class="kl jf hh kh b fi km kn l ko kp">data_analysis_cate &lt;- <strong class="kh hi">lapply</strong>(data_analysis_cate, <br/>                             <strong class="kh hi">function</strong>(x) <strong class="kh hi">as.numeric</strong>(<strong class="kh hi">as.character</strong>(x)))<br/>data_analysis = <strong class="kh hi">cbind</strong>(data_analysis_cont, data_analysis_cate)<br/><br/>feat_imp = <strong class="kh hi">c</strong>(w_cont, w_cate)<br/>overall = <strong class="kh hi">TOPSIS</strong>(data_analysis, <br/>                 feat_imp, <br/>                 criteriaMinMax = <strong class="kh hi">rep</strong>("max", <strong class="kh hi">length</strong>(feat_imp)))<br/>data_analysis$topsis_score = overall<br/>data_analysis$ID = cust_data %&gt;% <strong class="kh hi">filter</strong>(cluster_label==i) %&gt;% <strong class="kh hi">select</strong>(ID)<br/><br/><em class="jc"># Top 10 customers for cluster 1</em><br/>data_analysis %&gt;% <strong class="kh hi">arrange</strong>(<strong class="kh hi">desc</strong>(topsis_score)) %&gt;% <strong class="kh hi">as_tibble</strong>() %&gt;% <strong class="kh hi">head</strong>(10) %&gt;%  <strong class="kh hi">relocate</strong>(ID, topsis_score)</span></pre><figure class="kc kd ke kf fd ma er es paragraph-image"><div class="er es nd"><img src="../Images/e829bfec02615f2ea9ca3a5511881510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*dYeuDPWokcJuVzPeSjntEg.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx">Screenshots of R console. The full table can be found on <a class="ae jd" href="http://rpubs.com/JQ_programmer_92/907418" rel="noopener ugc nofollow" target="_blank">RPubs</a>.</figcaption></figure><h1 id="9481" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><p id="d9dc" class="pw-post-body-paragraph ie if hh ig b ih ld ij ik il le in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">让我们简要介绍一下这个实验的工作流程:</p><ol class=""><li id="31f5" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb md lr ls lt bi translated">数据采集。下载数据并导入到R workspace。</li><li id="f7f7" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb md lr ls lt bi translated">数据预处理。<br/> -去除零方差变量。<br/> -丢弃异常值/无效观察值。<br/> -特征提取。<br/> -缺失值插补。</li><li id="690c" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb md lr ls lt bi translated">数据可视化。</li><li id="1896" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb md lr ls lt bi translated">基于k-medoid聚类的聚类分析。</li><li id="75e1" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb md lr ls lt bi translated">TOPSIS客户排名。</li></ol><p id="a694" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我想强调一下这种客户细分方法的一些缺点:</p><ul class=""><li id="fb51" class="ll lm hh ig b ih ii il im ip ln it lo ix lp jb lq lr ls lt bi translated">这不是增量学习。如果我们想对新客户进行细分或分组，我们需要对整个数据集(包括新数据)运行聚类模型。</li><li id="122f" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">其他聚类方法可能无法重现这些结果。集群的稳定性和一致性一直是CA的一个固有问题。对于这个实验，我采用了基于分区的聚类方法，但是不能保证这是最好的方法。更好的方法是尝试具有不同假设的其他聚类方法，并比较聚类结果。</li></ul><p id="ec35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值得注意的是，客户细分模型应该不断地被监控和评估(迭代过程),以确保其有效性和稳健性，因为客户的行为会随着时间的推移而演变。</p><p id="eb4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，本文中的所有代码都可以在<a class="ae jd" href="https://github.com/Jacky-lim-data-analyst/programmer.git" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae jd" href="http://rpubs.com/JQ_programmer_92/907418" rel="noopener ugc nofollow" target="_blank"> RPubs </a>上找到。感谢阅读！</p></div><div class="ab cl ne nf go ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ha hb hc hd he"><p id="f8cb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]:詹姆斯，g .，威滕，d .，哈斯蒂，t .，蒂布拉尼，R. (2021)。引言。统计学习导论。统计学中的斯普林格文本。纽约州纽约市斯普林格。</p><p id="0cfd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]: K-medoid问题是NP-hard问题，PAM算法是一种启发式算法。</p><p id="1603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]:除了考虑集群内和集群间相似性的传统验证度量之外，<a class="ae jd" href="https://indigo.uic.edu/articles/journal_contribution/Selection_of_the_number_of_clusters_via_the_bootstrap_method/10773005/1" rel="noopener ugc nofollow" target="_blank">集群稳定性</a>和<a class="ae jd" href="https://statweb.stanford.edu/~gwalther/predictionstrength.pdf" rel="noopener ugc nofollow" target="_blank">预测强度</a>是一些其他可行的选项。</p><p id="e16a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]:每个集群的特征适用于大多数集群成员，而不是所有成员。</p><div class="nl nm ez fb nn no"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hi fi z dy nt ea eb nu ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc mb no"/></div></div></a></div></div></div>    
</body>
</html>