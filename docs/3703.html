<html>
<head>
<title>VGG — Very Deep Convolution Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VGG——深度卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/vgg-very-deep-convolution-neural-network-4a544fb9fd9b?source=collection_archive---------6-----------------------#2022-10-10">https://medium.com/mlearning-ai/vgg-very-deep-convolution-neural-network-4a544fb9fd9b?source=collection_archive---------6-----------------------#2022-10-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1ee7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我们将学习VGG模型的基本原理。</p><h2 id="57eb" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">背景</h2><p id="15c4" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">2011年计算机视觉竞赛ImageNet大规模视觉识别挑战赛(ILSVRC)的启动为计算机视觉任务领域的创新铺平了道路。使用ImageNet <a class="ae kc" href="https://www.image-net.org/download.php" rel="noopener ugc nofollow" target="_blank">数据集</a>，基于CNN的网络<a class="ae kc" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet </a>由Alex Krizhevsky于2012年提出。该网络以15.3% 的<a class="ae kc" href="https://en.wikipedia.org/wiki/AlexNet" rel="noopener ugc nofollow" target="_blank">前5名错误率赢得了当年的比赛。在接下来的几年里，卡伦·西蒙扬和安德鲁·齐泽曼提出了VGG网络的想法，并在2014年的ImageNet挑战赛中提交了基于该想法的实际模型，并获得了亚军。他们以他们所在的牛津大学视觉几何组的名字给这个模型命名为VGG。</a></p><h2 id="9ab1" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">什么是VGG？</h2><p id="95fd" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">VGG是一种标准的卷积神经网络(CNN)架构，具有多层，旨在通过增加CNN的深度来提高分类精度。“深度”是指由16和19个卷积层组成的VGG16或VGG19的卷积层数。VGG在ILSVRC数据集上接受训练，该数据集包括1000个类别的图像，分为三组，分别为130万个训练图像、10万个测试图像和5万个验证图像。该模型在ImageNet挑战中获得了92.7%的测试准确率。</p><p id="53d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">与之前的车型有何不同？</strong></p><p id="75a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与AlexNet和ZFNet等以前的模型中的大感受野相比，VGG使用了一个非常小的3×3感受滤镜(仍然可以捕捉左/右和上/下的最小尺寸)，跨度为1个像素；相比之下，Alex net使用了一个跨度为4的11×11滤镜，而ZFNet使用了一个大小为4的7×7滤镜。</p><p id="6022" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">拥有统一的3x3过滤器背后的想法是，它们可以复制AlexNet和ZFNet中使用的可变大小过滤器。例如，两个连续的3×3滤光器提供5×5的有效感受野。同样，三个3 x3滤镜相当于一个7x7滤镜。Aqeel Anwar的博客很好地解释了上述概念。</p><p id="757c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们检查需要训练的变量的数量。一个5x5卷积层过滤器有25个参数，而两个3x3过滤器总共有18个(=3x3x2)参数。多个3×3滤波器还导致具有多个非线性激活层，这使得模型更具区分性，并且使得网络能够更快地收敛。</p><h2 id="5c07" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">VGG配置</h2><p id="5b2a" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">作者提出了各种配置(VGG16、VGG19等。)基于网络的深度。这些配置的基本构建模块是一个由多个卷积层组成的堆栈，滤波器大小为3×3，跨距为1，填充为1，后面是一个大小为2×2的最大池层。卷积堆栈之后是三个完全连接的层。前两个大小为4096，最后一个大小为1000，是具有softmax激活功能的输出层。1，000的大小指的是ImageNet中可能的类的总数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/9f1778dad111990d3e485880eb627bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0VVNi2QX9GTc6y5W.jpg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Source: <a class="ae kc" href="https://viso.ai/" rel="noopener ugc nofollow" target="_blank">https://viso.ai/</a></figcaption></figure><p id="9630" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> VGG16架构</strong></p><p id="d613" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在所有配置中，VGG16被认为是ImageNet数据集上性能最好的模型。在本节中，我们将回顾这种配置的实际架构。</p><ul class=""><li id="cf4c" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated"><strong class="ig hi">输入:</strong>大小为224x224像素，3通道(RGB)的图像。唯一需要的预处理是通过从每个像素中减去平均值来标准化RGB值。</li><li id="09e5" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">卷积层:</strong>VGG的卷积层使用尺寸为3×3的非常小的接收滤波器，这是可以捕捉上下和左右的最小可能尺寸。还有<a class="ae kc" href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215#6238" rel="noopener" target="_blank"> 1x1卷积滤波器</a>(作为输入的线性变换)，后面是非线性激活函数ReLU，允许网络学习更复杂的函数。</li><li id="c8bb" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">全连接层:</strong> VGG有3个全连接层，前两层各有4096个通道，第三层有1000个通道，每个类别一个通道。</li><li id="a40a" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><strong class="ig hi">隐藏层:</strong>VGG网络中的所有隐藏层都使用ReLU。</li></ul><p id="a34f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">它是如何工作的？</strong></p><p id="6682" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">大小为224×224的输入图像通过第一叠2个卷积层，每个卷积层具有64个3×3的非常小感受野的滤波器，随后是ReLU激活。卷积的步距固定为1个像素，填充为1个像素。直到这个阶段，空间分辨率没有变化，并且特征图的大小与输入图像的大小相同。然后，特征地图通过2×2窗口上的max-pooling层，步长为2个像素，将特征地图的大小缩小一半。在第一叠层的末端的激活或特征的尺寸被减小到输入图像112×112×64的一半。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lh"><img src="../Images/98f27a1beab87127dc8c52a48a1d831a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_iMNNGBkfS2m4owUo_yxWg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">VGG16 architecture</figcaption></figure><p id="2f86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，生成的特征图随后流过具有128个过滤器(而不是第一层中的64个)的类似的第二层。结果这一层最后的输出进一步减半，变成了56×56×128。接下来是第三个堆栈，包含三个卷积层和一个最大池层。由于在此步骤中应用的过滤器数量是256，因此堆栈的输出大小是28×28×256。随后是两叠三个卷积层，每叠包含512个滤波器。这些堆栈末尾的输出是7x7x512。</p><p id="ea80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">卷积层叠层之后是三个完全连接的层以及一个展平层。前两层各有4，096个神经元，最后一个完全连接的层是输出层，有1，000个神经元，对应于ImageNet数据集中存在的一千个类。输出图层之后是用于类别分类的softmax激活图层。</p><p id="23d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图显示了VGG16模型的结构细节，解释了输入和输出的大小、内核和步幅的大小，以及每一步中参数的<a class="ae kc" href="https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d#8028" rel="noopener" target="_blank">数量。</a></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es li"><img src="../Images/cdd6378f05226ffd04b8ac238cc29e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzaaJ0CwI8w8K2fJs0dyBg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">VGG16 — Structural details</figcaption></figure><h2 id="5146" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak">挑战</strong></h2><p id="3d7d" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">尽管VGG是一个非常简单、优雅且易于使用的模型，但也有一些与之相关的挑战。VGG超过138M，模型的大小超过533 MB。这使得实施VGG网络成为一项耗时的任务。其次，网络的深度导致了<a class="ae kc" href="https://towardsdatascience.com/vggnet-vs-resnet-924e9573ca5c" rel="noopener" target="_blank">消失梯度</a>的问题。</p><p id="9e3e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">VGG模型仍然用于几个图像分类问题，但未来更小的网络架构，如谷歌网和挤压网往往是首选。</p><p id="3cea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献:</strong></p><ol class=""><li id="b627" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb lj kz la lb bi translated"><a class="ae kc" href="https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c" rel="noopener" target="_blank">https://towards data science . com/vgg-neural-networks-the-next-step-after-Alex net-3 f 91 fa 9 FFE 2c</a></li><li id="0036" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lj kz la lb bi translated"><a class="ae kc" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf" rel="noopener ugc nofollow" target="_blank">http://cs 231n . Stanford . edu/slides/2017/cs 231n _ 2017 _ lecture 9 . pdf</a></li><li id="4090" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lj kz la lb bi translated"><a class="ae kc" href="https://viso.ai/deep-learning/vgg-very-deep-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">https://viso . ai/deep-learning/vgg-very-deep-convolutionary-networks/</a></li><li id="a9fa" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lj kz la lb bi translated"><a class="ae kc" href="https://www.mygreatlearning.com/blog/introduction-to-vgg16/" rel="noopener ugc nofollow" target="_blank">https://www.mygreatlearning.com/blog/introduction-to-vgg16/</a></li><li id="518d" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lj kz la lb bi translated"><a class="ae kc" href="https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96" rel="noopener" target="_blank">https://towards data science . com/the-w3h-of-Alex net-vggnet-resnet-and-inception-7 baaaecccc 96</a></li></ol><p id="99b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">论文:</strong></p><ol class=""><li id="235b" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb lj kz la lb bi translated"><a class="ae kc" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings . neur IPS . cc/paper/2012/file/c 399862d 3 b 9d 6 b 76 c 8436 e 924 a 68 c 45 b-paper . pdf</a></li><li id="f64c" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb lj kz la lb bi translated"><a class="ae kc" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.1556</a></li></ol><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb kn ln"/></div></div></a></div></div></div>    
</body>
</html>