<html>
<head>
<title>Image Captioning using Deep Learning — With Source Code — Easy Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的图像字幕-带源代码-易于实施</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/image-captioning-using-deep-learning-with-source-code-easy-explanation-3f2021a63f14?source=collection_archive---------0-----------------------#2021-12-30">https://medium.com/mlearning-ai/image-captioning-using-deep-learning-with-source-code-easy-explanation-3f2021a63f14?source=collection_archive---------0-----------------------#2021-12-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0580" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以在今天的博客中，我们将实施图像字幕项目，这是一个非常先进的项目。对于这个用例，我们将结合使用LSTMs和CNN。所以没有任何进一步的原因。</p><p id="6ef3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">点击此处阅读带源代码的整篇文章—</strong>【https://machinelearningprojects.net/image-captioning/ T2】</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/7013a759e8a7dbeb4601af92df167180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*be-fl7zO_QIybEvw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image Captioning</figcaption></figure><h1 id="139e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">让我们开始吧…</h1><h2 id="c782" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤1-导入图像字幕所需的库。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="799a" class="kr ju hh lg b fi lk ll l lm ln">import os<br/>import pickle<br/>import string<br/>import tensorflow<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from keras.layers.merge import add<br/>from keras.models import Model,load_model<br/>from keras.callbacks import ModelCheckpoint<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.utils import to_categorical,plot_model<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.applications.vgg16 import VGG16,preprocess_input<br/>from keras.layers import Input,Dense,LSTM,Embedding,Dropout<br/>from keras.preprocessing.image import img_to_array,load_img<br/>from nltk.translate.bleu_score import sentence_bleu,corpus_bleu<br/>%matplotlib inline</span></pre><h2 id="7657" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">第二步——使用<a class="ae jc" href="https://neurohive.io/en/popular-networks/vgg16/#:~:text=VGG16%20is%20a%20convolutional%20neural%20network%20model%20proposed%20by%20K.&amp;text=Zisserman%20from%20the%20University%20of,images%20belonging%20to%201000%20classes." rel="noopener ugc nofollow" target="_blank"> VGG-16 </a>从图像中提取特征。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="54ac" class="kr ju hh lg b fi lk ll l lm ln">def extract_features(directory):<br/>  model = VGG16()<br/>  model.layers.pop()<br/>  model = Model(inputs=model.inputs,outputs=model.layers[-1].output)<br/>  print(model.summary())<br/>  features = {}<br/>  i=0<br/>  for name in os.listdir(directory):<br/>    print(i)<br/>    img = load_img(directory+'/'+name,target_size=(224,224))<br/>    img = img_to_array(img)<br/>    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))<br/>    img = preprocess_input(img)<br/>    feature = model.predict(img,verbose=0)<br/>    img_id = name.split('.')[0]<br/>    features[img_id] = feature<br/>    i+=1<br/>  return features<br/><br/>directory ='drive/My Drive/image_captioning/Flicker8k/Flicker8k_Dataset'<br/>features = extract_features(directory)</span></pre><h2 id="004b" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤3-加载、清理和保存图像描述。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="3f54" class="kr ju hh lg b fi lk ll l lm ln">def load_description(filename):<br/>  mappings = {}<br/>  file = open(filename,'r')<br/>  content = file.readlines()<br/>  file.close()<br/>  for lines in content:<br/>    tokens = lines.split()<br/>    if len(lines)&lt;2:<br/>      continue<br/>    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]<br/>    image_desc = ' '.join(image_desc)<br/>    if image_id not in mappings:<br/>      mappings[image_id] = []<br/>    mappings[image_id].append(image_desc)<br/>  return mappings<br/><br/><br/>def clean_description(descriptions):<br/>  table = str.maketrans('','',string.punctuation)<br/>  for k,image_descriptions in descriptions.items():<br/>    for i in range(len(image_descriptions)):<br/>      desc = image_descriptions[i]<br/>      desc = desc.split()<br/>      desc = [x.lower() for x in desc]<br/>      desc = [w.translate(table) for w in desc]<br/>      desc = [x for x in desc if len(x)&gt;1]<br/>      desc = [x for x in desc if x.isalpha()]<br/>      image_descriptions[i] = ' '.join(desc)<br/><br/>def create_corpus(descriptions):<br/>  corpus = set()<br/>  for k in descriptions.keys():<br/>    [corpus.update(x.split()) for x in descriptions[k]]<br/>  return corpus<br/><br/>def save_descriptions(desc,filename):<br/>  lines = []<br/>  for k,v in desc.items():<br/>    for description in v:<br/>      lines.append(k+' '+description)<br/>  data = '\n'.join(lines)<br/>  file = open(filename,'w')<br/>  file.write(data)<br/>  file.close()<br/><br/># load all descriptions<br/>filename = 'drive/My Drive/image_captioning/Flicker8k/Flickr8k.token.txt'<br/>descriptions = load_description(filename)<br/>print('Descriptions loaded: ',len(descriptions))<br/><br/># clean the loaded descriptions<br/>clean_description(descriptions)<br/><br/># check the vocabulary length<br/>vocabulary = create_corpus(descriptions)<br/>print('Vocabulary length: ',len(vocabulary))<br/>save_descriptions(descriptions,'drive/My Drive/image_captioning/descriptions.txt')<br/><br/>print('SAVED !!!')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/7638888dd9ca44369c61980ff889f590.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/0*0z-0hfgVo92zvmOU.png"/></div></figure><h2 id="5ba9" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤4-加载训练并测试图像特征和描述。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="9306" class="kr ju hh lg b fi lk ll l lm ln">def load_set_of_image_ids(filename):<br/>  file = open(filename,'r')<br/>  lines = file.readlines()<br/>  file.close()<br/>  image_ids = set()<br/>  for line in lines:<br/>    if len(line)&lt;1:<br/>      continue<br/>    image_ids.add(line.split('.')[0])<br/>  return image_ids<br/><br/>def load_clean_descriptions(all_desc,train_desc_names):<br/>  file = open(all_desc,'r')<br/>  lines = file.readlines()<br/>  descriptions = {}<br/>  for line in lines:<br/>    tokens = line.split()<br/>    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]<br/>    if image_id in train_desc_names:<br/>      if image_id not in descriptions:<br/>        descriptions[image_id] = []<br/>      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'<br/>      descriptions[image_id].append(desc)<br/>  return descriptions<br/><br/>def load_image_features(filename,dataset):<br/>  all_features = pickle.load(open(filename,'rb'))<br/>  features = {k:all_features[k] for k in dataset}<br/>  return features<br/><br/># load train image ids<br/>train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'<br/>train_image_ids = load_set_of_image_ids(train)<br/>print('Training images found: ',len(train_image_ids))<br/><br/># load training descriptions<br/>train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)<br/>print('training descriptions loaded: ',len(train_descriptions))<br/><br/># load training image features<br/>train_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',train_image_ids)<br/>print('training features loaded: ',len(train_features))<br/><br/>train_descriptions</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/d986d2543a0ec91c10dc43afd51ef828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q4MIZ4LBih9vCA_6.png"/></div></div></figure><h2 id="15d8" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">第5步——获得形状描述。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="bf49" class="kr ju hh lg b fi lk ll l lm ln">def to_list(descriptions):<br/>  all_desc_list = []<br/>  for k,v in descriptions.items():<br/>    for desc in v:<br/>      all_desc_list.append(desc)<br/>  return all_desc_list<br/><br/>def tokenization(descriptions):<br/>  # list of all the descriptions<br/>  all_desc_list = to_list(descriptions)  <br/>  tokenizer = Tokenizer()<br/>  tokenizer.fit_on_texts(all_desc_list)<br/>  return tokenizer<br/><br/># create tokenizer<br/>tokenizer = tokenization(train_descriptions)<br/><br/># word index is the dictionary /mappings of word--&gt;integer<br/>vocab_size = len(tokenizer.word_index)+1<br/>print('Vocab size: ',vocab_size)<br/><br/>def max_length(descriptions):<br/>  all_desc_list = to_list(descriptions)<br/>  return (max(len(x.split()) for x in all_desc_list))<br/><br/><br/>def create_sequences(tokenizer,desc_list,max_len,photo):<br/>  X1,X2,y = [],[],[]<br/>  # X1 will contain photo<br/>  # X2 will contain current sequence<br/>  # y will contain one hot encoded next word<br/><br/>  for desc in desc_list:<br/>    # tokenize descriptions<br/>    seq = tokenizer.texts_to_sequences([desc])[0]<br/>    for i in range(1,len(seq)):<br/>      # out seq is basically the next word in the sentence<br/>      in_seq,out_seq = seq[:i],seq[i]<br/>      # pad input sequence<br/>      in_seq = pad_sequences([in_seq],maxlen=max_len)[0]<br/>      # one hot encode output sequence<br/>      out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]<br/>      X1.append(photo)<br/>      X2.append(in_seq)<br/>      y.append(out_seq)<br/>  return np.array(X1),np.array(X2),np.array(y)<br/><br/># maximum length that a description can have OR the biggest description we are having<br/>max_len = max_length(train_descriptions)<br/>print(max_len)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/ace830800fcf274ea1e320e40022ba41.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/0*foSqIUtw4W03qd6g.png"/></div></figure><h2 id="e862" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤6-生成数据和创建模型的功能。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="2c84" class="kr ju hh lg b fi lk ll l lm ln">def data_generator(descriptions,photos,tokenizer,max_len):<br/>  while 1:<br/>    for k,desc_list in descriptions.items():<br/>      photo = photos[k][0]<br/>      in_img,in_seq,out_seq = create_sequences(tokenizer,desc_list,max_len,photo)<br/>      yield[[in_img,in_seq],out_seq]<br/><br/>def define_model(vocab_size, max_length):<br/>    # image features extractor model<br/>    inputs1 = Input(shape=(4096,))<br/>    fe1 = Dropout(0.5)(inputs1)<br/>    fe2 = Dense(256, activation='relu')(fe1)<br/> <br/>    # input sequence model<br/>    inputs2 = Input(shape=(max_length,))<br/>     # embedding(input_dimension,output_dimension,)<br/>     # input dim is always the vocabulary size <br/>    # output dimension tells the size of vector space in which the words will be embedded<br/>    # mask zero is used when the input itself is 0 then to not confuse it with padded zeros it is used as True<br/>    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)<br/>    se2 = Dropout(0.5)(se1)<br/>    se3 = LSTM(256)(se2)<br/><br/>    # decoder model OR output word model<br/>    decoder1 = add([fe2, se3])<br/>    decoder2 = Dense(256, activation='relu')(decoder1)<br/>    outputs = Dense(vocab_size, activation='softmax')(decoder2)<br/><br/>    # tie it together [image, seq] [word]<br/>    model = Model(inputs=[inputs1, inputs2], outputs=outputs)<br/>    model.compile(loss='categorical_crossentropy', optimizer='adam')<br/><br/>    # summarize model<br/>    print(model.summary())<br/>    return model</span></pre><h2 id="1102" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤7-创建和训练图像字幕模型。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="419d" class="kr ju hh lg b fi lk ll l lm ln">model = define_model(vocab_size,max_len)<br/>epochs = 20<br/>steps = len(train_descriptions)<br/>for i in range(epochs):<br/>  generator = data_generator(train_descriptions,train_features,tokenizer,max_len)<br/>  model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)<br/>  model.save('drive/My Drive/image_captioning/model_'+str(i)+'.h5')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/c39be30e4f45ad8ae6669a7211b10a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ern2hBtpR8xIxLhg.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/93f1de165f959109af7233e684ca498a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*zpEWR1qgFbMix2FC.png"/></div></figure><h2 id="264c" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤8-图像字幕模型的预测和评估函数。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="b89b" class="kr ju hh lg b fi lk ll l lm ln">def int2word(tokenizer,integer):<br/>  for word,index in tokenizer.word_index.items():<br/>    if index==integer:<br/>      return word<br/>  return None<br/><br/>def predict_desc(model,tokenizer,photo,max_len):<br/>  in_seq = 'startseq'<br/>  for i in range(max_len):<br/>    seq = tokenizer.texts_to_sequences([in_seq])[0]<br/>    seq = pad_sequences([seq],maxlen=max_len)<br/>    y_hat = model.predict([photo,seq],verbose=0)<br/>    y_hat = np.argmax(y_hat)<br/>    word = int2word(tokenizer,y_hat)<br/>    if word==None:<br/>      break<br/>    in_seq = in_seq+' '+word<br/>    if word=='endseq':<br/>      break<br/>  return in_seq<br/><br/>def evaluate_model(model,descriptions,photos,tokenizer,max_len):<br/>  actual,predicted = [],[]<br/>  for key,desc in descriptions.items():<br/>    y_hat = predict_desc(model,tokenizer,photos[key],max_len)<br/>    references = [d.split() for d in desc]<br/>    actual.append(references)<br/>    predicted.append(y_hat.split())<br/>  print('BLEU-1: %f' %corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))<br/>  print('BLEU-2: %f' %corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))<br/>  print('BLEU-3: %f' %corpus_bleu(actual,predicted,weights=(0.33,0.33,0.33,0)))<br/>  print('BLEU-4: %f' %corpus_bleu(actual,predicted,weights=(0.25,0.25,0.25,0.25)))</span></pre><h2 id="3ba5" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤9-评估图像字幕模型。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="b740" class="kr ju hh lg b fi lk ll l lm ln">####################  load training data (6k)  ##########################<br/>train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'<br/>train_image_ids = load_set_of_image_ids(train)<br/>print('Training images found: ',len(train_image_ids))<br/><br/># load training descriptions<br/>train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)<br/>print('training descriptions loaded: ',len(train_descriptions))<br/><br/>tokenizer = tokenization(train_descriptions)<br/>max_len = max_length(train_descriptions)<br/><br/>####################  load test data  ##########################<br/>test = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.testImages.txt'<br/>test_image_ids = load_set_of_image_ids(test)<br/>print('Test images found: ',len(test_image_ids))<br/><br/># load test descriptions<br/>test_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',test_image_ids)<br/>print('test descriptions loaded: ',len(test_descriptions))<br/><br/># load test image features<br/>test_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',test_image_ids)<br/>print('training features loaded: ',len(test_features))<br/>#################################################################<br/>filename = 'drive/My Drive/image_captioning/model_18.h5'<br/>model = load_model(filename)<br/>evaluate_model(model,test_descriptions,test_features,tokenizer,max_len)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/f574373cf1b979d94f1209099e8ddca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*DqWTUQPxW-Z1bRGR.png"/></div></div></figure><h2 id="510e" class="kr ju hh bd jv ks kt ku jz kv kw kx kd ip ky kz kh it la lb kl ix lc ld kp le bi translated">步骤10 —实时图像字幕。</h2><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="698e" class="kr ju hh lg b fi lk ll l lm ln">img_to_test = 'drive/My Drive/image_captioning/983801190.jpg'<br/>img = plt.imread(img_to_test)<br/>plt.imshow(img)<br/><br/>def extract_features(filename):<br/>    # load the model<br/>    model = VGG16()<br/>    # re-structure the model<br/>    model.layers.pop()<br/>    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)<br/>    # load the photo<br/>    image = load_img(filename, target_size=(224, 224))<br/>    # convert the image pixels to a numpy array<br/>    image = img_to_array(image)<br/>    # reshape data for the model<br/>    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))<br/>    # prepare the image for the VGG model<br/>    image = preprocess_input(image)<br/>    # get features<br/>    feature = model.predict(image, verbose=0)<br/>    return feature<br/><br/># pre-define the max sequence length (from training)<br/>max_length = 34<br/># load the model<br/>model = load_model('drive/My Drive/image_captioning/model_18.h5')<br/># load and prepare the photograph<br/>photo = extract_features(img_to_test)<br/># generate description<br/>description = predict_desc(model, tokenizer, photo, max_length)<br/><br/>description = ' '.join(description.split()[1:-1])<br/>print()<br/>print(description)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/7013a759e8a7dbeb4601af92df167180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*be-fl7zO_QIybEvw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image Captioning</figcaption></figure><p id="961c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果对图片说明有任何疑问，请通过电子邮件或LinkedIn联系我。</p><p id="3bdf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lt">这就是我写给这个博客的全部内容，感谢你的阅读，我希望你在阅读完这篇文章后，能有所收获，直到下一次👋… </em></p><p id="d62b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">还做检查我的其他</strong> <a class="ae jc" href="https://machinelearningprojects.net/machine-learning-projects/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">机器学习项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/deep-learning-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">深度学习项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/opencv-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">计算机视觉项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/flask-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">烧瓶项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/nlp-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">NLP项目</strong> </a> <strong class="ig hi"/></p><p id="9efc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">如需进一步的代码解释和源代码，请访问此处</strong>——【https://machinelearningprojects.net/image-captioning/ T2】</p><p id="180b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="lt">看我之前的帖子:</em> </strong> <a class="ae jc" href="https://machinelearningprojects.net/deep-convolutional-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="lt">利用深度卷积生成对抗网络(DCGAN)生成CIFAR-10伪图像</em> </strong> </a></p><div class="lu lv ez fb lw lx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hi fi z dy mc ea eb md ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">medium.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jn lx"/></div></div></a></div></div></div>    
</body>
</html>