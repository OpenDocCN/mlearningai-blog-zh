# 没有特征工程的巨大 XGBoost 优化

> 原文：<https://medium.com/mlearning-ai/titanic-xgboost-optimization-without-feature-engineering-8639e304ebd9?source=collection_archive---------1----------------------->

![](img/8737907afacac1c582faa22e2a56a6c9.png)

Photo by [Alonso Reyes](https://unsplash.com/@alonsoreyes?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

我认为这个游戏比赛作为初学者的起点是众所周知的。您可以尝试不同的 ML 模型和方法，并比较结果。这将有助于您了解参数的工作原理，比较它们之间的准确性，并了解结果如何与模型复杂性一起评估。有很多关于如何在“泰坦尼克号”数据集上进行 EDA(探索性数据分析)和执行特征工程的文章。尽管如此，最近我得到了一个令人兴奋的任务——在没有任何特征工程的情况下获得高于 0.78 的分数。我做到了。来看看我是怎么做到的吧！

## 数据准备

对于 boosting 模型，我们不需要标准化或缩放数据。增强模型不能“按原样”与文本期货一起工作，所以我们将放弃它们，因为我们不执行任何特征工程。让我们加载数据:

我们加载了数据并删除了不需要的列。在查看“年龄”列时，我们可以看到有一些值缺失。让我们用平均值填充它们(因为在测试中也会有一些缺失值，我们不能简单地删除这些行，因为提交的行数应该是固定的):

由于“幸存”了我们的目标列，我们将特性指定为:

现在，我们可以对我们的分类列执行一键编码:性别、P 级、已登机:

并对测试数据集进行相同的转换:

所以，上面所有代码所做的:

*   加载的测试和训练数据；
*   用平均值填充缺失的年龄值；
*   删除了其他列缺少的值；
*   一键编码我们的分类列；
*   指定了正确的数据类型。

我们的下一步是交叉验证。

## 交叉验证

在交叉验证方面，我们应该为我们的模型选择最佳参数。这是一个二元分类任务，所以我们将使用“误差”作为我们的评估指标。它被计算为#(错误情况)/#(所有情况)，并显示了我们的模型的准确性。让我们导入库并从我们的数据创建数据集:

现在，我们将指定不同的参数，并使用它们运行交叉验证:

```
[0]	train-error:0.14004+0.01089	test-error:0.19233+0.01700
[10]	train-error:0.12767+0.00648	test-error:0.19459+0.01126
[20]	train-error:0.12317+0.00596	test-error:0.19572+0.01258
[30]	train-error:0.12036+0.00790	test-error:0.19347+0.01385
[40]	train-error:0.11867+0.00790	test-error:0.19235+0.01259
[50]	train-error:0.11530+0.00676	test-error:0.19122+0.01409
[60]	train-error:0.11080+0.00692	test-error:0.19460+0.00967
[70]	train-error:0.10855+0.00552	test-error:0.20023+0.00177
[80]	train-error:0.10630+0.00597	test-error:0.20248+0.00304
[90]	train-error:0.10461+0.00234	test-error:0.20136+0.00596
[99]	train-error:0.10068+0.00484	test-error:0.20586+0.00760
```

培训和考试之间的差别是相当大的。这意味着我们应该给我们的参数增加一些正则化。还有。我们将增加早期停止，以避免过度拟合。

```
[0]	train-error:0.31889+0.00824	test-error:0.32507+0.01740
[10]	train-error:0.17323+0.02041	test-error:0.18785+0.01119
[20]	train-error:0.15748+0.00558	test-error:0.18334+0.01363
[25]	train-error:0.15242+0.00881	test-error:0.18672+0.00819
CPU times: user 938 ms, sys: 105 ms, total: 1.04 s
Wall time: 689 ms
```

训练和测试之间的差异减小了。您可以使用不同的参数，尤其是看看“max_depth”并尝试几个值。

然后我们调整参数甚至更多…

```
[0]	train-error:0.28291+0.01604	test-error:0.31496+0.01826
[76]	train-error:0.13330+0.00233	test-error:0.18448+0.00967
```

现在测试和训练中的误差减少了，但它们之间的差异有所增加。此外，早期停止回合等于 50，它建立 76 棵树。这是我们交叉验证的最佳结果。

## 训练和预测

我们可以看到早期停止的最佳迭代是 76。所以，基本上，这是 26 次迭代，因为损失没有减少 50 轮。

```
76
```

这就是为什么，实际上，我决定写这篇文章。如果我们不为预测指定迭代范围，将使用整个模型来使用预测，模型可能会过拟合。

这部分代码提交给 Kaggle 时得分为 0.78468。

当您使用完全相同的参数，但没有“迭代范围”和更新“n 估计值”参数时，得分为 0.78229。可以是细微差别，但这是排行榜的巨大差别。

如果你想查看整个笔记本，请使用这个[链接](https://github.com/varrek/prjct_ml/blob/main/hw8_boosting.ipynb)。它还提供了 LightGBM 和 Catboost 提升算法的示例。

其实有人告诉我，没有任何特征工程也有可能得 0.79 分。如果你成功了，请在评论中告诉我你是如何做到的！

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai 提交建议

### 如何成为 Mlearning.ai 上的作家

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)