<html>
<head>
<title>Generating Cifar-10 Fake Images using Deep Convolutional Generative Adversarial Networks (DCGAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用深度卷积生成对抗网络生成Cifar-10假图像</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/generating-cifar-10-fake-images-using-deep-convolutional-generative-adversarial-networks-dcgan-831cccbb0429?source=collection_archive---------6-----------------------#2022-01-20">https://medium.com/mlearning-ai/generating-cifar-10-fake-images-using-deep-convolutional-generative-adversarial-networks-dcgan-831cccbb0429?source=collection_archive---------6-----------------------#2022-01-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2fa2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，在今天的博客中，我们将看到如何使用<a class="ae jc" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络或DCGANs </a>来构建一些看起来真实的假图像。gan基本上以它们的两个网络而闻名，生成网络和鉴别网络。</p><p id="80f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们以这样一种方式训练我们的辨别模型，它可以告诉我们哪个图像是真的，哪个图像是假的。生成网络试图创造新的图像，甚至可以欺骗鉴别网络，证明自己是真实的。所以没有任何进一步的原因。</p><p id="3b57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">点击此处阅读带源代码的全文—</strong><a class="ae jc" href="https://machinelearningprojects.net/deep-convolutional-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning projects . net/deep-convolutionary-generative-adversarial-networks/</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5c1c6ab5ea55063b4954bc802c128296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*Ufrls7mURYp4gDot.png"/></div></figure><h1 id="37c6" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">让我们开始吧…</h1><h2 id="609d" class="kj jm hh bd jn kk kl km jr kn ko kp jv ip kq kr jz it ks kt kd ix ku kv kh kw bi translated">深度卷积生成对抗网络(DCGAN)的代码…</h2><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="54d2" class="kj jm hh ky b fi lc ld l le lf"># Deep Convolutional GANs<br/><br/># Importing the libraries<br/>from __future__ import print_function<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.parallel<br/>import torch.optim as optim<br/>import torch.utils.data<br/>import torchvision.datasets as dset<br/>import torchvision.transforms as transforms<br/>import torchvision.utils as vutils<br/>from torch.autograd import Variable<br/><br/># Setting some hyperparameters<br/>batchSize = 64 # We set the size of the batch.<br/>imageSize = 64 # We set the size of the generated images (64x64).<br/><br/># Creating the transformations<br/>transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.<br/><br/># Loading the dataset<br/>dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.<br/>dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.<br/><br/># Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.<br/>def weights_init(m):<br/>    classname = m.__class__.__name__<br/>    if classname.find('Conv') != -1:<br/>        m.weight.data.normal_(0.0, 0.02)<br/>    elif classname.find('BatchNorm') != -1:<br/>        m.weight.data.normal_(1.0, 0.02)<br/>        m.bias.data.fill_(0)<br/><br/># Defining the generator<br/><br/>class G(nn.Module): # We introduce a class to define the generator.<br/><br/>    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.<br/>        super(G, self).__init__() # We inherit from the nn.Module tools.<br/>        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).<br/>            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution.<br/>            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.<br/>            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.<br/>            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(256), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(128), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(64), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.<br/>        )<br/><br/>    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.<br/>        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.<br/>        return output # We return the output containing the generated images.<br/><br/># Creating the generator<br/>netG = G() # We create the generator object.<br/>netG.apply(weights_init) # We initialize all the weights of its neural network.<br/><br/># Defining the discriminator<br/><br/>class D(nn.Module): # We introduce a class to define the discriminator.<br/><br/>    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.<br/>        super(D, self).__init__() # We inherit from the nn.Module tools.<br/>        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).<br/>            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.<br/>            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(256), # We normalize again.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(512), # We normalize again.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.<br/>            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.<br/>        )<br/><br/>    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.<br/>        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.<br/>        return output.view(-1) # We return the output which will be a value between 0 and 1.<br/><br/># Creating the discriminator<br/>netD = D() # We create the discriminator object.<br/>netD.apply(weights_init) # We initialize all the weights of its neural network.<br/><br/># Training the DCGANs<br/><br/>criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.<br/>optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.<br/>optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator.<br/><br/>for epoch in range(25): # We iterate over 25 epochs.<br/><br/>    for i, data in enumerate(dataloader, 0): # We iterate over the images of the dataset.<br/>        <br/>        # 1st Step: Updating the weights of the neural network of the discriminator<br/><br/>        netD.zero_grad() # We initialize to 0 the gradients of the discriminator with respect to the weights.<br/>        <br/>        # Training the discriminator with a real image of the dataset<br/>        real, _ = data # We get a real image of the dataset which will be used to train the discriminator.<br/>        input = Variable(real) # We wrap it in a variable.<br/>        target = Variable(torch.ones(input.size()[0])) # We get the target.<br/>        output = netD(input) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errD_real = criterion(output, target) # We compute the loss between the predictions (output) and the target (equal to 1).<br/>        <br/>        # Training the discriminator with a fake image generated by the generator<br/>        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # We make a random input vector (noise) of the generator.<br/>        fake = netG(noise) # We forward propagate this random input vector into the neural network of the generator to get some fake generated images.<br/>        target = Variable(torch.zeros(input.size()[0])) # We get the target.<br/>        output = netD(fake.detach()) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errD_fake = criterion(output, target) # We compute the loss between the prediction (output) and the target (equal to 0).<br/><br/>        # Backpropagating the total error<br/>        errD = errD_real + errD_fake # We compute the total error of the discriminator.<br/>        errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.<br/>        optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.<br/><br/>        # 2nd Step: Updating the weights of the neural network of the generator<br/><br/>        netG.zero_grad() # We initialize to 0 the gradients of the generator with respect to the weights.<br/>        target = Variable(torch.ones(input.size()[0])) # We get the target.<br/>        output = netD(fake) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errG = criterion(output, target) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).<br/>        errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.<br/>        optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.<br/>        <br/>        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps<br/><br/>        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0])) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).<br/>        if i % 100 == 0: # Every 100 steps:<br/>            vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True) # We save the real images of the minibatch.<br/>            fake = netG(noise) # We get our fake generated images.<br/>            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True) # We also save the fake generated images of the minibatch.</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5c1c6ab5ea55063b4954bc802c128296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*Ufrls7mURYp4gDot.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Input Image</figcaption></figure><p id="48da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一个纪元:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/2e89fa3db298c9f27c134985b67dfe84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*EJPXc-AtwFHjYE8T.png"/></div></figure><h2 id="8ee4" class="kj jm hh bd jn kk kl km jr kn ko kp jv ip kq kr jz it ks kt kd ix ku kv kh kw bi translated"><span class="l lk ll lm bm ln lo lp lq lr di"> 2 </span>第二纪元:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/06dd9c3bd124f9408c5ca5ec5c2a6e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*lzwXp7RNgyOw9bYh.png"/></div></figure><p id="8e43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三纪元:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/e271e85b0ef9e81bf90e4fceb60494b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*apNTo6TGcsUNdvk2.png"/></div></figure><p id="30f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第23个纪元:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f2b88d4564941707e0601c545888115b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*y1qVKZtV44Eiu6oh.png"/></div></figure><p id="b81a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第24个纪元:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/257f55e2476d4d43fbc5ea236ddb4527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*71YzohBxEd-Zsf-R.png"/></div></figure><p id="d40a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第25个纪元:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c0bb6617d4a69f3c4c728b6ff82550e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*0vKhEOmNCMAJ4-vl.png"/></div></figure><p id="4646" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果对深度卷积生成对抗网络(DCGAN)有任何疑问，请通过电子邮件或LinkedIn联系我。</p><p id="35fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">阅读更多关于此博客的信息— <a class="ae jc" href="https://machinelearningprojects.net/deep-convolutional-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank"> DCGAN Cifar-10 </a></p><p id="82a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是我写给这个博客的全部内容，感谢你的阅读，我希望你在阅读完这篇文章后会有所收获，直到下次👋… </p><p id="2c87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="ls">看我以前的帖子:</em> </strong> <a class="ae jc" href="https://machinelearningprojects.net/helmet-and-number-plate-detection-and-recognition/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="ls">头盔和号牌检测识别使用YOLOV3 </em> </strong> </a></p><p id="c719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">查看我的其他</strong> <a class="ae jc" href="https://machinelearningprojects.net/machine-learning-projects/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">机器学习项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/deep-learning-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">深度学习项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/opencv-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">计算机视觉项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/nlp-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">NLP项目</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/flask-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">烧瓶项目</strong> </a> <strong class="ig hi"> at </strong> <a class="ae jc" href="https://machinelearningprojects.net/" rel="noopener ugc nofollow" target="_blank"/></p><div class="lt lu ez fb lv lw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lx ab dw"><div class="ly ab lz cl cj ma"><h2 class="bd hi fi z dy mb ea eb mc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="md l"><h3 class="bd b fi z dy mb ea eb mc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="me l"><p class="bd b fp z dy mb ea eb mc ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk jj lw"/></div></div></a></div></div></div>    
</body>
</html>