<html>
<head>
<title>Generating Cifar-10 Fake Images using Deep Convolutional Generative Adversarial Networks (DCGAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åˆ©ç”¨æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”ŸæˆCifar-10å‡å›¾åƒ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/generating-cifar-10-fake-images-using-deep-convolutional-generative-adversarial-networks-dcgan-831cccbb0429?source=collection_archive---------6-----------------------#2022-01-20">https://medium.com/mlearning-ai/generating-cifar-10-fake-images-using-deep-convolutional-generative-adversarial-networks-dcgan-831cccbb0429?source=collection_archive---------6-----------------------#2022-01-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2fa2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">å› æ­¤ï¼Œåœ¨ä»Šå¤©çš„åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨<a class="ae jc" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæˆ–DCGANs </a>æ¥æ„å»ºä¸€äº›çœ‹èµ·æ¥çœŸå®çš„å‡å›¾åƒã€‚ganåŸºæœ¬ä¸Šä»¥å®ƒä»¬çš„ä¸¤ä¸ªç½‘ç»œè€Œé—»åï¼Œç”Ÿæˆç½‘ç»œå’Œé‰´åˆ«ç½‘ç»œã€‚</p><p id="80f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">æˆ‘ä»¬ä»¥è¿™æ ·ä¸€ç§æ–¹å¼è®­ç»ƒæˆ‘ä»¬çš„è¾¨åˆ«æ¨¡å‹ï¼Œå®ƒå¯ä»¥å‘Šè¯‰æˆ‘ä»¬å“ªä¸ªå›¾åƒæ˜¯çœŸçš„ï¼Œå“ªä¸ªå›¾åƒæ˜¯å‡çš„ã€‚ç”Ÿæˆç½‘ç»œè¯•å›¾åˆ›é€ æ–°çš„å›¾åƒï¼Œç”šè‡³å¯ä»¥æ¬ºéª—é‰´åˆ«ç½‘ç»œï¼Œè¯æ˜è‡ªå·±æ˜¯çœŸå®çš„ã€‚æ‰€ä»¥æ²¡æœ‰ä»»ä½•è¿›ä¸€æ­¥çš„åŸå› ã€‚</p><p id="3b57" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç‚¹å‡»æ­¤å¤„é˜…è¯»å¸¦æºä»£ç çš„å…¨æ–‡â€”</strong><a class="ae jc" href="https://machinelearningprojects.net/deep-convolutional-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning projects . net/deep-convolutionary-generative-adversarial-networks/</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5c1c6ab5ea55063b4954bc802c128296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*Ufrls7mURYp4gDot.png"/></div></figure><h1 id="37c6" class="jl jm hh bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">è®©æˆ‘ä»¬å¼€å§‹å§â€¦</h1><h2 id="609d" class="kj jm hh bd jn kk kl km jr kn ko kp jv ip kq kr jz it ks kt kd ix ku kv kh kw bi translated">æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(DCGAN)çš„ä»£ç â€¦</h2><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="54d2" class="kj jm hh ky b fi lc ld l le lf"># Deep Convolutional GANs<br/><br/># Importing the libraries<br/>from __future__ import print_function<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.parallel<br/>import torch.optim as optim<br/>import torch.utils.data<br/>import torchvision.datasets as dset<br/>import torchvision.transforms as transforms<br/>import torchvision.utils as vutils<br/>from torch.autograd import Variable<br/><br/># Setting some hyperparameters<br/>batchSize = 64 # We set the size of the batch.<br/>imageSize = 64 # We set the size of the generated images (64x64).<br/><br/># Creating the transformations<br/>transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.<br/><br/># Loading the dataset<br/>dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.<br/>dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.<br/><br/># Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.<br/>def weights_init(m):<br/>    classname = m.__class__.__name__<br/>    if classname.find('Conv') != -1:<br/>        m.weight.data.normal_(0.0, 0.02)<br/>    elif classname.find('BatchNorm') != -1:<br/>        m.weight.data.normal_(1.0, 0.02)<br/>        m.bias.data.fill_(0)<br/><br/># Defining the generator<br/><br/>class G(nn.Module): # We introduce a class to define the generator.<br/><br/>    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.<br/>        super(G, self).__init__() # We inherit from the nn.Module tools.<br/>        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).<br/>            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution.<br/>            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.<br/>            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.<br/>            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(256), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(128), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.BatchNorm2d(64), # We normalize again.<br/>            nn.ReLU(True), # We apply another ReLU.<br/>            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.<br/>            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.<br/>        )<br/><br/>    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.<br/>        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.<br/>        return output # We return the output containing the generated images.<br/><br/># Creating the generator<br/>netG = G() # We create the generator object.<br/>netG.apply(weights_init) # We initialize all the weights of its neural network.<br/><br/># Defining the discriminator<br/><br/>class D(nn.Module): # We introduce a class to define the discriminator.<br/><br/>    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.<br/>        super(D, self).__init__() # We inherit from the nn.Module tools.<br/>        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).<br/>            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.<br/>            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(256), # We normalize again.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.<br/>            nn.BatchNorm2d(512), # We normalize again.<br/>            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.<br/>            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.<br/>            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.<br/>        )<br/><br/>    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.<br/>        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.<br/>        return output.view(-1) # We return the output which will be a value between 0 and 1.<br/><br/># Creating the discriminator<br/>netD = D() # We create the discriminator object.<br/>netD.apply(weights_init) # We initialize all the weights of its neural network.<br/><br/># Training the DCGANs<br/><br/>criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.<br/>optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.<br/>optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator.<br/><br/>for epoch in range(25): # We iterate over 25 epochs.<br/><br/>    for i, data in enumerate(dataloader, 0): # We iterate over the images of the dataset.<br/>        <br/>        # 1st Step: Updating the weights of the neural network of the discriminator<br/><br/>        netD.zero_grad() # We initialize to 0 the gradients of the discriminator with respect to the weights.<br/>        <br/>        # Training the discriminator with a real image of the dataset<br/>        real, _ = data # We get a real image of the dataset which will be used to train the discriminator.<br/>        input = Variable(real) # We wrap it in a variable.<br/>        target = Variable(torch.ones(input.size()[0])) # We get the target.<br/>        output = netD(input) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errD_real = criterion(output, target) # We compute the loss between the predictions (output) and the target (equal to 1).<br/>        <br/>        # Training the discriminator with a fake image generated by the generator<br/>        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # We make a random input vector (noise) of the generator.<br/>        fake = netG(noise) # We forward propagate this random input vector into the neural network of the generator to get some fake generated images.<br/>        target = Variable(torch.zeros(input.size()[0])) # We get the target.<br/>        output = netD(fake.detach()) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errD_fake = criterion(output, target) # We compute the loss between the prediction (output) and the target (equal to 0).<br/><br/>        # Backpropagating the total error<br/>        errD = errD_real + errD_fake # We compute the total error of the discriminator.<br/>        errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.<br/>        optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.<br/><br/>        # 2nd Step: Updating the weights of the neural network of the generator<br/><br/>        netG.zero_grad() # We initialize to 0 the gradients of the generator with respect to the weights.<br/>        target = Variable(torch.ones(input.size()[0])) # We get the target.<br/>        output = netD(fake) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).<br/>        errG = criterion(output, target) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).<br/>        errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.<br/>        optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.<br/>        <br/>        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps<br/><br/>        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0])) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).<br/>        if i % 100 == 0: # Every 100 steps:<br/>            vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True) # We save the real images of the minibatch.<br/>            fake = netG(noise) # We get our fake generated images.<br/>            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True) # We also save the fake generated images of the minibatch.</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5c1c6ab5ea55063b4954bc802c128296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*Ufrls7mURYp4gDot.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Input Image</figcaption></figure><p id="48da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç¬¬ä¸€ä¸ªçºªå…ƒ:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/2e89fa3db298c9f27c134985b67dfe84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*EJPXc-AtwFHjYE8T.png"/></div></figure><h2 id="8ee4" class="kj jm hh bd jn kk kl km jr kn ko kp jv ip kq kr jz it ks kt kd ix ku kv kh kw bi translated"><span class="l lk ll lm bm ln lo lp lq lr di"> 2 </span>ç¬¬äºŒçºªå…ƒ:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/06dd9c3bd124f9408c5ca5ec5c2a6e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*lzwXp7RNgyOw9bYh.png"/></div></figure><p id="8e43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç¬¬ä¸‰çºªå…ƒ:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/e271e85b0ef9e81bf90e4fceb60494b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*apNTo6TGcsUNdvk2.png"/></div></figure><p id="30f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç¬¬23ä¸ªçºªå…ƒ:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f2b88d4564941707e0601c545888115b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*y1qVKZtV44Eiu6oh.png"/></div></figure><p id="b81a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç¬¬24ä¸ªçºªå…ƒ:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/257f55e2476d4d43fbc5ea236ddb4527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*71YzohBxEd-Zsf-R.png"/></div></figure><p id="d40a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">ç¬¬25ä¸ªçºªå…ƒ:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c0bb6617d4a69f3c4c728b6ff82550e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*0vKhEOmNCMAJ4-vl.png"/></div></figure><p id="4646" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">å¦‚æœå¯¹æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(DCGAN)æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·é€šè¿‡ç”µå­é‚®ä»¶æˆ–LinkedInè”ç³»æˆ‘ã€‚</p><p id="35fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">é˜…è¯»æ›´å¤šå…³äºæ­¤åšå®¢çš„ä¿¡æ¯â€” <a class="ae jc" href="https://machinelearningprojects.net/deep-convolutional-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank"> DCGAN Cifar-10 </a></p><p id="82a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">è¿™å°±æ˜¯æˆ‘å†™ç»™è¿™ä¸ªåšå®¢çš„å…¨éƒ¨å†…å®¹ï¼Œæ„Ÿè°¢ä½ çš„é˜…è¯»ï¼Œæˆ‘å¸Œæœ›ä½ åœ¨é˜…è¯»å®Œè¿™ç¯‡æ–‡ç« åä¼šæœ‰æ‰€æ”¶è·ï¼Œç›´åˆ°ä¸‹æ¬¡ğŸ‘‹â€¦ </p><p id="2c87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="ls">çœ‹æˆ‘ä»¥å‰çš„å¸–å­:</em> </strong> <a class="ae jc" href="https://machinelearningprojects.net/helmet-and-number-plate-detection-and-recognition/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> <em class="ls">å¤´ç›”å’Œå·ç‰Œæ£€æµ‹è¯†åˆ«ä½¿ç”¨YOLOV3 </em> </strong> </a></p><p id="c719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">æŸ¥çœ‹æˆ‘çš„å…¶ä»–</strong> <a class="ae jc" href="https://machinelearningprojects.net/machine-learning-projects/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">æœºå™¨å­¦ä¹ é¡¹ç›®</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/deep-learning-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">æ·±åº¦å­¦ä¹ é¡¹ç›®</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/opencv-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">è®¡ç®—æœºè§†è§‰é¡¹ç›®</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/nlp-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">NLPé¡¹ç›®</strong></a><strong class="ig hi"/><a class="ae jc" href="https://machinelearningprojects.net/flask-projects/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">çƒ§ç“¶é¡¹ç›®</strong> </a> <strong class="ig hi"> at </strong> <a class="ae jc" href="https://machinelearningprojects.net/" rel="noopener ugc nofollow" target="_blank"/></p><div class="lt lu ez fb lv lw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lx ab dw"><div class="ly ab lz cl cj ma"><h2 class="bd hi fi z dy mb ea eb mc ed ef hg bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="md l"><h3 class="bd b fi z dy mb ea eb mc ed ef dx translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="me l"><p class="bd b fp z dy mb ea eb mc ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk jj lw"/></div></div></a></div></div></div>    
</body>
</html>