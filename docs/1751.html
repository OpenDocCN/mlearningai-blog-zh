<html>
<head>
<title>Nested Variational Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嵌套变分推理</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/nested-variational-inference%C2%B9-a666f95ceb07?source=collection_archive---------8-----------------------#2022-01-25">https://medium.com/mlearning-ai/nested-variational-inference%C2%B9-a666f95ceb07?source=collection_archive---------8-----------------------#2022-01-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="1451" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">回顾了一种基于深度生成模型的分层变分推理方法</h2></div><p id="182e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">讨论了一种新的训练复杂概率模型的变分推理方法。它被NeurIPS 2021接受了。</p><h1 id="c053" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">深度生成模型</h1><p id="2acd" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">作为本文的主要优化目标，我们首先给出几个这类模型的例子。</p><h2 id="fe07" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">深度潜在高斯模型</h2><p id="912b" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">这些是深度定向图形模型的一般类别，由处理层级的每一层的<strong class="iy hi">高斯潜变量组成。</strong></p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/33bcda0a6ed82245896216a0bd0a86e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3-FnWiMublQocXdszJvUg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 1: Computational graph for DLGM. Note the 2 sides are the same model in different use cases (not to be confused with a GAN-like construct which has 2 parts). Black arrows indicate the forward pass of sampling from the recognition and generative models: Solid lines indicate propagation of deterministic activations, dotted lines indicate propagation of samples. Red arrows indicate the backward pass for gradient computation: Solid lines indicate paths where deterministic backpropagation is used, dashed arrows indicate stochastic backpropagation. (Quoted from original paper)</figcaption></figure><p id="0941" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了从模型中生成一个样本(例如一个图像)，我们从顶层开始，绘制高斯分布图(如图1的左部所示)。然后在每一层应用被高斯噪声干扰的激活函数。最终，在底层，通过从观察似然中采样来生成观察值，该观察值可以是任何适当的分布。识别过程与流程相反，从底部向顶部传播(如图2右侧所示)。</p><p id="ca49" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作者还讨论了随机反向传播技术设计的训练这样的结构，可以值得进一步阅读。此外，该体系结构概括了许多常见的图形模型，如因子分析、高斯信念网络等。</p><h2 id="a851" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">生成对抗网</h2><p id="a8bf" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">该框架通过对抗过程来估计生成模型，该过程在<strong class="iy hi"> minimax </strong>游戏设置中同时训练一个<strong class="iy hi">生成</strong>模型<code class="du lt lu lv lw b">G</code>和一个<strong class="iy hi">鉴别</strong>模型<code class="du lt lu lv lw b">D</code>。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lx"><img src="../Images/a1b9681589a35d46bd0ff21a25eb4e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hsh91cwy4XD8-7cK_ONDRg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 1: Objective function for GAN. (Quoted from original paper)</figcaption></figure><p id="cfdc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了了解发生器在数据上的分布，在输入噪声变量<code class="du lt lu lv lw b">z</code>上定义先验<code class="du lt lu lv lw b">p(z)</code>，然后映射到数据空间的是<code class="du lt lu lv lw b">G(z)</code>，其中<code class="du lt lu lv lw b">G</code>通常是深度神经网络。鉴别器<code class="du lt lu lv lw b">D(x)</code>，通常也是一个深度神经网络，被定义为表示<code class="du lt lu lv lw b">x</code>来自数据的概率，而不是来自生成器的分布(真实与生成)。这样，我们可以训练<code class="du lt lu lv lw b">D</code>区分真实的训练样本和来自<code class="du lt lu lv lw b">G</code>的样本，同时训练<code class="du lt lu lv lw b">G</code>为“傻瓜”<code class="du lt lu lv lw b">D</code>，目标如等式1所示。</p><h1 id="c0cd" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">重要性抽样</h1><p id="ae3c" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">重要抽样是嵌套变分推理的一个重要组成部分，因此作者描述了这种方法的一般框架和两个鼓舞人心的例子。</p><p id="be68" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">假设我们有一个目标分布π，我们想从中取样，但很难做到，还有一个建议密度<code class="du lt lu lv lw b">q</code>，从中取样很简单。然后，我们可以通过定义如下所示的重要性权重<code class="du lt lu lv lw b">w</code>来重写关于π的期望值。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ly"><img src="../Images/de7c7908f345169e9ca76b96f23c094d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dO6YxrKEqyQqEhBd9eulGQ.png"/></div></div></figure><p id="6415" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通常，π是感兴趣的后验分布，因此我们可以用联合密度γ和边际可能性<code class="du lt lu lv lw b">Z</code>写出<a class="ae lz" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank"> π=γ/Z </a>。然后，上面的等式可以重写如下，其中更新了<code class="du lt lu lv lw b">w</code>的定义。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ma"><img src="../Images/c1a763aac4fa95f9c0a06d13802f0236.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*tp_jDTsdKzC0zQFahzg6XQ.png"/></div></figure><h2 id="820b" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">自标准化重要性采样器</h2><p id="439a" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">自归一化估计量使用加权样本来计算期望值和归一化常数的估计值，如下所示，其中<code class="du lt lu lv lw b">S</code>是样本数。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mb"><img src="../Images/a57918c691dfddf78b4220de290dd7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eC5Q9f3c8dRErMKeAyuOkg.png"/></div></div></figure><p id="3792" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于<code class="du lt lu lv lw b">1 / Z = 1 / E[Z_hat] &lt;= E[1 / Z_hat]</code>由<a class="ae lz" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="noopener ugc nofollow" target="_blank">詹森不等式</a>得出，因此得到的估计值是一致的，但有偏差。偏差随着<code class="du lt lu lv lw b">q</code>和π之间的差异缩小而减小，因此对于迭代算法，初始估计偏差会很大，但随着迭代的进行，这将会改善。</p><h2 id="e4dc" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">嵌套重要性抽样和适当加权</h2><p id="0e50" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">嵌套重要性抽样通过调用其他重要性抽样器来形式化建议的构造。现在想象一下变分分布<code class="du lt lu lv lw b">q</code>是未标准化的，因此不能直接从中取样。在这种情况下，我们可以应用相同的想法，使用<strong class="iy hi">另一个重要性采样器</strong>，但是建议η模拟来自<code class="du lt lu lv lw b">q</code>的样本。由此产生的加权样本随后可用于计算<code class="du lt lu lv lw b">q</code>的归一化常数，使我们能够随后从<code class="du lt lu lv lw b">q</code>采样。</p><p id="9de3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种想法可以形式化为<em class="mc">适当加权</em>，其陈述对于概率密度π和某个常数<code class="du lt lu lv lw b">c &gt; 0</code>，随机对(w，z)~π是<strong class="iy hi">适当加权的</strong> (p.w .)，对于未规格化的概率密度γ=Zπ，如果<code class="du lt lu lv lw b">w &gt;= 0</code>和对于所有可测函数<code class="du lt lu lv lw b">g</code>下面的等式成立。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es md"><img src="../Images/ffe61e8d78ae0efbfbc9eb59c807bb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*z_7W4hpZx1pOBDs-jLZ9WQ.png"/></div></figure><p id="9e9b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其重要性在于，只要取样器能产生目标密度的p.w .样本，就能计算出一致的估计值。</p><h1 id="f1b7" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">随机变分推理</h1><p id="caa7" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">作为所提出方法的另一个重要组成部分，随机VI将在此简要介绍，具有正向和反向<a class="ae lz" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler发散</a>。</p><p id="a9c4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随机变分方法用一个<strong class="iy hi">变分概率密度</strong> <code class="du lt lu lv lw b"><strong class="iy hi">q</strong></code>(通常是一个近似器和一个潜在变量的函数，但由<em class="mc">自己的</em>参数ϕ).定义)来近似一个<strong class="iy hi">目标概率密度π </strong>(潜在变量的函数，并由<em class="mc">模型</em>参数θ定义)如前所述，通常π是具有联合密度γ和边际可能性<code class="du lt lu lv lw b">Z</code>的感兴趣的后验分布。这种近似通常通过优化方程2所示的变分目标来实现，其中<code class="du lt lu lv lw b">f</code>是散度函数。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es me"><img src="../Images/15e6f4fded390a005c8ee5b0a998a05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*m6Lv_AkS00Iq8_RlKnDLWw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 2: Variational objective for stochastic VI. (Quoted from original paper)</figcaption></figure><h2 id="5be5" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">向前KL散度</h2><p id="455f" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">对于向前(也称为包含)KL的情况，<code class="du lt lu lv lw b">f</code>-散度具有<code class="du lt lu lv lw b">f(w) = w log(w)</code>的形式。该场景中的一个常见策略是通过最大化数据上的<a class="ae lz" href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="noopener ugc nofollow" target="_blank">证据下限</a> (ELBO)或似然性来训练<strong class="iy hi">生成型</strong>模型，并通过从后验<strong class="iy hi"> π </strong>采样来优化<strong class="iy hi">推理</strong>模型(这可以通过使用变分分布<code class="du lt lu lv lw b">q</code>的<a class="ae lz" href="https://en.wikipedia.org/wiki/Importance_sampling" rel="noopener ugc nofollow" target="_blank">重要性采样</a>来实现)。</p><h2 id="ad9e" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">反向KL散度</h2><p id="23e9" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">另一方面，对于反向(又名独占)KL，我们使用<code class="du lt lu lv lw b">f(w) = -log(w)</code>。这种情况通常通过最大化等式3中给出的下限来解决，其梯度可以使用重新参数化的samples⁴、似然比estimators⁵或两者的组合来估计。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mf"><img src="../Images/ffc18db8fbcddbe224ef15e604f2f405.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*yfymRCcU8v0MWVnJwI4Iow.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 3: Objective lower bound for reverse KL. (Quoted from original paper)</figcaption></figure><h1 id="89ef" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">嵌套变分推理</h1><p id="e0dd" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">重要性抽样的一个常用策略是定义一个非标准化密度序列，该序列介于易于抽样的初始密度和最终目标密度之间。这使我们能够利用重要性重采样或<a class="ae lz" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">马尔可夫链蒙特卡罗</a> (MCMC)等技术，从较简单的密度构建更复杂的密度。</p><h2 id="58d0" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">嵌套变分目标</h2><p id="bbd1" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">NVI以双向视角构建共同战略，为<strong class="iy hi">建议</strong>定义<em class="mc">正向密度</em>，为<strong class="iy hi">中间目标</strong>定义<em class="mc">反向密度</em>。例如，对于序列中的第<code class="du lt lu lv lw b">k</code>步，作者将<strong class="iy hi">前面的目标</strong> γ(k-1)与一个<strong class="iy hi">正向核</strong> <code class="du lt lu lv lw b">q(k)</code>组合形成<strong class="iy hi">正向密度</strong>，将<em class="mc">下一个目标</em> γ(k)与一个<em class="mc">反向核</em> <code class="du lt lu lv lw b">r(k-1)</code>组合形成<em class="mc">反向密度</em>。这在图2中用图形表示，在等式4中用符号表示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mg"><img src="../Images/ed5c679ec2ac65b0a0a9c348d52d47ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCe4O4Xui_vTfxUvBlFl0w.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 2: Nested variational inference minimizes an f-divergence at each step in a sequence of densities to learn forward proposals q(k), reverse kernels r(k-1), and intermediate densities π(k). (Quoted from original paper)</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mh"><img src="../Images/bf6a1ead50fe6306b135b7f16acbbadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*ikVeG56QyTXst0qs-HLVxA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 4: Forward and reverse densities defined for NVI. The symbols with a hat on top are for forward calculations, and the ones with a check are for reverse calculations.</figcaption></figure><p id="47d7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用这样的公式，目标是学习尽可能相似的正向和反向密度对。为了实现这一目标，NVI目标被最小化，定义如下。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mi"><img src="../Images/a5e0b2489d209af3f6be4081f5551c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*fSHEMqszzCbrPEfhF96N5Q.png"/></div></figure><h2 id="47fc" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">从中间密度取样</h2><p id="0e37" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">为了优化上述目标，需要从中间密度(k &gt; 1)抽取样本。这是通过迭代方法实现的。考虑到对于先前的目标密度γ(k-1)为<strong class="iy hi">适当加权的</strong>对<code class="du lt lu lv lw b">(w(k-1), z(k-1))</code>，可以应用<strong class="iy hi">顺序蒙特卡罗</strong> sampling⁶来为第<code class="du lt lu lv lw b">k</code>步via找到该对</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mj"><img src="../Images/1eb120bfbbed6e3cc9db9a54bfaf2670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCDnkTa3rlO4gBsJOZIvsg.png"/></div></div></figure><p id="17f4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用这种结构，我们可以首先证明<code class="du lt lu lv lw b">(w(k), z(k-1:k))</code>对于反向密度是<strong class="iy hi">适当加权的</strong>，如等式5所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mk"><img src="../Images/f194c352c9d4ea8cdc5849ea6f0d10ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qK2d8EFpMGqs_AJVTdARCQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 5: Proof for reverse density proper weights.</figcaption></figure><p id="cbd3" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">随后，我们可以很容易地表明<code class="du lt lu lv lw b">(w(k), z(k))</code>是<strong class="iy hi">对建议密度的适当加权</strong>，如等式6所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ml"><img src="../Images/f6804e3539632d5b8b8d7b06f30baecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBPTqUSqO6rxWvKTH2nHGA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 6: Proof for proposal density proper weights.</figcaption></figure><p id="5fc1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">顺序重要性采样可以与保持适当加权的其他操作相结合，包括拒绝采样、应用MCMC转换运算符和重要性重采样。作者考虑了退火重要性sampling⁷ (AIS)和序列蒙特卡罗Carlo⁸ (SMC ),前者具有等式7中给出的退火路径，后者具有作为中间目标的滤波分布。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mm"><img src="../Images/05ecf3a3d1904b69a0b4e2a2a26e70e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Xcnl8kdTHqDHFWn0xYbemg.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 7: Annealing path for annealed importance sampling.</figcaption></figure><h2 id="67d7" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">计算梯度估计值</h2><p id="5259" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">目标密度π、正向内核<code class="du lt lu lv lw b">q</code>和反向内核<code class="du lt lu lv lw b">r</code>有3组主要参数。这些可以进一步分组为用于正向密度和反向密度的参数。作者在附录中给出了梯度的计算，并给出了结果。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mn"><img src="../Images/925d219038f1df3ca843d81d7d50368f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZimM07VvuXuTlLAvPSZ03g.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 8: Gradient for forward KL objective w.r.t. forward parameters.</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mo"><img src="../Images/bb07a139339739cfe7e65328701293f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRFv9rJToJFVZHcmAMYZ-A.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 9: Gradient for forward KL objective w.r.t. reverse parameters.</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mp"><img src="../Images/b79caf122f51136c7abcb729024f22a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oh1fRkoiU4oRQ7vA183sQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 10: Gradient for reverse KL objective w.r.t. forward parameters.</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mq"><img src="../Images/9a7a13a6f799076c998e0038fd166972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1v-8A2sWs0YS_KumVhALw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Equation 11: Gradient for reverse KL objective w.r.t. reverse parameters.</figcaption></figure><h2 id="9743" class="kp jt hh bd ju kq kr ks jy kt ku kv kc jf kw kx ke jj ky kz kg jn la lb ki lc bi translated">与重要性加权和自归一化估计量的关系</h2><p id="b92d" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">NVI与现有方法的不同之处在于，它为每个嵌套层的变量对定义了一个目标，而不是为整个变量序列定义了一个目标。如图3所示，这可以避免<strong class="iy hi">信噪比</strong>随着样本数量的增加而矛盾地恶化的问题，这在普通VI构造中很常见。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mr"><img src="../Images/f0263e65dfa859266eafd83341cc9473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XC_6JE1BFk1Xk445gNlAPg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 3: Weight contributions in the self-normalized gradient estimators for the forward KL-divergence for VI and NVI using SIS (no resampling) and SMC (resampling). VI computes gradient estimates using the final weights (SIS), which simplify to the final incremental weight when resampling is performed (SMC). NVI computes gradient estimates based on the intermediate weights (SIS), which simplify to the intermediate incremental weights when resampling is performed (SMC). (Quoted from original paper)</figcaption></figure><p id="72de" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">此外，NVI允许我们计算特定采样器级别的梯度估计，这导致<strong class="iy hi">更低的方差</strong>以及潜在的<strong class="iy hi">内存</strong>优势(<code class="du lt lu lv lw b">O(SK) -&gt; O(S)</code>)。</p><h1 id="7624" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">实验</h1><p id="c7c6" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">执行的第一项任务是学习从非标准化目标密度取样，其中使用退火产生中间密度。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ms"><img src="../Images/2b0383148572f2fea86d586bf4432a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-PP2YCJgSplB1gNbO_Iyg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 4: Samples from forward kernels trained with annealed variational objective (AVO), and NVI with resampling and a learned annealing path (NVIR*). (Quoted from original paper)</figcaption></figure><p id="c2fb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第二个任务是学习启发因子，以近似状态空间模型中未来观察的边际可能性。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mt"><img src="../Images/1d45c0acc824ccc86e996c6c1c834e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghB97CH9ozv7iQg4Ebk8Bg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 5: (Top) qualitative results of an instance with K = 200 time steps (x-axis). Observations are color-coded based on the inferred assignments. Each colored band corresponds to the inferred cluster mean and standard deviation; grey bands indicate the ground truth of the clusters. (Bottom) We compute logarithm of Z-hat and the ESS using 1000 samples and report average values over 2000 test instances. (Quoted from original paper)</figcaption></figure><p id="ea8f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">第三个任务是从深度生成贝叶斯混合的少量例子中推断类的分布。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mu"><img src="../Images/5c1a04e466c95938b7e514a1db515bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTaMwhSbtW4mGQY0cDZDHg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Figure 6: BGMM-VAE trained on MNIST &amp; FashionMNIST with the RWS objective (Top) and the NVI objective (Bottom). (Left) Samples from a test mini-batch of size N = 300. (Middle) Samples from the generative model, generated from the <strong class="bd ju">λ </strong>inferred from the test mini-batch. (Right) Comparison of ground truth <strong class="bd ju">λ*</strong> and the expected inferred value.</figcaption></figure><h1 id="eb9b" class="js jt hh bd ju jv jw jx jy jz ka kb kc in kd io ke iq kf ir kg it kh iu ki kj bi translated">结论</h1><p id="52f1" class="pw-post-body-paragraph iw ix hh iy b iz kk ii jb jc kl il je jf km jh ji jj kn jl jm jn ko jp jq jr ha bi translated">作者成功地证明了经过NVI训练的采样器在从多峰密度、贝叶斯状态空间模型和分层深度生成模型采样时能够超越基线。</p><p id="59d2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇文章中，我们回顾了NeurIPS 2021年的论文<nested variational="" inference="">，尝试证明了一些结果，并修改了深度生成模型以及变分推理技术。但是，我们找不到相应的代码库，因此联系了作者。</nested></p></div><div class="ab cl mv mw go mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ha hb hc hd he"><p id="f120" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[1]齐默尔曼，海科，等。“嵌套变分推理。”arXiv预印本arXiv:2106.11302  (2021)。</p><p id="849e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[2]雷森德、达尼洛·希门尼斯、沙基尔·穆罕默德和金奎大·威斯特拉。"深度生成模型中的随机反向传播和近似推理."<em class="mc">机器学习国际会议</em>。PMLR，2014年。</p><p id="762e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[3]古德费勒，伊恩，等，“生成性对抗性网络。”<em class="mc">神经信息处理系统进展</em> 27 (2014)。</p><p id="22cb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[4]金玛、迪德里克·p和马克斯·韦林。"自动编码变分贝叶斯."<em class="mc"> arXiv预印本arXiv:1312.6114 </em> (2013)。</p><p id="c789" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[5] Ranganath、Rajesh、Sean Gerrish和David Blei。“黑盒变分推理。”<em class="mc">人工智能与统计</em>。PMLR，2014年。</p><p id="968e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[6]德尔·莫拉尔、皮埃尔、阿诺·杜塞和阿贾伊·亚斯拉。"顺序蒙特卡罗抽样器."皇家统计学会杂志:B辑(统计方法学)68.3(2006):411–436。</p><p id="3cf2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[7]拉德福德·尼尔，“退火重要性抽样”统计与计算11.2(2001):125–139。</p><p id="4e0e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">8德·弗雷塔斯、南多和尼尔·詹姆斯·戈登。<em class="mc">实践中的序贯蒙特卡罗方法</em>。由…编辑阿诺·杜塞。第一卷。№2.纽约:斯普林格出版社，2001年。</p><div class="nc nd ez fb ne nf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hi fi z dy nk ea eb nl ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ln nf"/></div></div></a></div></div></div>    
</body>
</html>