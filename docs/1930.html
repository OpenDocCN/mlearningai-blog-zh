<html>
<head>
<title>Differentiality meets Conditional Computation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微分满足条件计算</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/differentiality-meets-conditional-computation-60d88435ec24?source=collection_archive---------7-----------------------#2022-02-12">https://medium.com/mlearning-ai/differentiality-meets-conditional-computation-60d88435ec24?source=collection_archive---------7-----------------------#2022-02-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b5f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对2020年我最喜欢的一篇论文的简要而详尽的总结，这是一篇探索决策树和神经网络之间可能的错综复杂关系的研究工作。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/e0abc820668f4de7a99ed85975a88334.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*xJjCIvv6hkVTCCgkFIV0QA.jpeg"/></div></figure></div><div class="ab cl jl jm go jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="ha hb hc hd he"><p id="3b0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">我很高兴能这样做:)</em> <a class="ae js" href="https://arxiv.org/abs/2002.07772" rel="noopener ugc nofollow" target="_blank">这些天来，我倾向于人工智能研究，这抛弃了我最近在阅读和理解不断涌现在勇敢的人工智能世界中的令人兴奋的方法和理论方面的突破。</a></p><p id="2275" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章强调了在研究论文<a class="ae js" href="https://arxiv.org/abs/2002.07772" rel="noopener ugc nofollow" target="_blank"> <em class="jc">中解释的核心概念和理论:树集合层:可微性满足条件计算。</em> </a> <em class="jc"> </em>要全面了解，需要对决策树和神经网络的工作原理有一个基本的了解。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/f80f010ceae81bbbd685206c681374db.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/1*X13E0nevXnbwokQuo2UhgQ.gif"/></div></figure><h1 id="7f22" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">介绍</h1><p id="2bcc" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">作者开始结合神经网络和树集成的独特统计和计算优势，为神经网络引入一个新层，该层由称为<strong class="ig hi">软树</strong>的<em class="jc">可微分决策树</em>的集成组成。可微分树的讨论并不是什么新东西，它已经存在了一段时间，但一直很难推销，因为它们在训练和推理方面都很慢，因为它们无法执行<em class="jc">条件计算</em>。</p><blockquote class="kx ky kz"><p id="9d22" class="ie if jc ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">条件计算是指模型以依赖于输入的方式运行的能力，即通过将样本路由通过少量节点(单个根到叶路径)仅激活其架构的一小部分的能力。</p></blockquote><p id="2cc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树有几个吸引人的特性，其中一个最优秀的特性是它们执行条件计算的能力。这带来了计算上的好处和统计上的特性。但是决策树缺乏良好的表示学习机制，它们需要大量的特征工程。而这是神经网络(NNs)擅长的领域，它在图像识别、语音识别等领域的能力非常突出。然而，神经网络不支持条件计算，并且很难调整。</p><p id="ac02" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者提出了一种混合神经网络模型<strong class="ig hi"> <em class="jc"> TEL:树集成层</em> </strong> <em class="jc"> </em>，它结合了树集成和神经网络的优点。这将是一个可微分决策树的附加模型，可以将其插入神经网络的任何位置，并使用梯度下降与网络的其余部分一起训练。</p><p id="57ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可微分树的转换已经存在，并且在文献中，它已经在神经网络的上下文中显示出有希望的结果，但是它们在训练和推理期间都不提供真正的条件计算。在可微分树中联合优化是困难的，因为树的训练复杂性随着树的深度成指数增加。常见的方法是使用贪婪的“阶段式”程序来训练树，即一次只更新一棵树，并且不再更新，这是梯度增强模型的主要原则。TEL配备了一种新的机制来执行条件计算，该机制通过引入用于样本路由的新的稀疏激活函数以及探索稀疏性的专用前馈和反向传播算法来实现。所提出的算法使得在宽树和深树上的联合优化成为可能。</p><blockquote class="kx ky kz"><p id="19d9" class="ie if jc ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">联合优化指的是同时更新每棵树。作者假设联合优化产生比GBDT更紧凑和更有表现力的集合。这使得模型尺寸减小了20倍以上。</p></blockquote><p id="6a28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该文件的概念强调如下:</p><ol class=""><li id="2397" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated">一个新的激活函数，用于在树的小部分路由样本(类似于经典树),而不会失去其可区分性。</li><li id="d73e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">条件计算是通过专门的前馈和反向传播算法来实现的，这些算法利用稀疏性来获得最佳的时间复杂度。</li><li id="ce76" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">在26个数据集上的实验证实了TEL是CNN中当前可微分树、GBDT和密集层的有竞争力的替代物。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/081b2d3fcf11aa5ecee8af5bde4dec3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*Zb8xwSB7YVTHIIlGklyAkQ.gif"/></div></figure><h1 id="457e" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">树集合层</h1><p id="e2b5" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在这一部分，我将强调<strong class="ig hi"> TEL </strong>的要点。正如我前面所说，这是一个可微分决策树的附加模型。作者正式介绍了TEL层，并继续讨论了它的路由机制。我将略去数学表达式，从理论上作更多的解释。</p><h2 id="5c19" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">可区分决策树:</h2><p id="cfb7" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">经典决策树执行<em class="jc">硬路由</em>，即样本在每个内部节点处被路由到恰好一个方向。硬路由在损失函数中引入了不连续性，使得树不能连续优化。这就是为什么树是以贪婪的方式建造的。软树是执行软路由的决策树的变体，因此得名<em class="jc">软树</em>:)。</p><blockquote class="kx ky kz"><p id="a68f" class="ie if jc ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">软路由是一种样本路由，其中每个内部节点可以以不同的比例同时将样本路由到左边和右边，这种机制使得软树是可区分的。</p></blockquote><p id="0243" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是软树不能排他地将样本路由到左边或右边，这使得条件计算不可能。这是为软树引入的新的激活函数，它使得条件计算成为可能，同时保持可微性。TEL使用概率模型引入软树，概率用于建模路由过程，树的最终预测是对叶子的期望，使得T成为确定性函数。经典决策树使用轴对齐分裂，软树基于超平面(即<em class="jc">倾斜)</em>分裂，其中使用特征的线性组合来做出路由决策。每个内部节点都与定义节点超平面分裂的可训练权重向量<em class="jc"> w </em>相关联。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/c96302c783af9195bda86fd59ede2bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*OxbauK4vsFdoxyRY0_t3MQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 1 Probability of a sample reaching a leaf</figcaption></figure><p id="1f9e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设每个叶子存储在训练期间学习的权重向量<em class="jc"> o </em>。在正向传递期间，<em class="jc"> o </em>是一个恒定向量，即它不是输入样本的函数。对于样本<em class="jc"> x </em>，他们将树的预测定义为叶输出的期望值，即，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/74bcdd7dd056091fe44c151fdaa63060.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*YVELyxxjuiBxxZdqtWJHeg.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 2 The prediction of a tree.</figcaption></figure><h2 id="ca2a" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">激活功能</h2><p id="de1c" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">内部节点使用激活函数来计算路由概率。逻辑(又名sigmoid)函数是软树的常见选择。逻辑函数可以输出任意小的值，但不能输出精确的零。其含义是样本<em class="jc"> x </em>将以正概率到达树中的每个节点，因此随着树深度的增加，计算树的输出变得指数昂贵。提出了一种新颖的<strong class="ig hi"> <em class="jc">平滑阶跃激活函数</em> </strong>，可以输出精确的0和1。它是<em class="jc"> S形的</em>并且是连续可微的，类似于逻辑函数。平滑阶跃函数是区间[-γ/2，γ/2]内的三次多项式，0向左，1向右。假设该函数采用下图中的参数形式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/2a1c47aecad83a903165c0bf0b5cfa61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*Fft5i2nlgAw_8dg8L05Hzg.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 3 Smooth-step activation function</figcaption></figure><p id="5003" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">平滑阶跃函数对所有实数<em class="jc"> t、</em>连续可微，包括<em class="jc">γ/2</em>和<em class="jc"> γ/2。</em>下图比较了平滑阶跃函数<em class="jc">和</em> γ=1以及逻辑函数(这是标准逻辑函数的一个重新调整变量)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/be10b1602a4aa24c8fa13e727412f5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*UxvOAWkY8ry-irp3AprT2A.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 4 Logistic Function</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/c4b3f9576936d8b7bbb8d5755952f629.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*5nF7zD8bH7e_838YmQIwOg.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 4 Smooth-step v Logistic</figcaption></figure><p id="8bd1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在界限[-γ/2，γ/2]之外，平滑步长函数执行硬路由，类似于经典决策树。γ的选择控制硬路由样本的比例。非常小的γ会导致内部节点出现许多零梯度，而非常大的γ可能会限制条件计算的范围。在他们的实验中，在树层之前使用了批量归一化，以便平滑阶跃函数的输入保持居中和有界。他们注意到它在防止内部节点具有零梯度方面非常有效，至少在最初的几个训练时期是如此。超参数gamma( γ)用于在他们对模型进行实验的数据集之间平衡训练性能和条件计算。</p><p id="f812" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于样本<em class="jc"> x </em>，如果<em class="jc"> P (x → i) &gt;为0，即</em>，则称节点<em class="jc"> i </em>可达。如果该样本到达该节点的概率大于零，则称该节点是可达的。可达树的数量直接控制条件计算的范围。下面是对于深度为10(即，具有1024个叶子)和不同γ的单棵树，作为训练时期的函数的可到达叶子的平均数量(每个样本)的图。这是在糖尿病数据集上完成的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/44f8b9badbc3cae30c8f680d745cf522.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*2NnqUTCTi_wHyJFUyrUa7g.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 5 Number of reachable leaves per sample with a tree depth of 10</figcaption></figure><p id="2ca8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从图中，我们可以看到，在训练期间，可达叶的数量迅速收敛到1。</p><p id="3fa7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一节中，作者将展示如何利用平滑阶跃函数及其梯度中的稀疏性来开发高效的前向和后向传播算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/d27ece5530d24aca4fd95b889cd7ec0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*7vNac0eYkQXorZbqRdsA3A.gif"/></div></figure><h1 id="4dda" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">条件计算</h1><p id="9493" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">作者提出了一阶优化方法(如SGD及其变体)来优化TEL。在这种情况下，计算瓶颈是梯度计算，其时间和内存复杂性会随着树的深度呈指数增长。这促使作者通过利用平滑阶跃函数及其梯度中的稀疏性来开发用于TEL的更有效的前向和后向传播算法。还解释了为了有效地利用条件计算，每个样本应该到达相对较少数量的叶子。这可以通过将平滑阶跃函数的参数γ选择得足够小来实现。</p><h2 id="58a1" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">有条件向前传球</h2><p id="0975" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在计算梯度之前，需要对树进行正向传递。作者的算法利用了以下观察:如果到叶子<em class="jc"> l </em>的路径上的某个边具有零概率，那么<em class="jc"> P (x → l) = 0，</em>将没有必要沿着该路径继续评估。因此，从根开始遍历树，并且每次节点在一侧输出0概率时，其后代在该侧被忽略。图2 中<em class="jc">的求和仅在遍历到达的叶子上执行。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es mr"><img src="../Images/b7a657c3a673919f431e4936ad5a239f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5XB6OYxsJT71th7Sko9Tg.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 6 The pseudocode for the conditional forward pass algorithm</figcaption></figure><h2 id="54d5" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">条件反向传递</h2><p id="3c76" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在这里，作者开发了反向传递算法，以有效地计算恒定权重向量<em class="jc"> O、</em>权重向量<em class="jc"> w、</em>和样本<em class="jc"> x、</em>上的梯度。从这里开始，作者使用几个定理来展示该算法如何在网络上工作和计算。下面是运算的算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/620cef9450829a953611d9d205870bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*5RcVYqjsVLqt7kastsF2aw.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 7 The pseudocode for the conditional backward pass algorithm</figcaption></figure><p id="556c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者强调了关于条件后向传递和条件前向传递之间的时间复杂度的有趣观察，即后向传递具有更好的时间复杂度。这在NNs的标准反向传播中非常罕见，因为正向和反向传递遍历相同的计算图。他们将复杂性的提高归功于他们在分数树上操作的算法，与前向遍历的树相比，分数树上的节点数量明显较少。</p><blockquote class="kx ky kz"><p id="4035" class="ie if jc ig b ih ii ij ik il im in io la iq ir is lb iu iv iw lc iy iz ja jb ha bi translated">在这种情况下，分数树被视为简化的计算图</p></blockquote><h1 id="8ebe" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">实验</h1><p id="83b5" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">从预测、条件计算和紧凑性方面评估了<strong class="ig hi"> TEL </strong>的性能。将其作为独立层和NN中的层进行评估，并与标准软树、<strong class="ig hi">、</strong>和密集层进行比较。该模型在来自不同领域的26个分类数据集(二元&amp;多类)上进行评估。其中23个是<a class="ae js" href="https://epistasislab.github.io/pmlb/" rel="noopener ugc nofollow" target="_blank">宾机器学习基准</a>，其余3个是<strong class="ig hi"> CIFAR-10 </strong>、<strong class="ig hi"> MNIST、</strong>和<strong class="ig hi">时尚MNIST </strong>。</p><h2 id="e35b" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">模型实现</h2><p id="67b6" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated"><strong class="ig hi"> TEL </strong>在TensorFlow 2.0中实现，使用定制的C++内核进行前向和后向传播，以及Keras Python可访问的接口。这个实现是开源的，可以在这里<a class="ae js" href="https://github.com/google-research/google-research/tree/master/tf_trees" rel="noopener ugc nofollow" target="_blank">获得。</a></p><p id="6c10" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于所有实验，使用具有树形结构Parzen估计器(TPE)的Hyperopt进行超调。用分层5重交叉验证对模型的AUC或准确性进行了优化。对于神经网络，使用Adam和交叉熵损失。</p><h2 id="9789" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated"><strong class="ak"> <em class="mx">平滑步进vs .逻辑激活</em> </strong></h2><p id="be98" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在50个时期内测量训练时间，作为两个激活函数的树深度的函数。使用的参数是10的集合大小，对于平滑阶跃函数，γ=1，因为这对应于最差情况的训练时间，优化参数如下:批量大小= 256，学习率=0.1。下图中的表格显示了它们在不同数据集上的表现，它包含了<strong class="ig hi"> TEL </strong>架构中两个函数的AUC。A <strong class="ig hi"> * </strong>表示统计显著性，最佳结果以<strong class="ig hi">粗体</strong>表示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/21a60ec81966598cdafc3672e95d4d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*horHB7mwOTvk7Cf6-ae8RA.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 8 The comparative performance of the Smoothstep and Logistic function</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es mz"><img src="../Images/f7e8f3cd20b153ac07d2e13ed02a825e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TyTOhyd-Yx0aD4LnBvMhA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 9 Training time(sec) vs tree depth for smooth-step and logistic function over 5 repetitions</figcaption></figure><h2 id="e5cc" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">TEL vs梯度增强决策树</h2><p id="4513" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">对于<strong class="ig hi"> GBDT </strong>，使用了<strong class="ig hi"> XGBOOST </strong>。<strong class="ig hi"> CART </strong>和<strong class="ig hi"> L2正则化逻辑回归(LR) </strong>用作基线。为了公平起见，TEL被用作一个独立的层。对于<strong class="ig hi">电话</strong>和<strong class="ig hi"> GBDT </strong>，树的数量、深度、学习率和L2正则化被调整。批次大小、时期和γ针对<strong class="ig hi"> TEL </strong>进行了调整，并且在Hyperopt中以AUC为度量单位进行了50轮调整。下图显示了它们的比较性能。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es na"><img src="../Images/411fec74479f1a02196629ea65b08876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7PP7BXpUfudlN934naVpA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 10 Test AUC on 23 PMLB datasets over 15 random repetitions</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es nb"><img src="../Images/3e828de35c67416cfe6790893f1a3ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2ReWiGxPhLyVYHL_dddVg.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 11 Mean est AUC vs # of trees (15 trails)</figcaption></figure><h2 id="8166" class="ls jv hh bd jw lt lu lv ka lw lx ly ke ip lz ma ki it mb mc km ix md me kq mf bi translated">细胞神经网络中的TEL vs稠密层</h2><p id="1658" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">作者在CNN的<strong class="ig hi"> CIFAR-10 </strong>、<strong class="ig hi">、</strong>和<strong class="ig hi">时尚MNIST </strong>数据集上研究了用<strong class="ig hi"> TEL </strong>替代密集层的潜在好处。他们使用2个卷积层，然后是中间层(最大池化、丢弃、批量归一化)，最后是密集层；他们称之为<strong class="ig hi"> CNN密集</strong>。他们还考虑了类似的架构，其中密集层被替换为<strong class="ig hi">TEL</strong>；他们把这个叫做<strong class="ig hi"> CNN-TEL </strong>。他们以分类精度为目标度量，运行Hyperopt 25次迭代。调整后，使用5个随机权重初始化来训练模型。图中显示了它们的性能对比。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es nc"><img src="../Images/e5f38785ecbf2319125ee217b1bba7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWv_icQGh1Iy8h6DougTUg.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx">Fig. 12 Shows how both models compare to each other on the 3 datasets.</figcaption></figure><p id="b5b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到<strong class="ig hi"> CNN-TEL </strong>中的参数数量比<strong class="ig hi"> CNN-Dense </strong>少了大约8倍。这显示了表现层是如何被<strong class="ig hi"> TEL </strong>有效利用的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/ecd6bf9ce69db835ad11e7ce5a90c221.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/1*K5QMwR8xJS4QorhyUm8s_w.gif"/></div></figure><h1 id="6da1" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="70cd" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">我发现这篇论文很有趣，我期待着这种方法可能带来的令人兴奋的发展。作者已经在思考他们打算如何改进他们的小说方法。</p><p id="1dd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我的下一步行动是使用<strong class="ig hi"> PyTorch </strong>实现这个架构，我肯定会写一篇教程文章展示如何在<strong class="ig hi"> PyTorch </strong>中实现<strong class="ig hi"> TEL </strong>。</p><h1 id="4015" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><ol class=""><li id="a373" class="ld le hh ig b ih ks il kt ip ne it nf ix ng jb li lj lk ll bi translated">Hussein Hazimeh，Natalia Ponomareva，Petros Mol，Zhenyu Tan，Rahul Mazumder,《树集合层:可微性满足条件计算》, 2020年。</li></ol></div><div class="ab cl jl jm go jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="ha hb hc hd he"><p id="41dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">我将非常感谢对讨论的主题和我的写作方法的反馈和建议。我也仍然是ML研究的新手，我乐于接受将进一步帮助我成长的意见和合作。随时和我联系</em><a class="ae js" href="https://www.linkedin.com/in/busayo-awobade-107a94175/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"><em class="jc">LinkedIn</em></strong></a><em class="jc">。</em> <strong class="ig hi"> <em class="jc">我很乐意回答您的任何问题或ML工程任务:</em> </strong></p><div class="nh ni ez fb nj nk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hi fi z dy np ea eb nq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">medium.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny jj nk"/></div></div></a></div><p id="8b0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">🔵<a class="ae js" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ig hi">成为作家</strong> </a></p></div></div>    
</body>
</html>