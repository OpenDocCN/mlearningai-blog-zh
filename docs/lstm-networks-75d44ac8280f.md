# LSTM 网络公司

> 原文：<https://medium.com/mlearning-ai/lstm-networks-75d44ac8280f?source=collection_archive---------2----------------------->

![](img/73b6462219bbd1c85aa726bc2fc18cf5.png)

本文讨论了传统的 RNNs 的问题，如消失和爆炸梯度，并提出了一个简单的解决方案的形式长短期记忆(LSTM)。长短期记忆(LSTM)是递归神经网络(RNN)架构的一种更复杂的变体，创建该架构是为了比普通的 rnn 更精确地描述时间序列及其长期关系。

一个基本的 LSTM 细胞的内部设计，修改包括到 LSTM 的架构，以及 lstm 的一些应用，是在很大的需求之间的亮点。它还比较和对比了 LSTMs 和 GRUs。本文最后列出了 LSTM 网络的缺点，并简要概述了即将出现的基于注意力的模型，这些模型正在现实世界的应用中迅速取代 lstm。

# **简介:**

LSTM 网络是一种递归神经网络(RNN ),被开发用于处理 RNNs 失败的情况。就 RNNs 而言，它们是对当前输入起作用的网络，同时考虑先前的输出(反馈)并将它们保持在存储器中一小段时间(短期存储器)。最常见的应用是在语音处理、非马尔可夫控制和音乐创作等领域。然而，rnn 有几个缺点。首先，它无法长时间保留数据。为了预测当前的输出，通常需要求助于很久以前保存的信息。另一方面，rnn 完全不能处理这样的“长期依赖”

第二，对于应该保留哪些方面的内容以及应该遗忘多少内容，没有更精细的控制。在网络的回溯训练阶段出现的爆炸和消失梯度(稍后描述)是 RNNs 的另一个问题。因此，长短期记忆(LSTM)的概念被引入。它是以这样一种方式构建的，即几乎完全消除了消失梯度问题，但训练模型保持不变。LSTMs 用于桥接某些应用中的长时间延迟，它们还可以处理噪声、分布式表示和连续数据。与隐马尔可夫模型(HMM)一样，LSTMs 不需要从一开始就保留有限数量的状态。学习率、输入和输出偏差以及学习率只是 LSTMs 提供的几个参数。

因此，不需要精确的修改。使用 LSTMs，更新每个权重的难度降低到 O(1)，与通过时间的反向传播(BPTT)相当，这是一个好处。

# **爆炸和消失渐变:**

网络训练过程的基本目标是当训练数据通过它传递时，减少输出中注意到的损失量(在误差或成本方面)。我们计算某一组权重的梯度或损失，然后适当地改变权重，继续下去，直到我们得到一组损失最小的理想权重。回溯是一个用来描述这个过程的术语。

偶尔，梯度很小，几乎看不出来。值得注意的是，层的渐变会受到后续层中特定组件的影响。如果这些分量很小(小于 1)，获得的梯度将更小。规模效应就是这个术语。当这个梯度乘以学习率(0.1 到 0.001 之间的一个很小的数字)时，结果是一个更小的数字。

因此，权重的变化很小，导致输出几乎与之前相同。类似地，如果梯度由于大的分量值而非常大，则权重被改变为不是最佳的值。爆发渐变的问题就是所谓的。神经网络单元以这样的方式重建，即缩放因子固定为 1，以避免这种缩放影响。在增加了多个门控单元后，该单元被命名为 LSTM。

# **架构:**

RNN 和 LSTM 设计的主要区别是 LSTM 的埋层是一个门控单元或门控细胞。它由四层组成，这四层相互作用，生成单元的输出以及单元的状态。然后，这两个项目被传递到下一个隐藏层。LSTMs 包含三个逻辑 sigmoid 门和一个 tanh 层，这与 rnn 不同，rnn 只有一个 tanh 神经网络层。

开发门是为了限制通过单元传输的数据量。他们计算出数据的哪些部分将被下一个单元需要，哪些将被丢弃。结果通常在 0–1 范围内，0 表示“拒绝全部”，1 表示“包括全部”

# **变异:**

随着 LSTM 的日益流行，对传统的 LSTM 体系结构进行了一些修改，以简化单元的内部设计，从而使它们更有效地运行并最小化计算复杂度。Gers 和 Schmidhuber 引入了窥视孔连接，使栅极层可以随时了解单元的状况。一些 LSTMs 使用了一个链接的输入和遗忘门，而不是两个不同的门，这使得它们可以同时做出两个选择。另一项改进是引入了门控循环单元(GRU)，减少了门的数量，从而降低了设计的复杂性。它混合了单元状态和隐藏状态，以及一个结合了遗忘门和输入门的更新门。

# **GRUs Vs LSTMs**

尽管 gru 与 LSTMs 非常相似，但它们从未如此流行。但是 GRUs 到底是什么？门控循环单位(GRU)是门控循环单位的缩写。顾名思义，Cho 建议的循环单元包括一个门控方法，以高效和自适应地捕捉跨多个时间尺度的关系。他们有一个重置和更新门。前者负责决定哪部分知识将被继承，而后者负责决定在两个连续的循环单元之间有多少信息将被遗忘。

GRUs 的另一个值得注意的特征是，它们不以任何方式保留单元状态，因此它们不能控制下一个单元暴露于多少内存内容。另一方面，LSTMs 控制进入细胞的新鲜信息的数量。另一方面，当计算新的候选激活时，GRU 控制来自先前激活的信息流，但是不控制被添加的候选激活的数量(该控制通过更新门被绑定)。