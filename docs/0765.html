<html>
<head>
<title>Deep Learning Definition Cheatsheet: Sequence Model version</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习定义备忘单:序列模型版本</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/deep-learning-definition-cheatsheet-sequence-model-version-b292b1a90cbc?source=collection_archive---------1-----------------------#2021-07-10">https://medium.com/mlearning-ai/deep-learning-definition-cheatsheet-sequence-model-version-b292b1a90cbc?source=collection_archive---------1-----------------------#2021-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="2646" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">本文面向已经了解深度学习和序列模型的基本机制，并希望快速复习核心概念的人。</p></blockquote><p id="5f6a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">深度学习已经深深扎根于我们日常生活的许多方面。现在的人们可以很容易地免费获得高质量的深度学习教育。学了这么多之后，你的大脑会不知所措，所有的信息在你的大脑里变得混杂和纠结。本文概述了序列模型的定义和一些主要功能，包括RNNs，LSTM，GRU，比尔斯特姆。</p><h1 id="46e5" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">递归神经网络</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/24770dbb686b06e5ef3fd443d3c6bc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BqpR8H2n1A0dFRTfzJyfg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">RNN Architecture</figcaption></figure><h2 id="b973" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">主要功能</h2><ol class=""><li id="3804" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">克服了传统神经网络的限制，传统神经网络不能共享跨输入数据的不同位置学习的特征。</li><li id="cb01" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">使用时间上的反向传播来计算梯度。</li><li id="1c71" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">它有参数W，在某个时间步长(Xt)的输入向量。在每个时间步长使用相同的函数和相同的参数集(在每个时间步长保持单个隐藏向量)。</li></ol><h2 id="4cae" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">限制</h2><ol class=""><li id="8ec3" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">遭受消失梯度问题，因此在处理长序列时丢失有用信息，这也被称为长期依赖性问题。</li><li id="dc7e" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">计算量大且速度慢。该模型通过整个序列向前和向后传播，以计算损耗和梯度。</li></ol><h1 id="3613" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated"><strong class="ak"> LSTM(长短期记忆)</strong></h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mb"><img src="../Images/2ade773a0b6e56396b570d215607dfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqoxOh60C10QcVFJlLrKLw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">LSTM Structure</figcaption></figure><h2 id="4588" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">主要功能</h2><ol class=""><li id="3aef" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">RNN体系结构的一种变体——它不是在每个时间步长保持单个隐藏向量，而是在每个时间步长保持两个不同的隐藏向量(使用以前的隐藏状态和当前状态来计算更新的隐藏状态)。</li><li id="56aa" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">LSTM的结构:<em class="ij">输入门、遗忘门、输出门、门电路<br/> - </em> <strong class="ik hi"> <em class="ij">输入门</em> </strong> <em class="ij"> : </em>是否写入单元(范围:0–1)<br/>-<strong class="ik hi"><em class="ij">遗忘门</em> </strong>:是否擦除单元(范围:0–1)<br/>-<strong class="ik hi"><em class="ij">门电路</em> </strong>:写入单元多少(范围:-1–1</li><li id="b63f" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">Sigmoid函数输出0-1之间的数字。0表示不允许任何内容通过，1表示允许所有内容通过。</li><li id="3cd6" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">隐藏状态与单元格状态的区别<br/> - <strong class="ik hi"> <em class="ij">隐藏状态</em> </strong>:记住前一事件的信息，并在每个时间步忘记/覆盖。<br/>-<strong class="ik hi">-<em class="ij">单元格状态</em> </strong>:携带当前状态下远近时间步长的信息，创建一条不间断的梯度公路。单元状态是长期记忆，它通过用遗忘门实现逐点乘法并将输入门和输出门的逐点乘法的结果相加来存储来自先前单元的信息。</li></ol><h2 id="7e85" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">限制</h2><ol class=""><li id="f524" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">计算量大且训练时间长。</li><li id="5467" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">复杂的架构，但无法完全消除渐变消失的问题。</li><li id="8e21" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">无法从当前时间步长的另一侧获取信息。</li></ol><h1 id="e833" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">GRU(门控循环单元)</h1><h2 id="1cbb" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">主要功能</h2><ol class=""><li id="b514" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">提高了LSTM的设计复杂度和训练速度。</li><li id="1e1f" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">GRU的结构:<em class="ij">更新门，重置门</em> <br/> - <strong class="ik hi"> <em class="ij">更新门</em> </strong>:从前一时间步(0–1)通过多少信息。LSTM中遗忘门和输入门的组合。<br/>-<strong class="ik hi">-<em class="ij">复位门</em> </strong>:要从之前的时间步长(0-1)中忘记多少信息。</li><li id="c4d0" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">它没有保存长期记忆的细胞状态。GRU只有隐藏状态，能够保存长期和短期依赖的有用信息。</li></ol><h2 id="6196" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">限制</h2><ol class=""><li id="5186" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">收敛慢，学习效率低(见<em class="ij">参考文献1 </em>)。</li></ol><h1 id="e2a9" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">双向长短期记忆</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mc"><img src="../Images/42d43a34bdb1eeedaa2647de6fa0c3f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jvjxL7Ew6XoGa9ZmLFAFg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Bidirectional LSTM Architecture</figcaption></figure><h2 id="c256" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">主要功能</h2><ol class=""><li id="cc5f" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">这是一个由两个LSTMs组成的强大算法。它包含先前和未来时间步的信息。</li><li id="d808" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">在序列的开头安装一个LSTM，在序列的结尾安装另一个LSTM。</li><li id="6c5c" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">提高模型捕获序列中上下文信息的能力。</li></ol><h2 id="149c" class="kx jk hh bd jl ky kz la jp lb lc ld jt jg le lf jx jh lg lh kb ji li lj kf lk bi translated">限制</h2><ol class=""><li id="0543" class="ll lm hh ik b il ln ip lo jg lp jh lq ji lr jf ls lt lu lv bi translated">在做预测之前，你需要完整的数据序列。因此，它在实时应用中的部署有许多限制</li><li id="dab6" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">计算成本高且速度慢。</li><li id="0635" class="ll lm hh ik b il lw ip lx jg ly jh lz ji ma jf ls lt lu lv bi translated">将深层叠加到一定程度会产生消失梯度问题。</li></ol><p id="02dc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">参考文献</em></p><ol class=""><li id="1cb7" class="ll lm hh ik b il im ip iq jg md jh me ji mf jf ls lt lu lv bi translated">物理学杂志| <a class="ae mg" href="https://iopscience.iop.org/article/10.1088/1742-6596/1325/1/012089/pdf" rel="noopener ugc nofollow" target="_blank">王欣等2019 J. Phys.: Conf。爵士。<strong class="ik hi"> 1325 </strong> 012089 </a></li></ol></div></div>    
</body>
</html>