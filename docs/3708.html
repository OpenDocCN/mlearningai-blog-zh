<html>
<head>
<title>Heart Disease Detection using fastai and sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用fastai和sklearn进行心脏病检测</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/heart-disease-detection-using-fastai-and-sklearn-a27df70ed19f?source=collection_archive---------3-----------------------#2022-10-11">https://medium.com/mlearning-ai/heart-disease-detection-using-fastai-and-sklearn-a27df70ed19f?source=collection_archive---------3-----------------------#2022-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9a78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自从我开始我的人工智能硕士课程以来，我一直在寻找一个框架，它将帮助我更多地使用我的软件开发技能，设计可用于生产的系统，并将一些重复的日常ML代码包装在一个刚刚工作的框架周围。Fastai是一个只用几行代码就能构建多层神经网络的工具，它以Pytorch为主干，有能力做到这一点。</p><p id="7217" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2015年，创造一个可以识别鸟类的计算机系统的想法被认为是如此具有挑战性，以至于它成为了这个XKCD笑话的基础:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/f846e02ea95d0f102ae52f06fe871166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*l6ZfCihx8nYHSiy1zbphbg.png"/></div></div></figure><p id="3c81" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用fastai，只需写几行代码就可以建立一个卷积神经网络，能够以超过95%的准确率识别鸟类。</p><p id="25fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将探讨如何使用决策树、随机森林、梯度Bossting和神经网络来构建心脏病检测系统。我们将在使用fastai完成一些日常任务时完成所有这些工作，如数据清理、特征工程、数据可视化和模型评估。jupyter笔记本和数据集可以在这个资源库中找到:<a class="ae jc" href="https://github.com/nandangrover/heart-disease-classifier" rel="noopener ugc nofollow" target="_blank">nandangrover/心脏病分类器</a>。</p><h1 id="290a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">描述数据集</h1><p id="4716" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated"><strong class="ig hi">图2.2.2 </strong>简要概述了所提供的数据集。存在的特征总数是13(我们在相同的数据帧中添加了一个类用于预处理)。在训练中存在30，000个例子，在测试数据集中存在30，000个例子。在浏览数据集之后，我们可以看到数据是干净的，没有丢失值。如<strong class="ig hi">图2.2.4 </strong>所示，对数据集应用了特征工程。探索性数据分析表明<strong class="ig hi"> thal </strong>对预测影响最大，而<strong class="ig hi">静息血压</strong>对心脏病预测影响最小。下面的<strong class="ig hi">图2.2.1 </strong>中列出了每个特性的总体影响。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/30dab8bc1a62f323509f9d4f9cf7f3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/0*ONt-2TmbJvTmKB7x"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Figure 2.2.1</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/d81b1ef9031b1fbed5cb8ca1504a8d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*itUV8gXNc0n-YnL1"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Figure 2.2.2</figcaption></figure><h1 id="9514" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">构建模型</h1><p id="111d" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">顾名思义，决策树集成依赖于决策树。决策树会询问一系列关于数据的二进制(即是或否)问题。每问完一个问题，树中该部分的数据被分为“是”和“否”两个分支，如图<strong class="ig hi">图2.2.3 </strong>。在一个或多个问题之后，或者可以基于所有先前的答案进行预测，或者需要另一个问题。就处理时间和可伸缩性而言，它们比神经网络更好地进行分类，正如我们将在下面看到的。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="82f1" class="ld jq hh kz b fi le lf l lg lh">test_x = pd.read_csv('data/x_test.csv', low_memory=False)<br/>test_y = pd.read_csv('data/y_test.csv', low_memory=False)</span></pre><p id="129c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们首先使用熊猫导入数据，如上图所示<strong class="ig hi">。</strong></p><p id="c9f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了可视化决策树模型，我们首先构建一个max_lead_nodes设置为10的模型。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="432b" class="ld jq hh kz b fi le lf l lg lh">model = DecisionTreeClassifier(max_leaf_nodes=10)<br/>model.fit(xs, y);</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es li"><img src="../Images/0e543f8eac6e348df48b2d2b8920cdab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k66mMmiBPUQBWDxYRWoa8w.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx"><strong class="bd jr">Figure 2.2.3</strong></figcaption></figure><p id="e921" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当所有数据都在一个组中时，顶部节点表示任何拆分完成之前的初始模型。这是最简单的可能模型。它是询问零个问题的结果，并且将始终预测整个数据集的平均值。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="705f" class="ld jq hh kz b fi le lf l lg lh">model_decision = DecisionTreeClassifier(min_samples_leaf=25)<br/>model_decision.fit(xs, y)<br/>cross_val_score(model_decision, valid_xs, valid_y, cv=10)</span></pre><p id="3d8b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Sklearn的默认设置允许它继续拆分节点，直到每个叶节点中只有一个项目。因此，我们将min_sample_leaf设置更改为25，这样我们的模型就不会过拟合。在没有指定min_sample_leaf的情况下，也测量了准确度，仅为86%。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="e959" class="ld jq hh kz b fi le lf l lg lh">array([0.86806667, 0.86516667, 0.8636    , 0.8647    , 0.86746667, 0.8649    , 0.86413333, 0.86556667, 0.86706667, 0.86486667])</span></pre><p id="74db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将min_sample_leaf设置为25后，10个时期的交叉验证准确度如下所示。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="2716" class="ld jq hh kz b fi le lf l lg lh">array([0.88946667, 0.88743333, 0.8857    , 0.89053333, 0.88933333, 0.8873    , 0.8874    , 0.88836667, 0.88936667, 0.88643333])</span></pre><p id="4132" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们看到决策树的平均准确率约为89%。在下一节中，我们将看到移除低重要性变量是否对准确度有任何影响</p><h1 id="5375" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">去除低重要性变量的决策树的精度</h1><p id="69e5" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们想知道一个模型是如何产生预测的，除了仅仅知道它可以准确地做到这一点。这通过特征显著性来揭示。通过查看特征重要性属性，我们可以直接获得它们。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="b17f" class="ld jq hh kz b fi le lf l lg lh">def rf_feat_importance(model, df):<br/>   return pd.DataFrame({'cols':df.columns, 'imp':model.feature_importances_}<br/>                      ).sort_values('imp', ascending=False)</span></pre><p id="f639" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过消除低值的因素，似乎合理的是，我们可能只使用一部分列，但仍能获得良好的结果。因此我们删除了所有重要性小于0.005的特征(<strong class="ig hi">图2.2.4 </strong>)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/81fbbb071ce52d5f77f08fd66471f4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e4Eey0ez00kBmVzX"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Figure 2.2.4</figcaption></figure><p id="668d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们观察到，在去除不重要的特征后，准确度保持不变。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="7e8b" class="ld jq hh kz b fi le lf l lg lh">array([0.88943333, 0.88733333, 0.88553333, 0.89046667, 0.88896667, 0.8874    , 0.88736667, 0.88873333, 0.88933333, 0.88643333])</span></pre><h1 id="8013" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">模型评估</h1><p id="7a52" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们可以想象数据是如何从第一列thal，然后是前四页的胸部分割出来的。我们可以看到决策树算法已经成功地使用thal值分成了两个不同值的组。这可以通过使用特伦斯·帕尔强大的<a class="ae jc" href="https://explained.ai/decision-tree-viz/" rel="noopener ugc nofollow" target="_blank">dtrewiz</a>库来实现(图2.2.5)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/348ce25bfb2801816655e902ebb8965f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZGQ3NTnLpd6sAMnektxtw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx"><strong class="bd jr">Figure 2.2.5</strong></figcaption></figure><h1 id="dfd4" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">随机森林、梯度增强和神经网络</h1><p id="ddaf" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们已经建立了一个使用决策树集成的分类器，即使用随机森林算法和梯度推进。还构建了一个神经网络，以比较实现决策树所需的准确性、可伸缩性、时间和工作量。</p><h2 id="d6a5" class="ld jq hh bd jr ll lm ln jv lo lp lq jz ip lr ls kd it lt lu kh ix lv lw kl lx bi translated">随机森林</h2><p id="71f7" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">下面的函数规范指定了我们需要的估计数、为每棵树的训练采样的最大行数，以及在每个拆分点采样的最大列数(其中0.5表示“取总列数的一半”)。我们用于决策树的相同最小样本叶参数也可以用于定义何时停止分裂树节点，从而限制树的深度。最后，我们指示Sklearn通过传递n个jobs=-1来同时创建树。结果达到的最终准确度为90%。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="68ab" class="ld jq hh kz b fi le lf l lg lh">def rf(xs, y, n_estimators=200, max_samples=200_000,<br/>      max_features=0.5, min_samples_leaf=10, **kwargs):<br/>   return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,<br/>       max_samples=max_samples, max_features=max_features,<br/>       min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)</span></pre><h2 id="95da" class="ld jq hh bd jr ll lm ln jv lo lp lq jz ip lr ls kd it lt lu kh ix lv lw kl lx bi translated">梯度推进</h2><p id="d678" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们使用了histgradientsboostingclassifier，它具有对缺失值(nan)的本地支持。在训练期间，基于潜在的增益，树生长器在每个分裂点学习具有缺失值的样本应该去往左边还是右边的孩子。当预测时，具有缺失值的样本因此被分配给左边或右边的孩子。如果在训练期间对于给定的特征没有遇到缺失值，那么具有缺失值的样本被映射到具有最多样本的子代。结果达到的最终准确度是90%</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="90b3" class="ld jq hh kz b fi le lf l lg lh">model_gradient = HistGradientBoostingClassifier().fit(xs_imp, y)</span></pre><h2 id="626f" class="ld jq hh bd jr ll lm ln jv lo lp lq jz ip lr ls kd it lt lu kh ix lv lw kl lx bi translated">神经网络</h2><p id="cfdb" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">通过超过60，000个实例和5个时期，该网络被引入超过300，000个实例。对于神经网络来说，这是一个可以学习的大量数据。神经网络的精度是0.55，低于决策树，因为硬件资源有限(有限我指的是80GB的ram和来自Google collab的TPU)。神经网络也比决策树慢。我们使用了快速人工智能的表格学习器来建立神经网络。默认情况下，它使用2层，第一层有200次激活，第二层有100次激活。我们决定使用默认值，因为我们正在比较神经网络和决策树，并没有积极地尝试提高准确性。结果达到的最终准确度为55.75%，可以通过调整学习速率(使用fastai的lr_find()方法)和为更多时期训练模型来提高该准确度。</p><pre class="je jf jg jh fd ky kz la lb aw lc bi"><span id="9922" class="ld jq hh kz b fi le lf l lg lh">df = TabularPandas(combined_data, procs_nn, list(to_keep), [], y_names='class', splits=splits)<br/>dataLaoders = df.dataloaders(64)<br/><br/>learn = tabular_learner(dataLaoders)</span><span id="dc54" class="ld jq hh kz b fi ly lf l lg lh">if os.path.exists('data/models/nn'):<br/> learn.load('data/models/nn.pkl')<br/>else:<br/> learn.fit_one_cycle(5, 1e-2)</span></pre><h1 id="b012" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">参考</h1><ul class=""><li id="3e7c" class="lz ma hh ig b ih kn il ko ip mb it mc ix md jb me mf mg mh bi translated">代码库:<a class="ae jc" href="https://github.com/nandangrover/heart-disease-classifier" rel="noopener ugc nofollow" target="_blank">nandangrover/心脏病检测</a></li><li id="4c26" class="lz ma hh ig b ih mi il mj ip mk it ml ix mm jb me mf mg mh bi translated">快速AI课程:<a class="ae jc" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> https://course.fast.ai </a></li><li id="87c3" class="lz ma hh ig b ih mi il mj ip mk it ml ix mm jb me mf mg mh bi translated">昆兰，J. R. (1986)。决策树归纳。在<em class="mn">机器学习</em>(第一卷)。</li><li id="a09e" class="lz ma hh ig b ih mi il mj ip mk it ml ix mm jb me mf mg mh bi translated">张，林，齐，等(1993)。决策网络的计算理论。在<em class="mn">国际近似推理杂志</em>(第11卷)。</li></ul><div class="mo mp ez fb mq mr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">medium.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf jn mr"/></div></div></a></div></div></div>    
</body>
</html>