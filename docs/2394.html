<html>
<head>
<title>NLP-Day 20: You Better Pay Attention To Transformers (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP-第20天:你最好关注变形金刚(第2部分)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/nlp-day-20-you-better-pay-attention-to-transformers-part-2-c6889a0a301b?source=collection_archive---------1-----------------------#2022-04-26">https://medium.com/mlearning-ai/nlp-day-20-you-better-pay-attention-to-transformers-part-2-c6889a0a301b?source=collection_archive---------1-----------------------#2022-04-26</a></blockquote><div><div class="dt ha hb hc hd he"/><div class="hf hg hh hi hj"><h2 id="ce69" class="hk hl hm bd b fq hn ho hp hq hr hs dy ht translated" aria-label="kicker paragraph"># 30日</h2><div class=""/><div class=""><h2 id="35d8" class="pw-subtitle-paragraph is hv hm bd b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj dy translated">了解一般和变压器注意机制</h2></div><figure class="jl jm jn jo fe jp es et paragraph-image"><div class="es et jk"><img src="../Images/ca1daa3d7c8fbd01f77ec6837755fd86.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*ogqaSt3XDfawo46zDZxrcQ.png"/></div><figcaption class="js jt eu es et ju jv bd b be z dy">Transformer-based architecture #30DaysOfNLP [Image by Author]</figcaption></figure><p id="a579" class="pw-post-body-paragraph jw jx hm jy b jz ka iw kb kc kd iz ke kf kg kh ki kj kk kl km kn ko kp kq kr hf bi translated"><a class="ae ks" rel="noopener" href="/mlearning-ai/nlp-day-19-you-better-pay-attention-to-transformers-part-1-3b1784b2a7ee"> <strong class="jy hw">在上一集</strong> </a>中，我们温和地介绍了一般意义上以及机器学习背景下的注意力概念。然而，我们故意停留在表面，让我们得到一个总的看法。</p></div></div>    
</body>
</html>