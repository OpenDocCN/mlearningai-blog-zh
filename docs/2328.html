<html>
<head>
<title>Mastering TicTacToe with AlphaZero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用AlphaZero掌握TicTacToe</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/mastering-tictactoe-with-alphazero-cc28998bf36c?source=collection_archive---------3-----------------------#2022-04-15">https://medium.com/mlearning-ai/mastering-tictactoe-with-alphazero-cc28998bf36c?source=collection_archive---------3-----------------------#2022-04-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="081f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从头开始编写AlphaZero算法来玩TicTacToe的游戏，它从来没有输过！！</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="a3fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">AlphaZero(或者它更著名的前身AlphaGo)在人工智能领域取得了最著名的突破之一。能够在国际象棋、日本兵棋和围棋比赛中取得超人的表现，让网飞的电影讲述其成就(alpha go——这部电影),都是不小的成就。</p><p id="882a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">AlphaZero是由谷歌的人工智能研究部门DeepMind在2017年开发的。它能够完全通过自我游戏在各种<em class="jj">完美信息游戏</em>中达到超人的表现……这意味着它没有通过观察人类来学习玩游戏，而是通过与自己对抗来制定游戏策略。此外，没有硬编码的规则或启发来指导游戏。这意味着同样的算法可以推广到玩任何游戏(<a class="ae jk" href="https://en.wikipedia.org/wiki/Perfect_information" rel="noopener ugc nofollow" target="_blank"><em class="jj"/></a>)的完美信息游戏。这非常非常令人印象深刻。</p><p id="c437" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我将讨论AlphaZero背后的一般直觉，并解释算法中的各种组件和过程。我修改了论文里的算法来玩TicTacToe。我在这里放了一个你可以用<a class="ae jk" href="https://alphazerotictactoe.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">玩的网络演示。你可以在我的</a><a class="ae jk" href="https://github.com/kvsnoufal/alphazeroTicTacToe" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> Github </em> </a>上找到实现的完整代码。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/8ff6d2c6c76183ea3638f6b39a6a4cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*pewmDS3RWq-C9X5a2p1lPQ.gif"/></div></div></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/dd6013a4b918785fbd314acb0ab8bc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lvth4fldJOkOE9zJTqL_8Q.gif"/></div></div></figure><blockquote class="jx jy jz"><p id="5d62" class="ie if jj ig b ih ii ij ik il im in io ka iq ir is kb iu iv iw kc iy iz ja jb ha bi translated">机器人永远不会输！！它每场比赛都会赢或平。自己试试:<a class="ae jk" href="https://alphazerotictactoe.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">https://alphazerotictactoe.herokuapp.com/</a></p></blockquote><h1 id="5341" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">它是如何工作的？</h1><p id="4a34" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">AlphaZero的主要吸引力之一是它非常简单。算法简单易懂，直观。AlphaZero由两个主要部分组成:</p><ol class=""><li id="710f" class="lg lh hh ig b ih ii il im ip li it lj ix lk jb ll lm ln lo bi translated"><strong class="ig hi">本能:</strong></li></ol><p id="ba6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这包括<strong class="ig hi">值</strong>和<strong class="ig hi">策略神经网络</strong>。给定游戏的当前状态，价值网络给出玩家赢得游戏的偏好。策略网络为给定的状态建议下一步的动作。这两个函数都在模拟人类本能的算法等价物——人类本能是从我们过去的经验中发展出来的。我们将在后面讨论如何训练这两个网络。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lp"><img src="../Images/5b4c35f8b70334afe4a3065f7f32e188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADwqZTC1AQT4U96HkQQJzQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx">Value &amp; Policy Networks</figcaption></figure><p id="d442" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2。向前看:</strong></p><p id="853d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个利用你的直觉考虑几个假设场景，然后根据你考虑的场景的结果决定下一步的最佳行动的过程。这是通过蒙特卡罗树搜索算法实现的。我们将在下一节更详细地讨论这一点。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lu"><img src="../Images/c9dd8f3aaac2530a1f136d7a591e13e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAiKcVtbmjKY9cKNBi8-CA.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx">Looking Ahead -example</figcaption></figure><h2 id="ad2f" class="lv ke hh bd kf lw lx ly kj lz ma mb kn ip mc md kr it me mf kv ix mg mh kz mi bi translated">下一步怎么走？</h2><p id="28c7" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">选择下一步有两个步骤:</p><ol class=""><li id="8530" class="lg lh hh ig b ih ii il im ip li it lj ix lk jb ll lm ln lo bi translated">知情前瞻(使用价值和政策网络的蒙特卡洛树搜索)</li><li id="1035" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated">移动选择(选择在MCTS访问次数最多的活动)</li></ol><p id="50eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们试着用一个例子来解释这个。比方说游戏的当前状态如下:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es mo"><img src="../Images/340b14fd93c45033ab2f59b0ac70a68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*HDwXYoOQnSm4owqCKWWvOA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx">Current Game State</figcaption></figure><p id="88e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一步:知情前瞻</strong></p><p id="64b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对所有可能的未来可能性进行彻底搜索在计算上通常是不可行的。所以我们决定进行有限次数的模拟(向前看)，比如N=5。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mp"><img src="../Images/46ea42c97fbe56abc9c7db98730dff65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c13kaYHeMyXZ2V8qxlQgvg.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx">3 simulations — look ahead</figcaption></figure><p id="4590" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当决定进一步研究哪些未来可能性时，我们依靠预先训练的价值网络(本能)来指导我们。选择能产生最高价值的前瞻性举措将是开发方法。有时我们会做一个探索性的前瞻，选择一个非最大价值的行动。在每次模拟结束时，我们回滚结果，以分配每次移动/状态的平均获胜可能性。未来的模拟由这个分数决定。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mq"><img src="../Images/b1741420562e90d6ea6d74739bd3dfe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipFV70CSg4rzYwQqJZoKXg.png"/></div></div></figure><p id="8150" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这将确保在前瞻模拟期间更频繁地选择具有更高平均获胜可能性的移动。因此，良好举措产生的州对MCTS的访问次数会更多。</p><p id="64f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二步:移动选择</strong></p><p id="416f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在MCTS之后(向前看)，我们将会有这样一个州树:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mr"><img src="../Images/3fd5063e87f990a3dcec32206f14a247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXz9AwwE33PqQgfCozOCmw.png"/></div></div></figure><p id="9873" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jj">访问量越高，移动越好。然后，我们选择访问量最高的移动。</em> </strong></p><p id="8c46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">AlphaZero就是这样决定下一步该怎么走的。很直观吧？！！！</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h2 id="b37c" class="lv ke hh bd kf lw lx ly kj lz ma mb kn ip mc md kr it me mf kv ix mg mh kz mi bi translated">本能是如何建立的？</h2><p id="26ba" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">本能成分是价值网络和政策网络。这些本质上是需要数据来训练的神经网络。数据是通过自我游戏(self play)收集的，这是一种与自己对抗的算法。</p><blockquote class="jx jy jz"><p id="8e15" class="ie if jj ig b ih ii ij ik il im in io ka iq ir is kb iu iv iw kc iy iz ja jb ha bi translated">需要指出的是，通过自我游戏收集的数据并不详尽，因此不会涵盖所有可能的游戏状态和动作。因此，在此基础上训练的模型只会包含部分游戏知识。这就是为什么通过MCTS试探性打法在上一节详述移动选择阶段很重要</p></blockquote><p id="2702" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">价值网络将棋盘的状态作为输入，并预测玩家赢得游戏的可能性。这个数据集可以从收集的数据中设计出来。</p><p id="e4d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">策略网络将博弈的状态作为输入，并预测每一步棋的平均获胜概率。该网络旨在预测MCTS的结果——每次允许移动的归一化访问次数。这个值在自播放期间被记录，然后用于训练网络。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="c9f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章的目的是给AlphaZero一个高层次的直觉。我不得不省略几个实现细节。请检查我的<a class="ae jk" href="https://github.com/kvsnoufal/alphazeroTicTacToe" rel="noopener ugc nofollow" target="_blank"> Github </a>上的代码。</p><p id="d5bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Github回购:<a class="ae jk" href="https://github.com/kvsnoufal/alphazeroTicTacToe" rel="noopener ugc nofollow" target="_blank">https://github.com/kvsnoufal/alphazeroTicTacToe</a></p><p id="6e5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">演示:<a class="ae jk" href="https://alphazerotictactoe.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">https://alphazerotictactoe.herokuapp.com/</a></p><p id="074b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">巨人之肩</strong></p><ol class=""><li id="7390" class="lg lh hh ig b ih ii il im ip li it lj ix lk jb ll lm ln lo bi translated">在没有人类知识的情况下掌握Go游戏:<a class="ae jk" href="https://www.nature.com/articles/nature24270" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/nature24270</a></li><li id="207f" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated"><a class="ae jk" href="http://joshvarty.github.io/AlphaZero/" rel="noopener ugc nofollow" target="_blank">http://joshvarty.github.io/AlphaZero/</a></li><li id="6ddb" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated">alpha zero cheat sheet:<a class="ae jk" rel="noopener" href="/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0">https://media . com/applied-data-science/alpha go-zero-解释性一元图-365f5abf67e0 </a></li><li id="132b" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated">一个简单的阿尔法(Go)零教程:<a class="ae jk" href="https://web.stanford.edu/~surag/posts/alphazero.html" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~surag/posts/alphazero.html</a></li><li id="e32e" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated"><a class="ae jk" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Monte_Carlo_tree_search</a></li><li id="7cb8" class="lg lh hh ig b ih mj il mk ip ml it mm ix mn jb ll lm ln lo bi translated"><a class="ae jk" href="https://www.youtube.com/watch?v=MPXGiowUr0o&amp;ab_channel=SkowstertheGeek" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=MPXGiowUr0o&amp;ab _ channel = skowster geek</a></li></ol></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><blockquote class="jx jy jz"><p id="b4f7" class="ie if jj ig b ih ii ij ik il im in io ka iq ir is kb iu iv iw kc iy iz ja jb ha bi translated"><strong class="ig hi">关于作者</strong></p><p id="ec2e" class="ie if jj ig b ih ii ij ik il im in io ka iq ir is kb iu iv iw kc iy iz ja jb ha bi translated">我在阿联酋迪拜控股公司担任首席数据科学家。你可以联系我在kvsnoufal@gmail.com或<a class="ae jk" href="https://www.linkedin.com/in/kvsnoufal/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/kvsnoufal/</a></p></blockquote><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hi fi z dy na ea eb nb ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">如何成为Mlearning.ai的作者</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj jv mv"/></div></div></a></div></div></div>    
</body>
</html>