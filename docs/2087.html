<html>
<head>
<title>ML-K-means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML-K-均值聚类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/ml-k-means-clustering-5c11c1d2577b?source=collection_archive---------4-----------------------#2022-03-05">https://medium.com/mlearning-ai/ml-k-means-clustering-5c11c1d2577b?source=collection_archive---------4-----------------------#2022-03-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="71ab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">无监督学习</p><h2 id="8396" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">无监督&amp;聚类？</h2><p id="662f" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">现实生活中数据并不总是有标签的。我们用来在这些数据中寻找模式的方法被称为“无监督学习”。聚类是一种众所周知的无监督学习方法。它将相似的数据分组，以便我们可以找到数据的模式/结构。</p><h2 id="0397" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">K-means？</h2><p id="6bcc" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">“k”是我们想要将数据分组到的簇的数量。“平均值”是样本(数据点)和聚类中心之间的平均距离。我们称星团的中心为“质心”。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/0e9545a9985ecaca04ae7655bf72ac8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmvsQ4XaOxeb-TmKk1qgOw.png"/></div></div></figure><h2 id="c44a" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">k均值过程</h2><ol class=""><li id="e840" class="ko kp hh ig b ih jx il jy ip kq it kr ix ks jb kt ku kv kw bi translated">在数据集中指定k个质心。</li><li id="47dd" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">将数据示例分配给最近的集群。</li><li id="4bbf" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">计算聚类中示例的平均位置，并为每个聚类指定(移动)质心。</li><li id="2f2d" class="ko kp hh ig b ih kx il ky ip kz it la ix lb jb kt ku kv kw bi translated">重复2~3，直到质心保持不动。</li></ol><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lc"><img src="../Images/104cef06afe61de1b12e5b38267b5077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wQ41KvCyUX-NbDhw.gif"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">from <a class="ae lh" href="https://eunsukimme.github.io/ml/2019/12/16/K-Means/" rel="noopener ugc nofollow" target="_blank">https://eunsukimme.github.io/ml/2019/12/16/K-Means/</a></figcaption></figure><p id="026f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二个过程中的“最近”是指数据示例和质心之间的欧几里德距离。</p><h2 id="7e06" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">多少k？</h2><p id="24b9" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">k-means算法试图最小化失真，失真被定义为每个数据样本向量与其支配质心之间的平方距离之和。这个和可以高也可以低，我们可以画一个曲线图，记录下失真的变化。</p><p id="c900" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这叫做“肘法”。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es li"><img src="../Images/f6d5dd3c17ca312abd69baefda1b8e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*c3H3sJfBDTXB9CQCn-Keow.png"/></div></figure><p id="6c2e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们找到适当的k值，失真会慢慢减小。根据这个规律，我们可以找出合适的k。</p><p id="38f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，还有另一种方法被称为“剪影分析”。</p><p id="cc18" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">剪影分析可用于研究所得聚类之间的分离距离。轮廓图显示了一个聚类中的每个点与相邻聚类中的点的接近程度，从而提供了一种可视化评估聚类数量等参数的方法。这个度量的范围是[-1，1]。</p><p id="9549" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接近+1的轮廓系数(这些值被称为)表示样本远离相邻聚类。值为0表示样本位于或非常接近两个相邻聚类之间的判定边界，负值表示这些样本可能被分配到错误的聚类。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lj"><img src="../Images/07e21bc0f96590a6c0b01258f7b049f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*8R5d88l8jLDsEAgK4VjezA.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">K=4 is the most efficient way here</figcaption></figure><p id="ac86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">k-means算法的可视化代码，可以访问我的<a class="ae lh" href="https://github.com/ShinyJay2/Uni_3-2_ML/blob/main/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5%2006/K-means%20Algorithm.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>。虽然是用韩语写的，但是希望代码能有所帮助。</p><p id="88b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对了，这是我ML系列的结尾。下次我会带着新的系列回来。感谢您的阅读！</p><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hi fi z dy ls ea eb lt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb km ln"/></div></div></a></div></div></div>    
</body>
</html>