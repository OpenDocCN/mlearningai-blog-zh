<html>
<head>
<title>Fine-Tuning BERT using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow微调BERT</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fine-tuning-bert-using-tensorflow-21368d8414ba?source=collection_archive---------1-----------------------#2022-08-15">https://medium.com/mlearning-ai/fine-tuning-bert-using-tensorflow-21368d8414ba?source=collection_archive---------1-----------------------#2022-08-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/a8a888cc7b4b7ad4419da1862b9eb7c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A6S8Bij_jHIHx9F9"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@helloimnik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hello I'm Nik</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e056" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">大型预训练的基于转换器的语言模型(PLMs) </strong>如伯特和GPT已经彻底改变了自然语言处理(NLP)领域。PLMs在NLP领域创造了一种范式转换。传统的统计NLP方法(例如，文本分类任务)通常设计手工制作的特征，并应用诸如逻辑回归或支持向量机的机器学习模型来从那些手工制作的特征中学习分类功能。另一方面，深度学习方法除了分类功能之外，还通过深度神经网络学习潜在的特征表示。</p><p id="5a50" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这种范式转变包括<em class="js">“微调”</em>，这意味着在一个共享的、基本的预训练任务上训练一个大型模型，然后在第二步中使其适应各种任务。这样，我们可以利用语言模型的性能来适应我们的特定问题。</p><p id="b26e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">理想情况下，我们可以在最终的BERT层之后添加一个附加层，并以较小的学习速率(<em class="js"> 1e-5至5e-5 </em>)和<em class="js"> adam optimizer </em>)重新训练整个网络几个时期。在这个小故事中，我们将尝试学习如何根据我们的数据微调BERT基本模型。出于演示的目的，我们将使用从<a class="ae it" href="https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>派生的用于NLP 的<em class="js">情感数据集。让我们直接进入代码。</em></p><h2 id="0242" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">加载和预处理原始数据</h2><p id="6c56" class="pw-post-body-paragraph iu iv hh iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">首先，我们需要加载和预处理我们的文本数据。为了做到这一点，我们将加载前一段中提到的数据，并做一些文本预处理步骤。</p><figure class="kt ku kv kw fd ii"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="d4c1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用整个数据集，包括训练集、测试集和验证集，并将它们连接起来形成一个完整的数据集。对于预处理步骤，我们将删除链接、非ASCII字符、电子邮件地址、标点符号、下划线和大小写折叠。在这些步骤之后，我们必须在做一些预处理步骤之后计算数据的最大长度。我们需要这个数字来微调BERT模型，在这种情况下，我们得到的最大值是65个单词。我们还需要对标签进行编码，并制作一个字典以备不时之需。</p><h2 id="8a2a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">加载模型和标记器</h2><p id="6002" class="pw-post-body-paragraph iu iv hh iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">接下来的步骤是训练模型，加载数据，并用BERT记号化器对数据进行记号化。我们将按照70:30的比例划分数据并对其进行分层，因为我们正在处理一个不平衡的数据集。之后，我们使用代码中提到的配置将BERT记号赋予器应用于整个数据集。我们使用的最大长度等于70，因为我们的句子的最大长度是65。注意，我们将不使用<em class="js"> token_type_ids </em>，而仅使用<em class="js"> input_ids </em>和<em class="js"> attention_mask </em>。</p><figure class="kt ku kv kw fd ii"><div class="bz dy l di"><div class="kx ky l"/></div></figure><h2 id="b665" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">模型拟合</h2><p id="ee09" class="pw-post-body-paragraph iu iv hh iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在我们完成了所有的预处理步骤之后，我们就准备好去拟合模型了。注意，我们只使用了<em class="js">输入标识</em>和<em class="js">注意掩码</em>来传递给BERT模型。由于我们要处理6个类，因此我们将在输出层使用6个神经元及其softmax激活函数。此外，我们将使用Adam优化器，其学习速率非常小，建议用于微调目的，并且仅用于1个时期。</p><figure class="kt ku kv kw fd ii"><div class="bz dy l di"><div class="kx ky l"/></div></figure><h2 id="941d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">模型评估</h2><p id="3de5" class="pw-post-body-paragraph iu iv hh iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">下一步是用测试数据评估我们微调过的BERT模型。结果相当好，因为我们在所有课程中都获得了超过80%的f1分数。</p><figure class="kt ku kv kw fd ii"><div class="bz dy l di"><div class="kx ky l"/></div></figure><figure class="kt ku kv kw fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/9faa1b4a68d411673abebc520f45737c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*wJZ-R2T6q0H4HdERUIKVaQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Evaluation result</figcaption></figure><p id="2235" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请记住，我们可以“玩”超参数，以便为我们的具体问题获得更好的结果。</p><p id="2c57" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">参考文献:</strong></p><ul class=""><li id="c624" class="la lb hh iw b ix iy jb jc jf lc jj ld jn le jr lf lg lh li bi translated"><a class="ae it" href="https://arxiv.org/abs/2111.01243" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.01243</a></li><li id="b271" class="la lb hh iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated"><a class="ae it" href="https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/praveengovi/emotions-dataset-for-NLP</a></li><li id="0537" class="la lb hh iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated"><a class="ae it" href="https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bert#transformers.TFBertModel" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/docs/transformers/v 4 . 20 . 1/en/model _ doc/Bert # transformers。TFBertModel </a></li></ul></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><p id="9ce0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢阅读😄</p><div class="lv lw ez fb lx ly"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hi fi z dy md ea eb me ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm in ly"/></div></div></a></div></div></div>    
</body>
</html>