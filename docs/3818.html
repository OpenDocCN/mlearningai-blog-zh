<html>
<head>
<title>The need for Bidirectional Encoder Representations from Transformers (BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自变压器(BERT)的双向编码器表示的需求</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/the-need-for-bidirectional-encoder-representations-from-transformers-bert-7d8702aab5eb?source=collection_archive---------4-----------------------#2022-10-25">https://medium.com/mlearning-ai/the-need-for-bidirectional-encoder-representations-from-transformers-bert-7d8702aab5eb?source=collection_archive---------4-----------------------#2022-10-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c2f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来自变压器的双向编码器表示(BERT)是一个用于处理NLP的免费开源机器学习框架。BERT使用周围的文本来提供上下文，以便帮助计算机理解文本中歧义词的含义。在问答数据集的帮助下，BERT框架可以在根据维基百科的文本进行预训练后进行调整。它的目标是产生一个语言模型。我们也可以说BERT是为NLP设计的变压器神经网络架构。</p><p id="9b59" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然它建立在深度学习技术的基础上，但它需要大量的处理能力才能正常工作。不要总是试图从零开始训练这些模型，建议使用可公开访问的预训练模型作为起点。</p><p id="7d3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在BERT诞生之前，大多数模型只能单向处理文本。然而，BERT通过处理一个单词的上下文或者一般来说从从左到右和从右到左两个方向处理文本改变了这个游戏。我们称之为双向的。BERT使用Transformer，这是一种学习文本中单词(或子单词)之间上下文关系的注意力机制。</p><p id="2fb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以现在你可以看出，BERT的主要技术进步是将Transformer的双向训练(一种很受欢迎的注意力模型)应用于语言建模。相比之下，早期的研究从从左到右或者从左到右和从右到左相结合的训练角度来看待文本序列。这项研究的发现表明，双向训练的语言模型比单向语言模型更能理解语境和语言的流动。该论文的作者描述了一种称为掩蔽LM (MLM)的独特方法，这种方法使双向训练在以前不可行的模型中成为可能。该模型的架构使得有效地理解句子中的单词和上下文成为可能。</p><p id="bd49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在你可能会问什么是<strong class="ig hi">变形金刚</strong>，嗯？</p><blockquote class="jc jd je"><p id="044a" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">Transformer是一种深度学习模型，采用自我关注机制，对输入数据的每个部分的重要性进行不同的加权。</p></blockquote><p id="a38e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简而言之，transformer包括两个独立的机制:解码器和编码器。编码器读取文本输入，解码器产生预测。</p><p id="991c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BERT的主要技术进步是将Transformer的双向训练(一种很受欢迎的注意力模型)应用于语言建模。相比之下，早期的研究从从左到右或者从左到右和从右到左相结合的训练角度来看待文本序列。</p><p id="1fa1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">研究表明，双向训练的语言模型比单向语言模型能更深刻地理解语境和语言流程。该论文的作者描述了一种称为掩蔽LM (MLM)的独特方法，这种方法使双向训练在以前不可行的模型中成为可能。该模型的架构使得有效地理解句子中的单词和上下文成为可能。通过掩蔽15%的记号来训练BERT，目的是猜测它们。</p><h1 id="666a" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">为什么是伯特？</h1><p id="9634" class="pw-post-body-paragraph ie if hh ig b ih kh ij ik il ki in io ip kj ir is it kk iv iw ix kl iz ja jb ha bi translated">然而，Transformer是第一个仅利用自我注意而不使用卷积或序列对齐rnn来生成其输入和输出表示的转导模型。这意味着这个模型使得成功的句子嵌入比以前更有可能。实际上，基于RNN的设计在学习输入和输出序列中的长程相关性时会有困难，并且在并行化方面也很有挑战性。BERT是架构突破的结果，也是通过屏蔽一个或多个单词来使用这一概念训练网络的结果。</p><blockquote class="jc jd je"><p id="9786" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">屏蔽语言模型(MLM):随机屏蔽掉输入中15%的单词——用[屏蔽]标记替换它们——通过基于BERT注意力的编码器运行整个序列，然后根据序列中其他非屏蔽单词提供的上下文，仅预测屏蔽单词。</p><p id="a3d3" class="ie if jf ig b ih ii ij ik il im in io jg iq ir is jh iu iv iw ji iy iz ja jb ha bi translated">伯特训练过程也使用下一句预测(NSP)。在训练期间，该模型将句子对作为输入，并学习预测第二个句子是否也是原文中的下一个句子。</p></blockquote><p id="28f4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">伯特之所以是理想的选择，是因为这个模型同时接受了MLM和NSP的训练。这是为了最小化两种策略的组合损失函数。</p><div class="km kn ez fb ko kp"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hi fi z dy ku ea eb kv ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">medium.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld le kp"/></div></div></a></div></div></div>    
</body>
</html>