<html>
<head>
<title>Word Embedding Models for Low-Resourced GOA’n Konkani Language: NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">资源匮乏的果阿孔卡语的单词嵌入模型:NLP</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/word-embedding-models-for-low-resourced-goan-konkani-language-nlp-c1ac452e86a?source=collection_archive---------5-----------------------#2021-04-03">https://medium.com/mlearning-ai/word-embedding-models-for-low-resourced-goan-konkani-language-nlp-c1ac452e86a?source=collection_archive---------5-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/3c2d06e0703be914118bf2b95c507dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HnNn-FBvkr9FKAH0fb4gEw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Image by Author (Satrem Waterfall)</figcaption></figure><p id="c1bf" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><em class="jr">这篇博文参考了我提交的由</em> <strong class="iv hi"> <em class="jr"> CIE IIIT海德拉巴</em> </strong> <em class="jr">举办的</em><strong class="iv hi"><em class="jr">NLP HACK 2021</em></strong><em class="jr">。黑客马拉松的主题是研究</em> <strong class="iv hi"> <em class="jr">印度语言</em> </strong> <em class="jr">。尽管在AI/ML &amp; NLP领域取得了如此多的进步，但在资源匮乏的印度语言方面却几乎没有什么进展。Konkani是一种不采用基于AI/ML的NLP技术的语言。作为一名果阿人，我决定研究果阿的母语。</em> <strong class="iv hi"> <em class="jr">我能够训练3个单词嵌入&amp;开发一个基于Streamlit的应用程序。详情请跟随。</em> </strong></p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="4308" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">康卡尼简介</strong></h1><p id="5418" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi lc translated"><span class="l ld le lf bm lg lh li lj lk di">G</span>T28】OA🏖️是印度最小的邦。它不仅是印度，也是全球最受欢迎的旅游目的地。🌴果阿以其美丽的海滩、寺庙、教堂、古老的建筑结构、富饶的腹地而闻名，还有不可忘记的——腰果和菲尼🍹<em class="jr">。</em></p><p id="aea5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><a class="ae ll" href="https://en.m.wikipedia.org/wiki/Konkani_language" rel="noopener ugc nofollow" target="_blank"> <strong class="iv hi"> <em class="jr">康卡尼语</em> </strong> </a>是果阿邦的官方语言<em class="jr">🌴</em>。康卡尼语有许多方言，根据人口统计和宗教信仰而有所不同。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/f3b4eb4b0aae9a0d77a6b339cb3ff9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*YCOGAeKjjWo_NAeLC2B2ZA.png"/></div></figure><p id="dc40" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">康卡尼语可以用梵文、罗马文、卡纳达文、马拉雅拉姆文和波斯阿拉伯文书写。然而，<strong class="iv hi"> <em class="jr">梵文</em> </strong> <em class="jr">才是正式的书写文字。</em></p><blockquote class="lr ls lt"><p id="d3c6" class="it iu jr iv b iw ix iy iz ja jb jc jd lu jf jg jh lv jj jk jl lw jn jo jp jq ha bi translated">由于缺乏数字格式的文本，康卡尼语被认为是资源匮乏的语言。我也没有发现任何预训练的Konkani嵌入。💡💭</p></blockquote><blockquote class="lx"><p id="2381" class="ly lz hh bd ma mb mc md me mf mg jq dx translated">我已经为Konkani语言从头开始训练了3个不同的单词嵌入模型，并提供了演示如何使用这些模型的代码片段。还开发了一个streamlit应用程序。</p></blockquote><h1 id="a644" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw bi translated">单词嵌入</h1><p id="40a1" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">单词嵌入是对文本的学习表示，其中具有相同含义的单词具有相似的表示。<strong class="iv hi"> <em class="jr">查看我的文章了解更多详情:</em> </strong> <a class="ae ll" href="https://saurabhk30.medium.com/word-embedding-new-age-text-vectorization-in-nlp-3a2db1db2f5b" rel="noopener">单词嵌入:NLP中的新时代文本矢量化</a>。对于任何像神经机器翻译这样的下游NLP任务，信息检索&amp; others单词嵌入是它的核心。</p><p id="51b9" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><em class="jr">对于小数据集，训练基于BERT的模型不是一个选项，因为这会导致稀疏向量表示。我训练的word2vec，Glove，FastText型号:</em></p><h1 id="358d" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">Word2Vec</h1><p id="935c" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">Word2Vec使用浅层神经网络生成嵌入，在低维特征空间中高效存储高质量嵌入(Mikolov，t .等人，2013)。它是由谷歌开发的，并被证明是最先进的任务，如单词类比和单词相似性。Word2Vec可以使用两种不同的方法进行训练，即<em class="jr">连续单词包(CBOW):和Skip-gram。</em>我用<strong class="iv hi"> CBOW </strong>来训练嵌入。<em class="jr">给定上下文单词，预测目标单词。</em></p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><h1 id="7109" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">GloVe(单词表示的全局向量)</h1><p id="fa4e" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">GloVe (Pennington，J. et al.2014)结合了局部上下文窗口和全局矩阵分解两种方法，以无监督的方式为给定的词汇训练单词嵌入。手套捕获全局计数统计共现矩阵。GloVe无法为字典/词汇表中没有的单词提供任何矢量表示。我训练手套模特的时间比其他两个模特都长。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><h1 id="7fde" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">快速文本</h1><p id="a1b7" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">FastText (Bojanowski，P. et al. 2017)通过额外研究n-gram字符嵌入，用子词知识丰富了词嵌入。FastText是字符级嵌入。然后，一个单词被表示为其相关联的n元字符嵌入的总和。在实践中，未知单词的表示是通过添加其组成字符的3-6个字母的嵌入来获得的。与word2vec和GloVe不同，它可以为词汇表之外的单词生成嵌入。</p><figure class="ln lo lp lq fd ii"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="d6a2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><em class="jr">可以以此为参考开发自己的酷炫NLP项目</em> <strong class="iv hi"> <em class="jr">。<br/> </em> </strong> <em class="jr">训练好的模型文件都可以在这里下载</em><a class="ae ll" href="https://drive.google.com/drive/folders/1saxGg1BtuAWmXN1y8KX7aGfOxhC9xlHE?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="iv hi"><em class="jr"/></strong></a><em class="jr">。</em></p><h1 id="7ec0" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">估价</h1><p id="57ad" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">为了检查训练的模型，从3个嵌入中的每一个提取单词的向量表示，并提取向量空间中最相似的表示。这里，考虑属于每个更广泛类别的随机单词集，如名称、地点、星期几、对象、数值。</p><blockquote class="lr ls lt"><p id="9faf" class="it iu jr iv b iw ix iy iz ja jb jc jd lu jf jg jh lv jj jk jl lw jn jo jp jq ha bi translated">所有这些模型都是上下文无关的，也就是说，它将不同的词义组合成每个单词的一个向量。经过训练的嵌入具有200个向量维数。</p></blockquote><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/8de56aa6f1e8d2296cc2889f6c3b0c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*-RW6koAy00J8b72F4DKUrg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Word Vector Extracted from Euclidian Space</figcaption></figure><h1 id="e1c6" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated">推理</h1><ul class=""><li id="40ea" class="ms mt hh iv b iw kx ja ky je mu ji mv jm mw jq mx my mz na bi translated">手套模型不一定更好地利用全局共现矩阵；GloVe似乎没有Word2Vec和FastText那么有代表性。</li><li id="1c8f" class="ms mt hh iv b iw nb ja nc je nd ji ne jm nf jq mx my mz na bi translated">此外，FastText是字符级的单词，嵌入了返回的结果，这是因为FastText将每个字符视为最小单位；不像Word2Vec和GloVe那样认为Word是最小单位。次要字符级别的替换确实改变了词根的意思，它是<em class="jr">梵文</em>脚本的干扰。</li><li id="6821" class="ms mt hh iv b iw nb ja nc je nd ji ne jm nf jq mx my mz na bi translated">这些单词嵌入将作为文本分类、文档摘要和其他下游NLP任务的基础资源。</li></ul><blockquote class="lx"><p id="8ca7" class="ly lz hh bd ma mb mc md me mf mg jq dx translated">尽管聚类本身不是评估/判断的唯一参数；然而，Word2Vec始终给出最接近和合理的聚类。</p></blockquote><h1 id="205c" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw bi translated">演示</h1><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/12d6f94e85ade1b63d428c0d95c0a623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdANgIT2a023nPbXaxoY5Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx"><strong class="bd kb"><em class="nh">Find link to the </em></strong><a class="ae ll" href="https://github.com/GladiatorX/Konkani-Embeddings" rel="noopener ugc nofollow" target="_blank"><strong class="bd kb"><em class="nh">code </em></strong></a><strong class="bd kb"><em class="nh">Streamlit </em></strong><a class="ae ll" href="https://www.youtube.com/watch?v=1maOt8B4H2c" rel="noopener ugc nofollow" target="_blank"><strong class="bd kb"><em class="nh">app</em></strong></a></figcaption></figure><h1 id="4aaf" class="jz ka hh bd kb kc mh ke kf kg mi ki kj kk mm km kn ko mn kq kr ks mo ku kv kw bi translated"><strong class="ak">未来工作</strong></h1><ul class=""><li id="f7e3" class="ms mt hh iv b iw kx ja ky je mu ji mv jm mw jq mx my mz na bi translated">Konkani社区应该更加专注于为机器翻译系统、情感分析、文档分类、语音转文本等创建和开发有管理的标注数据集。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="309f" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考</h1><p id="cc3f" class="pw-post-body-paragraph it iu hh iv b iw kx iy iz ja ky jc jd je kz jg jh ji la jk jl jm lb jo jp jq ha bi translated">Mikolov，Chen，k .，Corrado，g .，Dean，J. (2013年)。向量空间中单词表示的有效估计。学习表征研讨会(ICLR)国际会议记录，美国亚利桑那州，第1301–3781页..【https://arxiv.org/pdf/1301.3781.pdf T2】号</p><p id="8a89" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Pennington、r . Socher和c . Manning(2014年)。Glove:单词表示的全局向量。2014年自然语言处理经验方法会议录(EMNLP)。【https://doi.org/10.3115/v1/d14-1162 T4】</p><p id="c269" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">Bojanowski，p .，Grave，e .，Joulin，a .，&amp; Mikolov，T. (2017年)。用子词信息丰富词向量。计算语言学协会汇刊，5，135–146。<a class="ae ll" href="https://doi.org/10.1162/tacl_a_00051" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1162/tacl_a_00051</a></p></div></div>    
</body>
</html>