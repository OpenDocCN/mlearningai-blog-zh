<html>
<head>
<title>Recurrent Neural Networks — Learn Without Forgetting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络——学习而不遗忘</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/recurrent-neural-networks-learn-without-forgetting-598ba9daafe3?source=collection_archive---------6-----------------------#2022-11-01">https://medium.com/mlearning-ai/recurrent-neural-networks-learn-without-forgetting-598ba9daafe3?source=collection_archive---------6-----------------------#2022-11-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="73fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章是对什么是<strong class="ig hi">递归神经网络</strong>以及它们如何工作的一般介绍。这是神经网络最重要的架构之一，因为它们背后的直觉。</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="ab fe cl jh"><img src="../Images/6390fdf88b002292324701d2d4c47765.png" data-original-src="https://miro.medium.com/v2/0*Z69lnB_e6nBxj6p3"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Photo by Xingye Jiang on Unsplash</figcaption></figure><p id="3ea9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">自然语言处理</strong>任务和<strong class="ig hi">时间序列</strong>预测由序列数据组成。这意味着将数据集作为一个整体并不能清楚地表示数据中的信息。这是因为数据点可能依赖于其他数据点。</p><p id="2e11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就拿两句话来说:<strong class="ig hi"> <em class="jo">它不好</em> </strong>和<strong class="ig hi"> <em class="jo">它真的很棒</em> </strong>。一台未经训练的机器不会区分<strong class="ig hi">不是</strong>和<strong class="ig hi">好</strong>，并且会因为两个句子之间的相似性而认为两者都是肯定或否定的。为了避免这一点，通过句子中的连续步骤进行学习将允许模型识别与积极和消极意义相关联的内容。例如，两个单词的步骤将使模型基于包含这些对的其他句子理解<strong class="ig hi">不好</strong>是否定的而<strong class="ig hi">非常好</strong>是肯定的。</p><p id="e226" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就解释了RNNs对于<strong class="ig hi">自动翻译</strong>、<strong class="ig hi">自动</strong> <strong class="ig hi">语音</strong> <strong class="ig hi">识别</strong> <strong class="ig hi">系统</strong>、<strong class="ig hi">时间</strong> <strong class="ig hi">系列</strong>等等的使用。这并不意味着它们是用于这些任务的唯一架构，传统的密集网络和卷积神经网络也可以很好地执行。</p><h1 id="7bc8" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是递归神经网络</h1><p id="c00c" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">一个<strong class="ig hi"> RNN </strong>可以被视为一个非循环神经网络(输入层→输出层)，但是在网络中有一个或多个反向连接。这是一个单神经元RNN的例子，它在每个时间步接受输入:<em class="jo"> x </em>和它自己的输出<em class="jo"> y </em>:</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="ab fe cl jh"><img src="../Images/0d160c437ae8eabc99d925126e1d8309.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Ai18Eag7Tyy6wRTJdSY7gg.jpeg"/></div></figure><p id="0d20" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一层递归神经元使用与每个神经元相同的原理，经典输入<em class="jo"> x </em>和前一时间的输出<em class="jo"> y_{t-1} </em>(作为向量)。权重与输入<em class="jo"> x </em> <em class="jo"> (w_x) </em>和之前的输出<em class="jo"> w_y </em>相关联。</p><p id="80a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是RNN方程的样子:</p><figure class="jc jd je jf fd jg er es paragraph-image"><div class="er es ks"><img src="../Images/468dd52c3741aa5955a3a19ae2e57d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*8NGvU0KWmJueIvN43CNQwA.png"/></div></figure><p id="cd63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jo"> ϕ </em>是激活函数<em class="jo"> ReLu </em>和<em class="jo"> b </em>，常数项。</p><p id="e5b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是记忆细胞？</strong></p><p id="b4dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">存储单元</strong>与RNN中保存一个或多个过去时间步的部分相关联。就简单的RNN来说，记忆是相当短暂的。因此，我们可以说<strong class="ig hi">隐藏状态</strong>是先前时间状态和输入的函数。在处理更复杂的RNN体系结构时，这种隐藏状态可能不同于单元本身的输出。</p><p id="0c39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">RNN是怎样训练出来的？</strong></p><p id="f315" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像经典的神经网络一样，训练方法包括优化网络中包含的权重，使得预测误差最小化。这可以通过<em class="jo">反向传播</em>实现，但在RNNs的情况下，这种反向传播略有不同，因为它是通过时间实现的:<strong class="ig hi">通过时间的反向传播</strong>。</p><p id="de9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种反向传播的修改版本对应于从网络的若干次获得的梯度来优化模型的参数(权重)。<em class="jo">例如，如果成本函数使用模型的最后3个输出(最后3次)，该方法将考虑最后3次的梯度以改变模型参数。</em></p><p id="cad3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是根据Keras/Tensorflow库创建的RNN的外观:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="e101" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个深邃的RNN看起来是这样的建筑:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="917b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当想法是获得一系列的几个预测值时，例如根据以前的销售额(时间序列)预测某个产品几天的销售额，RNN就改变了它的逻辑。<br/>事实上，我们可以用不同的方式来回答这个问题:</p><ul class=""><li id="0cf7" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated">使用训练好的模型来预测单个值，然后循环将预测值添加到输入值中，以便预测下一次。</li><li id="32a9" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">训练一个新模型，它不会预测单个值，而是在输出层预测一系列的几个值。</li><li id="6b1a" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">训练一个新模型，该模型将预测每个递归层的一系列值，从而优化每个步骤的参数，因为预测将在每个时间步骤进行。<br/> <em class="jo">例如，如果我们想要预测10个值，则只需获取20个已知输入值，并要求模型每次预测10个输入值，输入值为1至10，而输出值预测值为2至11，依此类推。知道输入2至11的实际值是已知的，就可以进行优化。以这种方式，在每个时间步长改进权重，以最终允许预测最初未知的值21至30。</em></li></ul><h1 id="4bb0" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">更复杂的RNN建筑</h1><p id="6be6" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">rnn变得更加复杂有两个原因:</p><ul class=""><li id="e118" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">梯度消失</strong>:训练中重量不再变化的点，导致训练停止。</li><li id="692d" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">RNN长期记忆的丧失</strong>:在一定时间后，在参数的优化中，关于RNN的第一个条目的信息几乎不再存在。</li></ul><p id="3178" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种复杂化可以用超参数(辍学、学习率、激活函数……)来处理，但也可以通过添加长记忆细胞来处理:<strong class="ig hi"> LSTM </strong>等<strong class="ig hi"> GRU </strong>。</p><h2 id="a53a" class="lj jq hh bd jr lk ll lm jv ln lo lp jz ip lq lr kd it ls lt kh ix lu lv kl lw bi translated">长短期记忆(LSTM)</h2><p id="9936" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">LSTM像元是为替换简单像元而创建的像元，以保持输入数据的长期相关性。</p><p id="fc14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些细胞以不同的方式分解:我们区分两种状态，短期和长期，其中有不同的层，各有用处。这个想法是，对于每个时间步:</p><ul class=""><li id="3271" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">长期状态</strong>可用于添加先前列出的信息，或者相反，用于删除它。</li><li id="96fb" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">短状态</strong>执行与经典RNN相同的工作，但是根据不同层的结果具有或多或少的信息。</li></ul><p id="4b86" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这些层在网络中处于不同的级别，但是它们都具有隐藏状态和作为输入的<em class="jo"> x </em>输入:</p><ul class=""><li id="8bef" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">主层</strong>对应于具有正常存储单元的单个RNN层。后者的输出将被与其相邻的不同层增强。</li><li id="51e9" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">遗忘层</strong>考虑输入，以决定(根据逻辑成本函数)在时间<em class="jo"> t </em>时输出不需要什么(通过删除长期状态中的信息)。</li><li id="fe3e" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">输入层</strong>定义了在长期状态下主层应保留的内容(根据逻辑函数)。</li><li id="b395" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi"> couche de sortie </strong>从输出的长期状态和新的短期状态中选择应该使用的内容(根据逻辑函数)。除了主层之外，所有层都是完全连接的层，并且与门相关联，以将它们的结果与通过的信息相乘，从而应用它们的选择。</li></ul><h2 id="b96e" class="lj jq hh bd jr lk ll lm jv ln lo lp jz ip lq lr kd it ls lt kh ix lu lv kl lw bi translated">门控循环单元(GRU)</h2><p id="eec7" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">GRU细胞是LSTM细胞的简化细胞，因为它们没有一个长期状态，并在一个层/门里聚集了遗忘和输入的想法，包括在LSTM一边。</p><p id="577c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下是从Keras/Tensorflow库中创建的具有LSTM或GRU像元的RNN的外观:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h1 id="c066" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h1><p id="28f4" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">rnn是具有记忆的神经网络，与完全连接的网络相比，它允许权重优化对数据有更好的理解。<br/> LSTM和GRU是改进的rnn，说明了长期记忆，更能抵抗梯度消失。</p><p id="0bc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇文章，我希望你喜欢它，现在能够实施RNN！如果你对数据科学和机器学习感兴趣，可以在这里查看我的其他文章<a class="ae lx" href="https://www.npogeant.com/#articles" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="be8b" class="lj jq hh bd jr lk ll lm jv ln lo lp jz ip lq lr kd it ls lt kh ix lu lv kl lw bi translated">资源</h2><div class="ly lz ez fb ma mb"><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hi fi z dy mg ea eb mh ed ef hg bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">通过最近的一系列突破，深度学习推动了整个机器学习领域。现在，甚至…</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">www.oreilly.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ji mb"/></div></div></a></div><div class="ly lz ez fb ma mb"><a href="https://www.tensorflow.org/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hi fi z dy mg ea eb mh ed ef hg bi translated">张量流</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">一个面向所有人的端到端开源机器学习平台。探索TensorFlow灵活的工具生态系统…</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">www.tensorflow.org</p></div></div><div class="mk l"><div class="mq l mm mn mo mk mp ji mb"/></div></div></a></div><div class="ly lz ez fb ma mb"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hi fi z dy mg ea eb mh ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">medium.com</p></div></div><div class="mk l"><div class="mr l mm mn mo mk mp ji mb"/></div></div></a></div></div></div>    
</body>
</html>