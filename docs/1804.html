<html>
<head>
<title>OMNIVORE: A Single Model for Many Visual Modalities |Paper Summary|</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">杂食动物:多种视觉形态的单一模型|论文摘要|</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/omnivore-a-single-model-for-many-visual-modalities-paper-summary-cf45a90bfc75?source=collection_archive---------4-----------------------#2022-01-31">https://medium.com/mlearning-ai/omnivore-a-single-model-for-many-visual-modalities-paper-summary-cf45a90bfc75?source=collection_archive---------4-----------------------#2022-01-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie"><p id="f355" class="if ig hh bd ih ii ij ik il im in io dx translated">这是一个伟大和令人钦佩的尝试，开发一个单一的模型，能够像人类视觉一样处理多项任务。很高兴看到计算机视觉的更多进步。</p></blockquote><figure class="iq ir is it iu iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es ip"><img src="../Images/07055df8882d77b8b34765c433722ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*evzHdgG7rpBJvhTqoqAHcA.png"/></div></div></figure><p id="6219" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">在这项研究中，研究人员提出了一个单一的模型来完成各种任务(对图像、视频和3D数据进行分类)，而不是为特定的任务(识别图像、视频和3D数据)单独开发模型架构。这个模型被命名为“<strong class="je hi"><em class="jz"/></strong>”它利用了基于<strong class="je hi"> <em class="jz">变压器</em> </strong>设计的灵活性。令人惊讶的是，该模型很容易使用<a class="ae ka" href="https://appen.com/off-the-shelf-datasets/" rel="noopener ugc nofollow" target="_blank"> <em class="jz">现成的标准数据集</em> </a> <em class="jz"> </em>进行训练，并且与同等规模的同行相比效果相当或更好。成绩尚可:在<a class="ae ka" href="https://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上86.0%，在<a class="ae ka" href="https://deepmind.com/research/open-source/kinetics" rel="noopener ugc nofollow" target="_blank"> Kinetics </a>上84.1%，在<a class="ae ka" href="https://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> SUN RGB-D </a>上67.1%。最后，该模型允许跨模态识别，而无需访问模态之间的对应关系。</p><p id="4e28" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">对人工智能有很多怀疑(消极的或积极的)，倡导者一直支持人工智能像人类一样存在、思考和行动(特别是在同时做几项任务时)。特定研究的特定模型确实表现出色；然而，这些模型架构并不具备比人类视觉更好甚至相似的能力。这个“<strong class="je hi"> <em class="jz">杂食者</em> </strong>”就是试图完成这样一个愿景。</p><p id="8b9f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">作者主张实现前面提到的目标；我们必须构建跨模态整体执行的模型，而不是针对每种方式进行过度优化。</p><p id="16f9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">除了灵活性之外，对于特定任务，这些传统模型还有一些优势:</p><ul class=""><li id="0bc0" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io kg kh ki kj bi translated">能够执行<em class="jz">跨模态概括</em></li><li id="3197" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated">能够节省专门为特定任务优化模型的研究和工程工作</li><li id="4503" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated">这个模型自然是多模型，当新的视觉传感器可用时<em class="jz">可以利用</em> <em class="jz"/></li></ul><blockquote class="kp kq kr"><p id="4640" class="jc jd jz je b jf jg jh ji jj jk jl jm ks jo jp jq kt js jt ju ku jw jx jy io ha bi translated">跨通道概括可以定义为将一个习得的模型从一个通道转移到另一个通道，而无需在后一个“交叉”通道中进行直接指导</p></blockquote></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><p id="98cc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">在本文中，我将说明提出的“<strong class="je hi"><em class="jz"/></strong><em class="jz"/>杂食模式的好处:</p><p id="9eb7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">出乎意料的是，研究人员已经看到<strong class="je hi">杂食</strong>指示可以很好地跨视觉模态推广，尽管该模型没有被训练成跨模态模型对应。<strong class="je hi"> OMNIVORE </strong>是许多不同视觉形态的单一模型。让我们以一个南瓜的例子来看看这三种数据:</p><p id="07ba" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">原始图像可以在这里看到:</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es lc"><img src="../Images/234424f824f61905be11094aae063836.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*MWWsuyFoWL91gu90fqy8zA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 1. The original <strong class="bd ll">image </strong>(RGB) (ImageNet-1K validation dataset) (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><figure class="ld le lf lg fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es lm"><img src="../Images/360de3cff42f843bfc6eab1947e98662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YC_2a0m-Xqrce1oLHiJdg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 2. <strong class="bd ll">Depth map</strong> (D) (ImageNet-1K training set) (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><figure class="ld le lf lg fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es ln"><img src="../Images/a700211d0bf03c96d9ca8fd2a082def8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxtOwCUZusgcYQU7kWqenw.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 3. <strong class="bd ll">Single-view 3D</strong> (RGBD) (ImageNet-1K training set) (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><figure class="ld le lf lg fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es lo"><img src="../Images/80ec60af0d8b3abcdb0d8f3e663f5824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1l72T6uMnbjsFPq3eDZWQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 4. <strong class="bd ll">Video </strong>(RGBT) (Kinetics-400 validation set) (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="ed63" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">记住<strong class="je hi"> OMNIVORE </strong>不为特定的模态使用特定的架构是有好处的；事实上，所有三种不同的形态(图像、视频和单视图3D)都采用相同的架构。为了清楚起见，模型(<strong class="je hi"> OMNIVORE </strong>)将每个输入模态转换成时空补片的嵌入，这些补片由同一转换器处理以生成输入的指示。那么你觉得模特是怎么训练的呢？🤔</p><p id="34a7" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">回答🤞:该模型在一组标准的现成分类数据集上进行训练，包括各种类型的输入。(虽然答案最初是在文章的开头提到的😅)</p></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><h1 id="944b" class="lp lq hh bd ll lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">杂食动物模型</h1><p id="947c" class="pw-post-body-paragraph jc jd hh je b jf mm jh ji jj mn jl jm jn mo jp jq jr mp jt ju jv mq jx jy io ha bi translated">所有的数据类型都被转换成一种通常的格式，并用嵌入来表示。然后，模型(<strong class="je hi">杂食者</strong>)利用一系列<em class="jz">时空注意</em>操作来建立所有模态的综合指示。</p><h2 id="5c08" class="mr lq hh bd ll ms mt mu lu mv mw mx ly jn my mz mc jr na nb mg jv nc nd mk ne bi translated">输入补丁</h2><p id="4a0b" class="pw-post-body-paragraph jc jd hh je b jf mm jh ji jj mn jl jm jn mo jp jq jr mp jt ju jv mq jx jy io ha bi translated">这三种数据被转换成<strong class="je hi"> <em class="jz"> 4D张量</em> </strong> ( <em class="jz">时间维度，两个空间维度和，通道维度</em>)。然后，输入数据被分离成一个面片集合，最后被表示为嵌入。该过程如下图所示:</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es nf"><img src="../Images/e296c375be0b4ed3bc9c07d575954164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*6Qfr-KTFTSzFdRKwGGcJlQ.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 5. <strong class="bd ll">Multiple visual modalities in the OMNIVORE model </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h2 id="c55e" class="mr lq hh bd ll ms mt mu lu mv mw mx ly jn my mz mc jr na nb mg jv nc nd mk ne bi translated">模型架构</h2><p id="76d6" class="pw-post-body-paragraph jc jd hh je b jf mm jh ji jj mn jl jm jn mo jp jq jr mp jt ju jv mq jx jy io ha bi translated">该模型的设计方式是允许跨视觉模态的最大参数共享。输入层将所有面片x分开操作，然后将它们投影到嵌入e中(使用线性层(<a class="ae ka" href="https://www.cs.utoronto.ca/~hinton/absps/LayerNormalization.pdf" rel="noopener ugc nofollow" target="_blank"> LayerNorm </a>))</p><ul class=""><li id="856c" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io kg kh ki kj bi translated">LayerNorm是一种减少训练时间消耗的技术，可以使神经元的活动正常化。</li></ul><p id="ab30" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">在<strong class="je hi">杂食</strong>中，研究人员利用一个单独的线性LN层来嵌入深度通道补丁，并将其输出添加到相应的RGB补丁的嵌入中。</p><p id="245d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">在<strong class="je hi"> OMNIVORE </strong>上的基础模型是<a class="ae ka" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> <em class="jz"> Swin Transformer架构</em> </strong> </a> <strong class="je hi"> <em class="jz"> </em> </strong>，这赋予了<strong class="je hi"> OMNIVORE </strong>强大的处理图像和视频的能力。而且，对于跨面片嵌入的时空建模，<strong class="je hi">杂食</strong>依赖于<a class="ae ka" href="http://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> <em class="jz">自我关注</em> </strong> </a>。最后，两套<a class="ae ka" href="https://arxiv.org/pdf/1803.02155v2" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> <em class="jz">相对位置编码</em></strong></a><strong class="je hi"><em class="jz"/></strong>(对于空间维度和时间维度)用于<strong class="je hi">杂食</strong>的构造。</p><ul class=""><li id="68cf" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io kg kh ki kj bi translated"><strong class="je hi">Swin(<em class="jz">S</em></strong><em class="jz">hifted</em><strong class="je hi"><em class="jz">win</em></strong><em class="jz">dows</em><strong class="je hi">)Transformer</strong>是<em class="jz">微软</em>于2021年开发的一款<strong class="je hi"> </strong>层级Transformer，用于解决视觉与语言之间的<strong class="je hi">差异</strong>。swin Transformer<strong class="je hi">将</strong>自关注计算限制在非重叠的局部窗口，而允许跨窗口连接。</li><li id="e2ad" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated"><strong class="je hi">自关注</strong>是描述单个序列的不同位置的机制，以便计算相同序列的指示；换句话说，它是一个<strong class="je hi"> seq-2-seq操作</strong>。</li><li id="877d" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated"><strong class="je hi">相对位置编码</strong>是一种位置嵌入，用于变压器产生成对的相对位置信息。我们没有将语义嵌入和绝对位置嵌入结合起来，而是将相对位置信息添加到键和值中。</li></ul></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><h1 id="49be" class="lp lq hh bd ll lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">杂食评估</h1><p id="39f1" class="pw-post-body-paragraph jc jd hh je b jf mm jh ji jj mn jl jm jn mo jp jq jr mp jt ju jv mq jx jy io ha bi translated">数据集的各种分支，包括图像、视频和单视图3D，用于评估<strong class="je hi">杂食动物</strong>。数据的汇总可以在表1中看到，其中#cls是类的数量，#train和#test分别是训练和测试样本的数量。蓝色数据集与图像相关，紫色数据集与视频相关，绿色数据集代表单视图3D模式。</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es ng"><img src="../Images/2d1af905786dc176406b45dedd019eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*fWfUv4rkzO6sS-Q_Khk7kQ.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 1.<strong class="bd ll"> Transfer datasets </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="7b51" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">然后将<strong class="je hi"> OMNIVORE </strong>与其他特定模式车型进行比较:</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es nh"><img src="../Images/bd268b884dead03580eb9e1513f1ca5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ElOVdu7Dsgxm7xUzbJFx7Q.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 2. <strong class="bd ll">OMNIVORE </strong>vs.<strong class="bd ll"> modality-specific models </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="a13e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">表二。是对相关数据集上的<strong class="je hi">杂食</strong>和特定模态模型的比较。很明显<strong class="je hi">杂食</strong>呈现出与其他车型相同甚至更好的性能。</p><figure class="ld le lf lg fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es ni"><img src="../Images/d56a6c3fc0a8a42e35b2020ff89b7989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HV2qtKqibxUFwwmqiMOQuA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 3. <strong class="bd ll">OMNIVORE </strong>vs <strong class="bd ll">modality-specific models </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="c048" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">表3。通过对各种下游任务进行微调，提供了对<strong class="je hi">杂食</strong>和特定模态模型的比较结果。同样，<strong class="je hi"> OMNIVORE </strong>呈现出比其他车型更好的性能。</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es nj"><img src="../Images/1d66eba12c7d7ef3469441beb2e9e97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*sNtEmcRsNmM9mW6EaLIYwQ.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 4. <strong class="bd ll">Comparing OMNIVORE </strong>with <strong class="bd ll">state-of-the-art models </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="6351" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">表4。展示了<strong class="je hi"> OMNIVORE </strong>与高级模型在所有三种数据类型上的比较(<strong class="je hi">image</strong>(<em class="jz">ImageNet-1K</em>)、<strong class="je hi">video</strong>(<em class="jz">Kinetics-400</em>)、<strong class="je hi">单视角3D </strong> ( <em class="jz"> SUN </em>)。同样，<strong class="je hi"> OMNIVORE </strong>表现出与高级车型不相上下或更好的性能(这是尖端和特殊的模式🙂).</p><figure class="ld le lf lg fd iv er es paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="er es nk"><img src="../Images/f33292fb45f894a35510dfe87c55d1c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*FTsXBCjjGQ4a1cVzyixcmA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 5. <strong class="bd ll">OMNIVORE </strong>vs <strong class="bd ll">state-of-the-art models </strong>in <strong class="bd ll"><em class="nl">image </em></strong>classification (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="e555" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">表5。提供了在三个数据集上的<strong class="je hi">图像</strong>分类微调实验中<strong class="je hi">杂食者</strong>与高级模型的比较。此次比较的结果与之前的比较相同。同样，表6和表7分别提供了<strong class="je hi">杂食</strong>与高级车型在<strong class="je hi">视频</strong>分类和<strong class="je hi"> RGBD </strong>微调实验中的比较结果。</p><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es nm"><img src="../Images/af6c2f5b57fd55570acaf823c00a7c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*_ugGSsmyY55hgIlcDMfkpA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 6. <strong class="bd ll">OMNIVORE </strong>vs <strong class="bd ll">state-of-the-art models </strong>in <strong class="bd ll"><em class="nl">image </em></strong>classification (<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><figure class="ld le lf lg fd iv er es paragraph-image"><div class="er es nn"><img src="../Images/4a794f9a4b62bcfc55798c6d01082854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*MDxupXKTBM7zxUb8mHjwmg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx">Table 7. <strong class="bd ll">OMNIVORE </strong>vs <strong class="bd ll">state-of-the-art models </strong>in<strong class="bd ll"> RGBD finetuning </strong>(<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><ul class=""><li id="f5bf" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io kg kh ki kj bi translated">RGB是计算机图形的基本颜色模型，因为彩色显示器使用红色、绿色和蓝色来创建想要的颜色。因此，RGB颜色空间的选择简化了系统的架构和设计。(<a class="ae ka" href="http://sun.aei.polsl.pl/~mkawulok/stud/graph/instr.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</li></ul></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><p id="2646" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated">看到这样一个优于各种模式的模型被开发出来，真的非常令人着迷。对了，周围还有一些<strong class="je hi">限制</strong>:</p><ul class=""><li id="6e2f" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io kg kh ki kj bi translated"><strong class="je hi">杂食者只需</strong>对<strong class="je hi">单视角3D图像</strong>进行操作；因此，<strong class="je hi">不会</strong>推广到其他3D授权，如<em class="jz">体素、点云等</em>。</li><li id="8c5d" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated"><strong class="je hi"> <em class="jz">深度输入</em> </strong>是<strong class="je hi">而不是<em class="jz">固定比例</em> </strong>(为了减少这种约束，使用了<strong class="je hi"> <em class="jz">归一化</em> </strong>)</li><li id="b1c3" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated"><strong class="je hi">杂食者仅</strong>对<strong class="je hi">视觉数据</strong>执行(不像<strong class="je hi"><em class="jz"/></strong><strong class="je hi">同现形态</strong>)</li><li id="b2cc" class="kb kc hh je b jf kk jj kl jn km jr kn jv ko io kg kh ki kj bi translated"><strong class="je hi"> OMNIVORE </strong>仅使用<strong class="je hi"><em class="jz">分类</em> </strong>和<strong class="je hi"> <em class="jz">结构化预测任务</em> </strong>进行预处理。</li></ul><blockquote class="kp kq kr"><p id="3ab9" class="jc jd jz je b jf jg jh ji jj jk jl jm ks jo jp jq kt js jt ju ku jw jx jy io ha bi translated">请注意，本研究的<strong class="je hi">源代码</strong>可在<strong class="je hi"> </strong> <a class="ae ka" href="https://facebookresearch.github.io/omnivore" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">这里</strong> </a> <strong class="je hi">查阅。</strong></p></blockquote></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><p id="6cce" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy io ha bi translated"><em class="jz">参考:</em></p><ol class=""><li id="25d3" class="kb kc hh je b jf jg jj jk jn kd jr ke jv kf io no kh ki kj bi translated">Girdhar，r .，等，<em class="jz"> Omnivore:多视觉模态的单一模型。</em> arXiv预印本arXiv:2201.08377，2022。<a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank">源</a></li></ol></div><div class="ab cl kv kw go kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ha hb hc hd he"><blockquote class="kp kq kr"><p id="48c2" class="jc jd jz je b jf jg jh ji jj jk jl jm ks jo jp jq kt js jt ju ku jw jx jy io ha bi translated"><strong class="je hi">请注意，本帖是为我日后或然的研究作出的回顾和复习有关此专题的材料而不完全阅读</strong> <a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">论文</strong> </a> <strong class="je hi">。本文所用图片来源均为</strong> <a class="ae ka" href="https://arxiv.org/pdf/2201.08377" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi">原纸</strong> </a> <strong class="je hi">。</strong></p><p id="4ea0" class="jc jd jz je b jf jg jh ji jj jk jl jm ks jo jp jq kt js jt ju ku jw jx jy io ha bi translated">如发现<strong class="je hi">错误</strong>，请及时与我联系。同时，你可以在<strong class="je hi"> Twitter </strong>这里<a class="ae ka" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>或者<strong class="je hi"> LinkedIn </strong>这里<a class="ae ka" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>联系我。最后，如果你有什么想法，我在<strong class="je hi">开</strong>商量，你唯一需要的就是在<a class="ae ka" href="http://www.linkedin.com/in/rezayazdanfar" rel="noopener ugc nofollow" target="_blank"> <strong class="je hi"> LinkedIn </strong>或</a>或<a class="ae ka" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">T57Twitter</a>上给我留言。🙂</p></blockquote><div class="np nq ez fb nr ns"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hi fi z dy nx ea eb ny ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">如何成为Mlearning.ai的作者</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">medium.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ja ns"/></div></div></a></div></div></div>    
</body>
</html>