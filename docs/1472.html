<html>
<head>
<title>Ch 8. Adversarial Discriminative Domain Adaptation (ADDA): Quest for Semantic Alignment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第八章。对抗性区分域适应(ADDA):语义对齐的探索</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/ch-8-adversarial-discriminative-domain-adaptation-adda-quest-for-semantic-alignment-1fc7e4ba6a86?source=collection_archive---------0-----------------------#2021-12-23">https://medium.com/mlearning-ai/ch-8-adversarial-discriminative-domain-adaptation-adda-quest-for-semantic-alignment-1fc7e4ba6a86?source=collection_archive---------0-----------------------#2021-12-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="7d5a" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">通过切换数据注释、训练框架和预训练数据集来优化领域适应性</h2></div><p id="1212" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在这篇文章中，我将介绍机器学习中的领域适应的概念，并讨论优化敌对歧视领域适应(ADDA)框架的过程。以下是目录:</p><ol class=""><li id="d877" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated">领域适应的动机——领域转换</li><li id="9f67" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">领域适应的目标——语义对齐</li><li id="7b9b" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">web→x射线域自适应</li><li id="fe2b" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">ADDA —算法</li><li id="9f80" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">快速查看多标签</li><li id="3fb2" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">实验#1:仅用web(源)域微调预先在ImageNet上训练的ResNet50</li><li id="e394" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">实验#2:在ImageNet上预训练编码器的ADDA</li><li id="abd5" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">break time:Web→Xray域转移的根源是什么？</li><li id="033f" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">实验#3:在风格化+原始ImageNet上预训练编码器的ADDA</li><li id="7d3a" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated">领域适应:前景</li></ol><h1 id="b418" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">1.领域适应的动机——领域转换</h1><p id="a7fb" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">看看这个有趣的观察:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/ddad91414f099cdf06f5ff9ffd85c896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVjUtw-Sx88cMPEJkovCjQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Classification accuracies of 4 CNN architectures and humans for classifying the images as “cat” 🐱 (Source: <a class="ae lt" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.12231</a>)</figcaption></figure><p id="6622" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个结果呈现在<a class="ae lt" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> ImageNet训练的CNN偏向于纹理；增加形状偏差提高了准确性和鲁棒性</em> </a> <em class="lu"> </em> (2019)显示，虽然大多数人类可以很容易地将所有四个图像识别为一只猫🐱尽管风格发生了转变，但所有四个基于CNN的图像分类模型AlexNet、LeNet、VGG16和ResNet50的性能对于《剪影和边缘》风格中描绘的猫来说都急剧下降。</p><p id="0bc8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在研究中，不同的风格/纹理被称为图像<em class="lu">的不同<strong class="iy hi"> <em class="lu">【域】</em> </strong>。</em>由于神经网络对输入数据的分布高度敏感，仅在图像的单个域(称为<strong class="iy hi"> <em class="lu">【源域】</em> </strong>)上训练的图像分类模型将<strong class="iy hi">学习编码图像，仅考虑该特定域的</strong> <strong class="iy hi">区别属性</strong>。因此，如上图所示，当在另一个图像域(称为<strong class="iy hi"> <em class="lu">【目标域】</em> </strong>)上测试时，同一模型可能表现不佳。这个问题被称为<strong class="iy hi"> <em class="lu">域转移</em> </strong>。</p><h1 id="86ab" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">2.领域适应的目标——语义对齐</h1><h2 id="9525" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">2.1输入空间和特征空间</h2><p id="367a" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">我们如何最小化域转移并使模型适应新目标域上的良好概括？这个任务，非常直观地称为<strong class="iy hi"> <em class="lu">域适应</em> </strong>，可以通过查看深度学习模型内的两个地方来实现:</p><ol class=""><li id="422e" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iy hi">输入空间</strong> —我们可以在目标领域收集或合成足够数量的输入数据，用于训练/微调模型。</li><li id="d833" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iy hi">特征空间</strong> —我们可以鼓励模型将来自不同领域但同一类别的输入数据在特征空间中紧密地映射在一起(任务称为<strong class="iy hi"> <em class="lu">语义对齐</em> </strong>)。</li></ol><h2 id="70c1" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">2.2语义(特征)对齐</h2><p id="9d5f" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">如果目标领域是小众的或在研究中未被探索，由于数据的缺乏，收集足够的数据可能是不可能的或昂贵的。出于这个原因，在特征空间中进行了许多领域适应的研究，旨在实现<strong class="iy hi"><em class="lu"/></strong>语义对齐，如下所示:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mj"><img src="../Images/2b96da131d3bc3c5e758f0a2be65d8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6z1i5beT5FFmS9Jtt0unAA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">t-SNE visualization of <a class="ae lt" href="http://ai.bu.edu/visda-2017/" rel="noopener ugc nofollow" target="_blank">VisDA-2017 dataset</a> using ResNet101 before and after domain adaptation with <a class="ae lt" href="https://arxiv.org/abs/1910.05562" rel="noopener ugc nofollow" target="_blank">Drop to Adapt</a> framework; t-SNE hyperparameters are consistent in both visualizations. (Source: Drop to Adapt <a class="ae lt" href="https://arxiv.org/abs/1910.05562" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="3b6e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在<a class="ae lt" href="https://arxiv.org/abs/1910.05562" rel="noopener ugc nofollow" target="_blank"><em class="lu">Drop to Adapt:Learning discriminant Features for Unsupervised Domain Adaptation</em></a>(2019)中呈现，这两个图显示了<a class="ae lt" href="http://ai.bu.edu/visda-2017/" rel="noopener ugc nofollow" target="_blank"> VisDA-2017数据集</a>(以合成3D模型图像作为源域，以真实摄影图像作为目标域的12类图像分类)在域适应之前和之后的特征表示。在域适应之前(左)，源域特征(红色)显示为12个独立的聚类，而目标域特征(蓝色)显示为一个大斑点。域适应后(右)，目标域特征显示出更好的分离。此外，尽管在图中没有标记不同的类，但是一些红色和蓝色聚类对出现在彼此附近(由青色矩形指示)，这可能代表<strong class="iy hi">相同类</strong>的聚类，说明语义对齐。</p><p id="1f0f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，我将给出一个在普通相机域和x射线相机域之间的真实世界域适应的例子。让我简单介绍一下这个项目以及为什么我决定使用领域适应方法。</p><h1 id="2598" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">3.web→x射线域自适应</h1><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mk"><img src="../Images/08b6e535d1638e7e040fa350fa35b0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTnoexNMAM8u0DcO-L8Omw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Samples of the three classes from web (source) and Xray (target) domains</figcaption></figure><h2 id="81b7" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">3.1给定任务</h2><p id="a447" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">对于我在多伦多大学的硕士研究项目，我被要求为机场x光行李扫描仪执行<strong class="iy hi">自动威胁检测</strong>，即给定一个看起来像上面的x光扫描图像，检测任何枪或刀(如果存在)。</p><h2 id="c175" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">3.2给定数据集</h2><p id="75a9" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">一个国际机场向我提供了450个x光行李扫描图像，分为3类:枪(117个图像)、刀(33个图像)和良性/无害(300个图像)。但问题是给定图像的数量<strong class="iy hi">不足以在不过度拟合</strong>的情况下训练一个神经网络。</p><h2 id="9069" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">3.3建议的研究路径</h2><p id="1ae1" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">考虑到卷积神经网络在计算机视觉方面的突破性表现，我的研究主管建议我开发一个<strong class="iy hi">深度学习</strong>模型。特别是，他建议我使用<strong class="iy hi">图像分类</strong>目标(将整个图像分类为一类)，而不是对象检测(预测对象周围的边界框)或对象分割(将每个像素分类为属于或不属于一类)，以保持模型复杂性适中。他还希望我采取<strong class="iy hi">域适应</strong>的方法，考虑到没有足够的x光图像来训练一个神经网络而不过度拟合。这种方法首先从网络上收集大量非x射线的、类似股票照片的相同对象类的图像，使用它们来训练模型，并使模型适应x射线图像。最后，我的研究小组的博士生建议从<strong class="iy hi">对抗性区分域适应(ADDA) </strong>框架开始，因为它的算法相对简单，但功能强大。</p><p id="4f2b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">更详细的项目背景，请参考本<a class="ae lt" rel="noopener" href="/@lucrece.shin/ml-masters-research-project-beginnings-43894d13b3cb">项目介绍帖</a>和我的项目帖列表<a class="ae lt" rel="noopener" href="/@lucrece.shin/list/machine-learning-research-portfolio-0437a30c89fa">。</a></p><h1 id="616e" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">4.ADDA —算法</h1><p id="2cde" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated"><a class="ae lt" href="https://arxiv.org/abs/1702.05464" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi">对抗性判别域自适应(ADDA) </strong> </a>框架(2017)引入了一个有效的无监督(意味着目标域数据是无标签的)域自适应框架<strong class="iy hi"><em class="lu">减少源和目标域分布之间的差异，从而提高泛化性能</em> </strong>。</p><h2 id="b34a" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">4.1 GAN与ADDA的关系</h2><p id="c46c" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">ADDA的名字中带有“对抗性”和“歧视性”的术语，你可能会想起<a class="ae lt" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成性对抗网络(GAN) </a> (2014)。让我们比较一下这两者:</p><h2 id="2a91" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">GAN —图像生成</h2><ul class=""><li id="5904" class="js jt hh iy b iz ky jc kz jf ml jj mm jn mn jr mo jy jz ka bi translated"><strong class="iy hi">输入，输出— </strong>潜在向量z，由G生成的伪图像</li><li id="22b6" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi">生成器G </strong>(反卷积层)——将1D潜向量z映射成3D假图像</li><li id="8253" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi">鉴别器D </strong>(卷积层+全连接层)——将3D图像映射为真实或虚假的二进制标签(即0代表虚假，1代表真实)</li><li id="8382" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi"> D的目标</strong> — <strong class="iy hi"> </strong>将输入x分类为真，将G的输出分类为假</li><li id="7f06" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi"> G的目标</strong> — <strong class="iy hi">迷惑D </strong>将G的输出归为真实</li></ul><h2 id="9fa0" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">ADDA —图像分类</h2><ul class=""><li id="72ae" class="js jt hh iy b iz ky jc kz jf ml jj mm jn mn jr mo jy jz ka bi translated"><strong class="iy hi">输入，输出</strong> —图像x，类别标签c</li><li id="417e" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi">编码器</strong> <strong class="iy hi"> E </strong>(卷积层数；通常是强骨干，例如ResNet50)，将3D图像映射到1D特征向量</li><li id="be09" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi">分类器C </strong>(全连接层)——将E的1D特征向量输出映射到<strong class="iy hi">类标签</strong> c中</li><li id="e38b" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi">鉴别器D </strong>(全连接层)—将E的1D特征向量输出映射到<strong class="iy hi">二进制域标签</strong> d (1表示源域，0表示目标域)</li><li id="f36d" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi"> D的目标</strong> —将源域和目标域特征分类为它们的<strong class="iy hi">真实域标签</strong> (1为源域，0为目标域)</li><li id="0853" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi"> E的目的</strong>——(1)以类别区分的方式对输入图像进行编码以及(2) <strong class="iy hi">混淆D </strong>以将源和目标域特征分类为它们的<strong class="iy hi">伪域标签</strong> (0表示源域，1表示目标域)，以便产生在特征空间中不可区分的源和目标域特征</li><li id="e847" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated"><strong class="iy hi"> C的目标</strong> —区分不同类别(例如交叉熵损失)</li></ul><h2 id="5cb2" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">4.2 ADDA —不对称映射</h2><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mp"><img src="../Images/5509c30a0e174eff249a837aec3a8056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldKEXlqL0PyA-FpKmLFUGQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Sequential training with asymmetric mapping (original ADDA)</figcaption></figure><p id="7c0f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">ADDA论文提出使用两个独立的编码器来映射源和目标域图像。如上图所示，<strong class="iy hi">分类和域适配任务依次执行，一个接一个</strong>。首先，在标记的源图像的<em class="lu">类标签</em>上预先训练源编码器。接下来，用源编码器的预训练权重初始化与源编码器具有相同架构的目标编码器，然后在源编码器权重被冻结时，使用二进制(源对目标)<em class="lu">域标签</em>来训练目标编码器。由于ADDA执行<strong class="iy hi"> <em class="lu">无监督</em>域自适应</strong>，它假设目标域数据是未标记的，并且不针对目标域上的类别分类进行优化。</p><p id="0225" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该论文提出，源编码器和目标编码器之间的这种不对称映射更加灵活，因为它允许学习更多特定于领域的特征提取。</p><h2 id="dbe7" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">4.3修改的ADDA对称映射</h2><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mq"><img src="../Images/4d754b208797c0a6919e2fc64fd6b77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0ndmgR3J_gMC1VEU5TEow.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Parallel training with symmetric mapping (modified ADDA)</figcaption></figure><p id="35fc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我稍微修改了ADDA，增加了一个<em class="lu">编码器来映射来自源域和目标域的图像。这消除了源编码器的预训练阶段。如上图所示，<strong class="iy hi">单个编码器在单个时期内同时<em class="lu">被训练用于分类</em></strong>(仅使用源域图像和类标签)、<strong class="iy hi">和域适应</strong>(使用源和目标域图像+二进制域标签)。</em></p><p id="49f2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我如何想到这种修改的是，一篇关于深度学习在x射线安全成像中的应用的<a class="ae lt" href="https://arxiv.org/abs/2001.01293" rel="noopener ugc nofollow" target="_blank">调查论文</a> (2021)报告说，大多数公认的用于域适应的对抗鉴别模型都使用对称映射。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mr"><img src="../Images/4c83497a3ef435a0a0deba9f66c72995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7Bs1FY5dM5xfP3RsoprRA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Comparison of different adversarial discriminative models, where ‘En’ is short for Encoder. ‘shared’ means symmetric mapping with a single encoder sharing weights for both source and target domain, while ‘unshared’ means asymmetric mapping with two separate encoders. Highlighted in yellow is the ADDA paper. (Source: <a class="ae lt" href="https://arxiv.org/abs/2009.00155" rel="noopener ugc nofollow" target="_blank">Survey paper</a> on unsupervised domain adaptation, 2021)</figcaption></figure><p id="ba94" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当使用对称映射时，我还观察到了更好的性能。这可能是因为对于非对称映射，源编码器的预训练权重可能已经<strong class="iy hi">太偏向于根据它们的类别标签映射源域图像</strong>，作为目标编码器的次优起点。因为对称映射允许在一个训练时期中同时优化分类和域适应，所以编码器可以考虑这两个任务来调整权重。</p><h2 id="b1af" class="lv kh hh bd ki lw lx ly km lz ma mb kq jf mc md ks jj me mf ku jn mg mh kw mi bi translated">4.4 ADDA —报告的性能</h2><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ms"><img src="../Images/96c3eb73502f17707b73f41fd330f56b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZoqYfkH9CxmX9RRB3FLYQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">(Left) MNIST, USPS, and SVHN samples and ADDA experimental results (Right) Office dataset samples and ADDA experimental results (Source: ADDA <a class="ae lt" href="https://arxiv.org/abs/1702.05464" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><p id="41ab" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上表显示了ADDA对于数字识别和办公对象识别任务的性能，其远优于仅使用源域数据训练的“仅源”模型，并且比之前的域适应框架相当好。这篇论文没有提到语义对齐；不过，我稍后会针对我自己的问题展示ADDA是如何实现的。</p><p id="862b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我的<a class="ae lt" href="https://github.com/lukysummer/Adversarial-Discriminative-Domain-Adaptation-with-multi-label-data/blob/main/ADDA_multi_label.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中包含了ADDA training的一步一步PyTorch实现(以及t-SNE绘图和定义多标签数据集的功能)。接下来，我将按时间顺序讨论我的领域适应实验。在每一步，我将呈现源和目标域特征的t-SNE图，以检查语义对齐。</p><h1 id="622b" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">5.快速查看多标签</h1><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mt"><img src="../Images/055431a60d0152a24289aad078aeffde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Yzos6hyrJ9jWR2dy.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Xray images and web images containing knife and gun</figcaption></figure><p id="28f1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如上所示，大多数x射线扫描图像包含<strong class="iy hi">其他良性(即无害)物体，其中混杂着枪支或刀具</strong>，而大多数网络图像显示的是孤立的物体。因此该模型可以很容易地注意到x射线图像中除枪或刀之外的其他物体。考虑到这一点，用标准的<em class="lu">单个</em>标签(类别0、1或2)注释每个图像不允许模型预测图像中存在一个以上的类别，例如枪<em class="lu">和</em>其他良性物体。为了解决这个问题，我为每个图像分配了一个<strong class="iy hi">多标签</strong>:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mu"><img src="../Images/30b4cc3fe38b8d44d10479ca5ef2c873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g_1i0vKhi90oP1ep.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Three different types of images and respective target labels</figcaption></figure><p id="aef9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">值得注意的是，我发现该模型倾向于将图像分类为良性类别，与枪和刀类别相比具有更高的可信度(非常直观地，因为良性类别代表除了枪和刀之外的宇宙🤨).因为检测良性物体没有检测枪或刀重要，所以我通过给良性类一个0.5的“软”标签来减弱良性信号，同时保持其他类为1。<em class="lu">关于多标签的更多细节在我的</em> <a class="ae lt" rel="noopener" href="/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7#ec5e"> <em class="lu">上一篇关于数据优化的</em> </a> <em class="lu">中讨论。</em></p><h1 id="c628" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">6.实验#1:仅用web(源)域微调预先在ImageNet上训练的ResNet50</h1><p id="0793" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">对于我的初始实验，我下载了ResNet50，其权重在<a class="ae lt" href="https://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集上进行了预训练，并针对单标签和多标签情况在web(源)域图像上对其进行了微调。<strong class="iy hi">该过程中未使用x射线(目标)域图像。</strong>对于这两种情况，源域图像的模型召回在不到10个训练时期内达到0.99+。相比之下，由于结构域转移，x射线(靶)结构域召回率很低:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mv"><img src="../Images/d6a3ae2401cd719fd46e6241a1e82da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBmzTRYYSr11LpspO6SDzw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Recall table for Xray images (v1)</figcaption></figure><p id="32ec" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">多标签案例的召回率虽然是单标签案例的两倍，但仍远低于预期的100%。下面是经过微调的ResNet50使用多标签数据编码的源和目标域特征的t-SNE图，带有按域(左)和域+类(右)的颜色标签。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mw"><img src="../Images/bcad971b68c5093acfeed626b9c16b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VW3bm9EkGInJbrdcq4-Frg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">t-SNE plot of source-only, multi-label model features, distinguished by domain (left) and by domain+class (right)</figcaption></figure><p id="5e3d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">左图显示了与第2.2节左图相似的模式，其中红色的web(源)域特征按类(枪和刀)紧密聚集，而蓝色的x射线(目标)域特征显示为一个斑点。右图显示，该模型远未实现语义对齐。我们确实看到x射线枪功能(黄色)倾向于web gun功能(红色)，但它们仍然更接近x射线刀和x射线良性功能。x射线刀特征(粉红色)随机分散，没有接近网状刀簇(钴蓝色)的迹象。这意味着<strong class="iy hi">模型不能完全通过x射线纹理</strong>来检测它被训练在x射线图像中检测的物体。</p><h1 id="5ad9" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">7.实验#2:在ImageNet上预训练编码器的ADDA</h1><p id="135a" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">接下来，我用ADDA框架进行了训练。我首先指定编码器具有与ResNet50相同的架构，并使用在<a class="ae lt" href="https://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集上预先训练的权重对其进行初始化。然后，我使用(1)用于分类的网页图像及其类别标签和(2)网页和x光图像以及用于域适应的0对1域标签来训练编码器。由于ADDA执行无监督域自适应，x射线图像的<strong class="iy hi">标签从未使用过</strong>。下面是在几个<strong class="iy hi"> </strong>中间训练时期:(时期1，5，10，15，21)编码器特性的t-SNE图</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mx"><img src="../Images/e7945a1ae4cfbfecb52a74438c47783e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9MtGDpuutFkEvXpHpgBj8Q.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">t-SNE plots of ADDA encoder (pre-trained on ImageNet) features, distinguished by domain (top) and by domain+class (bottom) at training epochs 1, 5, 10, 15, 21</figcaption></figure><p id="e44d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在时段1，我们看到一个蓝色的x射线特征的单个斑点，所有三个类别聚集在一起。随着ADDA培训的进展；然而，单个斑点开始分裂。x射线刀特征(粉色)开始向网状刀特征(钴蓝色)迁移，而x射线枪特征(黄色)开始向网状枪特征(钴蓝色)迁移。在epoch 21，x射线枪和x射线刀特征与x射线良性特征相距很小，这是实现语义对齐的巨大进步！这种质的提高也反映在枪支和刀具召回的增加上:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/50451f12572c98807637078f020fc610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLuC2AiPmqGVwFs6XhqPOw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Updated recall table for Xray images (v2)</figcaption></figure><h1 id="c87c" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">8.break time:Web→Xray域转移的根源是什么？</h1><p id="3cce" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">尽管有这样的改善，78%和70%的枪支和刀具召回仍然远远不能确保飞行安全。我们已经试过ADDA了，下一步我们能做什么？🤔我尝试用我的数据集训练最近引入的其他几个领域适应框架(如<a class="ae lt" href="https://arxiv.org/abs/1910.05562" rel="noopener ugc nofollow" target="_blank"> Drop to Adapt </a>、<a class="ae lt" href="https://arxiv.org/abs/1912.01805" rel="noopener ugc nofollow" target="_blank">Domain mix</a>、<a class="ae lt" href="https://arxiv.org/abs/1904.01886" rel="noopener ugc nofollow" target="_blank"> DADA </a>)，但无法获得任何更好的结果。所以我反而更多的想到了web的<strong class="iy hi"> <em class="lu">根</em></strong>→Xray域移位问题。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mz"><img src="../Images/5ef00238fc387079309ed76389f05f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-AV6k4hOgZAxMSLLTanCg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Samples of Web images and Xray images containing gun and knife</figcaption></figure><p id="3f98" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">查看web和x射线图像，我可以指出两个主要区别:</p><ol class=""><li id="3071" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iy hi">纹理偏移</strong>—与网络图像相比，x射线图像的颜色有限，透明度增加，且略微模糊</li><li id="a55d" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iy hi">对象混乱程度</strong> —与明显存在主要对象的网络图像相比，x射线图像包含杂乱的不同对象</li></ol><p id="cb65" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是，尽管如此困难，我们人类怎么能在x光图像中发现枪或刀呢？也许我们清楚地记得枪和刀的<strong class="iy hi">形状</strong>并试图在x光图像中定位它。想到这里，我想到了本文开头介绍的那篇论文:<a class="ae lt" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> ImageNet训练的CNN偏向于纹理；增加形状偏差提高精确度和鲁棒性</em> </a> <em class="lu"> </em> (2019)。</p><p id="eb47" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文提出了<strong class="iy hi"> <em class="lu">纹理假说</em> </strong>，该假说认为<em class="lu">“对于CNN目标识别，目标纹理比目标整体形状更重要。诸如纹理的局部信息实际上可能足以‘解决’ImageNet对象识别”。</em>这是CNN模型的一个致命缺陷，尤其与我的问题有关，从网络图像到x光图像发生了巨大的纹理变化。所以为了使模型对物体的<strong class="iy hi"> <em class="lu">形状</em> </strong>而不是纹理更加敏感，建议用<strong class="iy hi">风格化图像</strong>对模型进行预训练。“风格化”图像意味着保留图像中的内容/形状，而<strong class="iy hi">使用<a class="ae lt" href="https://arxiv.org/abs/1703.06868" rel="noopener ugc nofollow" target="_blank"> AdaIN风格转移</a>用数字数据集(包含79434幅画)从<a class="ae lt" href="https://www.kaggle.com/c/painter-by-numbers" rel="noopener ugc nofollow" target="_blank">画师的</a></strong>中随机选择一幅画来替换图像的风格/纹理。这是一个用十种不同绘画风格化的图像示例:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es na"><img src="../Images/0d226e38b63519231536d2318fd22465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cyTakbcblSzzuFZ_"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">10 stylized samples of an image of a ring-tailed lemur. The samples have content/shapes of the original image on the left and style/texture from 10 different paintings (Source: <a class="ae lt" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.12231</a>)</figcaption></figure><p id="670a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该论文报告称，模型<strong class="iy hi">在风格化的和原始的ImageNet数据集上进行预训练，然后在原始的</strong>上进行微调，表现最佳。论文作者的<a class="ae lt" href="https://github.com/rgeirhos/texture-vs-shape" rel="noopener ugc nofollow" target="_blank"> Github资源库</a>中提供了模型检查点和下载说明。</p><h1 id="7a0d" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">9.实验#3:在风格化+原始ImageNet上预训练编码器的ADDA</h1><p id="7a8a" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">因此，我再次使用ADDA进行训练，将编码器架构设置为与ResNet50相同，并使用上面提到的性能最好的模型的权重进行初始化。以下是最终得到的t-SNE图:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nb"><img src="../Images/ce2104beae1e3b234c046d0e771116c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-nC9jMrDRQf0H7M1w2UrA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">t-SNE plot of ADDA encoder (pre-trained on Stylized+Original ImageNet) features by domain (left) and by domain+class (right)</figcaption></figure><p id="8d10" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你发现以前实验的改进了吗？为了更好地理解，下面是三个实验的图表:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nc"><img src="../Images/3891ff5ecb478953cdf9c4037968adfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1blsCS5BgB3avZQ6hy6Krw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">t-SNE plots of encoded features by domain (top) / by domain+class (bottom) for three different experiments</figcaption></figure><p id="487d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从左到右看下面三个图，我们看到以下几个方面的进展:</p><ul class=""><li id="1539" class="js jt hh iy b iz ja jc jd jf ju jj jv jn jw jr mo jy jz ka bi translated">从x射线良性特征(青色)中分离出x射线枪和x射线刀特征(黄色和粉红色)</li><li id="c5cf" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated">x射线枪特征(黄色)向网状枪特征(红色)的迁移</li><li id="cb21" class="js jt hh iy b iz kb jc kc jf kd jj ke jn kf jr mo jy jz ka bi translated">x射线刀特征(粉色)向网状刀特征(蓝色)的迁移</li></ul><p id="64e5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这样的进展说明这个模型已经<strong class="iy hi">实现了语义对齐</strong>。该模型现在能够通过x射线纹理进行观察，并检测枪和刀，尽管纹理从普通相机转移到x射线。下表显示了最终型号增加的枪支和刀具召回:</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nd"><img src="../Images/0b119c7246fa78215e3b8e06739c2ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjrvtvlSle-e6ev4QVzQDw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx">Updated recall table for Xray images (v3)</figcaption></figure><h1 id="0736" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">10.领域适应:前景</h1><p id="1ed5" class="pw-post-body-paragraph iw ix hh iy b iz ky ii jb jc kz il je jf la jh ji jj lb jl jm jn lc jp jq jr ha bi translated">总之，我考虑了领域适应的各种观点，包括模型训练框架、预训练数据、定性度量(语义对齐)和定量度量(回忆)。在这篇文章中，我方便地将我的研究过程分为三个不同的实验，但事实上，在这之间进行了数百个实验(加上淋浴时的数千个思维实验)来测试，仅举几个例子，多标签的<a class="ae lt" rel="noopener" href="/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7#ec5e">有效性</a>，学习对象形状的<a class="ae lt" rel="noopener" href="/codex/ch-7-decoding-black-box-of-cnns-using-feature-map-visualizations-45d38d4db1b0">重要性</a>，以及t-SNE图在检查语义对齐中的<a class="ae lt" rel="noopener" href="/@lucrece.shin/chapter-4-using-t-sne-plots-as-human-ai-translator-c5ef9c2f2fa4#6d8a">有效性(每个都在我的其他链接文章中详细阐述)。还要注意ADDA论文从未提到语义对齐，但是在挖掘了许多其他关于领域适应的研究论文后，我认识到它是领域适应的主要目标。</a></p><p id="8be5" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作为机器学习实践者，我们经常旨在找到一篇解决我们自己的类似问题的研究论文，用我们的数据实现并运行算法，然后嘣！精确度达到很高，问题得到解决。但是如果结果不够好，我们必须努力寻找不同的视角来看待问题。我认为我的数据可能与ADDA论文中使用的数据集具有非常不同的特征，并寻找适应这种差异的方法。</p><p id="d439" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">同样，我的<a class="ae lt" href="https://github.com/lukysummer/Adversarial-Discriminative-Domain-Adaptation-with-multi-label-data/blob/main/ADDA_multi_label.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hi"> Colab笔记本</strong> </a>中包含了ADDA training的一步一步PyTorch实现(以及用于t-SNE绘图和定义多标签数据集的函数)。如有任何问题或反馈，您可以<a class="ae lt" href="mailto:lucrece.shin@mail.utoronto.ca" rel="noopener ugc nofollow" target="_blank">联系我</a>😊。感谢阅读，机器学习快乐！🦋🦋</p><div class="ne nf ez fb ng nh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hi fi z dy nm ea eb nn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ln nh"/></div></div></a></div></div></div>    
</body>
</html>