<html>
<head>
<title>Metric Learning for Landmark Image Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于地标图像识别的度量学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/metric-learning-for-landmark-image-recognition-6c1b8e0902bd?source=collection_archive---------10-----------------------#2022-10-31">https://medium.com/mlearning-ai/metric-learning-for-landmark-image-recognition-6c1b8e0902bd?source=collection_archive---------10-----------------------#2022-10-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a466" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">具有局部特征重排序的全局描述符相似性搜索的完整TensorFlow实现</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/2aed8a4f4419185b7591b48a27ea8bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HawZ2qXD8gs4pd0MjL9lug.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 1.</strong> Colosseum — Photo by <a class="ae jn" href="https://unsplash.com/@henrypaulphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Hank Paul</a> on <a class="ae jn" href="https://unsplash.com/@henrypaulphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText&quot;&gt;Hank Paul&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>.</figcaption></figure><p id="1ad0" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi kk translated">用于实例识别和信息检索的度量学习是一种已经在多个领域广泛实施的技术。这是一个与研究中的新应用高度相关的概念，如DeepMind的<a class="ae jn" href="https://www.nature.com/articles/s41586-021-03819-2" rel="noopener ugc nofollow" target="_blank">alpha fold</a>【11】在生物学中的最新人工智能突破[2]，也是一个成熟和成熟的概念，可以在行业中看到大量的实施，从谷歌搜索的上下文信息检索[12]，到人脸识别的图像相似性[7]，你可能每天都用它来解锁你的手机。在本文中，我将介绍一个完整的图像查询架构示例，它是当今计算机视觉挑战之一——<strong class="jq hi">地标识别</strong>的现代解决方案的基础。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="11fa" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">目录</h2><ul class=""><li id="899e" class="lu lv hh jq b jr lw ju lx jx ly kb lz kf ma kj mb mc md me bi translated"><a class="ae jn" href="#2846" rel="noopener ugc nofollow">本文的目标</a></li><li id="e1fa" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#a21d" rel="noopener ugc nofollow">谷歌地标数据集v2 </a></li><li id="5752" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#eedb" rel="noopener ugc nofollow">基线架构</a></li><li id="a889" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#b2c9" rel="noopener ugc nofollow">库和依赖关系</a></li><li id="3115" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#cb4b" rel="noopener ugc nofollow">入门</a></li><li id="9e52" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#1973" rel="noopener ugc nofollow">培训、测试和验证分割</a></li><li id="a1a2" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#7285" rel="noopener ugc nofollow">增强层</a></li><li id="09c0" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#0313" rel="noopener ugc nofollow">全球描述符</a></li><li id="46ef" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#9826" rel="noopener ugc nofollow">余弦相似度</a></li><li id="bec0" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#5b8e" rel="noopener ugc nofollow">根据局部特征重新分级</a></li><li id="3520" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#elf8" rel="noopener ugc nofollow">重新排序示例</a></li><li id="110f" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#de56" rel="noopener ugc nofollow">弧面损失</a></li><li id="46db" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="#9053" rel="noopener ugc nofollow">谷歌的统一DELG模式</a></li></ul></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="2846" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">本文的目标是</h2><p id="882c" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">在这篇文章的引言中，我列出了一些例子，这些例子应该可以让你了解相似性搜索的相关度量学习与现代机器学习的关系。在接下来的部分中，我将通过一个用于<strong class="jq hi">地标识别任务的基线解决方案来指导您完成该技术的一个示例实现。</strong>我们将使用Google Landmarks数据集v2 [8]的一个子集，从现在开始，我将其称为GLDv2。</p><p id="2088" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这个问题的解决可以分为两个任务:<em class="mn">图像检索</em>和<em class="mn">实例识别</em>。检索的任务是根据图像与查询图像的相关性对索引集中的图像进行排序。识别任务是识别在查询图像[8]中显示了对象类的哪个特定实例(例如<em class="mn">对象类“绘画”的实例“蒙娜丽莎”)。如<a class="ae jn" href="https://arxiv.org/abs/2004.01804" rel="noopener ugc nofollow" target="_blank"> GLDv2数据集论文</a>中的基准所示，最先进的方法在一定程度上使用了<strong class="jq hi">全局特征相似性搜索</strong>与<strong class="jq hi">局部特征匹配重新排序</strong>。在这里，我旨在解释、说明和实现这些概念，并希望让您更清楚地了解如何将它们扩展到自己的应用程序中。</em></p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="a21d" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">谷歌地标数据集v2</h2><p id="2d2b" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">该数据集是由谷歌在2020年推出的，其动机是深度学习方法在地标识别任务中的快速发展。以前的基准数据集，如<a class="ae jn" href="https://www.robots.ox.ac.uk/~vgg/publications/papers/philbin07.pdf" rel="noopener ugc nofollow" target="_blank"> Oxford5k </a>和<a class="ae jn" href="https://www.robots.ox.ac.uk/~vgg/publications/papers/philbin08.pdf" rel="noopener ugc nofollow" target="_blank"> Paris6k </a>，正在努力跟上新的解决方案[8]，并且不是确保可伸缩性和泛化的良好资源，因为它们只包含来自单个城市的少量实例的少量查询图像。</p><p id="f816" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">为了定义一个新的具有挑战性的基准，GLDv2被提议为迄今为止最大的数据集，具有超过5，000，000幅图像和200，000个不同的实例标签(类别或地标)。测试集包含大约118，000个带有基本事实注释的查询图像，最重要的是，只有1%的图像实际上在地标的目标域内。其他99%是域外的、不相关的图像[8]。除此之外，它还有两个旨在测试模型稳健性的核心特征:</p><ol class=""><li id="90ab" class="lu lv hh jq b jr js ju jv jx mo kb mp kf mq kj mr mc md me bi translated"><strong class="jq hi">极度偏斜的阶级分布</strong>。虽然著名的地标可能有成千上万的图像样本，但是57%的类最多有十个图像，38%的类最多有五个图像。</li><li id="7b3d" class="lu lv hh jq b jr mf ju mg jx mh kb mi kf mj kj mr mc md me bi translated"><strong class="jq hi">类内变异性</strong>。地标有来自不同有利位置和不同细节的视图，以及建筑物的室内和室外视图。</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ms"><img src="../Images/69d85fdd33c5ee861a6dddae8e6d3930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-ssWvilkpqK8SsLTcxh0Q.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 2.</strong> Google Landmarks Dataset v2 long-tailed class distribution [8].</figcaption></figure><p id="6d81" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">虽然本文出于说明的目的，将使用来自原始GLDv2训练集的具有75个类和11，438个地标图片的<strong class="jq hi">子集，但是我们仍然必须处理上面的一些挑战。</strong></p><p id="c3f7" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">随着GLDv2(以及之前的GLDv1)的发布，谷歌赞助了一系列Kaggle竞赛，包括<a class="ae jn" href="https://www.kaggle.com/c/landmark-recognition-2020" rel="noopener ugc nofollow" target="_blank">2020年版</a>【9】，其中排名靠前的解决方案激发了这里所示的架构。如果你想了解更多关于GLDv2的信息，我推荐你去浏览一下<a class="ae jn" href="https://github.com/cvdfoundation/google-landmark" rel="noopener ugc nofollow" target="_blank">数据集库</a>和<a class="ae jn" href="https://arxiv.org/abs/2004.01804" rel="noopener ugc nofollow" target="_blank">论文</a>。您还可以在这里直观地探索数据集<a class="ae jn" href="https://storage.googleapis.com/gld-v2/web/index.html" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="eedb" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">基线架构</h2><p id="ba35" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">我们的模型架构改编自<a class="ae jn" href="https://arxiv.org/abs/2010.01650" rel="noopener ugc nofollow" target="_blank"> 2020年识别挑战赛冠军</a>【10】和<a class="ae jn" href="https://arxiv.org/abs/1906.03990" rel="noopener ugc nofollow" target="_blank"> 2019年识别挑战赛第二名</a>【5】的论文，可以视为地标识别任务的基准解决方案。下图说明了使用全局特征搜索和利用局部特征杠杆进行重新排序的训练和检索例程，我们将在下面的部分中详细介绍。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mt"><img src="../Images/43ecead0976b6818a2803ab90baca645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fLxPS8MDuAw8SNj2BYkLg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 3.</strong> Landmark recognition baseline architecture. Diagram by the author.</figcaption></figure><p id="06d2" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">Google也优化了一个类似的架构到统一的模型中(DELG的深度局部和全局特性)[4]。我专门写了一小段，以后你可以在这里读到更多。</p><p id="8892" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我的完整Kaggle笔记本以及本文中的所有代码可以在这里找到<a class="ae jn" href="https://www.kaggle.com/code/erichhenrique/gldv2-2020-efficientnet-and-delf-reranking-tf/notebook" rel="noopener ugc nofollow" target="_blank"/>。如果你觉得有用的话，可以考虑留一张赞成票。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="b2c9" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">库和依赖项</h2><p id="c1b8" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">我们的实现将使用TensorFlow 2和OpenCV作为核心库。围绕这一点，我们将使用NumPy和Pandas进行数据辩论，使用SciPy进行距离度量，使用matplotlib和seaborn进行可视化。</p><p id="824f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">Google Collab和Kaggle笔记本上的托管笔记本实例预装了所有需要的库。然而，对于这个迷你项目，我建议在<a class="ae jn" href="https://www.kaggle.com/code" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>上工作，因为无需下载就可以轻松访问完整的GLDv2数据集(特别是如果您想要在完整的数据集上进行实验，该数据集的大小为105 GB)。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="cb4b" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">入门指南</h2><p id="6e25" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">为了开始，我们可以确认我们的工作环境设置为使用GPU加速运行。您可以使用bash命令<code class="du mu mv mw mx b">nvida-smi</code>验证它是否启用了GPU，并检查当前的CUDA版本。完成后，我们可以开始导入我们的库。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="7f83" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们现在可以定义数据集目录和。csv路径。如果您在Kaggle环境中为比赛工作，数据将分布在一个train和test文件夹中，其中每个图像根据图像<code class="du mu mv mw mx b">id</code>的前三个字符放置在三个子文件夹中(即图像<code class="du mu mv mw mx b">abcdef.jpg</code>放置在<code class="du mu mv mw mx b">a/b/c/abcdef.jpg</code>)。该目录还包含一个带有培训标签的train.csv文件。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es na"><img src="../Images/949730ea4c87015032edc66a886bcf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*KJi0iPPrRkUvoz-x8Z5hmA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 4.</strong> Kaggle GLDv2 folder structure.</figcaption></figure><p id="89ff" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们将继续读取train.csv文件，并使用从各自的<code class="du mu mv mw mx b">id</code>导出的训练图像路径定义一列。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nb"><img src="../Images/91ebbcc93765d5c92330119d5a061868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*_9px9GCTglu3cwF7H-py6w.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 5.</strong> Train DataFrame with image paths.</figcaption></figure><p id="41ed" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">然后，我们将定义要使用的子集。我们将研究数据的一个小子集，以保持实验在可行的时间内是可管理的，包括训练和余弦相似性搜索的检索。该子集由每类具有至少150个且不超过155个图像的界标来定义。我们还将为每个职业分配一个新的、更易理解的地标id。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nc"><img src="../Images/dcb622f96513e01ebdc1448a1bf33558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*LX9M_35Mn3XU6HQoXloTQQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 6.</strong> Subset with 11438 rows and 75 landmark classes.</figcaption></figure><p id="1bd0" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们从1，580，470张图片减少到11，438张，分布在75个不同的地标类别中。如果您想正面迎接挑战，并在完整的数据集上工作，我会推荐一些优化实现，特别是在余弦相似性搜索中，但我们将在后面的部分中讨论这一点。现在，让我们关注基线模型的理论和核心实现。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="1973" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">培训、测试和验证分离</h2><p id="6dcd" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">我们将进行分层划分来定义我们的训练、验证和测试集。为此，我们将使用sciki-learn <code class="du mu mv mw mx b">train_test_split</code>方法，同时将我们的标签传递给<code class="du mu mv mw mx b">stratify</code>参数。这将确保我们的子集中的75个类中的每一个都将在分割后出现在训练集、验证集和测试集中。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nd"><img src="../Images/20dc41b96fd1029e08fd895e8e42b2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*Xo8ZWj4oSuB_GY3Bf2YCjw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 7.</strong> Training, validation, and test shapes.</figcaption></figure><p id="073c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们现在可以确认，它是均匀分布的，每个子集上有一些直方图。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ne"><img src="../Images/0e743dc8018b22f8129d75b4b3421b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3SEVKs3tuhoYVKseJ806Q.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 8.</strong> Stratified training, validation, and test split distribution.</figcaption></figure><p id="a2f5" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">到目前为止，我们一直在处理从train.csv文件生成的数据帧。现在我们必须处理来自数据集的实际图像。我们将首先定义一个新的文件夹结构，将我们的每个子集图像放入一个以地标id命名的目录中。这是允许我们使用TensorFlow <code class="du mu mv mw mx b">image_dataset_from_directory</code>函数创建<code class="du mu mv mw mx b">tf.data.Dataset</code>对象的重要一步。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="3d4c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们现在可以检查我们的新文件夹结构是否就位。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nf"><img src="../Images/26dd35bc42df288abafd9137dd07afec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9P8YCQa4ZogvXNM13Nvh7w.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 9.</strong> New image directories.</figcaption></figure><p id="872f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">最后，从我们的训练、测试和验证集创建TensorFlow <code class="du mu mv mw mx b">tf.data.Dataset</code>。这些对象使我们整个管道中的数据流非常高效，并且肯定会有助于性能向前发展。</p><p id="e984" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">函数<code class="du mu mv mw mx b">image_dataset_from_directory</code>可以通过使用<code class="du mu mv mw mx b">image_size</code>参数调整图像大小来预处理我们的图像，并为训练和验证预设我们的批量大小。所以我们也要定义这些参数。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="062b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">现在我们的数据已经可以使用了。您可以通过<code class="du mu mv mw mx b">dataset</code>对象中的<code class="du mu mv mw mx b">.take()</code>方法查看其中一个训练批次。记住，我们的子集在最初的分割中被打乱了。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ng"><img src="../Images/1963a82c48e25f85c482704856e36249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*U3JccWV3B_coeP3C.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 10.</strong> Sample batch from the training dataset.</figcaption></figure><p id="af57" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">从这里，您可以看到GLDv2数据集的一些理想化挑战仍然存在于我们的子集中。由于类内可变性较高，我们可以看到从室内和室外视图拍摄的同一地标的图片。也有与其类别间接相关的照片，例如上图中来自<em class="mn">地标41 </em>的照片，代表一件可能位于数据集中所描绘的实际地标内部的博物馆藏品。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="7285" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">增强层</h2><p id="7175" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">我们将使用图像增强来帮助泛化，并防止模型过度拟合训练数据。有了问题本质的知识，我们就可以在这一步定义什么可行，什么不可行。理想的地标识别和检索模型应该能够从不同角度拍摄的非专业图片中识别实例级的地点。以下代码片段将定义一个基本的增强层，该层对训练图像应用随机平移、随机旋转和随机缩放。值得注意的是，在推断过程中会绕过增强层，这意味着它只会在训练步骤中对图像进行预处理。记住这一点，我们可以通过将<code class="du mu mv mw mx b">training = True</code>传递给我们的增强层来看一个增强示例。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nh"><img src="../Images/80d1866431b8c6433428ecbbf034724e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*kkQytL9E8erHIUaR.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 11.</strong> Sample augmentation step.</figcaption></figure><p id="078d" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们在这里避免使用的一种常见的增强方法是在垂直轴上随机镜像或翻转图像。虽然我们知道我们的模型应该是平移和视点不变的(它应该能够识别图片中不同位置和不同视点下的地标实例)，但是它不应该对垂直对称不变。虽然一些地标可能确实是关于它们的垂直轴对称的，但是大多数都不是。</p><p id="20a8" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">另一方面，一种被证明可以提高模型性能的增强技术[10]是随机剪切。它随机模糊原始图像的小区域，正如上面使用的例子一样，它也是一种有效的正则化方法。你可以在这里找到一篇关于它的简洁而信息丰富的媒体文章，作者是<a class="ni nj ge" href="https://medium.com/u/922346f406cd?source=post_page-----6c1b8e0902bd--------------------------------" rel="noopener" target="_blank"> Ombeline Lagé </a>。</p><p id="f901" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">有了我们的增强层，我们现在可以定义我们的分类模型。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="0313" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">全局描述符</h2><p id="3f21" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">如果你回头看看我们的模型架构，你会注意到一个关于度量学习的相似性搜索的工具性事实——我们并不是直接从我们的分类模型中推断出被查询图像的地标标签。在本节中，我们将建立一个预训练的EfficientNet，并使用它来训练我们的<strong class="jq hi">嵌入层</strong>，稍后我们将使用它来将我们的查询和关键图像编码到一个512维的特征向量中。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/31cb13c70bf52d48ca36ff9d47858fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uF3d9Uf66VbCYU9cN04FXg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 12.</strong> Global descriptor training architecture.</figcaption></figure><p id="8fd7" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">嵌入层将负责我们的<strong class="jq hi">全局描述符</strong>，上图显示了我们的<strong class="jq hi">全局检索模型</strong>的训练架构。</p><p id="6590" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">但是什么是全局图像描述符呢？简而言之，它是一个n维向量，作为我们的图像的编码版本，适合特定的用例。在我们的例子中的一个具有512个维度，具有优化的辨别能力来区分一个地标和另一个地标。它不仅从地标的一般视觉特征中学习，还从上下文信息中学习，例如背景和前景对象、照明条件和有利位置。</p><p id="a30a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">虽然下面的模型不用于推理，但是我们的全局描述符的质量与分类模型的性能直接相关。对于识别和检索竞赛中排名最高的解决方案，这些嵌入层通常在多个ResNet CNNs的集合上训练。在下面的示例中，我们将使用预训练的EfficientNetB0(也称为<em class="mn">主干模块</em>)实现一个简单的解决方案。然后，我们的分类头在一个平均池、一个批量规范化和一个用于规范化的删除层的基础上重新构建。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="d87b" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">嵌入图层正好构建在Sotfmax分类图层之前。它也被命名为<code class="du mu mv mw mx b">embedding_512</code>以备后用。请注意，我们是如何冻结EfficientNet块来保持预训练的权重的。为了提高性能，您还可以微调主干中的顶层(Keras文档中的<a class="ae jn" href="https://keras.io/guides/transfer_learning/" rel="noopener ugc nofollow" target="_blank">迁移学习指南</a>中有一节是关于微调的)。我们将保持简单，继续用ADAM优化器和学习率调度器进行训练。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nl"><img src="../Images/b226fa1036abb5c9430011ff8ac5640a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EImBNg-AL9RwRvZb5kOYRA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 13.</strong> Train and validation loss.</figcaption></figure><p id="f0cd" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">并评价最佳模型性能。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nm"><img src="../Images/b931034a0112aab960ad645c52f9acb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vph0Va3adIFq4t0zlZNLAg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 14.</strong> Best model performance in the validation and test set.</figcaption></figure><p id="30dc" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这样，我们就有了一个经过训练的嵌入层，现在可以用它来生成全局描述符。这将我们引向我们的检索步骤。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="9826" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">余弦相似性</h2><p id="3bd1" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">度量学习的核心概念围绕着一个<em class="mn">参数距离</em>或<em class="mn">相似性函数</em>在一个或多个<strong class="jq hi">相似性(或不相似性)判断</strong>上的优化。根据这个词的最佳定义，判断是关于手头任务的深思熟虑的决定，并且通常被编码为目标变量——在我们的例子中，是landmark类。</p><p id="6558" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">但是这个概念并不总是简单明了的。下面的例子来自<em class="mn">“相似性和距离度量学习及其在计算机视觉中的应用”</em> (Bellet，A. et al. 2015) [1]说明了一种定性判断，这种判断通常不可直接量化。这些是监督度量学习可以发挥最大作用的情况。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nn"><img src="../Images/fbbb6c661fc4513e56dfe22a795a482b.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*TpFszc540n9T6axMsCKh2Q.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 15.</strong> Example of a qualitative judgement fitting to a metric learning task. [1]</figcaption></figure><p id="0a39" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">有了定义的判断，我们就可以实现一个能够解决这个优化问题的模型架构。在我们的例子中，我们将使用一个<strong class="jq hi">余弦相似度</strong>作为我们的距离度量，如前所述，优化发生在我们的全局描述符的训练期间。我们的嵌入层越好(最具描述性)，我们就越接近问题的最佳解决方案。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es no"><img src="../Images/8d1558d5bffdfd9dc35aaa35fcbd49c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LeHQu5rstfFcDHQqIpQ3RA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 16.</strong> Similarity search with global features in the architecture diagram.</figcaption></figure><p id="3b54" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">余弦相似性(或角余弦距离)是两个向量之间角度的余弦度量，可以用数学方法描述为向量的点积与其长度的乘积(或其欧几里德范数的乘积)之比[3]。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es np"><img src="../Images/8e9fbeda3f40a228e09663de311fd354.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*_vrIwrXDsJNVBprNxRNWHA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 17.</strong> Cosine Similarity mathematic representation [3].</figcaption></figure><p id="6188" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">上式中，<em class="mn"> x </em>和<em class="mn"> y </em>是两个独立矢量的分量，<em class="mn"> n </em>是它们的维数。</p><p id="f046" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">在下面的代码片段中，我们实现了一系列辅助函数来加载和预处理单个图像，检索给定模型的图像嵌入(稍后我们将把我们的<code class="du mu mv mw mx b">embedding_512</code>层传递给它)，计算查询和键集之间的成对相似性，以及一些可视化函数来绘制我们最相似的候选图像。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><blockquote class="nq nr ns"><p id="b5e3" class="jo jp mn jq b jr js ii jt ju jv il jw nt jy jz ka nu kc kd ke nv kg kh ki kj ha bi translated"><strong class="jq hi">关于性能的注意事项</strong>:上面的余弦相似性实现基于简单的SciPy顺序CPU运行，旨在针对所有矢量化关键样本在单个矢量化查询图像之间单独执行。</p><p id="c928" class="jo jp mn jq b jr js ii jt ju jv il jw nt jy jz ka nu kc kd ke nv kg kh ki kj ha bi translated">当处理大规模场景(例如完整的数据集)时，您希望高效地计算查询向量矩阵和关键样本矩阵之间的余弦距离。为此，您可以使用TensorFlow tensors实现批处理GPU运行，以实现高度并行化的执行。您可以查看<a class="ae jn" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity" rel="noopener ugc nofollow" target="_blank"> keras余弦相似性损失</a>源代码，了解如何使用张量执行此操作。</p></blockquote><p id="96a6" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">对于接下来的例子，我们将使用我们的训练集作为我们的关键图像，并从我们的验证集中获取一些查询样本。下面的代码将通过函数<code class="du mu mv mw mx b">get_embeddings()</code>将两个子集传递到<code class="du mu mv mw mx b">embedding_layer</code>。它将为每个图像生成512维的全局描述符。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="1dc5" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">所以让我们使用我们的<code class="du mu mv mw mx b">query_top()</code>函数来查询一些图像并可视化结果。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nw"><img src="../Images/dc8f49025046045c495e4008483f959f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWujouW1W7xWonBW9HO1Vw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 18.</strong> Queried image and highest similarity candidates.</figcaption></figure><p id="caee" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们可以从查询集中的第一幅图像看出，我们的全局描述符是有效的，并且结果与我们的查询图像高度相关。他们不仅返回了所有前五名候选人的正确地标，还返回了具有相似有利位置和照明条件的图像。</p><p id="b505" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">下面的函数执行类似的搜索，但返回的结果是熊猫数据帧。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="9987" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们可以用它来查看我们的余弦相似性分数。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nx"><img src="../Images/ae363b2d418b5b42156aaa6e2e497909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*SINaTtRAJMyxSY8gZVBtfQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 19.</strong> Highest similarity candidates scores.</figcaption></figure><p id="971d" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们可以对其他查询图像示例重复这个过程。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ny"><img src="../Images/4f1fcab55c161543406ce688f5290456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twmljxOVQrDbf6NVKqPo1g.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 20.</strong> Queried image and highest similarity candidates example.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nz"><img src="../Images/995b32cacd525a7c8afbb6ff42c08486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uABJzbiuFM6xrdXQ1eHRJQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 21.</strong> Queried image and highest similarity candidates example.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oa"><img src="../Images/bc2dd48966e1b9423f3c263a56c37a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_hahfC8YWYX0jVjFXQLgQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 22.</strong> Queried image and highest similarity candidates example.</figcaption></figure><p id="0d03" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这样，我们就结束了具有全局特征的相似性搜索。到目前为止，我们已经取得了很好的结果，但是让我们看一个例子，在这个例子中，全局描述符在性能上有所欠缺。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="5b8e" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">使用局部特征重新排序</h2><p id="2ca1" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">让我们看一个物体遮挡的例子。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ob"><img src="../Images/435a2b1fd62197a2de76b1faa7797743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NHc8YNCVUXBYGOmXvCpaMw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 23.</strong> Landmark occlusion example.</figcaption></figure><p id="862a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">该地标不仅被遮挡，而且还占据了图片中相当小的区域，该区域由前景中的树主导。我们可以从结果中看到，它在相似性搜索中发挥了主要作用，并且在大多数顶部结果中，我们还可以在图像中看到一棵大树。</p><p id="093f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这是一个例子，在这个例子中，我们的地方特色将发挥重要作用。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oc"><img src="../Images/07592e0356d1f027dea22efb0396c5fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnpVevNKInFxPM2ZPyefPg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 24.</strong> Reranking with local features in the architecture diagram.</figcaption></figure><p id="3bf6" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我们将使用<strong class="jq hi">深度局部特征(DELF)</strong>【13】<strong class="jq hi"/>模块从我们的查询图像中提取关注的局部特征描述符，并将其与之前选择的排名最高的候选图像进行比较。</p><p id="88ea" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">DELF是一种卷积神经网络模型，在地标图像上进行训练。它实现了精确的特征匹配和几何验证，通过模型论文，谷歌宣布了第一个谷歌地标数据集(GLDv1) [13]。可以在<a class="ae jn" href="https://arxiv.org/abs/1612.06321" rel="noopener ugc nofollow" target="_blank">论文</a>中了解更多。</p><p id="b338" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">以下实现改编自DELF模块的<a class="ae jn" href="https://www.tensorflow.org/hub/tutorials/tf_hub_delf_module" rel="noopener ugc nofollow" target="_blank"> TensorFlow hub教程。由于这是一个优化步骤，我们将设置我们的图像大小为600 x 600。以下代码将从TensorFlow hub加载预训练的DELF模型，并定义一些函数来查找图像对之间的内联体(特征匹配)。</a></p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="00ce" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">现在，我们可以遍历之前的结果来寻找内联体。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es od"><img src="../Images/63f6006704cd0bd0827d00ee23439a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4UvUnwt8Evj1lHamFkKGTg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 25.</strong> DELF correspondences on candidate images.</figcaption></figure><p id="495c" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">DELF架构提出的局部注意特征是基于几何属性的相似性匹配的强大描述符。从上面的结果可以看出，尽管在比例和分辨率上存在差异，但它能够在正确的图像对之间识别建筑结构的相关特征。</p><p id="bcb6" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">这就是我们的最后一步。我们将使用找到的DELF对应的数量从全局相似性搜索中重新排列我们的候选图像。以下函数将通过将当前值乘以使用局部特征找到的内联体数量的平方根来重新计算置信度指数(之前为余弦相似度)。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="4cf2" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">最后，我们可以看看上面例子的重新排序结果。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oe"><img src="../Images/9750fb732f6ce6c2b8efdaafce9e48db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_EY-foZKt-jr1lSClE_Eg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 26.</strong> Reranked confidence index.</figcaption></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es of"><img src="../Images/fa09e451cb15b1b2445822e7603102b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3enbq5MufNRUaGLxDv2eMA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 27.</strong> Reranked candidate landmarks.</figcaption></figure><p id="8103" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">至此，我们已经介绍了利用全局相似性搜索和局部特征重新排序进行地标识别的完整度量学习架构。</p><p id="2cab" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">我将在下面的章节中留下额外的重新排序的例子和相关的补充阅读材料。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="e1f8" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">重新排序示例</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es og"><img src="../Images/0e04a8b79c2c768d8cca1521f0dda297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qx190Xhr9fwVl8tqL5u8gw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 28.</strong> Queried image and highest similarity candidates without reranking.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mt"><img src="../Images/51d6bfdf40609d1a9e8a7355035f17a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hH0_iUNobSAb1sGd-Axuog.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 29.</strong> Reranked results with local features.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es og"><img src="../Images/318c460512ad91fe23dc29536220909f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2TmLScba3O6Qi8sqnTyJw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 30.</strong> Queried image and highest similarity candidates without reranking.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mt"><img src="../Images/f5ca3ee90c4998ead8af57524c9036b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgayHWBjqAWWRAwDZ1PIIQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 31.</strong> Reranked results with local features.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oh"><img src="../Images/2b21bb14bc3c99056f2e940759f6b1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwzkDW6ZMtjUK835M7fC8w.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 32.</strong> Queried image and highest similarity candidates without reranking.</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oi"><img src="../Images/691900a0aa44e4327a725db63cb13b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPA2ym8ppyo5H3t3GjASWQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd jm">Figure 33.</strong> Reranked results with local features.</figcaption></figure></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="de56" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">弧面损失</h2><p id="e9dd" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">处理大规模分类问题(如GLDv2数据集中的问题)的挑战之一是，大量的类(在现实世界中可能会随着时间的推移而增加)会导致常规的softmax损失函数缺乏类间可分性。为了解决人脸识别算法的类似问题，ArcFace Loss于2018年提出，今天在地标识别算法中被高度采用。</p><p id="614e" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">ArcFace margin [6]是一个附加的角裕度损失函数，它强制执行较小的类内方差。与这里使用的softmax损失函数相反，由于其增强的辨别能力，已经证明它产生更好的全局特征描述符。</p><p id="6226" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">如果你想了解更多，我强烈推荐阅读<a class="ae jn" href="https://arxiv.org/pdf/1801.07698.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a> (Deng，J. et al. 2018) [6]和Slawek Biel的优秀的<a class="ae jn" href="https://www.kaggle.com/code/slawekbiel/arcface-explained" rel="noopener ugc nofollow" target="_blank">内核</a>，与通常的softmax损失相比，它提供了ArcFace学习嵌入的视觉直观。</p><h2 id="9053" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">谷歌的统一DELG模式</h2><p id="6dac" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">深度局部和全局特征(，Cao，b .等人，2020) [4]是谷歌提出的统一模型，该模型将DELF的注意力模块与更简单的训练管道相结合，该训练管道与同一架构下的全局特征检索相集成。它本质上是本文中举例说明的架构的单一模型实现。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="23b6" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">喜欢这个故事吗？</h2><p id="313b" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">你可以在Medium上关注我，获取更多关于数据科学、机器学习、可视化和数据分析的文章。</p><p id="c6ca" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">你也可以在 <a class="ae jn" href="https://www.linkedin.com/in/erich-henrique/" rel="noopener ugc nofollow" target="_blank"> <em class="mn"> LinkedIn </em> </a> <em class="mn">上找到我，我在那里分享这些内容的简短版本。</em></p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h2 id="0798" class="la lb hh bd jm lc ld le lf lg lh li lj jx lk ll lm kb ln lo lp kf lq lr ls lt bi translated">参考</h2><p id="f0cb" class="pw-post-body-paragraph jo jp hh jq b jr lw ii jt ju lx il jw jx mk jz ka kb ml kd ke kf mm kh ki kj ha bi translated">[1] Bellet，Aurélien和Matthieu Cord"相似性和距离度量学习及其在计算机视觉中的应用."里尔大学研究，2015年9月7日。<a class="ae jn" href="http://researchers.lille.inria.fr/abellet/talks/metric_learning_tutorial_CIL.pdf." rel="noopener ugc nofollow" target="_blank">http://researchers . lille . inria . fr/abellet/talks/metric _ learning _ tutorial _ cil . pdf</a></p><p id="a6eb" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[2] Callaway，Ewen。“‘它将改变一切’:deep mind的人工智能在解决蛋白质结构方面取得巨大飞跃。”自然新闻。自然出版集团，2020年11月30日。<a class="ae jn" href="https://www.nature.com/articles/d41586-020-03348-4." rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/d41586-020-03348-4.</a></p><p id="ae6f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[3]“余弦距离余弦相似度角余弦距离角余弦相似度。”余弦距离，余弦相似性，角余弦距离，角余弦相似性。2022年10月29日接入。<a class="ae jn" href="https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/cosdist.htm." rel="noopener ugc nofollow" target="_blank">https://www . ITL . NIST . gov/div 898/software/data plot/ref man 2/auxillar/cos dist . htm</a></p><p id="9680" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[4]曹、、阿劳若、安德烈和杰克·西姆。"统一图像搜索的深度局部和全局特征."arXiv ，(2020)。<a class="ae jn" href="https://doi.org/10.48550/arXiv.2001.05027." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2001.05027.</a></p><p id="9fe7" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[5]陈、凯冰、崔、程、杜、余宁、孟、向龙、惠仁。“2019年Kaggle地标识别和检索竞赛的第二名和第二名解决方案。”<em class="mn"> arXiv </em>，(2019)。<a class="ae jn" href="https://doi.org/10.48550/arXiv.1906.03990." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1906.03990.</a></p><p id="d1a2" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[6]邓建康、郭、贾、杨、景、薛、年南、科奇亚、艾琳和斯特凡诺斯·扎菲里乌。" ArcFace:深度人脸识别的附加角裕度损失."<em class="mn"> arXiv </em>，(2018)。<a class="ae jn" href="https://doi.org/10.1109/TPAMI.2021.3087709." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/TPAMI.2021.3087709.</a></p><p id="2753" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[7]“面部识别无处不在。这是我们能做的。”纽约时报。《纽约时报》，2020年7月15日。https://www . nytimes . com/wire cutter/blog/how-face-recognition-works/。</p><p id="9523" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[8]“谷歌地标数据集v2——实例级识别和检索的大规模基准”，T. Weyand，A. Araujo，B. Cao和J. Sim，Proc .CVPR 20年</p><p id="0c0e" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[9]“谷歌地标识别2020。”卡格尔。谷歌。2022年10月28日访问。<a class="ae jn" href="https://www.kaggle.com/c/landmark-recognition-2020." rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/landmark-recognition-2020.</a></p><p id="f2ce" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[10]汉高、克里斯托夫和菲利普·辛格。“支持域外样本的大规模图像识别。”<em class="mn"> arXiv </em>，(2020)。【https://doi.org/10.48550/arXiv.2010.01650. T4】</p><p id="a824" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[11] Jumper，j .，Evans，r .，Pritzel，A. <em class="mn">等</em>“用AlphaFold进行高度精确的蛋白质结构预测”<em class="mn">性质</em> <strong class="jq hi"> 596 </strong>，583–589(2021)。<a class="ae jn" href="https://doi.org/10.1038/s41586-021-03819-2" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1038/s41586-021-03819-2</a></p><p id="004a" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[12]纳亚克，潘杜。“比以往任何时候都更好地理解搜索。”谷歌。谷歌，2019年10月25日。<a class="ae jn" href="https://blog.google/products/search/search-language-understanding-bert/." rel="noopener ugc nofollow" target="_blank">https://blog . Google/products/search/search-language-understanding-Bert/。</a></p><p id="396d" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">[13] Noh、Hyeonwoo、Araujo、Andre、Sim、Jack、Weyand、Tobias和Bohyung Han。"关注深层局部特征的大规模图像检索."<em class="mn"> arXiv </em>，(2016)。<a class="ae jn" href="https://doi.org/10.48550/arXiv.1612.06321." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1612.06321.</a></p><div class="oj ok ez fb ol om"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="on ab dw"><div class="oo ab op cl cj oq"><h2 class="bd hi fi z dy or ea eb os ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ot l"><h3 class="bd b fi z dy or ea eb os ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ou l"><p class="bd b fp z dy or ea eb os ed ef dx translated">medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa jg om"/></div></div></a></div></div></div>    
</body>
</html>