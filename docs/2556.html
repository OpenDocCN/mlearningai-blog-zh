<html>
<head>
<title>Mathematical Approaches to Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的数学方法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/mathematical-approaches-to-machine-learning-2da4e97e881f?source=collection_archive---------10-----------------------#2022-05-18">https://medium.com/mlearning-ai/mathematical-approaches-to-machine-learning-2da4e97e881f?source=collection_archive---------10-----------------------#2022-05-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/eda81c76230b77b5fb4aba9315f22423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlK69rOYJgT7QREfGCD2QA.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@dancristianpaduret" rel="noopener ugc nofollow" target="_blank">Dan-Cristian Pădureț</a> on <a class="ae it" href="https://unsplash.com/photos/h3kuhYUCE9A" rel="noopener ugc nofollow" target="_blank">Unsplash</a>.</figcaption></figure><h1 id="ddfc" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">介绍</h1><p id="6eac" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">机器学习是一组应用于许多数据集的通用算法，通过利用数据、模型和学习实现的反复试验来获得意义或预测。机器学习背后的核心思想是自动化；因此，它旨在开发通用方法，在没有任何领域知识的情况下从数据中发现有意义的模式。为了实现这一点，需要使用数学概念，例如在神经网络中，使用微积分性质进行梯度下降以收敛，在朴素贝叶斯中，使用概率论进行分类，以及在数据压缩或降维中，通过基于线性代数的主成分分析。</p><h1 id="c791" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">基础和背景</h1><p id="67e8" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">神经网络:是受监督的机器学习算法，模拟生物大脑中神经元的运作。</p><p id="fcfa" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">梯度下降:一种依赖于衍生概念的算法，用于找到从成本函数中得出最小值的最佳参数/权重集，该成本函数必须是可微分的。</p><p id="30de" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">成本函数:评估算法对数据建模效果的方法；它输出的值越少，模型就越好。</p><p id="daec" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">θ:权重参数。η:学习率，即更新权重时我们采用的步长。</p><p id="e65e" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">朴素贝叶斯:是一组有监督的机器学习算法，假设特征之间是条件独立的。</p><p id="bba4" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">条件概率:是一个或多个事件在另一个事件发生时的概率:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/209d2774c4f4e5763eac7d858f47f12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*_BVrTZ8C-oTf4KjTcR_3KA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source <a class="ae it" href="https://latex.codecogs.com/" rel="noopener ugc nofollow" target="_blank">Codecogs</a></figcaption></figure><p id="098c" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">联合概率:多个事件同时发生的概率𝑃(𝐴 ∩ 𝐵).</p><p id="39b8" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">边际概率:一个事件发生的概率，不考虑其他随机变量的结果，例如𝑃(𝐵).</p><h1 id="32b7" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">讨论</h1><h2 id="896b" class="la iv hh bd iw lb lc ld ja le lf lg je kd lh li ji kh lj lk jm kl ll lm jq ln bi translated">结石</h2><p id="a2cf" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">微积分是机器学习中使用的基本数学方法之一(Deisenroth等人，2020年，第141页)。由于其研究变化率和面积的方法，它被用于机器学习，以迭代地改进算法的行为，以便像在优化问题中一样学习和适应数据。例如，在神经网络的反向传播中，梯度下降通过找到最佳参数/权重来最小化成本函数。首先，该算法为权重分配随机值，然后使用多变量微分法；它采用函数梯度，导致最陡上升的方向，使输出最大化；因此，它采取相反的方向(负)。然后，它同时根据下降更新参数并重新计算梯度。它继续这样做，直到达到最小值。</p><p id="188e" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">梯度下降方程:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/95401e8ddc74ea63bd10897c9627b399.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*SIJ7Cqa03Rq3yKOHyNYMWw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source <a class="ae it" href="https://latex.codecogs.com/" rel="noopener ugc nofollow" target="_blank">Codecogs</a></figcaption></figure><h2 id="bfb5" class="la iv hh bd iw lb lc ld ja le lf lg je kd lh li ji kh lj lk jm kl ll lm jq ln bi translated">线性代数</h2><p id="7713" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">如果输入特征过多，机器学习算法的性能会下降，因此需要压缩数据。最广泛使用的降维技术是主成分分析，它使用线性代数基本原理来执行数据到低维空间的线性映射。PCA首先通过减去平均值将数据集中在原点；然后，计算协方差矩阵，并计算矩阵的特征值和相应的特征向量，由于协方差矩阵是对称的，所以特征向量是正交的。特征向量是数据的方向，特征值是幅度；因此，具有最大相应特征值的特征向量是表示大部分数据方差的第一主分量，它表示数据中的重要模式，第二特征向量表示不太重要的模式，依此类推。因此，如果我们有一个n维数据集需要缩减到k维，我们将数据投影到代表大部分数据的前k个PC上。</p><h2 id="9544" class="la iv hh bd iw lb lc ld ja le lf lg je kd lh li ji kh lj lk jm kl ll lm jq ln bi translated">概率论</h2><p id="41af" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">不确定性是机器学习的一个基本概念；它既来自测量中的噪声，也来自有限大小的数据集。概率论为不确定性的量化和操纵提供了一个一致的框架(Bishop，2006，第12页)。此外，它是一些机器学习算法的基础，因为学习算法的设计通常依赖于数据的概率假设。例如，朴素贝叶斯是一种基于贝叶斯定理的概率模型，它在已知另一个已经发生的事件(B)的先验知识的情况下确定一个事件(A)的概率，并假设特征之间相互独立。从条件概率导出的贝叶斯定理，计算条件概率时不需要联合概率。这意味着它通过使用条件概率𝑃(𝐴 ∩ 𝐵) = 𝑃(𝐴|𝐵) * 𝑃(𝐵来使用联合概率的替代计算，并且因为联合概率是对称的；因此，联合概率可以是𝑃(𝐴 ∩ 𝐵) = 𝑃(𝐵|𝐴) *P(A)。因此，根据贝叶斯定理，条件概率变成:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/25e5a278c35171a8f2076c02c0f58615.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*6aS1b2vj2YN2WmCJ3IYigw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source <a class="ae it" href="https://latex.codecogs.com/" rel="noopener ugc nofollow" target="_blank">Codecogs</a></figcaption></figure><p id="4377" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">然而，由于朴素贝叶斯假设特征之间的独立性𝑃(𝐵|𝐴)*𝑃(𝐴𝑃(𝐵，𝑃(𝐵，即边际概率，变成常数，所以它被忽略，并且使用独立性假设𝑃(𝐵|𝐴变成所有其他给定特征的乘积:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/debd2a0221fe60bb25d9dbb3b5469861.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*SXE17rdYSUH9D2s-yrAfUQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source <a class="ae it" href="https://latex.codecogs.com/" rel="noopener ugc nofollow" target="_blank">Codecogs</a></figcaption></figure><p id="d90a" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">然后，它们被乘以，这被称为先验或证据，结果是:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/408bea8e9ce90ae1cacce3a9752f47c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*Byjli4jxRLsrx3k7ufcfSA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Source <a class="ae it" href="https://latex.codecogs.com/" rel="noopener ugc nofollow" target="_blank">Codecogs</a></figcaption></figure><h1 id="6874" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">结论</h1><p id="2341" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">机器学习模型都是建立在数学基础之上的；机器学习的四个核心支柱是微积分、线性代数、概率和统计。由于微积分导数工具，梯度下降能够迭代地改进模型的行为。线性代数特征分解和矩阵分解技术使得一些ML模型，如PCA，能够用于维数减少。最后，利用概率论中的贝叶斯定理，朴素贝叶斯可以预测未来事件的可能性。</p><h1 id="1322" class="iu iv hh bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">参考</h1><p id="98a4" class="pw-post-body-paragraph js jt hh ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp ha bi translated">Deisenroth，M. P .，Faisal，A. A .，&amp; Ong，C. S. (2020)。<em class="ls">机器学习的数学</em>。剑桥大学出版社。</p><p id="cc74" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">主教，C. M. (2006年)。<em class="ls">模式识别和机器学习</em>。斯普林格。</p><p id="ef3d" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">林赛·史密斯。(2002).<em class="ls">主成分分析教程</em>。奥塔哥大学。</p><p id="eb88" class="pw-post-body-paragraph js jt hh ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp ha bi translated">路易斯·塞拉诺。(2019).<em class="ls">主成分分析</em>。塞拉诺学院。</p><div class="lt lu ez fb lv lw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lx ab dw"><div class="ly ab lz cl cj ma"><h2 class="bd hi fi z dy mb ea eb mc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="md l"><h3 class="bd b fi z dy mb ea eb mc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="me l"><p class="bd b fp z dy mb ea eb mc ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk in lw"/></div></div></a></div></div></div>    
</body>
</html>