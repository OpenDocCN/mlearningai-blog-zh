<html>
<head>
<title>Reproducing the paper Deep Fruit Detection in Orchards</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">果园水果深度检测论文的复制</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/reproducibility-for-deep-fruit-detection-in-orchards-e518367ccf35?source=collection_archive---------2-----------------------#2021-04-16">https://medium.com/mlearning-ai/reproducibility-for-deep-fruit-detection-in-orchards-e518367ccf35?source=collection_archive---------2-----------------------#2021-04-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/></div><div class="ab cl ie if go ig" role="separator"><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij ik"/><span class="ih bw bk ii ij"/></div><div class="ha hb hc hd he"><p id="3583" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这篇博客的作者是斯坦·兹温克尔斯、泰德·德·弗里斯·伦奇和T2。</p><p id="c8f8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这篇博文旨在展示我们复制论文<a class="ae jj" href="https://arxiv.org/abs/1610.03677" rel="noopener ugc nofollow" target="_blank">果园深度水果检测</a>的尝试。该论文的作者创建了一个准确可靠的基于图像的水果检测系统，用于检测苹果、芒果和杏仁。在这篇博客中，我们解释了我们为复制论文中的图2所做的努力。我们将解释训练我们的模型和我们的思想所采取的步骤，以及与原始论文不同的实现步骤。作者没有公开他们的代码，因此Python代码完全是自制的。这篇博客强调了一些在原始论文中没有充分讨论的重要注意事项，这使得实现变得有点困难。本博客是代尔夫特大学深度学习课程(CS4240)的一部分。其他博文可以在这里<a class="ae jj" href="https://reproducedpapers.org" rel="noopener ugc nofollow" target="_blank">找到。</a></p><h1 id="6d0b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">1.介绍</h1><p id="6d86" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">有许多检测论文可用，但只有少数论文在真正适用的数据集上进行测试，而不是使用标准化数据集，如ImageNet或CIFAR-10/100。论文《果园中的深层水果检测》试图为农业任务，如产量测绘和机器人收获，创建一个准确可靠的基于图像的水果检测系统。该网络的架构基于更快的R- CNN。当时，当论文发表时(2016)，更快的R-CNN是一种最先进的对象检测框架(Ren等人，2015)。为了验证这个框架的准确性和可靠性，作者在果园水果检测的背景下使用了更快的R-CNN。目前，有其他更好的现有架构，其性能优于讨论中提到的更快的R-Cnn。</p><p id="4258" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在整个博客中，代码的小片段被呈现来充实文本。我们的代码可以在库<a class="ae jj" href="https://github.com/TedDeVriesLentsch/DeepLearning_Group20" rel="noopener ugc nofollow" target="_blank"> DeepLearning_Group20 </a>中找到。</p><h2 id="2f4c" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">资料组</h2><p id="8c7f" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">论文的作者创建了一个包含数千张苹果、芒果和杏仁图像的数据集。这些图像是从澳大利亚不同的果园农场拍摄的。数据集中的图像已经用文本文件进行了注释和清晰的分隔，以指示训练集、验证集和测试集。值得注意的是，训练数据集包含一些没有正确注释的图像。这确实对准确度分数有影响，但是需要在进一步研究的基础上进一步调查。由于时间有限，只使用苹果数据集进行检测。不需要太多的调整，人们也应该能够在架构上训练和测试其他水果图像。</p><p id="738a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">apple训练数据集包含1120幅苹果树图像，大小为202x308像素。苹果的注释被构造为[center_x，center_y，radius]，它被转换为[x_min，x_max，y_min，y_max]坐标，因为更快的R-CNN框架使用矩形框。由于VGG16器件进行了二次采样(二次采样系数为16)，所有图像的尺寸都调整为500x764像素。接下来，对于训练数据集，不包含任何苹果的图像被从数据集中移除。结果得到846个训练图像，然而，该论文指出应该有729个图像，因此我们使用更多的图像来训练网络。这种差异的原因不明。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/9b8a056f4283d4205662584d7d31f5ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KfZbHrFu_wXj1ISnEo_9LA.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Figure 1. A training image after rescaling including the annotations for the apples.</figcaption></figure><h1 id="9c5c" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.更快的R-CNN</h1><p id="a928" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">这不是第一个讨论实现更快的R-CNN的博客，R-CNN已经成为一种非常流行的方法，因为它可以实现对对象检测的高精度。快速R-CNN由三个部分组成，主干网络(VGG16或ZF)、区域提议网络(RPN)和快速R-CNN。使用VGG16作为主干，因为该网络通过PyTorch Torchvision(在ImageNet上培训)进行了预培训。在接下来的部分中，每个组件都将得到更详细的解释。</p><h2 id="b3a1" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">2.1.VGG16</h2><p id="7845" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">VGG16是一种卷积神经网络架构，用于提取每幅图像的特征。仅使用VGG16的前30层。尺寸为3x500x762的重新缩放图像作为输入给出。由于存在多个max-pooling图层，VGG16网络会输出一个大小为512x31x47的要素地图。这些特征地图既用于RPN，也用于快速R-CNN网络。获取VGG16前30层的代码如下所示。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Python implementation of VGG16.</figcaption></figure><h2 id="35fd" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">2.2.区域提案网络(RPN)</h2><p id="4834" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">VGG16网络的输出用于区域建议网络(RPN)。RPN的目标是创建盒子，并确定每个盒子包含对象的概率(二元分类:背景对前景)。由于跨度为16，重新缩放的图像可以被分成边长为16的正方形。每个方块的中心如下所示。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lt"><img src="../Images/dc5c2e845d886f5db87e6a749a1b049c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-E13ojkreOnzVjT7mxwMA.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Figure 2. The center of every square is shown on the rescaled image.</figcaption></figure><p id="65dc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于每个正方形的每个中心，创建九个锚盒(三种不同的大小和三种比率)。导致整个图像上总共有31*47*9=13.113个锚定框。锚箱的尺寸没有在文件中规定，但是设置为[64，128，256 ]，宽/高比为[0.5，1，2]。锚箱尺寸的计算如下所示。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">The calculation for computing the height and width of every anchor box.</figcaption></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lu"><img src="../Images/85f44c6b2fcef611b5615f8865578b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1Q5S3ZbLj8bOFSnGFetWA.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Figure 3. The 9 anchor boxes for only one center point.</figcaption></figure><p id="615d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">13113个锚盒覆盖了整个图像。位于图像外部的锚定框不用于训练并被移除。</p><p id="c671" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下一步是将注释数据与创建的锚定框结合使用。对于每个锚定框，计算每个注释框的并集交集(IoU)。IoU用于测量锚框与注释数据的重叠。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Python code to compute the Intersection over Union for every anchor box.</figcaption></figure><p id="3eaf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">每个锚定框都有一个特定的分数，该分数指定了它与特定注释框的重叠程度。下一步是仅获得具有高于某个阈值的值的锚盒，并将它们标记为阳性。更快的R-CNN论文的作者使用0.7的阈值。IoU小于0.3的锚框被标记为负锚框。所有其他锚盒都标记为-1，不能作为训练的一部分。</p><p id="9b1a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于回归，使用回归参数创建一个数组，其中包含图像内锚定框的地面实况注释框参数。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Parameterization of the 4 coordinates.</figcaption></figure><p id="8a06" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最后，我们获得回归参数为<strong class="in hi"> regression_gt </strong>和一个数组，每个锚框有一个标签，<strong class="in hi"> anchor_labels </strong>。</p><h2 id="de16" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">模型结构</h2><p id="635c" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">锚盒的训练是用每个包含256个锚盒的小批量来完成的。对于每个小批量，使用一个图像，如果可能的话，一个批量包含128个阴性和128个阳性锚盒。如果具有正标签的锚盒少于128个，则取更多的负标签盒以达到256个盒。正负指数都是随机选取的。</p><p id="4c54" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在应用于分类和回归之前，特征图首先通过较小的卷积层。回归层给出一个提议框，可以用来分类里面有没有苹果。并且分类器用于检查背景或前景。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Convolutional layers for regression and classification.</figcaption></figure><p id="428b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">下一步是计算分类器和回归的损失。使用交叉熵损失来计算分类损失。对于回归，使用平滑L1损失计算损失。更快的R-CNN论文指出，为了计算总损失回归，需要使用以下公式。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lv"><img src="../Images/6d01996bbb6a6da40e84f5be39f3593d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QfUaaJEBW5gGfWSUrJZSjg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Figure 4. Total loss for both classification and regression for training the RPN.</figcaption></figure><p id="e6fc" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">N_cls是小批量的大小(256)，N_reg是锚中心的总数(1457)。对于lambda，作者没有指定他们的设置，所以它被设置为10，如fast R-CNN论文中所述。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Losses for classification, regression, and total RPN loss.</figcaption></figure><p id="e106" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">有了损失函数，RPN现在可以被训练来给出在预测的盒子中可能包含苹果的提议位置。在快速R-CNN可以用于预测所提出的盒子是否包含任何苹果之前，需要使用<strong class="in hi">convert _ to _ coordinates(…)</strong>函数将回归参数转换回[x_min，x_max，y_min，y_max]坐标。</p><h2 id="af1d" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">非最大抑制(NMS) — RPN</h2><p id="8a91" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">回归提出的坐标连同预测的标签被用于非最大值抑制，以去除重叠的建议锚框。对于非最大抑制，IoU阈值为0.7。因此，如果一个锚定框与另一个锚定框的IoU值大于0.7，并且如果另一个锚定框具有更高的分数(更有可能它包含对象)，则移除该锚定框。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Non-maximum suppression after performing RPN.</figcaption></figure><h2 id="950b" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">2.3.快速R-CNN</h2><p id="2e05" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">有了RPN创建的提案箱，下一步就是分类它们是否包含苹果。在第一步中，与RPN一样，将建议的锚框与注释数据进行比较，以再次使用IoU将这些框标记为负或正。然而，这一次使用了不同的阈值。如果某项提案的IoU值在区间[0.5，1]内，则该提案将获得正标签，如果IoU值在区间[0.1，0.5]内，则该提案将获得负标签。既不积极也不消极的建议会被贴上“无效”的标签。坐标被转移回建议的基本事实注释框的回归参数中，<strong class="in hi">建议_标签</strong>。</p><h2 id="6a14" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">模型结构</h2><p id="b26b" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">建议标签用于创建小批量。这些小批各包含128个提案，来自两个不同的图像(各64个)。在这64个建议中，最多16个是正面的，最少48个是负面的，这取决于图像有多少正面标签。对于数据集，正框和建议框都是随机抽样的。</p><p id="a5c4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在分类和回归可以应用于小批量之前，建议的箱子经过自适应最大池，因为每个建议可以有不同的大小。每个建议的输出都是[7x7]的特征图。使用自适应最大池后，输出通过两个完全连接的层。然后，该输出将用于回归图层和分类图层。与RPN相同，回归预测框<strong class="in hi"> <em class="lw"> y_reg </em> </strong>的回归参数。分类器对<strong class="in hi"> <em class="lw"> y_cls </em> </strong>内是否有苹果进行分类。</p><figure class="lc ld le lf fd lg"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ln lo et er es lp lq bd b be z dx">Architecture for Fast R-CNN</figcaption></figure><p id="6bd0" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">分类器预测建议的盒子是否包含苹果。为了计算分类器和回归的损失，使用交叉熵损失来计算分类损失。对于回归，只有正面标记的建议框用于计算损失。使用平滑L1损耗来计算损耗。快速R-CNN的总损耗可以用与RPN类似的公式计算。只是这一次，lambda被设置为1。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lx"><img src="../Images/f3834e3bfcf9b7c66700589e82a60d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6O_pJv38D2EFPfSwpZ3rNQ.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Figure 5. The loss function for an image, both classification, and regression.</figcaption></figure><p id="f452" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果作为苹果的得分高于0.5，则用分类器生成的预测得分仅被标记为苹果。从集合中移除所有其他提议的盒子。接下来，只需要应用非最大抑制来移除重叠的建议框。</p><h2 id="7064" class="kn jl hh bd jm ko kp kq jq kr ks kt ju iw ku kv jy ja kw kx kc je ky kz kg la bi translated">非最大抑制(NMS) — Fast-R-CNN</h2><p id="e999" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">首先，需要使用<strong class="in hi">convert _ to _ coordinates(…)</strong>函数将回归图层预测的坐标转换回[x_min，x_max，y_min，y_max]坐标。对于这些提议，应用非最大值抑制来移除阈值0.2以上的重叠框。这将删除大多数与其他框重叠过多的建议框。</p><h1 id="cff1" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">3.培养</h1><p id="77a2" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">训练是实现更快的R-CNN网络的最后一步。RPN和更快的R-CNN都用下面的设置分别训练。用于训练的优化器是随机梯度下降(SGD ),动量为0.9，权重衰减为0.0005，学习率为0.001。这允许在RPN和R-CNN组件之间共享卷积层。在训练期间，VGG16的前10层正在被冷冻。以避免过拟合并减少计算时间。此外，提前停止的停止次数为10。最初的“水果”论文没有提到他们在训练数据时应用的任何其他方法。</p><h1 id="cb02" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">4.结论</h1><p id="49ff" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">最终结果包含更快的R-CNN的完整实现。我们试图让我们的代码尽可能简单，让其他用户更容易复制或继续使用代码。然而，有一个问题。由于使用我们自己的GPU有问题，只能训练RPN网络。当运行整个代码时，我们都有一个内存不足的问题，我们不能及时解决。因此，我们推荐任何想要实现我们更快的R-CNN的人使用google的云计算。</p><p id="df65" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">尽管代码不能完全在标准的膝上型电脑上运行。该代码应该让其他用户更深入地了解更快的R-CNN的原理。我们确实有一些提示给将来想自己实现更快的R-CNN或其他网络的用户。</p><ul class=""><li id="58e5" class="ly lz hh in b io ip is it iw ma ja mb je mc ji md me mf mg bi translated">在使用CPU和GPU时保持一致，混合使用可能会导致错误。此外，从GPU中删除不使用的张量和模型。</li><li id="44d3" class="ly lz hh in b io mh is mi iw mj ja mk je ml ji md me mf mg bi translated">请务必仔细检查您将要使用的数据集。</li><li id="f9b4" class="ly lz hh in b io mh is mi iw mj ja mk je ml ji md me mf mg bi translated">首先只在一个映像上实现您的网络，这将有助于您更容易地调试代码。</li></ul><p id="0ebd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们希望你能从我们的博客帖子和代码中学到一些东西。如果您有任何意见，请告诉我们，或者如果这篇文章有用，请鼓掌。谢谢大家！</p><h1 id="3363" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">5.参考</h1><p id="46ab" class="pw-post-body-paragraph il im hh in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">[1].任，孙，何，吉希克(2015)。更快的r-cnn:用区域建议网络实现实时目标检测。<em class="lw"> arXiv预印本arXiv:1506.01497 </em>。</p><p id="b338" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">[2].吉尔希克河(2015年)。快速r-cnn。在<em class="lw">IEEE计算机视觉国际会议论文集</em>(第1440-1448页)。</p><p id="f480" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">[3].巴尔戈蒂，s .，&amp;安德伍德，J. (2017年5月)。果园深度水果检测。在<em class="lw"> 2017 IEEE机器人与自动化国际会议(ICRA) </em>(第3626–3633页)。IEEE。</p><p id="1fec" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">实现的代码可以在库<a class="ae jj" href="https://github.com/TedDeVriesLentsch/DeepLearning_Group20" rel="noopener ugc nofollow" target="_blank"> DeepLearning_Group20 </a>中获得。</p></div></div>    
</body>
</html>