<html>
<head>
<title>Generating the Best Article Names using Neural Networks and Data Analytics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络和数据分析生成最佳文章名称</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/generating-the-best-article-names-using-neural-networks-and-data-analytics-194d406a8d9d?source=collection_archive---------4-----------------------#2021-08-30">https://medium.com/mlearning-ai/generating-the-best-article-names-using-neural-networks-and-data-analytics-194d406a8d9d?source=collection_archive---------4-----------------------#2021-08-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/739cd26fe74c3cc1652786904a384b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VizYnNYZ6ZxSZ2hI"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nick Fewings</a> on <a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8f21" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">文章名称很难决定。读者可能会也可能不会根据标题点击你的文章。然而，制作标题很难，所以作为一名数据科学家，我决定使用GPT2和NLP来自动完成这项任务</p><h1 id="2096" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">数据</h1><p id="fe3d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我制作了一个csv文件，其中包含了我在Medium.com网站上使用Parsehub收集的各种标签中最好的数据科学文章。csv文件包含文章标题、使用的标签、出版物、获得的掌声、回复数量等信息。这个数据集可以在Kaggle上获得，被称为<a class="ae it" href="https://www.kaggle.com/aristotle609/mediumsearchdataset" rel="noopener ugc nofollow" target="_blank">中型搜索数据集</a>。</p><h1 id="00ed" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">工作</h1><p id="4f71" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我的任务是制作一个文本生成器来生成连贯的文章标题。我将使用变形金刚库进行预处理和建模，然后使用PyTorch Lightning对模型进行微调。</p><h1 id="a73a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">安装变压器</h1><p id="247d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">要在您的环境中安装转换器，请转到您的环境并使用以下命令。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="0e2d" class="le jt hh la b fi lf lg l lh li">pip install transformers</span></pre><p id="ce6b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">它将在您的环境中安装库。如果您想避免这一步，请在Kaggle内核上运行您的笔记本，因为它已经在环境中预装了transformers库。</p><h1 id="5def" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">笔记本</h1><p id="350f" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">这款笔记本使用GPU在Kaggle上运行。你可以在<a class="ae it" href="https://www.kaggle.com/aristotle609/medium-titles-generator" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和<a class="ae it" href="https://github.com/Aristotle609/Medium-Title-Generator" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看完整的笔记本。我建议在Kaggle内核上运行这个笔记本，而不是在本地机器或collab上，因为Kaggle已经在环境中安装了大部分的依赖项。Pytorch Lightning将作为一个包装类来加速模型的构建。</p><h2 id="5fc3" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">属国</h2><p id="ce62" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">运行下面的单元格，以确保您的环境中安装了所有必需的软件包。如果您没有安装所有的软件包，它将抛出一个错误。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="90d1" class="le jt hh la b fi lf lg l lh li">from transformers import  GPT2LMHeadModel, GPT2Tokenizer,AdamW<br/>import pandas as pd<br/>from torch.utils.data import Dataset , DataLoader<br/>import pytorch_lightning as pl<br/>from sklearn.model_selection import train_test_split</span></pre><h2 id="fdad" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">数据</h2><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="7a36" class="le jt hh la b fi lf lg l lh li">df = pd.read_csv("../input/mediumsearchdataset/Train.csv")<br/>df</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/795fd7c11d3ee66ee13c64007c915a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCSobjVwg6sdPAAhduHSGQ.png"/></div></div></figure><h2 id="5353" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">下载GPT2</h2><p id="9979" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我将下载GPT2-large供公众使用。它的大小为3 GB，这就是为什么我建议使用像Kaggle这样的远程笔记本电脑。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="36ea" class="le jt hh la b fi lf lg l lh li">tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")<br/>gpt2 = GPT2LMHeadModel.from_pretrained("gpt2-large")</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/758d6ccf51202d3ffcdf69d0f056bb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gzJLzdEnELOh7BpekmxJQ.png"/></div></div></figure><h2 id="c780" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">测试GPT2模型(微调前)</h2><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="d434" class="le jt hh la b fi lf lg l lh li">tokenizer.pad_token = tokenizer.eos_token<br/>prompt = tokenizer.encode("machine learning", max_length = 30 , padding = "max_length" , truncation = True , return_tensors = "pt")<br/>output = gpt2.generate(prompt,do_sample = True, max_length = 100,top_k = 10, temperature = 0.8)<br/>tokenizer.decode(output[0]  , skip_special_tokens = True)</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/66d4e9c0c932308cdd1d09735213173f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gdcVTWLP6WlJsHabtWZPmg.png"/></div></div></figure><p id="0a54" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">正如我们所见，该模型确实在提示“机器学习”上生成了文本，但这与标题材料相去甚远。在接下来的部分中，我们将对模型进行微调，以生成更好的文本。</p><h2 id="977e" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">资料组</h2><p id="e787" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">数据集将创建标记化的标题并将其发送到数据集。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="6a54" class="le jt hh la b fi lf lg l lh li">class TitleDataset(Dataset):<br/>    def __init__(self,titles):<br/>        self.tokenizer = tokenizer<br/>        self.titles = titles<br/>    <br/>    def __len__(self):<br/>        return len(self.titles)<br/>    <br/>    def __getitem__(self,index):<br/>        title = self.titles[index]<br/>        title_token = tokenizer.encode(title , max_length = 30 , padding = "max_length" , truncation = True, return_tensors = "pt").reshape(-1)<br/>        return title_token</span><span id="51d4" class="le jt hh la b fi lz lg l lh li">#sanity check</span><span id="1eae" class="le jt hh la b fi lz lg l lh li">dset = TitleDataset(df["post_name"].values)<br/>title = next(iter(DataLoader(dset , batch_size = 1,shuffle = True)))<br/>display(title)</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ma"><img src="../Images/c9cb0d1a9a7fb4757c5bdc4ea981123c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0iCjmXdw-3ZpdnKLF53yw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Tokenized title</figcaption></figure><h2 id="9563" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">数据模块</h2><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="ff16" class="le jt hh la b fi lf lg l lh li">class <strong class="la hi">TitleDataModule</strong>(pl.LightningDataModule):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.train = TitleDataset(x_train["post_name"].values )<br/>        self.test = TitleDataset(x_test["post_name"].values )<br/>        self.val = TitleDataset(x_test["post_name"].values)<br/>    <br/>    def train_dataloader(self):<br/>        return DataLoader(self.train , batch_size = 1 , shuffle = True)<br/>    def test_dataloader(self):<br/>        return DataLoader(self.test , batch_size = 1 , shuffle = False)<br/>    def val_dataloader(self):<br/>        return DataLoader(self.val , batch_size = 1 , shuffle = False)</span></pre><h2 id="1665" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">模型</h2><p id="7f8d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">当标记化的文本传递给GPT2时，GPT 2返回输出logits和模型的损失。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="c80e" class="le jt hh la b fi lf lg l lh li">class TitleGenerator(pl.LightningModule):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.neural_net = gpt2_model<br/>        <br/>    def forward(self,x):<br/>        return self.neural_net(x , labels = x)<br/>    <br/>    def configure_optimizers(self):<br/>        return AdamW(self.parameters(), 1e-4)<br/>        <br/>    def training_step(self,batch,batch_idx):<br/>        x= batch<br/>        output = self(x)<br/>        return output.loss<br/>    <br/>    def test_step(self,batch,batch_idx):<br/>        x= batch<br/>        output = self(x)<br/>        return output.loss<br/>    <br/>    def validation_step(self,batch,batch_idx):<br/>        x= batch<br/>        output = self(x)<br/>        return output.loss</span></pre><h2 id="58eb" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">培养</h2><p id="7190" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">微调GPT2模型需要很长时间，如果可能的话，我建议使用GPU。Lightning允许我们在训练器中声明GPU，而它处理其余的。六个纪元大约需要30分钟。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="530c" class="le jt hh la b fi lf lg l lh li">from pytorch_lightning import Trainer<br/>model = TitleGenerator()<br/>module = TitleDataModule()<br/>trainer = Trainer(max_epochs = 6,gpus = 1)<br/>trainer.fit(model,module)</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mb"><img src="../Images/79817d16c55e8a8546e8240844a9afd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pqqwZs1QTDcl8gsDWfJuA.png"/></div></div></figure><h2 id="f668" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">测试和预测</h2><p id="03af" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">如果您计划将代码部署到产品中，我不建议这样做，因为这可能会导致错误。下面的代码是一个改变原始模型权重的快速而不实用的方法</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="058f" class="le jt hh la b fi lf lg l lh li">gpt2.state_dict = model.state_dict</span></pre><h2 id="52fb" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">生成标题</h2><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="03ab" class="le jt hh la b fi lf lg l lh li">raw_text = ["The" ,"machine Learning"  , "A" , "Data science" , "AI" , "A" , "The" , "Why" , "how"]<br/>for x in raw_text:<br/>    prompts = tokenizer.encode(x , return_tensors = "pt")<br/>    outputs = gpt2.generate(prompt,do_sample = True, max_length = 32,top_k = 10, temperature = 0.8)<br/>    display(tokenizer.decode(outputs[0] , skip_special_tokens = True))</span></pre><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/ca0de4dec274a6c667e6e83e2f43dda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUHMEcrudbDs7nkngtny7g.png"/></div></div></figure><h2 id="a8ce" class="le jt hh bd ju lj lk ll jy lm ln lo kc jf lp lq kg jj lr ls kk jn lt lu ko lv bi translated">尾注</h2><p id="a9e5" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">我会将该模型部署为API，但是该模型超过3 GB，将其托管在网站上在成本上确实没有意义。你也可以试着在微调后把模型上传到huggingface hub上。查看<a class="ae it" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> huggingface网站</a>获取更多模型和教程。</p></div></div>    
</body>
</html>