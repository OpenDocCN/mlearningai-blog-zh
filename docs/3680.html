<html>
<head>
<title>Building a Neural Network Zoo From Scratch: Feed Forward Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建神经网络动物园:前馈神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/building-a-neural-network-zoo-from-scratch-feed-forward-neural-networks-f754cc88eca2?source=collection_archive---------3-----------------------#2022-10-08">https://medium.com/mlearning-ai/building-a-neural-network-zoo-from-scratch-feed-forward-neural-networks-f754cc88eca2?source=collection_archive---------3-----------------------#2022-10-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/8380549fbcbdd1a4712c7c553e5aa0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*3QnrIcPV_MOutJP9J0jr_Q.png"/></div><figcaption class="il im et er es in io bd b be z dx">Visualization of a Feed Forward Neural Network from the <a class="ae ip" href="https://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">Asimov Institute</a>.</figcaption></figure><p id="6c92" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><a class="ae ip" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>是一大类神经网络，包括任何正向移动的网络(即大多数网络)。因此，出于本文的目的，我们将主要关注其中最简单的:多层感知器。</p><p id="db39" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果你还没有读过我的<a class="ae ip" rel="noopener" href="/mlearning-ai/building-a-neural-network-zoo-from-scratch-the-perceptron-335759f48089">上一篇关于单层感知器的文章</a>，我强烈建议你在继续之前先读一读。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><p id="d65a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">70年代被称为人工智能的“黑暗时代”是有原因的:自从弗兰克·罗森布拉特(Frank Rosenblatt)在1958年发表论文以来，神经网络几乎没有取得任何进展，神经网络的前景已经成为认知研究界的一个笑话，有些人甚至提供了神经网络永远无法工作的正式证明。然而，1985年，在最初的论文发表27年后，加州大学圣地亚哥分校的一个小型研究小组向<a class="ae ip" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf" rel="noopener ugc nofollow" target="_blank">提交了这篇</a>论文，并且永远地改变了人工智能的世界。</p><h2 id="c38f" class="jv jw hh bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">单层感知器与多层感知器</h2><p id="dd41" class="pw-post-body-paragraph iq ir hh is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn ha bi translated">说实话，“多层感知器”有点用词不当。这是因为这个研究小组提出的关键问题(单层感知器和多层感知器之间的真正区别)不是感知器的数量，而是它们之间的非线性。</p><p id="7a60" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">非线性就是它听起来的样子:一个图形不是直线的函数。现在有许多非线性激活函数，但研究小组最初提出的一个被称为sigmoid。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/0632b55b2026daa102f285221c9eedc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/1*ZBxE2B4ZWAzdmfjgFSEvWw.gif"/></div><figcaption class="il im et er es in io bd b be z dx">The formula for the sigmoid function.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es la"><img src="../Images/ca191788c82a604865b42916a3ad3eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*PTsKwTdiI0PT_s3s.png"/></div><figcaption class="il im et er es in io bd b be z dx"><a class="ae ip" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">Graph of the sigmoid function</a>.</figcaption></figure><p id="2e87" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果你还记得在我以前的文章中，我将单层感知器比作一条线的方程，并解释了当试图分离像XOR这样的函数时，这是如何出现问题的。多层感知器通过在层间添加类似sigmoid的非线性激活函数来扭曲“感知器线”来解决这个问题。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/29d16620cde9a2ae00ba8028edfb72a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n7ot-XjZOlgRPPaicDR1iw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">One possible seperation performed by a Multilayer Perceptron on the XOR graph.</figcaption></figure><p id="f03e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">与单层感知器不同，多层感知器今天仍然被用于像<a class="ae ip" href="https://en.wikipedia.org/wiki/Sentiment_analysis" rel="noopener ugc nofollow" target="_blank">情绪分析</a>这样的任务，天气预报甚至基本的图像识别，这是我们将在本教程中涉及的。这部分是由于所谓的<a class="ae ip" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>，该定理指出任何足够大的神经网络都可以逼近任何连续函数<em class="lg"> f(x) </em>。这意味着，由于这些非线性，给定足够的时间和足够的<em class="lg">精确的</em>数据，神经网络几乎可以学习<a class="ae ip" href="https://ai.stackexchange.com/questions/5539/which-functions-cant-neural-networks-learn-efficiently" rel="noopener ugc nofollow" target="_blank"><em class="lg"/></a><em class="lg">任何东西</em>。</p><h2 id="c5fc" class="jv jw hh bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">好吧，那它是怎么工作的？</h2><p id="4c00" class="pw-post-body-paragraph iq ir hh is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn ha bi translated">像感知器一样，多层感知器有一个向前传递和一个向后传递。这两个网络之间的前向传递保持相对不变，尽管由于多层而变得更长。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/1919d5e1c036ac0b41d9de6e16420798.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/1*ggbg_VxR1Z4DzHFoQNxTgQ.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Layer 1 function.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/111d6d247728e206b194adc17cf0687c.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/1*ljTzWXBZ7qCL84LeztKwgA.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Layer 2 function.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es li"><img src="../Images/2f0947c8985336e8d2c741ec93710e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SuWJKqIkCwOpkGGUqFArWg.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">Total network function.</figcaption></figure><p id="c922" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">由于网络的两层是相同的，除了<em class="lg"> w </em>和<em class="lg"> b </em>之外，我们可以想象网络被分成两个独立的“感知器功能”，如上图所示。</p><p id="fd01" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">由于这些附加的非线性，反向传递变得更加复杂。出于这个原因，我们也将把我们的计算图分成两部分。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lj"><img src="../Images/b61dae746badae17a151962815a6f109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FbH0o_RlTrZhTsl4qb2pPA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Computational graph of layer 2.</figcaption></figure><p id="e1fb" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">和以前一样，我们将每个变量放在图上，并在绿色连接上方的每一步向前传播当前函数。然后，从右边开始，我们用红色反向传播我们的误差:最后一个误差将总是<em class="lg"> e </em>并且每隔一个误差将是上游误差(右边的误差)乘以上游函数(右边的函数)相对于当前函数(连接上面的函数)的导数。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lk"><img src="../Images/61b1fea79c6062e962909190e80919e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*do-87Jkq44iiCmLWpbWxMA.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">Error of W2x+b2.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ll"><img src="../Images/6bae2c911ea5c2011d7978c37f19cdbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qe9uec9Q-8ZZda02MHWFsQ.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">Error of b2.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/20bd997d40a776e713da983655e14b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*cfZvWgNnqLc_-lFYFKp2zw.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">Error of W2x.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ln"><img src="../Images/904de2105ebaf40b41ce2b5d74aff227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*H_1IrdM_i7a9279kkBtMOg.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">Error W2.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/6360d21d3cc52c64ceac65fb1c6d6042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/1*6_cHJgvuyQVM6NEy-0llyg.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error x.</figcaption></figure><p id="5093" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">注意，由于这是网络的第二层，<em class="lg"> x </em>将真正是第一层的输出。这也意味着我们通过第一层反向传播的误差，即<em class="lg"> e </em>值，将是我们在第二层发现的<em class="lg"> x </em>的误差。接下来，为了找到第一层的误差，我们只需获得第二层的误差，并用我们的误差<em class="lg"> x. </em>替换<em class="lg"> e </em></p><p id="bca4" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这让我们跳过了wx + b、wx等等的中间步骤。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/8152fdd51a26be1a4e153ac00f9e0e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/1*NuD1fzJ9_fXlHg0c8ARj2g.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of b1.</figcaption></figure><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/b6720a724dafbcb13ba158fbef26dc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/1*vtNMhx_fNnmojoBYIciE_A.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of W1.</figcaption></figure><p id="aba3" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">毫无疑问，这一开始会令人困惑，但是就像任何事情一样，熟能生巧。如果你还在纠结，我会再次提醒你注意斯坦福大学的这个视频。</p><h2 id="caa9" class="jv jw hh bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">数学算够了，让我们编码吧！</h2><p id="c13a" class="pw-post-body-paragraph iq ir hh is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn ha bi translated">正如我在上一篇文章中提到的，为了清楚起见，我将只使用NumPy来实现这些网络的结构。然而，在本教程中，我还将使用Matplotlib来可视化网络输入。因此，继续导入NumPy和Matplotlib。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Import NumPy &amp; Matplotlib.</figcaption></figure><p id="f38a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这一次我们的代码增加了一个新的激活函数sigmoid。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Activation function.</figcaption></figure><p id="6a38" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">该函数有两个输入:<code class="du lt lu lv lw b">input</code>和<code class="du lt lu lv lw b">derivative</code>，默认设置为<code class="du lt lu lv lw b">False</code>。<code class="du lt lu lv lw b">input</code>将是我们实际的数学输入，而<code class="du lt lu lv lw b">derivative</code>将是一个布尔值，它告诉我们的函数是返回输入的sigmoid还是它的导数(<em class="lg"> σ(x) </em>或<em class="lg"> σ'(x) </em>)。如果你想知道sigmoid函数的导数是如何推导出来的，<a class="ae ip" href="https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/#:~:text=The%20derivative%20of%20the%20sigmoid%20function%20%CF%83(x)%20is%20the,1%E2%88%92%CF%83(x)." rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的起点，但是对于我们的目的来说，它是一个可以忽略的细节。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Multilayer Perceptron class.</figcaption></figure><p id="0fb8" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">接下来我们可以定义我们的多层感知器类，它有5个输入。<code class="du lt lu lv lw b">input_size</code>是我们输入的长度，<code class="du lt lu lv lw b">hidden_size</code>是我们希望我们的层有多大，<code class="du lt lu lv lw b">output_size</code>是我们输出的长度，<code class="du lt lu lv lw b">num_epochs</code>和<code class="du lt lu lv lw b">learning_rate</code>是我们的超参数，它们分别改变我们学习的时间长度和速度。在我们的初始化函数中，我们还创建了网络的层，其中<code class="du lt lu lv lw b">w1</code>和<code class="du lt lu lv lw b">b1</code>是第一层的权重和偏差，<code class="du lt lu lv lw b">w2</code>和<code class="du lt lu lv lw b">b2</code>是第二层的权重和偏差。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Forward propagation function.</figcaption></figure><p id="1f49" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如前所述，<code class="du lt lu lv lw b">forward()</code>函数非常简单:我们的<code class="du lt lu lv lw b">layer1_output</code>是<em class="lg"> W1x + b1，</em> <code class="du lt lu lv lw b">activation1_output</code>是那个的sigmoid，<code class="du lt lu lv lw b">layer2_output</code>是<em class="lg"> W2x + b2 </em>，其中<em class="lg"> x </em>是<code class="du lt lu lv lw b">activation1_output</code>，而<code class="du lt lu lv lw b">activation2_output</code>是<code class="du lt lu lv lw b">layer2_output</code>的sigmoid。注意，我们使用类变量来存储这些值，而不是从函数中返回它们。这使得我们的网络类更加简洁，因为我们将需要这些变量中的每一个来进行反向传播。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Backpropagation function.</figcaption></figure><p id="ef16" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在进行反向传播。我们的<code class="du lt lu lv lw b">backward()</code>函数取<code class="du lt lu lv lw b">error</code>和<code class="du lt lu lv lw b">input</code>，分别是网络和原始网络输入的最终误差。<code class="du lt lu lv lw b">error2</code>是<em class="lg"> b2 </em>的误差，是网络的误差乘以第二层输出的sigmoid导数。<code class="du lt lu lv lw b">dw2</code>为第二层权重的误差，等于<em class="lg"> b2 </em>的误差乘以第二层的输入。然后<code class="du lt lu lv lw b">dw1</code>和<code class="du lt lu lv lw b">error1</code>是第一层的权重和偏差的误差，如上所示计算。注意，所有额外的操作仅用于固定矩阵的大小，以执行必要的计算；<code class="du lt lu lv lw b">.T</code>转置矩阵，<code class="du lt lu lv lw b">.flatten()</code>将一个<em class="lg"> (1，n) </em>二维矩阵变为一个<em class="lg"> n </em>长度的数组。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Train &amp; test functions.</figcaption></figure><p id="fc1e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">多层感知器的<code class="du lt lu lv lw b">train()</code>功能与单层感知器相同，因此无需解释。两个感知器之间的<code class="du lt lu lv lw b">test()</code>功能也非常相似，迭代输入并向前传播以获得预测。不同的是，这次我们将使用Matplotlib来可视化我们的输入，并手动确定我们的网络是否准确。</p><figure class="kw kx ky kz fd ii"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="il im et er es in io bd b be z dx">Multilayer Perceptron initialization and utilization.</figcaption></figure><p id="eac7" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">最后初始化我们的网络。我们的<code class="du lt lu lv lw b">input_size</code>设置为30，因为我们的网络正在5x6图像上进行训练。这些图像可以在下面看到。</p><div class="kw kx ky kz fd ab cb"><figure class="lx ii ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/53c792d2750855453e749b02e2071bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*dlO5SEJYZPvehgZNoNeU3A.jpeg"/></div></figure><figure class="lx ii ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/b55aa43712f64262d29ba7454b1ca00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*hVN250vrStZoyei0037-vg.jpeg"/></div></figure><figure class="lx ii ly lz ma mb mc paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/5dbb49378281b02ed7b28fc269d493b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*OpT7vNk9K7xkWNpefbXD2A.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx md di me mf">Inputs for our network.</figcaption></figure></div><p id="5e98" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们的<code class="du lt lu lv lw b">hidden_size</code>被设置为5，(和往常一样，我会建议摆弄这些数字，看看它如何影响网络的训练，尽管一个好的经验法则是网络的隐藏大小在输入和输出大小之间)，<code class="du lt lu lv lw b">output_size</code>被设置为3，因为每个输入有三个可能的分类，<code class="du lt lu lv lw b">num_epochs</code>和<code class="du lt lu lv lw b">learning_rate</code>被设置为1000和0.1，再次是任意的。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><p id="a12a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这就把我们带到了本系列第二篇文章的结尾。我希望您觉得这很有用，或者至少很有趣。请随意与你的朋友和同事分享这篇文章，并关注我的下一篇文章。这篇文章的完整代码可以在<a class="ae ip" href="https://github.com/CallMeTwitch/Neural-Network-Zoo/blob/main/MultilayerPerceptron.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="883e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">非常感谢<a class="mg mh ge" href="https://medium.com/u/5732f22c71f9?source=post_page-----f754cc88eca2--------------------------------" rel="noopener" target="_blank">艾米丽·赫尔</a>的剪辑。</p><div class="mi mj ez fb mk ml"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mm ab dw"><div class="mn ab mo cl cj mp"><h2 class="bd hi fi z dy mq ea eb mr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ms l"><h3 class="bd b fi z dy mq ea eb mr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mt l"><p class="bd b fp z dy mq ea eb mr ed ef dx translated">medium.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz ij ml"/></div></div></a></div></div></div>    
</body>
</html>