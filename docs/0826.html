<html>
<head>
<title>Character-Based Neural Language Modeling using LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTM的基于字符的神经语言建模</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/character-based-neural-language-modelling-using-lstm-bfb136bebb3b?source=collection_archive---------0-----------------------#2021-07-29">https://medium.com/mlearning-ai/character-based-neural-language-modelling-using-lstm-bfb136bebb3b?source=collection_archive---------0-----------------------#2021-07-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/c14512adcd14f511680d3eddd253e6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TYSJ7T8SV9sLuTrSsZfmQQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://www.visor.ai/nlp-chatbots/" rel="noopener ugc nofollow" target="_blank">Visor.ai</a></figcaption></figure><p id="99d5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">神经语言建模是神经网络在语言建模中的应用。最初，使用前馈神经网络，但是长短期记忆网络或LSTM已经变得流行，因为它允许模型在比简单的神经网络长得多的输入序列上学习相关上下文。</p><p id="dd68" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">基于字符的语言模型根据序列中出现在它之前的特定字符来预测序列中的下一个字符。基于字符的语言模型有许多好处，因为它能够处理任何单词、标点符号和其他结构。然而，有时训练会比较慢，尤其是较大的模型。我们可以用拉迪亚德·吉卜林的《如果》这首诗来发展我们基于角色的语言模型。</p><p id="6969" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> IF —拉迪亚德·吉卜林</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es js"><img src="../Images/53ee3e61a5c00d10f30efae7b661afe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*up1c_bXQY3MX9KHhDEvuGQ.jpeg"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://www.amazon.com/Print-Rudyard-Kipling-Poster-Ivory/dp/B07Q3ZD7Z8" rel="noopener ugc nofollow" target="_blank">AmazonPosters</a></figcaption></figure><p id="8dc6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">数据准备</strong></p><p id="f76a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用load_doc()函数将文本加载到内存中。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5ff5" class="kc kd hh jy b fi ke kf l kg kh">def load_doc(filename):<br/>  file = open(filename,'r')<br/>  text = file.read() <br/>  file.close()<br/>  return text</span></pre><p id="1fb3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们加载包含这首诗的歌词的If_Rudyard_Kipling.txt文件并打印它。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="05b7" class="kc kd hh jy b fi ke kf l kg kh">raw_text = load_doc('/content/drive/MyDrive/If_Rudyard_Kipling.txt')<br/>print(raw_text)</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/ec51bc43f1cea693ca7984f7f30e1186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbfbWh6z8uTvOgHzbtuyPw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">IF by Rudyard Kipling</figcaption></figure><p id="9db3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">干净的文字</strong></p><p id="545a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用以下方式清理文本数据</p><ol class=""><li id="db27" class="kj kk hh iw b ix iy jb jc jf kl jj km jn kn jr ko kp kq kr bi translated">在空白上分割标记。</li><li id="41d3" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">去掉单词中的所有标点符号。</li><li id="b67e" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">删除所有不完全由字母字符组成的单词。</li><li id="bce7" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">把所有的单词都变成小写。</li></ol><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="98f2" class="kc kd hh jy b fi ke kf l kg kh">import string</span><span id="a2c3" class="kc kd hh jy b fi kx kf l kg kh">def clean_data(text):<br/>  tokens = text.split()<br/>  tokens = [t for t in tokens if t not in string.punctuation]<br/>  tokens = [t for t in tokens if t.isalpha()]<br/>  tokens = [t.lower() for t in tokens]<br/>  tokens = ' '.join(tokens)<br/>  return tokens</span></pre><p id="155a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">清理数据并打印出来。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="09d8" class="kc kd hh jy b fi ke kf l kg kh">raw_text = clean_data(raw_text)</span><span id="37a7" class="kc kd hh jy b fi kx kf l kg kh">print(raw_text)</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ky"><img src="../Images/a00f1de2886a7f5dc2a600c9de1834cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4eGf8g8UZ9lEeVXnHWy0Uw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Cleaned Text</figcaption></figure><p id="a721" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">创建序列</strong></p><p id="abf6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了训练模型，我们创建输入-输出序列，其中每个输入序列的长度为10个字符，一个输出字符使每个序列的长度为11个字符。在预测了第一个字符(第11个字符)之后，我们将它添加到输入序列中，模型使用该输入来预测下一个字符。我们为模型使用10个字符的任意长度。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="24aa" class="kc kd hh jy b fi ke kf l kg kh"># Each input sequence will be 10 characters long with one output character, making each seq 11 characters long</span><span id="82d1" class="kc kd hh jy b fi kx kf l kg kh">length = 10<br/>sequences = list()</span><span id="1087" class="kc kd hh jy b fi kx kf l kg kh">def create_seq(raw_text):<br/>   for i in range(length,len(raw_text)):<br/>     sequences.append(raw_text[i-length:i+1])<br/>   print('Total Sequences',len(sequences))</span><span id="8ea9" class="kc kd hh jy b fi kx kf l kg kh">create_seq(raw_text) # Total Sequences 1129</span><span id="e760" class="kc kd hh jy b fi kx kf l kg kh">print(sequences)</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kz"><img src="../Images/8745fc034b16310a7431efea14c4c4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPlV9Q1C7hhOBm-zTHRntw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Sequences</figcaption></figure><p id="0d62" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">保存序列</strong></p><p id="aecb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">可以保存处理过的数据，以便在以后的阶段使用它来开发我们的模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5cb9" class="kc kd hh jy b fi ke kf l kg kh">def save_doc(lines,filename):<br/>   text = '\n'.join(lines)<br/>   file = open(filename,'w')<br/>   file.write(text)<br/>   file.close()</span><span id="ab05" class="kc kd hh jy b fi kx kf l kg kh">output_file = 'char_seq.txt'<br/>save_doc(sequences,output_file)</span></pre><p id="eaf6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">编码序列</strong></p><p id="85e4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">每个独特的字符将被赋予一个整数值，因此每个字符序列将被编码为一个整数序列。我们可以创建一个映射，作为从字符值到整数值字典。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d9ae" class="kc kd hh jy b fi ke kf l kg kh">chars = sorted(list(set(raw_text)))<br/>mapping = dict((c,i) for i,c in enumerate(chars))</span><span id="1540" class="kc kd hh jy b fi kx kf l kg kh">print(mapping)</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/259403e41c1363d9446f2f8b5a366659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIrSU8YrrxqNhos61Hc0dw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Mapping</figcaption></figure><p id="7f80" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">接下来，我们处理每个字符序列，并使用字典映射查找每个字符的整数值。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3a67" class="kc kd hh jy b fi ke kf l kg kh"># Encoding each character according to our above mapping<br/>encoded_sequences = list()</span><span id="dbc2" class="kc kd hh jy b fi kx kf l kg kh">for line in lines:<br/>   encode_seq = [mapping[char] for char in line]<br/>   encoded_sequences.append(encode_seq)</span><span id="b6f5" class="kc kd hh jy b fi kx kf l kg kh">print(encoded_sequences[0])</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/bfe798ba119a624497f0f561f5815efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*Do94_AoTd6Ibs-IQOKTx6w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">First encoded sequence</figcaption></figure><p id="5e79" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">词汇的大小被用作我们模型的输入。我们可以用len函数来计算。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5201" class="kc kd hh jy b fi ke kf l kg kh">vocab_size = len(mapping)<br/>print(vocab_size) # 26</span></pre><p id="0c1f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">分离输入和输出</strong></p><p id="f034" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">由于现在序列是整数编码的，我们可以使用数组切片将它们分成输入和输出(10个字符作为输入，第11个字符作为输出)。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5c3f" class="kc kd hh jy b fi ke kf l kg kh"># Splitting into inputs and outputs</span><span id="5114" class="kc kd hh jy b fi kx kf l kg kh">import numpy as np</span><span id="b678" class="kc kd hh jy b fi kx kf l kg kh">encoded_sequences = np.array(encoded_sequences)<br/>X,y = encoded_sequences[:,:-1], encoded_sequences[:,-1]</span><span id="6315" class="kc kd hh jy b fi kx kf l kg kh">print(X[0]) #[10  7  1 25 16 21  1  4  2 15]<br/>print(y[0]) # 1<br/>print(X[1]) #[ 7  1 25 16 21  1  4  2 15  1]</span><span id="5f13" class="kc kd hh jy b fi kx kf l kg kh">print(X.shape) #(1129, 10)<br/>print(y.shape) #(1129,)</span></pre><p id="61fb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了给网络提供更精确的输入表示，我们需要对每个字符进行一次热编码，其中每个字符都变成一个向量，只要词汇表(在我们的例子中是26个元素)为特定字符标有1。字符上的概率分布可以由模型输出，并与实际的下一个字符的全0值和1值的情况进行比较。Keras to _ categorical()函数用于对输入和输出序列进行一次性编码。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0872" class="kc kd hh jy b fi ke kf l kg kh">from tensorflow.keras.utils import to_categorical</span><span id="d52c" class="kc kd hh jy b fi kx kf l kg kh">onehot_encoded_seq = [to_categorical(x,num_classes=vocab_size) for x in X]</span><span id="fea6" class="kc kd hh jy b fi kx kf l kg kh">X = np.array(onehot_encoded_seq)<br/>y = to_categorical(y,num_classes=vocab_size)</span><span id="9b61" class="kc kd hh jy b fi kx kf l kg kh">print(X.shape) # (1129, 10, 26)<br/>print(y.shape) # (1129, 26)</span></pre><p id="3361" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">训练模型</strong></p><p id="b653" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">该模型具有一个输入层，该输入层采用10个时间步长和26个特征的序列作为一个热编码输入表示。它有一个LSTM隐藏层，有50个存储单元，以及3个完全连接的输出层，每个输出层有50、100和150个神经元。输出层使用softmax激活函数，并输出词汇表中所有字符的概率分布的1个向量。由于这是一个多类分类问题，我们使用分类对数损失和Adam优化器来编译模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="f1e1" class="kc kd hh jy b fi ke kf l kg kh">def define_model(X):<br/>  model = Sequential()<br/>  model.add(LSTM(50,input_shape=(X.shape[1],X.shape[2])))<br/>  model.add(Dense(50,activation='relu'))<br/>  model.add(Dense(100,activation='relu'))<br/>  model.add(Dense(150,activation='relu'))<br/>  model.add(Dense(vocab_size,activation='softmax'))<br/>  model.compile(loss='categorical_crossentropy',<br/>optimizer='adam',metrics=['accuracy'])<br/>  model.summary()<br/>  return model</span></pre><p id="b619" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">训练我们的模型200个纪元。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d9a1" class="kc kd hh jy b fi ke kf l kg kh">model = define_model(X)</span><span id="db65" class="kc kd hh jy b fi kx kf l kg kh">model.fit(X,y,epochs=200,verbose=2)</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/0c34f6fbd7eaaf3e7bdcdaa262f3044f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPKxhp41TLYgclcfw9EYIQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Model Summary</figcaption></figure><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es ld"><img src="../Images/d0dd3a1c5f37d0e0be0a50712269891d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*_77gWQskWYbYLMuppwY00w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Training the model</figcaption></figure><p id="982b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">生成文本</strong></p><p id="e612" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将使用我们的模型来生成具有相同统计属性的新文本。</p><p id="407e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们的模型期望10个字符作为用于训练模型的经过处理的格式的输入。因此，我们执行上述所有步骤，将我们的训练文本转换为经过处理的文本。</p><ol class=""><li id="6f6b" class="kj kk hh iw b ix iy jb jc jf kl jj km jn kn jr ko kp kq kr bi translated">使用映射对序列进行整数编码。</li><li id="9036" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">使用Keras to _ categorical()函数对整数进行热编码。</li><li id="04c0" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">由于我们只有一个序列，它需要被重新整形为三维形状，因为LSTM期望所有输入都是三维的(样本(1)、时间步长(10)、特征(26))</li><li id="df61" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">接下来，我们使用predict_classes方法预测下一个字符，该方法将直接为概率最高的字符选择整数。</li><li id="f3a8" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">使用我们的映射来解码预测的整数，以查看字符。</li><li id="d474" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">预测的字符然后被附加到我们的输入序列。</li><li id="8f1b" class="kj kk hh iw b ix ks jb kt jf ku jj kv jn kw jr ko kp kq kr bi translated">由于输入序列现在有11个字符长，我们需要通过删除第一个字符来截断它。</li></ol><p id="fcab" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">函数generate_seq执行上述所有步骤。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="c363" class="kc kd hh jy b fi ke kf l kg kh"># generate a sequence of characters with a language model</span><span id="a030" class="kc kd hh jy b fi kx kf l kg kh">def generate_seq(model,mapping, seq_length, seed_text, n_chars):<br/>  in_txt = clean_data(seed_text)<br/>  #genarating n number of chars<br/>  for _ in range(n_chars):<br/>    #encode the text as integers<br/>    encoded_seq = [mapping[char] for char in in_txt]</span><span id="783a" class="kc kd hh jy b fi kx kf l kg kh">  #truncate sequences to a fixed legth<br/>  encoded_seq =  pad_sequences([encoded_seq],maxlen=seq_length,truncating='pre')</span><span id="2836" class="kc kd hh jy b fi kx kf l kg kh">  #one hot encoding<br/>  encoded_seq = to_categorical(encoded_seq,num_classes=len(mapping))</span><span id="30aa" class="kc kd hh jy b fi kx kf l kg kh">  #reshaping<br/>  encoded_seq =  encoded_seq.reshape(1,encoded_seq.shape[0],encoded_seq.shape[1])</span><span id="6849" class="kc kd hh jy b fi kx kf l kg kh">  yhat = model.predict_classes(encoded_seq)</span><span id="c366" class="kc kd hh jy b fi kx kf l kg kh">  #Integer to character</span><span id="8766" class="kc kd hh jy b fi kx kf l kg kh">  out_char = ''<br/>  for char,i in mapping.items():<br/>    if i == yhat:<br/>      out_char = char<br/>      break</span><span id="4e72" class="kc kd hh jy b fi kx kf l kg kh">  in_txt += out_char</span><span id="65cc" class="kc kd hh jy b fi kx kf l kg kh">return in_txt</span></pre><p id="6bb0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以使用我们的模型生成一些文本序列</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="72fa" class="kc kd hh jy b fi ke kf l kg kh">print(generate_seq(model, mapping, 10, 'two impost', 50))</span><span id="06db" class="kc kd hh jy b fi kx kf l kg kh">print(generate_seq(model, mapping, 10, 'If you can', 50))</span><span id="d3b2" class="kc kd hh jy b fi kx kf l kg kh">print(generate_seq(model, mapping, 10, 'start agai', 50))</span></pre><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es le"><img src="../Images/7fd2c0c1cb540d8b87e02454c865f078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ttMiOGp4dqx_n2LWHOy2g.png"/></div></div></figure><p id="811b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">模型输出有意义的单词，但是句子的整体上下文可能没有意义。可以使用更长的输入字符长度、单词嵌入或更深的网络来改进该模型。</p><p id="3476" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">资源</strong></p><p id="6d19" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://machinelearningmastery.com/time-series-forecasting/" rel="noopener ugc nofollow" target="_blank">机器学习掌握度</a></p><p id="8a22" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf" rel="noopener ugc nofollow" target="_blank">用Python进行深度学习</a></p><p id="aa2b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras文档</a></p></div></div>    
</body>
</html>