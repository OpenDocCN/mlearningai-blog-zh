<html>
<head>
<title>Support Vector Machine — A Line Is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——你只需要一条线</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/support-vector-machine-a-line-is-all-you-need-10a80e87cc64?source=collection_archive---------1-----------------------#2022-06-06">https://medium.com/mlearning-ai/support-vector-machine-a-line-is-all-you-need-10a80e87cc64?source=collection_archive---------1-----------------------#2022-06-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="2dea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文解释了支持向量机(SVM)算法如何在回归和分类问题中工作。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/3ebc699eef4782c631aef83e597fc002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ElfEPpysr4vsr5_a"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@conti_photos?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Fabrizio Conti</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a7a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，<strong class="ig hi">支持向量机</strong>是对分类问题直观的算法。我想说的是，当你必须区分两个阶级时，SVM背后的思想是很容易解释的。然而，这并不意味着这种算法只用于分类，相反，它可以很好地用于任何回归，但逻辑会有所改变！</p><p id="2b80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们深入了解支持向量机的概念…</p><h1 id="6478" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">线性可分情况下的SVM</h1><p id="5f6f" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">SVM是一种用于<strong class="ig hi">二元分类</strong>的算法(当然当你的问题是一个分类的时候)。因此，即使今天，有可能在多类问题中使用SVM，背后的数学本质上只划分两类。</p><p id="d131" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">理解SVM的最好和最简单的方法是使用两个类是线性可分的情况，这意味着您可以在两个类的数据点之间画一条线:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/499800fdd0019079deaeae735d13adb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*5AXpmk1V1wF2hzTwNG06Lg.png"/></div></figure><p id="f6f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以用多种方式分割计划，但SVM算法的逻辑是找到离两个类别最远的<strong class="ig hi">边界</strong>:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/c79b4db6f22146a0207d2f156140f338.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*qNltvYr8y1SI4-8ZSWX_AA.png"/></div></figure><p id="f36b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">连续的蓝色线是<strong class="ig hi"> <em class="kx">边界</em> </strong>，它是两个类的完美分界线。虚线是页边距，两个页边距之间的空间可以称为<strong class="ig hi"> <em class="kx">路径</em> </strong>(或<strong class="ig hi"> <em class="kx">超平面</em> </strong>)。两边距边框的距离相等。</p><p id="70ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如图所示，位于边缘的数据点被称为<strong class="ig hi">支持向量</strong>(这解释了支持向量机的名称)。</p><p id="cb26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的是，如果你在图上有更多的观察值(来自任何类别)，只要它不在<em class="kx">路径上，</em>边距就不会改变，这种情况将保持最优。</p><blockquote class="ky"><p id="5704" class="kz la hh bd lb lc ld le lf lg lh jb dx translated">SVM算法的目标是找到分隔两个类别的最大路径</p></blockquote><p id="b055" class="pw-post-body-paragraph ie if hh ig b ih lj ij ik il lk in io ip ll ir is it lm iv iw ix ln iz ja jb ha bi translated">让我们看看它在数学上是如何工作的…</p><p id="2cc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于所有数学符号，我们使用向量，因为它代表数据集的所有观察/特征。</p><p id="8810" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kx">边界</em>可以被声明为满足该等式的任何输入(<em class="kx"> x </em>):</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lo"><img src="../Images/b78fd3cab77fb57d79dafc7e61854a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/1*1e7dBi47HMrLvLc_Zd6EyQ.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">The decision function</figcaption></figure><p id="bd2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也是算法的<strong class="ig hi">决策函数</strong>，以<strong class="ig hi"> <em class="kx"> w </em> </strong>作为特征(其他项参数)的<strong class="ig hi">权重</strong>，<strong class="ig hi"> <em class="kx"> x </em> </strong>作为输入，<strong class="ig hi"> <em class="kx"> b </em> </strong>作为<strong class="ig hi">常数</strong> <strong class="ig hi">项</strong>。</p><p id="053c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">两个<strong class="ig hi">边距</strong>的定义如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lp"><img src="../Images/50be060f6353de8d086bf17d5c3768e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/1*hzwDqF7rYM4HLkUy0abeCA.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Margins</figcaption></figure><p id="75e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，算法的决策函数将根据函数值给出<strong class="ig hi">估计</strong> <strong class="ig hi">输出</strong>(<strong class="ig hi"><em class="kx"/></strong>):0或1:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lq"><img src="../Images/f04a697a2de138e6690ae8467e948d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/1*O1Z3a1JuMDgT2ACYEckokA.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Estimated output</figcaption></figure><p id="0da1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kx"> →线性SVM的目的是找到优化决策函数的权重和常数项。</em>T11】</strong></p><h2 id="1b71" class="lr ju hh bd jv ls lt lu jz lv lw lx kd ip ly lz kh it ma mb kl ix mc md kp me bi translated">如何找到<em class="li">优化决策函数的参数(w </em>和<em class="li"> b) </em>？</h2><p id="6b7d" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">让我们记住:用逻辑回归解决一个分类问题基本上就是最小化目标函数的损失函数。</p><p id="2cd0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于支持向量机，寻找最优参数(<em class="kx"> w </em>和<em class="kx"> b </em>)是通过<strong class="ig hi">最小化</strong><strong class="ig hi"/><strong class="ig hi">斜率</strong>的<strong class="ig hi">决策</strong> <strong class="ig hi">函数</strong>来解决的:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mf"><img src="../Images/c836fa1bcf2988890d8bba4c9e1ff3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:56/1*USt-ysiQ6nYwiOfqVki7xQ.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">The slope of the decision function</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mg"><img src="../Images/a6e337956724daaf1f72756b4a39c7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/1*Q6imYeRGY3uoaWNmC1D2QQ.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Minimizing the slope</figcaption></figure><p id="749c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们希望这两个类尽可能分开，并且在超平面内没有数据点，所以我们必须在最小化期间声明这个约束:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mh"><img src="../Images/25387dc2bfa0f572202a5d40c07b54ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/1*08z0fJlerEzJgl2p7fPG_g.gif"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Constraint during optimization</figcaption></figure><p id="c200" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它只是指定没有数据点会在超平面内(边缘之间)。</p><p id="d189" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们得到了最终最小化的<strong class="ig hi">目标:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mi"><img src="../Images/d5c9e311246964e3b701cc2f98491afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/1*x01Ql9UbQaP94aYjNlbbFg.gif"/></div></figure><ul class=""><li id="b3f5" class="mj mk hh ig b ih ii il im ip ml it mm ix mn jb mo mp mq mr bi translated">最小化决策函数的修正斜率</li><li id="e570" class="mj mk hh ig b ih ms il mt ip mu it mv ix mw jb mo mp mq mr bi translated">在边距的约束下(判定函数= 1且判定函数= -1，在1和-1之间没有任何东西)</li></ul><p id="ef1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kx"> → SVM最大化两个边距之间的距离(两个类的支持向量之间)。</em> </strong></p></div><div class="ab cl mx my go mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ha hb hc hd he"><p id="56a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们快速说明一下:</p><p id="3db9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策函数的斜率看起来像一个独特的观察:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/c9e491eed9777b48a3440fa0f9ca2d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*LxuMTJEjxScyTtLm2RPMRw.png"/></div></figure><p id="e05f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y轴是决策函数，X轴是观测值。函数的斜率告诉我们1和-1之间的距离(边距)等于2(<em class="kx">x1</em>1和-1之间)。</p><p id="8309" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们<strong class="ig hi">将</strong>的斜率除以2 ，结果如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/5c7511ecff4f2f31bc7fcade08ceaca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*WKdtDtlCRmW0qqonQP6X6A.png"/></div></figure><p id="d714" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们注意到1和-1之间的<strong class="ig hi">距离</strong>随着<strong class="ig hi">斜率</strong>的<strong class="ig hi">减小</strong>(<em class="kx">x1</em>在2和-2之间)。</p><blockquote class="ky"><p id="c624" class="kz la hh bd lb lc ld le lf lg lh jb dx translated">这就是SVM正在寻找的，最大化边距之间距离的权重(和常数项)(决策函数在1和-1之间)。</p></blockquote></div><div class="ab cl mx my go mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ha hb hc hd he"><h1 id="901f" class="jt ju hh bd jv jw ne jy jz ka nf kc kd ke ng kg kh ki nh kk kl km ni ko kp kq bi translated">硬利润与软利润</h1><p id="89bd" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">我没有说明之前的解释是关于<strong class="ig hi">硬边界线性SVM分类</strong>。然而，与<strong class="ig hi">软余量</strong>的区别很简单:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nj"><img src="../Images/b239acc15f5be214a4fecd4fb9b17126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*9SFbg3g-CtNgx1GZLRDdkQ.png"/></div></figure><p id="9f20" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">软边界分类</strong>基本上是允许一些数据点在超平面内(边界之外)，硬边界分类不是这种情况。</p><p id="7e9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数学上，我们需要在最小化方程中增加一个变量<strong class="ig hi"> ζ (Zêta) </strong>和一个超参数<strong class="ig hi"> C </strong>。</p><ul class=""><li id="16dd" class="mj mk hh ig b ih ii il im ip ml it mm ix mn jb mo mp mq mr bi translated"><strong class="ig hi"> ζ </strong>:测量第I次观察被允许如何冲击边缘。</li><li id="1aad" class="mj mk hh ig b ih ms il mt ip mu it mv ix mw jb mo mp mq mr bi translated"><strong class="ig hi"> C </strong>:定义增加作为<strong class="ig hi">第一目标</strong>的超平面(边距之间的距离)和减少边距侵占(<strong class="ig hi">第二目标</strong>)之间的权衡。</li></ul><h1 id="88e0" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">当数据不是线性可分的时候会发生什么？</h1><p id="5d90" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">SVM非线性分类使用多项式要素变换来创建要素之间更复杂的关系，并允许SVM像线性分类一样将两个类分开:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nj"><img src="../Images/e42fedd8ae404407df28e5d1ae11d2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*DXA6dDOawo1GJqSdBqvFBw.png"/></div></figure><p id="8c97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了避免添加多项式特性的缺点(那样做是可行的)，有一个技巧:内核技巧<strong class="ig hi"/>。</p><p id="7ba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kx">基本上与添加多项式特征相同，但实际上并没有这样做。</em></p><p id="c541" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你想了解更多这方面的技巧，我向你推荐德鲁·威利米蒂斯的这篇文章。</p><h1 id="d0d5" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">SVM回归</h1><p id="c865" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">对于回归问题，SVM算法的思想不是分离类，而是在路径(超平面)内拥有最多的数据点:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kw"><img src="../Images/910b38054fadf1a3f0444e55a2d44435.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*UY-keRpstq1_CNpmeYxBiQ.png"/></div></figure><p id="e841" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">软余量也可以通过定义超参数来实现:<strong class="ig hi"> ε </strong>。</p><h1 id="f87a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="6843" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">综上所述，<strong class="ig hi"> SVM </strong>是基于<strong class="ig hi">数据</strong> <strong class="ig hi">点</strong>不同 <strong class="ig hi">类</strong>之间的<strong class="ig hi">距离</strong>的算法(或者在回归问题的情况下只是数据点)。它只在观测值从两个类或<strong class="ig hi">线性</strong> <strong class="ig hi">可分</strong>时才起作用。然而，<strong class="ig hi">可以转换</strong><strong class="ig hi"/><strong class="ig hi">数据</strong>之间的<strong class="ig hi"/><strong class="ig hi">关系</strong> <strong class="ig hi">，并创建一种方法来分离它们。<br/>我们称之为支持向量，即两个类别之间距离最远的数据点。<strong class="ig hi"> <em class="kx">超平面/路径</em> </strong>是那些支持向量之间的空间。</strong></p><p id="c4fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，SVM算法的目标是<strong class="ig hi">通过区分这两个类来优化</strong>超平面<strong class="ig hi">和</strong>使其尽可能的精确。事实上，支持向量分离得越多，模型就应该越好。然而，有时候，算法必须允许一些<strong class="ig hi">侵占</strong>(超平面内的数据点)能够真正地创建类之间的差异。</p><h2 id="5ad9" class="lr ju hh bd jv ls lt lu jz lv lw lx kd ip ly lz kh it ma mb kl ix mc md kp me bi translated">参考来源:</h2><div class="nk nl ez fb nm nn"><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hi fi z dy ns ea eb nt ed ef hg bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">通过最近的一系列突破，深度学习推动了整个机器学习领域。现在，甚至…</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">www.oreilly.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jm nn"/></div></div></a></div><div class="nk nl ez fb nm nn"><a href="https://machinelearningmastery.com/support-vector-machines-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hi fi z dy ns ea eb nt ed ef hg bi translated">机器学习的支持向量机-机器学习掌握</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">支持向量机可能是最受欢迎和谈论最多的机器学习算法之一。他们是…</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob jm nn"/></div></div></a></div><div class="nk nl ez fb nm nn"><a href="https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hi fi z dy ns ea eb nt ed ef hg bi translated">机器学习中的支持向量回归</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">支持向量机(SVM)在机器学习中广泛应用于分类问题。我经常…</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="nw l"><div class="od l ny nz oa nw ob jm nn"/></div></div></a></div><div class="nk nl ez fb nm nn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hi fi z dy ns ea eb nt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">medium.com</p></div></div><div class="nw l"><div class="oe l ny nz oa nw ob jm nn"/></div></div></a></div></div></div>    
</body>
</html>