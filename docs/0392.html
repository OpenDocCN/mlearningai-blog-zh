<html>
<head>
<title>Transformer based Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于转换器的语言模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/recent-language-models-9fcf1b5f17f5?source=collection_archive---------6-----------------------#2021-04-03">https://medium.com/mlearning-ai/recent-language-models-9fcf1b5f17f5?source=collection_archive---------6-----------------------#2021-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="d6a2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">罗伯塔:</h1><ul class=""><li id="e867" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">RoBERTa是对BERT的再培训，它改进了培训方法，增加了1000%的数据和计算能力。</li><li id="278c" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">消除了NSP(下一句预测)损失，引入了动态屏蔽，使得屏蔽的标记在训练时段期间改变。还发现较大的批量训练规模在训练过程中更有用。优于BERT和XLNet</li></ul><h1 id="6424" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">蒸馏器:</h1><ul class=""><li id="de14" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">学习经过提炼(近似)的BERT版本，保留97%的性能，但仅使用一半数量的参数。具体来说，它没有令牌类型的嵌入，pooler，并且只保留了Google的BERT的一半层。</li></ul><h1 id="94dc" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">艾伯特:</h1><p id="07a1" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">与BERT的三个主要区别:</p><ul class=""><li id="cc25" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">它对于记号(E)和层(H)具有不同的嵌入大小。即H！=E. H set通常更大，因为它需要对基于上下文的令牌与上下文无关的令牌嵌入进行建模。但是这带来了巨大的Vocab和令牌矩阵(VxE)的问题。因此，我们分解这个矩阵(我们假设一个低秩稀疏矩阵就足够了)</li><li id="ac75" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">原始BERT中的下一句预测不是很有帮助，因为它将主题预测和连贯预测合并在一个任务中。这里我们使用句子顺序预测(SOP)，这意味着我们以正确的顺序给出两个句子作为正例，并交换作为反例。</li></ul><h1 id="33b2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">巴特:</h1><p id="fbec" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">巴特试图融合GPT和伯特世界的优点。</p><ul class=""><li id="0966" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">BERT是双向的，因此不能用于生成</li><li id="e2eb" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">GPT是单向的，所以它不能学习双向互动。</li><li id="6f3b" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">它使用格鲁而不是RELU</li><li id="24d1" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">它确实引起了对编码器最后一个隐藏层的关注</li><li id="b61b" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">BERT在字预测之前使用额外的FF层，而BART没有</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/914e5ebf90de145a7a27ee0eeed8b22f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NIc2NzYlfidhygJW"/></div></div></figure><ul class=""><li id="c7bd" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">有不同的方法来引入噪声并迫使模型从中学习:</li></ul><p id="7864" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">令牌屏蔽:(就像BERT一样)</p><p id="cebc" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">令牌删除:删除令牌，但是应该由模型来决定删除哪些令牌。</p><p id="880c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">文本填充:删除具有泊松分布的记号范围</p><p id="a766" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">句子排列。</p><p id="13df" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">文档旋转</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es li"><img src="../Images/126271b32bce4c870f9979ff5dd9840b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/0*gKSBVrA3rsYVBGkt"/></div></div></figure><h1 id="412c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">伊莱克特拉:</h1><ul class=""><li id="4054" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">在BERT中，我们用掩码替换一些令牌，而不是屏蔽输入，我们的方法通过用从小型发电机网络中采样的似是而非的替代物替换一些令牌来破坏输入。然后，不是训练预测被破坏的记号的原始身份的模型，而是训练预测被破坏的输入中的每个记号是否被生成器样本替换的判别模型。是一个<strong class="je hi">替换的令牌检测任务</strong>。</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lj"><img src="../Images/0d783846137f5eae5acc65c9b1b97b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wxyUSOvHVqKgHfrb"/></div></div></figure><ul class=""><li id="4a38" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">生成器(是一个MLM)然后学习预测被屏蔽的令牌的原始身份。鉴别器被训练来区分数据中的记号和已经被生成器样本替换的记号。</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lk"><img src="../Images/485726812327f200cde4e806b988b888.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/0*-gTvItU02qn6_kd4"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ll"><img src="../Images/22a8ff609500bc493e3cd3caff19474f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JRIKbvoj74OjnWHe"/></div></div></figure><p id="e454" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">我们将总损失降至最低:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lm"><img src="../Images/5a81003162c69e424a0f8e119c54dc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*XhijUPb0VRRiXAlL"/></div></figure><p id="2d8c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">我们推测，拥有太强的生成器可能会给鉴别器带来太大的挑战，阻止它有效地学习。特别是，鉴别器可能必须使用它的许多参数来模拟生成器，而不是实际的数据分布。</p><h1 id="a736" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">稀疏变压器:</h1><h2 id="d132" class="ln if hh bd ig lo lp lq ik lr ls lt io jj lu lv is jl lw lx iw jn ly lz ja ma bi translated">自适应量程变压器</h2><p id="4629" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">变压器的每个注意力头部共享相同的注意力跨度s。这假设每个头部需要相同的跨度来形成其表示。如下图所示，这个假设在字符级语言建模的上下文中不成立:一些Head(如Head A)关注最近的历史，而其他Head(如Head B)从整个可用的上下文中获取信息</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mb"><img src="../Images/7147a2c8f27893ecd623e642b9387cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/0*gqmzXHDSnfU79JSe"/></div></figure><p id="afa4" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">对于每个头部，我们添加一个掩蔽函数来控制注意力的范围。掩蔽函数是非递增函数，它将距离映射到[0，1]中的值。我们采用由[0，S]中的实数值z参数化的以下软屏蔽函数mz:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mc"><img src="../Images/2b6abc6e3f49d58364c8f273be3a98fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/0*MzJx8hR1Ej3qhXoD"/></div></figure><p id="e4dd" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">其中R是控制其柔软度的超参数。然后在掩蔽的跨度上计算注意力权重，即</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es md"><img src="../Images/c1bb9c038ec07561dd7e76112d5fbabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*_MDHRfs2oz-zsn1d"/></div></figure><p id="6b42" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">我们在损失函数中增加了一个L1罚函数，对模型中每个注意头I的参数zi进行惩罚</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es me"><img src="../Images/f2a4b8be3309e2dd8dcb162c64fd8da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*YhrvMGCwoIxFWZLS"/></div></figure><h2 id="faa6" class="ln if hh bd ig lo lp lq ik lr ls lt io jj lu lv is jl lw lx iw jn ly lz ja ma bi translated">变压器-XL</h2><p id="cad6" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">Transformer-XL没有试图降低密集注意力操作的成本，而是选择从RNNs获得灵感，并在transformers的自我注意力机制之外引入了一种递归机制。他们的工作引入了两个新概念——一个将先前“片段”的隐藏状态作为输入输入到当前片段层的组件，以及一个促进这一策略的相对位置编码方案。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mf"><img src="../Images/54ae2a270e75e1939fda003b79b7d07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bZLs3f1dbWzEXQuF"/></div></div></figure><p id="4b20" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">Transformer-XL通过强制串行处理数据段克服了这一限制。在第一段之后，后续令牌中的令牌将总是具有512个令牌的立即上下文大小，因为先前的段激活作为上下文被传递给后续段的注意操作。这意味着来自N个上下文大小* L层之外的信息可以被传播到给定的令牌。假设上下文大小为640，模型有16层，Transformer-XL理论上可以合并来自多达10，240个令牌的信号。</p><h2 id="7f05" class="ln if hh bd ig lo lp lq ik lr ls lt io jj lu lv is jl lw lx iw jn ly lz ja ma bi translated">重整器:</h2><ul class=""><li id="dd43" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">对于长序列来说，大转换器的成本高得惊人，我们用一种使用位置敏感散列法的转换器来代替点积注意力，将其复杂度从O(L2)变为O(L log L)，其中L是序列的长度。</li><li id="3e1a" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">我们使用可逆残差层代替标准残差，这允许在训练过程中仅存储一次激活而不是N次，其中N是层数。</li></ul><h2 id="4d28" class="ln if hh bd ig lo lp lq ik lr ls lt io jj lu lv is jl lw lx iw jn ly lz ja ma bi translated">长成形器</h2><ul class=""><li id="c452" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">基于变换器的模型不能处理长序列，因为它们的自注意操作与序列长度成平方比例。</li></ul><p id="c0da" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">它提出了一种新的注意机制:</p><ul class=""><li id="c1d9" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated"><strong class="je hi">滑动窗口</strong>:考虑到局部注意力的重要性，新的注意力在每个标记周围采用固定大小的窗口注意力。使用这种窗口式注意力的多个堆叠层会产生一个大的感受野，其中顶层可以访问所有的输入位置。</li><li id="fa3c" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated"><strong class="je hi">扩大滑动窗口</strong>:为了在不增加计算量的情况下进一步增加感受野，可以“扩大”滑动窗口。</li><li id="a51f" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated"><strong class="je hi">全球关注</strong>:相应地，我们在几个预选的输入位置上增加了“全球关注”。重要的是，我们使这种注意操作对称:也就是说，一个具有全局注意的记号关注序列中的所有记号，序列中的所有记号也关注它。例如，对于分类，全局注意力用于[CLS]标记，而在问答中，全局注意力提供给所有问题标记。</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mg"><img src="../Images/55a71d3727da3f78a5a5603f0ce36862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qy3Ph5s-sYQUfTdk"/></div></div></figure><ul class=""><li id="8cdf" class="jc jd hh je b jf ko jh kp jj kq jl kr jn ks jp jq jr js jt bi translated">我们对较低的层使用小的窗口尺寸，并且随着我们移动到较高的层而增加窗口尺寸。这允许顶层学习整个序列的更高层表示，同时让较低层捕捉本地信息。</li></ul><h1 id="561a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">T5:</h1><ul class=""><li id="71d6" class="jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">它通过引入一个统一的框架，将每一个语言问题转换成文本到文本的格式，探索了自然语言处理的迁移学习技术的前景。</li><li id="d96e" class="jc jd hh je b jf ju jh jv jj jw jl jx jn jy jp jq jr js jt bi translated">我们考虑的每一项任务——包括翻译、问答和分类——都是将模型文本作为输入，并训练它生成一些目标文本。这允许我们使用相同的模型、损失函数、超参数等。在我们不同的任务中。它还为我们的实证调查中包含的方法提供了一个标准的测试平台。“T5”指的是我们的模型，我们称之为“文本到文本转换转换器”</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mh"><img src="../Images/fb7f8d16e378ff96d45ea641d5556f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iazY-4vitwiMD5ot"/></div></div></figure><h2 id="dfd3" class="ln if hh bd ig lo lp lq ik lr ls lt io jj lu lv is jl lw lx iw jn ly lz ja ma bi translated">林former</h2><p id="9880" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">实验表明，注意力矩阵(P)是一个低秩矩阵，因此它基本上可以用低得多的维数来很好地近似。换句话说，仅用128个奇异值，我们就可以重建95%的原始注意力矩阵。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mi"><img src="../Images/401477bcf430de113923640ed3478b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*26ONKW5mXb-ZZpYh"/></div></div></figure><p id="42c2" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">如果我们可以使用奇异值分解，那么我们可以使用P_low</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mj"><img src="../Images/6965fc6adb4f2f2fb3819c18173def7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MsfzI7IqsJBJZHfJ"/></div></div></figure><p id="449d" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">但这在实践中是不可能的，因为它需要更多的计算。</p><p id="6b2d" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">诀窍是分别使用线性变换E_I和F_i将K和V从nxd向下投影到kxd大小。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mk"><img src="../Images/dfd3e11b4b29ac9fbef72f83df62f6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/0*h0_uB-eBQesraCNw"/></div></figure><p id="c5db" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">现在，计算成本是线性的，因为我们用固定的线性变换去掉了一个n。可以证明(在论文中)这是一个很好的近似，可以精确到我们想要的程度。</p><p id="b424" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">国王的顺序是</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ml"><img src="../Images/110e09407cb24c832158666124a76907.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/0*xx-8Qvm-Z_OIuzno"/></div></figure><p id="bf1c" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">随着n的增加，第一项将是最小值，因此k是d*log(d)。因为d是常数O(nk)=O(n * d*log(d))=O(n)</p><h1 id="2eed" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">有效注意:具有线性复杂性的注意</h1><p id="b740" class="pw-post-body-paragraph jz ka hh je b jf jg kb kc jh ji kd ke jj kf kg kh jl ki kj kk jn kl km kn jp ha bi translated">如果没有softmax函数，那么只需改变乘法的顺序:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mm"><img src="../Images/3229474e359bbd4bf0cdec368548bd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/0*lfu3flZCLSlUW5ee"/></div></figure><p id="7f59" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">我们可以从O(n*n*d)到O(d*d*n ),因为q，k和v是nxd，所以QK^T是n*d*n</p><p id="cf18" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">但是X = K^T V是d*n*d乘法运算，QX是n*d*d</p><p id="59f9" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">我们不能删除softmax，但我们可以忘记它，从其他东西开始:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mn"><img src="../Images/a47727887b1b320b37cd521c39c85dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*aTLYXyVdZrF6UAhB"/></div></figure><p id="63d3" class="pw-post-body-paragraph jz ka hh je b jf ko kb kc jh kp kd ke jj lf kg kh jl lg kj kk jn lh km kn jp ha bi translated">在这个新公式中，我们只缩放Q和K！。在下图中，我们可以看到不同之处。事实证明，这种方法是可行的，即使它没有正当理由。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mo"><img src="../Images/0b6690a99918c7e7dcdab87a08684639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x83mF9mMBIFe4g8t"/></div></div></figure></div></div>    
</body>
</html>