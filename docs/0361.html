<html>
<head>
<title>Weekly review of Reinforcement Learning papers #2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习论文的每周回顾#2</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/weekly-review-of-reinforcement-learning-papers-2-649e96b00c66?source=collection_archive---------1-----------------------#2021-03-29">https://medium.com/mlearning-ai/weekly-review-of-reinforcement-learning-papers-2-649e96b00c66?source=collection_archive---------1-----------------------#2021-03-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="d60b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">每周一，我都会发表我研究领域的4篇论文。大家来讨论一下吧！</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/098ccf672027ab44fa1cf7c02019ade6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUAkHMXAT4j-ORFbPGg7Gw.jpeg"/></div></div></figure><p id="1099" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">[ <a class="ae ke" rel="noopener" href="/mlearning-ai/weekly-review-of-reinforcement-learning-papers-1-b9de230a4158?source=friends_link&amp;sk=78551cd3d1f21f54d6b5ed944f4aa087"> ←上一次回顾</a> ][ <a class="ae ke" href="https://qgallouedec.medium.com/weekly-review-of-reinforcement-learning-papers-3-32f03633066e?source=friends_link&amp;sk=c53ec970bac2d20ac7c0853391b83e12" rel="noopener">下一次回顾→ </a></p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="a5e0" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated">论文1:用例子代替奖励:通过递归分类的基于例子的政策搜索</h1><p id="cad6" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">艾森巴赫，b .，莱文，s .，和萨拉胡特季诺夫，R. (2021)。<a class="ae ke" href="https://arxiv.org/abs/2103.12656" rel="noopener ugc nofollow" target="_blank">用实例代替奖励:通过递归分类进行基于实例的策略搜索</a>。<em class="lj"> arXiv预印本arXiv:2103.12656 </em>。</p><p id="1b30" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi lk translated">强化学习的核心是奖励函数:代理做得有多好。在某些情况下，这个奖励函数很容易描述:在电子游戏中:让我们看看分数。在其他情况下，不容易给出一个奖励函数。让我们以文章为例:关闭一个抽屉。如果观察空间是场景(机器人和抽屉)的图像，那么定义任务是否成功就非常复杂:我们需要一种算法来估计抽屉的位置，考虑到家具的遮挡以及一系列条件等等…</p><p id="6c68" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">尽管如此！很容易产生关闭抽屉的图像。如果任务成功，这些将是观察的例子。这就是这篇文章背后的想法。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lt"><img src="../Images/8dca618ffbc0810300eb0c0752b82362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*57n0KCYzJSXJQVMpzr7yug.gif"/></div><figcaption class="lu lv et er es lw lx bd b be z dx"><em class="ly">Success Examples of closed drawer</em></figcaption></figure><p id="3c4d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">不要搞错，这不是模仿学习:动作的中间状态并没有给出。基本上，基于示例的学习接近目标条件的RL:最大化达到给定状态的概率。</p><p id="0083" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">该算法不从成功的例子中学习奖励函数。相反，<strong class="jk hi">它基于递归学习，直接学习预测任务未来是否成功</strong>。在每次迭代中，训练分类器来预测成功状态示例的<em class="lj"> y=1 </em>，以及<em class="lj"> y=γ w/(1+γ w) </em>，其中<em class="lj"> w </em>是分类器在下一次交互中的预测(因此是递归项)。然后用相互作用收集新的轨迹。诸如此类。他们称他们的算法为<strong class="jk hi">递归分类的例子(RCE)。</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lz"><img src="../Images/9d438c77d6fa8e3db3a959f42c0bb13b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*n438DkgZdeFQCJ5bqCpPSw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx">Compared with prior methods, the RCE approach solves the task of hammering a nail into a board more reliably that prior approaches based on imitation learning [<a class="ae ke" href="https://arxiv.org/abs/1905.11108" rel="noopener ugc nofollow" target="_blank">SQIL</a>, <a class="ae ke" href="https://arxiv.org/abs/1809.02925" rel="noopener ugc nofollow" target="_blank">DAC</a>] and those that learn an explicit reward function [<a class="ae ke" href="https://arxiv.org/abs/1805.11686" rel="noopener ugc nofollow" target="_blank">VICE</a>, <a class="ae ke" href="https://arxiv.org/abs/2011.13885" rel="noopener ugc nofollow" target="_blank">ORIL</a>, <a class="ae ke" href="https://arxiv.org/abs/1911.00459" rel="noopener ugc nofollow" target="_blank">PURL</a>]. <a class="ae ke" href="https://ai.googleblog.com/2021/03/recursive-classification-replacing.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9a3f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">结果似乎相当不错。我想知道这样的结果是否会在所有经典的RL基准上得到。这是一种试图简化强化学习范式的方法。我很喜欢的就是这种刊物。</p><h1 id="8390" class="km kn hh bd ko kp ma kr ks kt mb kv kw in mc io ky iq md ir la it me iu lc ld bi translated">论文2:学习垄断游戏:一种混合的无模型深度强化学习和模仿学习方法</h1><p id="8f03" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">Haliem，m .、Bonjour，t .、Alsalem，a .、Thomas，s .、Li，h .、Aggarwal，v .…&amp; Kejriwal，M. (2021)。<a class="ae ke" href="https://arxiv.org/abs/2103.12656" rel="noopener ugc nofollow" target="_blank">学习垄断玩法:一种混合的无模型深度强化学习和模仿学习方法</a>。<em class="lj"> arXiv预印本arXiv:2103.00683 </em>。</p><p id="684a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di">这本出版物没有重大突破，但我确实想谈谈它。作者在一个大富翁游戏上训练了一个代理人！这个游戏我玩过很多，有很多策略可以考虑。当然有随机性，因为你必须掷骰子或使用幸运牌。这是一个我没有想到的强化学习环境！</span></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mf"><img src="../Images/d932007bf1a218122d618df16f73a5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wngMa6o7VSFCR2JLwf-Bow.jpeg"/></div></div></figure><p id="3fe1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了做到这一点，他们从模仿学习开始，使用一个代理，它的选择是预先设定好的(硬编码策略)。然后，代理继续使用带有经验回放的DQN算法进行训练。为了评估他们的代理人，他们提出了4个代理人，他们的水平在增加，他们的策略是硬编码的(称为P1，…，P4)。为了评估，他们组织了锦标赛(50场比赛，4名球员)，让他们的代理人与代理人P1对抗，…，第4页。不管对手的构成如何，他们的经纪人至少赢得了75%的锦标赛。不知道大富翁会不会成为RL基准测试的经典环境？</p><h1 id="e801" class="km kn hh bd ko kp ma kr ks kt mb kv kw in mc io ky iq md ir la it me iu lc ld bi translated">论文3:样本有效强化学习表征学习与好奇心对比正向动力学模型</h1><p id="bcfe" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">阮泰荣、吕泰明、吴泰荣、刘忠德(2021)。<a class="ae ke" href="https://arxiv.org/abs/2103.08255" rel="noopener ugc nofollow" target="_blank">利用好奇号对比正向动力学模型进行样本高效强化学习表征学习</a>。<em class="lj"> arXiv预印本arXiv:2103.08255 </em>。</p><p id="e31b" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi lk translated">如何有效地从原始图像中学习？这是强化学习中的一个重要问题:通常，大多数像素都没有什么重要性。就好像重要的信息隐藏在一个非常高维的空间里。</p><p id="ecd6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了解决这个问题，作者提出了一种算法，他们称之为<em class="lj">好奇心对比正向动态模型</em> (CCFDM)。该算法的三个主要方面是对比学习、<em class="lj">正向动态模型</em> (FDM)和好奇心模块。让我用两个词来解释一下。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mg"><img src="../Images/d8fa1d50b07bead413cf244c40ac6226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLjQNgG6aepmDiux38vjPQ.png"/></div></div></figure><p id="a955" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">对比学习方面借鉴了无监督学习。<a class="ae ke" href="https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607" rel="noopener" target="_blank">这里</a>是一篇很好的文章，会帮助你理解它是什么。这一切都始于重放缓冲区中的转换采样。简单来说，当前观察将是查询，下一个观察将是关键。它们都被CNN增强和编码(它给出了<em class="lj"> q </em>和<em class="lj">k’</em>)。这就是FDM的用武之地。观察特征(<em class="lj"> q </em>)和动作特征(<em class="lj"> a_e </em>)被输入到FDM中，并且输出在概念上将是下一个状态的预测(<em class="lj">q’</em>)。这个<em class="lj">q’</em>是查询，然后通过对比无监督学习来处理关键字。</p><p id="102f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">他们还增加了“好奇号”模块。我不太了解它是如何工作的。最主要的一点是，它增加了对探索新州的奖励。这是一个经常看到的想法，也是相当有效的。至此，强化学习的一切准备就绪。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mh"><img src="../Images/44a9a884d74f94a5df76772649a14eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiDY_FNQ3jZqPLYBcpydpA.png"/></div></div></figure><p id="720d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">结果相当不错。它们接近于他们所谓的学习上限(用状态上的SAC获得的上限)。与DrQ相比，将对比学习和DRL方法联系起来很有趣，即使我对算法的复杂性感到遗憾。</p><h1 id="0266" class="km kn hh bd ko kp ma kr ks kt mb kv kw in mc io ky iq md ir la it me iu lc ld bi translated">论文4:深度强化学习中的可解释性</h1><p id="11a5" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">赫伊莱，a .，库图伊斯，f .，&amp;迪亚斯-罗德里格斯，N. (2021)。<a class="ae ke" href="https://arxiv.org/abs/2103.08255" rel="noopener ugc nofollow" target="_blank">深度强化学习中的可解释性</a>。<em class="lj">基于知识的系统</em>，<em class="lj"> 214 </em>，106685。</p><p id="af85" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi lk translated">这就是人工智能的魔力和陷阱，模型的可解释性。作为一名优秀的科学家，我们不能只说“<em class="lj">有效</em>”。我们必须能够解释学习机制。这是一个重复出现的问题，在分类问题中被广泛研究，但在强化学习中却很少被研究。本文对旨在解释强化学习的论文进行了综述。他们将这些论文分为两类:透明算法和事后解释能力。这是一篇17页的非常密集的论文。由于每篇讨论的论文都有自己处理问题的方式，所以不可能对本文中的所有材料进行忠实的综合。因此，我将满足于一个单一的，可视化的例子。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mi"><img src="../Images/35ca20de3ea6149f03e204b163a53f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faabeD5cZmO0bPwLFirbkw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">Figure from the article : Comparison of Jacobian saliency (left) first introduced by Simonyan et al. to the authors’ perturbation-based approach (right) in an actor-critic model. Red indicates saliency for the critic; blue is saliency for the actor. Reproduced with permission of Sam Greydanus.</figcaption></figure><p id="8ebc" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一种基于扰动计算显著性的方法:对图像应用扰动，这将移除信息。这种扰动给扰动区域增加了空间不确定性。通过测量这种扰动对主体的影响，我们能够区分主体选择的决定区域。结果如右图所示。结果看起来很自然:球和球拍周围的区域具有较高的值。</p><p id="77bc" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这是本文介绍的众多方法之一。我真的邀请你去看看这个出版物的细节。</p><h1 id="d105" class="km kn hh bd ko kp ma kr ks kt mb kv kw in mc io ky iq md ir la it me iu lc ld bi translated">奖励论文:第一个M87事件视界望远镜的结果。七。环的极化</h1><p id="7e2d" class="pw-post-body-paragraph ji jj hh jk b jl le ii jn jo lf il jq jr lg jt ju jv lh jx jy jz li kb kc kd ha bi translated">事件视界望远镜合作和人工智能。(2021).<a class="ae ke" href="https://iopscience.iop.org/article/10.3847/2041-8213/abe71d" rel="noopener ugc nofollow" target="_blank">第一个M87事件视界望远镜的结果。七。环的极化</a>。<em class="lj">《天体物理学杂志快报</em>，910:L12</p><p id="4691" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di">你还记得事件视界望远镜在2019年4月10日拍摄的历史性图像吗？黑洞的第一张图片，叫做M87！这个黑洞位于5300万光年之外。为了实现这样一个非凡的壮举，不是一个，而是8个射电天文台，分布在4个大洲。这项合作被称为<em class="lj">事件视界望远镜</em>。通过结合他们的观察，有可能获得一幅图像，清晰地显示粒子和尘埃围绕所谓的事件视界运行:黑洞的引力如此之强，以至于任何东西，甚至是光或物质，都无法逃脱。</span></p><p id="bef8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">自从这个神话般的突破，研究人员没有闲着。上周，他们公布了这张照片。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mj"><img src="../Images/a4c88f0d20859a312d4b7519b8c82652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARriO5kQErNO3D3TrY8DJw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">The image of M87’s supermassive black hole, depicting the lines of polarized light surrounding the black hole’s event horizon. <a class="ae ke" href="https://www.technologyreview.com/2021/03/24/1021170/m87-supermassive-black-hole-polarized-light-magnetic-field/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0d6c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">通过检查偏振光的运动，天文学家注意到圆盘的一个区域显示出有趣的图案。这种偏振光实际上描述了它所来自的空间的偏振。通过使这些偏振出现在原始图像上，我们看到了非常美丽的场线。除了美观之外，这将有助于天文学家更好地理解超大质量黑洞的形成机制。</p><p id="0fa1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一个让你晕头转向的小备注:拍摄这张图像的光子在恐龙灭绝后不久就离开了黑洞区域。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><p id="afe8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我很高兴向你们展示我本周的阅读材料。请随时向我发送您的反馈。<br/>要阅读我周日晚上的评论，请访问我的博客:<a class="ae ke" href="https://qgallouedec.github.io" rel="noopener ugc nofollow" target="_blank">https://qgallouedec . github . io</a></p></div></div>    
</body>
</html>