<html>
<head>
<title>Detecting Heart Failure using Machine Learning (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用机器学习检测心力衰竭(下)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/detecting-heart-failure-using-machine-learning-part-2-8cca4cac266c?source=collection_archive---------4-----------------------#2021-03-29">https://medium.com/mlearning-ai/detecting-heart-failure-using-machine-learning-part-2-8cca4cac266c?source=collection_archive---------4-----------------------#2021-03-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9015906dd7c48c08f3759e39cb048289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usxEtfAnN4JWdOF43jZ3zA.jpeg"/></div></div></figure><p id="c122" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在本系列的最后一部分中，我讨论了数据集并执行了一些基本的EDA。因此，下一步应该是利用特征工程和基线建模来利用这些见解。因此，本文将集中讨论这两个步骤。</p><p id="0edb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我提到的笔记本和数据集都在Github存储库中，以防你想深入研究(链接:<a class="ae jn" href="https://github.com/preeyonuj/Heart-Failure-Detection" rel="noopener ugc nofollow" target="_blank">https://github.com/preeyonuj/Heart-Failure-Detection</a>)。如果你想跟进系列的上一篇文章，这里有链接<a class="ae jn" href="https://preeyonujb1.medium.com/detecting-heart-failure-using-machine-learning-part-1-4c99475f4da5" rel="noopener">https://preeyonujb 1 . medium . com/detecting-heart-failure-using-machine-learning-part-1-4c 99475 F4 da 5</a>。</p></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><h1 id="f0a7" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">特征工程</h1><p id="67d6" class="pw-post-body-paragraph ip iq hh ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">特征工程是将给定数据转换成更容易解释的形式的过程。从以前文章中的EDA中，我发现我可以使用“年龄”和“时间”特性来创建新的有用特性。</p><h2 id="51bd" class="ky jw hh bd jx kz la lb kb lc ld le kf ja lf lg kj je lh li kn ji lj lk kr ll bi translated">年龄</h2><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lm"><img src="../Images/ed1915581fe914203e797c1526637c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*OPEGvFFP0j8CpmRuQ3riAQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">Binned ‘age’ statistics</figcaption></figure><p id="da17" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我观察到，超过一定年龄后，死亡率会翻倍。所以，我把这个特征分组，计算他们死亡的概率，并把它作为一个特征引入。随着年龄的增长，人们死亡的概率也在增加。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/5ece651f74e538b4429968f64df233e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*MhOw1RkAXcsv6u0avmfhug.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">Function to create the ‘prob_of_death’ feature</figcaption></figure><p id="1b12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我最初的笔记本中，我已经加入了一些统计测试来检查“死亡概率”特征和预测特征“死亡事件”之间的相关性。我使用了卡方检验，但是意识到一个先决条件是两个特征都是绝对的。我惊讶地发现，在互联网上很难找到检验分类变量和定量变量之间相关性的统计测试。所以，现在，我已经把它从笔记本和这篇文章中删除了，因为我可能使用了错误的统计测试。我一定会在我以后的系列文章中包含它。</p><h2 id="1dd0" class="ky jw hh bd jx kz la lb kb lc ld le kf ja lf lg kj je lh li kn ji lj lk kr ll bi translated">时间</h2><p id="42fe" class="pw-post-body-paragraph ip iq hh ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">时间是另一个特征，在这个特征中，我看到了谓词特征‘DEATH _ EVENT’的类之间的大量分离。因此，我决定创建一个更简单的、具有特定阈值的二进制版本的特性。</p><p id="c85e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">根据我上一篇文章中的图表，我估计了一个可以作为阈值的值范围，即95–100。我检查了每个阈值的准确性度量，发现96和96.5的性能最好。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/992a596252e507d0a517e3a371ea2c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQsaAIqkf82GdEZec6P5tQ.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">Snippet for calculating accuracy with each threshold</figcaption></figure><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/c3951ee6f0631debef65bb94a529572d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*AhzX9xWQMsFvvEc99ah9IQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">Accuracy results for each threshold</figcaption></figure><p id="2bf6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">肯定有优化的方法，但是由于数据集很小，我可以通过它运行整个数据集。最后，使用阈值96，我创建了一个特性，如果“时间”变量高于阈值，则设置为1，如果低于阈值，则设置为0。这背后的目的是线性参数算法中分类特征的重要性。</p><h1 id="2e06" class="jv jw hh bd jx jy ly ka kb kc lz ke kf kg ma ki kj kk mb km kn ko mc kq kr ks bi translated">系统模型化</h1><p id="e587" class="pw-post-body-paragraph ip iq hh ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">该数据集混合了分类特征和数字特征。所以我计划分离数字特征，并通过不同的管道运行它们。我还通过一些算法运行了整个特性集，最后，我创建了一个基于多数投票的集成推理管道。在深入研究之前，我将数据分为70/30的训练/测试比率。</p><h2 id="178f" class="ky jw hh bd jx kz la lb kb lc ld le kf ja lf lg kj je lh li kn ji lj lk kr ll bi translated">数字特征</h2><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/38d80bac1c696bff85caff3cadbafda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*zSkCxQlPZDITKE74a9d4Og.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">Numerical Features in the dataset</figcaption></figure><p id="444f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数字特征已经在侧面列出。我手动将数字特征从数据集中分离出来。我做的第一件事是使用标准缩放比例将它们缩放到一个共同的范围。</p><p id="cae8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我在这些数字特征上实现的算法是逻辑回归和SVM。</p><p id="30ed" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于<a class="ae jn" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">的逻辑回归</a>，结果出来的相当不错。我重点关注的指标是F1分数，因为这是一个更全面的衡量标准，此外还有Kappa ROC AUC分数，以便更好地理解。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/86d3dbcf8e259714d61f51900ef42cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*a7w-LHC5vr1KD4770Nvs7w.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">Logistic Regression Results</figcaption></figure><p id="5419" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于<a class="ae jn" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> SVM </a>，结果与逻辑回归的结果相当接近。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/fa90d645109d56b8ac1434739ffcb50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*cAqWjy6RJcxxwjOOJQqDGg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx">SVM Results</figcaption></figure><h2 id="0221" class="ky jw hh bd jx kz la lb kb lc ld le kf ja lf lg kj je lh li kn ji lj lk kr ll bi translated">整个功能集</h2><p id="3675" class="pw-post-body-paragraph ip iq hh ir b is kt iu iv iw ku iy iz ja kv jc jd je kw jg jh ji kx jk jl jm ha bi translated">在处理整个数据集时，我做的前提步骤是替换原始数据集中的缩放数值。然后，由于数据集具有数字和分类特征的组合，我选择了理论上可以处理这两种特征类型的算法。</p><p id="119d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我使用的第一个算法是带有gbtree助推器的<a class="ae jn" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>分类器。我设置的初始超参数纯粹基于直觉和以前的经验。结果比早先的推论要好。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/15379463a42c12e22e8f2f5d245f6b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*t6ZOjlRsy8330BPlrO9WVw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">XGBoost Results</figcaption></figure><p id="f278" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面我实现的算法是<a class="ae jn" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>带Log loss目标函数的分类器。CatBoost非常快，并且具有一种新颖的梯度增强技术来减少过拟合。同样，我从直观的初始超参数值开始，看起来效果不是很好。结果比XGBoost差。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/5bf6a41ccfc731962464e772459dee83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*ajXxzAp8nKB7v5IbEd0iqQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">CatBoost Results</figcaption></figure><p id="ee14" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，我使用了算法<a class="ae jn" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>分类器和传统的梯度提升决策树，在速度上和CatBoost差不多。使用直观的超参数进行初始化，模型结果与CatBoost和XGBoost的范围相同。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/4064fc0ae7a976cdbeed046caaf5de86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*chu3A9LsIeILNnFFzQC6jw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">LightGBM Results</figcaption></figure><p id="2279" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我实现的最后一个算法是随机森林，带有随机的初始超参数。结果类似于XGBoost。</p><figure class="ln lo lp lq fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/efb7f71d7883888e283625ff494b836a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*PitCtaY34stj_YRSZalwaw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx">Random Forest Results</figcaption></figure></div><div class="ab cl jo jp go jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="ha hb hc hd he"><p id="1329" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，我执行了一些基本的特征工程，并使用直观设置的初始超参数实现了一些初始模型。现在，我可以通过超参数优化进一步改善这些结果。这是本系列下一篇文章的主题。因此，在下一篇文章中，我将介绍超参数优化的过程，并最终创建一个包含所有模型的集成推理管道。敬请期待下一部分！</p><p id="522b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与此同时，如果你想更多地了解我和我的工作，这里是我的:</p><ol class=""><li id="85a4" class="mk ml hh ir b is it iw ix ja mm je mn ji mo jm mp mq mr ms bi translated">Github简介:<a class="ae jn" href="https://github.com/preeyonuj" rel="noopener ugc nofollow" target="_blank">https://github.com/preeyonuj</a></li><li id="7926" class="mk ml hh ir b is mt iw mu ja mv je mw ji mx jm mp mq mr ms bi translated">上一篇媒体文章:<a class="ae jn" rel="noopener" href="/analytics-vidhya/aptos-blindness-challenge-part-1-baseline-efficientnet-c7a256daa6e5?sk=d0e445f99daa71d79f0452665f1a59db">https://Medium . com/analytics-vid hya/aptos-blindness-challenge-part-1-baseline-efficient net-c 7a 256 da a6e 5？sk = d0e 445 f 99 DAA 71d 79 f 0452665 f1 a59 db</a></li><li id="cf91" class="mk ml hh ir b is mt iw mu ja mv je mw ji mx jm mp mq mr ms bi translated">本系列往期文章:<a class="ae jn" rel="noopener" href="/mlearning-ai/detecting-heart-failure-using-machine-learning-part-1-4c99475f4da5?source=friends_link&amp;sk=fee63820d5743abf90c4caedb68cc781">https://medium . com/mlearning-ai/detecting-heart-failure-using-machine-learning-part-1-4c 99475 F4 da 5？source = friends _ link&amp;sk = fee 63820d 5743 abf 90 C4 caedb 68 cc 781</a></li><li id="b310" class="mk ml hh ir b is mt iw mu ja mv je mw ji mx jm mp mq mr ms bi translated">领英简介:<a class="ae jn" href="http://www.linkedin.com/in/pb1807" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/pb1807</a></li><li id="d23f" class="mk ml hh ir b is mt iw mu ja mv je mw ji mx jm mp mq mr ms bi translated">推特简介:<a class="ae jn" href="https://twitter.com/preeyonuj" rel="noopener ugc nofollow" target="_blank">https://twitter.com/preeyonuj</a></li></ol></div></div>    
</body>
</html>