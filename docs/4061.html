<html>
<head>
<title>What’s the Difference Between Self-Attention and Attention in Transformer Architecture?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《变形金刚》架构中的自我关注和关注有什么区别？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3?source=collection_archive---------0-----------------------#2022-12-03">https://medium.com/mlearning-ai/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3?source=collection_archive---------0-----------------------#2022-12-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="3c2f" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">如果你仍然分不清哪个是哪个，你需要一篇文章</h2></div><p id="d067" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您是否有兴趣了解transformer架构，这是一种用于自然语言处理(NLP)任务的流行神经网络模型？如果是这样，你可能听说过自我关注和注意力，这两个相关但不同的概念是变形金刚模型的核心。在这篇博文中，我们将解释在transformer架构中<strong class="iy hi">自我关注</strong>和<strong class="iy hi">关注</strong>的区别，以及为什么它们对transformer模型的性能很重要。</p><p id="7d19" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们不会深究这些算法所涉及的数学——已经有像这篇文章一样的博客文章<a class="ae js" href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" rel="noopener" target="_blank">，它们比我能解释得更好。相反，我们的目标是从高层次上了解他们的差异。</a></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/1d43d820bf250a8a480d2ff9f45a654c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Pdmv8-ZrJ3Q3-ACa"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Samule Sun</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0079" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">注意力</h1><p id="8dbb" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">自我关注和关注都是允许变压器模型在进行预测时关注输入或输出序列的不同部分的机制。这些机制对于transformer模型在语言翻译、文本摘要和情感分析等任务中的性能至关重要，在这些任务中，模型需要理解输入和输出序列中不同单词或短语之间的关系。</p><p id="a0b6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意力指的是变压器模型在进行预测时关注另一个序列的不同部分的能力。这通常用于编码器-解码器架构，其中编码器对输入序列进行矢量化，而解码器在进行预测时会关注整个<em class="lg">输入的编码表示。例如，在语言翻译任务中，编码器处理源语言句子并生成其编码表示，然后解码器在生成目标语言的翻译时关注该编码表示。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lh"><img src="../Images/054fd7944ce2f1b2533e61614627cce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-rOFGRYUIG9fIz3DsxXEQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Decoder using attention mechanism to produce a single output Yt from encoder-<br/>created vectors h</figcaption></figure><p id="c995" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有什么大不了的？以前，如果您使用像LSTM这样的递归架构，您的架构会遇到很大的瓶颈——输入序列必须编码到单个摘要向量中，解码器必须在后续解码步骤中进一步传递其信息。这意味着我们能够传播的信息量是非常有限的，而且保留信息的窗口比注意力的情况要短得多。</p><p id="2836" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，信息窗口实际上是无限的(只受硬件能力的限制)，因为您可以从输入序列的任何元素中访问信息。</p><h1 id="6580" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">自我关注</h1><p id="c92b" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">另一方面，自我关注指的是变压器模型在进行预测时关注<strong class="iy hi">输入</strong>序列的不同部分的能力。这个名字来源于这样一个事实:与“常规”注意力相反，<strong class="iy hi">自我注意力指的是当前被编码的相同序列。</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es li"><img src="../Images/8cc0e7e8694c50246ffe0acf574a2fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwPNYk2gSe3tWuPhGhkAAA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Encoder with self-attention mechanism replacing recurrence. Each input t<br/>gets encoded into vector ht</figcaption></figure><p id="ab35" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这一突破类似于attention的突破——在递归架构中，编码器必须将所有需要的信息压缩到一组向量中，这些向量由递归单元传递。您可能已经看到了这种情况:如果信息窗口太大，这种设置也容易“忘记”一些事实。</p><p id="473c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">自我关注让我们在对每个输入元素进行编码的同时，能够看到序列的整体背景</strong>。这里不会发生遗忘，因为我们保留信息的窗口正好和我们需要的一样大(至少在自我关注的基础版本中)。</p><h1 id="5d43" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">结论</h1><p id="2314" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf ld jh ji jj le jl jm jn lf jp jq jr ha bi translated">总之，自我注意允许变换器模型关注同一输入序列的不同部分，而注意允许变换器模型关注另一序列的不同部分。这两种机制对于转换器模型在NLP任务中的性能都很重要，因为它们允许模型理解输入和输出序列中不同元素之间的关系，并做出更准确的预测。</p><p id="0b0a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自我关注和关注是transformer架构中的两个关键概念，transformer架构是NLP任务中使用的一个强大的神经网络模型。理解这两种机制之间的区别以及它们是如何工作的，可以帮助您理解transformer模型的功能和局限性，并在您自己的NLP项目中有效地使用它们。</p><div class="lj lk ez fb ll lm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hi fi z dy lr ea eb ls ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">medium.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kd lm"/></div></div></a></div></div></div>    
</body>
</html>