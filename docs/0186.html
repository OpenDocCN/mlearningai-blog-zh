<html>
<head>
<title>Generative Adversarial Networks (GAN): An Intuitive Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成对抗网络(GAN):直观介绍</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/generative-adversarial-networks-gan-an-intuitive-introduction-815b953751fb?source=collection_archive---------2-----------------------#2021-02-28">https://medium.com/mlearning-ai/generative-adversarial-networks-gan-an-intuitive-introduction-815b953751fb?source=collection_archive---------2-----------------------#2021-02-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="8dbd" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">计算机生成模型综合介绍</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/533b060fc3b8ab14266d492b645e64f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLkh-RLgG3_kQpgYxpdZHg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">An image of Lee Hsien Loong, currently the prime minister of Singapore, transformed into Korean “oppa” [Photo reproduced with permission from content creator <a class="ae jm" href="https://www.facebook.com/jazephua/photos/a.1055621384455017/4060143500669442/?type=3" rel="noopener ugc nofollow" target="_blank">Jaze Phua</a>. Watch the transformation video <a class="ae jm" href="https://www.facebook.com/watch/?v=848928798984222" rel="noopener ugc nofollow" target="_blank">here</a>.]</figcaption></figure><p id="39d4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi kj translated">信不信由你，上面右手边的图片完全是由电脑根据左边李显龙的真实形象制作的。在这里，他的面部形象被“反相”，同时保持他独特的外观。<em class="ks"> Oppa </em>在朝鲜语中是指一个人的哥哥。这也是对英俊的韩国演员的爱称。这是一个松散的<em class="ks">图像</em> <em class="ks">风格变形</em>的例子，其中数字图像被变形为不同但特定的风格。鉴于它们是由计算机人工生成的，输出图像的真实性令人印象深刻。</p><p id="060e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">上图是使用一个名为<a class="ae jm" href="https://www.faceapp.com/" rel="noopener ugc nofollow" target="_blank"> FaceApp </a>的应用程序创建的。除了一些隐私问题之外，它还因为能够以高度的真实感改变数字图像中的面部特征而获得了极大的关注。可以达到各种效果，比如性别互换，衰老，逆转衰老。这样的图像，或者任何一般的合成对象——诗歌、音乐，甚至假新闻——都可以用<em class="ks">生成模型来合成。</em></p><p id="18e0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a> (GAN)是一个利用深度学习的<strong class="jp hi">生成</strong> <strong class="jp hi">建模</strong>框架。它在计算机视觉领域有许多成功的应用。在本文中，我将提供这个框架的概述(主要是理论背景)。这应该是一个轻松阅读，提供一些直觉到甘。对于感兴趣的读者来说，这中间确实有点技术性。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="9711" class="la lb hh bd lc ld le lf lg lh li lj lk in ll io lm iq ln ir lo it lp iu lq lr bi translated">概述</h1><ol class=""><li id="1963" class="ls lt hh jp b jq lu jt lv jw lw ka lx ke ly ki lz ma mb mc bi translated"><strong class="jp hi">直觉。【T21简介】提供直觉入甘的概念。</strong></li><li id="93eb" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated"><strong class="jp hi">生成型模型。</strong>为什么生成模型(甘是其中的一个建模框架)值得研究？</li><li id="3dfd" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated"><strong class="jp hi">生成对抗网。</strong>GAN的基本框架和架构。</li><li id="8d66" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated"><strong class="jp hi">甘模特培训。</strong>如何在GAN中训练发生器和鉴别器？</li><li id="2f60" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated"><strong class="jp hi">甘的基本变异。一种</strong>结构改进，能够在各种图像合成任务中使用GANs。</li><li id="1f6b" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated"><strong class="jp hi">甘的应用。</strong>一些应用和当前GAN模型。</li></ol></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mi"><img src="../Images/265e14109bcd9195a28a04c067ad579b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toqWHLgBr5ahpIhzBBJ7VA.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Counterfeit items sold on the streets in South Korea [Photo by <a class="ae jm" href="https://unsplash.com/@adliwahid?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Adli Wahid</a> on <a class="ae jm" href="https://unsplash.com/s/photos/counterfeit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>]</figcaption></figure><h1 id="f70f" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">1.直觉</h1><p id="e9b0" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">你看过莱昂纳多·迪卡普里奥主演的电影《如果你能抓住我就抓住我》吗？这是根据真实生活故事改编的，一个骗子<a class="ae jm" href="https://en.wikipedia.org/wiki/Frank_Abagnale" rel="noopener ugc nofollow" target="_blank"/><em class="ks"/>伪造了价值数百万美元的工资支票。他变得如此擅长，以至于最终联邦调查局求助于他来帮助抓捕其他伪造者。</p><p id="0655" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">甘很像一个骗子对联邦调查局的对抗游戏。这里我们有一个<em class="ks">生成器</em>试图“伪造”一些东西——梵高的画、莎士比亚的小说、贝多芬的钢琴曲——或者<a class="ae jm" href="https://www.yamaha.com/en/about/ai/dear_glenn/" rel="noopener ugc nofollow" target="_blank">模仿已故钢琴家格伦·古尔德</a>的演奏风格。我们有一个鉴别器<em class="ks">试图鉴别一件物品是伪造的还是真的。</em></p><p id="446f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">最初，生成器(康曼)产生蹩脚的模仿。鉴别者(FBI)鉴别出了这些假货，并轻而易举地抓住了骗子。但是在无数次进出监狱和改进他的设计后，骗子学会了生产越来越多看起来逼真的仿制品。在某一点上，他的仿制品变得与真品难以区分。</p><h1 id="c54c" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">2.生成模型</h1><p id="1e8c" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">我们大多数学习机器学习的人，可能更熟悉<em class="ks">判别模型</em>:二元分类器、多类分类器、多标签分类器等等。给定观察值<strong class="jp hi"> x </strong>，分类器捕获类别标签<em class="ks"> c </em>的条件概率<strong class="jp hi">P(</strong><em class="ks">c</em><strong class="jp hi">| x)</strong>:<strong class="jp hi"/>概率<strong class="jp hi"> </strong>。<em class="ks">另一方面，生成模型</em>捕捉观察的概率P( <strong class="jp hi"> x </strong>)或联合概率P( <strong class="jp hi"> x </strong>，c)。</p><p id="47df" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">生成模型从<em class="ks">潜在空间</em>中提取样本，以合成新数据。潜在空间是多维空间，其中空间中的每个点对应于一个观察值(即合成数据)。简单地说，它可以被认为是领域特征的数字表示。例如，面部图像的表示类似于:</p><ul class=""><li id="e8fd" class="ls lt hh jp b jq jr jt ju jw mr ka ms ke mt ki mu ma mb mc bi translated">两只眼睛，一个鼻子，一张嘴</li><li id="15d3" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki mu ma mb mc bi translated">眼睛紧挨着，位于鼻子上方</li><li id="aa51" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki mu ma mb mc bi translated">嘴位于噪音的下方，决定了脸部是微笑、中性还是皱眉</li><li id="73c8" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki mu ma mb mc bi translated">面部特征可以代表男性或女性</li><li id="116a" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki mu ma mb mc bi translated">可能会有眼镜、口罩、化妆品之类的配饰…</li></ul><p id="7d67" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">每个特征都由空间中的一个维度(变量)来表示。这种表示允许非常有趣的潜在空间矢量算法。在<a class="ae jm" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">这篇论文</a>中，作者对人脸图像进行了如下所示的潜在向量运算。这些向量表示男人、女人和眼镜的图像特征，并且可以用算术方法处理，得到令人信服的输出。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mv"><img src="../Images/1a56ef3f21d55d0abaa57c1739efe5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*UdltJwavGCZyTXtJD75b5Q.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Latent vector arithmetic, where the resulting vector is input to a generator to generate the output image. [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><p id="feba" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">训练生成模型来合成特定领域中的数据类似于学习该领域的潜在空间表示。随后，该模型通过从学习空间中的无限数量的点进行采样，生成合成数据，每个数据都略有不同。</p><p id="1015" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在更正式的定义中，由<strong class="jp hi"> 𝜃 </strong>参数化的生成模型<strong class="jp hi"> 𝐺 </strong>将随机噪声向量<strong class="jp hi"> <em class="ks"> z </em> </strong>作为输入，并生成输出样本<strong class="jp hi"><em class="ks">𝐺(z；𝜃 ) </em> </strong>用概率分布<strong class="jp hi"><em class="ks">p _生成</em> </strong>。一个好的生成模型能够生成新的、可信的样本，这些样本与真实的输入样本无法区分。换句话说，<strong class="jp hi"><em class="ks">p _生成</em></strong>∾<strong class="jp hi"><em class="ks">p _输入</em> </strong>。</p><p id="f47b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这篇论文中，作者给出了一个有趣的例子来说明为什么生成模型值得研究。生成模型，尤其是GANs，使机器学习能够与<em class="ks">多模态</em>输出一起工作。例如，让我们采用一个回归模型，该模型被训练来最小化实际和预测输出之间的均方误差。这种模型对于每个输入样本只能产生单个预测输出。然而，对于许多任务，单个输入可能对应于许多不同的正确答案，每个答案都是可接受的。下图说明了视频序列的预测帧的计算机渲染。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mw"><img src="../Images/2a6f776ef3d9328e16ef4aa3eafe43e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5VIdQtAI18_iclaM5zDng.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">The output of next frame prediction task from a video sequence, showing the outputs from a model trained with only mean square error loss (MSE) and additinal GAN loss (Adversarial). [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1701.00160" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><p id="79fc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这项任务中，模型被训练来预测视频序列中的下一帧。左边的图像是地面实况，或者是监督训练通常意义上的目标输出。中间的图像显示了使用均方误差(MSE)训练的模型的预测输出。该模型被迫输出下一帧看起来像什么的单一答案。因为有许多可能的未来，对应于头部稍微不同的位置，模型选择的单一答案对应于许多稍微不同的图像的平均值。这导致耳朵几乎消失，眼睛变得模糊。</p><p id="f76d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">使用额外的g an损耗，该模型能够理解有许多可能的输出。已经学习了输出图像的分布的生成模型能够从可能的输出中的一个进行采样。结果是右边的图像是清晰的，可识别为一个现实的，详细的图像。</p><h1 id="ccec" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">3.生成对抗网络</h1><p id="8a8d" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">GAN是一个深度学习框架，通过对抗过程来训练生成模型。基本框架如下图所示，包括:</p><ul class=""><li id="2d5a" class="ls lt hh jp b jq jr jt ju jw mr ka ms ke mt ki mu ma mb mc bi translated">一个<strong class="jp hi">发生器网络G </strong>，它采用一个随机噪声矢量<strong class="jp hi"> z </strong>来产生合成数据<strong class="jp hi"> G(z) </strong>，以及</li><li id="0d3e" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki mu ma mb mc bi translated">一个<strong class="jp hi">鉴别器网络D </strong>，其获取真实数据<strong class="jp hi"> x </strong>和合成数据，并检测特定数据样本是真是假。</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mx"><img src="../Images/524f2ab9486b56ef92b7ac257c05e436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*r4-nDRkLLsYN-IvRyYu6PA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">GAN framework with generator and discriminator networks. [Image by author]</figcaption></figure><p id="b4e5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">两个网络都以这样的方式训练，即发生器最终合成鉴别器不能从真实数据样本中区分的数据样本。GAN已经成功地用于计算机视觉应用，特别是用于图像合成和图像到图像的翻译。多年来，已经提出了许多架构变化。然而，众所周知，这个框架不稳定，难以训练。在网络训练期间，它遭受<em class="ks">消失梯度</em>和<em class="ks">模式崩溃</em>问题。这些问题的解决方案仍然是一个活跃的研究领域。</p><h1 id="f0fb" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">4.甘模特培训</h1><p id="77f6" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">训练被制定为双人<strong class="jp hi"> <em class="ks"> minimax </em> </strong>游戏。在这里，生成器𝐺试图<strong class="jp hi"> <em class="ks">最小化</em> </strong>合成样本和真实样本之间的差距，以便欺骗鉴别器，而鉴别器𝐷试图<strong class="jp hi"> <em class="ks">最大化</em> </strong>其对合成样本的理解，以便更好地区分真实数据和合成数据。</p><p id="3ba5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">直觉上，生成器可以被认为类似于一组伪造者，他们试图制造假支票并在不被发现的情况下使用它。鉴别器类似于联邦调查局，试图检测假币。对抗训练的过程类似于一场竞赛，两个团队都在不断改进他们的方法，直到无法将赝品与真品区分开来。</p><p id="220e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这两个网络以交替的步骤进行训练。鉴别器网络被训练一个或多个时期，同时保持发生器网络常数，反之亦然。重复这些步骤，直到鉴别器具有50%的准确度。此时，生成的图像与真实图像无法区分，鉴别器只是猜测。</p><h2 id="3a45" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">4.1.鉴频器训练阶段。</h2><p id="a288" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">鉴别器对来自生成器的真实数据和虚假数据进行分类。因此，它只是一个二元分类器，旨在最大化正确分类真实和合成输入的概率。目标可以通过交叉熵损失来定义。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nm"><img src="../Images/f91f04bba448df6b31b0279fa840eb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*cbKCdLeOQJGEU1mkESGTEg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Discriminator training. [Image by author]</figcaption></figure><h2 id="5e73" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">4.2发电机培训阶段</h2><p id="fd2a" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">发电机网络的培训稍微复杂一些。它包括发生器和鉴别器网络。发电机训练的迭代包括以下步骤:</p><ol class=""><li id="4524" class="ls lt hh jp b jq jr jt ju jw mr ka ms ke mt ki lz ma mb mc bi translated">样本随机噪声。</li><li id="b50e" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">产生发电机输出。</li><li id="38ce" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">将输出传递给鉴别器。</li><li id="0d7d" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">计算二元分类损失。</li><li id="b884" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">通过鉴别器和发生器网络反向传播。</li><li id="09b8" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">仅更新<strong class="jp hi">发电机重量</strong>。</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nn"><img src="../Images/6c83aba79bb785e206903ce42b4170ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*SDjUHt8PB_hlp9fTK0mODg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Generator training. [Image by author]</figcaption></figure><h1 id="f8e0" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">5.GAN的变体</h1><p id="08b8" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">GAN许多变体已经被提出来执行各种任务。在这里，我们将看看几个基本的变化。这一部分提供了一个简要的概述，并提供了更多详细阅读资料的原始论文链接。它相对来说技术性更强，需要对神经网络体系结构有基本的了解。</p><h2 id="b4e6" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">5.1深度卷积GAN (DCGAN)</h2><p id="840e" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">原始GAN中的发生器和鉴别器由全连接网络构成。然而，GAN在图像合成领域具有广泛的应用，其中卷积神经网络(CNN)是天然的。因此，一个名为<a class="ae jm" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank"> DCGAN </a>(深度卷积GAN)的网络架构家族被提出。它允许训练一对深度卷积生成器和鉴别器网络。</p><p id="30ad" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">通常，卷积网络使用空间池层，如<em class="ks">最大值</em>和<em class="ks">平均池</em>，在图像沿网络移动时对其进行上采样或下采样。DCGAN使用<em class="ks">步长</em>和<em class="ks">分数步长</em>卷积，这是可以学习的。这允许在训练期间学习空间下采样和上采样算子。这些算子处理采样率和位置的变化，这是从图像空间映射到可能更低维度的潜在空间以及从图像空间映射到鉴别器的关键要求。</p><p id="e7ef" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这篇论文中，作者提出了一种为他们的实验需要而设计的深度卷积生成器结构。结构如下图所示。发生器的第一层将100维均匀分布噪声<strong class="jp hi"> 𝑧 </strong>作为全连接网络的输入。结果被整形为4维张量，并用作卷积叠加的开始。然后，一系列四个分数步长卷积将这种高级表示转换为64 × 64像素图像。对于鉴频器，最后一个卷积层被展平，然后馈入单个sigmoid输出</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es no"><img src="../Images/c7f1b0f82929447330559670877da996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAI1Fd8anXQeW8gbqo40LQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Deep convolutional generator structure for DCGAN. [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><h2 id="1518" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">5.2有条件GAN (CGAN)</h2><p id="1c96" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">在原始GANs中，对生成的图像没有控制，因为它们仅依赖于随机噪声作为输入。GAN 的一个<a class="ae jm" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">条件版本是在GAN推出后不久提出的。可以通过根据附加信息来调节模型来指导生成过程。典型地，条件向量<strong class="jp hi"> y </strong>与噪声向量<strong class="jp hi"> z </strong>连接，并且结果向量被输入到原始GAN。</a></p><p id="9bd1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">条件向量是任意的。例如，在CGAN论文中，作者使用类别标签的一个热向量来调节GAN以生成MNIST数字。他们还展示了由预先训练的AlexNet的卷积层提取的图像的特征向量调节的图像的自动标记。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es np"><img src="../Images/7169fbcc96210e9333cd46c91aa5bea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuU63gJJMzngKLec45S7sA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Structure of CGAN, in which the generator and discriminator are conditioned by additional vector <strong class="bd lc">y.</strong> [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><h2 id="80db" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">5.3辅助分类器GAN (ACGAN)</h2><p id="0283" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated"><a class="ae jm" href="https://arxiv.org/abs/1610.09585" rel="noopener ugc nofollow" target="_blank"> ACGAN </a>是条件GAN的扩展，它通过增加一个额外的任务来修改鉴别器:在真/假分类之上预测给定图像的类别标签。众所周知，强迫一个模特执行额外的任务(<a class="ae jm" href="https://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank"> <em class="ks">【多任务学习】</em> </a>)可以提高原始任务的表现。它具有稳定训练过程并允许生成大的高质量图像的效果，同时学习独立于类别标签的潜在空间中的表示。ACGAN的结构如下所示。作者论证了这种方法有助于产生更清晰、分辨率更高的图像。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nq"><img src="../Images/311a2af8daaa098435930e6ba6671d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*ygkMYGEieWL1LD30WTCGDg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Structure of ACGAN in image synthesis, where the discriminator performs additional task of predicting the class label of image. [Figure taken from <a class="ae jm" href="https://arxiv.org/abs/1610.09585" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><h2 id="84d9" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">5.4双向GAN(甘比)</h2><p id="931a" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">GAN可以将噪声向量<strong class="jp hi"> z </strong>从简单的潜在分布转换成数据分布相当复杂的合成数据样本<strong class="jp hi">【𝐺(z】</strong>。证明了这种生成器的潜在空间捕获数据分布中的语义变化。直观地，训练来预测给定数据的这些语义潜在表示的模型可以充当语义相关的辅助问题的有用特征表示。然而，GANs缺乏将数据样本<strong class="jp hi"> x </strong>映射到潜在特征<strong class="jp hi">z</strong>的能力</p><p id="6012" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">甘比被提出来作为学习这种逆映射的方法。结果表明，所得到的学习特征表示对于辅助监督辨别任务是有用的，与无监督和自监督特征学习的当代方法相竞争。</p><p id="1537" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">甘比的结构如下所示。除了来自标准GAN框架的生成器<strong class="jp hi"> 𝐺 </strong>，甘比还包括一个编码器<strong class="jp hi"> 𝐸 </strong>，它将数据<strong class="jp hi"> x </strong>映射到潜在表示<strong class="jp hi"> z </strong>。甘比鉴别器<strong class="jp hi"> 𝐷 </strong>不仅在数据空间(<strong class="jp hi"> x </strong>对<strong class="jp hi"> 𝐺 (z) </strong>进行鉴别，而且在数据和潜在空间(元组(<strong class="jp hi"> x </strong>，<strong class="jp hi"> 𝐸(x) </strong>)对(<strong class="jp hi"> 𝐺(z) </strong>，<strong class="jp hi"> z </strong>)进行联合鉴别，其中潜在分量是编码器输出<strong class="jp hi"> 𝐸(x) </strong>或发生器输入<strong class="jp hi"> z</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nr"><img src="../Images/30e1ccbdf8bbbf000093e980111c8e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UC5XJcLva64zNCTs20VzxA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Structure of BiGAN. [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1605.09782" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><h1 id="232a" class="la lb hh bd lc ld mj lf lg lh mk lj lk in ml io lm iq mm ir lo it mn iu lq lr bi translated">6.应用</h1><p id="8d59" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">GAN一直是许多现实世界应用的支柱，尤其是在图像合成方面。这也是一个非常活跃的研究领域。这里讨论一些有趣的应用。</p><h2 id="cfc8" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">6.1图像超分辨率</h2><p id="d8ac" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">在该应用中，GAN用于基于低分辨率输入重建更高分辨率的图像。<a class="ae jm" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">第一个此类框架</a>于2017年推出，能够推断逼真的自然图像以进行4倍放大操作。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ns"><img src="../Images/66727aadedff68e163d94c5a05cb8439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u2Wxy1MeOda_VNqCjaPzDQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">The quality of image upscaling (4x) comparing bicubic interpolation, super-resolution ResNet optimized with MSE, and super-resolution with GAN optimized with perceptual loss. [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><p id="47ae" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在本文的<a class="ae jm" href="https://arxiv.org/abs/1710.10196" rel="noopener ugc nofollow" target="_blank">中，GAN训练方法被引入到生成可信的面部图像的背景中。这种方法的关键思想是逐渐增加生成器和鉴别器，从低分辨率开始，随着训练的进行增加新的层，以使模型增加精细的细节。生成的图像令人印象深刻，非常逼真。此后，图像质量和分辨率不断提高。</a></p><p id="ae36" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">最新款</strong> : <a class="ae jm" href="https://arxiv.org/abs/1908.08239" rel="noopener ugc nofollow" target="_blank">渐进式人脸超分辨率</a> (2019)，对监控摄像头采集的人脸图像的<a class="ae jm" href="https://arxiv.org/abs/2102.03113" rel="noopener ugc nofollow" target="_blank">超分辨率</a> (2021)</p><h2 id="f141" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">6.2图像修复</h2><p id="feab" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">图像修复是指基于背景信息恢复和重建图像的技术。期望生成的图像看起来非常自然，并且难以与地面真实相区分。高质量的图像修复不仅要求生成的内容语义合理，而且要求生成的图像纹理足够清晰逼真。例如，<a class="ae jm" href="https://arxiv.org/abs/1712.03999" rel="noopener ugc nofollow" target="_blank"> ExGAN </a>能够将闭着眼睛的面部图像修改为睁开眼睛的图像，同时保留对象的身份。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nt"><img src="../Images/6fd4866fe94e1a7e5b2e23c089510467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*4_Gc72c0D0BnFftarnEvOQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Result from eye opening ExGAN model. [Image taken from <a class="ae jm" href="https://arxiv.org/abs/1712.03999" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><p id="31c9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">近期车型</strong> : <a class="ae jm" href="https://arxiv.org/abs/1806.03589" rel="noopener ugc nofollow" target="_blank"> Deepfillv2 </a> (2019)、<a class="ae jm" href="https://arxiv.org/abs/1901.00212" rel="noopener ugc nofollow" target="_blank"> EdgeConnect </a> (2019)</p><h2 id="dbc8" class="my lb hh bd lc mz na nb lg nc nd ne lk jw nf ng lm ka nh ni lo ke nj nk lq nl bi translated">6.3图像到图像的翻译</h2><p id="fb51" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi translated">最初的GAN被提出用于从噪声中生成图像。图像到图像GAN从图像生成不同的图像。图像翻译的目标是学习从源图像域到目标图像域的映射，即在保持图像内容不变的情况下，将源图像域的风格或其他一些属性改变到目标图像域。</p><p id="fd21" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">2017年，<em class="ks"> pix2pix </em>软件与一项关于<a class="ae jm" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">多用途图像到图像翻译与条件GAN </a>的工作联合发布。例如，它能够从草图中生成逼真的图像。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nu"><img src="../Images/f74eb1cad3ad546ea948734a3dbd6ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nTSeevZQ-dknvFo-4gTpdA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Results from pix2pix, enabling realistic image generation and transformation. [Image taken from <a class="ae jm" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">this paper</a>]</figcaption></figure><p id="36c6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">近期车型:<a class="ae jm" href="https://arxiv.org/abs/2011.12636" rel="noopener ugc nofollow" target="_blank"> pix2pixHD-Aug </a> (2020年)<a class="ae jm" href="https://arxiv.org/abs/2102.01143" rel="noopener ugc nofollow" target="_blank"> toon2real </a> (2021年)</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="8e65" class="la lb hh bd lc ld le lf lg lh li lj lk in ll io lm iq ln ir lo it lp iu lq lr bi translated">结论</h1><p id="a698" class="pw-post-body-paragraph jn jo hh jp b jq lu ii js jt lv il jv jw mo jy jz ka mp kc kd ke mq kg kh ki ha bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di">在</span>这篇文章中我介绍了GAN，一个深度生成模型类。氮化镓被广泛应用于计算机视觉领域，尤其是图像合成。不仅限于图像，GAN还学习了音频、文本和一般数据中隐含的复杂和高维分布。</p><p id="54bb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">作为临别赠言，我想强调一下GAN在数据增强方面的潜力。由于GAN能够在数据样本的域中产生样本，因此GAN具有用于此目的的良好前景。它已经被提出用于<a class="ae jm" href="https://arxiv.org/abs/1904.09135" rel="noopener ugc nofollow" target="_blank">图像数据增强</a>。在我的研究领域，即语音分析，GAN已经成功地用于语音情感识别中的语音数据增强。</p></div><div class="ab cl kt ku go kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="ha hb hc hd he"><h1 id="3cdb" class="la lb hh bd lc ld le lf lg lh li lj lk in ll io lm iq ln ir lo it lp iu lq lr bi translated">参考</h1><ol class=""><li id="5390" class="ls lt hh jp b jq lu jt lv jw lw ka lx ke ly ki lz ma mb mc bi translated">I.J. Goodfellow，J. Pouget-Abadie，M. Mirza，B. Xu，D. Warde-Farley，S. Ozair，a . c . &amp; y . beng io，<a class="ae jm" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a> (2014)，<em class="ks"> NIPS </em>。</li><li id="c977" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">I.J. Goodfellow，<a class="ae jm" href="https://arxiv.org/abs/1701.00160" rel="noopener ugc nofollow" target="_blank"> NIPS 2016教程:生成对抗网络</a> (2017)，<em class="ks"> ArXiv，abs/1701.00160 </em>。</li><li id="3e2d" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">A.拉德福德，l .梅斯和s .钦塔拉，<a class="ae jm" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络的无监督表示学习</a> (2016)，<em class="ks"> arXiv:cs。LG/1511.06434 </em></li><li id="653c" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">米（meter的缩写））Mirza &amp; S. Osindero，<a class="ae jm" href="https://arxiv.org/abs/1411.1784" rel="noopener ugc nofollow" target="_blank">条件生成通用网</a> (2014)，<em class="ks"> arXiv:cs。LG/1411.1784 </em></li><li id="13a6" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">A.Odena，C. Olah &amp; J. Shlens，<a class="ae jm" href="https://arxiv.org/abs/1610.09585" rel="noopener ugc nofollow" target="_blank">使用辅助分类器的条件图像合成GANs </a> (2017)，<em class="ks"> arXiv:stat。ML/1610.09585 </em></li><li id="60fd" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">J.多纳休，P. Krähenbühl和t .达雷尔，<a class="ae jm" href="https://arxiv.org/abs/1605.09782" rel="noopener ugc nofollow" target="_blank">对抗性特征学习</a> (2017)，<em class="ks"> arXiv:cs。LG/1605.09782 </em></li><li id="b931" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">C.Ledig，L. Theis，F. Huszár，J. Caballero，A. Aitken，A. Tejani，J. Totz，Z. Wang &amp; W. Shi，<a class="ae jm" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">使用生成式对抗网络的照片真实感单幅图像超分辨率</a> (2017)，<em class="ks"> 2017年IEEE计算机视觉与模式识别大会()</em>，105–114。</li><li id="5252" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">B.Dolhansky &amp; C. Canton-Ferrer，<a class="ae jm" href="https://arxiv.org/abs/1712.03999" rel="noopener ugc nofollow" target="_blank">具有范例生成对抗网络的眼内画</a> (2018)，<em class="ks"> 2018 IEEE/CVF计算机视觉与模式识别会议</em>，7902–7911。</li><li id="8f38" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">页（page的缩写）Isola，J. Zhu，t . Zhu &amp; a . a . ef ROS，<a class="ae jm" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank">条件对抗网络的图像到图像翻译</a> (2017)，<em class="ks"> 2017年IEEE计算机视觉与模式识别大会()</em>，5967–5976。</li><li id="2ecb" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">F.H. Tanaka &amp; C. Aranha，<a class="ae jm" href="https://arxiv.org/abs/1904.09135" rel="noopener ugc nofollow" target="_blank">使用GANs的数据增强</a> (2019)，<em class="ks"> ArXiv，abs/1904.09135 </em>。</li><li id="2c93" class="ls lt hh jp b jq md jt me jw mf ka mg ke mh ki lz ma mb mc bi translated">A.Chatziagapi，G. Paraskevopoulos，D. Sgouropoulos，G. Pantazopoulos，M. Nikandrou，t .詹纳科普洛斯，A. Katsamanis，A. Potamianos &amp; S.S. Narayanan，<a class="ae jm" href="https://www.isca-speech.org/archive/Interspeech_2019/abstracts/2561.html" rel="noopener ugc nofollow" target="_blank">使用GANs进行语音情感识别的数据增强</a> (2019)，<em class="ks"> INTERSPEECH </em>。</li></ol></div></div>    
</body>
</html>