<html>
<head>
<title>Heatmap For Correlation Matrix &amp; Confusion Matrix | Extra Tips On Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关联矩阵和混淆矩阵的热图|关于机器学习的额外提示</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/heatmap-for-correlation-matrix-confusion-matrix-extra-tips-on-machine-learning-b0377cee31c2?source=collection_archive---------3-----------------------#2022-02-19">https://medium.com/mlearning-ai/heatmap-for-correlation-matrix-confusion-matrix-extra-tips-on-machine-learning-b0377cee31c2?source=collection_archive---------3-----------------------#2022-02-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5499" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们经常在解释性数据分析(EDA)中计算<strong class="ig hi">相关系数</strong>来检查数值变量相互关联的程度。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/515a2e6fa73fae8d68e66fae8a8fe478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m7k7zZqaRHeT5z2_Sd2Jpw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Source: <a class="ae js" href="https://www.pexels.com/photo/bonfire-wallpaper-266388/" rel="noopener ugc nofollow" target="_blank">Pexels@Pixabay</a></figcaption></figure><p id="e6d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果一个变量增加，另一个也增加，那么这两个变量被称为<strong class="ig hi">正相关</strong>。相反，如果一个变量的高值与另一个变量的低值一致，那么它们就是<strong class="ig hi">负相关</strong>。</p><ul class=""><li id="3a92" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb jy jz ka kb bi translated">corr= +1表示完全正相关</li><li id="e944" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">corr =–1表示完全负相关</li><li id="5970" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb jy jz ka kb bi translated">corr = 0表示无相关性</li></ul><p id="6e6d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据科学家通常使用<a class="ae js" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Pearson的相关系数</a>，或其针对EDA的稳健替代方案，尽管也有其他类型的相关系数(例如<a class="ae js" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Spearman的rho </a>或<a class="ae js" href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Kendall的tau </a>)，它们基于数据的等级，对异常值稳健，并且可以处理某些类型的非线性。这是因为这些基于排名的估计大多是针对较小的数据集和某些假设检验。</p><p id="5fd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设我有一个名为“快乐21”的数据框架，我可以简单地使用下面几行代码来计算变量之间的相关性。</p><pre class="jd je jf jg fd kh ki kj kk aw kl bi"><span id="685c" class="km kn hh ki b fi ko kp l kq kr">import numpy as np<br/>import pandas as pd</span><span id="7da0" class="km kn hh ki b fi ks kp l kq kr"># calculate correlation<br/>happiness21[np.array(happiness21.columns)].corr()<br/># cols = ['happiness_score', 'gdp_per_capita','social_support', 'life_expectancy', 'freedom', 'generosity','perceptions_of_corruption']<br/># happiness21[cols].corr()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kt"><img src="../Images/8274d579b3420412bbe135b097e3b102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dkgn1t80WThPz-GlL0kHGw.png"/></div></div></figure><pre class="jd je jf jg fd kh ki kj kk aw kl bi"><span id="cd3b" class="km kn hh ki b fi ko kp l kq kr">import seaborn as sns</span><span id="8cb7" class="km kn hh ki b fi ks kp l kq kr"># create heatmap for the calculated correlation<br/>plt.figure(figsize=(16,10))<br/>sns.heatmap(happiness21[np.array(happiness21.columns)].corr(),annot=True, fmt='.2g')<br/>plt.title('Correlation between Variables in happiness21', fontsize=14)</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ku"><img src="../Images/459a66b554a2e60e486bb8b469f35412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FF4CV81G0_PlyezJsHlTFw.png"/></div></div></figure><p id="2ada" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，当我们对分类数据执行监督机器学习(分类)时，我们经常使用一个<strong class="ig hi">混淆矩阵</strong>来获得不同类别的准确和不准确预测的计数。</p><p id="8fa1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">混淆矩阵可以是二元分类器(可能是两个以上类别的情况)。假设我正在创建一个预测模型来决定产品是否会准时到达。“1”表示产品延迟到达，“0”表示产品准时到达。测试数据由总共2，750条记录组成。实际上，有1648种产品按时到达，1102种产品迟到。然而，分类器预测“0”1937次，预测“1”813次。</p><ol class=""><li id="80e6" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb kv jz ka kb bi translated">True positive:产品到达较晚，分类器预测为“1”。<em class="kw">TP/总记录数= 768/2750 = 0.2793</em></li><li id="e839" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated">假阳性(I型错误):产品准时到达；然而，它被预测为“1”。<em class="kw">FP/总记录数= 45/2750 = 0.0164</em></li><li id="26e8" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated">真否定<strong class="ig hi"> : </strong>产品准时到达，预测为“0”。<em class="kw">TN/总记录数= 1057/2750 = 0.3844</em></li><li id="719b" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated">假阴性(第二类错误):产品到达较晚；但预测为“0”。<em class="kw">FN/总记录数= 880/2750 = 0.3200</em></li></ol><pre class="jd je jf jg fd kh ki kj kk aw kl bi"><span id="877d" class="km kn hh ki b fi ko kp l kq kr">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns</span><span id="9043" class="km kn hh ki b fi ks kp l kq kr"><em class="kw"># confusion matrix </em><br/>group_names = ['True Pos', 'False Pos', 'False Neg', 'True Neg']<br/>test_cnf_matrix = confusion_matrix(Y_test,pred_test_DT)<br/>test_counts = ["<strong class="ki hi">{0:0.0f}</strong>".format(value) for value <strong class="ki hi">in</strong> test_cnf_matrix.flatten()]<br/>test_percentage = ["<strong class="ki hi">{0:.2%}</strong>".format(value) for value <strong class="ki hi">in</strong> test_cnf_matrix .flatten()/np.sum(test_cnf_matrix)]<br/>test_labels = [f"<strong class="ki hi">{</strong>v1<strong class="ki hi">}\n{</strong>v2<strong class="ki hi">}\n{</strong>v3<strong class="ki hi">}</strong>" for v1, v2, v3 <strong class="ki hi">in</strong> zip(group_names,test_counts,test_percentage)]<br/>test_labels = np.asarray(test_labels).reshape(2,2)<br/>plt.figure(figsize = (16,5))<br/>sns.heatmap(test_cnf_matrix, annot=test_labels, fmt='', cmap='Blues');</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kx"><img src="../Images/1730a24cc0fa79be6e2c73218b08c8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xo18KatocTUfOeduS7McFg.png"/></div></div></figure><p id="bc70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一方面，我们可以从混淆矩阵本身计算出以下5个特征，其中部分特征可以从scikit-learn的<code class="du ky kz la ki b">classification_report</code>中得到验证，如下图所示。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/e794fb72f1e12ee78becd9646dd177b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*RayggwFh1W9-DuetN-Emdw.png"/></div></figure><ol class=""><li id="9e28" class="jt ju hh ig b ih ii il im ip jv it jw ix jx jb kv jz ka kb bi translated"><strong class="ig hi">准确率</strong>:案例分类正确的比例。<em class="kw"> (TP+TN)/总记录数=(1057+768)/2750 = 0.66</em></li><li id="5580" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated"><strong class="ig hi">精度</strong>:预测的1实际上是1的比例。<em class="kw">TP/(TP+FP)= 768/(768+45)= 0.94</em></li><li id="8765" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated"><strong class="ig hi">回忆</strong>(又称<strong class="ig hi">灵敏度</strong>):正确分类的1的比例。<em class="kw">TP/(TP+FN)= 768/(768+880)= 0.47</em></li><li id="29a5" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated"><strong class="ig hi">特异性</strong>:正确分类的0的比例。<em class="kw">TN/(TN+FN)= 1057/(1057+880)= 0.54</em></li><li id="f74d" class="jt ju hh ig b ih kc il kd ip ke it kf ix kg jb kv jz ka kb bi translated"><strong class="ig hi">患病率:</strong>1s实际发生的比例。(TP+FN) <em class="kw"> /总记录=(768+880)/2750 = 0.5992</em></li></ol><div class="lc ld ez fb le lf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lg ab dw"><div class="lh ab li cl cj lj"><h2 class="bd hi fi z dy lk ea eb ll ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lm l"><h3 class="bd b fi z dy lk ea eb ll ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ln l"><p class="bd b fp z dy lk ea eb ll ed ef dx translated">medium.com</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt jm lf"/></div></div></a></div><p id="af06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="ig hi">成为作家</strong> </a></p></div></div>    
</body>
</html>