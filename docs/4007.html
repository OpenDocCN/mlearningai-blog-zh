<html>
<head>
<title>Gradient Descent, all you need to know!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降，你需要知道的！！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/gradient-descent-all-you-need-to-know-ba358c838e82?source=collection_archive---------5-----------------------#2022-11-23">https://medium.com/mlearning-ai/gradient-descent-all-you-need-to-know-ba358c838e82?source=collection_archive---------5-----------------------#2022-11-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="045b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">用线性回归的例子解释梯度下降。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/3703cf6c22d4b94f3389b665116cafd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbzy3PYnXmMzlUVuc8ztBQ.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image from <a class="ae jm" href="https://unsplash.com/photos/hkhCV41gOpA" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="7a9f" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">这是什么？</h2><p id="5013" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> G </span>梯度下降是一种优化算法，能够找到各种问题的最优解。</p><p id="6842" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">梯度下降的主要功能是最小化成本函数。</p></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><p id="ff5b" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">这个定义很难理解，对吧？！好吧，那就简单点，用一个例子来理解。</p><p id="8a7c" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">假设我们有两个变量的数据，<code class="du lz ma mb mc b">x</code>和<code class="du lz ma mb mc b">y</code>，其中<code class="du lz ma mb mc b">x</code>是自变量，<code class="du lz ma mb mc b">y</code>是<code class="du lz ma mb mc b">x</code>的因变量。所以我们的目标是预测给定<code class="du lz ma mb mc b">x</code>的<code class="du lz ma mb mc b">y</code>。假设数据如下所示。这里，我们考虑的是<strong class="kn hi">线性回归</strong>问题。所以我们得到了那条线，这样我们就可以预测任何<code class="du lz ma mb mc b">x</code>的<code class="du lz ma mb mc b">y</code>。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es md"><img src="../Images/0376f8d1f487dca977d438a3a67bfcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9QehL4kwUwPI27SJTTwlTg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Linear data</figcaption></figure><p id="fff4" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">如果你不知道什么是线性回归，请点击这里查看:</p><div class="me mf ez fb mg mh"><a href="https://python.plainenglish.io/linear-regression-machine-learning-from-scratch-c9143bb44995" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hi fi z dy mm ea eb mn ed ef hg bi translated">从零开始学习机器学习中的线性回归</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">机器学习中线性回归的初学者教程。</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">python .平原英语. io</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv jg mh"/></div></div></a></div><p id="f933" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">众所周知，直线方程是<code class="du lz ma mb mc b">y = mx + c</code>。这里，<code class="du lz ma mb mc b">m</code>是直线的斜率，<code class="du lz ma mb mc b">c</code>是与<code class="du lz ma mb mc b">Y</code>的交点。因此对于线性回归，<strong class="kn hi">假设函数</strong>与此等式相同，如下所示。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mw"><img src="../Images/5a06d727349e4157c877e28320c1b74d.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/0*YXaxLOnqRh132y0m"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Hypothesis function for Linear Regression</figcaption></figure><p id="a0cf" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">这里θ₀和θ₁称之为重量。我们必须找到这些权重，以获得数据的最佳拟合线。</p><p id="21f3" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">但问题是，如何得到这条线，使它能适应数据？为此，首先我们必须找出误差，即实际输出<code class="du lz ma mb mc b">y</code>和预测输出<code class="du lz ma mb mc b">h(x)</code>之间的差异，预测输出是通过我们的假设函数和一些随机权重计算出来的。为此，我们需要一个成本函数来计算这个误差。对于线性回归，<strong class="kn hi">成本函数</strong>定义如下:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mx"><img src="../Images/048b895eb8ea3e18effceb2176ad6065.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/0*8B4xUsnxxvPnk9Rl"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Cost function for Linear Regression</figcaption></figure><p id="d0a0" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">如果你看清楚了，然后你发现这个成本函数什么都不是，但它首先取实际值和预测值的差然后取它的平方然后取所有数据的平均值。所以基本上它是在计算我们的线与数据的吻合程度。如果成本函数的值接近0，那么它是好的，但是如果它是一个大值，那么我们必须改变假设函数的权重，然后再试一次。</p><p id="f2e1" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">那么，我们是否必须随机尝试不同的权重来获得数据的最佳拟合线？？🤔🤔</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="313c" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">不不放松。这时<strong class="kn hi">梯度下降</strong>就来帮忙了。</p><p id="9f2e" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">基本上，我们的目标是尽可能降低成本函数。</p><p id="0010" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">这里，成本函数是θ₀和θ₁.的函数如果你知道一些微分，那么你可以很容易地解读它。我们先了解一下那部分。</p><p id="35ef" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">例如，我们有<code class="du lz ma mb mc b">X</code>和它的功能。现在我们想知道<code class="du lz ma mb mc b">X</code>的值，此时<code class="du lz ma mb mc b">f(X)</code>具有最小值。</p><p id="5dc4" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">众所周知，在某点对函数进行<strong class="kn hi">求导，意味着在该点的斜率值</strong>。所以我们可以说，在这一点上，微分变为0，意味着在这一点上，斜率的值为0，意味着我们得到了斜率的水平线。</p><p id="b491" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">请看下图:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es na"><img src="../Images/5c5f7d0f766a47cc8df97ea7449cf7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snR8uEr6QEkkwPpfGDzjqQ.png"/></div></div></figure><p id="42d9" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">这里你可以看到在值<code class="du lz ma mb mc b">x'</code>处，slop变成了水平。这意味着在那个点上<code class="du lz ma mb mc b">f(X)</code>的值最小。你可以在图像中清楚地看到。此时，<code class="du lz ma mb mc b">f(X)</code>的微分为0。所以我们现在要做的是找到图的最小值。(<em class="nb">可以是</em> <strong class="kn hi"> <em class="nb">局部极小值</em> </strong> <em class="nb">或</em> <strong class="kn hi"> <em class="nb">全局极小值</em> </strong> <em class="nb">)。</em>)</p><p id="7522" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">现在让我们回到梯度下降。</p><p id="afaa" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">我们有θ₀和θ₁作为参数和成本函数，它是θ₀和θ₁.的函数因此，这里我们希望最小化成本函数，以便我们可以获得模型的最佳拟合线。如上例所述，我们必须找到成本函数的微分，在该点上它变成0，我们将获得最佳拟合线的θ₀和θ₁的值。所以我们想找到<strong class="kn hi">极小值</strong>。</p><p id="2f7d" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">所以我们有两个参数和一个成本函数。如果我们将它绘制在图表中，它将是3D的，如下所示。看图片，你会有更好的理解。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nc"><img src="../Images/63a7440709b73391bda94c172ffe1f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-kArSZzIMoXOddzaYP9Jg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Gradient Descent for 2 parameters</figcaption></figure><p id="cd4f" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">这里我们有θ₁的θ₀和θ₁的J(θ₀。现在取θ₁θ₀的一个随机值，并计算其成本函数。在图表中设置该值，假设图表中显示的点是<code class="du lz ma mb mc b">A</code>。我们希望那个点在最小值，在点<code class="du lz ma mb mc b">A'</code>。您可以看到，在点<code class="du lz ma mb mc b">A'</code>处，斜率变为0，对于θ₀，θ₁成本函数具有最低值，这意味着我们的数据具有最佳拟合线。</p><p id="b1b9" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">现在的问题是，如何到达那个点？</p><p id="58d7" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">程序是在一点上第一个计算斜率。之后，朝那个方向走下去。重复这个步骤，直到我们得到slop 0的值。</p><blockquote class="nd ne nf"><p id="589b" class="kl km nb kn b ko ln ii kq kr lo il kt ng lp kv kw nh lq ky kz ni lr lb lc ld ha bi translated">F <!-- -->或者举例，假设你在浓雾中迷失在群山之中，你只能感觉到脚下地面的坡度。快速到达底部的一个好策略是朝着最陡的斜坡方向下山。</p></blockquote><p id="0c53" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">让我们看看梯度下降方程，这样你就容易理解了。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nj"><img src="../Images/2c458da7e2bc7541fb9c5f0a4c333fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*X0pEZBqfq6spGFdtuSgm5Q.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Equation for Gradient Descent</figcaption></figure><p id="45c5" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">我认为除了α(阿尔法)；你知道所有其他参数。这个α称为<strong class="kn hi">学习率。</strong></p><p id="8b2e" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">学习率(也称为步长或alpha)是达到最小值所需的步长。这通常是一个很小的值，根据成本函数的行为对其进行评估和更新。</p><p id="1be5" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">所以现在，如果你看到这个等式，你就能理解它是如何工作的。对于任何θ，它将首先计算斜率，然后将该斜率乘以学习率，使其成为小值，然后从原始θ中减去该值，并用我们得到的值替换θ。这个过程不断重复，直到找到最小值或收敛。基本上，它将θ移动到局部或全局最小值。</p><p id="b7a9" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">如果α很小，那么算法会经历多次迭代，耗费大量时间。<br/>如果α很高，你可能会跳过山谷，这可能会使算法发散，无法找到最佳解决方案。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/9dc8acb223082a5a8467f192f9dfd49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faRLFdqpm_zvnDlDZfuX2w.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Learning rate</figcaption></figure></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><h2 id="54a1" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">梯度下降的类型</h2><p id="733f" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">有3种类型的梯度下降:</p><ol class=""><li id="5366" class="nl nm hh kn b ko ln kr lo jy nn kc no kg np ld nq nr ns nt bi translated"><strong class="kn hi">批量梯度下降</strong> <br/>它计算训练集中每个例子的误差。在对所有进行评估后，它会更新模型参数。它的计算效率很高，可以产生稳定的误差梯度和收敛性，但是它需要在内存中有一个完整的训练集。</li><li id="3074" class="nl nm hh kn b ko nu kr nv jy nw kc nx kg ny ld nq nr ns nt bi translated"><strong class="kn hi">随机梯度下降</strong> <br/>根据单个训练样本的误差梯度更新参数。<br/>比批量梯度下降快。频繁的更新提供了详细的改进率，但是这些更新更昂贵。</li><li id="c5cd" class="nl nm hh kn b ko nu kr nv jy nw kc nx kg ny ld nq nr ns nt bi translated"><strong class="kn hi">小批量梯度下降</strong> <br/>它将训练集分成小批量，并对这些批量中的每一个执行更新。<br/>它平衡批次&amp;随机梯度下降，因为它使用两者的组合。</li></ol></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><h2 id="e100" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">梯度下降挑战</strong></h2><ul class=""><li id="38ec" class="nl nm hh kn b ko kp kr ks jy nz kc oa kg ob ld oc nr ns nt bi translated">不是所有的成本函数看起来都不像一个规则的碗。在左侧，可能是它卡在了局部最小值。在右侧，需要很长时间才能达到全局最小值。</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es od"><img src="../Images/fe902cf3fcd2bc5b30913c63f9f961ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvrwjAQmjUMHtMvzQ1AQRg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Local Minima and Plateau problem</figcaption></figure><ul class=""><li id="4b2d" class="nl nm hh kn b ko ln kr lo jy nn kc no kg np ld oc nr ns nt bi translated"><strong class="kn hi">消失渐变<br/> </strong>当渐变过小时会出现这种情况。当我们在反向传播过程中向后移动时，梯度继续变小，导致网络中的早期层比后期层学习得更慢。发生这种情况时，权重参数会更新，直到它们变得不重要。</li><li id="d44f" class="nl nm hh kn b ko nu kr nv jy nw kc nx kg ny ld oc nr ns nt bi translated"><strong class="kn hi">爆炸梯度<br/> </strong>当梯度过大时会发生这种情况，产生不稳定的模型。在这种情况下，模型权重将变得过大，并且它们最终将被表示为NaN。这个问题的一个解决方案是利用降维技术，这有助于最小化模型中的复杂性。</li></ul><p id="737b" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">梯度下降就是这样！</p></div><div class="ab cl ls lt go lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ha hb hc hd he"><p id="d622" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">感谢阅读！如果你喜欢，那就给它鼓掌，分享一下。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="9b73" class="pw-post-body-paragraph kl km hh kn b ko ln ii kq kr lo il kt jy lp kv kw kc lq ky kz kg lr lb lc ld ha bi translated">关注更多关于<a class="ae jm" href="https://kishanmodasiya.medium.com/" rel="noopener"> <strong class="kn hi">中</strong> </a>的内容，我会很快分享更多机器学习的东西。这里是我的<a class="ae jm" href="https://twitter.com/kishumds" rel="noopener ugc nofollow" target="_blank"> <strong class="kn hi"> Twitter </strong> </a>，关注，在那里和我联系，随时DM。</p><div class="me mf ez fb mg mh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hi fi z dy mm ea eb mn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">medium.com</p></div></div><div class="mq l"><div class="oe l ms mt mu mq mv jg mh"/></div></div></a></div></div></div>    
</body>
</html>