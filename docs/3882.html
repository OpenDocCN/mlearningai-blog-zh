<html>
<head>
<title>Demystifying Graph based Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开基于图的机器学习的神秘面纱</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/demystifying-graph-based-machine-learning-ed6b6b7c4081?source=collection_archive---------0-----------------------#2022-11-03">https://medium.com/mlearning-ai/demystifying-graph-based-machine-learning-ed6b6b7c4081?source=collection_archive---------0-----------------------#2022-11-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="e8cf" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">以及为什么你不能不使用图表</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/f0b20f1a76a637cb90e23324ebe04757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0mf1uMVEj1CzmctfaGDVg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image Created by <a class="ae jm" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/dall-e-2/</a></figcaption></figure><p id="d874" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi kj translated">我花了很长时间来尝试图形学习…我可能患上了数据科学家中最常见的流行病:找到一把锤子，四处寻找更多的钉子来敲打…当你进行深度学习时，一切看起来都像一个漂亮的钉子！</p><p id="4b31" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">更糟糕的是，当我还是一名计算机科学博士生时，我在机器学习(ML)实验室工作时，选修了一系列高级图算法课程……但我从未想过图如何有利于机器学习，更不用说彻底改变它了……公平地说，那是10多年前，那时深度学习还没有兴起，更不用说GNNs了……但仍然…</p><p id="dfdb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">主要的问题是，在机器学习的经典学派中，数据的表格表示总是被假定，因此，重点是设计最好的特征工程和模型架构；所以我从来没有质疑过表格表示法是否能捕捉和表示数据中的所有信号；遗憾的是，我关于机器学习的全部知识都基于一个隐含的假设，即你如何表示数据不会影响模型的性能。</p><p id="d980" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">什么本吉奥等人。al在10年前说过“……<em class="ks">机器学习算法的成功一般取决于数据表示方式……因为不同的表示方式可以或多或少地纠缠和隐藏数据背后变异的不同解释因素。</em>“[1]真正抓住了过去十年机器学习的根本革命:表征学习。</p><p id="6db0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">嵌入将数据从稀疏的高维空间映射到密集的低维空间，同时保持数据点之间的关系，这可能是表征学习最著名的例子。但是图形表示与我们常规的特征化或嵌入有什么不同呢？</p><h2 id="ad53" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak"> <em class="lo">数据表示图:以</em> </strong>为例</h2><p id="dd3d" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di"> L </span> et来看一个简单的例子:CORA引文网络[2]是一个包含一堆论文及其引文的数据集:它由2708个节点组成，<strong class="jp hi"> </strong>其中每个节点代表一个文档/论文，边代表2个文档之间通过引文的联系。每个文档还有一个单词包表示，表示文档中存在一个单词(<strong class="jp hi"> 1433 </strong>单词词汇表)。虽然词袋是论文的稀疏表示，但是文档嵌入(例如，Doc2Vec)可以给我们密集表示，这对于基于距离的算法更友好。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lu"><img src="../Images/6d2d171b891b034928d5b5efef1caee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*DwnVJ5KBo4kO5lLwrCQ_qg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 1. Example Subgraph of a Citation Network</figcaption></figure><p id="efaa" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在图1中，我们看到了6篇论文，分为计算机科学(C)和物理学(P)两组。如果我们要猜测论文1和6的标签，一种方法是使用简单的ML模型，使用其他仅基于文档的单词包表示的标签示例(或者甚至获得爱好者并为这些单词和文档创建嵌入)，但是我们真的需要这样做吗？仅仅通过查看节点之间的关系，而不是节点的特征，我们就可以将这两篇论文分配到相应的组中，也许甚至不需要查看它们的单词内容。</p><p id="5f3c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">节点之间的关系是图中的一等公民，其中在表格表示中通常需要根据两个节点的特征值之间的相似性来推断；这个例子可能过于简单，但你得到了要点:基于图的方法具有不公平的优势，即除了节点的特定特征之外，还能够使用网络的全局和局部结构来学习:</p><p id="3a9b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="ks">图信号=单个节点特征+图结构</em> </strong></p><p id="291a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">图形也可以表示<strong class="jp hi"> <em class="ks">异构数据点</em> </strong>并包含在它们的各种算法中；例如，在上面的例子中，假设我们也想表示作者:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lv"><img src="../Images/d7475a9a06b6ef14cc2fc821c1ec8394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gYzeggW3BubRltoV5zxbQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 2. adding a different node type (author) creates a heterogeneous graph</figcaption></figure><h2 id="5065" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak"> <em class="lo">但是学习呢？</em>T11】</strong></h2><p id="4b35" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi translated">但是ML如何从中受益呢？许多标准的ML算法不需要表格数据吗？答案是肯定的，ML方法仍然可以通过以下方式利用图形丰富的表示:</p><p id="c6d7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">—创建<strong class="jp hi"> <em class="ks">图形嵌入</em> </strong>以同时捕获节点特征和图形结构，并像使用表格特征一样使用它们</p><p id="2c26" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">—使用<strong class="jp hi"> <em class="ks">图形原生学习</em> </strong>算法</p><p id="8092" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">——使用<strong class="jp hi"> <em class="ks">图形神经网络</em> </strong></p><h2 id="8fc2" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak"> <em class="lo">图形嵌入</em> </strong></h2><p id="02d2" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi kj translated">图嵌入的目的是将图空间中的相似性映射到欧几里德空间中的相似性，你可能会说，我们为什么不用邻接矩阵来表示一个图结构呢，这不是已经在欧几里德空间中了吗？是的，但这种表示是非常高维度和稀疏的，我们都知道维数灾难…这也是我们更喜欢将词袋转换为文本嵌入的确切原因:从高维度稀疏表示到低维度密集表示。</p><p id="52ce" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">但是我们还能如何对图形结构进行编码呢？我们随便走走吧…</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lw"><img src="../Images/ed563e42a183b63d8688617c7150852a.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*TZ_dqUkRT0uzxr770GdHuQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 3. Random Walks (image created by https://huggingface.co/spaces/dalle-mini)</figcaption></figure><p id="80a5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">想象一下，我们问图中的作者。1在图上走一些路，在随机的路径上，现在你能仅仅通过看那些走过的路径来判断作者a是否比作者d离作者b更近吗？没错！如果两个节点在图上靠得更近，它们在随机行走时可能会共享更多的路径。量化这一概念的一种方法是将遍历转化为被访问节点的序列，并应用word2vec算法来创建这些节点序列的嵌入(而不是单词序列)…很棒吧？除了节点，我们还可以嵌入边和子图，但是节点嵌入是最实用的图嵌入类型。</p><p id="854c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">实际上，我们的图有加权的边，所以行走不是完全随机的:行走者选择一条边而不是另一条边的几率与这两条边的权重比成正比…这被称为有偏随机行走。</p><p id="dc63" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Node2Vec </strong>算法【3】只是把上面描述的步骤(有偏的随机行走后面跟着一个Word2Vec)打包在一把保护伞下。虽然Node2Vec可能是最直观的节点嵌入算法，但它不是唯一的方法:快速随机投影(FastRP) [4]是一种基于Johnsson-Lindenstrauss引理的节点嵌入算法，根据该引理，可以将<em class="ks">任意</em>维的<em class="ks"> n </em>个向量投影到<em class="ks"> O(log(n)) </em>维，并且仍然保持点之间的成对距离:即使是随机线性投影也可以满足这一特性。随着我们增加遍历的长度，Node2Vec可能会变得非常慢，fastRP可能是一个快速的替代方案，尽管选择最佳迭代权重并不总是直观的。</p><p id="7102" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当然，除了边或甚至图/子图嵌入之外，还有更多节点嵌入算法，但它们并不常见。</p><h2 id="9a0a" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated">图形原生学习</h2><p id="f789" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi kj translated">基于图的学习算法使用图结构进行学习。众所周知的图形本地算法有:</p><p id="339b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">中心性检测:</strong>利用图的结构来评估图中不同节点的重要性。PageRank可能是最著名的中心性算法，它基于连接的源节点和输入边的重要性迭代地更新每个节点的重要性。谷歌用它来检索与查询相关的重要网页。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lw"><img src="../Images/de5c4e49778d52c9bffe8ecb8b0e688c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*zJEXOMmX3C3COkFXoIfUAg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 4. Centrality Detection (image created by <a class="ae jm" href="https://huggingface.co/spaces/dalle-mini" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/dalle-mini</a>)</figcaption></figure><p id="21db" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">社区检测:</strong>顾名思义，这些算法寻找图中节点的自然分组。这个组中的一个众所周知的算法是标签传播，其在整个图中传播任意节点标签，并通过其邻居的大多数标签来更新每个节点的标签。收敛后，一个图中密集连接的部分往往具有相同的标号。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lw"><img src="../Images/95b82b7e7fe17a9b98c9ee910cc12cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*LLl7Nj-Q8RP9S5p40x3DLA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 5. Community Detection (image created by <a class="ae jm" href="https://huggingface.co/spaces/dalle-mini" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/dalle-mini</a>)</figcaption></figure><p id="c7dd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">节点相似性算法:</strong>这些算法使用图形结构和/或节点属性来估计图形中两个节点的相似性。这也与确定两个节点是否应该连接的拓扑链路预测算法有关。</p><p id="4a3c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当然，还有更多的图形学习算法，如路径查找、拓扑链接预测或最小生成树，它们可能位于ML和优化算法的交叉点。</p><h2 id="5ae8" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak">图形神经网络</strong></h2><p id="b82c" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di">G</span>nn是深度学习模型，旨在捕捉图形结构和常规节点特征。你可能会说，递归神经网络(RNNs)和卷积神经网络(CNN)已经在网络内编码某种级别的结构，那么GNNs有什么大不了的？大多数真实世界的网络没有固定大小的输入维度，甚至也没有固定大小的邻域，此外节点排序也不固定；与rnn和CNN不同，gnn是排列不变的，并且也适用于任意输入和邻域大小。我们需要一个完整的研究生课程来涵盖GNN，但是这里将回顾它们最重要的操作，过滤和池，以及一个流行的GNN架构:GraphSAGE。</p><p id="727b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="ks">过滤:</em> </strong>图形过滤创建一个新图形，具有相同数量的节点和边，但具有新的节点特征集。过滤层也被称为<strong class="jp hi">置换等变</strong>层，因为它将图形表示映射为同一图形的更新表示。在大多数情况下，这是通过成对的<strong class="jp hi">消息在图节点之间传递</strong>来实现的，其中节点<em class="ks">通过<em class="ks">聚合</em>从其直接邻居接收的<em class="ks">消息</em>来更新</em>它们的表示。因此，每个消息传递层都会将GNN的范围增加一跳。</p><p id="e23a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="ks">池化:</em> </strong>图形池化创建一个节点较少的新层，可以是局部的，也可以是全局的。本地池类似于节点的下采样，通常通过选择最重要的节点或将节点聚集到较少的组中来实现。另一方面，全局池层提供整个图形的固定大小表示，并且必须是<strong class="jp hi">排列不变的</strong>。正如您可能已经猜到的，GNNs的最后一层通常是全局池层。</p><p id="df49" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">当然，除了过滤和池层之外，还有激活功能来将非线性注入网络，但这与任何其他深度学习架构几乎相同。节点级任务(节点分类或嵌入)的gnn通常包含过滤和激活，而图级任务(图分类)包含所有3层。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lx"><img src="../Images/f781c10cbe02d96ad5412a3144aba39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2G16bVSRCyTppEn4cMRyLg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 6. GNN as a combination of filtering and pooling layers and activation functions</figcaption></figure><h2 id="baec" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak"><em class="lo"/></strong></h2><p id="9703" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi translated">GraphSAGE [5]是一个简单但有效的归纳框架，它使用邻域采样和聚合来为大型图创建新的节点级表示(嵌入)。邻域采样是一种聪明的策略，它在图中的不同节点上创建相同大小的邻域馈送，并将图学习的其他直推设置转换为归纳设置。换句话说，GraphSAGE不是为每个节点学习一个单独的嵌入，而是学习一个采样和聚集节点的本地邻域并可以在所有节点之间共享的函数。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ly"><img src="../Images/30762fb98f5d4845b487798c45dc661d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PO4qFfCchZkSBV4VU8tpGA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig 7. GraphSAGE sample and aggregate approach (figure from the original paper [5])</figcaption></figure><p id="5cff" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">换句话说，GraphSAGE提供了一种归纳过滤策略，该策略使用节点先前的状态和节点邻居的随机样本来更新节点级特征。</p><p id="2970" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">GraphSAGE的另一个好处是，它可以用于监督和非监督学习任务。当用作无监督算法时，它甚至可以为异构图创建节点嵌入(参见HinSAGE [6])。</p><h2 id="058a" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated"><strong class="ak">结论</strong></h2><p id="4bae" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi translated">在这篇博客中，我想从10，000英尺的角度介绍基于图的ML，但没有触及一些重要的领域，包括图的谱表示(拉普拉斯矩阵和所有有趣的矩阵代数！)，其他类型的gnn或示例python代码。但即使没有这些，你也应该能够理解为什么图表是表征学习中最热门的话题，以及为什么你应该考虑尝试一下，如果你还没有的话。</p><p id="339d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果你觉得这很有帮助，并且有兴趣在另一个博客上扩展我在这里谈到的内容，请告诉我，我会确保优先考虑它。</p><h2 id="0a89" class="kt ku hh bd kv kw kx ky kz la lb lc ld jw le lf lg ka lh li lj ke lk ll lm ln bi translated">参考</h2><p id="6811" class="pw-post-body-paragraph jn jo hh jp b jq lp ii js jt lq il jv jw lr jy jz ka ls kc kd ke lt kg kh ki ha bi translated">[1] Y .本吉奥，A .库维尔和p .文森特，“表征学习:回顾和新观点”，载于《IEEE模式分析和机器智能汇刊》，第35卷，第8期，第1798-1828页，2013年8月，doi: 10.1109/TPAMI.2013.50</p><p id="7717" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">[2]<a class="ae jm" href="https://relational.fit.cvut.cz/dataset/CORA" rel="noopener ugc nofollow" target="_blank">https://relational.fit.cvut.cz/dataset/CORA</a></p><p id="c1f7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">[3] Grover，Aditya和Leskovec，Jure。" node2vec:网络的可扩展特征学习."在2016年第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集会议上发表的论文。</p><p id="4d8b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">[4] H. Chen，S.F. Sultan，Y. Tian，M. Chen，S. Skiena:通过非常稀疏随机投影实现快速准确的网络嵌入，2019年。</p><p id="bf39" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">[5] W. L. Hamilton、R. Ying和J. Leskovec，“大型图上的归纳表示学习”，arXiv.org。2017年6月8日。</p><p id="e18e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://stellargraph.readthedocs.io/en/stable/hinsage.html" rel="noopener ugc nofollow" target="_blank">https://stellargraph.readthedocs.io/en/stable/hinsage.html</a></p><div class="lz ma ez fb mb mc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="md ab dw"><div class="me ab mf cl cj mg"><h2 class="bd hi fi z dy mh ea eb mi ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mj l"><h3 class="bd b fi z dy mh ea eb mi ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mk l"><p class="bd b fp z dy mh ea eb mi ed ef dx translated">medium.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq jg mc"/></div></div></a></div></div></div>    
</body>
</html>