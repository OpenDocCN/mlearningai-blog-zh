<html>
<head>
<title>Baum-Welch algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">鲍姆-韦尔奇算法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/baum-welch-algorithm-4d4514cf9dbe?source=collection_archive---------0-----------------------#2021-03-14">https://medium.com/mlearning-ai/baum-welch-algorithm-4d4514cf9dbe?source=collection_archive---------0-----------------------#2021-03-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="50b6" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">在隐马尔可夫模型训练下</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/e8389ab5d49e6a3b803f9ccd5d579b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y6lhjvtw9zvIYpaEC5rvQg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="https://pixabay.com/photos/chain-link-metal-strong-connect-690088/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="2d41" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h2><p id="cc77" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">这篇文章旨在展示如何使用Baum-Welch算法训练隐马尔可夫模型(hmm)。如果你想了解更多关于隐马尔可夫模型的知识，我建议看一些帖子:</p><ul class=""><li id="3540" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated"><a class="ae jm" href="https://jonathan-hui.medium.com/machine-learning-hidden-markov-model-hmm-31660d217a61" rel="noopener">机器学习—隐马尔可夫模型(HMM) </a></li><li id="7248" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated"><a class="ae jm" rel="noopener" href="/@Ayra_Lux/hidden-markov-models-part-1-the-likelihood-problem-8dd1066a784e">可能性问题</a></li><li id="5259" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated"><a class="ae jm" rel="noopener" href="/@Ayra_Lux/hidden-markov-models-part-2-the-decoding-problem-c628ba474e69">解码HMM </a></li></ul><p id="0f85" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">这篇文章假设你熟悉像跃迁、发射概率、隐藏状态、观察、向前和向后算法这样的概念。</p><h2 id="d38a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">目标</strong></h2><p id="dc30" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">Baum-Welch算法寻找最适合观察数据的HMM参数值。对于培训，我们需要:</p><ul class=""><li id="f45e" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">O₁，O₂，…，Oₙ</li><li id="4e78" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">这些观察的隐藏状态序列S₁，S₂，…，Sₙ</li></ul><p id="e1e3" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">算法尝试使用O和s的值来估计转移矩阵A和发射矩阵B。如果我们知道A和B，我们可以使用HMM和新的未看到的数据来找到与观察值相对应的最可能的隐藏状态序列(这称为解码问题)。</p><h2 id="cd59" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">期望最大化</h2><p id="829e" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">Baum-Welch算法使用<a class="ae jm" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a> (EM)方法来计算A和b的值。流程如下:</p><ul class=""><li id="3882" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">用一些初始值初始化A和B(只做一次)</li><li id="2430" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">使用a、b、o和s估计潜在变量ξᵢⱼ(t和γᵢ(t)(我将在后面解释这些变量的含义)。这一步的目标是估计每个转换和排放使用了多少。这是评估步骤</li><li id="dfea" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">使用前面步骤中的估计值(潜在变量)最大化A和B的值。这是最大化步骤</li><li id="a89f" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">继续前面的步骤，直到收敛</li></ul><h2 id="066a" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">初始方程</h2><p id="f20b" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">为了估计A和B值，我们可以使用以下公式(<a class="ae jm" href="https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>):</p><ul class=""><li id="a59c" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">A = aᵢⱼ(从隐藏状态I转换到隐藏状态j的概率)=从隐藏状态I转换到状态j的预期次数/从隐藏状态I转换的预期次数</li><li id="1ce1" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">B = bⱼₖ(在隐藏状态下观察观察Oₖ的概率j)=模型在隐藏状态下的期望次数j而我们在隐藏状态下观察Oₖ/的期望次数j</li></ul><p id="47e7" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">给定观察序列o和模型(<a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">源</a>)，aᵢⱼ可以被定义为在时间t处于隐藏状态I和在时间t+1处于隐藏状态j的概率。从图形上看，它可以描述如下:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lx"><img src="../Images/8a2a17e6f294dc9bc4cf61b103ec142f.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*n91Dcd8QRiYu3rGlwMVXTA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Current step probability with forward, backward and emission probability. Inspiration from <a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="990b" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">在图中，我们在时间t，我们知道我们在当前隐藏状态Sᵢ的概率(这是前向概率αᵢ(t)，我们知道从隐藏状态Sⱼ到序列末端的隐藏概率，使用后向概率βᵢ₊₁(t).我们想得到从Sᵢ到Sⱼ的概率，假设我们在t+1时刻观察到Sⱼ的Oₜ₊₁。</p><p id="86ba" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">这里我们将利用潜变量ξᵢⱼ(t)和γᵢ(t):</p><ul class=""><li id="8ef7" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated"><strong class="kn hi">ξᵢⱼ(t)</strong>-在给定的观测值下，在时间t从隐藏状态I转移到隐藏状态j的概率:</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ly"><img src="../Images/7ac4228d05c7cb3c5c647a87ec79f5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MfZmuQ4qnZr6wTSXoQaEbw.gif"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="https://jonathan-hui.medium.com/machine-learning-hidden-markov-model-hmm-31660d217a61" rel="noopener">source</a></figcaption></figure><p id="3ac9" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">注意，分母P(O |θ)表示在给定模型θ的情况下，观察序列O通过任何路径的概率。ξᵢⱼ(t)仅针对时间t来定义。我们必须对所有时间步长求和，以获得从隐藏状态I到隐藏状态j的所有转换的总联合概率(计算aᵢⱼ).这将是我们aᵢⱼ.方程的分子对于分母，我们可以使用边际概率，这意味着在时间t处于状态I的概率，整个方程具有以下形式:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lz"><img src="../Images/e4fb99f9667e5ec07e7a56ee0875d7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/1*YLRGdWIR-mnNd5SHPUJIqQ.gif"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="61a7" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">分母可以有不同的表达，并导致一个新的潜在变量γᵢ(t):</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ma"><img src="../Images/459f1e5f38f4cf83aac3609fe63c7722.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/1*jOuE7zlBqGEVEA09WgjUog.gif"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><ul class=""><li id="a27b" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated"><strong class="kn hi"/>——给定状态I在时间t给定观测值下的概率。我们可以用它来计算aᵢⱼ(我们之前对aᵢⱼ的公式也是有效的):</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mb"><img src="../Images/122b3087592d6d06805f7f5ab9916b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/1*KtwqTgpGbsxiiEl_y6ruMw.gif"/></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="9d24" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">我们可以使用γᵢ(t)来计算bⱼₖ(它是来自给定隐藏状态j的观测值o的观测值Oₖ的概率):</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mc"><img src="../Images/a9d86106e79dd8fae81d28472068f52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/1*YpkDZ2AEUfl9v-PwpUychg.gif"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><a class="ae jm" href="https://jonathan-hui.medium.com/machine-learning-hidden-markov-model-hmm-31660d217a61" rel="noopener">source</a></figcaption></figure><p id="998e" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">注意，1ₒₜ₌ₖ是一个指示函数，如果观测值Oₜ属于类k，则该函数的值为1，如果不属于类k，则该函数的值为0。</p><p id="94fa" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated"><strong class="kn hi">HMM中的期望和最大化</strong></p><p id="b3f5" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">现在我们有了分别计算分量的方程，我们可以形成EM方法。我们有两个步骤:</p><ul class=""><li id="9d8a" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">计算潜在变量ξᵢⱼ(t和γᵢ(t).的期望值首先，我们将随机初始化A和B，或者使用一些以前的知识，如果我们有的话</li><li id="79bd" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">使用aᵢⱼ和bⱼₖ.的等式，最大化a和b的值并且通过使用用于估计ξᵢⱼ(t和γᵢ(t).的新的a和b值来进行下一轮</li></ul><p id="ef7f" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">这就像鸡和蛋的问题。我们只有观察，我们从随机猜测开始(或者如果我们有更多的信息，我们可以使用它)。我们估计我们的潜在变量，我们将使用它来最大化A和B。在每一步，我们应该得到更多更好的估计A和B，直到改进很小，算法已经收敛。</p><p id="8d9f" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated"><strong class="kn hi"> Python实现</strong></p><p id="8f2e" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">拥有方程是很好的，但是用代码实现可以更好地理解重要的细节是什么，以及整个过程是什么样子的。这里我将首先给出和实现，然后解释是怎么回事。完整代码可在<a class="ae jm" href="https://github.com/RRisto/learning/blob/master/markov_chain_learn/baum_welch.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得，大部分是从<a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">这里</a>借用的。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="5c3f" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">让我们来看看这个函数:</p><ul class=""><li id="191c" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">第一行只是初始化(注意这个函数期望A和B(对应于前面部分中的A和B)用一些(随机)值初始化)。<code class="du mf mg mh mi b">M = a.shape[0]</code>是隐藏状态的个数。<code class="du mf mg mh mi b">T = len(O)</code>是我们观察到的时间步数(观察次数)</li><li id="45cf" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">接下来我们将开始迭代<code class="du mf mg mh mi b">n_iter</code>的迭代次数</li><li id="c879" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">在每次迭代中，我们将计算阿尔法(αᵢ(t)-forward概率)和贝塔(βⱼ(t+1)-backward概率)。我们将对所有时间步长做一次:</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="e54a" class="jn jo hh mi b fi mn mo l mp mq">alpha = forward(O, a, b, initial_distribution)<br/>beta = backward(O, a, b)</span></pre><ul class=""><li id="1446" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">我们必须初始化一个变量<code class="du mf mg mh mi b">xi</code>，它将保存ξᵢⱼ(t):的值</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="0cf1" class="jn jo hh mi b fi mn mo l mp mq">xi = np.zeros((M, M, T - 1))</span></pre><p id="1647" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated"><code class="du mf mg mh mi b">xi</code>是三维矩阵/阵列(或者我们可以说是张量)它的维度有如下含义。一侧是状态I，另一侧是状态j和时间T的一维(示例假设有两个唯一的隐藏状态和两个时间步长):</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/c5b3a385a59ddad5f8864bd29568d13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*KE1Mm97TIHOKd6YTScxczg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Dimensions of variable xi</figcaption></figure><p id="2c7c" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">对于每个时间步长t，将有一个从状态I转换到j的概率矩阵/阵列(给定的观察值)</p><ul class=""><li id="84c7" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">如果我们进入循环<code class="du mf mg mh mi b">for t in range(T-1)</code>，我们开始计算<code class="du mf mg mh mi b">xi</code>的值。使用<code class="du mf mg mh mi b">T-1</code>,因为我们在具有T个时间步长(观察值)的序列中有<code class="du mf mg mh mi b">T-1</code>个转换。首先，我们将计算ξᵢⱼ(t的分母(记住，分母是指给定模型的任何路径的观测序列<code class="du mf mg mh mi b">O</code>的概率):</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="14af" class="jn jo hh mi b fi mn mo l mp mq">denominator = (alpha[t,:].T @ a * b[:, O[t + 1]].T) @ beta[t + 1, :]</span></pre><p id="9ffb" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">我们取时间t的α(前向概率)和转移概率(矩阵<code class="du mf mg mh mi b">a</code>)之间的点积，乘以时间t的观测值<code class="du mf mg mh mi b">O</code>的发射概率，最后我们取β(后向概率)的点积。</p><ul class=""><li id="c23b" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">下一部分涉及计算ξᵢⱼ(t的分子)并将分子除以分母:</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="42f6" class="jn jo hh mi b fi mn mo l mp mq">for i in range(M):                <br/>    numerator = alpha[t,i] * a[i,:] * b[:,O[t+1]].T * beta[t+1,:].T                <br/>    xi[i, :, t] = numerator / denominator</span></pre><p id="741f" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">请注意，分子的计算与分母的计算非常相似。我们现在关心的是特定的状态<code class="du mf mg mh mi b">i</code>，而不是在时间<code class="du mf mg mh mi b">t</code>获取所有状态的α和<code class="du mf mg mh mi b">a</code>。我们在每个时间步<code class="du mf mg mh mi b">t</code>循环它们。对于每个时间步长，我们将分子除以分母，并将结果存储在变量<code class="du mf mg mh mi b">xi</code>中。</p><ul class=""><li id="2333" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">接下来，我们将通过在维度1(在γᵢ(t是j)上求和来计算伽马(γᵢ(t)的真值:</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="c4fe" class="jn jo hh mi b fi mn mo l mp mq">gamma = np.sum(xi, axis=1)</span></pre><ul class=""><li id="b66a" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">现在我们可以做一个最大化步骤。这直接来自aᵢⱼ的公式，其中我们对维度2(在时间步长上)上的<code class="du mf mg mh mi b">xi</code>求和，并将其除以维度1上的【γᵢ(t】的求和(在时间步长上，伽马已经少了一个维度，因为之前对维度1上的<code class="du mf mg mh mi b">xi</code>求和):</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="ac79" class="jn jo hh mi b fi mn mo l mp mq">a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))</span></pre><ul class=""><li id="2919" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">接下来，我们将添加额外的元素到伽玛。这是需要的，因为我们已经为<code class="du mf mg mh mi b">T-1</code>时间步长计算了伽马，但是我们需要t个发射概率(bⱼₖ)(例如，如果我们有3个观测值，我们将有两个状态之间的跃迁和3个来自隐藏状态的发射概率)。我们将再次添加倒数第二个元素作为最后一个元素:</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="7567" class="jn jo hh mi b fi mn mo l mp mq">gamma=np.hstack((gamma, np.sum(xi[:,:,T-2],axis=0).reshape((-1,1))))</span></pre><ul class=""><li id="a12e" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">计算bⱼₖ的最后准备工作包括设置参数k，它表示唯一观测值的数量。此外，我们还将计算分母，这涉及到在维度1上对gamma求和(跨时间步长):</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="119b" class="jn jo hh mi b fi mn mo l mp mq">K = b.shape[1]<br/>denominator = np.sum(gamma, axis=1)</span></pre><ul class=""><li id="633f" class="le lf hh kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">最后，我们可以通过循环唯一的观察值(唯一的O-s)来计算bⱼₖ。这是需要的，因为对于每个唯一的观测值，我们必须在时间步长上对伽马求和(如果观测值有这个值)。注意<code class="du mf mg mh mi b">O == l</code>是指示灯功能。最后一步是分子除以分母:</li></ul><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="fbbc" class="jn jo hh mi b fi mn mo l mp mq">for l in range(K):<br/>            b[:, l] = np.sum(gamma[:, O == l], axis=1)<br/>b = np.divide(b, denominator.reshape((-1, 1)))</span></pre><p id="e7f7" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">我们可以重复这个过程足够长的时间，我们可以近似A和b的精确值。</p><h2 id="9eab" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">使用</h2><p id="136e" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">如何使用我们的Baum-Welch实现的完整示例是这里的<a class="ae jm" href="https://github.com/RRisto/learning/blob/master/markov_chain_learn/baum_welch.ipynb" rel="noopener ugc nofollow" target="_blank"/>。这里我将展示一些代码片段:</p><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="8f24" class="jn jo hh mi b fi mn mo l mp mq">data = pd.read_csv('data_python.csv.txt')<br/>V = data['Visible'].values</span><span id="1123" class="jn jo hh mi b fi ms mo l mp mq"># Transition Probabilities<br/>a = np.ones((2, 2))<br/>a = a / np.sum(a, axis=1)<br/># Emission Probabilities<br/>b = np.array(((1, 3, 5), (2, 4, 6)))<br/>b = b / np.sum(b, axis=1).reshape((-1, 1))<br/># Equal Probabilities for the initial distribution<br/>initial_distribution = np.array((0.5, 0.5))</span><span id="5254" class="jn jo hh mi b fi ms mo l mp mq">#train model<br/>n_iter = 100<br/>a_model, b_model = baum_welch(V.copy(), a.copy(), b.copy(), initial_distribution.copy(), n_iter=n_iter)<br/>print(f'Custom model A is \n{a_model} \n \nCustom model B is \n{b_model}')</span><span id="43ca" class="jn jo hh mi b fi ms mo l mp mq">#Custom model A is <br/>#[[0.53816345 0.46183655]<br/># [0.48664443 0.51335557]]</span><span id="41c2" class="jn jo hh mi b fi ms mo l mp mq">#Custom model B is <br/>#[[0.16277513 0.26258073 0.57464414]<br/># [0.2514996  0.27780971 0.47069069]]</span></pre><p id="f979" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">为了验证结果，我们应该获得与<code class="du mf mg mh mi b">hmmlearn</code>包实施接近的结果:</p><pre class="ix iy iz ja fd mj mi mk ml aw mm bi"><span id="09a6" class="jn jo hh mi b fi mn mo l mp mq">model = hmm.MultinomialHMM(n_components=2, n_iter=n_iter, init_params="")<br/>model.startprob_ = initial_distribution<br/>model.transmat_ = a<br/>model.emissionprob_ = b</span><span id="681d" class="jn jo hh mi b fi ms mo l mp mq">model.fit([V])</span><span id="9c5a" class="jn jo hh mi b fi ms mo l mp mq">print(np.allclose(a_model, model.transmat_, atol=0.1))<br/>print(np.allclose(b_model, model.emissionprob_, atol=0.1))</span><span id="ad46" class="jn jo hh mi b fi ms mo l mp mq">#True<br/>#True</span></pre><p id="8e8b" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy lu kv kw kc lv ky kz kg lw lb lc ld ha bi translated">简单的实现给出了相当相似的结果，这给了我们的Baum-Welch版本工作的信心。</p><h2 id="0b6e" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">摘要</h2><p id="868f" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">这展示了使用Baum-Welch算法训练hmm背后的一些数学知识，并展示了一个python实现。希望这篇文章能让你了解hmm是如何训练的，以及如何用python实现它。</p><h2 id="e96d" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h2><ul class=""><li id="f62f" class="le lf hh kn b ko kp kr ks jy mt kc mu kg mv ld ll lm ln lo bi translated">期望值最大化算法，<a class="ae jm" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">维基百科</a></li><li id="19d4" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">隐马尔可夫模型的Baum Welch算法的推导与实现，Abhisek Jana，<a class="ae jm" href="http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/" rel="noopener ugc nofollow" target="_blank">开发者日记</a></li><li id="89b9" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">隐马尔可夫模型12:鲍姆-韦尔奇算法，威斯蒙特学院，<a class="ae jm" href="https://www.youtube.com/watch?v=JRsdt05pMoI" rel="noopener ugc nofollow" target="_blank"> Youtube </a></li><li id="f5b1" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">隐马尔可夫模型—第一部分:似然问题，<br/>玛利亚·伯兰多，<a class="ae jm" rel="noopener" href="/@Ayra_Lux/hidden-markov-models-part-1-the-likelihood-problem-8dd1066a784e">中等</a></li><li id="a261" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">隐马尔可夫模型——第二部分:解码问题，Maria Burlando，<a class="ae jm" rel="noopener" href="/@Ayra_Lux/hidden-markov-models-part-2-the-decoding-problem-c628ba474e69"> Medium </a></li><li id="0b65" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">机器学习—隐马尔可夫模型(HMM)，乔纳森·惠，<a class="ae jm" href="https://jonathan-hui.medium.com/machine-learning-hidden-markov-model-hmm-31660d217a61" rel="noopener"> Medium </a></li><li id="ce07" class="le lf hh kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">语音识别中的隐马尔可夫模型和选定应用教程，劳伦斯·拉宾纳，<a class="ae jm" href="https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf" rel="noopener ugc nofollow" target="_blank">电气和计算机工程系</a></li></ul></div></div>    
</body>
</html>