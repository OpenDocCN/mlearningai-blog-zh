<html>
<head>
<title>PyTorch basics: The only guide you need to get started — Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch基础知识:入门所需的唯一指南—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/pytorch-basics-the-only-guide-you-need-to-get-started-f8e67b3d056a?source=collection_archive---------5-----------------------#2022-04-17">https://medium.com/mlearning-ai/pytorch-basics-the-only-guide-you-need-to-get-started-f8e67b3d056a?source=collection_archive---------5-----------------------#2022-04-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d131da4fa7446d89d82f8474bac19b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRdFYNeH9HJA7Vv74OS11g.jpeg"/></div></div></figure><p id="1179" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你已经决定学习深度学习，并且已经学了一些理论，现在想做一些动手实践，但是你陷入了从哪个框架开始(Tensorflow还是PyTorch)的两难境地？</p><p id="2103" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">别担心，我也经历过这种情况，这也是我开始写这一系列文章的原因，我会写一系列文章来帮助你开始使用PyTorch！</p><p id="6317" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">那么，我们开始吧！</strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es jn"><img src="../Images/83068f3c0686bbe235c3f21d4f9a870e.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*BBRofVv3XmXjC02I.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://tenor.com/view/lets-get-started-rosanna-pansino-lets-started-lets-make-a-start-lets-begin-gif-19252897" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9548" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">文章的流程:</strong></p><ul class=""><li id="6d6e" class="jx jy hh ir b is it iw ix ja jz je ka ji kb jm kc kd ke kf bi translated">PyTorch简介</li><li id="563b" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">张量</li><li id="f0b6" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">初始化张量</li><li id="2470" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">张量的属性</li><li id="d0c5" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">张量上的运算</li><li id="ca59" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">PyTorch-NumPy互操作性</li><li id="a4e7" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm kc kd ke kf bi translated">GPU/CPU兼容性</li></ul><h1 id="c04f" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">py torch是什么？</strong></h1><p id="68f2" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">PyTorch是一个基于Torch库的开源机器学习框架，用于计算机视觉和自然语言处理等应用，主要由脸书(现为Meta)人工智能研究实验室开发。它为深度学习的科学计算提供了最大的灵活性和速度。还有，<strong class="ir hi">用GPU的力量替代NumPy。</strong></p><h1 id="96f5" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">为什么选择PyTorch？</strong></h1><p id="ef75" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">根据我的研究，TensorFlow、Keras和PyTorch是机器学习社区中提到最多的库。如果您想要开发用于生产的模型，开发需要在移动平台上部署的模型，以及需要使用大规模分布式模型训练，Tensorflow非常适合。</p><p id="0a8d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">而PyTorch很适合用于研究目的，如果你想拥有Pythonic式的一切(比如我😉)!</p><h1 id="6f24" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">张量</h1><p id="ba2c" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">张量是非常类似于数组和矩阵的特殊数据结构。在PyTorch中，我们使用张量来编码模型的输入和输出，以及模型的参数。<strong class="ir hi">tensor类似于</strong> <a class="ae jw" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ir hi"> NumPy的</strong> </a> <strong class="ir hi"> ndarrays，唯一的区别就是可以在GPU和其他硬件加速器上运行。</strong></p><p id="d7d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">比如1d-张量是向量，2d-张量是矩阵，3d-张量是立方体，4d-张量是立方体的向量。</p><p id="cfcf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想直观地理解它，这里有一个惊人的Youtube视频，维滕贝格大学物理系教授丹尼尔·弗莱舍在视频中对此进行了精彩的解释。</p><h2 id="a6af" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">初始化张量:</h2><p id="b6e9" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">让我们看看如何在PyTorch中创建张量。张量可以用各种方法初始化。</p><p id="d615" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们导入必要的库:</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="be29" class="lo km hh md b fi mh mi l mj mk">import torch<br/>import numpy as np</span></pre><ol class=""><li id="8287" class="jx jy hh ir b is it iw ix ja jz je ka ji kb jm ml kd ke kf bi translated"><strong class="ir hi">直接来自数据:</strong></li></ol><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="2d3d" class="lo km hh md b fi mh mi l mj mk">data = [[1, 2],[3, 4]]<br/><strong class="md hi">tensor_data = torch.tensor(data)</strong></span><span id="a722" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:<br/></strong>print(tensor_data)</span><span id="2323" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[1, 2],<br/>        [3, 4]])</span></pre><p id="7b8c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。用随机值创建张量:</strong></p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="f10e" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">random_tensor = torch.Tensor(n,m)</strong>   # where n is the number of rows                 <br/>and m is the number of columns (n x m)</span><span id="6536" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:<br/></strong>random_tensor = torch.Tensor(2,3)<br/>print(random_tensor)</span><span id="48d7" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[6.8664e-44, 7.7071e-44, 1.1771e-43],<br/>        [6.7262e-44, 7.9874e-44, 8.1275e-44]])</span></pre><p id="e8e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 3。用a和b之间的随机值创建一个张量:</strong></p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="3d11" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">uniform_tensor = torch.Tensor(n, m).uniform_(a, b)</strong>   # where a and b are any natural numbers</span><span id="29ca" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:<br/></strong>uniform_tensor = torch.Tensor(3,3).uniform_(1,5)<br/>print(uniform_tensor)</span><span id="ef18" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[2.2433, 3.1906, 4.2577],<br/>        [3.6101, 1.9020, 2.6306],<br/>        [1.2986, 3.6643, 1.4555]])</span></pre><p id="5b59" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 4。从区间[0，1]上的均匀分布创建一个填充了随机数的张量:</strong></p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="1475" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">rand_tensor = torch.rand(n, m)</strong></span><span id="2f39" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:</strong><br/>rand_tensor = torch.rand(3, 3)<br/>print(rand_tensor)</span><span id="5fcc" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[0.6445, 0.5295, 0.8926],<br/>        [0.6845, 0.2812, 0.7323],<br/>        [0.9651, 0.4299, 0.8283]])</span></pre><p id="3d9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 5。创建一个大小为<em class="mn"> n </em> x <em class="mn"> m </em> : </strong>的零张量</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="6c26" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">zero_tensor = torch.zeros(n, m)</strong></span><span id="7afd" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:</strong><br/>zero_tensor = torch.zeros(3, 3)<br/>print(zero_tensor)</span><span id="d303" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[0., 0., 0.],<br/>        [0., 0., 0.],<br/>        [0., 0., 0.]])</span></pre><p id="3b19" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 6。创建大小为<em class="mn"> n </em> x <em class="mn"> m </em> : </strong>的一个张量</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="7059" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">ones_tensor = torch.ones(n, m)</strong></span><span id="d5c6" class="lo km hh md b fi mm mi l mj mk"><br/><strong class="md hi"># Example:</strong><br/>ones_tensor = torch.ones(3, 3)<br/>print(ones_tensor)</span><span id="7adf" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[1., 1., 1.],<br/>        [1., 1., 1.],<br/>        [1., 1., 1.]])</span></pre><h2 id="60ac" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">张量的属性:</h2><p id="8e7b" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">让我们看看如何获得张量的属性，如形状、数据类型以及它们存储在哪个设备上。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="f307" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">tensor = torch.randn(n, m)</strong></span><span id="19dc" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi">print("Shape of a tensor: {}".format(tensor.shape))<br/>print("Data type of a tensor: {}".format(tensor.dtype))<br/>print("Dimension of a tensor: {}".format(tensor.dim()))<br/>print("Device tensor is stored on: {}".format(tensor.device))</strong></span><span id="5b3f" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Example:</strong><br/>tensor = torch.randn(3, 3)</span><span id="6338" class="lo km hh md b fi mm mi l mj mk">print("Shape of a tensor: {}".format(tensor.shape))<br/>print("Data type of a tensor: {}".format(tensor.dtype))<br/>print("Dimension of a tensor: {}".format(tensor.dim()))<br/>print("Device tensor is stored on: {}".format(tensor.device))</span><span id="2ffb" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>Shape of a tensor: torch.Size([3, 3])<br/>Dimension of a tensor: 2<br/>Data type of a tensor: torch.float32<br/>Device tensor is stored on: cpu</span></pre><h2 id="5465" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">张量上的运算:</h2><p id="8955" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">张量上有许多运算，我们将在这里讨论其中最常见和最重要的运算。如果你有兴趣了解所有的操作——在这里<a class="ae jw" href="https://pytorch.org/docs/stable/torch.html" rel="noopener ugc nofollow" target="_blank">讨论它们</a>。</p><ol class=""><li id="955b" class="jx jy hh ir b is it iw ix ja jz je ka ji kb jm ml kd ke kf bi translated"><strong class="ir hi">分度操作:</strong></li></ol><p id="1a26" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用此操作，您可以访问或替换张量中的元素。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="8ce8" class="lo km hh md b fi mh mi l mj mk"><strong class="md hi">tensor = torch.Tensor([[1, 2], [3, 4]])</strong></span><span id="0502" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Replace an element at position 0,1</strong><br/>tensor[0][1] = 7<br/>print(tensor)</span><span id="9a90" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[1., 7.],<br/>        [3., 4.]])</span><span id="26d0" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Access an element at position 1,0<br/></strong>print(tensor[1][0])<br/>print(tensor[1][0].item())    # using .item() you can access the scalar object</span><span id="9458" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor(3.)<br/>3.0</span></pre><p id="612e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。切片操作:</strong></p><p id="e301" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用此操作，可以访问特定索引范围内的元素。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="23f6" class="lo km hh md b fi mh mi l mj mk">tensor = torch.Tensor([[1, 2], [3, 4], [5,6]])</span><span id="86d8" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># First element of every row<br/></strong>print(tensor[:,0])</span><span id="77de" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([1., 3., 5.])</span><span id="5908" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Last element from every row<br/></strong>print(tensor[:,-1])</span><span id="9d58" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([2., 4., 6.])</span><span id="fbdd" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># All elements from second row<br/></strong>print(tensor[1,:])</span><span id="3509" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([3., 4.])</span><span id="161c" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># All elements from last two rows<br/></strong>print(tensor[1:,:])</span><span id="bc87" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>tensor([[3., 4.],<br/>        [5., 6.]])</span></pre><p id="0ce5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 3。连接或连接张量:</strong></p><p id="aa41" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该操作用于沿着给定的维度连接一系列张量。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="b638" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2], [3, 4], [5,6]])<br/>tensor_2 = torch.Tensor([[8,9],[10,11],[12,13]])</span><span id="ec3c" class="lo km hh md b fi mm mi l mj mk">new_tensor = torch.cat([tensor, ten2], dim=1)<br/>print(new_tensor)</span><span id="36be" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[ 1.,  2.,  8.,  9.],<br/>        [ 3.,  4., 10., 11.],<br/>        [ 5.,  6., 12., 13.]])</span></pre><p id="c863" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">还有一个类似于连接的操作，称为<a class="ae jw" href="https://pytorch.org/docs/stable/generated/torch.stack.html" rel="noopener ugc nofollow" target="_blank"> torch.stack </a>。</p><p id="a725" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 4。重塑张量:</strong></p><p id="8e02" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用这个操作，你可以将你的张量整形为<em class="mn">n</em>×m的大小。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="5e2e" class="lo km hh md b fi mh mi l mj mk">tensor = torch.Tensor([[1, 2], [3, 4]])<br/>reshaped_tensor_1 = reshape_tensor.view(1,4)<br/>reshaped_tensor_2 = reshape_tensor.view(4,1)</span><span id="4448" class="lo km hh md b fi mm mi l mj mk">print("Reshaped tensor 1: ",reshaped_tensor_1)<br/>print("\nReshaped tensor 2: ",reshaped_tensor_2)</span><span id="8014" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>Reshaped tensor 1 tensor:<br/>([[1., 2., 3., 4.]])<br/><br/>Reshaped tensor 2 tensor:<br/>([[1.],<br/>  [2.],<br/>  [3.],<br/>  [4.]])</span></pre><p id="6f3e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 5。转置:</strong></p><p id="4766" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以使用<code class="du mo mp mq md b">.t()</code>或<code class="du mo mp mq md b">.permute(-1, 0).</code>对张量执行转置操作</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="6b2c" class="lo km hh md b fi mh mi l mj mk">tensor = torch.Tensor([[1, 2], [3, 4], [5,6]])<br/>print("Tensor: ",tensor)<br/>print("\nTensor after transpose: ",tensor.t())</span><span id="130b" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>Tensor:  tensor([[1., 2.],<br/>        [3., 4.],<br/>        [5., 6.]])<br/><br/>Tensor after transpose:  tensor([[1., 3., 5.],<br/>        [2., 4., 6.]])</span></pre><p id="2c12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用置换函数进行转置:</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="b216" class="lo km hh md b fi mh mi l mj mk">tensor = torch.Tensor([[1, 2], [3, 4], [5,6]])<br/>print("Tensor: ",tensor)<br/>print("\nTensor after transpose: ",tensor.permute(-1,0))</span><span id="835b" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>Tensor:  <br/>tensor([[1., 2.],<br/>        [3., 4.],<br/>        [5., 6.]])<br/><br/>Tensor after transpose:  <br/>tensor([[1., 3., 5.],<br/>        [2., 4., 6.]])</span></pre><p id="3c55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 6。叉积:</strong></p><p id="b215" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以用两个中的任何一个得到两个张量的叉积:<code class="du mo mp mq md b">tensor_1<strong class="ir hi">.</strong>cross(tensor_2)</code>或者<code class="du mo mp mq md b">torch.cross(tensor_1, tensor_2)</code></p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="910f" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2], [3, 4], [5,6]])<br/>tensor_2 = torch.Tensor([[8, 9],[10, 11],[12, 13]])</span><span id="cb79" class="lo km hh md b fi mm mi l mj mk">cross_prod = tensor_1.cross(tensor_2)<br/>print(cross_prod)</span><span id="75c9" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[-14., -14.],<br/>        [ 28.,  28.],<br/>        [-14., -14.]])</span></pre><p id="ce92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 7。矩阵产品:</strong></p><p id="1e98" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">执行两个张量的矩阵乘法。如果tensor_1是一个(<em class="mn"> n </em> × <em class="mn"> m </em>)张量，tensor_2是一个(<em class="mn"> m </em> × <em class="mn"> p </em>)张量，那么输出将是(<em class="mn"> n </em> × <em class="mn"> p </em>)张量。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="ce18" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2, 3], [3, 5, 6], [7, 8, 9]])<br/>tensor_2 = torch.Tensor([[10, 11, 12],[13, 14, 15],[16, 17, 18]])</span><span id="bbae" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Using tensor_1.mm(tensor_2)<br/></strong>matrix_prod = tensor_1.mm(tensor_2)<br/>print(matrix_prod)</span><span id="b9e5" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[ 84.,  90.,  96.],<br/>        [191., 205., 219.],<br/>        [318., 342., 366.]])<br/></span><span id="175d" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Using torch.mm(tensor_1, tensor_2)<br/></strong>maxtrix_prod = torch.mm(tensor_1,tensor_2)<br/>print(matrix_prod)</span><span id="7043" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[ 84.,  90.,  96.],<br/>        [191., 205., 219.],<br/>        [318., 342., 366.]])</span></pre><p id="08b2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 8。逐元素乘法:</strong></p><p id="0327" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该操作执行两个张量的元素乘积。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="6462" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2, 3], [3, 5, 6], [7, 8, 9]])<br/>tensor_2 = torch.Tensor([[10, 11, 12],[13, 14, 15],[16, 17, 18]])</span><span id="f42d" class="lo km hh md b fi mm mi l mj mk">product = tensor_1.mul(tensor_2)</span><span id="8b0c" class="lo km hh md b fi mm mi l mj mk">print(product)</span><span id="8178" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[ 10.,  22.,  36.],<br/>        [ 39.,  70.,  90.],<br/>        [112., 136., 162.]])</span></pre><p id="cfab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 9。张量之和:</strong></p><p id="cdd4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该操作返回张量所有值的和。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="9e3a" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[10, 11, 12],[13, 14, 15],[16, 17, 18]])</span><span id="e2ab" class="lo km hh md b fi mm mi l mj mk">agg = tensor_1.sum()<br/>agg_item = agg.item()<br/>print(agg_item)</span><span id="4312" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:</strong><br/>126.0</span></pre><p id="68f6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 10。就地添加操作:</strong></p><p id="dba2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个操作将一个标量值加到张量的每个元素上。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="130d" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2, 3], [3, 5, 6], [7, 8, 9]])</span><span id="8c18" class="lo km hh md b fi mm mi l mj mk">print(tensor_1)<br/>tensor_1.add_(5)<br/>print("\n",tensor_1)</span><span id="916f" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>tensor([[1., 2., 3.],<br/>        [3., 5., 6.],<br/>        [7., 8., 9.]])<br/><br/> tensor([[4.,  5.,  6.],<br/>        [ 6.,  8.,  9.],<br/>        [10., 11., 12.]])</span></pre><h2 id="1c03" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">PyTorch-NumPy互操作性:</h2><p id="e967" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">您可以轻松地将NumPy数组转换为PyTorch张量，反之亦然。</p><ol class=""><li id="f822" class="jx jy hh ir b is it iw ix ja jz je ka ji kb jm ml kd ke kf bi translated"><strong class="ir hi"> PyTorch张量到NumPy数组:</strong></li></ol><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="a9c6" class="lo km hh md b fi mh mi l mj mk">t = torch.Tensor([[1, 2, 3], [3, 5, 6], [7, 8, 9]])<br/>print("Pytorch tensor: ", t)<br/>print("\n",type(t))</span><span id="56dc" class="lo km hh md b fi mm mi l mj mk">n = t.numpy()<br/>print("\nNumpy array: ", n)<br/>print("\n",type(n))</span><span id="b061" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>Pytorch tensor:<br/>tensor([[1., 2., 3.],<br/>        [3., 5., 6.],<br/>        [7., 8., 9.]])<br/><br/>&lt;class 'torch.Tensor'&gt;<br/><br/>Numpy array:<br/>[[1. 2. 3.]<br/> [3. 5. 6.]<br/> [7. 8. 9.]]</span><span id="26ce" class="lo km hh md b fi mm mi l mj mk">&lt;class 'numpy.ndarray'&gt;</span></pre><p id="ee54" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> 2。NumPy数组到PyTorch张量:</strong></p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="911a" class="lo km hh md b fi mh mi l mj mk">numpy_array = np.ones(5)  # this method creates a numpy array with all elements as 1</span><span id="576c" class="lo km hh md b fi mm mi l mj mk">print(numpy_array)<br/>print(type(numpy_array))</span><span id="2325" class="lo km hh md b fi mm mi l mj mk">pytorch_tensor = torch.from_numpy(n)<br/>print("\n",pytorch_tensor)<br/>print(type(pytorch_tensor))</span><span id="0a8e" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi"># Output:<br/></strong>[1. 1. 1. 1. 1.]<br/>&lt;class 'numpy.ndarray'&gt;<br/><br/>tensor([1., 1., 1., 1., 1.], dtype=torch.float64)<br/>&lt;class 'torch.Tensor'&gt;</span></pre><h2 id="4e39" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">GPU/CPU兼容性:</h2><p id="8b33" class="pw-post-body-paragraph ip iq hh ir b is lj iu iv iw lk iy iz ja ll jc jd je lm jg jh ji ln jk jl jm ha bi translated">如果你的计算机上有GPU，并且还安装了用于深度学习的CUDA工具包，那么你可以释放GPU的力量，实现比CPU更快的计算。</p><pre class="jo jp jq jr fd mc md me mf aw mg bi"><span id="265f" class="lo km hh md b fi mh mi l mj mk">tensor_1 = torch.Tensor([[1, 2, 3], [3, 5, 6], [7, 8, 9]])<br/>tensor_2 = torch.Tensor([[10, 11, 12],[13, 14, 15],[16, 17, 18]])</span><span id="2bb8" class="lo km hh md b fi mm mi l mj mk"><strong class="md hi">if</strong> torch.cuda.is_available():<br/>    tensor_1 = tensor_1.cuda()<br/>    tensor_2 = tensor_2.cuda()</span></pre></div><div class="ab cl mr ms go mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ha hb hc hd he"><p id="cc9f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">今天就到这里吧！</p><p id="5018" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望你喜欢学习PyTorch张量的基础知识。在下一篇教程中，我们将学习更多关于PyTorch中的数据集和数据加载器的知识，这样您就可以享受使用PyTorch处理数据的乐趣。</p></div><div class="ab cl mr ms go mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ha hb hc hd he"><h2 id="cb15" class="lo km hh bd kn lp lq lr kr ls lt lu kv ja lv lw kz je lx ly ld ji lz ma lh mb bi translated">文章时间表</h2><ol class=""><li id="fa1c" class="jx jy hh ir b is lj iw lk ja my je mz ji na jm ml kd ke kf bi translated"><a class="ae jw" href="https://pawarsaurav842.medium.com/pytorch-basics-the-only-guide-you-need-to-get-started-f8e67b3d056a" rel="noopener">张量</a></li><li id="a023" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">使用PyTorch进行数据处理</li><li id="3e6b" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">转换</li><li id="0086" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">使用PyTorch构建模型</li><li id="2a46" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">自动微分</li><li id="83ec" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">优化循环</li><li id="b54e" class="jx jy hh ir b is kg iw kh ja ki je kj ji kk jm ml kd ke kf bi translated">保存、加载和使用保存的模型</li></ol><p id="db15" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你喜欢这篇文章，不要忘记给一些掌声！😉</p><p id="741b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="mn">拜拜！</em> </strong></p><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/4cc410949985f2218a90501062adc368.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/1*elHL2pB261LrdTSu-S6_yg.gif"/></div></figure><div class="nc nd ez fb ne nf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hi fi z dy nk ea eb nl ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt in nf"/></div></div></a></div></div></div>    
</body>
</html>