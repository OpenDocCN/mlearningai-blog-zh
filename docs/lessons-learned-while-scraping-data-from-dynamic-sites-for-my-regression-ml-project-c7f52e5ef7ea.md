# ä¸ºæˆ‘çš„å›å½’ ML é¡¹ç›®ä»åŠ¨æ€ç«™ç‚¹æ”¶é›†æ•°æ®æ—¶å­¦åˆ°çš„ç»éªŒæ•™è®­

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/lessons-learned-while-scraping-data-from-dynamic-sites-for-my-regression-ml-project-c7f52e5ef7ea?source=collection_archive---------4----------------------->

![](img/f2bc025c2fa7cdaea5aeef6c6c769e13.png)

Photo Credit: Shutter Stock

å¸Œæœ›è¿™ç¯‡æ–‡ç« æ˜¯å…³äºåˆ†äº«æˆ‘çš„ç»éªŒï¼Œå…³äºæˆ‘ç”¨â€œä¸¤è¡Œä»£ç â€æŠ“å–ä¸€ä¸ªç½‘ç«™èµšäº†å¤šå°‘é’±ï¼Œæˆ–è€…â€œæˆ‘å¦‚ä½•åˆ›å»ºä¸€ä¸ªçˆ¬è™«æ¥è¿è¡Œå®ƒå‡ å¹´â€ã€‚ä½†æˆ‘çš„ç›®æ ‡æ˜¯ä»**Indeed.com**æ”¶é›†å·¥èµ„æ•°æ®ï¼Œæˆ–è€…ä»**Zillow.com**æ”¶é›†æˆ¿åœ°äº§æ•°æ®ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œç”±äºåŠ¨æ€ HTML å†…å®¹ï¼Œæˆ‘æ— æ³•æˆåŠŸæ”¶é›†æ•°æ®ï¼Œæˆ–è€…ä» **Youtube** æˆ–**Medium.com**ä¸Šè·å¾—çš„ä»»ä½•æ•™ç¨‹éƒ½æ˜¯æœ‰ç”¨çš„ã€‚

æˆ‘è¿½æ±‚ Zillow çš„åŠ¨æœºæ˜¯ä¸ºäº†ç»™æˆ‘çš„æ•°æ®æ·»åŠ æ›´å¤šçš„â€œç‰¹å¾â€,å› ä¸ºä»–ä»¬æœ‰å­¦æ ¡è¯„çº§ä¿¡æ¯ã€‚ç”±äºæœé›†åˆ°çš„æ•°æ®å°†è¢«ç”¨æ¥åˆ›å»ºä¸€ä¸ªè®¨è®ºå­¦æ ¡è¯„çº§å’Œæˆ¿ä»·ä¹‹é—´å…³ç³»çš„çº¿æ€§å›å½’ç®—æ³•ï¼ŒZillow æ›´æœ‰æ„ä¹‰ã€‚

Zillow æ–¹é¢çš„å¦ä¸€ä¸ªå‘å±•æ˜¯ï¼Œä»–ä»¬å¤§çº¦åœ¨ä¸€å¹´å‰æ”¾å¼ƒäº†ä»–ä»¬çš„å…è´¹ APIã€‚ç°åœ¨å®ƒæ˜¯é€šè¿‡å¦ä¸€ä¸ªå¹³å°æä¾›çš„ï¼Œä½ éœ€è¦è¢«é‚€è¯·åˆ°è¿™ä¸ªå¹³å°ã€‚

å› ä¸ºè¿™å·²ç»æˆä¸ºæˆ‘æƒ³è¦å…‹æœçš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘å¿…é¡»æ‰¾åˆ°ä¸€ç§æ–¹æ³•ä»ç½‘ç«™ä¸Šæå–è‡³å°‘ä¸€äº›æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä½¿ç”¨äº† **Apify** ï¼Œå®ƒä½¿ç”¨åç«¯çš„**æœ¨å¶å¸ˆ**æ¥æŠ“å–æ‚¨æƒ³è¦æŠ“å–çš„é¡µé¢ã€‚ä½ å¯ä»¥æŸ¥çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=i2Dy9fDqbRk)äº†è§£è¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ã€‚

**è­¦å‘Š**:æˆ‘æ„è¯†åˆ°çš„ä¸€ä¸ªå…±åŒç‚¹æ˜¯ï¼ŒYoutube ä¸Šçš„å¤§å¤šæ•°æ•™ç¨‹(å¦‚æœä¸æ˜¯å…¨éƒ¨çš„è¯)éƒ½å·²ç»è¶…è¿‡ä¸€å¹´äº†ï¼Œæ‰€ä»¥å½“ä½ æŸ¥çœ‹å®ƒä»¬çš„æ—¶å€™ï¼Œå®ƒä»¬é¡µé¢çš„åŠ¨æ€å†…å®¹å¯èƒ½å·²ç»åœ¨è¿™äº›ç½‘ç«™ä¸Šè¢«æ”¹å˜äº†ã€‚æ­¤å¤–ï¼Œä»–ä»¬ä¸»è¦æ˜¯è§£å†³é™æ€ç½‘ç«™ï¼Œè¡¨æ ¼ï¼Œç­‰åˆ›é€ å†…å®¹ï¼Œæ‰€ä»¥ä»–ä»¬å¤§å¤šæ•°åªæ˜¯ç‚¹å‡»è¯±é¥µè¿™ä¸€ç‚¹ã€‚

# è®®ç¨‹

1.  ç½‘ç»œæŠ“å–ä¸ç½‘ç»œçˆ¬è¡Œ
2.  ä½ èƒ½åšä»€ä¹ˆï¼Ÿ
3.  "/robots.txt "çš„äº‹æƒ…

æˆ‘å†™è¿™ç¯‡æ–‡ç« çš„ç›®çš„ä¸æ˜¯æ•™ä½ å¦‚ä½•æŠ“å–ç½‘é¡µï¼Œè€Œæ˜¯åœ¨ä¸€äº›æœ€å¸¸ç”¨çš„å·¥å…·ä¸­ï¼Œå¸®åŠ©ä½ å…‹æœä¸€äº›ä½ å¯èƒ½ä¼šé‡åˆ°çš„é™·é˜±ã€‚

# 1.ç½‘ç»œæŠ“å–ä¸ç½‘ç»œçˆ¬è¡Œ

æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬æƒ³åšç½‘ç»œæŠ“å–ï¼Œä½†å› ä¸ºæˆ‘ä»¥å‰æœ‰è¿‡è¿™ç§å›°æƒ‘ï¼Œæ‰€ä»¥æˆ‘æƒ³è°ˆè°ˆä¸åŒä¹‹å¤„ã€‚æ ¹æ®ç»´åŸºç™¾ç§‘çš„è¯´æ³•ï¼Œ

> **ç½‘é¡µæŠ“å–**ã€**ç½‘é¡µæŠ“å–**æˆ–**ç½‘é¡µæ•°æ®æå–**æ˜¯ç”¨äºä»ç½‘ç«™ä¸­æå–æ•°æ®çš„æ•°æ®æŠ“å–ã€‚ç½‘ç»œæŠ“å–è½¯ä»¶å¯ä»¥ä½¿ç”¨è¶…æ–‡æœ¬ä¼ è¾“åè®®æˆ–ç½‘ç»œæµè§ˆå™¨ç›´æ¥è®¿é—®ä¸‡ç»´ç½‘ã€‚è™½ç„¶ web æŠ“å–å¯ä»¥ç”±è½¯ä»¶ç”¨æˆ·æ‰‹åŠ¨å®Œæˆï¼Œä½†è¯¥æœ¯è¯­é€šå¸¸æŒ‡çš„æ˜¯ä½¿ç”¨ bot æˆ– web crawler å®ç°çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹ã€‚è¿™æ˜¯ä¸€ç§å¤åˆ¶å½¢å¼ï¼Œä»ç½‘ç»œä¸Šæ”¶é›†å¹¶å¤åˆ¶ç‰¹å®šçš„æ•°æ®ã€‚

å®ƒä¹ŸåŒ…å«äº†ä¸€äº›ç½‘ç»œçˆ¬è¡Œçš„å…ƒç´ ã€‚ä½†æ˜¯ç½‘ç»œçˆ¬è¡Œé€šå¸¸æ˜¯æŒ‡æœç´¢å¼•æ“æ‰€åšçš„äº‹æƒ…ã€‚å®ƒæ›´å¤šçš„æ˜¯ç´¢å¼•ï¼Œè€Œä¸æ˜¯æ˜¾ç¤ºç½‘é¡µçš„å…¨éƒ¨å†…å®¹ã€‚

# **2ã€‚ä½ èƒ½åšä»€ä¹ˆï¼Ÿ**

é€šè¿‡ä½¿ç”¨åº”ç”¨æœ€å¤šçš„ä¸‰ä¸ªåº“ *BeautifulSoupï¼ŒRequests å’Œç¡’*ã€‚

é€šè¿‡ä½¿ç”¨ *BeautifulSoup* åº“ï¼›

```
from urllib.request import urlopen
from bs4 import BeautifulSoupurl = ''
html = urlopen(url)
bs = BeautifulSoup(html, 'html.parser')

for child in bs.find('table',{'id':'giftList'}).children:
    print(child)
```

ä½œä¸ºä¸€ä¸ªè‡ªç„¶çš„ Selenium ç”¨æˆ·ï¼Œæˆ‘å¯¹è¯¥å·¥å…·çš„ç¬¬ä¸€å°è±¡æ˜¯ï¼Œå› ä¸ºæˆ‘è§‰å¾—ä¸éœ€è¦å®ƒã€‚ä½†æ˜¯éšç€æˆ‘å¼€å§‹åœ¨æ›´å¤šçš„é¡¹ç›®ä¸­ä½¿ç”¨å®ƒï¼Œå¹¶å°†å…¶ä¸ Selenium ç»“åˆï¼Œæˆ‘æ„è¯†åˆ°å®ƒå¯ä»¥äº§ç”Ÿä¸€äº›ç¾å¥½çš„ä¸œè¥¿ã€‚

é€šè¿‡ä½¿ç”¨*è¯·æ±‚*ï¼›

```
import requests
from pandas.io.json import json_normalize

url = 'url you want to scrape'
jsonData = requests.get(url).json()

table = json_normalize(jsonData['data'])
```

ä½ å¯ä»¥ç‚¹å‡»æŸ¥çœ‹è¯·æ±‚æ–‡æ¡£[ã€‚å®ƒç¡®å®è¿”å› JSON æ ¼å¼ï¼Œä½ åªéœ€è¦ä»é‚£é‡Œå¼€å§‹ã€‚](https://docs.python-requests.org/en/latest/)

æˆ–è€…ä½¿ç”¨ Selenium WebDriver

```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import json

driver=webdriver.Chrome(executable_path='./chromedriver.exe')
driver.get(url)
rating=WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.XPATH, 'your xpath locator'))
    )
```

Selenium WebDriver æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å·¥å…·ï¼Œé€šè¿‡ä½¿ç”¨ CSS é€‰æ‹©å™¨ã€Xpathã€Idã€Name ç­‰å®šä½å™¨æ¥å®šä½ç½‘ç«™ DOMï¼Œä»è€Œå¸®åŠ©æ‚¨è‡ªåŠ¨åŒ–æµè§ˆå™¨çš„ç§»åŠ¨ã€‚å®ƒæ˜¯è‡ªåŠ¨åŒ–æµ‹è¯•äººå‘˜ä¸­å¸¸ç”¨çš„å·¥å…·ï¼Œç¼–å†™çš„æµ‹è¯•å¯èƒ½ä¸å¯é ï¼Œå› ä¸ºå¦‚æœ DOM ä¸­æœ‰ä»»ä½•å˜åŒ–ï¼Œå·¥å…·å°±ä¸å¯èƒ½æ‰¾åˆ°æŸä¸ªé¡¹ç›®ã€‚ä¸Šé¢æ˜¯ä¸€äº›æ ·æ¿ä»£ç ï¼Œä½ å¯ä»¥ç”¨æ¥å¼€å§‹ä½ çš„æ—…ç¨‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨ CroPath Chrome æ‰©å±•ï¼Œæ‚¨å¯ä»¥é€šè¿‡æŸ¥æ‰¾ç›¸å¯¹å’Œç»å¯¹ Xpaths æ¥è½»æ¾å®šä½å…ƒç´ ã€‚

è¯¥å·¥å…·çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯å®ƒå¤„ç† AJAX è°ƒç”¨ã€‚AJAX = **A** åŒæ­¥**J**avaScript**A**nd**X**MLã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œè·å¾—æ›´å¤šå…³äºå®ƒçš„ä¿¡æ¯ï¼Œä½†æ€»è€Œè¨€ä¹‹ï¼Œå®ƒæ˜¯

> AJAX å…è®¸é€šè¿‡åœ¨åå°ä¸ web æœåŠ¡å™¨äº¤æ¢æ•°æ®æ¥å¼‚æ­¥æ›´æ–°ç½‘é¡µã€‚è¿™æ„å‘³ç€å¯ä»¥æ›´æ–°ç½‘é¡µçš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸éœ€è¦é‡æ–°åŠ è½½æ•´ä¸ªé¡µé¢ã€‚

è¿™ä¹Ÿæ˜¯æµ‹è¯•è‡ªåŠ¨åŒ–å·¥ç¨‹å¸ˆçš„å™©æ¢¦ã€‚å› ä¸º Selenium(ä¸åƒ Cypress)åœ¨æµè§ˆå™¨ä¸Šå·¥ä½œï¼Œå®ƒå¯¹ AJAX è°ƒç”¨æ²¡æœ‰ä»»ä½•æ§åˆ¶ã€‚è¿™é‡Œçš„ [**ç­‰å¾…**](https://www.selenium.dev/documentation/webdriver/waits/) å‰æ¥æ•‘æ´ã€‚ç­‰å¾…åªæ˜¯è®©ç”¨æˆ·æ˜¾å¼æˆ–éšå¼åœ°ç­‰å¾…ï¼Œç›´åˆ°é¢„æœŸçš„å…ƒç´ åŠ è½½ã€‚

ç¤ºä¾‹ä»£ç :

```
from selenium.webdriver.support.ui import WebDriverWait

driver.navigate("file:///race_condition.html")
el = WebDriverWait(driver).until(lambda d: d.find_element_by_tag_name("p"))
assert el.text == "Hello from JavaScript!"
```

åœ¨å¿˜è®°æåŠä¹‹å‰ï¼Œæˆ‘è¿˜è¢« Zillow å’Œ Realtor ç¦æ­¢ä½¿ç”¨ Seleniumï¼Œæ‰€ä»¥è¯·ç¡®ä¿æ‚¨å°† ***sleep()*** æ–¹æ³•æ·»åŠ åˆ°æ‚¨çš„è°ƒç”¨ä¸­ã€‚æˆ–è€…æ›´å¥½çš„é€‰æ‹©æ˜¯ä½¿ç”¨ä»£ç†ã€‚å´è¢«å‘ŠçŸ¥ä»£ç†æ˜¯ä¸ºäº†å®‰å…¨[æ¼æ´ è€Œå¼€æ”¾çš„ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼Œç”¨äºæ£€æŸ¥ä»£ç†æ˜¯å¦å·¥ä½œï¼Œç„¶åæ‚¨å¯ä»¥ä½¿ç”¨å®ƒä½¿æ‚¨çš„ä»£ç åœ¨ç«™ç‚¹ä¸Šè¿è¡Œã€‚](https://www.itbriefcase.net/4-vulnerabilities-of-a-proxy-server)

```
# Import the required Modules
import requests# Create a pool of proxies
proxies = {
 '[http://'](http://114.121.248.251:8080'),
 '[http://'](http://222.85.190.32:8090'),
 '[http://'](http://47.107.128.69:888'),

}url = '['](https://ipecho.net/plain')# Iterate the proxies and check if it is working.
for proxy in proxies:
 try:
  page = requests.get(
  url, proxies={"http": proxy, "https": proxy})# Prints Proxy server IP address if proxy is alive.
  print("Status OK, Output:", page.text)except OSError as e:# Proxy returns Connection error
  print(e)
```

# 3."/robots.txt "çš„äº‹æƒ…ğŸ¤”

![](img/9707427f727ea62dc506c16500e3b99a.png)

Screenshot credit to the article writer

å½“ä½ åœ¨ç½‘ç«™ URL åæ·»åŠ â€œ/robots.txtâ€æ—¶ï¼Œå®ƒä¼šå‘Šè¯‰ä½ ä»€ä¹ˆæ˜¯å…è®¸çš„ï¼Œä»€ä¹ˆæ˜¯ä¸å…è®¸çš„ã€‚ä»æˆ‘å°è¯•æŠ“å–çš„ä¸‰ä¸ªç½‘ç«™å¯ä»¥çœ‹å‡ºï¼Œä»–ä»¬å€¾å‘äºä¸å…è®¸æŠ“å–ä»»ä½•å†…å®¹ï¼Ÿï¼

## ç»“è®º

å¦‚æœä½ éœ€è¦å¤„ç†æˆ¿åœ°äº§æ•°æ®ï¼Œæˆ‘å»ºè®®ä½ å»çœ‹çœ‹å…¶ä»–çš„æˆ¿åœ°äº§ç½‘ç«™ï¼Œæ¯”å¦‚ä¸–çºª 21 æˆ– realtor.comã€‚ä½ å¯èƒ½ä¼šæœ‰æ›´å¥½çš„è¿æ°”ã€‚åœ¨åˆ®**Indeed.com**æ–¹é¢ï¼Œæˆ‘è¯šå®çš„åé¦ˆæ˜¯ä¸è¦å°è¯•ã€‚Indeed.com æ˜¯ä¸€ä¸ªéå¸¸æ··ä¹±çš„åœ°æ–¹ï¼Œå¤§å¤šæ•°å·¥ä½œç”šè‡³æ²¡æœ‰å·¥èµ„ä¿¡æ¯æˆ–èŒƒå›´å…±äº«ã€‚ä½†æ˜¯å¦‚æœä½ æƒ³æµè§ˆ Craigslist è¿™æ ·çš„ç½‘ç«™ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ä¸€ä¸ªæˆ‘åœ¨[åšè¿‡çš„å°ä¾‹å­](https://github.com/METIS-DATA-SCIENCE-PROJECTS/webscraping-projects/blob/main/Philadelphia-Area-Craigslist-2-br-Apt-Scraping.ipynb)ã€‚ä¹Ÿè¯·æ³¨æ„ç½‘ç«™çš„ç½‘é¡µæŠ“å–è§„åˆ™ã€‚

æ„Ÿè°¢é˜…è¯»ï¼

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)