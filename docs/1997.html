<html>
<head>
<title>Forget Complex Traditional Approaches to handle NLP Datasets, HuggingFace Dataset Library is your saviour! Part-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">忘记复杂的传统方法来处理NLP数据集，HuggingFace数据集库是您的救星！第二部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-fe5de16d88c8?source=collection_archive---------2-----------------------#2022-02-20">https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-fe5de16d88c8?source=collection_archive---------2-----------------------#2022-02-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a05a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">作者</h1><p id="3329" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">纳巴伦·巴鲁阿</strong></p><p id="45b8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/nabarunbaruaAIML" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/nabarun-barua-aiml-engineer/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@nabarun.barua">towards data science</a></p><p id="e720" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">阿尔琼·库姆巴克拉</p><p id="bec9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/arjunKumbakkara" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/arjunkumbakkara/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@arjunkumbakkara">towards data science</a></p><p id="926c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这是前一部分的延续，如果你想看我们的<a class="ae kf" rel="noopener" href="/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f">早些时候公布的文件</a>，这里我们将讨论一些更高级的东西。</p><p id="5db0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在本文档中，我们将重点关注:</p><ul class=""><li id="7fed" class="kg kh hh je b jf ka jj kb jn ki jr kj jv kk jz kl km kn ko bi translated">合并数据集</li><li id="ce28" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">缓存数据集</li><li id="d8df" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">和云存储</li><li id="967b" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">如何创建文献检索系统</li></ul><h1 id="5222" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">合并数据集</h1><p id="1e28" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">有些情况下，数据科学家可能需要将多个数据集合并成一个数据集。有两种方法可以合并数据集:</p><h2 id="7c7b" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated"><strong class="ak">串联</strong></h2><p id="d1fd" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这种情况下，如果轴为零，我们可以合并具有相同列数和共享相同列类型的不同数据集。</p><p id="e8c6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果轴是一个，那么我们可以连接两个数据集，如果数据集中的行数相同。</p><p id="4c0d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">让我们看一个例子(取自huggingface.co的例子)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8a04" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset  <br/>bookcorpus = load_dataset("bookcorpus", split="train")     <br/>wiki = load_dataset("wikipedia", "20200501.en", split="train")     wiki = wiki.remove_columns("title")  # only keep the text      assert bookcorpus.features.type == wiki.features.type     bert_dataset = concatenate_datasets([bookcorpus, wiki])</span></pre><h2 id="d27a" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">交错数据集</h2><p id="4e77" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这里，我们可以将几个数据集合并在一起，从每个数据集中选择另一个例子来创建新数据集。这就是所谓的交错。</p><p id="8808" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这可用于常规数据集和流数据集中</p><p id="e1fc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在下面的例子中，我们针对流数据集(同样可以针对常规数据集)进行操作，并在可选的例子中给出概率。如果给定了概率，则最终数据集是基于相同的概率形成的。(huggingface.co的例子)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="a5bc" class="ku if hh ln b fi lr ls l lt lu">from datasets import interleave_datasets<br/>from itertools import islice<br/>en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)<br/>fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)</span><span id="4e3d" class="ku if hh ln b fi lv ls l lt lu">multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)</span></pre><p id="3224" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">最终数据集的大约80%由en_dataset构成，20%由fr_dataset构成。</p><h1 id="1cbb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">缓存数据集</h1><p id="efd2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">下载数据集时，处理脚本和数据存储在本地计算机上。缓存允许🤗数据集，以避免每次使用时重新下载或处理整个数据集。</p><h2 id="de99" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">缓存目录</h2><p id="fd06" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们可以改变当前目录的默认缓存目录，即<code class="du lw lx ly ln b">~/.cache/huggingface/datasets</code>。只需设置环境变量。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7b03" class="ku if hh ln b fi lr ls l lt lu">$ export HF_DATASETS_CACHE=”/path/to/another/directory”</span></pre><p id="916d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似地，我们可以通过在不同的关键字中传递参数<em class="lz"> cache_dir </em>来做同样的事情，如load_dataset、load_metric &amp;等。</p><p id="e6a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例子</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="4a56" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset</span><span id="9df7" class="ku if hh ln b fi lv ls l lt lu">bookcorpus = load_dataset("bookcorpus", split="train")<br/>wiki = load_dataset("wikipedia", "20200501.en", split="train",cache_dir="/path/to/another/directory")</span></pre><h2 id="def9" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">下载模式</h2><p id="fdfc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一旦数据集被下载，它就会被缓存，因此当我们加载数据集时，它不是从源下载，而是从缓存加载。</p><p id="a881" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，如果数据集中有任何变化，并且如果我们想要从源加载数据集未改变的数据集，那么我们需要使用参数<code class="du lw lx ly ln b">download_mode</code>。</p><p id="cc90" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例子</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="a5b8" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset</span><span id="3ba5" class="ku if hh ln b fi lv ls l lt lu">bookcorpus = load_dataset("bookcorpus", split="train")<br/>wiki = load_dataset("wikipedia", "20200501.en", split="train",download_mode='force_redownload')</span></pre><h2 id="453a" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">清理缓存文件</h2><p id="7819" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">如果需要清理缓存文件，我们可以通过执行以下命令来完成</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="95b7" class="ku if hh ln b fi lr ls l lt lu"># Below function just clears the cache of Dataset<br/>ds.cleanup_cache_files()</span></pre><h2 id="ac4e" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">启用或禁用缓存</h2><p id="28cb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">可能会出现我们不想缓存的情况。在这种情况下，我们可以在本地或全局禁用缓存。</p><p id="2d01" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">本地:</strong>如果我们在本地使用一个缓存文件，它将自动重新加载您之前应用到数据集的任何转换。我们可以在<code class="du lw lx ly ln b">datasets.Dataset.map()</code>中使用参数<code class="du lw lx ly ln b">load_from_cache=False</code>禁用缓存。</p><p id="b92c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">示例:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="6085" class="ku if hh ln b fi lr ls l lt lu">updated_dataset = small_dataset.map(tokenizer_function, load_from_cache=False)</span></pre><p id="ef36" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在上面的例子中，🤗数据集将对整个数据集再次执行函数<code class="du lw lx ly ln b">tokenizer_function</code>,而不是从先前的状态加载数据集。</p><p id="9ae1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">全局:</strong>如果我们想要全局禁用缓存，那么需要使用<code class="du lw lx ly ln b">datasets.set_caching_enabled()</code>为全局禁用缓存设置以下参数:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="0833" class="ku if hh ln b fi lr ls l lt lu">from datasets import set_caching_enabled<br/>set_caching_enabled(False)</span></pre><p id="a7b6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当您禁用缓存时，🤗对数据集应用变换时，数据集将不再重新加载缓存文件。您在数据集上应用任何转换都需要重新应用。</p><h1 id="dd85" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">云存储</h1><p id="5380" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Huggingface数据集可以存储到流行的云存储中。Hugginface数据集具有内置功能来满足这一需求。</p><p id="2d39" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">它支持的云列表以及需要安装的文件系统，以便在加载数据集时直接使用(表格取自Huggingface.co):</p><figure class="li lj lk ll fd mb er es paragraph-image"><div class="er es ma"><img src="../Images/24439a3e7debbaf55a60917eceb0c007.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*3xAbsnCqsrnxcE86RlqLJw.jpeg"/></div><figcaption class="me mf et er es mg mh bd b be z dx">Cloud Table</figcaption></figure><p id="209e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在这里，我们将尝试展示如何使用s3fs将数据集加载和保存到S3存储桶。对于其他云，请参见文档。尽管可以类似地使用其他云文件系统实现。</p><p id="5ff3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">首先安装数据集的S3依赖项。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="af40" class="ku if hh ln b fi lr ls l lt lu">pip install datasets[s3]</span></pre><p id="0388" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">加载数据集:</strong>现在，通过输入您的aws_access_key_id和aws_secret_access_key，从私有S3存储桶访问数据集</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="11ce" class="ku if hh ln b fi lr ls l lt lu">import datasets<br/>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)</span><span id="7061" class="ku if hh ln b fi lv ls l lt lu"># load encoded_dataset to from s3 bucket<br/>dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)</span></pre><p id="3170" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">保存数据集:</strong>处理完数据集后，您可以使用<code class="du lw lx ly ln b">datasets.Dataset.save_to_disk()</code>将其保存到S3:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7fe4" class="ku if hh ln b fi lr ls l lt lu">import datasets<br/>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)</span><span id="56c4" class="ku if hh ln b fi lv ls l lt lu"># saves encoded_dataset to your s3 bucket<br/>encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)</span></pre><h1 id="96c7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">如何创建文献检索系统</h1><p id="54b3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">文档检索系统是用于问答系统等自然语言处理任务的数据集的一个具体例子。本文档将向您展示如何为您的数据集构建索引搜索，从而允许您从数据集中搜索项目。</p><h2 id="8027" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">脸书人工智能相似性搜索</h2><p id="7eda" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">数据集有一种机制，我们可以基于嵌入在数据集上进行相似性搜索，这里首先将长段落转换成单个a嵌入，该嵌入稍后可以用作检索系统。在将段落转化为嵌入之后，我们将问题转化为嵌入。现在有了数据集的FAISS索引机制，我们可以比较这两个嵌入，从数据集中得到最相似的段落。</p><figure class="li lj lk ll fd mb er es paragraph-image"><div class="er es mi"><img src="../Images/ac0b59e6e5302d5aeff8310f12e76733.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*qIpq1BYSJCus-TcSUmGNpA.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx">Credits: <a class="ae kf" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co</a></figcaption></figure><p id="a953" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了做到这一点，我们有一个这样的模型命名为DPR(密集段落检索)。取自Huggingface数据集文档的示例。请随意使用任何其他模型，如句子变形金刚等。</p><p id="4f92" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第一步:</strong>加载上下文编码器模型&amp;分词器。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8c0d" class="ku if hh ln b fi lr ls l lt lu">from transformers import DPRContextEncoder, DPRContextEncoderTokenizer<br/>import torch<br/>torch.set_grad_enabled(False)<br/>ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")<br/>ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")</span></pre><p id="42ac" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤2: </strong>加载数据集并获得嵌入</p><p id="bb2e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">正如我们前面提到的，我们希望将每个条目表示为一个向量。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="b39d" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>ds = load_dataset('crime_and_punish', split='train[:100]')<br/>ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example["line"], return_tensors="pt"))[0][0].numpy()})</span></pre><p id="a693" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第三步:</strong>向FAISS搜索索引添加嵌入。</p><p id="8194" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在使用FAISS进行有效的相似性搜索，我们将使用<code class="du lw lx ly ln b">datasets.Dataset.add_faiss_index()</code>创建索引</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="3476" class="ku if hh ln b fi lr ls l lt lu">ds_with_embeddings.add_faiss_index(column='embeddings')</span></pre><p id="f1bf" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤4: </strong>使用FAISS搜索索引进行搜索查询</p><p id="34c9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，您可以使用嵌入索引查询您的数据集。加载DPR问题编码器，用<code class="du lw lx ly ln b">datasets.Dataset.get_nearest_examples()</code>搜索问题</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="2005" class="ku if hh ln b fi lr ls l lt lu">from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer<br/>q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")<br/>q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")</span><span id="c071" class="ku if hh ln b fi lv ls l lt lu">question = "Is it serious ?"<br/>question_embedding = q_encoder(**q_tokenizer(question, return_tensors="pt"))[0][0].numpy()<br/>scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)<br/>retrieved_examples["line"][0]</span></pre><p id="fd3e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第五步:</strong>保存FAISS指标</p><p id="ac56" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们可以用<code class="du lw lx ly ln b">datasets.Dataset.save_faiss_index()</code>把索引保存在磁盘上</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="035a" class="ku if hh ln b fi lr ls l lt lu">ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')</span></pre><p id="c743" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤6: </strong>加载FAISS指数</p><p id="f682" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在可以从磁盘加载索引了<code class="du lw lx ly ln b">datasets.Dataset.load_faiss_index()</code></p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="fb14" class="ku if hh ln b fi lr ls l lt lu">ds = load_dataset('crime_and_punish', split='train[:100]')<br/>ds.load_faiss_index('embeddings', 'my_index.faiss')</span></pre><h2 id="b9e6" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">弹性搜索</h2><p id="515b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">与FAISS不同，ElasticSearch基于搜索上下文中的精确匹配来检索文档。</p><p id="2f94" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">更多信息请参见<a class="ae kf" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html" rel="noopener ugc nofollow" target="_blank">安装&amp;配置指南</a></p><p id="d359" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">弹性搜索与FAISS搜索非常相似。示例取自标准Huggingface数据集文档。</p><p id="33fd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤1: </strong>加载数据集</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="0f99" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>squad = load_dataset('squad', split='validation')</span></pre><p id="823c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤2: </strong>向数据集添加弹性搜索</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7940" class="ku if hh ln b fi lr ls l lt lu">squad.add_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")</span></pre><p id="b0ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">第三步:</strong>执行查询搜索</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="4c4c" class="ku if hh ln b fi lr ls l lt lu">query = "machine"<br/>scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)<br/>retrieved_examples["title"][0]</span></pre><p id="b147" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">步骤4: </strong>如果提供了姓名，则加载弹性搜索索引</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="b1b8" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>squad = load_dataset('squad', split='validation')<br/>squad.load_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")<br/>query = "machine"<br/>scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)</span></pre><p id="8783" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">正如所承诺的，我们在这里扩展了之前的<a class="ae kf" rel="noopener" href="/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f">使用HugginFace数据集库的一站式指南</a></p><p id="d794" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果你喜欢这个博客，请表达你的爱，给我们一个大拇指，给我们加星，如果不喜欢，请在评论区给我们一个反馈。希望你会在图书馆玩得开心！</p><p id="ae8d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了合作，帮助和一起学习-</p><p id="8da3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">加入我们的不和谐服务器:<a class="ae kf" href="https://discord.gg/Z7Kx96CYGJ" rel="noopener ugc nofollow" target="_blank">https://discord.gg/Z7Kx96CYGJ</a></p><p id="def4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">一路平安！</p><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na mc mm"/></div></div></a></div><p id="734f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">🔵<a class="ae kf" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="je hi">成为作家</strong> </a></p></div></div>    
</body>
</html>