<html>
<head>
<title>Forget Complex Traditional Approaches to handle NLP Datasets, HuggingFace Dataset Library is your saviour! Part-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¿˜è®°å¤æ‚çš„ä¼ ç»Ÿæ–¹æ³•æ¥å¤„ç†NLPæ•°æ®é›†ï¼ŒHuggingFaceæ•°æ®é›†åº“æ˜¯æ‚¨çš„æ•‘æ˜Ÿï¼ç¬¬äºŒéƒ¨åˆ†</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-fe5de16d88c8?source=collection_archive---------2-----------------------#2022-02-20">https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-fe5de16d88c8?source=collection_archive---------2-----------------------#2022-02-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a05a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ä½œè€…</h1><p id="3329" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated"><strong class="je hi">çº³å·´ä¼¦Â·å·´é²é˜¿</strong></p><p id="45b8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/nabarunbaruaAIML" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/nabarun-barua-aiml-engineer/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@nabarun.barua">towards data science</a></p><p id="e720" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">é˜¿å°”ç¼Â·åº“å§†å·´å…‹æ‹‰</p><p id="bec9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/arjunKumbakkara" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/arjunkumbakkara/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@arjunkumbakkara">towards data science</a></p><p id="926c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">è¿™æ˜¯å‰ä¸€éƒ¨åˆ†çš„å»¶ç»­ï¼Œå¦‚æœä½ æƒ³çœ‹æˆ‘ä»¬çš„<a class="ae kf" rel="noopener" href="/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f">æ—©äº›æ—¶å€™å…¬å¸ƒçš„æ–‡ä»¶</a>ï¼Œè¿™é‡Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æ›´é«˜çº§çš„ä¸œè¥¿ã€‚</p><p id="5db0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨:</p><ul class=""><li id="7fed" class="kg kh hh je b jf ka jj kb jn ki jr kj jv kk jz kl km kn ko bi translated">åˆå¹¶æ•°æ®é›†</li><li id="ce28" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">ç¼“å­˜æ•°æ®é›†</li><li id="d8df" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">å’Œäº‘å­˜å‚¨</li><li id="967b" class="kg kh hh je b jf kp jj kq jn kr jr ks jv kt jz kl km kn ko bi translated">å¦‚ä½•åˆ›å»ºæ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ</li></ul><h1 id="5222" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">åˆå¹¶æ•°æ®é›†</h1><p id="1e28" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æœ‰äº›æƒ…å†µä¸‹ï¼Œæ•°æ®ç§‘å­¦å®¶å¯èƒ½éœ€è¦å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆå¹¶æ•°æ®é›†:</p><h2 id="7c7b" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated"><strong class="ak">ä¸²è”</strong></h2><p id="d1fd" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœè½´ä¸ºé›¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆå¹¶å…·æœ‰ç›¸åŒåˆ—æ•°å’Œå…±äº«ç›¸åŒåˆ—ç±»å‹çš„ä¸åŒæ•°æ®é›†ã€‚</p><p id="e8c6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">å¦‚æœè½´æ˜¯ä¸€ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿æ¥ä¸¤ä¸ªæ•°æ®é›†ï¼Œå¦‚æœæ•°æ®é›†ä¸­çš„è¡Œæ•°ç›¸åŒã€‚</p><p id="4c0d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­(å–è‡ªhuggingface.coçš„ä¾‹å­)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8a04" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset  <br/>bookcorpus = load_dataset("bookcorpus", split="train")     <br/>wiki = load_dataset("wikipedia", "20200501.en", split="train")     wiki = wiki.remove_columns("title")  # only keep the text      assert bookcorpus.features.type == wiki.features.type     bert_dataset = concatenate_datasets([bookcorpus, wiki])</span></pre><h2 id="d27a" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">äº¤é”™æ•°æ®é›†</h2><p id="4e77" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å°†å‡ ä¸ªæ•°æ®é›†åˆå¹¶åœ¨ä¸€èµ·ï¼Œä»æ¯ä¸ªæ•°æ®é›†ä¸­é€‰æ‹©å¦ä¸€ä¸ªä¾‹å­æ¥åˆ›å»ºæ–°æ•°æ®é›†ã€‚è¿™å°±æ˜¯æ‰€è°“çš„äº¤é”™ã€‚</p><p id="8808" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">è¿™å¯ç”¨äºå¸¸è§„æ•°æ®é›†å’Œæµæ•°æ®é›†ä¸­</p><p id="e1fc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æµæ•°æ®é›†(åŒæ ·å¯ä»¥é’ˆå¯¹å¸¸è§„æ•°æ®é›†)è¿›è¡Œæ“ä½œï¼Œå¹¶åœ¨å¯é€‰çš„ä¾‹å­ä¸­ç»™å‡ºæ¦‚ç‡ã€‚å¦‚æœç»™å®šäº†æ¦‚ç‡ï¼Œåˆ™æœ€ç»ˆæ•°æ®é›†æ˜¯åŸºäºç›¸åŒçš„æ¦‚ç‡å½¢æˆçš„ã€‚(huggingface.coçš„ä¾‹å­)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="a5bc" class="ku if hh ln b fi lr ls l lt lu">from datasets import interleave_datasets<br/>from itertools import islice<br/>en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)<br/>fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)</span><span id="4e3d" class="ku if hh ln b fi lv ls l lt lu">multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)</span></pre><p id="3224" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">æœ€ç»ˆæ•°æ®é›†çš„å¤§çº¦80%ç”±en_datasetæ„æˆï¼Œ20%ç”±fr_datasetæ„æˆã€‚</p><h1 id="1cbb" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">ç¼“å­˜æ•°æ®é›†</h1><p id="efd2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">ä¸‹è½½æ•°æ®é›†æ—¶ï¼Œå¤„ç†è„šæœ¬å’Œæ•°æ®å­˜å‚¨åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šã€‚ç¼“å­˜å…è®¸ğŸ¤—æ•°æ®é›†ï¼Œä»¥é¿å…æ¯æ¬¡ä½¿ç”¨æ—¶é‡æ–°ä¸‹è½½æˆ–å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚</p><h2 id="de99" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">ç¼“å­˜ç›®å½•</h2><p id="fd06" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æˆ‘ä»¬å¯ä»¥æ”¹å˜å½“å‰ç›®å½•çš„é»˜è®¤ç¼“å­˜ç›®å½•ï¼Œå³<code class="du lw lx ly ln b">~/.cache/huggingface/datasets</code>ã€‚åªéœ€è®¾ç½®ç¯å¢ƒå˜é‡ã€‚</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7b03" class="ku if hh ln b fi lr ls l lt lu">$ export HF_DATASETS_CACHE=â€/path/to/another/directoryâ€</span></pre><p id="916d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ä¸åŒçš„å…³é”®å­—ä¸­ä¼ é€’å‚æ•°<em class="lz"> cache_dir </em>æ¥åšåŒæ ·çš„äº‹æƒ…ï¼Œå¦‚load_datasetã€load_metric &amp;ç­‰ã€‚</p><p id="e6a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ä¾‹å­</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="4a56" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset</span><span id="9df7" class="ku if hh ln b fi lv ls l lt lu">bookcorpus = load_dataset("bookcorpus", split="train")<br/>wiki = load_dataset("wikipedia", "20200501.en", split="train",cache_dir="/path/to/another/directory")</span></pre><h2 id="def9" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">ä¸‹è½½æ¨¡å¼</h2><p id="fdfc" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">ä¸€æ—¦æ•°æ®é›†è¢«ä¸‹è½½ï¼Œå®ƒå°±ä¼šè¢«ç¼“å­˜ï¼Œå› æ­¤å½“æˆ‘ä»¬åŠ è½½æ•°æ®é›†æ—¶ï¼Œå®ƒä¸æ˜¯ä»æºä¸‹è½½ï¼Œè€Œæ˜¯ä»ç¼“å­˜åŠ è½½ã€‚</p><p id="a881" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç°åœ¨ï¼Œå¦‚æœæ•°æ®é›†ä¸­æœ‰ä»»ä½•å˜åŒ–ï¼Œå¹¶ä¸”å¦‚æœæˆ‘ä»¬æƒ³è¦ä»æºåŠ è½½æ•°æ®é›†æœªæ”¹å˜çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä½¿ç”¨å‚æ•°<code class="du lw lx ly ln b">download_mode</code>ã€‚</p><p id="cc90" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ä¾‹å­</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="a5b8" class="ku if hh ln b fi lr ls l lt lu">from datasets import concatenate_datasets, load_dataset</span><span id="3ba5" class="ku if hh ln b fi lv ls l lt lu">bookcorpus = load_dataset("bookcorpus", split="train")<br/>wiki = load_dataset("wikipedia", "20200501.en", split="train",download_mode='force_redownload')</span></pre><h2 id="453a" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">æ¸…ç†ç¼“å­˜æ–‡ä»¶</h2><p id="7819" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å¦‚æœéœ€è¦æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®Œæˆ</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="95b7" class="ku if hh ln b fi lr ls l lt lu"># Below function just clears the cache of Dataset<br/>ds.cleanup_cache_files()</span></pre><h2 id="ac4e" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">å¯ç”¨æˆ–ç¦ç”¨ç¼“å­˜</h2><p id="28cb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">å¯èƒ½ä¼šå‡ºç°æˆ‘ä»¬ä¸æƒ³ç¼“å­˜çš„æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æˆ–å…¨å±€ç¦ç”¨ç¼“å­˜ã€‚</p><p id="2d01" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æœ¬åœ°:</strong>å¦‚æœæˆ‘ä»¬åœ¨æœ¬åœ°ä½¿ç”¨ä¸€ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œå®ƒå°†è‡ªåŠ¨é‡æ–°åŠ è½½æ‚¨ä¹‹å‰åº”ç”¨åˆ°æ•°æ®é›†çš„ä»»ä½•è½¬æ¢ã€‚æˆ‘ä»¬å¯ä»¥åœ¨<code class="du lw lx ly ln b">datasets.Dataset.map()</code>ä¸­ä½¿ç”¨å‚æ•°<code class="du lw lx ly ln b">load_from_cache=False</code>ç¦ç”¨ç¼“å­˜ã€‚</p><p id="b92c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç¤ºä¾‹:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="6085" class="ku if hh ln b fi lr ls l lt lu">updated_dataset = small_dataset.map(tokenizer_function, load_from_cache=False)</span></pre><p id="ef36" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼ŒğŸ¤—æ•°æ®é›†å°†å¯¹æ•´ä¸ªæ•°æ®é›†å†æ¬¡æ‰§è¡Œå‡½æ•°<code class="du lw lx ly ln b">tokenizer_function</code>,è€Œä¸æ˜¯ä»å…ˆå‰çš„çŠ¶æ€åŠ è½½æ•°æ®é›†ã€‚</p><p id="9ae1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">å…¨å±€:</strong>å¦‚æœæˆ‘ä»¬æƒ³è¦å…¨å±€ç¦ç”¨ç¼“å­˜ï¼Œé‚£ä¹ˆéœ€è¦ä½¿ç”¨<code class="du lw lx ly ln b">datasets.set_caching_enabled()</code>ä¸ºå…¨å±€ç¦ç”¨ç¼“å­˜è®¾ç½®ä»¥ä¸‹å‚æ•°:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="0833" class="ku if hh ln b fi lr ls l lt lu">from datasets import set_caching_enabled<br/>set_caching_enabled(False)</span></pre><p id="a7b6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">å½“æ‚¨ç¦ç”¨ç¼“å­˜æ—¶ï¼ŒğŸ¤—å¯¹æ•°æ®é›†åº”ç”¨å˜æ¢æ—¶ï¼Œæ•°æ®é›†å°†ä¸å†é‡æ–°åŠ è½½ç¼“å­˜æ–‡ä»¶ã€‚æ‚¨åœ¨æ•°æ®é›†ä¸Šåº”ç”¨ä»»ä½•è½¬æ¢éƒ½éœ€è¦é‡æ–°åº”ç”¨ã€‚</p><h1 id="dd85" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">äº‘å­˜å‚¨</h1><p id="5380" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Huggingfaceæ•°æ®é›†å¯ä»¥å­˜å‚¨åˆ°æµè¡Œçš„äº‘å­˜å‚¨ä¸­ã€‚Hugginfaceæ•°æ®é›†å…·æœ‰å†…ç½®åŠŸèƒ½æ¥æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚</p><p id="2d39" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">å®ƒæ”¯æŒçš„äº‘åˆ—è¡¨ä»¥åŠéœ€è¦å®‰è£…çš„æ–‡ä»¶ç³»ç»Ÿï¼Œä»¥ä¾¿åœ¨åŠ è½½æ•°æ®é›†æ—¶ç›´æ¥ä½¿ç”¨(è¡¨æ ¼å–è‡ªHuggingface.co):</p><figure class="li lj lk ll fd mb er es paragraph-image"><div class="er es ma"><img src="../Images/24439a3e7debbaf55a60917eceb0c007.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*3xAbsnCqsrnxcE86RlqLJw.jpeg"/></div><figcaption class="me mf et er es mg mh bd b be z dx">Cloud Table</figcaption></figure><p id="209e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å°è¯•å±•ç¤ºå¦‚ä½•ä½¿ç”¨s3fså°†æ•°æ®é›†åŠ è½½å’Œä¿å­˜åˆ°S3å­˜å‚¨æ¡¶ã€‚å¯¹äºå…¶ä»–äº‘ï¼Œè¯·å‚è§æ–‡æ¡£ã€‚å°½ç®¡å¯ä»¥ç±»ä¼¼åœ°ä½¿ç”¨å…¶ä»–äº‘æ–‡ä»¶ç³»ç»Ÿå®ç°ã€‚</p><p id="5ff3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">é¦–å…ˆå®‰è£…æ•°æ®é›†çš„S3ä¾èµ–é¡¹ã€‚</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="af40" class="ku if hh ln b fi lr ls l lt lu">pip install datasets[s3]</span></pre><p id="0388" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">åŠ è½½æ•°æ®é›†:</strong>ç°åœ¨ï¼Œé€šè¿‡è¾“å…¥æ‚¨çš„aws_access_key_idå’Œaws_secret_access_keyï¼Œä»ç§æœ‰S3å­˜å‚¨æ¡¶è®¿é—®æ•°æ®é›†</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="11ce" class="ku if hh ln b fi lr ls l lt lu">import datasets<br/>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)</span><span id="7061" class="ku if hh ln b fi lv ls l lt lu"># load encoded_dataset to from s3 bucket<br/>dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)</span></pre><p id="3170" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">ä¿å­˜æ•°æ®é›†:</strong>å¤„ç†å®Œæ•°æ®é›†åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨<code class="du lw lx ly ln b">datasets.Dataset.save_to_disk()</code>å°†å…¶ä¿å­˜åˆ°S3:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7fe4" class="ku if hh ln b fi lr ls l lt lu">import datasets<br/>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)</span><span id="56c4" class="ku if hh ln b fi lv ls l lt lu"># saves encoded_dataset to your s3 bucket<br/>encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)</span></pre><h1 id="96c7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">å¦‚ä½•åˆ›å»ºæ–‡çŒ®æ£€ç´¢ç³»ç»Ÿ</h1><p id="54b3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ˜¯ç”¨äºé—®ç­”ç³»ç»Ÿç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ•°æ®é›†çš„ä¸€ä¸ªå…·ä½“ä¾‹å­ã€‚æœ¬æ–‡æ¡£å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸ºæ‚¨çš„æ•°æ®é›†æ„å»ºç´¢å¼•æœç´¢ï¼Œä»è€Œå…è®¸æ‚¨ä»æ•°æ®é›†ä¸­æœç´¢é¡¹ç›®ã€‚</p><h2 id="8027" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">è„¸ä¹¦äººå·¥æ™ºèƒ½ç›¸ä¼¼æ€§æœç´¢</h2><p id="7eda" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">æ•°æ®é›†æœ‰ä¸€ç§æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºåµŒå…¥åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œè¿™é‡Œé¦–å…ˆå°†é•¿æ®µè½è½¬æ¢æˆå•ä¸ªaåµŒå…¥ï¼Œè¯¥åµŒå…¥ç¨åå¯ä»¥ç”¨ä½œæ£€ç´¢ç³»ç»Ÿã€‚åœ¨å°†æ®µè½è½¬åŒ–ä¸ºåµŒå…¥ä¹‹åï¼Œæˆ‘ä»¬å°†é—®é¢˜è½¬åŒ–ä¸ºåµŒå…¥ã€‚ç°åœ¨æœ‰äº†æ•°æ®é›†çš„FAISSç´¢å¼•æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒè¿™ä¸¤ä¸ªåµŒå…¥ï¼Œä»æ•°æ®é›†ä¸­å¾—åˆ°æœ€ç›¸ä¼¼çš„æ®µè½ã€‚</p><figure class="li lj lk ll fd mb er es paragraph-image"><div class="er es mi"><img src="../Images/ac0b59e6e5302d5aeff8310f12e76733.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*qIpq1BYSJCus-TcSUmGNpA.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx">Credits: <a class="ae kf" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co</a></figcaption></figure><p id="a953" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¿™æ ·çš„æ¨¡å‹å‘½åä¸ºDPR(å¯†é›†æ®µè½æ£€ç´¢)ã€‚å–è‡ªHuggingfaceæ•°æ®é›†æ–‡æ¡£çš„ç¤ºä¾‹ã€‚è¯·éšæ„ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å‹ï¼Œå¦‚å¥å­å˜å½¢é‡‘åˆšç­‰ã€‚</p><p id="4f92" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">ç¬¬ä¸€æ­¥:</strong>åŠ è½½ä¸Šä¸‹æ–‡ç¼–ç å™¨æ¨¡å‹&amp;åˆ†è¯å™¨ã€‚</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8c0d" class="ku if hh ln b fi lr ls l lt lu">from transformers import DPRContextEncoder, DPRContextEncoderTokenizer<br/>import torch<br/>torch.set_grad_enabled(False)<br/>ctx_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")<br/>ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")</span></pre><p id="42ac" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤2: </strong>åŠ è½½æ•°æ®é›†å¹¶è·å¾—åµŒå…¥</p><p id="bb2e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">æ­£å¦‚æˆ‘ä»¬å‰é¢æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ¯ä¸ªæ¡ç›®è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ã€‚</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="b39d" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>ds = load_dataset('crime_and_punish', split='train[:100]')<br/>ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example["line"], return_tensors="pt"))[0][0].numpy()})</span></pre><p id="a693" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">ç¬¬ä¸‰æ­¥:</strong>å‘FAISSæœç´¢ç´¢å¼•æ·»åŠ åµŒå…¥ã€‚</p><p id="8194" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç°åœ¨ä½¿ç”¨FAISSè¿›è¡Œæœ‰æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="du lw lx ly ln b">datasets.Dataset.add_faiss_index()</code>åˆ›å»ºç´¢å¼•</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="3476" class="ku if hh ln b fi lr ls l lt lu">ds_with_embeddings.add_faiss_index(column='embeddings')</span></pre><p id="f1bf" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤4: </strong>ä½¿ç”¨FAISSæœç´¢ç´¢å¼•è¿›è¡Œæœç´¢æŸ¥è¯¢</p><p id="34c9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åµŒå…¥ç´¢å¼•æŸ¥è¯¢æ‚¨çš„æ•°æ®é›†ã€‚åŠ è½½DPRé—®é¢˜ç¼–ç å™¨ï¼Œç”¨<code class="du lw lx ly ln b">datasets.Dataset.get_nearest_examples()</code>æœç´¢é—®é¢˜</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="2005" class="ku if hh ln b fi lr ls l lt lu">from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer<br/>q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")<br/>q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")</span><span id="c071" class="ku if hh ln b fi lv ls l lt lu">question = "Is it serious ?"<br/>question_embedding = q_encoder(**q_tokenizer(question, return_tensors="pt"))[0][0].numpy()<br/>scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)<br/>retrieved_examples["line"][0]</span></pre><p id="fd3e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">ç¬¬äº”æ­¥:</strong>ä¿å­˜FAISSæŒ‡æ ‡</p><p id="ac56" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">æˆ‘ä»¬å¯ä»¥ç”¨<code class="du lw lx ly ln b">datasets.Dataset.save_faiss_index()</code>æŠŠç´¢å¼•ä¿å­˜åœ¨ç£ç›˜ä¸Š</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="035a" class="ku if hh ln b fi lr ls l lt lu">ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')</span></pre><p id="c743" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤6: </strong>åŠ è½½FAISSæŒ‡æ•°</p><p id="f682" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ç°åœ¨å¯ä»¥ä»ç£ç›˜åŠ è½½ç´¢å¼•äº†<code class="du lw lx ly ln b">datasets.Dataset.load_faiss_index()</code></p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="fb14" class="ku if hh ln b fi lr ls l lt lu">ds = load_dataset('crime_and_punish', split='train[:100]')<br/>ds.load_faiss_index('embeddings', 'my_index.faiss')</span></pre><h2 id="b9e6" class="ku if hh bd ig kv kw kx ik ky kz la io jn lb lc is jr ld le iw jv lf lg ja lh bi translated">å¼¹æ€§æœç´¢</h2><p id="515b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">ä¸FAISSä¸åŒï¼ŒElasticSearchåŸºäºæœç´¢ä¸Šä¸‹æ–‡ä¸­çš„ç²¾ç¡®åŒ¹é…æ¥æ£€ç´¢æ–‡æ¡£ã€‚</p><p id="2f94" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">æ›´å¤šä¿¡æ¯è¯·å‚è§<a class="ae kf" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html" rel="noopener ugc nofollow" target="_blank">å®‰è£…&amp;é…ç½®æŒ‡å—</a></p><p id="d359" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">å¼¹æ€§æœç´¢ä¸FAISSæœç´¢éå¸¸ç›¸ä¼¼ã€‚ç¤ºä¾‹å–è‡ªæ ‡å‡†Huggingfaceæ•°æ®é›†æ–‡æ¡£ã€‚</p><p id="33fd" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤1: </strong>åŠ è½½æ•°æ®é›†</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="0f99" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>squad = load_dataset('squad', split='validation')</span></pre><p id="823c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤2: </strong>å‘æ•°æ®é›†æ·»åŠ å¼¹æ€§æœç´¢</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="7940" class="ku if hh ln b fi lr ls l lt lu">squad.add_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")</span></pre><p id="b0ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">ç¬¬ä¸‰æ­¥:</strong>æ‰§è¡ŒæŸ¥è¯¢æœç´¢</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="4c4c" class="ku if hh ln b fi lr ls l lt lu">query = "machine"<br/>scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)<br/>retrieved_examples["title"][0]</span></pre><p id="b147" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">æ­¥éª¤4: </strong>å¦‚æœæä¾›äº†å§“åï¼Œåˆ™åŠ è½½å¼¹æ€§æœç´¢ç´¢å¼•</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="b1b8" class="ku if hh ln b fi lr ls l lt lu">from datasets import load_dataset<br/>squad = load_dataset('squad', split='validation')<br/>squad.load_elasticsearch_index("context", host="localhost", port="9200", es_index_name="hf_squad_val_context")<br/>query = "machine"<br/>scores, retrieved_examples = squad.get_nearest_examples("context", query, k=10)</span></pre><p id="8783" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">æ­£å¦‚æ‰€æ‰¿è¯ºçš„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ‰©å±•äº†ä¹‹å‰çš„<a class="ae kf" rel="noopener" href="/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f">ä½¿ç”¨HugginFaceæ•°æ®é›†åº“çš„ä¸€ç«™å¼æŒ‡å—</a></p><p id="d794" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">å¦‚æœä½ å–œæ¬¢è¿™ä¸ªåšå®¢ï¼Œè¯·è¡¨è¾¾ä½ çš„çˆ±ï¼Œç»™æˆ‘ä»¬ä¸€ä¸ªå¤§æ‹‡æŒ‡ï¼Œç»™æˆ‘ä»¬åŠ æ˜Ÿï¼Œå¦‚æœä¸å–œæ¬¢ï¼Œè¯·åœ¨è¯„è®ºåŒºç»™æˆ‘ä»¬ä¸€ä¸ªåé¦ˆã€‚å¸Œæœ›ä½ ä¼šåœ¨å›¾ä¹¦é¦†ç©å¾—å¼€å¿ƒï¼</p><p id="ae8d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ä¸ºäº†åˆä½œï¼Œå¸®åŠ©å’Œä¸€èµ·å­¦ä¹ -</p><p id="8da3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">åŠ å…¥æˆ‘ä»¬çš„ä¸å’Œè°æœåŠ¡å™¨:<a class="ae kf" href="https://discord.gg/Z7Kx96CYGJ" rel="noopener ugc nofollow" target="_blank">https://discord.gg/Z7Kx96CYGJ</a></p><p id="def4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ä¸€è·¯å¹³å®‰ï¼</p><div class="mj mk ez fb ml mm"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hi fi z dy mr ea eb ms ed ef hg bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">medium.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na mc mm"/></div></div></a></div><p id="734f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">ğŸ”µ<a class="ae kf" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="je hi">æˆä¸ºä½œå®¶</strong> </a></p></div></div>    
</body>
</html>