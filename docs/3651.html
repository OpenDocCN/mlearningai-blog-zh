<html>
<head>
<title>Regularization Techniques for Regression: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归的正则化技术:第1部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/regularization-techniques-for-regression-part-1-f583c0929c92?source=collection_archive---------7-----------------------#2022-10-03">https://medium.com/mlearning-ai/regularization-techniques-for-regression-part-1-f583c0929c92?source=collection_archive---------7-----------------------#2022-10-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="b5e4" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">了解什么是山脊和套索，为什么他们是如此强大的工具</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/cb2cb22e6404b6877ff7e138c21fc930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IKmkFDE4OGQQqIZdmKOPPw.png"/></div></div></figure><p id="e8d8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">正则化包括通过添加一个过程来调整回归模型，该过程通过将这些估计“缩小”到原点(到零)的惩罚来限制或正则化系数估计。</p><p id="9216" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">虽然正则化改进模型调整的原因并不明显，但一般来说，这组“收缩”技术有助于减少方差(这可以导致增加模型的预测能力)并允许变量选择。</p><p id="1378" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">特别是对于线性模型(包括<a class="ae ke" href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="noopener ugc nofollow" target="_blank">广义线性模型</a>，还有另一个优势:那些技术可以修复<a class="ae ke" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性</a>，这通常会产生不一致的系数估计。</p><p id="af74" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">最常用的正则化技术是Ridge和Lasso，它们之间的唯一区别是应用于系数的惩罚的格式。</p><p id="1a4d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">接下来，将根据找到使残差平方和(RSS)最小化的β_ 0，β_ 1，…，β_ p的过程来介绍这些技术，尽管也可以使用其他函数。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es kf"><img src="../Images/23f9e127affcb0ae2c486843f2b5cbbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3zV4Jl4QI1CrR2-s-jGHg.png"/></div></div></figure><p id="d338" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">以α为0，我们有山脊，α为1，我们有套索。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es kg"><img src="../Images/5a018372f6176a941112513c30d5a0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5N4lqUGMpXK1dDxO1v_vDw.png"/></div></div></figure><p id="ae7a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">岭回归</strong></p><p id="5c80" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">岭回归通过系数平方和的形式的惩罚来缩小系数。</p><p id="9de3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">λ被称为单独确定的调谐参数。注意，当λ为零时，罚值没有影响，岭回归将产生与普通最小二乘法相同的系数估计值。</p><p id="e375" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">当λ趋于无穷大时，惩罚的影响增加，系数估计将接近零(不完全是零)。</p><p id="021c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">对于每个λ，产生一组不同的系数。选择一个好的λ值是至关重要的，选择它的方法将在后面解释。</p><p id="67d3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在岭回归中，当一个预测因子乘以一个常数时，系数估计值可以被显著修改。所以建议把之前的预测因子标准化。</p><p id="370e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">值得一提的是，对于线性模型(包括广义线性模型)，系数估计中的惩罚有助于解决多重共线性带来的问题。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es kh"><img src="../Images/60ec44ec0629d913ca7b6642d74350be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoQq0uOUyOnQC-pnYDlRWg.png"/></div></div></figure><p id="9faf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">当存在多重共线性时，估计会产生不一致的系数估计值(用于模型训练的预测值的微小变化会导致估计值的重大变化)。该现象可在<strong class="jk hi">左图</strong>中观察到(出于解释目的，仅考虑两个预测值):该表面具有非常不稳定的脊/峰，用于寻找最小化先前呈现的函数的系数。</p><p id="a62a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">但是添加的惩罚在表面上创建了一个定义明确的凹陷(<strong class="jk hi">右图</strong>)，稳定了估计过程，返回了更一致的系数估计。</p><p id="804c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">拉索回归</strong></p><p id="82cc" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">套索回归与岭回归非常相似，但不同之处在于罚函数的格式，它是绝对值的和，而不是平方。</p><p id="58ce" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">Lasso回归也缩小了系数估计值，并具有相同的调整参数。</p><p id="c353" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">此外，之前为岭回归提供的所有信息也适用于套索:</p><ul class=""><li id="6a85" class="ki kj hh jk b jl jm jo jp jr kk jv kl jz km kd kn ko kp kq bi translated">λ等于零相当于没有应用惩罚，并且λ增加到无穷大返回零估计；</li><li id="5652" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd kn ko kp kq bi translated">有助于解决多重共线性，减少方差；</li><li id="a710" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd kn ko kp kq bi translated">建议将预测因子标准化。</li></ul><p id="fd45" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">岭回归不能准确地返回零系数估计值(可以返回接近零的值)，但Lasso回归可以。因此，这种正则化技术可以用作预测值选择(许多人可能称之为特征选择)和重要性排序(估计值等于零的预测值将被丢弃)的方法。</p><p id="70f5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">调谐参数选择</strong></p><p id="4d8c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">正如我之前所说，选择一个好的λ值是一个关键的决定。那么，我们如何做出这个决定呢？我们通过以下步骤使用交叉验证(adicionar link ):</p><ul class=""><li id="c7e1" class="ki kj hh jk b jl jm jo jp jr kk jv kl jz km kd kn ko kp kq bi translated">[1]定义调谐参数可能值的网格；</li><li id="6f38" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd kn ko kp kq bi translated">[2]训练模型，并为网格的每个可能值计算适当的误差度量。例如，用于回归的MSE(均方误差)或MAE(平均绝对误差)或用于分类的准确度；</li><li id="6f6b" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd kn ko kp kq bi translated">[3]选择调谐参数作为产生最小误差的值；</li><li id="87b9" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd kn ko kp kq bi translated">[4]用所有观察值和选择的调整参数训练最终模型。</li></ul><p id="4174" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在第2部分中，我将通过一些代码示例来应用这些正则化技术。</p><p id="b136" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我会把我的信息留在这里！</p><blockquote class="kw kx ky"><p id="75aa" class="ji jj kz jk b jl jm ii jn jo jp il jq la js jt ju lb jw jx jy lc ka kb kc kd ha bi translated"><strong class="jk hi">Linkedin:</strong><a class="ae ke" href="https://www.linkedin.com/in/marcos-augusto-47o47/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/marcos-augusto-47o47/</a><br/><strong class="jk hi">Github:</strong><a class="ae ke" href="https://github.com/MarcosAugusto47" rel="noopener ugc nofollow" target="_blank">https://github.com/MarcosAugusto47</a></p></blockquote><p id="4c33" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">资源</strong></p><ol class=""><li id="9a25" class="ki kj hh jk b jl jm jo jp jr kk jv kl jz km kd ld ko kp kq bi translated">Hastie，Tibshirani，r .，，j . h . Friedman(2009年)。统计学习的要素:数据挖掘、推理和预测。第二版。纽约，斯普林格。</li><li id="babe" class="ki kj hh jk b jl kr jo ks jr kt jv ku jz kv kd ld ko kp kq bi translated">加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼。(2013).统计学习导论:应用。</li></ol><div class="le lf ez fb lg lh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="li ab dw"><div class="lj ab lk cl cj ll"><h2 class="bd hi fi z dy lm ea eb ln ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lo l"><h3 class="bd b fi z dy lm ea eb ln ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lp l"><p class="bd b fp z dy lm ea eb ln ed ef dx translated">medium.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv jg lh"/></div></div></a></div></div></div>    
</body>
</html>