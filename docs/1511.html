<html>
<head>
<title>Paper Summary [Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要[Axial-DeepLab:独立Axial-全景分割注意]</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/paper-summary-axial-deeplab-stand-alone-axial-attention-for-panoptic-segmentation-bae2d8f35015?source=collection_archive---------0-----------------------#2021-12-28">https://medium.com/mlearning-ai/paper-summary-axial-deeplab-stand-alone-axial-attention-for-panoptic-segmentation-bae2d8f35015?source=collection_archive---------0-----------------------#2021-12-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="ef95" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated"><strong class="ik hi"> <em class="hh">请注意，这篇帖子是为了我将来很可能的研究在没有完全阅读</em> </strong> <a class="ae jg" href="https://arxiv.org/pdf/2003.07853" rel="noopener ugc nofollow" target="_blank"> <strong class="ik hi"> <em class="hh">论文</em> </strong> </a> <strong class="ik hi"> <em class="hh">的情况下回过头来复习关于这个题目的材料。</em>T13】</strong></p></blockquote></div><div class="ab cl jh ji go jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="ha hb hc hd he"><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/f80968fa0675b11fc35b881f9dad0ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OOes72dKwxrvXbkhNVOhQ.png"/></div></div></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es ka"><img src="../Images/c717216b3e8dc8c291385765f2110c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*G-5DLbwXHtXPckE-dN7oXQ.png"/></div></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kb"><img src="../Images/4c9181d3ae0a998dfa38500a255dd15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-sQKHfuAq90sz1vCXn3s9g.png"/></div></div></figure><p id="fdd6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">在摘要中，作者提到卷积运算符使用局部性来提高效率，使用代价来获得长程相关性。还提到，通过最近的研究，认识到通过限制到局部来堆叠自我注意层以采取完全注意网络是可能的。</p><p id="aaa9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">在本文中，作者试图消除这一限制(如何？)，通过把2D的注意力转移到1D身上。因此，结果是，计算复杂度降低，并且使网络在更大的区域中工作。此外，本研究提出了一种位置敏感的自我注意结构，并将其应用于4个大型数据集进行分类、全景分割、实例分割和语义分割。</p><h1 id="79df" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><em class="ld">简介</em></h1><p id="8f9d" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">卷积被认为是计算机视觉中的核心模块。因为两个特征:平移等方差性(等同于成像的性质，因此，将模型推广到各种位置)和局部性(减少参数计数和M-加法)。然而，这使得长期依赖变得困难。</p><p id="f572" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">注意力为诸如语言建模、语音识别和神经字幕等各种工作中的长期依赖提供了建模能力。而且在计算机视觉(图像分类、物体检测、语义分割、视频分类、对抗性防御)方面潜力巨大。</p><p id="f188" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">虽然堆叠注意层已经显示出有希望的结果，但是它们的简单架构在计算上是昂贵的。在另一项研究中，应用了局部限制，结果降低了成本，尽管这限制了模型。</p><p id="a34b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">在这项研究中，提出了轴向注意，它允许有效的计算和恢复注意模型中的大感受野。主要思想是沿高度轴和宽度轴依次分解2D到1D的注意力。此外，位置术语是根据上下文添加的，这导致注意力对位置敏感(成本很低)。</p><h1 id="14ff" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">方法</h1><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lj"><img src="../Images/07ec80f15cc93421a8ef569b938d7977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UIJnew-jBFuVSZFcq5qsQ.png"/></div></div></figure><h2 id="c442" class="lk kg hh bd kh ll lm ln kl lo lp lq kp kc lr ls kt kd lt lu kx ke lv lw lb lx bi translated">位置敏感自我注意</h2><p id="d2a3" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">位置<em class="ij"> o = (i，j) </em>处的输出可以通过合并预计输入来计算，如下所示:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ly"><img src="../Images/7e007fc3b9407129c3413d3d96847503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ncLZyC0_h7dh3-BUs0JSg.png"/></div></div></figure><p id="0da3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">机制池值让我们在整个特征图中捕获相关但非局部的上下文(<em class="ij"> NB。</em>卷积算子只是捕捉局部关系)。</p><p id="ebb5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">缺点有两个:1。自我关注是非常昂贵的，仅限于高级别的CNN(例如，下采样特征图)或小图像。2.全局池不提取位置信息(这在计算机视觉中至关重要)。通过为自我关注添加局部约束和位置编码，这两个问题得到了缓解。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lz"><img src="../Images/9cf57cece184b78b152531367e24f1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPk_9zb4XlhFfHv8nLq8pA.png"/></div></div></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ma"><img src="../Images/9afd0ccb7a4ba0e2f38b32e51263002a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snZRzwYYrn93JjbqHA_C9w.png"/></div></div></figure><h2 id="96be" class="lk kg hh bd kh ll lm ln kl lo lp lq kp kc lr ls kt kd lt lu kx ke lv lw lb lx bi translated">位置敏感性</h2><p id="1a97" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">注意，先前的位置偏差与查询像素Xo高度相关，而与关键像素(Xp)无关。但是，关键像素可以具有关于要连接的位置的信息。因此，除了依赖于查询的偏差之外，还增加了依赖于键的位置偏差。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mb"><img src="../Images/edec06f7ea81936dbadd86bb1eb81f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voZw4XItinvF0IsTAg8PIA.png"/></div></div></figure><p id="a1fa" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">这种设计可以被称为<em class="ij">位置敏感</em>自我注意，用精确的位置信息捕捉长依赖。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mc"><img src="../Images/363ed9fe8ceec92879dcec39cad58693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-1c0nsCOl7z_738BkWE7w.png"/></div></div></figure><h2 id="364c" class="lk kg hh bd kh ll lm ln kl lo lp lq kp kc lr ls kt kd lt lu kx ke lv lw lb lx bi translated">axia-注意</h2><p id="1229" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">局部约束大大降低了计算成本，并提供了一个完全自注意模型的建设。在局部正方形区域中操作的局部自注意仍然具有与区域长度成平方的复杂度，这引入了另一个在性能和计算复杂度之间进行折衷的超参数。</p><p id="9fa5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">本文提出了独立自注意中的轴向注意，以保证全局连接和高效计算。其层描述如下:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es md"><img src="../Images/fbcd9e10a61853626164dcdc9a058095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjKhjzvDMfw-Eppe17_JYA.png"/></div></div></figure><h2 id="d187" class="lk kg hh bd kh ll lm ln kl lo lp lq kp kc lr ls kt kd lt lu kx ke lv lw lb lx bi translated">残差神经网络</h2><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es me"><img src="../Images/c5153b435ad2b1760b31220972552192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DuwbsIECJKV67VZsX_vP6w.png"/></div></div></figure><p id="48fc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">剩余瓶颈块中的3×3卷积由两个多头轴向注意层代替，以便将ResNet转换为轴向ResNet。为了混洗特征，保持两个1×1卷积。这样，在图2(上图)中可以看到轴向注意阻断。</p><h2 id="6608" class="lk kg hh bd kh ll lm ln kl lo lp lq kp kc lr ls kt kd lt lu kx ke lv lw lb lx bi translated">轴向深度实验室</h2><p id="9799" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">对于分段任务，实施了更多的更改以将Axial-ResNet转换为Axial-DeepLab，如下所述:</p><ol class=""><li id="539d" class="mf mg hh ik b il im ip iq kc mh kd mi ke mj jf mk ml mm mn bi translated">DeepLab改变了ResNet最后一两个阶段的步幅和节奏。同样，最后一个阶段的步幅被移除，但“atrous”注意力模块没有实现。</li><li id="4166" class="mf mg hh ik b il mo ip mp kc mq kd mr ke ms jf mk ml mm mn bi translated">不采用atrous空间金字塔池模块(ASPP)。结果表明，无论有无ASPP，Axial-DeepLab都能正常工作。</li><li id="4312" class="mf mg hh ik b il mo ip mp kc mq kd mr ke ms jf mk ml mm mn bi translated">在接下来的panopic-deep lab中采用了相同的三卷积、双解码器和预测头。</li></ol><p id="7927" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kc iu iv iw kd iy iz ja ke jc jd je jf ha bi translated">在此之后，作者们带来了他们在这里没有提到的各种数据集上的结果。</p><h1 id="5fee" class="kf kg hh bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">结论和讨论</h1><p id="87a1" class="pw-post-body-paragraph ih ii hh ik b il le in io ip lf ir is kc lg iv iw kd lh iz ja ke li jd je jf ha bi translated">这篇科学论文可以被认为是已经做出的完全摆脱卷积并部署单独注意力模型的尝试之一。然而，轴向注意模型保持M-Adds，它比卷积花费更多的时间，主要原因可以认为是目前在各种加速器上缺乏专门的内核。</p><blockquote class="ie if ig"><p id="3e3a" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">如果发现任何错误，请发电子邮件到rezayazdanfar1111@gmail.com找我。与此同时，在我的Twitter上关注我<a class="ae jg" href="https://twitter.com/reza__yazdanfar" rel="noopener ugc nofollow" target="_blank">这里</a>，在LinkedIn上访问我的<a class="ae jg" href="https://www.linkedin.com/in/reza-yazdanfar-b69055156/" rel="noopener ugc nofollow" target="_blank">这里</a>。最后，如果你发现它有用，并想继续写文章，请在<a class="ae jg" href="https://rezayazdanfar.medium.com/" rel="noopener">媒体中关注我。</a>最后，如果你有任何想法或建议，我很乐意接受，你只需要在LinkedIn上给我发消息。🙂</p></blockquote><div class="mt mu ez fb mv mw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hi fi z dy nb ea eb nc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">medium.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk jy mw"/></div></div></a></div></div></div>    
</body>
</html>