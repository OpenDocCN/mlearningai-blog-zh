<html>
<head>
<title>Backpropagation &amp; Neural-Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播和神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/backpropagation-neural-nets-e4d5f65608fa?source=collection_archive---------2-----------------------#2021-05-12">https://medium.com/mlearning-ai/backpropagation-neural-nets-e4d5f65608fa?source=collection_archive---------2-----------------------#2021-05-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/943c2fc2279df28b8ce582a2fd9b6d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOps850M_ih0mMSsvN44uA.png"/></div></div></figure><div class=""/><div class=""><h2 id="1da3" class="pw-subtitle-paragraph ip hr hs bd b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg dx translated">视觉方法</h2></div><h1 id="e298" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">动机</h1><p id="cd47" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">有时候，对某个概念形成直觉的最好方式是把它从它的<em class="kv">黑箱</em>中拿出来。反向传播是机器学习中的一个核心概念，但它很少出现在所有库和标准化函数之后。让我们看看引擎盖下，解开神经网络的代数和微积分之谜。</p><h1 id="24be" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">神经元</h1><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kw"><img src="../Images/3e4f8f512ed6631f9298789e9dd555b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_dZmT-DrG1jZoGmZeYakA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Diagram of a neuron and the 2 objects that define it — weights (<strong class="bd jj">w</strong>) and bias term (b)</figcaption></figure><p id="17ce" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">简单来说，一个神经元接收一组输入，对这些输入执行某种计算:输入的一个<em class="kv">线性组合</em>，一个加权和，包装成一个激活函数。</p><blockquote class="lk ll lm"><p id="1b42" class="jz ka kv kb b kc lf it ke kf lg iw kh ln lh kk kl lo li ko kp lp lj ks kt ku ha bi translated"><strong class="kb ht">激活功能</strong></p><p id="268f" class="jz ka kv kb b kc lf it ke kf lg iw kh ln lh kk kl lo li ko kp lp lj ks kt ku ha bi translated">虽然这不是本文的重点，但激活函数只是一个转换，它将神经元累积部分的结果的不定范围(- <strong class="kb ht"> ∞ </strong>，<strong class="kb ht"> ∞ </strong>)转换为具有特定形状的期望域，例如(0，1)或(-1，1)。</p></blockquote><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lq"><img src="../Images/4738b5ef47fe6d3c5462f50aaff926f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kHgYVL2fwckD4NWb1aeM1Q.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Sigmoid and ReLU activation functions. Sigmoid squashes the range (-<strong class="bd jj">∞</strong>,<strong class="bd jj">∞</strong>) to (0,1) while ReLU cancels negative outputs.</figcaption></figure><h2 id="2859" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">神经元中的计算</h2><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mf"><img src="../Images/53aa29dfce37d3dfd3ad8b3a3eee496e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mObM0EgIf8GQyAPEcQte6w.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">The computations in a neuron.</figcaption></figure><p id="888b" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">神经元的输出(<em class="kv"> a </em>)被计算为应用于预激活(<em class="kv"> pa </em>)的激活函数——加权(<strong class="kb ht"> <em class="kv"> w </em> </strong>)输入(<strong class="kb ht"> <em class="kv"> x </em> </strong>)和自由项，即偏差(<em class="kv"> b </em>)的总和。激活函数与架构一起选择，因此只有权重(<strong class="kb ht"> <em class="kv"> w </em> </strong>)和偏置(<em class="kv"> b </em>)项可以被<em class="kv">微调</em>以改变神经元的行为。一点一点地调整这些变量就构成了网络的训练。</p><p id="4a5f" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">接下来是神经元的完整图表(明确显示了偏置和激活)。在本文的其余部分，为简单起见，神经元将仅用它的激活来表示。</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mf"><img src="../Images/04558d25c907e2a399e37c095829a1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*47LtvtcApqSVRK9pVTRESQ.png"/></div></div></figure><h1 id="03f3" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">该层</h1><p id="3934" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">我们现在考虑一层多个神经元。输入由相同的<em class="kv"> p </em>神经元给出，但是现在它们中的每一个都连接到下一层中的<em class="kv"> q </em>神经元中的每一个(密集<em class="kv">连接)。</em></p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kw"><img src="../Images/31f1627fb8326eb90af2fc2bda6d6c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bo8mSYcEYB2Z4FwkYOGzw.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Diagram of a layer and the 2 objects that define it — weights (<strong class="bd jj">W</strong>) and biases (<strong class="bd jj">b</strong>)</figcaption></figure><p id="0d94" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">自然地，所有的<em class="kv"> b </em>、<em class="kv"> w </em>和<em class="kv"> a </em>都获得了一个额外的维度(我们用粗体表示数组，用大写字母表示矩阵)。权重现在形成一个矩阵，包含所有的成对组合。</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kw"><img src="../Images/de83d783d0d60ac3ce8c3992a9374fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjpHSmxLS_28yxf-QnPWeQ.png"/></div></div></figure><h2 id="def2" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">层中的计算</h2><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mg"><img src="../Images/5b161d8bdf7f390300eb4a089501f079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ea7O_lExT4I3KUZ06NSHcQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">The computations in a layer.</figcaption></figure><p id="3cc5" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">层的输出(<strong class="kb ht"> <em class="kv"> a </em> </strong>)是一个维度为<em class="kv"> q </em>(层中神经元的数量)的数组。权重矩阵(<strong class="kb ht"> <em class="kv"> W </em> </strong>)有<em class="kv"> p </em>行(输入数)和<em class="kv"> q </em>列。</p><h1 id="d591" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">网络</h1><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kw"><img src="../Images/dbb49b4e75e13ba4e4b2caf85421ba06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i08o5QtBKmCf7DTCzuD4cA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Diagram of a part of a network defined by a collection of matrixes — weights (<strong class="bd jj">W</strong>) and biases (<strong class="bd jj">b</strong>)</figcaption></figure><p id="473f" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">进一步展开讨论，网络只是多个层的集合，其输入是前一层的输出(因此，我们不再使用<strong class="kb ht"> <em class="kv"> x </em> </strong>来表示)。此外，<em class="kv"> (L) </em>表示层的索引<em class="kv">。</em></p><h2 id="be36" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">输入/输出</h2><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es kw"><img src="../Images/3801646a6f3a05373886a7f0a0be20aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23zZ_DhyOe_dAhm9i80LMw.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Inputs &amp; Outputs of a neural network.</figcaption></figure><p id="6e61" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">将神经网络包装回它的黑盒中，我们可以参考它的输入和输出。因此，<em class="kv">推理</em>是针对输入<strong class="kb ht"> <em class="kv"> x </em> </strong>获得相应输出<strong class="kb ht"><em class="kv">【y *</em></strong>的过程，而<em class="kv">训练</em>是逐渐调整网络内部参数(权重和偏差)的过程，使得输出<strong class="kb ht"><em class="kv"/></strong>尽可能接近地匹配已知的基本事实<strong class="kb ht"> <em class="kv"> y. </em> </strong></p><blockquote class="lk ll lm"><p id="b0e7" class="jz ka kv kb b kc lf it ke kf lg iw kh ln lh kk kl lo li ko kp lp lj ks kt ku ha bi translated">基本事实是我们所知道的网络的期望输出。它可以是一个数字(例如预测价格)或形成概率分布的多个数字(例如分类任务——每个值将是对象属于某一类的可能性)或者甚至是二维矩阵、图像。输出可以是任何东西，只要我们有办法知道它离期望的结果有多远。这项措施将是我们的损失，我们将训练我们的网络，以尽量减少它。</p></blockquote><h1 id="3d29" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">反向传播</h1><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/7957c4d7728b97e9257fe5a422175db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ubrVLEmBs4_5qP4lqwONZw.png"/></div></div></figure><p id="e2c5" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">在大多数情况下，训练数据集成对出现(<strong class="kb ht"> <em class="kv"> x，y </em> </strong>)。<em class="kv">损失— </em>预测值<strong class="kb ht"><em class="kv"/></strong>与实际值<strong class="kb ht"><em class="kv">y</em></strong><em class="kv">—</em><strong class="kb ht"><em class="kv"/></strong>之差是我们想要优化的指标，即我们希望它最小。为了使它变得最小，我们计算对权重和偏差的必要调整。这是导数告诉我们的:<em class="kv">这里的一点点推动会对那里的结果产生多大的影响？我们想知道在所有权重和偏差的值中所有微小的推动的效果。</em></p><h1 id="0308" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">偏导数</h1><p id="60d1" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">导数测量变化率。<em class="kv">速度是位置的变化率，加速度是速度的变化率</em>。当函数有多个决定其值的参数时，偏导数就起作用了。在这种情况下，我们想知道在所有其他参数保持不变的情况下，一个参数的变化会在多大程度上改变函数的结果。</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/09a21d44fc9d1dc6ee69c84718947814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I7XPqAAMRADbNqo8l7fVCA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Visualizing nudges in the values of the parameters (<strong class="bd jj">x1, x2</strong>) of a function. The 2 directions are perpendicular, symbolizing that one parameter does not affect the other. We imagine the slope in the <strong class="bd jj">x1</strong> direction to be steeper, thus a nudge in that direction produces a greater effect than a nudge in the <strong class="bd jj">x2</strong> direction.</figcaption></figure><h2 id="005c" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">最优化中的偏导数</h2><p id="ef27" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">任何优化任务的最终目标都是找到极值点:最大值或最小值。这很少是无关紧要的，因为函数值在整个定义域中都是未知的，只在我们选择评估的点上是未知的。因此，优化策略(如梯度下降)用于在最优方向上迭代选择点。</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mh"><img src="../Images/d1dc45777fddd69a1bfbd935a6b69f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bYyjR-0a1Ya4ptYdCaDiA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Surface Plot of <a class="ae mi" href="https://en.wikipedia.org/wiki/Himmelblau%27s_function" rel="noopener ugc nofollow" target="_blank">Himmelblau’s function</a></figcaption></figure><p id="0255" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">导数是一个数学指南针:正的导数表示函数值在那个方向增加，负的导数表示值在减少。因此，寻找最小值需要遵循负导数的方向。</p><h1 id="78b4" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">概述:</h1><ul class=""><li id="6ebe" class="mj mk hs kb b kc kd kf kg ki ml km mm kq mn ku mo mp mq mr bi translated">权重和偏差是决定网络输出的可调参数</li><li id="3f4d" class="mj mk hs kb b kc ms kf mt ki mu km mv kq mw ku mo mp mq mr bi translated">我们将结果输出与来自训练数据集的期望的基本事实进行比较</li><li id="6687" class="mj mk hs kb b kc ms kf mt ki mu km mv kq mw ku mo mp mq mr bi translated">损失是当前值和目标值之间的差值，我们希望它最小</li><li id="a479" class="mj mk hs kb b kc ms kf mt ki mu km mv kq mw ku mo mp mq mr bi translated">我们借助偏导数来计算权重和偏差的微小调整如何影响损失的价值</li><li id="9e68" class="mj mk hs kb b kc ms kf mt ki mu km mv kq mw ku mo mp mq mr bi translated">我们应用那些最小化损失的微小调整，逐渐使我们的神经网络适应数据</li></ul><h1 id="cc9d" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">计算变化</h1><p id="8b1e" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">我们知道问题的核心:反向传播中执行的实际计算。</p><h2 id="8237" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">向前传球的逻辑顺序</h2><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mg"><img src="../Images/908597fefda65e8697778961e5d44a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1A-aBclNAGaYtMO_7yzqQ.png"/></div></div></figure><p id="288e" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">正向传递意味着输入<strong class="kb ht"> <em class="kv"> x </em> </strong>沿着整个网络传递。我们认为最后一次激活<strong class="kb ht">T5</strong>是网络的输出。因此，我们考虑一个基本的损失函数:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/b51189b93c7e40acecfbac8c04de816f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kj4QsPgbh8XNG8N6cSOPCA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">A basic loss function. <strong class="bd jj">y*</strong> is the output of the net, equal to the activation <strong class="bd jj">a</strong> of the last layer <strong class="bd jj">(L)</strong>. <strong class="bd jj">y</strong> is the ground-truth label.</figcaption></figure><h2 id="7149" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">链式法则</h2><p id="7ae3" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">我们想知道损耗相对于最后一层激活是如何变化的:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/f5d50ae66a15ded46d9cfd61f0ef7941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8yQxRelGkjl4xQuIHyBfqQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">How the loss changes with regards to the activation <strong class="bd jj">a</strong> of the last layer <strong class="bd jj">(L)</strong>.</figcaption></figure><p id="7308" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">这很简单(因为<strong class="kb ht"> <em class="kv"> y </em> </strong>被认为是常数)，但是我们仍然没有太多的信息。我们需要更深入一点。回想一下，对于层<strong class="kb ht"> <em class="kv"> (L) </em> </strong>我们有:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/db27420aae5c9aeae4e242e25a5e9c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_R0Ilwsi6iWNRxJUlnNIvA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">Activation <strong class="bd jj">a</strong> of a layer <strong class="bd jj">(L)</strong> as a function of the activation of the previous layer <strong class="bd jj">(L-1)</strong>. <strong class="bd jj">pa</strong> stands for pre-activation. Weights matrix <strong class="bd jj">W</strong> is transposed but for simplicity, we will ignore this as it doesn’t affect the derivatives.</figcaption></figure><p id="31cf" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">因此，应用<em class="kv">链式法则</em>得到损耗相对于最后一层的预激活<strong class="kb ht"> <em class="kv"> pa </em> </strong>的变化，我们得到:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/4e529c8bb8ee385432fa03d40cba0258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bn3aogP7fnoUePsji2LE6A.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">How the loss changes with regards to the pre-activation <strong class="bd jj">pa</strong> of the last layer <strong class="bd jj">(L)</strong>.</figcaption></figure><blockquote class="lk ll lm"><p id="c221" class="jz ka kv kb b kc lf it ke kf lg iw kh ln lh kk kl lo li ko kp lp lj ks kt ku ha bi translated">这就是为什么我们需要选择容易区分的激活函数，用于层的激活相对于其预激活的偏导数。</p></blockquote><p id="8369" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">但是我们知道预激活实际上是由权重和偏差决定的，所以我们扩展了链式法则:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/5b5cc5f69a826a52fdf89ed54388d68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOjv8HmwxYRi4OXF-_vqmQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">How the loss changes with regards to the weights <strong class="bd jj">W</strong> of the last layer <strong class="bd jj">(L)</strong>. The transpose operation of the weights has been ignored for simplicity, as it doesn’t affect the derivatives.</figcaption></figure><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/c3763413b21183d056b52986069a1c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRZqy05KHZzg3EIpc1T6tg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">How the loss changes with regards to the weights <strong class="bd jj">b</strong> of the last layer <strong class="bd jj">(L)</strong>. The transpose operation of the weights has been ignored for simplicity, as it doesn’t affect the derivatives.</figcaption></figure><h2 id="dbb0" class="lr ji hs bd jj ls lt lu jn lv lw lx jr ki ly lz jt km ma mb jv kq mc md jx me bi translated">进一步反向传播</h2><p id="5542" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">现在，我们要做的就是对每一层都应用相同的原则，从上一层开始往回走:</p><figure class="kx ky kz la fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mx"><img src="../Images/80991e5b3dd877f967cbdcc779ea633d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rX3a4z9B7JvBCvvNDhgfdQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx">How the loss changes with regards to the activation <strong class="bd jj">a</strong> of the layer <strong class="bd jj">(L-1)</strong></figcaption></figure><p id="af7d" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">计算损耗相对于所有其他层的权重和偏差如何变化也是类似的。</p><h1 id="06f3" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">更新权重和偏差</h1><p id="9898" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">知道了权重和偏差如何影响损失值，现在优化器的工作就是决定参数更新的实际值。</p><blockquote class="lk ll lm"><p id="3233" class="jz ka kv kb b kc lf it ke kf lg iw kh ln lh kk kl lo li ko kp lp lj ks kt ku ha bi translated">一个基本的优化器，梯度下降，更新当前值与当前梯度成比例。它的目标是通过以下递减值找到最小值，并以与当前斜率成比例的步长前进:越陡，步长越大。但是优化器的工作并不是无足轻重的——有时，要到达山的底部，仍然需要向上爬一点。</p></blockquote><figure class="kx ky kz la fd hj er es paragraph-image"><div class="er es my"><img src="../Images/c5b6854117e0264ebbcd261884727fe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*IxMLWG1xsZ50b91M5VJXSA.jpeg"/></div><figcaption class="lb lc et er es ld le bd b be z dx">Visualization of a Gradient Descent algorithm for a function with 2 parameters.</figcaption></figure><p id="83ac" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated">从本质上说，偏导数就像是我们用来定位自己的<em class="kv">罗盘</em>，而优化器就像是T2的方向盘(和油门踏板)，推动我们前进，希望能达到最优。但是这仍然是另一篇文章的主题。</p><h1 id="6c30" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">结论</h1><p id="dd91" class="pw-post-body-paragraph jz ka hs kb b kc kd it ke kf kg iw kh ki kj kk kl km kn ko kp kq kr ks kt ku ha bi translated">我希望这能给神经网络背后的计算带来一些启发和直觉。我鼓励任何人分享他或她正在研究的概念的类似的个人解释。没有“完美”的解释，但我相信最重要的是让好奇心运转起来。</p><h1 id="dbd7" class="jh ji hs bd jj jk jl jm jn jo jp jq jr iy js iz jt jb ju jc jv je jw jf jx jy bi translated">来源</h1><div class="hg hh ez fb hi mz"><a href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd ht fi z dy ne ea eb nf ed ef hr bi translated">神经网络和深度学习</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">在上一章中，我们看到了神经网络如何使用梯度下降算法学习它们的权重和偏差…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">neuralnetworksanddeeplearning.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ho mz"/></div></div></a></div><p id="f0fb" class="pw-post-body-paragraph jz ka hs kb b kc lf it ke kf lg iw kh ki lh kk kl km li ko kp kq lj ks kt ku ha bi translated"><a class="ae mi" href="https://towardsdatascience.com/part-2-gradient-descent-and-backpropagation-bf90932c066a" rel="noopener" target="_blank">https://towards data science . com/part-2-gradient-descent-and-back propagation-BF 90932 c 066 a</a></p></div></div>    
</body>
</html>