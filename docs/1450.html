<html>
<head>
<title>What are the activation functions in Machine Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的激活函数有哪些？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/what-are-the-activation-functions-in-machine-learning-37bae6888858?source=collection_archive---------5-----------------------#2021-12-19">https://medium.com/mlearning-ai/what-are-the-activation-functions-in-machine-learning-37bae6888858?source=collection_archive---------5-----------------------#2021-12-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/293c131099d59f0fa7a2096368c1ae7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vBB7p89YwOco4bYQ"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx">Photo by <a class="ae hu" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Possessed Photography</a> on <a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="8a7d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">人们会对机器学习<strong class="iw hy"> </strong>和数据科学的流行应用感到惊讶。有许多行业转向人工智能以增强其应用。据<strong class="iw hy"> Forbes </strong>报道，数据科学被认为是当前就业市场上极具价值的工作。有公司寻求数据科学和人工智能方面的专业人士。</p><figure class="jt ju jv jw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es js"><img src="../Images/5847f49535ac5b057b16a6631996b0a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uHo6x-P243u1WYYK"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx">Photo by <a class="ae hu" href="https://unsplash.com/@rocknrollmonkey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rock'n Roll Monkey</a> on <a class="ae hu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a3f5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有了一个健壮的机器学习模型，就有可能为社会中一些错综复杂的问题提供解决方案。例如，来自算法的预测可以用于检测癌症的<strong class="iw hy">存在或不存在。或者它们可以被部署在其他应用中，例如<strong class="iw hy">通信网络</strong>，其中这些可以被用来预测线路故障的可能性等等。随着人工智能研究和潜在应用的兴起，应用似乎是无止境的。</strong></p><p id="d295" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">因此，有必要了解这些算法工作背后的理论，以便实际做出预测，为许多应用产生最佳结果。机器学习中经常出现的东西是正在使用的激活函数，尤其是在深度学习中。因此，深入理解激活功能在深度学习领域也非常有用。现在让我们回顾一下正在使用的一些激活功能。</p><h2 id="f80f" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated"><strong class="ak">什么是激活功能？</strong></h2><p id="1ae7" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">训练神经网络时，在网络中添加激活单元以确保模型表现良好是很重要的。激活函数增加了结构中的非线性，从而可以识别输入和输出之间的复杂关系。现在让我们回顾一下可以在我们的网络中使用的各种激活功能。</p><h2 id="24c2" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">乙状结肠:</h2><p id="757c" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">下面是<strong class="iw hy"> sigmoid </strong>函数的数学表达式。它使用指数来计算输入的激活。对于<strong class="iw hy">二级分类问题</strong>，使用<strong class="iw hy">s形</strong>最为合适。</p><figure class="jt ju jv jw fd hj er es paragraph-image"><div class="er es kx"><img src="../Images/9c7c50f83d42312e12e1cc48eb8298de.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*fSrKrnOPIoRMkk4BFmUGgA.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx">Sigmoid Activation Function</figcaption></figure><figure class="jt ju jv jw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ky"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">Sigmoid function — Wikipedia</a></figcaption></figure><p id="a6ff" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">高度负值的<strong class="iw hy"> sigmoid </strong>函数的值趋向于0。类似地，从图中可以看出，随着对它的输入的增加，它的值改变并趋向于1。基于此，可以看出使用<strong class="iw hy"> sigmoid </strong>函数来解决<strong class="iw hy">二级分类问题</strong>最为合适。</p><h2 id="0339" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated"><strong class="ak">谭:</strong></h2><p id="0e26" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">从下面可以看出<strong class="iw hy"> tanh </strong>激活与<strong class="iw hy"> sigmoid </strong>功能的不同之处。有许多指数值，这也导致不同于a<strong class="iw hy">s形</strong>的曲线。</p><figure class="jt ju jv jw fd hj er es paragraph-image"><div class="er es kz"><img src="../Images/4611b35114572ca370da3e803285d0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*SBcMLQ2rP77M9uuXYtX0hA.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" rel="noopener" href="/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e">Activation Functions — All You Need To Know! | by Sukanya Bag | Analytics Vidhya | Medium</a></figcaption></figure><figure class="jt ju jv jw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es la"><img src="../Images/f28a55143bc35a63c19e962b2a97b964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vv2nOKhVJ1rtrE8PxhUV7Q.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">Activation function — Wikipedia</a></figcaption></figure><p id="771c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">双曲正切函数的值分别位于<strong class="iw hy"> -1 </strong>和<strong class="iw hy"> 1 </strong>之间。随着输入值不断向正方向增加，来自<strong class="iw hy"> tanh </strong>的输出将趋向于1。类似地，随着输入不断减少，由<strong class="iw hy"> tanh，</strong>产生的输出也会减少，并趋向于-1。</p><h2 id="d155" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated"><strong class="ak"> ReLU: </strong></h2><p id="e8bb" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">这个函数在深度学习应用中相当流行。当训练具有良好层数的大型深度神经网络时，在执行<strong class="iw hy">反向传播</strong>时，梯度可能会变为0。结果，这就导致了一个叫做消失梯度的问题。</p><p id="4f66" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为了避免这种情况，使用<strong class="iw hy"> ReLU </strong>来避免该问题，因为该值在0之后增加，以此类推。通过实验还得知<strong class="iw hy"> ReLU </strong>在深度学习方面的表现优于sigmoid。下面是<strong class="iw hy"> ReLU </strong>的表达式。</p><figure class="jt ju jv jw fd hj er es paragraph-image"><div class="er es lb"><img src="../Images/d03f749ef86acdefb8185aca094a9e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*y--mv2_OMJw8d6GvNHqYUA.png"/></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" rel="noopener" href="/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e">Activation Functions — All You Need To Know! | by Sukanya Bag | Analytics Vidhya | Medium</a></figcaption></figure><figure class="jt ju jv jw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es la"><img src="../Images/7d11fa889d55235c9efa3d794996907e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwABi2MYg-Hy8k4ce7wfbw.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">Activation function — Wikipedia</a></figcaption></figure><h2 id="3118" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated"><strong class="ak">泄漏的ReLU: </strong></h2><p id="600c" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">该函数与<strong class="iw hy"> ReLU </strong>函数非常相似，除了当其输入小于0时，会有一个小斜率，而不是值完全变为0。结果，这导致消失梯度问题的良好减少。</p><figure class="jt ju jv jw fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lc"><img src="../Images/25a8d83b4688cc4afc8be95583611420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGaJoDp1qWx4G44FFQbhug.png"/></div></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" rel="noopener" href="/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e">Activation Functions — All You Need To Know! | by Sukanya Bag | Analytics Vidhya | Medium</a></figcaption></figure><figure class="jt ju jv jw fd hj er es paragraph-image"><div class="er es ld"><img src="../Images/c927256979e273c88e8b6cac0941c773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FIBp_2YvsMrgP7Tnx8i1lw.jpeg"/></div><figcaption class="hq hr et er es hs ht bd b be z dx"><a class="ae hu" rel="noopener" href="/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e">Activation Functions — All You Need To Know! | by Sukanya Bag | Analytics Vidhya | Medium</a></figcaption></figure><p id="4fc1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">从上面可以看出<strong class="iw hy"> ReLU </strong>和<strong class="iw hy">泄漏ReLU </strong>之间的区别。随着输入值不断下降到0以下，输出也会有小幅下降。然而，在<strong class="iw hy"> ReLU </strong>的情况下，这种减少导致输出等于0。</p><h2 id="70a6" class="jx jy hx bd jz ka kb kc kd ke kf kg kh jf ki kj kk jj kl km kn jn ko kp kq kr bi translated">结论</h2><p id="dd9c" class="pw-post-body-paragraph iu iv hx iw b ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn kw jp jq jr ha bi translated">在看了这些函数之后，对于大多数深度学习功能来说，使用<strong class="iw hy"> ReLU </strong>将是理想的。但是S <strong class="iw hy"> igmoid </strong>可以用在二进制分类问题的最后一层。<strong class="iw hy"> ReLU </strong>和<strong class="iw hy"> Leaky ReLU </strong>是在许多应用中使用的相当流行的激活函数。</p><p id="e079" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">但是，请在您的一些应用程序中随意测试这些激活函数，通过比较实际输出和模型的预测输出，看看哪一个执行得最好！</p><p id="274c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">希望这篇文章对你有所帮助。欢迎分享您的想法和反馈。谢了。</p><div class="hg hh ez fb hi le"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lf ab dw"><div class="lg ab lh cl cj li"><h2 class="bd hy fi z dy lj ea eb lk ed ef hw bi translated">Mlearning.ai提交建议</h2><div class="ll l"><h3 class="bd b fi z dy lj ea eb lk ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lm l"><p class="bd b fp z dy lj ea eb lk ed ef dx translated">medium.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls ho le"/></div></div></a></div></div></div>    
</body>
</html>