<html>
<head>
<title>Compression of Convolutional Neural Networks by Filter pruning utilizing Information Bottleneck theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用信息瓶颈理论通过滤波器修剪压缩卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/filter-pruning-based-on-information-bottleneck-theory-bd826c4b76a1?source=collection_archive---------10-----------------------#2022-09-15">https://medium.com/mlearning-ai/filter-pruning-based-on-information-bottleneck-theory-bd826c4b76a1?source=collection_archive---------10-----------------------#2022-09-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="025c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我将讨论我最近基于信息瓶颈理论通过过滤器修剪压缩深度卷积神经网络(CNN)的研究工作。这项<a class="ae jc" href="https://doi.org/10.1016/j.neunet.2021.12.017" rel="noopener ugc nofollow" target="_blank">工作</a> [1]发表在2022年3月的《神经网络杂志》上。</p><p id="1700" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从简单介绍什么是滤波器修剪开始，我们将浏览<strong class="ig hi"> I </strong>信息<strong class="ig hi"> B </strong>奥特内克(<strong class="ig hi"> IB </strong>)理论的基础知识，随后是它如何在我们提出的方法中被用于滤波器修剪，实验结果，并以总结结束。</p><h2 id="6b40" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak"> 1。简介</strong></h2><p id="b576" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">CNN正被用于在广泛的领域提供有效和可靠的解决方案。CNN的缺点包括非常高的内存和处理需求。因此，为了在功率受限的边缘设备(如智能手机和无人机)上有效地部署它们，必须压缩沉重的、经过GPU训练的模型。压缩CNN的一种方式是从每个卷积层中存在的整组滤波器中移除/删除不重要的滤波器。CNN的这种压缩被称为<em class="kd">过滤器修剪。</em>修剪后的模型称为<em class="kd">修剪或压缩模型，</em>如图1所示。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ke"><img src="../Images/0910c392dfeca48df10120ba3e408eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*9sMEhDlJ-4P1Pdp7vqWhRA.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx">Figure 1: Illustration of complete pruning process: The original heavy model is initially trained before pruning. The pruning starts by selecting the least important filters, followed by a retrain step. The pruning and retraining process continues until the desired pruning limit is achieved.</figcaption></figure><p id="c773" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">压缩模型的性能取决于决定过滤器重要性的标准的有效性。根据过滤器修剪文献，过滤器的重要性可以通过两种方式确定。</p><ol class=""><li id="5592" class="kq kr hh ig b ih ii il im ip ks it kt ix ku jb kv kw kx ky bi translated">一种是利用称为<em class="kd">无数据方法的过滤器权重。</em></li><li id="3e6d" class="kq kr hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">另一种是由过滤器生成的激活图，称为<em class="kd">数据驱动方法</em>。</li></ol><p id="31ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们的工作中选择了<strong class="ig hi">数据驱动方法</strong>,因为在数据驱动方法中，可以捕捉每个隐藏层的变换输入(应用非线性变换后的输入)和类别标签之间的关系。单独使用过滤器的权重，我们不能确定得到了多少关于类别标签的相关特征。</p><blockquote class="le lf lg"><p id="1de0" class="ie if kd ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">为了捕捉由过滤器和<strong class="ig hi">生成的<strong class="ig hi">激活图与它们各自的类别标签</strong>之间的关系(即，过滤器检索了多少与类别标签相关的信息)，我们利用了信息瓶颈理论。</strong></p></blockquote><p id="6cf0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们回顾一下信息瓶颈理论。</p><h2 id="8c2a" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak"> 2。什么是信息瓶颈理论？</strong></h2><p id="0afa" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">Naftali Tishby提出了IB理论，并将其应用于深度神经网络[2]。IB理论主要研究利用<strong class="ig hi">信息平面(IP) </strong>动力学<em class="kd">训练神经网络。</em></p><p id="0f43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">称为<strong class="ig hi">互信息(MI) </strong>的信息论量主要用于IP动力学。任意两个变量U和V之间的互信息(表示为<em class="kd">I(U；V) </em>)，由下式给出；</p><p id="26a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kd">我(U；V) = H(U) + H(V) — H(U，V) ………… (1) </em></p><p id="8ec2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中H(U)和H(V)表示熵[3]，H(U，V)表示联合熵[3]。最小MI值为0表示变量是独立的。MI值越高，表明变量之间的相关性越强。虽然变量之间的相关性度量只能捕捉变量之间的线性关系，但MI可以识别变量之间的线性和非线性关系。</p><p id="a2c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">信息平面动力学的IB理论</strong>【2】</p><blockquote class="le lf lg"><p id="4dac" class="ie if kd ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">在IB理论中，使用如图2所示的IP动力学来分析神经网络的学习过程。在训练神经网络期间，输入数据为X的每个隐藏层h的两个量MI表示为I(X；h)和每个隐藏层h的MI，其中类标签Y表示为I(h；y)(简称<strong class="ig hi">相关性</strong>)，保持增加。在训练期间的某一点，量I(X；h)开始减小，而I(h；y)继续增加，如图2所示。这被称为压缩阶段[2]。然而，这两个量都固定在一个值上，并且在进一步训练神经网络时不会改变。IP动力学的应用包括自动编码器的优化设计[4]，数据表示的内在维度(ID)的研究[5]。</p></blockquote><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/1b16d0e6a2089167caab5535f9bdb378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eaMWhbxuCBbKsQqCFKTNwg.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx">Figure 2: (Source [1]) Information Plane dynamics of LeNet-5 architecture trained on MNIST dataset for 20 epochs (X- input, Y-Class labels, and h- hidden layer). The layers are represented with different colors, and each color’s saturation indicates the training progress.</figcaption></figure><p id="f622" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们看看如何利用IB理论来导出用于决定所提出的方法中的滤波器的重要性的标准，随后研究不重要的滤波器的选择和整个修剪过程。</p><h2 id="4fef" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">3.提议的方法</h2><p id="5be3" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated"><strong class="ig hi"> 3.1决定过滤器重要性的标准</strong></p><p id="0f35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">提出的“<strong class="ig hi">HRel”</strong><strong class="ig hi">剪枝方法</strong>利用了<strong class="ig hi">相关性</strong>(带有类别标签的隐藏层的MI)，这是IB理论的信息平面动力学的关键组成部分。尽管关于压缩阶段的存在存在持续的争论[6]，但是在隐藏层的相关性的增加和饱和(I(h；y))训练的时候。因此，在分析信息平面的所有工作中，观察到最初，所有层在训练开始时具有较少的相关性。但是随着训练的进行，每层的相关性也逐渐增加并达到饱和，如图1所示。因此，隐藏层在训练期间获得的更高的相关性意味着隐藏层学习了关于类别标签的更多相关信息。</p><blockquote class="le lf lg"><p id="c379" class="ie if kd ig b ih ii ij ik il im in io lh iq ir is li iu iv iw lj iy iz ja jb ha bi translated">类似于隐藏层，单个过滤器的相关性，即每个过滤器的激活图和类别标签之间的MI，也决定了由过滤器提取的关于类别标签的相关信息量。<strong class="ig hi">因此，在所提出的方法中使用了滤波器的相关性来确定每一层上的滤波器的重要性。</strong></p></blockquote><p id="dd40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">深度神经网络中的MI计算需要高维变量的联合概率和边际概率，这很难计算。因此，我们使用非参数MI估计器[7]来估计相关性。MI估计器将每个过滤器的激活图和类别标签作为输入，并返回它们之间的MI，如图3所示。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lp"><img src="../Images/78114e9f091bc2419eaee3f0f8e95ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsGBNd2bHvXNznfenvqwYQ.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx">Figure 3: (Source [1]) The steps involved in calculating the Relevance of activation map A for mini-batch ‘k’. The subscripts of filters ‘f’ and activation maps ‘A’ denote layer number and filter number, respectively. Y represents class labels. I denote MI.</figcaption></figure><p id="a467" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3.2过滤器选择</strong></p><p id="6056" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们正在利用激活图，并且模型是在小批量中训练的，所以我们需要为每个小批量计算每个过滤器的相关性，如图4所示。对于每个过滤器，考虑跨小批量的相关性的平均值。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lq"><img src="../Images/936930a7bf17d389c44ed7989ec2d463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQjhtINi_C8jF2S8nHrwjQ.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx">Figure 4: (Source [1]) Relevance of filters from the second convolutional layer in LeNet-5 trained over MNIST dataset. The X-axis denotes the filter number. Y-axis denotes the mini-batches of the training data.</figcaption></figure><p id="b087" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3.3修剪过程</strong></p><p id="530f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所提出的方法以迭代的方式修剪滤波器，如图1所示。首先，训练CNN模型以达到基线精度。然后，过滤器修剪的过程从修剪每层中相关性较低的过滤器开始，随后是重新训练或微调模型。每次要修剪的过滤器的百分比和要保留的过滤器的最终数量是超参数。因此，修剪和微调会继续进行，直到达到指定的修剪限制。</p><h2 id="831e" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak"> 5。结果</strong></h2><ol class=""><li id="4ba4" class="kq kr hh ig b ih jy il jz ip lr it ls ix lt jb kv kw kx ky bi translated">对于各种数据集和使用的架构组合MNIST + LeNet-5、CIFAR10 + VGG-16、CIFAR10 + ResNet-56、CIFAR10 + ResNet-110、ImageNet + ResNet-50，比较了HRel压缩的模型的精度。提出的HRel剪枝方法优于许多最先进的滤波器剪枝方法</li><li id="1715" class="kq kr hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">除了HRel压缩模型的性能，我们还分析了剪枝过程中的信息平面动力学(在文献中首次观察到)。如图5所示，即使在修剪之后，也观察到模型的IP的最小变化的有趣观察。</li></ol><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lu"><img src="../Images/e5f640148042dec7e0a807938c43aab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ScHCQUxeiwmh24Zcv2IFw.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx">Figure 5: (Source [1]) Information Plane (IP) dynamics of LeNet-5 architecture. The left figure corresponds to the IP dynamics without pruning, and the right figure shows the IP dynamics after pruning. The layers are represented with different colors, and each color’s saturation indicates the training progress.</figcaption></figure><p id="b96c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">详细结果请参考论文<a class="ae jc" href="https://doi.org/10.1016/j.neunet.2021.12.017" rel="noopener ugc nofollow" target="_blank">【1】</a></p><h2 id="6858" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak"> 6。结束语</strong></h2><p id="a0c7" class="pw-post-body-paragraph ie if hh ig b ih jy ij ik il jz in io ip ka ir is it kb iv iw ix kc iz ja jb ha bi translated">HRel过滤器修剪方法通过仅保留那些可以检索关于类标签的相关信息的过滤器来提供压缩模型。</p><h2 id="546f" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">7.摘要</h2><ol class=""><li id="2389" class="kq kr hh ig b ih jy il jz ip lr it ls ix lt jb kv kw kx ky bi translated">在本文中，CNN中的滤波器基于它们的<strong class="ig hi">相关值</strong>被修剪。基于IB理论选择<strong class="ig hi">相关性</strong>度量，使用各个过滤器的<em class="kd">激活图和<em class="kd">类别标签</em>之间的互信息(MI)来测量。</em></li><li id="1d32" class="kq kr hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">使用HRel方法获得的修剪结果优于许多当前的修剪方法。</li><li id="3e5e" class="kq kr hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated"><strong class="ig hi"> IP动态</strong>显示了修剪标准的重要性。对不同CNN修剪前后的IP平面动态的分析表明，修剪后的信息损失可以忽略不计。</li><li id="650e" class="kq kr hh ig b ih kz il la ip lb it lc ix ld jb kv kw kx ky bi translated">除了ImageNet上的ResNet-50之外，从初始修剪迭代到最终迭代，观察到过滤器的相关性<em class="kd">增加</em>。</li></ol><p id="fdf5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kd">希望您喜欢阅读这篇文章，并对过滤器修剪和信息瓶颈理论的概念有所了解！！！！！</em></p><p id="1ec3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考文献</strong></p><p id="f7e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]萨尔瓦尼CH，古拉伊M，杜贝SR，巴沙SS。HRel:基于激活图和类标签之间高相关性的过滤器修剪。神经网络。2022年3月1日；147:186–97.</p><p id="d81a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]施瓦兹-齐夫R，蒂什比n .通过信息打开深度神经网络的黑匣子。arXiv预印本arXiv:1703.00810。2017年3月2日。</p><p id="3d8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]托马斯m .盖和乔伊a .托马斯。信息论的要素(电信和信号处理中的威利系列)。Wiley- <br/> Interscience，美国，2006年。</p><p id="49d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]余S，Principe JC。用信息论概念理解自动编码器。神经网络。2019年9月1日；117:104–23.</p><p id="1545" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5]安苏尼A，莱奥A，麦基JH，佐科兰d .深层神经网络中数据表示的固有维度。神经信息处理系统进展。2019;32.</p><p id="7be1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6]Andrew M . Saxe、Yamini Bansal、Joel Dapello、Madhu Advani、Artemy Kolchinsky、Brendan D Tracey和David D Cox。深度学习的信息瓶颈理论。统计力学学报:理论与实验，2019(12):124020，2019。</p><p id="06d9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7]wickstrm K，kse S，Kampffmeyer M，Yu S，Principe J，Jenssen R .通过基于矩阵的Renyi熵和张量核对深度神经网络进行信息平面分析。arXiv预印本arXiv:1909.113962019年9月25日。</p><div class="lv lw ez fb lx ly"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hi fi z dy md ea eb me ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm kk ly"/></div></div></a></div></div></div>    
</body>
</html>