<html>
<head>
<title>Semantic Search with S-BERT is all you need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用S-BERT进行语义搜索是您所需要的</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/semantic-search-with-s-bert-is-all-you-need-951bc710e160?source=collection_archive---------0-----------------------#2021-06-05">https://medium.com/mlearning-ai/semantic-search-with-s-bert-is-all-you-need-951bc710e160?source=collection_archive---------0-----------------------#2021-06-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="5376" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">从头开始构建内部语义搜索引擎—快速而准确</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/f82bb2dafa1fc759fac98e021be48540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AJb_MW-y0gkKfZXt"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Winkler</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e918" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"><em class="kj"/></strong><em class="kj">-</em>语义搜索是一种<strong class="jp hi">数据搜索技术</strong>，其中搜索查询不仅旨在找到关键词，还旨在<strong class="jp hi">确定一个人用于<a class="ae jm" href="https://www.youtube.com/watch?v=Yo4NqGPISXQ" rel="noopener ugc nofollow" target="_blank">搜索</a>的词语</strong>的意图和上下文含义。</p><p id="b620" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">语义学是指对意义的哲学研究。诚然，哲学很少与软件工程押韵，但这个概念确实帮助我们达成了一个定义。的确，<strong class="jp hi">语义搜索是要搞清楚你的用户是什么意思。</strong></p><p id="c3e4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=Yo4NqGPISXQ" rel="noopener ugc nofollow" target="_blank">语义</a>搜索试图通过理解搜索查询的内容来提高搜索的准确性。传统搜索引擎只能根据词汇匹配来查找文档，与之相反，语义搜索也可以查找同义词。</p><p id="6347" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">事实上，这种类型的搜索通过几乎准确地理解用户试图询问的内容，而不是简单地将关键词与页面匹配，从而使浏览更加完整。语义搜索背后的思想是将你的语料库中的所有条目嵌入到一个向量空间中，这些条目可以是句子、段落或文档。</p><p id="a45b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在搜索时，<em class="kj">查询被嵌入到相同的向量空间中，并且从您的语料库中找到最接近的嵌入</em>。这些条目应该与查询有很高的语义重叠。</p><p id="b107" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">参考消息——在这篇博客中，我坚持使用句子变形金刚的变体。<em class="kj"> SentenceTransformers的设计使得微调你自己的句子/文本嵌入模型变得很容易</em>。它提供了大多数构建模块，您可以将这些模块结合在一起，为您的特定任务调整嵌入。如果你不知道这个概念，请在这里阅读他们的出版物:<a class="ae jm" href="https://www.sbert.net/docs/publications.html" rel="noopener ugc nofollow" target="_blank">https://www.sbert.net/docs/publications.html</a></p><p id="9aff" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在我们开始之前，让我们问自己这些问题，并寻找它的解决方案，而不是直接跳入解决方案:</p><div class="kk kl ez fb km kn"><a href="https://subirverma.medium.com/simulating-content-personalization-with-contextual-bandits-6f4efb902af" rel="noopener follow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">用上下文强盗模拟内容个性化🤖</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">用Vowpal Wabbit模拟一个内容个性化场景，使用上下文强盗在动作之间做出选择…</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">subirverma.medium.com</p></div></div><div class="kw l"><div class="kx l ky kz la kw lb jg kn"/></div></div></a></div><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="lc ld l"/></div></figure><blockquote class="le lf lg"><p id="39d9" class="jn jo kj jp b jq jr ii js jt ju il jv lh jx jy jz li kb kc kd lj kf kg kh ki ha bi translated">Q1。什么样的嵌入会起作用？Q2。如果使用BERT，如何存储文档及其庞大的嵌入？<br/> Q3。如果我们有像博客这样的长文档和像产品描述这样的小块内容会怎么样？方法将如何改变？<br/> Q4。模型微调如何能给我好的结果？</p></blockquote><p id="3458" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们看看是否能找到所有问题的答案。当我们这样做的时候，下面的概念将被涵盖。搜索类型<br/> 2。余弦和点积度量<br/> 3。文件嵌入技术<br/> 4。存储和检索。<br/> 5。合成查询生成。<br/> 6。双编码器微调</p><p id="043d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我已经通过视频<a class="ae jm" href="https://www.youtube.com/watch?v=Yo4NqGPISXQ" rel="noopener ugc nofollow" target="_blank">在这里</a>分享了这篇博客的详细分析。</p><h1 id="ef76" class="lk ll hh bd lm ln lo lp lq lr ls lt lu in lv io lw iq lx ir ly it lz iu ma mb bi translated">对称与非对称语义搜索</h1><p id="e3c9" class="pw-post-body-paragraph jn jo hh jp b jq mc ii js jt md il jv jw me jy jz ka mf kc kd ke mg kg kh ki ha bi translated"><strong class="jp hi">Q1&amp;Q3<br/>T12】A<strong class="jp hi">关键区别</strong>对于您的设置是<em class="kj">对称</em> vs. <em class="kj">非对称语义搜索</em>:</strong></p><ul class=""><li id="43b3" class="mh mi hh jp b jq jr jt ju jw mj ka mk ke ml ki mm mn mo mp bi translated">对于<strong class="jp hi">对称语义搜索，</strong>您的查询和您的语料库中的条目具有大约相同的长度和相同的内容量。一个例子是搜索类似的问题:例如，你的查询可以是<em class="kj">“如何在线学习Python？”</em>而你想找一个类似<em class="kj">“如何在web上学习Python？”</em>。对于对称任务，您可能会翻转查询和语料库中的条目。</li><li id="283b" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated">对于<strong class="jp hi">非对称语义搜索</strong>，你通常有一个<strong class="jp hi">短查询</strong>(比如一个问题或者一些关键词)，你想找到一个较长的段落回答查询。一个例子是类似于<em class="kj">“什么是Python”</em>的查询，并且你想要找到段落<em class="kj">“Python是一种解释的、高级的和通用的编程语言。Python的设计哲学……”</em>。对于非对称任务，翻转查询和语料库中的条目通常没有意义。</li></ul><p id="d1be" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">上述概念非常重要，因为这将帮助我们更好地理解我们的问题，因此我们可以使用专门为我们手头的任务/问题开发的方法。</p><p id="294a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="kj">为你的任务类型选择合适的型号</em>  <em class="kj">很关键。它最大的区别在于它接受训练的数据类型。此外，针对余弦相似性调整的模型将更喜欢检索短文档，而针对点积调整的模型将更喜欢检索较长的文档。根据您的任务，这种或那种类型的模型是更可取的。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mv"><img src="../Images/d7d31e544fd8005c215e15c4e41fe3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-lVFYR44EnS1LpRtN6ZVvg.png"/></div></div></figure><p id="8760" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">对称语义搜索</strong>的适用模型:</p><ul class=""><li id="a329" class="mh mi hh jp b jq jr jt ju jw mj ka mk ke ml ki mm mn mo mp bi translated"><strong class="jp hi">释义-蒸馏-基础-v1 /释义-xlm-r-多语种-v1 </strong></li><li id="4bb4" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><strong class="jp hi">quora-distilbert-base/quora-distilbert-多语言</strong></li><li id="c079" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><strong class="jp hi">distiluse-base-multilingual-cased-v2</strong></li></ul><p id="a1db" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">不对称语义搜索的适用模型</strong>:</p><ul class=""><li id="8600" class="mh mi hh jp b jq jr jt ju jw mj ka mk ke ml ki mm mn mo mp bi translated"><strong class="jp hi">msmarco-distilbert-base-v2</strong></li><li id="7814" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><strong class="jp hi"> msmarco-bert-base-v3 </strong></li></ul><p id="d737" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">更多详情请点击:<a class="ae jm" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">https://www.sbert.net/docs/pretrained_models.html</a></p><p id="0df9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> <em class="kj">为了理解为什么S-BERT不是BERT我在这里极力推荐这篇论文:</em></strong><a class="ae jm" href="https://arxiv.org/pdf/1908.10084.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi"><em class="kj">【https://arxiv.org/pdf/1908.10084.pdf】</em></strong></a></p><p id="a2a2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">Q2的解决方案</strong> <br/>此时，我们已经了解了我们的数据，因此，我们选择了嵌入模型。接下来，我们需要了解如何对数据进行编码，以及我们需要用编码存储的其他哪些信息将有助于检索搜索结果。</p><p id="b1ab" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于存储，我们有这样的选项:<br/> (a) <strong class="jp hi"> ElasticSearch </strong>:如果我们有很多元信息存储，并且我们想要运行一些跨集群搜索，这可能是一个非常好的选项。话虽如此，但如果将产品投入生产和规模化，您可能会花费大量成本，而且维护可能会花费您另一位专家的成本。<br/> (b) <strong class="jp hi"> FAISS </strong>:(脸书AI相似性搜索)是一个允许开发者快速搜索彼此相似的多媒体文档嵌入的库。它解决了针对基于散列的搜索而优化的传统查询搜索引擎的局限性，并提供了更多可扩展的相似性搜索功能。<br/>(C)<strong class="jp hi"/>:一个C++库，使用Python绑定来搜索空间中靠近给定查询点的点。它还创建基于文件的大型只读数据结构，这些数据结构被映射到内存中，这样许多进程可以共享相同的数据。</p><p id="85f4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">你的选择完全取决于你和你的需求。我现在选择FAISS，因为它的易用性(pythonic)和GPU支持使得它非常快。FAISS的唯一限制是你必须用你的FAISS id的映射在一些数据库中单独维护你的元数据信息。<br/>  <strong class="jp hi"> <em class="kj">看这里:</em></strong><a class="ae jm" rel="noopener" href="/swlh/add-similarity-search-to-dynamodb-with-faiss-c68eb6a48b08"><strong class="jp hi">https://medium . com/swlh/add-similarity-search-to-dynamo db-with-faiss-c 68 EB 6 a 48 b 08</strong></a></p><h1 id="fc86" class="lk ll hh bd lm ln lo lp lq lr ls lt lu in lv io lw iq lx ir ly it lz iu ma mb bi translated">开始—快速构建一些东西</h1><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mw ld l"/></div></figure><p id="fd7e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">数据集:<a class="ae jm" href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots" rel="noopener ugc nofollow" target="_blank">维基百科电影剧情</a> <br/>内容:数据集包含了全球34886部电影的描述。下面列出了列说明:</p><ul class=""><li id="a8f3" class="mh mi hh jp b jq jr jt ju jw mj ka mk ke ml ki mm mn mo mp bi translated"><em class="kj">上映年份</em> —电影上映的年份</li><li id="2621" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">片名</em> —电影片名</li><li id="f930" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">起源/种族</em> —电影的起源(即美国、宝莱坞、泰米尔等。)</li><li id="d965" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">导演</em> —导演</li><li id="cc77" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">演员阵容</em> —主要演员</li><li id="79c0" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">类型</em> —电影类型</li><li id="10fb" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">维基页面</em> —从中抓取情节描述的维基页面的URL</li><li id="9d3b" class="mh mi hh jp b jq mq jt mr jw ms ka mt ke mu ki mm mn mo mp bi translated"><em class="kj">剧情</em> —电影剧情的长篇描述(警告:可能包含剧透！！！)</li></ul><p id="98d1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们想为用户建立一个搜索栏来搜索电影，为了简单起见，我们假设我们只想在<em class="kj">电影情节字段</em>中进行搜索。用户可以输入一些关键字或句子来描述他们的电影，我们通过密切理解查询和电影情节来提供最好的服务。</p><p id="1c6e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在编码之前，我们先来看看这个文本信息有多长。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="b976" class="nc ll hh my b fi nd ne l nf ng">df['doc_len'] = df['Plot'].apply(lambda words: len(words.split()))<br/>max_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)<br/>sns.distplot(df['doc_len'], hist=True, kde=True, color='b', label='doc len')<br/>plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')<br/>plt.title('plot length'); plt.legend()<br/>plt.show()</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nh"><img src="../Images/055af1603dd917324fd9689e9e522fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqGjcIV7akrBgCx2n0sLFg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Movie Plot Length Distribution</figcaption></figure><p id="3cc5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有几种方法可以处理文档长度超过512的情况。<br/> (1)最容易也是最可怕的是将长度大于512的所有东西都切掉。<br/> (2)运行提取或抽象总结<br/> (3)文档池嵌入的平均值。</p><p id="c35b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们将选择最可怕的一个。我浏览了一些情节样本，得出结论，采取最大长度应该足以满足我们试图建立电影搜索。<br/>在这里，如上所述，我们正在处理不对称搜索，并试图检索长段落，因此对于我们的情况，点积模型将非常适合。对于这个实验，我选择了<strong class="jp hi">"<em class="kj">sentence-transformers/ms Marco-distilbert-base-dot-prod-v3</em>"</strong>模型，它在语义文本相似性(不对称)任务中表现很好，并且它比bert快得多，因为它小得多。该模型被优化为使用点积作为查询和文档之间的相似性函数。<br/> <em class="kj">注:如果你有简短的描述，</em><strong class="jp hi"><em class="kj">distilbert-base-nli-stsb-mean-tokens】，</em> </strong> <em class="kj">效果更好更快。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ni"><img src="../Images/2c0bfeb544148b45c81dac64217cb94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bO2SPS9O1f8vhZosAhK7Xg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Search Architecture</figcaption></figure><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="52ec" class="nc ll hh my b fi nd ne l nf ng">from sentence_transformers import SentenceTransformer<br/>model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')</span></pre><p id="6bf8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因为我们使用FAISS，一旦我们解决了索引工厂和映射，存储嵌入就很容易了。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="05ae" class="nc ll hh my b fi nd ne l nf ng">import faiss<br/>encoded_data = model.encode(df.Plot.tolist())<br/>encoded_data = np.asarray(encoded_data.astype('float32'))<br/>index = faiss.IndexIDMap(faiss.IndexFlatIP(768))<br/>index.add_with_ids(encoded_data, np.array(range(0, len(df))))<br/>faiss.write_index(index, 'movie_plot.index')</span></pre><p id="181b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们已经对我们的电影情节进行了编码，其中每个情节都用768维向量进行了编码，并以movie_plot.index名称存储到磁盘上。<br/>注意这里我们使用了<strong class="jp hi">索引</strong>。<strong class="jp hi">add _ with _ id</strong>它按照数据帧的顺序对数据进行编码，并存储它们的索引id。</p><p id="6095" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们写几个实用函数来帮助编码用户查询，并从FAISS索引目录中获取类似的电影。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="50ff" class="nc ll hh my b fi nd ne l nf ng">def fetch_movie_info(dataframe_idx):<br/>    info = df.iloc[dataframe_idx]<br/>    meta_dict = dict()<br/>    meta_dict['Title'] = info['Title']<br/>    meta_dict['Plot'] = info['Plot'][:500]<br/>    return meta_dict<br/>    <br/>def search(query, top_k, index, model):<br/>    t=time.time()<br/>    query_vector = model.encode([query])<br/>    top_k = index.search(query_vector, top_k)<br/>    print('&gt;&gt;&gt;&gt; Results in Total Time: <strong class="my hi">{}</strong>'.format(time.time()-t))<br/>    top_k_ids = top_k[1].tolist()[0]<br/>    top_k_ids = list(np.unique(top_k_ids))<br/>    results =  [fetch_movie_info(idx) for idx <strong class="my hi">in</strong> top_k_ids]<br/>    return results</span></pre><p id="582d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">函数:搜索<br/> </strong>查询:string - &gt;用户键入查询<br/> top_k: integer - &gt;返回的结果数<br/>index:faiss _ index-&gt;index查询for结果<br/> model: sbert - &gt; model对用户进行编码-查询<br/> <strong class="jp hi">函数:fetch _ movie _ info<br/></strong>data frame _ idx:integer-&gt;索引值提取自movie_plot.index，可用于</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="98ec" class="nc ll hh my b fi nd ne l nf ng">from pprint import pprint</span><span id="63b3" class="nc ll hh my b fi nj ne l nf ng">query="Artificial Intelligence based action movie"<br/>results=search(query, top_k=5, index=index, model=model)</span><span id="58da" class="nc ll hh my b fi nj ne l nf ng">print("\n")<br/>for result in results:<br/>    print('\t',result)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/ab943f45b93a039a8c2dfb1382a57970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pmZwpa-WLTC9NQru3g5Rag.png"/></div></div></figure><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="0774" class="nc ll hh my b fi nd ne l nf ng">from pprint import pprint<br/><br/>query="movie about romance and pain of separation"<br/>results=search(query, top_k=5, index=index, model=model)<br/><br/>print("<strong class="my hi">\n</strong>")<br/>for result <strong class="my hi">in</strong> results:<br/>    print('<strong class="my hi">\t</strong>',result)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/d0a4887f5f46ff55dc7d4dd2c4efc585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svJKopW2U9wZSdzCOXeaWQ.png"/></div></div></figure><p id="04d8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我试着运行一些非常模糊的查询，得到了一些不错的结果。印象不深，但相比之下，仅仅通过缩小任务范围，对其进行训练和微调，然后将其与我们的用例联系起来，就给了我们一些好的结果。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nl ld l"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Not impressed</figcaption></figure></div><div class="ab cl nm nn go no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ha hb hc hd he"><h1 id="6d69" class="lk ll hh bd lm ln nt lp lq lr nu lt lu in nv io lw iq nw ir ly it nx iu ma mb bi translated">微调</h1><p id="916c" class="pw-post-body-paragraph jn jo hh jp b jq mc ii js jt md il jv jw me jy jz ka mf kc kd ke mg kg kh ki ha bi translated"><strong class="jp hi">Q4的解决方案</strong> <br/>如果我们有<strong class="jp hi">查询&amp;相关段落信息，我们可以很容易地在给定的数据集上微调句子转换器模型。但是如果从零开始建造，你就不会有这些数据。(这里我们不处理变压器模型的预训练方法。它很昂贵，并且需要大量的数据。这里不要忘记域)</strong></p><p id="82e6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这一点上，我们一无所获。但是等等，我们所有的电影情节和文本信息呢？我们能设计一些无监督的方法来微调我们数据集上的模型吗？</p><h2 id="2f7d" class="nc ll hh bd lm ny nz oa lq ob oc od lu jw oe of lw ka og oh ly ke oi oj ma ok bi translated">1.合成查询生成</h2><p id="5688" class="pw-post-body-paragraph jn jo hh jp b jq mc ii js jt md il jv jw me jy jz ka mf kc kd ke mg kg kh ki ha bi translated">我们使用<strong class="jp hi">综合查询生成</strong>来实现我们的目标。我们从文档集合中的段落开始，并为用户可能询问/可能搜索的这些可能的查询创建。<br/> <strong class="jp hi"> BEIR </strong>:信息检索模型零命中率评测异构基准提出了一种不需要训练数据就能学习(或适应)不对称语义搜索模型的方法。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ol"><img src="../Images/4dd08fe49bc635efe06ac3232512c087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-TLe_ia0ZGoyEYRaO57_Q.png"/></div></div></figure><p id="a60b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">论文【https://arxiv.org/abs/2104.08663】<a class="ae jm" href="https://arxiv.org/abs/2104.08663" rel="noopener ugc nofollow" target="_blank">:</a><br/><strong class="jp hi">GitHub</strong>:<a class="ae jm" href="https://github.com/UKPLab/beir" rel="noopener ugc nofollow" target="_blank">https://github.com/UKPLab/beir</a></strong></p><blockquote class="le lf lg"><p id="e042" class="jn jo kj jp b jq jr ii js jt ju il jv lh jx jy jz li kb kc kd lj kf kg kh ki ha bi translated">这篇论文在多任务学习和微调方面有一些惊人的想法。<br/> <strong class="jp hi">转自</strong> <br/> GenQ类似以往的工作(梁等，2020；马等，2021)，我们提出了一个无监督的领域适应方法密集检索模型使用合成查询。首先，我们微调T5-base (Raffel等人，2020)模型，以生成给定段落的查询。我们使用MSMARCO数据集并训练2个时期。然后，对于目标语料库，我们使用top-k和nucleus-sampling(top-k:25；top-p: 0.95)。由于资源限制，我们将每个数据集中目标文档的最大数量限制为100K。我们发现T5模型比BART (Lewis等人，2020)执行得更好，我们的解码设置比beam-search更好。对于检索，我们继续在合成查询和文档对上微调SBERT模型(第4.1.3节)。注意，这为每个任务创建了一个独立的模型。</p></blockquote><p id="72d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">受论文的启发，我们将对SBERT使用类似的微调技术。我们先来看看合成查询生成代码。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="a610" class="nc ll hh my b fi nd ne l nf ng"><em class="kj"># Parameters for generation</em><br/>batch_size = 16 <em class="kj">#Batch size</em><br/>num_queries = 5 <em class="kj">#Number of queries to generate for every paragraph</em><br/>max_length_paragraph = 512 <em class="kj">#Max length for paragraph</em><br/>max_length_query = 64   <em class="kj">#Max length for output query</em></span><span id="fd0b" class="nc ll hh my b fi nj ne l nf ng">def _removeNonAscii(s): return "".join(i for i <strong class="my hi">in</strong> s if ord(i) &lt; 128)</span><span id="1e79" class="nc ll hh my b fi nj ne l nf ng">with open('generated_queries_all.tsv', 'w') as fOut:<br/>    for start_idx <strong class="my hi">in</strong> tqdm(range(0, len(paragraphs), batch_size)):<br/>        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]<br/>        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)<br/>        outputs = model.generate(<br/>            **inputs,<br/>            max_length=max_length_query,<br/>            do_sample=True,<br/>            top_p=0.95,<br/>            num_return_sequences=num_queries)<br/><br/>        for idx, out <strong class="my hi">in</strong> enumerate(outputs):<br/>            query = tokenizer.decode(out, skip_special_tokens=True)<br/>            query = _removeNonAscii(query)<br/>            para = sub_paragraphs[int(idx/num_queries)]<br/>            para = _removeNonAscii(para)<br/>            fOut.write("<strong class="my hi">{}\t{}\n</strong>".format(query.replace("<strong class="my hi">\t</strong>", " ").strip(), para.replace("<strong class="my hi">\t</strong>", " ").strip()))<br/></span></pre><p id="99cc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">输入到这段代码中的段落不过是电影情节的片段，每个片段最多有5个综合生成的查询。如果你想一下，我们在这里尝试做的是将段落中可能存在的信息表示为问题，然后使用这个<strong class="jp hi">知识元组</strong>来微调s-bert模型，该模型将捕获这些元组之间的语义和句法信息映射。</p><h2 id="eb91" class="nc ll hh bd lm ny nz oa lq ob oc od lu jw oe of lw ka og oh ly ke oi oj ma ok bi translated">2.双编码器微调</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es om"><img src="../Images/0d7cddb280b959520cb693833f197d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*JetFFNLgZ9oGzBA7IfqkeA.png"/></div></figure><p id="3689" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">SBERT是一个连体双编码器，使用平均池进行编码，使用余弦相似度进行检索。SentenceTransformers的设计使得微调你自己的句子/文本嵌入模型变得很容易。它提供了大部分构建模块，我们可以将这些模块结合在一起，为我们的特定任务调整嵌入。<br/>我们可以通过定义各个层来从头开始创建网络架构。例如，以下代码将创建所示的网络架构:</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="4944" class="nc ll hh my b fi nd ne l nf ng"><em class="kj"># Now we create a SentenceTransformer model from scratch</em><br/>word_emb = models.Transformer('sentence-transformers/msmarco-distilbert-base-dot-prod-v3')<br/>pooling = models.Pooling(word_emb.get_word_embedding_dimension())<br/>model = SentenceTransformer(modules=[word_emb, pooling])</span></pre><p id="9874" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">注意:这里我使用了与前半部分相同的"<strong class="jp hi">句子-transformers/ms Marco-distil Bert-base-dot-prod-v3</strong>"进行微调。没有变化。<br/>我想在这里谈谈我们将要使用的损失函数。<code class="du on oo op my b">sentence_transformers.losses</code>定义不同的损失函数，可用于根据训练数据微调网络。在微调模型时，损失函数起着至关重要的作用。它决定了我们的嵌入模型对于特定的下游任务的效果如何。遗憾的是，没有“一刀切”的损失函数。哪个损失函数是合适的取决于可用的训练数据和目标任务。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="1f91" class="nc ll hh my b fi nd ne l nf ng"><em class="kj">MultipleNegativesRankingLoss: MultipleNegativesRankingLoss</em> is a great loss function if you only have positive pairs, for example, only pairs of similar texts like pairs of paraphrases, pairs of duplicate questions, pairs of (query, response), or pairs of (source_language, target_language).</span><span id="7fe6" class="nc ll hh my b fi nj ne l nf ng"><strong class="my hi">paper: </strong><a class="ae jm" href="https://arxiv.org/pdf/1705.00652.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="my hi">https://arxiv.org/pdf/1705.00652.pdf</strong></a></span></pre><p id="889e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这种损失期望作为输入的是一批由句子对(a_1，p_1)，(a_2，p_2)…，(a_n，p_n)组成的句子对，其中我们假设(a_i，p_i)为正对，而(a_i，p_j)为I！=j一个负对。对于每个a_i，它使用所有其他p_j作为负样本，即对于a_i，我们有1个正例(p_i)和n-1个负例(p_j)。然后，它最小化softmax标准化分数的负对数似然性。这个损失函数非常适合于为具有正对(例如(query，relevant_doc))的检索设置训练嵌入，因为它将在每批n-1个负文档中随机采样。性能通常会随着批量的增加而提高。还可以通过如下方式构造数据，为每个锚正对提供一个或多个硬否定:(a_1，p_1，n_1)，(a_2，p_2，n_2)。这里，n_1是(a_1，p_1)的硬否定。损失将用于对(a_i，p_i)所有p_j (j！=i)且所有n_j为负数。</p><p id="bc0a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">但为什么不是CosineSimilarityLoss呢？最简单的方式是让<strong class="jp hi">知识元组</strong>用指示其相似性的分数来注释，例如在0到1的范围内。然后，我们可以使用连体网络架构来训练网络。但是我们没有标注分数。<br/>使用NLI数据的softmax-loss产生(相对)好的句子嵌入是相当巧合的。<a class="ae jm" href="https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi">multiple negative ranking loss</strong></a>更加直观，产生的句子表达也明显更好。</p><p id="f4c8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">multiple negativerankingloss</strong>的训练数据由句子对[(a1，b1)，…，(an，bn)]组成，其中我们假设(ai，bi)是相似的句子，而(ai，bj)是I！= j。最小化(ai，bj)之间的距离，同时最大化所有I！= j。</p><p id="3171" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">例如在下图中:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oq"><img src="../Images/c258cce512bd036c9bf8c0225a6a7cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xatOCnEN5IgiAEyponVW9A.png"/></div></div></figure><p id="8fdd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">(a1，b1)之间的距离减小，而(a1，b2…5)之间的距离将增大。对于a2，…，a5也是如此。</p><p id="8605" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对NLI使用MultipleNegativeRankingLoss相当简单:我们将带有<em class="kj">蕴涵</em>标签的句子定义为正对。例如，我们有像(<em class="kj">“多名男性参与的足球比赛”这样的配对</em>，<em class="kj">“一些男人在玩一项运动。”</em>)并希望这些对在向量空间中接近。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="d9a8" class="nc ll hh my b fi nd ne l nf ng">from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets<br/>from torch import nn<br/>import os<br/><br/>train_examples = [] <br/>with open('../input/user-query-data/generated_queries_all (1).tsv') as fIn:<br/>    for line <strong class="my hi">in</strong> fIn:<br/>        try:<br/>            query, paragraph = line.strip().split('<strong class="my hi">\t</strong>', maxsplit=1)<br/>            train_examples.append(InputExample(texts=[query, paragraph]))<br/>        except:<br/>            pass<br/><br/><em class="kj"># For the MultipleNegativesRankingLoss, it is important</em><br/><em class="kj"># that the batch does not contain duplicate entries, i.e.</em><br/><em class="kj"># no two equal queries and no two equal paragraphs.</em><br/><em class="kj"># To ensure this, we use a special data loader</em><br/>train_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=8)</span><span id="843e" class="nc ll hh my b fi nj ne l nf ng"><em class="kj"># MultipleNegativesRankingLoss requires input pairs (query, relevant_passage)</em><br/><em class="kj"># and trains the model so that is is suitable for semantic search</em><br/>train_loss = losses.MultipleNegativesRankingLoss(model)</span></pre><p id="260a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">微调框架已准备好进行微调。我们已经成功地建立了模型架构和损失函数以及为什么要使用它们。</p><pre class="ix iy iz ja fd mx my mz na aw nb bi"><span id="1a91" class="nc ll hh my b fi nd ne l nf ng"><em class="kj">#Tune the model</em><br/>num_epochs = 3<br/>warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)<br/>model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs, warmup_steps=warmup_steps, show_progress_bar=True)<br/><br/>os.makedirs('search', exist_ok=True)<br/>model.save('search/search-model')</span></pre><p id="fa76" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">调优完成后，不要忘记使用该模型再次对数据集运行FAISS编码和索引。</p><p id="a580" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">完成后，我对新模型发出了类似的查询，瞧！！推荐或搜索结果改善了很多。如果你不相信，看看情节，你就会明白了。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/0813b8976a97cb4de951d9f22099f7d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tgDGsDtKU4G386CA-p-IeQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd lm">Artificial Intelligence based action movie</strong></figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/efa387888c861f995b295ee7970e1d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sNERpFt_H1uQhcn5wzOEoA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx"><strong class="bd lm">movie about romance and pain of separation</strong></figcaption></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nl ld l"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">somewhat impressed now</figcaption></figure><p id="965b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是成功的一半。我们只是触及了搜索的表面。我们还没有谈到检索、重新排序和个性化的质量。</p><div class="kk kl ez fb km kn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/search-rank-and-recommendations-35cc717772cb"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">搜索、排名和推荐</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">重新排列搜索结果和个性化推荐的简单方法。</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">medium.com</p></div></div><div class="kw l"><div class="or l ky kz la kw lb jg kn"/></div></div></a></div><p id="a2d4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在下一篇文章中，我将详细讨论用用户行为重新排序和改善搜索结果的策略。在那之前继续学习。请提出您的想法和意见。</p><p id="fadc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">干杯！！</p><p id="bbf0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Youtube链接:<a class="ae jm" href="https://youtu.be/Yo4NqGPISXQ" rel="noopener ugc nofollow" target="_blank">https://youtu.be/Yo4NqGPISXQ</a></p><p id="fa8e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">注意:感谢SBERT论文和他们的文档。</p><div class="kk kl ez fb km kn"><a href="https://github.com/99sbr/semantic-search-with-sbert" rel="noopener  ugc nofollow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">99sbr/带sbert的语义搜索</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">github.com</p></div></div><div class="kw l"><div class="os l ky kz la kw lb jg kn"/></div></div></a></div><p id="3b2a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">此外，如果您正在为基于ML/DL的项目寻找基于Fast-API的项目框架，请查看</p><div class="kk kl ez fb km kn"><a href="https://github.com/99sbr/fastapi-template" rel="noopener  ugc nofollow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">99sbr/fastapi-模板</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">ML/DL和NLP驱动的基于微服务的项目需要干净和可伸缩的代码架构吗？运行命令uvicon…</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">github.com</p></div></div><div class="kw l"><div class="ot l ky kz la kw lb jg kn"/></div></div></a></div><div class="kk kl ez fb km kn"><a href="https://www.linkedin.com/in/sbrvrm/" rel="noopener  ugc nofollow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">苏比尔维尔马-数据科学家lll -塔塔1mg | LinkedIn</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">Subir是一位充满激情、自学成才的数据科学家。他从2016年开始从事数据科学领域的工作…</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">www.linkedin.com</p></div></div><div class="kw l"><div class="ou l ky kz la kw lb jg kn"/></div></div></a></div><p id="6e57" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">RedisAI矢量搜索演示—<a class="ae jm" href="https://github.com/RedisAI/vecsim-demo" rel="noopener ugc nofollow" target="_blank">https://github.com/RedisAI/vecsim-demo</a></p><div class="kk kl ez fb km kn"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hi fi z dy ks ea eb kt ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">medium.com</p></div></div><div class="kw l"><div class="ov l ky kz la kw lb jg kn"/></div></div></a></div><p id="f7a5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">🔵<a class="ae jm" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"> <strong class="jp hi">成为ML作家</strong> </a></p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ow ld l"/></div></figure></div></div>    
</body>
</html>