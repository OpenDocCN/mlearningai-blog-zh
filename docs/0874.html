<html>
<head>
<title>Embedding similarity search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嵌入相似性搜索</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/embedding-similarity-search-25c6911240af?source=collection_archive---------0-----------------------#2021-08-10">https://medium.com/mlearning-ai/embedding-similarity-search-25c6911240af?source=collection_archive---------0-----------------------#2021-08-10</a></blockquote><div><div class="dt gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ew ey ig ih ii ij es et paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="es et if"><img src="../Images/95ec77dfb215ef7db82f29c669541d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ArauOFg8i1yAIWZ4"/></div></div><figcaption class="iq ir eu es et is it bd b be z dy">Photo by <a class="ae iu" href="https://unsplash.com/@lazycreekimages?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Michael Dziedzic</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b513" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">搜索相似的东西是许多信息检索系统、推荐引擎、同义词搜索等的关键概念。基本上到处都是，在数据库中进行精确搜索是不可行的(例如，您不能只保存单词，还保存其含义和上下文的表示)。</p><p id="0cc4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种情况下，大多数工具、深度学习算法或统计ML方法都使用高维空间中向量形式的初始项目表示进行操作，称为嵌入。这允许表示初始项目(如文本、图像等。)以更有效和灵活的方式保存其特征，有时甚至保存其上下文。</p><p id="27f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，基本上搜索相似项意味着搜索其表示空间中彼此接近的向量表示(找到它们之间的欧几里德或其他距离)。</p><p id="5bce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑到上述情况，搜索相似项目将遵循以下算法:</p><ul class=""><li id="8151" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">将数据集中的所有项目表示为嵌入</li><li id="f55e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">为一对嵌入定义一个<a class="ae iu" href="https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity" rel="noopener ugc nofollow" target="_blank">度量</a>。该度量可以是<a class="ae iu" href="https://wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>、<a class="ae iu" href="https://wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>或点积。</li><li id="1349" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">搜索查询对象的嵌入</li><li id="dfa1" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">选择靠近查询对象的嵌入</li></ul><p id="1564" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">检索这些结果是一个k近邻搜索，可以用几种不同的方式完成。如果数据集中没有那么多项目(几百或几千)，则可以计算所有向量之间的距离度量。尽管如果实时需要结果，并且数据集非常大，搜索最近邻需要是近似的。这可以通过创建允许快速搜索项目的索引数据结构来实现。</p><p id="ba81" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们回顾一下搜索k近邻的不同方法，从最简单的方法到可用于大数据集的方法:</p><p id="2e23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kh">采用直接计算</em> </strong></p><p id="f4d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用numpy从头开始:</strong>最简单的方法是计算从查询向量X到S中每隔一个向量的距离，并识别具有最小距离的k个向量。由于评估了所有可能的匹配，这是一个<em class="kh">强力</em>搜索。该操作需要计算N*N距离，然后找到底部的k值:</p><pre class="ki kj kk kl fe km kn ko kp aw kq bi"><span id="bd44" class="kr ks hi kn b fj kt ku l kv kw">import numpy as np<br/>N = 10000<br/>d = 10<br/>k = 5<br/># create an array of N d-dimensional vectors (our search space)<br/>S = np.random.random((N, d)).astype('float32')<br/># create a random d-dimensional query vector<br/>x = np.random.random(d)<br/># compute distances<br/>distances = np.linalg.norm(S - x, axis = 1)<br/># select indices of vectors having the lowest distances from the X<br/>neighbours = np.argpartition(distances, range(0, k))[:k]</span></pre><p id="7e36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用scikit-learn: </strong> scikit-learn提供了一种简单的方法来搜索k个最近的邻居(使用<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html" rel="noopener ugc nofollow" target="_blank">sk learn . nearest neighbors . nearest neighbors</a>):</p><pre class="ki kj kk kl fe km kn ko kp aw kq bi"><span id="c08d" class="kr ks hi kn b fj kt ku l kv kw">from sklearn.neighbors import NearestNeighbors<br/>knn = NearestNeighbors(n_neighbors=k)<br/>knn.fit(S)<br/># select indices of k nearest neighbours<br/>neighbours = knn.kneighbors([x], return_distance = False)<br/>print(neighbours)</span></pre><p id="71ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果与使用numpy相同(因为精确的距离是计算出来的)。在这两种方法中，我们都需要将整个距离矩阵放入内存，这可能有点问题。</p><p id="d470" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kh">利用近似相似度</em> </strong></p><p id="05d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当数据集比RAM大得多时，或者应该实时提供答案时，有两种主要方法来近似嵌入相似性:基于树的方法和散列方法。</p><p id="51b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本思想类似于建立数据库索引。在基于树的方法中，以分而治之的方式递归地划分数据，这使得树中的相似向量彼此靠近。树索引需要大量内存，并且性能会随着高维数据而降低。</p><p id="0a66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在基于散列的方法中，想法是学习一个将项目转换成代码的模型，其中相似的项目将产生相同或相似的代码。这种方法大大减少了所需的内存。</p><p id="752c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有几个开源库<a class="ae iu" href="https://github.com/erikbern/ann-benchmarks" rel="noopener ugc nofollow" target="_blank">实现了近似相似性匹配技术。让我们回顾一下其中最受欢迎的。</a></p><p id="71e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">非度量空间库(NMSLIB) </strong>核心库没有任何第三方依赖关系。它包含在<a class="ae iu" href="https://aws.amazon.com/about-aws/whats-new/2020/03/build-k-nearest-neighbor-similarity-search-engine-with-amazon-elasticsearch-service/" rel="noopener ugc nofollow" target="_blank">亚马逊弹性搜索服务</a>中。它是用C++编写的，可以两种方式使用——直接用C++和Python。</p><p id="7b4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可以精确地回答查询，即通过返回不包含错误元素的完整结果集，或者近似地回答查询，例如通过仅找到一些邻居。因此，这些方法是根据效率-效果权衡来评估的，而不仅仅是根据它们的效率。一个常见的有效性度量是recall，它被计算为由该方法返回的真实邻居的平均分数(任意打破平局)。</p><pre class="ki kj kk kl fe km kn ko kp aw kq bi"><span id="b7ff" class="kr ks hi kn b fj kt ku l kv kw">import hnswlib<br/>import numpy as np</span><span id="a680" class="kr ks hi kn b fj kx ku l kv kw"># Declaring index<br/>p = hnswlib.Index(space='l2', dim=d)  # possible options are l2, cosine or ip</span><span id="7cef" class="kr ks hi kn b fj kx ku l kv kw">p.init_index(max_elements=N+1, ef_construction=100, M=16)<br/># Controlling the recall by setting ef:<br/># higher ef leads to better accuracy, but slower search<br/>p.set_ef(10)<br/>p.add_items(S)<br/># Query the elements for themselves and measure recall:<br/>labels, distances = p.knn_query(S, k=1)<br/>print("Recall:", np.mean(labels.reshape(-1) == np.arange(len(S))), "\n")</span><span id="83f5" class="kr ks hi kn b fj kx ku l kv kw"># Query the elements for themselves and measure recall:<br/>labels, distances = p.knn_query(x, k=k)<br/>print(labels[0])</span></pre><p id="09cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们是在一个非常小的数据集上进行测试，所以使用的搜索方法不一定优于强力搜索。通常，语料库越大，相对于强力搜索的效率改善越大。</p><p id="7285" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Faiss(脸书人工智能搜索)</strong></p><p id="86dc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Faiss是脸书开发的一个库，可以有效处理大型数据集和高维稀疏数据。它包含了几种相似性搜索的方法。它们中的大多数使用向量的压缩表示，并且不需要保持原始向量。这通常以更近似的搜索为代价，但是这些方法可以扩展到主存储器中的数十亿个向量。</p><p id="619b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Faiss是围绕存储一组向量的索引类型构建的，并提供了一个通过L2和/或点积向量比较在其中进行搜索的功能。一些索引类型是简单的基线，比如精确搜索。大多数可用的索引结构对应于以下各种权衡</p><ul class=""><li id="0a25" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">搜索时间</li><li id="3a33" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">搜索质量</li><li id="4bdc" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">每个索引向量使用的内存</li><li id="db67" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">训练时间</li><li id="e77b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">无监督训练需要外部数据</li></ul><p id="5630" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用Faiss选择索引类型有几种不同的可能性:</p><ul class=""><li id="a2a6" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">如果没有太多的搜索，内存是一个问题，需要精确匹配，那么最好的选择是“平面”索引</li><li id="0de9" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">如果数据集很大，内存是一个问题，那么需要选择NHSW。和在NMSLIB中一样，我们选择了M参数。4个&lt;= M &lt;= 64 is the number of links per vector, higher is more accurate but uses more RAM. The memory usage is (d * 4 + M * 2 * 4) bytes per vector.</li></ul><p id="3543" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Also there are different settings for the clustering of the dataset has to be done beforehand. After clustering, “Flat” parameter just organizes vectors into the buckets, and does not compress them, the storage size is the same as that of the original dataset. The tradeoff between speed and accuracy is set via the nprobe parameter which can reduce dimensionality, make quantization of the vectors into <em class="kh"> M个</em> 4位码等(更多<a class="ae iu" href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index" rel="noopener ugc nofollow" target="_blank">在此</a>)。</p><pre class="ki kj kk kl fe km kn ko kp aw kq bi"><span id="c3a0" class="kr ks hi kn b fj kt ku l kv kw">import faiss<br/>import numpy as np</span><span id="d165" class="kr ks hi kn b fj kx ku l kv kw"># Param of PQ<br/>M = 8  # The number of sub-vector. Typically this is 8, 16, 32, etc.<br/>nbits = 8 # bits per sub-vector. This is typically 8, so that each sub-vec is encoded by 1 byte<br/># Param of IVF<br/>nlist = 100  # The number of cells (space partition). Typical value is sqrt(N)<br/># Param of HNSW<br/>hnsw_m = 32  # The number of neighbors for HNSW. This is typically 32<br/># Setup<br/>quantizer = faiss.IndexHNSWFlat(d, hnsw_m)<br/>index = faiss.IndexIVFPQ(quantizer, d, nlist, M, nbits)<br/># Train<br/>index.train(S)<br/># Add<br/>index.add(np.array([x])<br/># Search<br/>index.nprobe = 8  # Runtime param. The number of cells that are visited for search.<br/>dists, ids = index.search(x=np.array([x])[:3], k=k)<br/>print(ids)</span></pre><p id="7384" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Spotify的</strong> <a class="ae iu" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">惹库</strong> </a></p><p id="3457" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Annoy是一个带有Python绑定的C++库，可以构建<a class="ae iu" href="http://wikipedia.org/wiki/Locality-sensitive_hashing#Random_projection" rel="noopener ugc nofollow" target="_blank">随机投影</a>树。索引是用一个由<code class="dv ky kz la kn b">k</code>棵树组成的森林构建的，其中<code class="dv ky kz la kn b">k</code>是一个可调参数，在精度和性能之间进行权衡。它还创建了大型的只读的、基于文件的数据结构，可以在许多进程之间共享。</p><pre class="ki kj kk kl fe km kn ko kp aw kq bi"><span id="689d" class="kr ks hi kn b fj kt ku l kv kw">from annoy import AnnoyIndex<br/>import random</span><span id="73f2" class="kr ks hi kn b fj kx ku l kv kw">t = AnnoyIndex(d, 'angular')  # Length of item vector that will be indexed<br/>for i in range(len(S)):<br/>    t.add_item(i, S[i])<br/>    <br/>t.build(10) # 10 trees<br/>print(u.get_nns_by_vector(x, k)) # will find the k nearest neighbours</span></pre><p id="42a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结论</strong></p><p id="c088" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/erikbern/ann-benchmarks" rel="noopener ugc nofollow" target="_blank">这里的</a>代表了具有不同数据集和距离度量的所有库的基准。正如我们所见，数据集越小，不同方法的工作速度之间的差异就越小。</p><figure class="ki kj kk kl fe ij es et paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="es et lb"><img src="../Images/3686e9c85052b37900969387e0710741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HldpfF6adu5xTmlY"/></div></div><figcaption class="iq ir eu es et is it bd b be z dy">Benchmarks for different datasets [<a class="ae iu" href="https://github.com/erikbern/ann-benchmarks" rel="noopener ugc nofollow" target="_blank">from here</a>]</figcaption></figure><p id="7c24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在本教程中，我们使用不同的库和方法构建了一个brut-force和一个近似k-NN相似性搜索特性。此外，还回顾了用于近似k-NN相似性的不同类型的指标。</p><p id="df0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">代码可以用<a class="ae iu" href="https://colab.research.google.com/drive/18kCP5TpSScL2SbujJORiQcxRfNaxT0FN?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>试用。</p><p id="53cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来源:</strong></p><div class="lc ld fa fc le lf"><a href="https://github.com/facebookresearch/faiss" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">GitHub - facebookresearch/faiss:一个高效的相似性搜索和密集聚类库…</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">Faiss是一个用于高效相似性搜索和密集向量聚类的库。它包含算法搜索…</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">github.com</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt io lf"/></div></div></a></div><div class="lc ld fa fc le lf"><a href="https://github.com/spotify/annoy" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">GitHub-Spotify/aroy:c++/Python中的近似最近邻居，针对内存使用和…</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">是一个带有Python绑定的C++库，用来搜索空间中的点…</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">github.com</p></div></div><div class="lo l"><div class="lu l lq lr ls lo lt io lf"/></div></div></a></div><div class="lc ld fa fc le lf"><a href="https://github.com/nmslib/hnswlib" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">GitHub - nmslib/hnswlib:用于快速近似最近邻的仅头文件C++/python库</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">使用python绑定的仅头文件C++ HNSW实现。新闻:Hnswlib现在是0.5.2。错误修正-感谢@ marekhanus</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">github.com</p></div></div><div class="lo l"><div class="lv l lq lr ls lo lt io lf"/></div></div></a></div><div class="lc ld fa fc le lf"><a href="https://davidefiocco.github.io/nearest-neighbor-search-with-faiss/" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">大搜索空间中k-最近邻搜索的第一步</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">TL；faiss库允许以一种有效的方式执行最近邻搜索，可以扩展到几百万…</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">davidefiocco.github.io</p></div></div><div class="lo l"><div class="lw l lq lr ls lo lt io lf"/></div></div></a></div><div class="lc ld fa fc le lf"><a href="https://github.com/erikbern/ann-benchmarks" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">GitHub-erikbern/ann-Benchmarks:Python中近似最近邻库的基准</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">在高维空间中快速搜索最近邻是一个越来越重要的问题，但到目前为止…</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">github.com</p></div></div><div class="lo l"><div class="lx l lq lr ls lo lt io lf"/></div></div></a></div><div class="lc ld fa fc le lf"><a href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dx"><div class="lh ab li cl cj lj"><h2 class="bd hj fj z dz lk eb ec ll ee eg hh bi translated">选择索引的指南facebookresearch/faiss Wiki</h2><div class="lm l"><h3 class="bd b fj z dz lk eb ec ll ee eg dy translated">选择指数不是显而易见的，所以这里有几个基本问题可以帮助选择指数。他们…</h3></div><div class="ln l"><p class="bd b fq z dz lk eb ec ll ee eg dy translated">github.com</p></div></div><div class="lo l"><div class="ly l lq lr ls lo lt io lf"/></div></div></a></div></div></div>    
</body>
</html>