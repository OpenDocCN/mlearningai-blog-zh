<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/serving-multiple-machine-learning-models-using-an-api-gateway-add068389b1f?source=collection_archive---------3-----------------------#2022-10-15">https://medium.com/mlearning-ai/serving-multiple-machine-learning-models-using-an-api-gateway-add068389b1f?source=collection_archive---------3-----------------------#2022-10-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="c638" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">使用API网关的实时ML推理路由</h2><p id="9c1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi iz translated">如今，像谷歌和优步这样的公司已经建立了面向消费者的实时推荐系统，同时为数千个产品模型提供服务。这些模型中的每一个都解决了<strong class="ig ji"> <em class="jj">相同的业务问题</em> </strong>，因此需要在请求处理时在几个下游模型推理服务之间划分用户。以下是几个保证推理请求路由的场景:</p><ol class=""><li id="c800" class="jk jl hh ig b ih jm il jn hr jo hv jp hz jq iy jr js jt ju bi translated">基于不同数据训练的模型用于为不同地区推荐产品</li><li id="d94d" class="jk jl hh ig b ih jv il jw hr jx hv jy hz jz iy jr js jt ju bi translated">金丝雀部署新培训的推荐模型—增量新模型推广降低了业务风险。</li><li id="b1c3" class="jk jl hh ig b ih jv il jw hr jx hv jy hz jz iy jr js jt ju bi translated">由于不同的模型导致的非确定性后端变化的A/B测试绝对是值得的。</li><li id="68ae" class="jk jl hh ig b ih jv il jw hr jx hv jy hz jz iy jr js jt ju bi translated">强化学习模型，其中基于生产中用户行为的奖励会影响模型的后续预测。我们希望比较每个模型的预测如何随着生产用户的输入而发展。</li></ol><p id="3962" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">在这篇文章中，我列出了实时推理路由的设计考虑，并比较了可扩展架构的选择。</p><h1 id="f3fb" class="kd hg hh bd hi ke kf kg hm kh ki kj hq kk kl km hu kn ko kp hy kq kr ks ic kt bi translated">模型复用解决方案</h1><p id="7197" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">我们将需要一个<a class="ae ku" href="https://www.nginx.com/resources/glossary/layer-7-load-balancing/" rel="noopener ugc nofollow" target="_blank"> L7负载平衡器</a>，它将接收传入的推理请求，并将它们路由到适当的下游服务。让我们来看看这种平台的关键设计目标，以及为什么这些目标对使用该模型的客户很重要。</p><p id="3a5c" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">推理微服务:</strong>接受A/B测试的每个模型都可能有不同的数据转换，可能使用不同的框架(scikit-learn或PyTorch)，并且必须可以独立部署。因此，随着时间的推移，基于monolith的进程内推理路由将不是管理的最佳选择。每个型号都有自己的微服务</p><p id="a218" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">动态路由:</strong> <a class="ae ku" href="https://istio.io/latest/docs/tasks/traffic-management/request-routing/" rel="noopener ugc nofollow" target="_blank">有几种现成的解决方案可以跨微服务随机分割流量或执行基于报头的路由</a>。然而，我们真正需要的是一个系统，它根据可动态配置的策略将传入的请求映射到特定的下游模型推理服务。</p><p id="556d" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">低延迟:</strong>路由系统是请求路径上不可避免的额外一跳，该请求路径已经处于<a class="ae ku" rel="noopener" href="/mlearning-ai/an-introduction-to-serving-real-time-machine-learning-predictions-4bab3bf245fb">处理时间</a>的紧张预算中，并且路由不应超过亚毫秒延迟。“Lua”是插件开发的编程语言选择，因为它速度快，保持最小的延迟。</p><p id="71f6" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">高并发性:</strong>让我们假设我们的模型被全球数百万客户使用，并且在任何给定时间都有数万个连接处于活动状态。</p><p id="c1be" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">路由解决方案必须符合上述所有设计目标。尽管这些目标看似相互冲突，但它们都是不可或缺的。</p><h1 id="4ab2" class="kd hg hh bd hi ke kf kg hm kh ki kj hq kk kl km hu kn ko kp hy kq kr ks ic kt bi translated"><strong class="ak">深度动态路由</strong></h1><h2 id="323f" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated"><strong class="ak">静态配置&amp;随机路由</strong></h2><p id="4d23" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">比方说，我们正在为全国几家汽车经销商使用的二手车定价估价模型提供服务。假设我们已经开始在生产中试验新的模型。有些请求比其他的更倾向于问责和解释。例如，在街对面经营经销商的两个相互竞争的客户更有可能比较和匹配彼此的车辆估价。在这种情况下，将请求路由到不同的模型(由于随机路由而导致的意外)会破坏估价产品的可信度。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/29365f5efc39f267124dd1394476fa03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d2Xnfjb_oQ3xhp-v"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Should customer_A ‘s inference go to Model_1 and customer_B ‘s inference go to Model_2?</figcaption></figure><blockquote class="ll lm ln"><p id="4e43" class="ie if jj ig b ih jm ij ik il jn in io lo ka iq ir lp kb it iu lq kc iw ix iy ha bi translated">静态配置导致生产请求的随机路由。当比较源自用不同超参数或数据训练的多个模型的预测时，这可能导致不一致</p></blockquote><h2 id="ac10" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated"><strong class="ak">解决方案:动态路由</strong></h2><p id="7183" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">源自任一客户的推理请求由中介进行检查，并通过实施定制路由策略(有时引用高度可用的数据存储)的方式<strong class="ig ji">有意识地路由到同一个模型</strong>。这样的路由策略将确保两个预测之间的一致性，使我们能够在客户质疑时解释预测。因此，在构建服务于多个模型的平台时，可配置的动态路由是一个重要的设计目标。</p><p id="a71d" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">考虑以下在模型部署中非常常见的场景</p><ol class=""><li id="d779" class="jk jl hh ig b ih jm il jn hr jo hv jp hz jq iy jr js jt ju bi translated">推出新型号，淘汰旧型号</li><li id="b89e" class="jk jl hh ig b ih jv il jw hr jx hv jy hz jz iy jr js jt ju bi translated">路由策略随着漂移模型性能和一个模型相对于另一个模型对于特定用户类别的适用性而发展。</li></ol><blockquote class="ll lm ln"><p id="6a8b" class="ie if jj ig b ih jm ij ik il jn in io lo ka iq ir lp kb it iu lq kc iw ix iy ha bi translated"><!-- -->动态路由网关不需要重新部署或重启以使上述改变生效，因为路由决策计算被推迟到请求处理时间</p></blockquote><p id="229b" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">动态路由中的并发方法</strong></p><p id="7f50" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">使用Flask等web服务器进行路由的问题是<em class="jj"> 1个进程一次只能处理1个请求。</em>处理请求的串行性质使得flask在单独部署时不适合生产规模。每个<strong class="ig ji"> gunicorn </strong>实例(管理Flask进程)只能勉强同时服务十几个连接。因此，在我们寻求路由可配置性的过程中，需要认识到反模式</p><h1 id="29bc" class="kd hg hh bd hi ke kf kg hm kh ki kj hq kk kl km hu kn ko kp hy kq kr ks ic kt bi translated"><strong class="ak">解决方案:反向代理</strong></h1><p id="e1f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">NGINX是一个反向代理，它将每个工作器的数万个传入用户请求连接解复用为几十个TCP连接，并由推理服务工作器维护。并发性声明假设最佳NGINX系统调优在实现期间有效。</p><p id="4d70" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">反向代理充当一个层，web服务器将并发性问题卸载到该层。提供模型推理的web服务器有责任实现无状态的亚毫秒级非阻塞操作。</p><p id="f14b" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">每个NGINX worker通过在内存中为每个传入的连接请求创建一个轻量级描述符来实现这个规模。还记得flask如何为每个请求创建一个重量级进程，从而抑制了它在生产环境中的扩展能力。</p><p id="735f" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">事实上，生产中的大多数模型已经使用现成的NGINX或云原生反向代理，在这种情况下，并发性问题被卸载。然而，在不忽略并发管理的情况下，在反向代理中配置动态路由策略并不简单。大多数使用反向代理拆分流量的解决方案实现了随机化的请求路由，这可能会有问题(如前一节所述)</p><h1 id="5156" class="kd hg hh bd hi ke kf kg hm kh ki kj hq kk kl km hu kn ko kp hy kq kr ks ic kt bi translated"><strong class="ak">可配置反向代理的市场解决方案</strong></h1><p id="a945" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated"><strong class="ig ji"> NGINX: </strong> NGINX是一个可配置的API网关/反向代理，可以使用名为OpenResty的框架进行定制。我个人认为，对于不是全职前端开发人员的人来说，编写NGINX插件可能有些危险</p><p id="1955" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">Kong——一个基于NGINX的API网关:</strong> Kong是一个构建在NGINX之上的开源网关，能够编写插件实现动态路由。虽然它几乎提供了与NGINX相同的好处，但它对编写插件提供了高级别的支持，因此您不必处理原始的NGINX变量操作。插件开发可以用多种语言完成，定制插件可以与免费的&amp;商业插件共存，从而最大化我们讨论的可配置性目标。</p><p id="febf" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">上述产品是与云无关的解决方案。它们可以自托管，在调配的kubernetes集群上几乎不需要任何成本，不像SaaS解决方案(GCP Apigee和AWS API Gateway)那样按请求收费。尽管成本很低，但我们可以使用Kong /NGINX实现高并发和动态路由可配置性，而无需云厂商锁定！</p><p id="dc11" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji"> NGINX插件&amp;路由可配置性</strong></p><p id="77a4" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">一个<strong class="ig ji"> API网关</strong>实现了一个状态机，它转换来自客户端的输入请求以及来自web服务器<strong class="ig ji">的输出响应。</strong></p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lr"><img src="../Images/f467fd3b9d4f692358e5676dd7917011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KVP175Q1kfC1acUy"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">The state machine container runs within each worker Pod</figcaption></figure><p id="2237" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">状态机中的每个转换或"<strong class="ig ji">阶段</strong>在一个框中指定，例如"<strong class="ig ji">生成内容</strong>或"<strong class="ig ji">请求重写</strong>，它们由NGINX在传入请求被代理到下游模型推理服务之前执行。要实现动态路由，我们遵循以下步骤:</p><ol class=""><li id="1bc6" class="jk jl hh ig b ih jm il jn hr jo hv jp hz jq iy jr js jt ju bi translated">通过注入插件代码来定制合适的“阶段”。您可以将插件代码视为类似于在请求/响应转换期间触发某个“阶段”时调用的回调。</li><li id="9a49" class="jk jl hh ig b ih jv il jw hr jx hv jy hz jz iy jr js jt ju bi translated">这些阶段以及插件代码应该通过非阻塞和使用高度可用的在线数据存储来实现延迟目标</li></ol><h1 id="e684" class="kd hg hh bd hi ke kf kg hm kh ki kj hq kk kl km hu kn ko kp hy kq kr ks ic kt bi translated"><strong class="ak">权衡:</strong></h1><p id="c8c0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">1 .通常用于网页A/B测试的开箱即用解决方案，如<a class="ae ku" href="https://www.optimizely.com/" rel="noopener ugc nofollow" target="_blank"> Optimizely </a>，似乎不太适合机器学习推理路由。拥有全栈开发人员的工程团队可能喜欢将数据分析产品构建工作交给供应商的奢侈享受。对于喜欢处理原始数据并构建自己的假设进行测试的数据科学团队来说，情况就不一样了。因此，拆分流量和收集这些拆分的原始性能数据可能比不灵活的端到端产品更有价值</p><p id="dff5" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">2.当扩展像Kong这样的API网关时，该层使我们能够灵活地添加安全性、速率限制、代理缓存以减少延迟，所有这些都是实时推理设置中至关重要的质量参数。相反，如果我们选择使用Optimizely的开箱即用网关/Saas解决方案，我们将无法调整我们需要构建的规模。</p><p id="7276" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">3.当使用Kong时，<strong class="ig ji">总是使用Lua来编写插件代码，因为它是最快的选择</strong>。用于插件开发的Go/Python代码的问题在于，插件驻留在外部服务器中，请求转换通过网络(在同一集群中)发送到该服务器，这可能会增加几十毫秒的延迟。</p><p id="4f82" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated">4.请始终注意控制平面增加了多少开销。与服务网格相比，API网关的实现要简单得多，因此有助于降低延迟。</p><blockquote class="ll lm ln"><p id="d9e9" class="ie if jj ig b ih jm ij ik il jn in io lo ka iq ir lp kb it iu lq kc iw ix iy ha bi translated">在<!-- -->深度学习实时推理中，高维特征会影响缓存读取延迟，前向传播中的几个层会影响预测时间。因此，迫切需要在推理请求路径的上游节省尽可能多的毫秒。在Lua中定制的KONG反向代理在合理扩展时可以精确地实现这些目标。</p></blockquote><p id="cffb" class="pw-post-body-paragraph ie if hh ig b ih jm ij ik il jn in io hr ka iq ir hv kb it iu hz kc iw ix iy ha bi translated"><strong class="ig ji">结论:</strong>生产中的实时推理路由和预测后端中/外的阶段模型有无数的应用，但是需要仔细考虑沿着请求路径的架构选择。概述的方法存在一些限制，如<strong class="ig ji">插件开发不成熟，插件代码变更管理，无法区分插件错误和代理安装错误，lua等语言增加了技术蔓延，管理复杂的Kubernetes安装等</strong>。虽然上述解决方案解决了关键的设计目标，但定制的反向代理平台还有很长的路要走</p><div class="ls lt ez fb lu lv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lw ab dw"><div class="lx ab ly cl cj lz"><h2 class="bd ji fi z dy ma ea eb mb ed ef mc bi translated">Mlearning.ai提交建议</h2><div class="md l"><h3 class="bd b fi z dy ma ea eb mb ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="me l"><p class="bd b fp z dy ma ea eb mb ed ef dx translated">medium.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk lf lv"/></div></div></a></div></div></div>    
</body>
</html>