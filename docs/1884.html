<html>
<head>
<title>An introduction to 3D Object Detection in Autonomous Navigation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自主导航中的三维目标检测导论。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/an-introduction-to-3d-object-detection-in-autonomous-navigation-3a8779de6443?source=collection_archive---------0-----------------------#2022-02-08">https://medium.com/mlearning-ai/an-introduction-to-3d-object-detection-in-autonomous-navigation-3a8779de6443?source=collection_archive---------0-----------------------#2022-02-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0d24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与2D相比，点云中的物体检测是相当复杂的。在这里，您可以找到关于在实现3D数据对象检测器的过程中需要考虑的最重要部分的简要介绍。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/be631d58c54b12dcbcd9a14082d46b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HxpGva6URtGzqKGzBAmdxA.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">from <a class="ae js" href="https://ouster.com/" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="078c" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结构</h1><ul class=""><li id="d32c" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated">(1) <strong class="ig hi">为什么是3D数据</strong></li><li id="e788" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">(2) <strong class="ig hi">传感器</strong></li><li id="1ae1" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">(3) <strong class="ig hi">数据集</strong></li><li id="aebd" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">(4) <strong class="ig hi">神经网络</strong></li></ul><h1 id="4511" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated">(1)为什么是3D数据</h1><p id="fa09" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">3D数据是<strong class="ig hi">在另一个层面上理解世界</strong>的关键，但不幸的是不容易获得。这也是3D网络尚未接管的原因。第三维度使我们能够以更人性化的方式理解这个世界，甚至更好。想象一下，我们正驾驶着一辆汽车驶向下面这个类似大门的建筑:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lw"><img src="../Images/5612b9b684c06adc8e2e5011a092852c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QrC8Bj-Yy6xGUiOqJO1sfw.jpeg"/></div></div></figure><p id="ce88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">只有2D的数据，我们不可能知道开车穿过这个门(然后避开它后面的障碍物)就能通过。另一个真实世界的例子是通过隧道。</p><p id="699a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3D数据</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/c0fd6834c2af929805be28b8de237046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*TnlJNAVf6NJrhkti7LH0WA.jpeg"/></div></div></figure><p id="9576" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3D数据不是由其RGB值描述的，而是由其在空间中的位置(x，y，z坐标)描述的。原点(0，0，0)被设置为捕捉数据的对象(在本例中为无人机)。相应地，其他所有东西，比如人，都是相对于<strong class="ig hi">原点</strong>来描述的。例如，这个人可能在点(x，y，z) = (5，1，0)。(x，y，z)值的其他排列是可能的，这取决于您正在处理的数据集。例如，KITTI数据表示如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/deb73871b78c3dd2f5b2b8f6336a8008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*d7hT4qg8iDEQpjGHQBkbqQ.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">from <a class="ae js" href="https://towardsdatascience.com/kitti-coordinate-transformations-125094cd42fb" rel="noopener" target="_blank">here</a></figcaption></figure><h1 id="f4da" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated">(2)传感器</h1><h1 id="4ab0" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated"><strong class="ak">激光雷达</strong></h1><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/2f2be0d74cda89f795ac4fa5ed12b773.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*9T7leY9nhsGJNyJj9gr1fQ.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">LiDAR sensors calculate the distance with the speed of light. <a class="ae js" href="https://airccj.org/CSCP/vol5/csit54618.pdf" rel="noopener ugc nofollow" target="_blank">from here</a></figcaption></figure><p id="37e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们谈论点云，激光雷达是当然不得不提的。最著名的传感器是威力登HDL-64E ，这是一个12.7公斤的庞然大物，于2007年推出，一直主导着高端激光雷达。2021年，威力登宣布停止使用这种传感器，取而代之的是更轻的传感器，如<a class="ae js" href="https://velodynelidar.com/products/" rel="noopener ugc nofollow" target="_blank">冰球</a>(不到1公斤)。这些传感器的功能很简单，它们发出多个垂直排列的光束<strong class="ig hi">来测量距离。从表面反射后，光束随后被传感器捕获，因此可以计算出<strong class="ig hi">距离</strong>:</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ma"><img src="../Images/d2c33c543ed117b78d24a25f9afaf423.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*1Vpu5sTr6QV_my-bkO51yA.png"/></div></figure><p id="0318" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">，其中<em class="mb"> c </em> ₐᵢᵣ是光速。你可能想知道，我们用单一的距离测量做什么？没什么！相反，我们需要成千上万的照片来获得我们周围环境的有意义的图像。因此，激光雷达<strong class="ig hi">传感器旋转</strong>，同时不断进行测量，以获得<strong class="ig hi">360°全景</strong>。最终结果是一个点云:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mc"><img src="../Images/2072e1438666dc1309e91a18512e1337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Eofyupkvv5r19fNUq71EMg.gif"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">from <a class="ae js" href="https://www.playment.io/blog/list-of-lidar-datasets-for-autonomous-vehicles-till-2018" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="64fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">单点云由一系列(x，y，z)点坐标组成，其中<strong class="ig hi">中心点(0，0，0)位于传感器的位置</strong>。你可能注意到了围绕中心点(汽车的位置)的点的环形分布，这是由激光雷达传感器的<strong class="ig hi">旋转</strong>造成的。</p><p id="b51b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了威力登，还有其他激光雷达传感器制造商，如<a class="ae js" href="https://ouster.com/" rel="noopener ugc nofollow" target="_blank">驱逐者</a>、<a class="ae js" href="https://innoviz.tech/innovizone" rel="noopener ugc nofollow" target="_blank">创新地带</a>、<a class="ae js" href="https://www.aeva.ai/" rel="noopener ugc nofollow" target="_blank"> Aeva </a>和<a class="ae js" href="https://www.luminartech.com/" rel="noopener ugc nofollow" target="_blank"> Luminar </a>。</p><ul class=""><li id="74fa" class="ky kz hh ig b ih ii il im ip md it me ix mf jb lf lg lh li bi translated">激光雷达<strong class="ig hi"> pro </strong>:高范围/精度</li><li id="dbbd" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">激光雷达<strong class="ig hi">缺点</strong>:重量/成本高</li></ul><h1 id="1f7f" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated"><strong class="ak">固态激光雷达</strong></h1><p id="57be" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">我们要研究的下一种传感器是固态激光雷达。尽管机械激光雷达已经走过了漫长的道路，最轻的解决方案(<a class="ae js" href="https://velodynelidar.com/products/puck-lite/" rel="noopener ugc nofollow" target="_blank"> Puck LITE </a>)仍然非常重，只有0.5千克。对于像无人机这样的小型机器人来说，这一点意义重大。此外，由于振动、机械冲击、温度和湿度变化，<strong class="ig hi">机械激光雷达</strong> s的移动部件需要大量能量和<a class="ae js" href="https://www.linkedin.com/pulse/spot-differences-what-look-lidar-sensor-aditya-srinivasan/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">维护</strong> </a> <strong class="ig hi">。因此，一种叫做SSD的新技术被开发出来。</strong></p><blockquote class="mg mh mi"><p id="3b70" class="ie if mb ig b ih ii ij ik il im in io mj iq ir is mk iu iv iw ml iy iz ja jb ha bi translated">光学发射器以特定的模式和相位发出光子脉冲，从而产生定向发射。</p></blockquote><p id="fcdb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SSD的两种主要方法是OPA(光学相控阵)和MEMS(微机电系统)。一个<strong class="ig hi"> OPA </strong> <strong class="ig hi">以<strong class="ig hi">特定的模式</strong>和相位发出光子</strong>脉冲，产生定向发射。另一方面，<strong class="ig hi">基于MEMS </strong>的扫描仪<a class="ae js" href="https://medium.com/r?url=https%3A%2F%2Fios.gadgethacks.com%2Fnews%2Flidar-vs-3d-tof-sensors-apple-is-making-ar-better-for-smartphones-0280778%2F" rel="noopener"> <strong class="ig hi">使用微镜</strong>控制发射方向和聚焦</a>。换句话说，OPA和MEMS在使用的激光器数量上有所不同:<strong class="ig hi">OPA使用许多激光器，而MEMS仅使用一个</strong>，由微镜瞄准。这些技术也用在了<strong class="ig hi"> iPhone 12 Pro和2020 iPad Pro </strong>上。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mm"><img src="../Images/b0d71d3cf105917f931a48ee69ad011d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuowX81mpFGOno6VYtceLQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Left: <a class="ae js" href="https://www.allaboutcircuits.com/news/solid-state-lidar-faster-cheaper-better/" rel="noopener ugc nofollow" target="_blank">OPA</a>, Right: <a class="ae js" href="https://www.researchgate.net/publication/260332215_A_Two-Dimensional_MEMS_Scanning_Mirror_Using_Hybrid_Actuation_Mechanisms_With_Low_Operation_Voltage" rel="noopener ugc nofollow" target="_blank">MEMS</a></figcaption></figure><p id="6e4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于SSD是一项相对较新的技术，所以还没有发布多少传感器。英特尔和微软在2019年和2020年推出了固态解决方案，首次在市场上取得了成功。微软Azure Kinect的<a class="ae js" href="https://azure.microsoft.com/de-de/services/kinect-dk/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi"/></a>和英特尔L515的<strong class="ig hi"/>价格都很低，都在400美元左右，然而，Kinect的重量是L515的五倍(440克比100克)。另一方面，Azure似乎有稍微好一点的范围和精确度，如测试镜头所示。正如最大9米的有限范围所示，这两款相机都不是为自动驾驶而制造的。此外，英特尔明确声明L515不适合户外使用，因为太阳光会干扰工作在860纳米的激光。此外，看看这篇文章，L515在不同的光线条件下进行了测试。微软Azure Kinect <a class="ae js" href="https://www.mdpi.com/1424-8220/21/2/413" rel="noopener ugc nofollow" target="_blank">也有同样的问题</a>。对于未来，其他制造商如<strong class="ig hi">威力登和驱逐者</strong>已经宣布<strong class="ig hi">远程户外固态</strong>传感器将于<strong class="ig hi"> 2022 </strong>问世。这些新的传感器可能最终会通过降低成本使激光雷达技术在大众市场上买得起，并使其适应道路、农田、建筑工地和矿井等崎岖地形。</p><ul class=""><li id="5ded" class="ky kz hh ig b ih ii il im ip md it me ix mf jb lf lg lh li bi translated">SSD <strong class="ig hi"> pro </strong>:重量轻/精度中等</li><li id="a1c0" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">固态硬盘<strong class="ig hi">缺点</strong>:低范围/仅室内(两者都将很快改变)</li></ul><h1 id="1199" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated"><strong class="ak">立体视觉</strong></h1><p id="48f7" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">立体视觉是一种<strong class="ig hi">非激光雷达</strong>方法，通过使用<strong class="ig hi">常规RGB或红外</strong>相机获取深度信息，来降低成本、重量和<strong class="ig hi">机械复杂性</strong>。为了完整起见，应该提到的是，非激光雷达方法可以是单目的(单个相机)或多视图的(多个相机)。这里，我们将集中在多视图方法上，因为它们更加精确<a class="ae js" href="https://arxiv.org/abs/1803.06199" rel="noopener ugc nofollow" target="_blank">而没有明显的缺点</a>。由于在立体视觉<strong class="ig hi"> </strong>中<strong class="ig hi"> </strong>距离<strong class="ig hi">不是直接测量的</strong>(像在激光雷达中一样)基于相机的方法使用<strong class="ig hi">计算机视觉技术</strong>来寻找图像中的<strong class="ig hi">深度线索</strong>。为了更好地理解核心问题，请参见下图，其中3D场景被映射到<strong class="ig hi"> 2d相机平面</strong>(标记为“近裁剪平面”)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mn"><img src="../Images/8fd56f4404c3727925f6b0dff9c6ad6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*X5Ee2BqCZ3jqEnpH62RMnw.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">from <a class="ae js" href="https://towardsdatascience.com/depth-estimation-1-basics-and-intuition-86f2c9538cd1" rel="noopener" target="_blank">here</a></figcaption></figure><p id="1097" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种映射也被称为<strong class="ig hi"> 2D投影</strong>，描述了相机的正常<br/>功能，以产生真实3D世界的<strong class="ig hi"> 2D表示</strong>。由于<strong class="ig hi">深度轴在2D投影过程中</strong>被不可逆删除，因此<strong class="ig hi">无法从图像中读取深度信息</strong>。</p><p id="d56d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mb">补充:当使用单个相机时，算法可以用来获得深度信息，同样的技术也可以在人脑</em>  <em class="mb">中找到</em> <strong class="ig hi"> <em class="mb">(例如，通过搜索所谓的</em> <strong class="ig hi"> <em class="mb">深度线索</em> </strong> <em class="mb">，如大小、纹理、视角和运动)。</em></strong></p><p id="a18d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在使用多个摄像机(立体视觉)的情况下，可以应用一种称为<strong class="ig hi">立体视觉</strong> [49]的强大技术来获得深度信息。立体视觉描述了<a class="ae js" href="http://www.cs.cornell.edu/~rdz/Papers/BVZ-PAMI98.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">借助<strong class="ig hi">三角测量<strong class="ig hi">从不同视角拍摄同一物体的两幅相机图像</strong>计算距离</strong> </strong></a>的可能性。理解立体视觉的直观例子如下图所示，当从两个不同的角度观察<strong class="ig hi">时，较近的物体(食指)比较远的物体(房子)移位更多。这些物体之间的位移被称为<strong class="ig hi">视网膜视差</strong>，使我们能够通过公式z =(f∫b)/d精确计算距离，其中z是我们要搜索的距离，f是相机的焦距，b是两个相机之间的距离，d等于x1 x2。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mo"><img src="../Images/b2127c41d1f2078d9738a310b0881267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgYXh3mFr5Ebd8YUuZnCHw.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Calculate distance with retina disparity. from <a class="ae js" href="https://towardsdatascience.com/depth-estimation-1-basics-and-intuition-86f2c9538cd1" rel="noopener" target="_blank">here</a></figcaption></figure><p id="7059" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管三角测量方程使我们能够精确计算到任何物体的距离，但真正的困难在于前一步——在<strong class="ig hi">中确定物体的位移</strong>。这个问题可以通过使用<strong class="ig hi">极线几何</strong>(我会单独写一篇文章)<strong class="ig hi">有效解决。</strong>但是，距离计算在<a class="ae js" href="https://medium.com/r?url=https%3A%2F%2Fwww.andreasjakl.com%2Funderstand-and-apply-stereo-rectification-for-depth-maps-part-2%2F" rel="noopener">以下情况</a>中容易不准确:</p><ul class=""><li id="6a91" class="ky kz hh ig b ih ii il im ip md it me ix mf jb lf lg lh li bi translated"><strong class="ig hi"> <em class="mb">图案纹理区域</em> </strong>:一个图案有多个看起来相等的区域。</li><li id="9b0d" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> <em class="mb">无纹理区域</em> </strong>:多个像素的像素强度相同。</li><li id="2a2c" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> <em class="mb">反光面</em> </strong>:镜面状的物体表现出其他物体的纹理。</li><li id="6eec" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> <em class="mb">遮挡</em> </strong>:物体在一个视图中被遮挡，而在另一个视图中没有被遮挡。</li><li id="b8fc" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi"> <em class="mb">违反朗伯性质</em> </strong>:当物体的亮度不(不)随透视而变化时。</li></ul><h2 id="da44" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated">制造商</h2><p id="117f" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">立体视觉方法的市场由英特尔和Stereolabs主导。英特尔的立体视觉传感器，d415、<a class="ae js" href="https://www.intelrealsense.com/depth-camera-d435i/" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">、d435i </strong> </a>和D455。最轻的型号d435i的重量为<strong class="ig hi"> 72g </strong>，因此是所有制造商中第二轻的。Stereolabs提供了ZED 2和ZED Mini。主要区别在于英特尔提供板载计算和一个<strong class="ig hi">红外发射器</strong>。<strong class="ig hi">板载计算</strong>通过使用<br/>英特尔实感视觉处理器D4，允许直接输出深度图。相比之下，ZED 2和ZED mini都依赖外部GPU(如<a class="ae js" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" rel="noopener ugc nofollow" target="_blank"> Jetson Nano </a>)进行计算，这意味着额外的功耗(仅Jetson Nano就有5-10瓦)。d435i的另一个优势是，由于其<strong class="ig hi">红外摄像头</strong> (Stereolabs使用RGB)，它可以在<strong class="ig hi">完全黑暗的</strong>中工作。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nd"><img src="../Images/c7bcb869540532bd4b7b53ee95f54098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E3DblIEzIwDb4g7f4e2XgQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Depth images created by the Intel Realsense d435i. from <a class="ae js" href="https://dev.intelrealsense.com/docs/optical-filters-for-intel-realsense-depth-cameras-d400" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="01a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我的实验中，我看到d435i在最远距离内产生有用的结果。<strong class="ig hi">7-10米</strong>，最佳作业范围<br/>低得多(1-3米)，而ZED系列可使用至<strong class="ig hi">20</strong>24】米，如<a class="ae js" href="https://www.youtube.com/watch?v=tv4B01ELFWo" rel="noopener ugc nofollow" target="_blank">本</a>图所示。</p><h1 id="e164" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated">(3)数据集</h1><p id="3ba4" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">我发现3D对象检测有三个有趣的类别:</p><h2 id="9441" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated">a)自动驾驶</h2><ul class=""><li id="1022" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated"><a class="ae js" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"> KITTI </a>，<a class="ae js" href="https://www.nuscenes.org/" rel="noopener ugc nofollow" target="_blank"> nuScenes </a>，<a class="ae js" href="https://github.com/waymo-research/waymo-open-dataset" rel="noopener ugc nofollow" target="_blank"> Waymo </a>，<a class="ae js" href="https://level-5.global/data/" rel="noopener ugc nofollow" target="_blank"> Lyft </a>，<a class="ae js" href="http://apolloscape.auto/" rel="noopener ugc nofollow" target="_blank"> apolloscape </a>，<a class="ae js" href="https://avdata.ford.com/downloads/default.aspx" rel="noopener ugc nofollow" target="_blank">福特</a></li></ul><h2 id="4fdf" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated">b) <strong class="ak"> <em class="ne">三维物体检测</em> </strong></h2><ul class=""><li id="16ee" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated"><a class="ae js" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank"> NYU深度</a>，<a class="ae js" href="https://rgbd.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">孙RGB-D </a>，<a class="ae js" href="http://www.scan-net.org/" rel="noopener ugc nofollow" target="_blank"> ScanNet </a>，<a class="ae js" href="http://graphics.stanford.edu/data/3Dscanrep/" rel="noopener ugc nofollow" target="_blank">斯坦福</a> 3D</li></ul><h2 id="ffb2" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated">c)人体姿态检测</h2><ul class=""><li id="2c06" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated"><a class="ae js" href="https://cocodataset.org/#densepose-2020" rel="noopener ugc nofollow" target="_blank">田蜜</a>，<a class="ae js" href="http://human-pose.mpi-inf.mpg.de/" rel="noopener ugc nofollow" target="_blank"> MPII </a>，<a class="ae js" href="https://github.com/Jeff-sjtu/CrowdPose" rel="noopener ugc nofollow" target="_blank">众筹</a>，<a class="ae js" href="https://posetrack.net/" rel="noopener ugc nofollow" target="_blank"> PoseTrack </a>，<a class="ae js" href="http://domedb.perception.cs.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> CMU全景</a></li></ul><p id="0e84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">a)最明显的数据集当然是用于自动驾驶的数据集，其中我们找到了对<strong class="ig hi"> 3d长方体</strong>、<strong class="ig hi">激光雷达分割标签</strong>、<strong class="ig hi"> 2d框</strong>、<strong class="ig hi">实例遮罩、</strong>和<strong class="ig hi"> 2d分割遮罩</strong>的注释。在这里，KITTI是最知名的，因为它描绘了最早的自动驾驶3D数据集之一。nuScenes更加通用，因为它还包括夜间和不同天气条件下的录音，这对训练一个强大的网络非常重要。请记住，如果训练数据中没有包含某些内容，网络很可能在以后的现实生活中出错。因此，在选择正确的数据时要明智地选择。大多数数据集使用的传感器是<strong class="ig hi">威力登HDL-64E </strong>或其他高成本/高精度选项。</p><p id="0ef1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">b)第二类数据集是为自动驾驶之外的3D对象检测的一般任务而制作的。这里的常见物品大多出现在<strong class="ig hi">家庭中，比如椅子</strong>和杯子。注释也是2D / 3D对象和分段。与类别a)无关，大多数数据集都是用像<a class="ae js" href="https://rgbd.cs.princeton.edu/paper.pdf" rel="noopener ugc nofollow" target="_blank"> Kinect </a>这样的RGB-D传感器或用3D软件人工创建的。必须指出的是，RGB-D传感器远不如激光雷达精确，因此点云中的噪声(<strong class="ig hi">距离误差</strong>)更高。例如，在这张图片中，您可以看到超轻量级<strong class="ig hi">英特尔实感d435i </strong>在一米和三米距离内的输出:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/39d6b81a506b252bb69753fc425d4f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*flVvKb-OvFOqbv6GUL0UIA.jpeg"/></div></div></figure><p id="df45" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这里，你从侧面看到一个人，坐在椅子上。在三米远的地方，点云已经变得如此混乱，以至于几乎认不出这个人了。然而，列出的数据集不是用d435i记录的，而是用其他精度稍高的传感器记录的，从而获得更好的数据。<strong class="ig hi">请记住，根据您使用的传感器，您可能找不到适合您的训练数据。</strong>例如，d435i没有可用的公共数据集。</p><p id="c7b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">c)最后一类数据集不直接属于3D检测，因为它们中的大多数仅包含2D数据和注释。这里，像人的关节这样的某些关键点是感兴趣的。然而，<a class="ae js" href="http://domedb.perception.cs.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> CMU全景</a>是用RGB-D传感器记录的，因此也可以用于使用点云的网络。</p><h1 id="35e1" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated">(4)神经网络</h1><p id="c75e" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">大多数出版物可以归入以下类别之一:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/e2cb74874155a02f707ad657bf38d2e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhIIlSa_MeZLxsnqeloWuQ.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">The three approaches for 3D object detection.</figcaption></figure><h2 id="5aa9" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated"><strong class="ak">答:逐点特征提取</strong></h2><p id="d24d" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">3D对象检测的第一种方法是特征的<strong class="ig hi">逐点提取</strong>。这可以通过使用线性层或2D/3D卷积层来实现。无论您使用哪种图层类型，网络都会非常慢，因为每个云中存在大量的点(10k-200k个点)。一个例子就是<a class="ae js" href="https://github.com/charlesq34/pointnet" rel="noopener ugc nofollow" target="_blank">点网</a> (2017)。</p><h2 id="085f" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated"><strong class="ak"> B:聚合视图对象检测(AVOD) </strong></h2><p id="607a" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">目前在研究中最受关注的第二种方法是<strong class="ig hi">将点聚集成子集</strong>，其中每个子集可以被视为3D空间中某个<strong class="ig hi">区域的代表。落入该区域的所有点现在都与该区域相关联，并通过卷积</strong>聚合<strong class="ig hi">。最后，您可以像对待A部分中的点(逐点要素提取)一样对待这些聚合区域，方法是通过一堆图层转发它们。这些方法可能非常复杂，因为<strong class="ig hi">聚合过程考虑了点</strong>之间的各种相关性。最常见的聚合区域类型是<strong class="ig hi">网格</strong>、<strong class="ig hi">集合</strong>和<strong class="ig hi">图形</strong>，其中最简单的解决方案是网格。对于网格方法，整个三维区域被分割成子部分(均等地切片或切块)，并且每个子部分中的点因此被聚集成单个特征列表。这个过程也被称为<strong class="ig hi">体素化</strong>。使用体素化的著名网络有<a class="ae js" href="https://github.com/qianguih/voxelnet" rel="noopener ugc nofollow" target="_blank"> VoxelNet </a> (2017)、<a class="ae js" href="https://github.com/traveller59/second.pytorch" rel="noopener ugc nofollow" target="_blank"> SECOND </a> (2018)、<a class="ae js" href="https://github.com/nutonomy/second.pytorch" rel="noopener ugc nofollow" target="_blank"> Pointpillars </a> (2019)、以及<a class="ae js" href="https://github.com/Vegeta2020/SE-SSD" rel="noopener ugc nofollow" target="_blank"> SE-SSD </a> (2021)。</strong></p><h2 id="df5e" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated"><strong class="ak"> C:感兴趣的截锥/区域</strong></h2><p id="f673" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">第三种类型的网络试图<strong class="ig hi">使用</strong>在2D检测器中发现的<strong class="ig hi">高精度，通过(1):从RGB图像中提取感兴趣区域(ROI)，该图像是在点云被捕获的同时捕获的，以及(2): <strong class="ig hi">当检测点云中的3D盒子时，仅考虑ROI</strong>。例如，如果2D探测器在RGB图像中发现一个人，我们可以通过使用<strong class="ig hi">透视变换</strong>将点云映射到图像的2D平面上，这样图像像素和点(来自云)就可以完美地相互关联(它们现在位于同一平面上)。现在，我们可以很容易地从云</strong>中选择点，2D探测器认为一个人在那里，因此，将这些点转发到另一个网络进行3D处理。这种方法工作良好，因为2D检测器对3D检测器进行预选，从而减少了误差。然而，这种方法也有缺点:</p><ul class=""><li id="5ee0" class="ky kz hh ig b ih ii il im ip md it me ix mf jb lf lg lh li bi translated">处理两个而不是一个数据流需要<strong class="ig hi">更多的处理能力</strong></li><li id="bf4d" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">点云传感器和RGB相机需要<strong class="ig hi">完全对齐</strong>才能计算透视变换</li><li id="3a74" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">多个传感器意味着<strong class="ig hi">多个故障源</strong></li></ul><p id="6de0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于平截头体提取的一个网络是<a class="ae js" href="https://arxiv.org/abs/1711.08488" rel="noopener ugc nofollow" target="_blank">平截头体点网</a>。</p><h2 id="9254" class="mp kb hh bd kc mq mr ms kg mt mu mv kk ip mw mx ko it my mz ks ix na nb kw nc bi translated"><strong class="ak">速度</strong></h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nf"><img src="../Images/40b09d5107dbc384d68c53acf17a83e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*P7oK5fMxVHtEw_Wv4_b28Q.jpeg"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">from <a class="ae js" href="https://arxiv.org/pdf/2104.09804v1.pdf" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="0d6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">速度和精度显然必须平衡。网络越精致，计算时间就越长。可以看到，2019年的Pointpillars仍然以接近<strong class="ig hi"> 50 fps </strong>的速度位居榜首，其次是30 fps的SE-SSD，它在图表创建时也具有最佳的精度(Arpil 2021)。</p><h1 id="0002" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated">总结:</h1><p id="0980" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated">您已经简要了解了3D对象检测中的重要主题。对于未来，网络<strong class="ig hi">速度和准确性</strong>以及传感器都有很多值得期待的地方。由于更可靠的感知和更自信的导航，新的导航机会将会出现，特别是随着<strong class="ig hi">更复杂的固态</strong> <strong class="ig hi">激光雷达</strong>解决方案的兴起。想象一下完美感知的可能性。例如，无人机可以高速飞行而不会发生事故，就像电脑游戏中的<strong class="ig hi">机器人</strong> <strong class="ig hi">。完美的感知是迈向更高智能算法的重要一步。</strong></p><h1 id="c2b0" class="ka kb hh bd kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ls kv kw kx bi translated"><strong class="ak">未来主题:</strong></h1><p id="85ca" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lt ir is it lu iv iw ix lv iz ja jb ha bi translated"><strong class="ig hi">未来文章</strong>将考虑:</p><ul class=""><li id="fca6" class="ky kz hh ig b ih ii il im ip md it me ix mf jb lf lg lh li bi translated">点云中的<strong class="ig hi">增强</strong></li><li id="c399" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">在<strong class="ig hi"> </strong>立体视觉和<strong class="ig hi">极线几何</strong>中创建点云</li><li id="3227" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi">从2d到3d的透视投影</strong></li><li id="de97" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi">点柱</strong>网络的Tensorflow实现</li></ul><p id="5993" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于这个和其他ML主题的更多文章将很快出现。所以如果你喜欢就订阅吧。</p><p id="be7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">培养基:</strong> <a class="ae js" rel="noopener" href="/@krull.matthes"> Matthes Krull—培养基</a></p><p id="c0f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">关于我</strong>:我是一名热情的深度学习工程师，工作在柏林。你可以通过<strong class="ig hi"><em class="mb">krull.matthes@gmail.com</em></strong><em class="mb">联系我。</em></p><div class="ng nh ez fb ni nj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nk ab dw"><div class="nl ab nm cl cj nn"><h2 class="bd hi fi z dy no ea eb np ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nq l"><h3 class="bd b fi z dy no ea eb np ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nr l"><p class="bd b fp z dy no ea eb np ed ef dx translated">medium.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx jm nj"/></div></div></a></div><p id="5779" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">成为作家</a></p></div></div>    
</body>
</html>