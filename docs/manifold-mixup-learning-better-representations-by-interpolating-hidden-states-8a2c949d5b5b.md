# æµå½¢æ··åˆ:é€šè¿‡æ’å…¥éšè—çŠ¶æ€å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/manifold-mixup-learning-better-representations-by-interpolating-hidden-states-8a2c949d5b5b?source=collection_archive---------6----------------------->

![](img/0ee723ea372fdc106eefa104a48bcde1.png)

Photo by [Matt Moloney](https://unsplash.com/@mattmoloney?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

æµå½¢æ··åˆæ—©åœ¨ 2019 å¹´å°±åœ¨[è¿™ç¯‡](https://arxiv.org/abs/1806.05236)è®ºæ–‡ä¸­ä»‹ç»è¿‡ï¼Œä¸ä¹‹å‰å‘è¡¨çš„è®ºæ–‡`[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)`ç±»ä¼¼ã€‚æˆ‘ä»¬å¯ä»¥æŠŠ`Mixup`çœ‹ä½œæµå½¢æ··æ­çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚

# æ··åˆ(æˆ–è¾“å…¥æ··åˆ)

æ··åˆå¯ä»¥é€šè¿‡ä»¥ä¸‹å…¬å¼å®ç°:

```
new_image = alpha * image_1 + (1-alpha) * image_2
new_target = alpha * target_1 + (1-alpha) * target_2
```

æˆ‘ä»¬æ­£åœ¨æ··åˆä¸¤ä¸ªå›¾åƒ(`image_1`å’Œ`image_2`)æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾åƒã€‚æˆ‘ä»¬ç›¸åº”åœ°æ›´æ–°ç›®æ ‡å€¼ã€‚

`alpha`å€¼æ˜¯ä»è´å¡”åˆ†å¸ƒä¸­å–æ ·çš„ï¼Œå¹¶ä¸”åœ¨èŒƒå›´`[0,1]`å†…ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä¸`alpha=0.4`æ··æ·†çš„ä¾‹å­ã€‚

![](img/947f99c3519a503370426ce7405ca8d9.png)

Mixup example

è¿™é‡Œ`image_1`çš„ç›®æ ‡æ˜¯`[1,0]`ï¼Œ`image_2`çš„ç›®æ ‡æ˜¯`[0,1]`ã€‚å› æ­¤ï¼Œæœ€ç»ˆç›®æ ‡å°†æ˜¯`[0.4, 0.6]`ã€‚

## å±¥è¡Œ

*   è·Ÿéšæœ¬ç¬”è®°æœ¬ä¸€èµ·[æ‰§è¡Œã€‚](https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb)
*   æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ä¸‹é¢çš„ä»£ç æ¥å®ç°å®ƒã€‚

```
def mixup_data(x, y, alpha=1.0):
    '''Returns mixed inputs, targets, and lambda
    Parameters
    ----------
    x: input data
    y: target
    alpha: value of alpha and beta in beta distribution 
    '''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1batch_size = x.size()[0]
    index = torch.randperm(batch_size) # shuffle indexmixed_x = lam * x + (1 - lam) * x[index, :] # mixup between original image order and shuffled image order
    y_a, y_b = y, y[index] # return target of both images order

    return mixed_x, y_a, y_b, lam
```

*   æˆ‘ä»¬è¿”å›`y_a`å’Œ`y_b`è€Œä¸æ˜¯`y_mix (alpha * y_a + (1-alpha)*y_b)`åŸå› æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°ä¸­è¿›è¡Œæ··åˆæ ‡ç­¾æ“ä½œï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…æ”¹å˜æŸå¤±å‡½æ•°ã€‚
*   ä¸‹é¢æ˜¯åœ¨æ··ä¹±ä¸­æŸå¤±çš„ä»£ç ã€‚

```
def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """ Updated loss for mixup.
    Args:
    -----
    criterion: loss function to use, example: crossentropy loss
    preds: predictions from network
    y_a: original labels
    y_b: labels of the shuffled batch
    lam: alpha used for mixup
    """
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)
```

*   æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬å–ä¸¤å¹…å›¾åƒ(`image_1`å’Œ`image_2`)å’Œå®ƒä»¬çš„ç›®æ ‡(`target_1`å’Œ`target_2`)ã€‚å°†æœ‰ä¸€ä¸ªæ–°çš„å›¾åƒ`new_image = alpha * image_1 + (1-alpha) * image_2`å’Œä¸€ä¸ªæ–°çš„ç›®æ ‡`new_target = alpha * target_1 + (1-alpha) * target_2`ã€‚
*   è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬é€šè¿‡æ¨¡å‹ä¼ é€’`new_image`ï¼Œå¹¶åœ¨ softmax ä¹‹åå¾—åˆ°`preds`ç»“æœå‘é‡ã€‚é‚£ä¹ˆæŸå¤±å¦‚ä¸‹:

![](img/cc79d4fffab961b6c418527f89d6efbf.png)

Loss with Mixup augmentation

*   æˆ‘ä»¬ä½¿ç”¨äº†æœ€åä¸€ä¸ªç­‰å¼æ¥è®¡ç®—æŸå¤±ï¼Œè€Œä¸æ˜¯ä¸Šå›¾ä¸­çš„ç¬¬ä¸€ä¸ªã€‚
*   ç±»ä¼¼äºæŸè€—ï¼Œæˆ‘ä»¬è®¡ç®—è¾“å…¥æ··åˆæ‰¹æ¬¡çš„å‡†ç¡®åº¦å¦‚ä¸‹:

```
def mixup_accuracy(metric, preds, y_a, y_b, lam):
    """
    Updated metric calculation:
    Args:
    -----
    metric: metric to use, example: accuracy
    preds: predictions from network
    y_a: original labels
    y_b: labels of the shuffled batch
    lam: alpha used for mixup
    """return lam * metric(preds, y_a) + (1 - lam) * metric(preds, y_b)
```

è®©æˆ‘ä»¬ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚

*   åˆ›å»ºä¸€ä¸ªæ”¯æŒæ··æ­å’Œä¼ ç»Ÿè®­ç»ƒçš„é—ªç”µæ¨¡å‹:

```
class Model(pl.LightningModule):
    """
    Lightning model
    """
    def __init__(self, model_name, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):
        """Model trainer class
        Parameters
        ----------
        model_name: Name of the timm model
        num_classes: number of classes in the dataset
        lr: learning rate
        max_iter: maximum iterations
        mix_up: use mixup augmentation or not
        alpha: alpha for beta distribution in mixup
        """
        super().__init__()
        # setup the model
        self.num_classes = num_classes
        self.model = timm.create_model(model_name=model_name, pretrained=True, num_classes=num_classes)
        # setup accuracy metric
        self.metric = torchmetrics.functional.accuracy
        # setup cross entropy loss function 
        self.loss = torch.nn.CrossEntropyLoss()
        self.lr = lr
        self.max_iter = max_iter
        self.mix_up = mix_up
        self.alpha = alphadef forward(self, x):
        return self.model(x)

    def shared_step(self, batch, batch_idx, is_train=False):
        x, y = batch
        if is_train and self.mix_up: # if mixup is true and train
            # prepare the mixup date
            x, y_a, y_b, lam = mixup_data(x, y, self.alpha)
            x, y_a, y_b = map(Variable, (x, y_a, y_b))
            # pass the new data through model
            logits = self(x)
            # calculate loss
            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)
            # calculate accuracy
            preds = torch.argmax(logits, dim=1)
            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)
        else: # if mixup is false or validation
            # no change in data, we padd the batch data as is
            # pass the data through model
            logits = self(x)
            # calculate loss
            loss = self.loss(logits, y)
            # calculate accuracy
            preds = torch.argmax(logits, dim=1)
            acc = self.metric(preds, y)

        return loss, acc

    def training_step(self, batch, batch_idx):
        loss, acc = self.shared_step(batch, batch_idx, is_train=True)
        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        loss, acc = self.shared_step(batch, batch_idx, is_train=False)
        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)

        return [optim], [scheduler]
```

**æ³¨æ„**:åœ¨ mixup ç­‰å¼ä¸­æåˆ°çš„ alpha å’Œ lightning model è®ºè¯ä¸­æåˆ°çš„ alpha(å§‘ä¸”ç§°ä¹‹ä¸º`arg_alpha`)å¯èƒ½ä¼šæ··æ·†ã€‚è¿™ä¸¤ä¸ªä¸ä¸€æ ·ã€‚æˆ‘ä»¬é€šè¿‡ä»`(-arg_alpha, arg_alpha)`ä¹‹é—´çš„ beta åˆ†å¸ƒä¸­é€‰æ‹©ä¸€ä¸ªéšæœºå€¼æ¥è·å¾— mixup æ–¹ç¨‹çš„ alphaã€‚æˆ‘ä»¬æ­£åœ¨ç”¨`mixup_data`å‡½æ•°åšè¿™ä»¶äº‹ã€‚

å…³äºä¸Šé¢çš„ lightning æ¨¡å‹ï¼Œæœ‰å‡ ç‚¹éœ€è¦æ³¨æ„:

*   åŸºäº`mixup`æ˜¯å¦å¯ç”¨ï¼Œæˆ‘ä»¬æ”¹å˜`shared_step`ä¸­çš„æ•°æ®å¤„ç†ã€åº¦é‡å’ŒæŸå¤±å‡½æ•°ã€‚
*   æˆ‘ä»¬åªåœ¨è®­ç»ƒæ­¥éª¤ä¸­åš`mixup`ï¼Œä¸ºäº†éªŒè¯ï¼Œæˆ‘ä»¬åšæ­£å¸¸å¤„ç†ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ CIFAR-10 æ•°æ®é›†ã€æ•°æ®åŠ è½½å™¨å’Œè½¬æ¢ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
# standard image transform for classifier
transform = transforms.Compose(
    [transforms.Resize(224),
     transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
# batch size, reduce if cuda out of memory (should work fine in colab with gpu)
batch_size = 128
# get cifar-10 train set
trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
# split train-full set 
# used trainset have 20000
trainset, trainset_remains = torch.utils.data.random_split(trainset_full, [20000, len(trainset_full)-20000])
# create train dataloader
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)
# val dataloader
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)
# classes in cifar 10
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 
```

å®šä¹‰æ•™ç»ƒå’Œæ£€æŸ¥ç‚¹å›å«ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨`Model`ä¸­çš„`mix_up`å‚æ•°è¿›è¡Œæœ‰æ— æ··æ·†çš„è®­ç»ƒ

```
model = Model(model_name="resnet18", num_classes=len(classes), lr = 0.001, max_iter=20, mix_up=False)
checkpoint_callback = ModelCheckpoint(
    monitor='val_loss',
    dirpath='./checkpoints',
    filename='resnet_18_org-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'
)
trainer = Trainer(
    deterministic=True, 
    logger=True, 
    callbacks=[checkpoint_callback], 
    gpus=[0], # change it based on gpu or cpu availability
    max_epochs=5)
```

è®­ç»ƒåˆ†ç±»å™¨:

```
trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=testloader)
```

# æµå½¢æ··åˆ

*   æµå½¢æ··åˆè¿›ä¸€æ­¥å°†æ··åˆæ€æƒ³æ‰©å±•åˆ°éšè—å±‚å’Œè¾“å…¥å±‚ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰æµå½¢æ··åˆå¦‚ä¸‹ã€‚

```
new_input = alpha * input_1 + (1-alpha) * input_2
new_target = alpha * target_1 + (1-alpha) * target_2
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»å°†`image_1`æ›´æ”¹ä¸º`input_1`ã€`image_2`æ›´æ”¹ä¸º`input_2`å¹¶å°†`new_image`æ›´æ”¹ä¸º`new_input`ã€‚è¿™é‡Œï¼Œå½“æˆ‘ä»¬è¯´è¾“å…¥æ—¶ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•æ·±å±‚ç¥ç»ç½‘ç»œå±‚è¾“å…¥ã€‚å½“è¯¥å±‚æ˜¯ç¬¬ä¸€å±‚æ—¶ï¼Œåˆ™`input`å°†æ˜¯`image`ã€‚

*   æˆ‘ä»¬éšæœºé€‰æ‹©è¿™ä¸€å±‚ã€‚
*   æˆ‘ä»¬æ‹æ‘„ä¸¤å¼ å›¾åƒï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œä¼ é€’å®ƒä»¬ï¼Œç›´åˆ°æˆ‘ä»¬åˆ°è¾¾é‚£ä¸€å±‚ã€‚
*   æˆ‘ä»¬å–å‡ºä¸­é—´ç‰¹å¾è¡¨ç¤º(`image_1`çš„`input_1`å’Œ`image_2`çš„`input_2`)ã€‚
*   æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢æåˆ°çš„ç­‰å¼å°†å®ƒä»¬æ··åˆèµ·æ¥ï¼Œä»¥è·å¾—æ–°çš„è¡¨ç¤º(`new_input`ã€`new_target`)ã€‚
*   å¯¹äºå…·æœ‰æ··åˆæ•°æ®çš„å…¶ä½™å±‚ï¼Œæˆ‘ä»¬ç»§ç»­åœ¨ç½‘ç»œä¸­å‘å‰ä¼ é€’ã€‚
*   æ··åˆæ•°æ®çš„è¾“å‡ºç”¨äºè®¡ç®—æŸè€—å’Œæ¢¯åº¦ã€‚

## å±¥è¡Œ

*   æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä¿®æ”¹è¿‡çš„`resnet18`æ¨¡å‹ï¼Œå®ƒæ”¯æŒæµå½¢æ··åˆã€‚
*   é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹ timm åº“æä¾›çš„åŸºæœ¬ resnet18 æ¨¡å‹ã€‚

```
model = timm.create_model(model_name=model_name, pretrained=pretrained, num_classes=classes)
```

*   æˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€äº›å›¾å±‚æ¥è¿›è¡Œæ··éŸ³ã€‚resnet18 æ¶æ„æœ‰ 4 å±‚ï¼Œç±»å‹ä¸º`nn.Sequential`ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨æ­¤åŸºç¡€ä¸Šæ‹†åˆ†æ¨¡å‹ã€‚

```
def _model_setup(model):
        model_list = []
        count=0
        d = [] # start and end index of the layer blocks
        start_index = 0
        for index, layer in enumerate(model.children()): # check all the layers of the model
            count+=1
            if isinstance(layer, nn.Sequential): # if it is nn.Sequential then update the list d with start and end index
                d.append((start_index, count-1)) 
                start_index = count-1

        d.append((start_index, len(list(model.children())))) # append any remaining layers
        module_blocks = [ 
                nn.Sequential(*list(model.children())[index[0]: index[1]]) for index in d
        ] # insert each module blocks into a list, blocks are created based on the start and end index of each block

        return nn.ModuleList(module_blocks) # return the list as ModuleList
```

*   æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä¸­é—´è½¬å‘å‡½æ•°ï¼Œå®ƒå°†å¼€å§‹å’Œç»“æŸç´¢å¼•ä¸è¾“å…¥ä¸€èµ·æ¥å—ã€‚å®ƒå°†åªé€šè¿‡èµ·å§‹å’Œç»“æŸç´¢å¼•å†…çš„æ¨¡å‹å—ä¼ é€’è¾“å…¥ã€‚

```
def _forward(self, x, i=0, j=None):
    assert i>=0 # make sure start index is >=0
    assert j is None or j<=len(self.model_list)-1 # max end index is num of blocks - 1
    assert j is None or i<=j # start index is < the end index
    if j is None: # if j is None pass till the end block
        j = len(self.model_list) for model_layer in self.model_list[i:j]:
        x = model_layer(x) return x
```

*   ç°åœ¨æˆ‘ä»¬å°†åˆ›å»ºè½¬å‘å‡½æ•°ã€‚

```
def forward(self, x, mixup=False):
    index = None
    lam = None
    if mixup:
        k = np.random.randint(0, self.num_model) # select a random intermediate layer to mixup the o/p
        batch_size = x.size()[0]
        index = torch.randperm(batch_size) # shuffle index
        lam = np.random.beta(self.alpha, self.alpha) # select alpha randomly 
        if hasattr(self, "log"): # logging
            self.log("k", k, on_step=True, on_epoch=False, logger=True, prog_bar=True)
            self.log("lam", lam, on_step=True, on_epoch=False, logger=True, prog_bar=True)
        op_int = lam * self._forward(x, i=0, j=k) + (1 - lam) * self._forward(x[index, :], i=0, j=k) # mixup the op of k the layer
        op = self._forward(op_int, i=k, j=None) # pass the mixup remaining layers
    else: # if not mixup pass through all model blocks
        op = self._forward(x, i=0, j=None) return op, index, lam # return model output, shuffle order, lambda 
```

*   æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰çš„éƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬çš„`resnet18`æ¨¡å‹å°†ä¼šæ˜¯

*   è®­ç»ƒæ—¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Resnet18MM`è·Ÿéšæ–¹å¼

å› æ­¤ï¼Œç”¨äºæµå½¢æ··åˆçš„é—ªç”µè®­ç»ƒå™¨å¯ä»¥å¦‚ä¸‹å®ç°

```
class ModelMM(pl.LightningModule):
    """
    Lightning model
    """
    def __init__(self, mixup_model, num_classes, lr = 0.001, max_iter=20, mix_up=False, alpha=1):
        """Model trainer class for manifold mixup
        Parameters
        ----------
        mixup_model: mixup model
        num_classes: number of classes in the dataset
        lr: learning rate
        max_iter: maximum iterations
        mix_up: use mixup augmentation or not
        alpha: alpha for beta distribution in mixup
        """
        super().__init__()
        # setup the model
        self.num_classes = num_classes
        self.model = mixup_model
        # setup accuracy metric
        self.metric = torchmetrics.functional.accuracy
        # setup cross entropy loss function 
        self.loss = torch.nn.CrossEntropyLoss()
        self.lr = lr
        self.max_iter = max_iter
        self.mix_up = mix_up
        self.alpha = alphadef forward(self, x, mixup=False):
        return self.model(x, mixup)

    def shared_step(self, batch, batch_idx, is_train=False):
        x, y = batch
        if is_train and self.mix_up:
            logits, index, lam = self(x, True)
            y_a, y_b = y, y[index]# # x, y_a, y_b, lam = mixup_data(x, y, self.alpha)
            # # x, y_a, y_b = map(Variable, (x, y_a, y_b))
            # logits = self(x)
            loss = mixup_criterion(self.loss, logits, y_a, y_b, lam)
            preds = torch.argmax(logits, dim=1)
            acc = mixup_accuracy(self.metric, preds, y_a, y_b, lam)
        else:
            logits, _, _ = self(x, False)
            loss = self.loss(logits, y)
            preds = torch.argmax(logits, dim=1)
            acc = self.metric(preds, y)

        return loss, acc

    def training_step(self, batch, batch_idx):
        loss, acc = self.shared_step(batch, batch_idx, is_train=True)
        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('train_acc', acc, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        loss, acc = self.shared_step(batch, batch_idx, is_train=False)
        self.log('val_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)
        self.log('val_acc', acc, on_epoch=True, logger=True, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optim, T_max=self.max_iter)

        return [optim], [scheduler]
```

*   å¯¹äºæµå½¢æ··åˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ã€æ•°æ®åŠ è½½å™¨å’Œè½¬æ¢ã€‚

# ç»“æœ

ä¸‹è¡¨æ˜¾ç¤ºäº†åœ¨ CIFAR-10 å’Œ CIFAR-100 æ•°æ®é›†ä¸Šæ··åˆ(è¾“å…¥æ··åˆ)å’Œæµå½¢æ··åˆçš„ç»“æœã€‚äº”æ¬¡é‡å¤çš„æ ‡å‡†åå·®ã€‚

![](img/b4db5606d98b54972b592a41c7992178.png)

Performance of Mixup augmentations from the paper

æµå½¢æ··åˆæ”¹è¿›äº†å¤šå±‚ç¥ç»ç½‘ç»œçš„éšè—è¡¨ç¤ºå’Œå†³ç­–è¾¹ç•Œã€‚è¿™è§£å†³äº†åˆ†å¸ƒå˜åŒ–ã€å¼‚å¸¸å€¼å’Œå¯¹ç«‹ä¾‹å­ç­‰é—®é¢˜ã€‚

æˆ‘ä¸ªäººå–œæ¬¢ä½¿ç”¨è¿™ç§å¢å¼ºï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºåˆ›å»ºå¥å£®çš„æ¨¡å‹ã€‚å¸Œæœ›è¿™å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚å–œæ¬¢å°±æ‹æ‹æ–‡ç« ï¼Œå–œæ¬¢å°±å…³æ³¨æˆ‘ã€‚è¿‡å¾—æ„‰å¿«ğŸ˜ƒã€‚

èµ„æº:

*   å®æ–½ç¬”è®°æœ¬:[è¿™é‡Œ](https://github.com/souvik3333/medium_blogs/blob/main/transforms/manifold_mixup.ipynb)

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)