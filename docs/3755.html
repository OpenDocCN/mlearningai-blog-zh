<html>
<head>
<title>Why are Data Scientists obsessed with PySpark over Pandas — A Truth of Data Science Industry</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么数据科学家痴迷PySpark而不是熊猫——数据科学行业的一个真相</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/why-are-data-scientists-obsessed-with-pyspark-over-pandas-a-truth-of-data-science-industry-87c708307c62?source=collection_archive---------0-----------------------#2022-10-17">https://medium.com/mlearning-ai/why-are-data-scientists-obsessed-with-pyspark-over-pandas-a-truth-of-data-science-industry-87c708307c62?source=collection_archive---------0-----------------------#2022-10-17</a></blockquote><div><div class="dt gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ew ey ig ih ii ij es et paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="es et if"><img src="../Images/b11007479aa6f7b7c6a5e7ddc70f54f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71TTPkOJaZrLAD9z4EEoDw.jpeg"/></div></div><figcaption class="iq ir eu es et is it bd b be z dy">Life of Data Scientist — CSV</figcaption></figure><p id="b554" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">近年来，我们已经看到了数据的增长，因此，它也增加了计算时间和内存。在大型数据集的情况下，像Pandas这样按顺序工作的工具无法在所需的时间内获得结果。</p><p id="47d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有一些软件包可以并行处理pandas操作，如Dask、Swift、Ray等。对pandas进程进行并行处理可以大大提高速度，但是当Pandas将数据帧加载到内存中时，系统会对内存造成一定的限制，即使在特定情况下并不需要这样做。对于桌面系统来说，这可能是一个大问题，因为我们需要保持UI活动。</p><p id="814b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们需要一个框架来解决上述问题，并在给定的内存限制下实现并行化。在一些给定的阈值下，Spark解决了其中的一些问题。它能够实现更快的处理速度，并使用一种称为惰性评估的概念(顾名思义，它在需要时评估数据)，这解决了由内存引起的一些限制。</p><h1 id="5bb5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">PySpark相对于熊猫的优势</h1><ol class=""><li id="4355" class="kr ks hi iw b ix kt jb ku jf kv jj kw jn kx jr ky kz la lb bi translated">Pandas在一台机器上执行操作，而PySpark在多台机器上执行操作，这使得它在大数据集上比Pandas快<strong class="iw hj">100倍** </strong>。</li><li id="b6ab" class="kr ks hi iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">熊猫坚持渴望执行，这意味着任务尽快完成，而Pyspark遵循<strong class="iw hj">懒惰执行</strong>，这意味着任务直到一个动作被执行才被执行。</li><li id="e2ab" class="kr ks hi iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">Pandas数据框架无法构建可伸缩的应用程序，但是PySpark数据框架是开发可伸缩应用程序的理想选择。</li><li id="07d8" class="kr ks hi iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">Pandas数据帧不保证容错，但PySpark <strong class="iw hj">数据帧保证容错。</strong></li></ol><h1 id="13c4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">什么时候用PySpark或者熊猫？</h1><p id="c588" class="pw-post-body-paragraph iu iv hi iw b ix kt iz ja jb ku jd je jf lh jh ji jj li jl jm jn lj jp jq jr hb bi translated">正如我前面提到的，PySpark比熊猫快100倍，这只是半个事实。Pandas和Pyspark对于最初的GB数据有相同的运行时间，如下面的基准测试所示。</p><p id="72fd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于最初的20 GB，它们具有相同的斜率，但是随着文件大小的增加，Pandas耗尽了内存，PySpark能够成功地完成这项工作。</p><figure class="ll lm ln lo fe ij es et paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="es et lk"><img src="../Images/62e37d498452d80ab2120c9db4af7d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*COF6YRlPAPQFeeG5.png"/></div></div><figcaption class="iq ir eu es et is it bd b be z dy">Benchmark Pandas and PySpark (<a class="ae js" href="https://hirazone.medium.com/benchmarking-pandas-vs-spark-7f7166984de2" rel="noopener">Source</a>)</figcaption></figure><p id="5893" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，对于10–12gb的小数据集，由于相同的运行时间和较低的复杂性，您可能更喜欢Pandas而不是PySpark，此外，您必须使用PySpark。</p><h1 id="95fb" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">PySpark的局限性</h1><p id="f356" class="pw-post-body-paragraph iu iv hi iw b ix kt iz ja jb ku jd je jf lh jh ji jj li jl jm jn lj jp jq jr hb bi translated">凡是擅长某一方面的东西，在其他方面都落后了。PySpark也是如此。Pyspark的一些限制是:</p><ol class=""><li id="9c9c" class="kr ks hi iw b ix iy jb jc jf lp jj lq jn lr jr ky kz la lb bi translated">PySpark延迟较高，导致吞吐量较低。</li><li id="a061" class="kr ks hi iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">对内存的消耗非常大。</li><li id="58c9" class="kr ks hi iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">为PySpark开发的算法和库更少。</li></ol><p id="373d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的讨论中，我们可以得出结论，随着数据的增加，对更多像PySpark这样的框架的需求也在增加，这些框架可以轻松处理这样的过程，如果你是一名有抱负的数据科学家，你应该开始了解PySpark。</p><h2 id="bc0d" class="ls ju hi bd jv lt lu lv jz lw lx ly kd jf lz ma kh jj mb mc kl jn md me kp mf bi translated"><a class="ae js" href="https://mlearning.substack.com/about" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">成为MLearning.ai </strong>的作家T3】</a></h2><div class="mg mh fa fc mi mj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mk ab dx"><div class="ml ab mm cl cj mn"><h2 class="bd hj fj z dz mo eb ec mp ee eg hh bi translated">Mlearning.ai提交建议</h2><div class="mq l"><h3 class="bd b fj z dz mo eb ec mp ee eg dy translated">如何成为Mlearning.ai上的作家</h3></div><div class="mr l"><p class="bd b fq z dz mo eb ec mp ee eg dy translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx io mj"/></div></div></a></div></div></div>    
</body>
</html>