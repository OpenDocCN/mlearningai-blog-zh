<html>
<head>
<title>Multiclass Logistic Regression With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现多类逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/multiclass-logistic-regression-with-python-2ee861d5772a?source=collection_archive---------0-----------------------#2022-03-04">https://medium.com/mlearning-ai/multiclass-logistic-regression-with-python-2ee861d5772a?source=collection_archive---------0-----------------------#2022-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="de92" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将我们的知识从二项逻辑回归扩展到多项逻辑回归</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/00395ea60ef3f3633c2796d08cca60b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XdsFOSwOqZHqigptkCDkkA.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@federicoscarionati?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Federico Scarionati</a> on <a class="ae js" href="https://unsplash.com/s/photos/multiple-choice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="90df" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">多类逻辑回归</h1><p id="cceb" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">多类逻辑回归也称为多项逻辑回归。与<a class="ae js" href="https://python.plainenglish.io/logistic-regression-a-classifier-with-a-sense-of-regression-83ce49ba3f5b" rel="noopener ugc nofollow" target="_blank">二项式逻辑回归</a>相反，多类逻辑回归用于将输出标签分为两类以上。</p><p id="1603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在多类逻辑回归的情况下，我们用<a class="ae js" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax函数</a>替换sigmoid函数:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/68dec5f6c960f2b56ccb55e979fede6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*LO5mTZqPl94ksFZN0YMthg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation.1 Softmax Function. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="d0c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将y定义为</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es le"><img src="../Images/3883e9dc891a8517feba790edc77608c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*WxLtxtFFE-6fqZvr5Zkk5A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 2 Softmax input y. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="ed0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，给定权重和净输入y(i)，该softmax函数计算特征x(i)属于类<em class="lf"> j. </em>的概率。因此，我们计算<em class="lf"> j = 1，…，k </em>中每个类标签的概率φ。请注意分母中的归一化项，它导致这些类概率的总和为1。</p><h2 id="097b" class="lg kb hh bd kc lh li lj kg lk ll lm kk ip ln lo ko it lp lq ks ix lr ls kw lt bi translated">梯度下降</h2><p id="c635" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">现在，为了通过梯度下降训练我们的逻辑模型，我们需要定义一个我们希望最小化的成本函数<em class="lf"> J </em>:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lu"><img src="../Images/254a6dc7f388232655ed41e0e9120c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*tvrXAk3vSdT2q_F87qZN2w.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 3 cost function. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="7874" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中H是交叉熵函数，定义如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/1975c777bb4c95847b82a07ee2891321.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*RpNNnOQHu0FXv_mpjShAcA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 4 cross-entropy. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="a79f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，y代表已知标签，φ代表通过softmax计算的概率；<em class="lf">不是</em>预测的类标签。</p><p id="b9f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了找到最佳权重，我们需要成本函数的梯度</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/6530c6e811346b7bd4bb39908b88323a.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*xq8JFCDEp0bG0KrLoIlo9Q.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 5 gradient of the cost function. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="64cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">或者以矩阵形式:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/da1d5f31cd4d1d98e8c655ac2c44eed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*ikoSA9EIbZ6UrzBXWMiyfg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 6 gradient of the cost function. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="cc4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在哪里</p><p id="5114" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">θ=成本函数的梯度</p><p id="691c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">x =特征矩阵</p><p id="405e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">y =已知标签的向量</p><p id="b84a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">φ=未知标签的概率向量</p><p id="1076" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以在成本函数中加入一个L2正则项</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/11dcff1ab1509f574aa34c801bd8d7b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*nPVXG2uic98GeH1zDFrsiQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 6 gradient of the cost function with l2 regularization. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="a7c5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中:</p><p id="4e1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">μ=正则化因子</p><p id="1988" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">w =权重矩阵</p><p id="6c64" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，权重矩阵的更新步骤写成:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lz"><img src="../Images/9fb5066205eb50795159eb4c358eed8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*QNiOf3sY4Kz1MH-OQTlYxg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Equation. 7 Updating steps of weights. Image by <strong class="bd kc">the Author.</strong></figcaption></figure><p id="bb55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，α是学习率。注意，w是类别的权重向量<em class="lf"> y=j </em></p><h2 id="a41d" class="lg kb hh bd kc lh li lj kg lk ll lm kk ip ln lo ko it lp lq ks ix lr ls kw lt bi translated">履行</h2><p id="3591" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">现在我们将使用Python构建逻辑回归。导入必需品模块</p><pre class="jd je jf jg fd ma mb mc md aw me bi"><span id="e071" class="lg kb hh mb b fi mf mg l mh mi">from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.datasets import load_iris<br/>import numpy as np<br/>import pandas as pd<br/>from scipy.special import softmax<br/>from sklearn.model_selection import train_test_split</span></pre><p id="393c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后将该类定义为</p><pre class="jd je jf jg fd ma mb mc md aw me bi"><span id="65d6" class="lg kb hh mb b fi mf mg l mh mi">class MultipleLogRegression:<br/>    <br/>    def __init__(self,learning_rate=0.1,n_iters=1000):<br/>        self.lr=learning_rate<br/>        self.iters=n_iters<br/>        self.W=None<br/>        <br/>    def fit(self,X,y,mu):<br/>        ones=np.ones(X.shape[0])<br/>        features=np.c_[ones,X]<br/>        onehot_encoder = OneHotEncoder(sparse=False)<br/>        y_encode=onehot_encoder.fit_transform(y.reshape(-1,1))<br/>        self.W=np.zeros((features.shape[1], y_encode.shape[1]))<br/>        samples=X.shape[0]<br/>        <br/>        for i in range(self.iters):<br/>            Z=-features@self.W<br/>            prob_y=softmax(Z,axis=1)<br/>            error=y_encode-prob_y<br/>            dW=1/samples * (features.T @ error) + 2 * mu * self.W<br/>            self.W-=self.lr*dW<br/>            <br/>    def predict(self,X):<br/>        ones=np.ones(X.shape[0])<br/>        features=np.c_[ones,X]<br/>        Z=-features@self.W<br/>        y=softmax(Z,axis=1)<br/>        return np.argmax(y,axis=1)</span></pre><p id="956b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中:</p><ol class=""><li id="146d" class="mj mk hh ig b ih ii il im ip ml it mm ix mn jb mo mp mq mr bi translated"><code class="du ms mt mu mb b">__init__</code>是标准的构造器方法，以<code class="du ms mt mu mb b">learning_rate</code>和<code class="du ms mt mu mb b">n_iters</code>作为梯度下降中的学习速率和迭代次数。</li><li id="0634" class="mj mk hh ig b ih mv il mw ip mx it my ix mz jb mo mp mq mr bi translated"><code class="du ms mt mu mb b">fit</code>是拟合方法。这是梯度下降将运行的地方。首先，我们将1的向量的一列添加到特征矩阵<code class="du ms mt mu mb b">X</code>中。然后使用<code class="du ms mt mu mb b">sklearn</code>的<code class="du ms mt mu mb b">OneHotEncoder</code>模块的<code class="du ms mt mu mb b">fit_transform</code>将已知标签<code class="du ms mt mu mb b">y</code>编码成相应的数值类。然后在迭代中，我们使用来自<code class="du ms mt mu mb b">scipy</code>模块的<code class="du ms mt mu mb b">softmax</code>函数计算标签的概率。然后计算将在权重矩阵<code class="du ms mt mu mb b">W</code>的更新步骤中使用的成本函数<code class="du ms mt mu mb b">dW</code>的梯度</li><li id="3809" class="mj mk hh ig b ih mv il mw ip mx it my ix mz jb mo mp mq mr bi translated"><code class="du ms mt mu mb b">predict</code>是预测法。使用<code class="du ms mt mu mb b">softmax</code>函数从特征矩阵<code class="du ms mt mu mb b">X</code>简单计算标签的概率向量。然后使用<code class="du ms mt mu mb b">np.argmax</code>确定最大概率</li></ol><h2 id="066d" class="lg kb hh bd kc lh li lj kg lk ll lm kk ip ln lo ko it lp lq ks ix lr ls kw lt bi translated">结论</h2><p id="20a7" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">在本文中，我们了解到:</p><ol class=""><li id="3aee" class="mj mk hh ig b ih ii il im ip ml it mm ix mn jb mo mp mq mr bi translated">多类逻辑回归作为多类标签的机器学习分类器算法。</li><li id="69fd" class="mj mk hh ig b ih mv il mw ip mx it my ix mz jb mo mp mq mr bi translated">使用softmax函数确定输出标签的概率</li><li id="557d" class="mj mk hh ig b ih mv il mw ip mx it my ix mz jb mo mp mq mr bi translated">在多类逻辑回归中实现梯度下降。</li></ol><div class="na nb ez fb nc nd"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hi fi z dy ni ea eb nj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jm nd"/></div></div></a></div></div></div>    
</body>
</html>