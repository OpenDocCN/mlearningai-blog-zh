# 基于神经网络的依存句法分析

> 原文：<https://medium.com/mlearning-ai/dependency-parsing-with-neural-networks-e36f5166628d?source=collection_archive---------1----------------------->

> 什么是依存句法分析，如何用神经网络分析句子？

![](img/040c1965bf57f821a260c85a61ab3ba2.png)

Photo by [Djordje Vukojicic](https://unsplash.com/@djordjevukojicic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

本文是陈在斯坦福大学攻读博士期间的论文摘要，可以在这里直接阅读。

## 依存句法分析

首先，依存句法分析接受一个句子，并产生一组从**中心**到**从属**的有向带标签的弧线。但是什么是首长和家属呢？直觉上，依存解析器想要识别句子中的关键概念。以此行为例:

我爱你。

这里，关键概念是描述“爱”的行为或状态:某人爱某物或某人。“爱”这个词是一个头，而“我”和“你”是修饰的从属词，或者换句话说，指明了爱的行动。让我们以另一个稍微复杂的句子为例:

![](img/931732f1eecce211d3e171e62fedebc5.png)

[https://web.stanford.edu/~jurafsky/slp3/14.pdf](https://web.stanford.edu/~jurafsky/slp3/14.pdf)

“偏好”是这个句子中的关键概念，这里主语“我”偏好具有某些属性的飞行。所有其他的词仅仅是说明这是什么样的航班。在这个例子中，单词‘prefer’是依存解析中的**词根**，指的是所有头的头。

依存解析中的一些规则:

*   只有人头指向家眷。
*   这些标签描述了从属对其头部意味着什么。
*   这被称为**类型依赖结构**，因为类型依赖标签是从固定的语法关系清单中抽取的。

![](img/12f94294538735d78f94fe49b2eba5e8.png)

Some of the Universal Dependency relations ([de Marneffe et al., 2014](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf))

## 传统解析器的问题

该论文引用了三个主要警告:

> 1.从统计学的角度来看，这些解析器使用了数百万个估计很差的特征权重。
> 
> 2.几乎所有现有的解析器都依赖于手工设计的一组特征模板，这需要大量的专业知识，并且通常是不完整的。
> 
> 3.许多特征模板的使用导致了一个很少被研究的问题:在现代依存解析器中，大部分运行时间不是被核心解析算法消耗，而是在特征提取步骤中消耗。

现在，这些问题的解决方案显而易见:使用密集要素制图表达代替稀疏指示要素。(例如 word2vec)

在本文中，作者训练了一个神经网络分类器，在一个基于变迁的依存句法分析器中做出句法分析决策。神经网络学习单词、词性(POS)标签和依赖性标签的紧凑密集向量表示。这产生了快速、紧凑的分类器，其仅使用 200 个学习的密集特征，同时在两种语言(英语和汉语)和两种不同的依存关系表示(CoNLL 和 Stanford 依存关系)上产生解析准确性和速度的良好增益。

## 基于转换的依存句法分析

正如我们之前看到的，依存解析的输出是一系列有向的、有标签的弧，从头到从属。生成这种序列的过程由过渡组成。在我们继续之前，让我们稍微思考一下“过渡”这个词。

本文采用了 **arc-standard** 系统，其中基于转换的依存分析旨在预测从初始配置到某个终端配置的转换序列。配置如下所示:

> **配置 c= (s，b，A)，**

其中 s 代表堆栈，b 代表缓冲区，A 代表弧集。堆栈记录了迄今为止尚未决定的内容，缓冲区记录了我们的算法根本没有考虑的内容，弧线集记录了预测，即箭头如何在从词根开始的句子中的单词之间跳跃。

![](img/1e3a72088f1bf84a5280ab505c55a934.png)

[https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf](https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf)

上面的图表显示了一个 arc 标准系统是如何处理“他控制得很好”这句话的。我们将对此进行研究，以理解过渡意味着什么。

*   **移位:将单词按顺序移入堆栈以供考虑。**

我们从初始配置开始:一个只有词根的空栈，一个有整句的满缓冲区，以及空集 a，从' he '开始，我们顺序地将单词移入栈，然后思考是否可以得出一个合理的依存弧。请记住:堆栈中的字是我们所查看的唯一的字。如果我们不能提出一个依赖弧，我们就把一个字从缓冲区转移到堆栈。

*   **左弧:堆栈中最右边的两个单词有一个依赖弧。**

当我们在堆栈中有[ROOT he has]时，对于最右边的单词:“He”和“has”，我们知道“He”是指定“has”的单词，这是这个例句的关键概念。意思是‘has’是**头**，而‘He’是**从属**。然后我们可以想出下一个过渡:左弧，为了从头部“has”指向从属“he ”,我们需要一个左弧。然后我们从堆栈中移除依赖项。对于 CS 的人来说，开始时通常会不舒服，因为堆栈数据结构通常以 FIFO(先进先出)的方式执行，但这里不是。不管它是最右边的还是第二个最右边的单词，我们都要去掉它。

*   **右弧:与左弧相似，但方向不同。**

以[ROOT has control]为例，对于最右边的两个单词:“has”和“control”，我们知道“control”是 head“has”的依赖项。因此，我们提出了右弧转换(因为它在堆栈中从左词指向右词)。与 left-arc 的唯一区别是，这次我们删除了 stack 中最右边的单词“control”。同样，我们选择删除哪个单词只是通过判断它是否是一个依存词。

*   当堆栈中只剩下根时停止。

可以肯定地说，最后一个过渡总是右弧，因为堆栈中的最后两个单词是词根和句子中一个单词的组合。

## 基于神经网络的分析器

这里的神经网络在“根据堆栈中最右边的两个单词得出依赖弧”过程中充当大脑。它接受单词的密集表示向量，并以 softmax 方式预测转换。

现在我们都知道深度学习世界中的全局感知场有多强大(Transformer architecture 的注意力机制就是其中之一)，不太可能像之前讨论的那样，我们仍然只看堆栈中最右边的两个单词。

在本文中，作者使用单词/词性标签/弧标签的级联嵌入作为输入。训练集是使用“最短堆栈”oracle 生成的，它总是更喜欢左弧而不是 SHIFT。细节在这里并不重要，但是我们应该知道，对于每一个可能的配置，都有一个基本事实转换。我们以监督的方式训练我们的神经网络分析器。

我不认为以下列表中的任何实现是至关重要的，但这里有一些在本文中作为查找应用的培训细节:

*   立方体激活函数
*   选择堆栈和缓冲区中的前 3 个单词
*   辍学的小批量 AdaGrad
*   选择未标记附件分数最高的参数

## 解析期间的贪婪解码

在这个阶段它是相当直截了当的。给定一个配置，我们首先提取所选单词/标记/标签的表示，然后将其用作神经网络的输入来预测下一次转换，最后基于预测生成新的配置。值得一提的是，作者对最常见的 10000 个单词进行了预计算，以加快解析过程。

## 参考资料:

*   https://web.stanford.edu/~jurafsky/slp3/14.pdf
*   [http://www . lrec-conf . org/proceedings/lrec 2014/pdf/1062 _ paper . pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf)