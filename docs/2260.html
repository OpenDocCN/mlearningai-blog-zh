<html>
<head>
<title>Neural Network For Beginners 🧠</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络初学者🧠</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/neural-network-for-beginners-accb6fc1d1ab?source=collection_archive---------2-----------------------#2022-04-03">https://medium.com/mlearning-ai/neural-network-for-beginners-accb6fc1d1ab?source=collection_archive---------2-----------------------#2022-04-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="4227" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">第4部分:不同类型的神经元连接结构和处理图像数据集。</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/624c0c1be9d2c003242fe14e3b338c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dPsH6ezdnAmsnQuj.jpg"/></div></div></figure><p id="8e6a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">欢迎回到这个面向初学者的Pytorch和神经网络系列的第4部分。如果你正在阅读这篇文章，并且不知道这个系列的其他部分，我希望你能看看下面的链接</p><div class="ke kf ez fb kg kh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/pytorch-for-beginners-62c3fcd75f69"><div class="ki ab dw"><div class="kj ab kk cl cj kl"><h2 class="bd hi fi z dy km ea eb kn ed ef hg bi translated">Pytorch适合初学者💫</h2><div class="ko l"><h3 class="bd b fi z dy km ea eb kn ed ef dx translated">第一部分:张量的基本运算</h3></div><div class="kp l"><p class="bd b fp z dy km ea eb kn ed ef dx translated">medium.com</p></div></div><div class="kq l"><div class="kr l ks kt ku kq kv jg kh"/></div></div></a></div><div class="ke kf ez fb kg kh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/pytorch-for-beginners-d759cb85ff1a"><div class="ki ab dw"><div class="kj ab kk cl cj kl"><h2 class="bd hi fi z dy km ea eb kn ed ef hg bi translated">Pytorch适合初学者💫</h2><div class="ko l"><h3 class="bd b fi z dy km ea eb kn ed ef dx translated">第2部分:神经网络基础及其从头实现</h3></div><div class="kp l"><p class="bd b fp z dy km ea eb kn ed ef dx translated">medium.com</p></div></div><div class="kq l"><div class="kw l ks kt ku kq kv jg kh"/></div></div></a></div><div class="ke kf ez fb kg kh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/pytorch-for-beginners-efb19e529dc3"><div class="ki ab dw"><div class="kj ab kk cl cj kl"><h2 class="bd hi fi z dy km ea eb kn ed ef hg bi translated">Beginners✨火炬报</h2><div class="ko l"><h3 class="bd b fi z dy km ea eb kn ed ef dx translated">第3部分:使用Pytorch内置方法构建神经网络</h3></div><div class="kp l"><p class="bd b fp z dy km ea eb kn ed ef dx translated">medium.com</p></div></div><div class="kq l"><div class="kr l ks kt ku kq kv jg kh"/></div></div></a></div><p id="f9bf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们已经完成了张量的基础知识、神经网络的基础知识，并从头开始使用PyTorch内置库实现了神经网络。</p><p id="d4dc" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在我们将讨论人工神经网络中不同的神经元连接架构，以及如何在Pytorch上实现它们。</p><p id="3a61" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在本文中，我们将只讨论<strong class="jk hi">单层和多层</strong>,因为我的主要动机是向您解释神经网络的结构，但对于其他架构，我们将只进行概述，因为我将在接下来的文章中非常详细地向您解释它们。</p><p id="dbac" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">存在五种基本类型的神经元连接结构:</p><ol class=""><li id="9bf2" class="kx ky hh jk b jl jm jo jp jr kz jv la jz lb kd lc ld le lf bi translated">单层前馈网络</li><li id="0788" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">多层前馈网络</li><li id="255e" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">具有自身反馈的单个节点</li><li id="2d85" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">单层递归网络</li><li id="c186" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">多层递归网络</li></ol><p id="910c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在本文中，我们将使用CIFAR-10数据集，它包含10类60000幅32x32的彩色图像，每类6000幅图像。有50000个训练图像和10000个测试图像。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ll"><img src="../Images/949b3078b31c354f78fb0df179a313c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I6ufAo-LC8s-GorY.jpg"/></div></div></figure><p id="23a1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我会考虑到你们以前从未处理过图像数据集。让我简单地告诉你，我们将如何把这些数据作为神经网络的输入。</p><p id="8075" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">图像是一组具有确定形状、大小和通道的像素。考虑来自MNIST数据集中的图像，该图像包含手写数字的图像，其中每个图像是具有灰度通道的28×28的形状</p><p id="7dbf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里，我们有一个数字5的图像。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lm"><img src="../Images/dbc59b9a3e5303f6d3ae214297503e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*S06wGTe8cnKt7yPP.png"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ln"><img src="../Images/19ec25ce94f9edfd14fa252dc0644851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vkcLpfiynMLMzFaq.png"/></div></div></figure><p id="8cbf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里，图像的大小为28高，28宽，只有一个通道，因此每个像素的值可以达到0到255，其中0表示黑色，255表示白色。值可以在0到255之间变化。</p><blockquote class="lo lp lq"><p id="4800" class="ji jj lr jk b jl jm ii jn jo jp il jq ls js jt ju lt jw jx jy lu ka kb kc kd ha bi translated"><strong class="jk hi"> <em class="hh">所以用简单的语言来说我们的单个图像就是一个形状(高度、宽度、通道)的数组。</em> </strong></p></blockquote><p id="bfe8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">但是在我们的例子中，我们有3个通道的大小为32 x 32的每个图像。现在3个通道表示RGB(红、绿、蓝)，所以每个像素值的范围可以在(0–255，0–255，0–255)之间</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lv"><img src="../Images/b84c20bce0d1a7687a59cd0de5c7e974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EOXQgaDDWE07rCPG.png"/></div></div></figure><p id="1c82" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们提供这些像素值作为神经网络的输入。</p><p id="f706" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，我已经告诉过你们，单个神经元如何从输入中获取所有值，将所有输入与权重相乘，然后相加。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/68863ade02137625de9d0a0921c12055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UOx2Fgx4fcaN75YA.png"/></div></div></figure><p id="2818" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我已经告诉过你，为了简单起见，当我们有多个输入或神经元时，我们可以把输出写成w(转置)* x  </p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lx"><img src="../Images/ea0ed7f3eece01affff00d8d6b7931c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*X8B0H7fw1veH3_ffKOjwnA.png"/></div></figure><p id="42ca" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一层由许多神经元组成，每个神经元执行乘法和加法运算<strong class="jk hi">一个神经元用一个圆表示。</strong></p><p id="2451" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在我们的人工神经网络中有三种类型的层</p><ol class=""><li id="eb6f" class="kx ky hh jk b jl jm jo jp jr kz jv la jz lb kd lc ld le lf bi translated">输入层</li><li id="94be" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">隐蔽层</li><li id="a144" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">输出层</li></ol><p id="c622" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">在输入层和输出层之间，所有的层都被认为是隐藏层</strong></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ly"><img src="../Images/6f516214b3c0b4f8446f896b4ba8ef77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IF_MAq3mN1m77soo.png"/></div></div></figure><blockquote class="lo lp lq"><p id="3917" class="ji jj lr jk b jl jm ii jn jo jp il jq ls js jt ju lt jw jx jy lu ka kb kc kd ha bi translated"><strong class="jk hi">请记住，我们不将输入层算作一个层，因为它不参与任何类型的处理，它只是显示输入的数量，在许多文本和书籍中，您会看到输入层神经元由单个点(。)而不是圆圈</strong></p></blockquote><p id="df64" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">因此，如果有人问你上图中有多少层，你的答案应该是3(包括输出层)</p><p id="b662" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，我们终于为主题做好了准备！</p><h1 id="5c85" class="lz ma hh bd mb mc md me mf mg mh mi mj in mk io ml iq mm ir mn it mo iu mp mq bi translated">不同的神经元连接架构</h1><h2 id="1cff" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated">1.单层神经网络</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nf"><img src="../Images/7361e5d8fec093ff12016de37cc91c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aXyUIYgTSHEg38Ms.png"/></div></div></figure><p id="a226" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">单层神经网络中没有任何隐藏层。它们只有两层输入和输出(<strong class="jk hi">这里我们不认为输入层是一层，所以它只有一层</strong>)。现在使用这个架构似乎没什么用，但是当我解释感知器模型的时候就清楚了。</p><h2 id="a86d" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated">2.多层前馈网络</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ng"><img src="../Images/71bac5728913251abbeabc8bf3b3e482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uSkEjYd7r0LwAK_X.jpeg"/></div></div></figure><p id="83fa" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这里，我们的输入层和输出层之间有不止一层。一个或多个隐藏层的存在使得网络的计算能力更强。</p><p id="acf5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在我们将学习如何在Pytorch中创建和使用多层网络。为此，我们将使用CIFAR-10数据集。我们已经讨论过这个数据集。</p><p id="0451" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们开始吧！</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="21a6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">从transform，我们使用To。Tensor()获取数据并将数据转换成张量。Torchvision在<code class="du nj nk nl nm b">torchvision.datasets</code>模块中提供了许多内置数据集，以及用于构建您自己的数据集的实用程序类。</p><div class="ke kf ez fb kg kh"><a href="https://pytorch.org/vision/stable/datasets.html" rel="noopener  ugc nofollow" target="_blank"><div class="ki ab dw"><div class="kj ab kk cl cj kl"><h2 class="bd hi fi z dy km ea eb kn ed ef hg bi translated">数据集- Torchvision 0.12文档</h2><div class="ko l"><h3 class="bd b fi z dy km ea eb kn ed ef dx translated">Torchvision在torchvision.datasets模块中提供了许多内置数据集，以及用于构建…</h3></div><div class="kp l"><p class="bd b fp z dy km ea eb kn ed ef dx translated">pytorch.org</p></div></div></div></a></div><p id="c4e4" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">本文档包含火炬视觉提供的所有数据集。</p><p id="e874" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里，我们使用了transform = transform，它使用To。张量，这意味着无论我们将从CIFAR-10数据集获得什么数据，转换都会将该数据转换为张量。</p><p id="0fbd" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><code class="du nj nk nl nm b">Dataset</code>存储样品及其相应的标签，<code class="du nj nk nl nm b">DataLoader</code>在<code class="du nj nk nl nm b">Dataset</code>周围包裹一个可重复标签，以便于获取样品。把可重复的单词记在心里。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="e302" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这里，我们将train = False作为一个参数，它将创建一个test_data，为了加载和创建数据集的iterable，我们还为test_data提供了一个Dataloader。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="3782" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这里，我们创建了一个函数来显示数据集中的图像。这里我们首先将张量转换成NumPy，因为matplotlib不能处理张量。正如我已经告诉你的，图像是一个三维数组(高度，宽度，通道)。看看这个例子</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nn"><img src="../Images/6fb6a7c1928aaf44b13c19c1b2b21ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvWfdoX7rKDe_-inqLpV-w.png"/></div></div></figure><p id="09ac" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，当我们将图像从张量转换为NumPy时，我们会得到尺寸(通道、宽度、高度),因此我们使用转置方法来更改数组的方向，以便我们可以使用matplotlib打印图像。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="6b05" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">还记得iterable这个词吗？</p><p id="3ce4" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里我们创建一个iterable对象，并使用make_grid(用于在网格中显示图像)打印一些图像及其标签值。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="570c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里0级标签表示飞机，1级标签表示汽车，以此类推。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><blockquote class="lo lp lq"><p id="74cd" class="ji jj lr jk b jl jm ii jn jo jp il jq ls js jt ju lt jw jx jy lu ka kb kc kd ha bi translated">这是为你的图像数据集创建神经网络时最重要的一步。这里我们的图像是3×32×32的形状(3个通道，32个高度和32个宽度),因此第一层中神经元的总数将是3×32×32。</p></blockquote><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="2c91" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">看这里nn。Linear用于创建一个带2个参数的线性图层</p><ol class=""><li id="839f" class="kx ky hh jk b jl jm jo jp jr kz jv la jz lb kd lc ld le lf bi translated">第一层中的神经元数量</li><li id="8d95" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">第二层的神经元数量</li></ol><p id="1605" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们已经讨论了为什么我们将在第一层中采用3*32*32个神经元，但在此之后，您可以根据自己的选择给出任意数量的神经元(点击和尝试)。要在神经网络中添加另一层，只需创建一个新变量，如self.layer2，并将前一层输出层的神经元数量作为输入。</p><h2 id="77f0" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated">您可以添加任意多的层/神经元，但请记住，您的模型可能会过度拟合，但我们将在接下来的讲座中学习如何克服过度拟合。</h2><p id="0b69" class="pw-post-body-paragraph ji jj hh jk b jl no ii jn jo np il jq jr nq jt ju jv nr jx jy jz ns kb kc kd ha bi translated">还有一件事你可以注意到，在我们的最后一层(第三层)的3000个神经元中，我们在输出中写了10个神经元。原因是我们的输出中有10个类，所以我们需要10个输出，也就是10个神经元。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div><figcaption class="nt nu et er es nv nw bd b be z dx"><code class="du nj nk nl nm b">for i,data in enumerate(train_loader, 0 ):</code></figcaption></figure><p id="338a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我知道我们还没有谈到优化器和激活函数！因为我会在以后的文章里给你详细解释。</p><p id="edce" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们已经在上一篇文章中讨论了如何编写一个train_function。</p><p id="a8d4" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这里不同的是:</p><ol class=""><li id="660a" class="kx ky hh jk b jl jm jo jp jr kz jv la jz lb kd lc ld le lf bi translated">我们使用Adam作为优化器(我们很快就会明白什么是Adam Chill)</li><li id="eea6" class="kx ky hh jk b jl lg jo lh jr li jv lj jz lk kd lc ld le lf bi translated">在这一行中，我们在train-loader中枚举，它有两个参数(train-loader和从哪里开始的索引)</li></ol><pre class="ix iy iz ja fd nx nm ny nz aw oa bi"><span id="61af" class="mr ma hh nm b fi ob oc l od oe">for i,data in enumerate(train_loader, 0 ):</span></pre><p id="0500" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">3.至(设备)</p><pre class="ix iy iz ja fd nx nm ny nz aw oa bi"><span id="4e4a" class="mr ma hh nm b fi ob oc l od oe">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu' )</span></pre><p id="c0ae" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这一行中，我们检查GPU是否可用。如果GPU可用，模型会将张量发送到GPU进行快速处理。</p><p id="61da" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">休息一切照旧。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="92a8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，你知道如何训练一个多层神经网络！</p><blockquote class="lo lp lq"><p id="5ac9" class="ji jj lr jk b jl jm ii jn jo jp il jq ls js jt ju lt jw jx jy lu ka kb kc kd ha bi translated">从这里开始，每个架构都将归入递归神经网络，我们将在后续文章中详细研究，但出于知识目的，我们将对这些架构进行概述。</p></blockquote><h2 id="1958" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated">3.<strong class="ak">具有自身反馈的单个节点</strong></h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es of"><img src="../Images/b6df7162ee7a87bef0f852cbcdeec80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WI26BmoVBOG6PdxX.jpeg"/></div></div></figure><p id="6546" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">当输出可以作为输入被引导回同一层或前一层节点时，就会产生反馈网络。递归网络是具有闭环的反馈网络。上图显示了一个单个递归网络，该网络具有一个对自身进行反馈的单个神经元。</p><h2 id="7c80" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated"><strong class="ak"> 4。</strong> <strong class="ak">单层递归网络</strong></h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es og"><img src="../Images/f880e3aa4eccefc6706eb91852ca92e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/0*P1R69P8NfKNuIHwK.png"/></div></figure><p id="9839" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">上述网络是具有反馈连接的单层网络，其中处理元件的输出可以被引导回其自身或另一个处理元件或两者。</p><h2 id="a80b" class="mr ma hh bd mb ms mt mu mf mv mw mx mj jr my mz ml jv na nb mn jz nc nd mp ne bi translated">5.<strong class="ak">多层循环网络</strong></h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oh"><img src="../Images/44c8a92a7a73792b2fcd2236970ccbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IpohmNKgdD5lIWq_.jpeg"/></div></div></figure><p id="ef41" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这种类型的网络中，处理元件输出可以被引导到同一层和前一层中的处理元件，形成多层循环网络。它们对序列中的每个元素执行相同的任务，输出取决于前面的计算。每个时间步都不需要输入。</p><p id="e59c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果你有任何问题，请在评论中提问！</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oi"><img src="../Images/0a7850f9c4a8632c9204becca90d2201.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*Z8E0o-YpBakZ_vCC.jpg"/></div></figure><div class="ke kf ez fb kg kh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ki ab dw"><div class="kj ab kk cl cj kl"><h2 class="bd hi fi z dy km ea eb kn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ko l"><h3 class="bd b fi z dy km ea eb kn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kp l"><p class="bd b fp z dy km ea eb kn ed ef dx translated">medium.com</p></div></div><div class="kq l"><div class="oj l ks kt ku kq kv jg kh"/></div></div></a></div></div></div>    
</body>
</html>