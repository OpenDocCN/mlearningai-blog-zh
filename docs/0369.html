<html>
<head>
<title>A quick overview of ResNet models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ResNet模型的快速概述</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-quick-overview-of-resnet-models-f8ed277ae81e?source=collection_archive---------0-----------------------#2021-03-31">https://medium.com/mlearning-ai/a-quick-overview-of-resnet-models-f8ed277ae81e?source=collection_archive---------0-----------------------#2021-03-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="ae73" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">一.导言</h1><p id="814e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在之前的帖子中，我们发现了一些流行的卷积神经网络(CNN)，如<a class="ae ka" href="https://lekhuyen.medium.com/lenet-and-mnist-handwritten-digit-classification-354f5646c590" rel="noopener"> LeNet </a>、<a class="ae ka" href="https://lekhuyen.medium.com/alexnet-and-image-classification-8cd8511548b4" rel="noopener"> AlexNet </a>、<a class="ae ka" href="https://lekhuyen.medium.com/an-overview-of-vgg16-and-nin-models-96e4bf398484" rel="noopener"> VGG、NiN </a>、<a class="ae ka" href="https://lekhuyen.medium.com/implementation-of-googlenet-on-keras-d9873aeed83c" rel="noopener"> GoogLeNet </a>，在这些网络中，模型性能随着层数的增加而成比例增加。有人可能会问，模型的层数越多，学习效果越好吗？一般来说，它并不总是正确的。因为具有大量层数的模型中的消失梯度现象从一开始就损害了这些模型的收敛性。下图就是一个具体的证据。56层的模型表现不如20层的模型。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/12d50b363fe24dad80df40723d3bb95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2-Plh0_3EU7Axvl5qsHoQ.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">Figure 1: Training error (left) and test error (right) on CIFAR-10 dataset of “plain” networks with 20 layers and 56 layers. Source [1]</figcaption></figure><p id="c218" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">为了克服这一缺点，微软的一个研究小组在2015年提出了一种深度卷积神经网络，即ResNet。该网络由关于输入层的多个残差块组成，并且它们的操作原理与优化残差函数有关。这种特殊的架构允许通过增加层深度来获得精度。因此，该模型在ILSVRC和COCO 2015竞赛中获得了关于ImageNet检测、ImageNet定位、COCO检测和COCO分割任务的第一名。</p><p id="55da" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">在原始论文中，作者提出通过残差映射来拟合附加层。通常，如果我们用底层映射来表示H(x)，那么残差映射由<em class="kw"> F(x) := H(x) - x </em>来确定。剩余学习模块具有以下形式(图2):</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kx"><img src="../Images/e3b643b46ab505f6ee4d8c5eb1e3e96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*pihQ-w3fc9PoJ0_oLadfzw.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">Figure 2: The architecture of a residual building block. Source [1]</figcaption></figure><p id="cc1a" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">该块的一部分由身份映射层组成，其他层从学习的较浅模型复制。</p><ul class=""><li id="6fca" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">当输入x和输出y = H(x)具有相同的维数时，残差块函数由下式定义:</li></ul><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lh"><img src="../Images/5089fcc0c929d16fa31b5a2f00d28c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/1*ze5zGZBxkRQMUt2ph1VAbA.gif"/></div></figure><p id="5275" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">其中F(x，{W_i})表示要学习的残差映射，W_i表示该块中i^{th}层的权重。</p><ul class=""><li id="197a" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">当输入x和输出y不具有相同的维数时，我们可以使用线性投影W_s到恒等函数中来匹配维数:</li></ul><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es li"><img src="../Images/6d93ef1175c5d8eb4f49ded2fff07795.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/1*DVUpUckeTQqxDy_E_QtJxQ.gif"/></div></figure><p id="152d" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">残余块可具有两层或更多层。在最初的文章中，作者使用大小为二和三层的残差块构建了不同的网络(表1):</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lj"><img src="../Images/6c0ebdcfad104c1f75e74ef4c389a9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAXVJiNPgsoDwU3ULneb7A.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx">Table 1: Architectures of Resnet for ImageNet dataset. The residual building blocks are shown in brackets with the numbers of stacked blocks. Source [1]</figcaption></figure><p id="e585" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">上表显示了用于分类ImageNet数据集的ResNet的不同架构。层数从18到152不等。剩余块有两层或三层(图3):</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lk"><img src="../Images/4dde1f418112b425022154ca33dc2799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*yx06xFE1XnStx1DQj6j5DQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx">Figure 3: The architecture of a residual block with two layers (Left) and three layers (Right). Source [1]</figcaption></figure><p id="9774" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">下图显示了34层架构残差网络(ResNet-34)与34层普通网络和VGG-19模型的对比。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ll"><img src="../Images/16b4e5625bdefa8f4f6874b753348259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*XvtFC9toRfGWUXhi0jClag.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx">Figure 4: <strong class="bd ig">Left: </strong>VG-19 model, <strong class="bd ig">Middle: </strong>a plain network with 34 layers, <strong class="bd ig">Right: </strong>ResNet-34. Source [1]</figcaption></figure><p id="5915" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">总而言之:</p><ul class=""><li id="0255" class="ky kz hh je b jf kr jj ks jn la jr lb jv lc jz ld le lf lg bi translated">在ResNet模型中，所有卷积层都应用大小为3 × 3的相同卷积窗口，滤波器的数量随着网络的深度而增加，从64到512(对于ResNet-18和ResNet-34)，从64到2048(对于ResNet-50、ResNet-101和ResNet-152)。</li><li id="995b" class="ky kz hh je b jf lm jj ln jn lo jr lp jv lq jz ld le lf lg bi translated">在所有模型中，只有一个最大池层，池大小为3 × 3，在第一个层之后应用步长2。因此，在训练过程中，降低输入的分辨率受到很大限制。</li><li id="91f6" class="ky kz hh je b jf lm jj ln jn lo jr lp jv lq jz ld le lf lg bi translated">在所有模型的末尾，应用平均池层来替换完全连接的层。这种替换有一些优点。首先，在这一层中没有要优化的参数，因此它有助于降低模型的复杂性。其次，这一层更自然地加强了特征图和类别之间的对应关系。</li><li id="28d2" class="ky kz hh je b jf lm jj ln jn lo jr lp jv lq jz ld le lf lg bi translated">输出层有1000个神经元，对应于ImageNet数据集中的类别数。此外，在该层中应用了softmax激活函数，以给出输入属于每个类的概率。</li></ul><h1 id="b17c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">二。在Keras上导入一些可用的ResNet模型</h1><p id="803b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Keras平台上有不同版本的ResNet模型，如ResNet-50、ResNet-101和ResNet-152。可以很容易地从模块<strong class="je hi"><em class="kw">tensor flow . keras . applications:</em></strong>中导入</p><p id="c784" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">进口ResNet-50型号:</strong></p><pre class="kc kd ke kf fd lr ls lt lu aw lv bi"><span id="835c" class="lw if hh ls b fi lx ly l lz ma">from <strong class="ls hi">tensorflow.keras.applications</strong> import <strong class="ls hi">ResNet50</strong></span><span id="9f7c" class="lw if hh ls b fi mb ly l lz ma">ResNet_50 = ResNet50(weights = None)</span><span id="9c8d" class="lw if hh ls b fi mb ly l lz ma"><em class="kw"># parameter number: <br/></em>ResNet_50.count_params()<br/>&gt;&gt;&gt; 25 636 712</span></pre><p id="9439" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">导入ResNet-101型号:</strong></p><pre class="kc kd ke kf fd lr ls lt lu aw lv bi"><span id="b570" class="lw if hh ls b fi lx ly l lz ma">from tensorflow.keras.applications import ResNet101</span><span id="e044" class="lw if hh ls b fi mb ly l lz ma">ResNet_101 = ResNet101(weights = None)</span><span id="5090" class="lw if hh ls b fi mb ly l lz ma"><em class="kw"># parameter number: </em><br/>ResNet_101.count_params()<br/>&gt;&gt;&gt; 44 707 176</span></pre><p id="510b" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">导入ResNet-152型号:</strong></p><pre class="kc kd ke kf fd lr ls lt lu aw lv bi"><span id="11a4" class="lw if hh ls b fi lx ly l lz ma">from tensorflow.keras.applications import ResNet152</span><span id="56a2" class="lw if hh ls b fi mb ly l lz ma">ResNet_152 = ResNet152()</span><span id="cc1d" class="lw if hh ls b fi mb ly l lz ma"><em class="kw"># parameter number:</em><br/>ResNet_152.count_params()<br/>&gt;&gt;&gt; 60 419 944</span></pre><h1 id="0455" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">三。结论</h1><p id="6bb2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这篇文章中，我们发现了不同ResNet模型的架构。它们由多个剩余块组成，剩余块的构造与学习剩余函数有关。这种架构允许避免额外层的过度配合。特别是一些ResNet型号，如ResNet-50、ResNet-101和ResNet-152在Keras上有售。因此，它们可以很容易地导入，而不需要像我们在以前的文章中那样从头实现。:-)</p><p id="f0bd" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">希望这篇帖子对你有帮助。如果您有任何问题，请随时联系我。</p><p id="0026" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">感谢阅读！</p><p id="bcf7" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi"> <em class="kw"> Github代码</em></strong><em class="kw">:</em><a class="ae ka" href="https://github.com/KhuyenLE-maths/A-quick-overview-of-ResNet-models/blob/main/A_quick_overview_of_ResNet_models.ipynb" rel="noopener ugc nofollow" target="_blank">https://Github . com/khu yenle-maths/A-quick-overview-of-ResNet-models/blob/main/A _ quick _ overview _ of _ ResNet _ models . ipynb</a></p><p id="7382" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">我的博客页面:</strong><a class="ae ka" href="https://lekhuyen.medium.com/" rel="noopener">https://lekhuyen.medium.com/</a></p><p id="4c93" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi">____________________________________________________________</p><p id="e4df" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated"><strong class="je hi">参考文献:</strong></p><p id="dddf" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn kt jp jq jr ku jt ju jv kv jx jy jz ha bi translated">[1]何，，等.“用于图像识别的深度残差学习”IEEE计算机视觉和模式识别会议论文集。2016.</p><div class="mc md ez fb me mf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hi fi z dy mk ea eb ml ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">medium.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt kl mf"/></div></div></a></div></div></div>    
</body>
</html>