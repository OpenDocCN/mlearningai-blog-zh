<html>
<head>
<title>Batch, Iteration, Epoch Concepts in Deep Learning Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习训练中的批量、迭代、纪元概念</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/batch-iteration-epoch-concepts-in-deep-learning-training-ad8ebedf2408?source=collection_archive---------4-----------------------#2022-07-30">https://medium.com/mlearning-ai/batch-iteration-epoch-concepts-in-deep-learning-training-ad8ebedf2408?source=collection_archive---------4-----------------------#2022-07-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8ad5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章揭示了这三个词可能会让深度学习世界的新人生活有点艰难。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/75584fa37434b608af5a216b7fe7574b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19iTGt-yOcAZsL97pYzYwA.png"/></div></div></figure><h1 id="f75d" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">梯度下降</h1><p id="4414" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">故事从训练神经网络的<strong class="ig hi">梯度下降</strong>优化算法开始。该算法根据训练集改变参数。它有三种变体。第一种是随机梯度下降法(<strong class="ig hi"> SGD </strong>)，为每个训练<strong class="ig hi">样本</strong>一次更新一个参数。一个<strong class="ig hi">时期</strong>意味着看到一个数据集的所有样本，并基于所有样本更新参数。SGD会更新数据样本的数量。批量梯度下降(<strong class="ig hi"> BGD </strong>)对训练集中每个点的误差进行求和，仅在评估了所有训练样本后更新模型。第三种，小批量梯度下降(<strong class="ig hi"> MBGD </strong>)，结合了前两种变体。它将训练数据集分成更小的<strong class="ig hi">批次</strong>大小，并对这些批次中的每一个执行更新。批次大小表示每批中数据样本的数量。这些更新被称为<strong class="ig hi">迭代；</strong>所有迭代覆盖整个数据集。覆盖整个数据集或所有批次在<strong class="ig hi"> MBGD </strong>中称为一个时期。</p><h1 id="d2c1" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">例子</h1><p id="11c3" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated"><strong class="ig hi">数据集</strong> : MNIST带<strong class="ig hi"> 60_000 </strong>图像</p><p id="1e90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">数据样本数</strong> = <strong class="ig hi"> 60_000 </strong></p><h2 id="b269" class="kr jp hh bd jq ks kt ku ju kv kw kx jy ip ky kz kc it la lb kg ix lc ld kk le bi translated">签名于</h2><p id="1e43" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">为了检查整个数据集，它对每个时期的参数进行60_000次更新。</p><h2 id="9cf0" class="kr jp hh bd jq ks kt ku ju kv kw kx jy ip ky kz kc it la lb kg ix lc ld kk le bi translated">BGD</h2><p id="68e0" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">BGD对数据集中所有样本的误差求和，然后更新模型。它对每个时期进行一次更新。</p><h2 id="6591" class="kr jp hh bd jq ks kt ku ju kv kw kx jy ip ky kz kc it la lb kg ix lc ld kk le bi translated">MBGD</h2><p id="eda0" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">批量= 128</p><p id="95a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每批图像(数据样本)的数量= 128</p><p id="076b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">迭代次数= 60_000 / 128 = 469</p><p id="6e3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个时期对参数进行469次更新，因为有469次迭代。</p><div class="lf lg ez fb lh li"><a href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener  ugc nofollow" target="_blank"><div class="lj ab dw"><div class="lk ab ll cl cj lm"><h2 class="bd hi fi z dy ln ea eb lo ed ef hg bi translated">梯度下降-维基百科</h2><div class="lp l"><h3 class="bd b fi z dy ln ea eb lo ed ef dx translated">在数学中，梯度下降(通常也称为最速下降)是一种一阶迭代优化算法…</h3></div><div class="lq l"><p class="bd b fp z dy ln ea eb lo ed ef dx translated">en.wikipedia.org</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw jm li"/></div></div></a></div><div class="lf lg ez fb lh li"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lj ab dw"><div class="lk ab ll cl cj lm"><h2 class="bd hi fi z dy ln ea eb lo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lp l"><h3 class="bd b fi z dy ln ea eb lo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lq l"><p class="bd b fp z dy ln ea eb lo ed ef dx translated">medium.com</p></div></div><div class="lr l"><div class="lx l lt lu lv lr lw jm li"/></div></div></a></div></div></div>    
</body>
</html>