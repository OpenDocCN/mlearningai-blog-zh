<html>
<head>
<title>Demystifying Batch Normalization vs Drop out</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘批处理规范化与删除</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/demystifying-batch-normalization-vs-drop-out-1c8310d9b516?source=collection_archive---------0-----------------------#2021-10-11">https://medium.com/mlearning-ai/demystifying-batch-normalization-vs-drop-out-1c8310d9b516?source=collection_archive---------0-----------------------#2021-10-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="2c7b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">批量规范化真的是经验法则吗？将结果与CIFAR10数据集上的丢失进行比较</h2></div><p id="b39a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">众所周知，批量归一化(BN)可以提高模型性能，减轻内部协变量偏移，并应用较小的正则化效果。国阵的这些功能和证明国阵有效性的实证研究有助于巩固人们使用国阵而不是辍学的偏好。BN迅速取代了许多深度学习模型中的辍学层。为什么会这样呢？BN用自己的平均值和标准偏差对每批的单位值进行标准化。另一方面，Dropout随机丢弃神经网络中预定义比例的单元，以防止过度拟合。因此，使用dropout图层和batch normalization图层(为了更加明确起见，将它们放在一起)会造成这两者之间的不协调。虽然BN有轻微的正则化效果，但它更多的是归一化过程的副作用。相反，Dropout是一种简单但强大的正则化方法，用于解决过度拟合问题。</p><p id="d98f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里出现了一个难题:你应该选择批量规范化还是放弃，反之亦然？回答这个问题到一些问题，还是挺简单的。当你使用序列模型，如RNN或LSTM，你不能使用BN。取而代之的是，可以使用层标准化或丢弃作为替代方案。在序列模型中，剔除是一种更广泛采用的正则化方法。然而，对于其他深度神经网络，尤其是在卷积神经网络(CNN)中，使用BN而不是dropout通常被认为是经验法则。</p><p id="0e7c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">出于我对国阵几乎总是赢退学生的深深怀疑和好奇，我进行了一项实验。在开始之前，我想指出我做这个实验的动机远不是拒绝BN的作用或者证明一个比另一个好。这个实验的目的是看看BN是否真的优于dropout，以及在我们的用例中我们是否可以安全地选择BN而不是dropout。</p><p id="34c2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">实验是通过colab环境使用CIFAR10数据集完成的。我用不同的模型架构构建了四个不同的模型来比较结果。基线模型是用两层和BN层的VGG块构建的CNN模型。第二个模型是比基线模型更深的模型。第三个模型与第二个模型具有相同的结构，但是ADAM是为优化器实现的。所有其他模型都使用SGD作为优化器。最后，最后一个模型用dropout代替BN。你可以在这个<a class="ae js" href="https://github.com/Irene-kim/Demystify_NeuralNets" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>中看到每个模型是如何设计的，以及运行实验的一步一步的代码指南。为了清楚起见，这个实验使用了四个不同的模型:</p><ol class=""><li id="5bfd" class="jt ju hh iy b iz ja jc jd jf jv jj jw jn jx jr jy jz ka kb bi translated">带BN + SGD的VGG2(基线)</li><li id="74f5" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">VGG2，每个CNN层上有BN+SGD</li><li id="14c8" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">VGG2，每个CNN层上有BN+ADAM</li><li id="5596" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">VGG2带压差+ SGD</li></ol><p id="3b03" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">下图显示了数据预处理后CIFAR 10数据集中十个不同类别的三个示例。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/6bda980bbe57e0562ec79f4e34f96a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*OGFzCT1rTcJTFLnq4EzbCg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">CIFAR 10 dataset</figcaption></figure><h2 id="2a42" class="kx ky hh bd kz la lb lc ld le lf lg lh jf li lj lk jj ll lm ln jn lo lp lq lr bi translated">每个模型的训练和验证过程及结果如下(蓝线—训练集，红线—验证集):</h2><p id="da5d" class="pw-post-body-paragraph iw ix hh iy b iz ls ii jb jc lt il je jf lu jh ji jj lv jl jm jn lw jp jq jr ha bi translated">1.具有BN + SGD的VGG2(基线):<br/> acc: 0.6658，loss: 0.9705，auc: 0.9457</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lx"><img src="../Images/1b0bb67cdc184248729770438cf35d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0JbcoIYteigLAwp5-dHOg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Model Accuracy, Loss, and Learning Rate</figcaption></figure><p id="7437" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.在每个CNN层上具有BN的vgg 2+SGD:<br/>ACC:0.6786，loss: 0.9437，auc: 0.9489</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ly"><img src="../Images/7377397042222f2b8984bc8a52034363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70vnxbAd8dSibou9FhK2UA.png"/></div></div></figure><p id="567a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.在每个CNN层上具有BN的vgg 2+ADAM:<br/>ACC:0.7512，loss: 0.7312，auc: 0.9680</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lz"><img src="../Images/b8fa922198c5c5a955d87365c3a1649b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbXGVEkpCDZtI7_BWVGyfQ.png"/></div></div></figure><p id="2d32" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.vgg 2+SGD:<br/>ACC:0.7803，loss: 0.6453，auc: 0.9741</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lz"><img src="../Images/250cb06de4ab5c0f03c859b9fe9f3f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EcWIfEYsrT9S7k0tffB-Bg.png"/></div></div></figure><p id="4ae6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">与其他模型相比，具有SGD优化器的BN模型似乎减少了过度拟合。有意思的是，dropout模型在准确率和损耗方面远远胜过其他有BN的模型！检查训练和验证历史，具有脱落层的模型4呈现最佳性能，其次是模型3。其测试精度达到约0.8，而其他模型的精度较低。同样，模型4的训练和验证数据集的损失也是最低的。</p><p id="141b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">过度拟合似乎是所有模型的问题。为了缓解这个问题，我引入了提前停止并获取最佳超参数来构建最佳性能模型。</p><h2 id="b211" class="kx ky hh bd kz la lb lc ld le lf lg lh jf li lj lk jj ll lm ln jn lo lp lq lr bi translated">让我们用自己的眼睛来看看每个模型是如何对一艘船的形象进行分类的！船只图像的标签是8。</h2><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ma"><img src="../Images/eea3917ee1f8797fed19919c0418676f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*5qg9RvtIPhiN4Y0x6W7Qtw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Image of a ship</figcaption></figure><ol class=""><li id="0b4e" class="jt ju hh iy b iz ja jc jd jf jv jj jw jn jx jr jy jz ka kb bi translated">带BN + SGD的VGG2(基线)<br/> —预测:8</li><li id="30e6" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">每个CNN层上有BN的vgg 2+SGD<br/>—预测:0</li><li id="33e3" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">每个CNN层上有BN的vgg 2+ADAM<br/>—预测:8</li><li id="cb87" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">带压差的vgg 2+SGD<br/>—预测:0</li></ol><p id="6b25" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这里的模型2和4做出了不正确的预测。他们认为那是一架飞机而不是一艘船。</p><p id="1b0d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们试试另一个！这次图像是平面的，标记为0。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mb"><img src="../Images/138a7ef68b88be72bea6adffd0ff6a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*uT7uhEBCJ3sOeQqLeyUf0w.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Image of a plane</figcaption></figure><ol class=""><li id="0695" class="jt ju hh iy b iz ja jc jd jf jv jj jw jn jx jr jy jz ka kb bi translated">带BN + SGD的VGG2(基线)<br/> —预测:0</li><li id="4ac2" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在每个CNN层上有BN的vgg 2+SGD<br/>—预测:4</li><li id="3612" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在每个CNN层上有BN的vgg 2+ADAM<br/>—预测:0</li><li id="6b41" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">带压差的vgg 2+SGD<br/>—预测值:0</li></ol><p id="7b25" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这次只有model 2做了一个不准确的决定。模型2预测它是一只鹿。也许尖角状的飞机尾部与这个错误有关。</p><p id="0a49" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在让我们看看模特们是如何表现动物形象的。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mc"><img src="../Images/e218ce7a40001e7c92645012821cdefc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*enOX14VUfeuh9hTJJ4cvVA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Image of a frog</figcaption></figure><ol class=""><li id="a196" class="jt ju hh iy b iz ja jc jd jf jv jj jw jn jx jr jy jz ka kb bi translated">2带BN + SGD(基线)<br/> —预测:6</li><li id="b66c" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">每个CNN层上有BN的vgg 2+SGD<br/>—预测:2</li><li id="4266" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在每个CNN层上有BN的vgg 2+ADAM<br/>—预测:6</li><li id="d3b3" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">带压差的vgg 2+SGD<br/>—预测:6</li></ol><p id="8b0b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">再次，模型2犯了一个错误。模型2预测青蛙是一只鸟。</p><p id="71e6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这次让我们看看模型是如何预测这张看起来很奇怪的鹿照片的。鹿的地面真相标签是4。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/ae3007466549c3be0983045778cb6951.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Ypr9o9VnFJotYLPLCAchcg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Image of a deer</figcaption></figure><ol class=""><li id="c100" class="jt ju hh iy b iz ja jc jd jf jv jj jw jn jx jr jy jz ka kb bi translated">2带BN + SGD(基线)<br/> —预测:0</li><li id="bfd0" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在每个CNN层上有BN的vgg 2+SGD<br/>—预测:0</li><li id="6415" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在每个CNN层上有BN的vgg 2+ADAM<br/>—预测:4</li><li id="6693" class="jt ju hh iy b iz kc jc kd jf ke jj kf jn kg jr jy jz ka kb bi translated">带压差的vgg 2+SGD<br/>—预测:4</li></ol><p id="42f8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">模型1和2认为鹿的图片是一架飞机！背景和喇叭可能会混淆天空和飞机尖尖的尾翼的模型。</p><p id="ad33" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">总的来说，与BN模型相比，具有dropout层的模型似乎表现得相当好。这个结果对我来说非常有趣，因为数据科学领域的许多人盲目地相信BN比dropout层工作得更好，最终从未尝试比较这两者。结果表明，在某些情况下，漏失层的性能优于BN。最后但并非最不重要的是，结果告诉我们永远要用自己的眼睛测试和观察！虽然BN是一种先进而复杂的技术，但在某些情况下，简单的脱落层也可以利用图像数据获得类似甚至更好的结果。</p></div></div>    
</body>
</html>