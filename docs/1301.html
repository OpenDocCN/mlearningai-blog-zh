<html>
<head>
<title>Improve ML service throughput using Tensorflow Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow服务提高ML服务吞吐量</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/improve-ml-service-throughput-using-tensorflow-serving-b211508d72ea?source=collection_archive---------2-----------------------#2021-11-16">https://medium.com/mlearning-ai/improve-ml-service-throughput-using-tensorflow-serving-b211508d72ea?source=collection_archive---------2-----------------------#2021-11-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3627" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将机器学习引入现实世界应用/产品的关键因素之一是能够有效地将机器学习(ML)模型作为生产服务。越来越频繁地成为标准的大尺寸ML模型使得这项任务更加困难。在本文中，我们将了解如何通过使用高性能服务系统Tensorflow Serving来提高大型ML模型的ML服务性能和吞吐量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/8a9678bac793827a6397dad964d2113d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60u4gOXkDzBLRnDj5rWgtQ.jpeg"/></div></div></figure><h1 id="96ab" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">规模MaaS成本高昂</h1><p id="0cfd" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">ML模型即服务(MaaS)支持将ML预测作为后端服务插入到典型的web应用服务链中，从而由另一个web服务使用，甚至由前端应用程序通过API调用直接使用。然而，仅仅将ML模型包装到web服务器中并输出令人兴奋的推理结果是不够的。为了满足真实世界的流量，ML服务必须能够扩展，支持每秒数百甚至数千个请求(RPS)。如果单个ML服务实例支持非常低的RPS，则需要大量的ML服务实例来满足生产流量。这将显著增加MaaS在生产中的成本，有时(如果不是经常)会成为企业采用机器学习技术开发创新产品的巨大障碍。</p><p id="1c5b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们处理一些数字。假设我们的ML模型总共有10GB，我们希望在一个GPU实例中为这些模型提供服务，以实现更快的计算速度和更好的延迟。AWS为ML推理提供的一个常见EC2实例是<a class="ae kr" href="https://aws.amazon.com/ec2/instance-types/p3/" rel="noopener ugc nofollow" target="_blank"> p3.2xlarge </a>，它的成本为<strong class="ig hi">$ 3.06/小时点播</strong>，并为我们提供了1个16GB内存的GPU。它还具有8个CPU和61GB CPU内存，用于web服务器工作人员的任何非GPU计算相关任务或数据传输。由于我们的ML模型适合16GB的GPU内存，我们可以将ML模型预加载到内存中，以改善实时推理延迟。考虑到所有这些因素，假设一个客户端对我们的ML服务的请求从开始到结束需要500毫秒。这使得我们的吞吐量为2 RPS(每个实例的请求数)。</p><p id="91c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，假设在现实世界中，我们需要支持100 RPS的客户流量，那么我们的月成本将是<strong class="ig hi"> 100/2(实例)* 3.06美元(每小时)* 24(小时)* 30(天)= 110，160美元</strong>。也就是每个月100K多！您当然可以通过使用CPU实例来获得更低的成本，但许多数据和计算密集型ML服务需要GPU来实现更好的或可接受的生产客户延迟。仅仅生产实例的每月10万美元的成本当然可以被资金充足的企业所吸收，但这对于小企业或初创公司的预算来说是一个巨大的缺口。</p><h1 id="f763" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">ML服务缩放存在内存问题</h1><p id="7a5f" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">信不信由你，这就是我们最初构建ML服务时必须处理的问题。此外，我们的真实用例甚至更糟糕，因为我们必须使用更昂贵的EC2实例，因为我们必须服务更大的ML模型(<strong class="ig hi">总共将近40GB</strong>)。因此，我们踏上了寻找解决方案的征程，以提高我们ML服务的性能和吞吐量。在下面的小节中，我们将了解如何使用Tensorflow作为ML服务的服务系统，并将我们的服务吞吐量<strong class="ig hi">从2 RPS提高到12 RPS(每秒请求数)</strong>。</p><p id="23e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们开始之前，这里有一个关于我们需求的快速描述。我们总的ML型号尺寸是<strong class="ig hi"> 40GB </strong>。我们使用AWS GPU实例来实现更低的推理延迟。我们使用的实例类型是<strong class="ig hi"> g4dn.12xlarge </strong>，这是一个多gpu实例，具有<strong class="ig hi"> 64GB GPU内存</strong>，可以包含我们所有的ML模型。</p><h2 id="8510" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ip kz la kc it lb lc kg ix ld le kk lf bi translated">典型的ML服务架构</h2><p id="03f8" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">当我们第一次开始时，我们使用一个典型的ML服务架构来服务我们的模型，如下图所示。我们有<strong class="ig hi"> Nginx </strong>作为代理web服务器，<strong class="ig hi"> Gunicorn </strong>作为WSGI (Web服务器网关接口)，Flask作为Web框架，<strong class="ig hi"> Tensorflow </strong>作为ML推理框架。在运行时，用户请求到达Flask服务器，服务器调用python代码/函数在Tensorflow模型上运行推理以产生预测。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lg"><img src="../Images/ab4d0dafe7e2b0628515480480e0e0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sHjVVxfwPMw4-G0T"/></div></div></figure><p id="6da9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lh">图1:使用Tensorflow的典型ML服务架构</em></p><h2 id="42ec" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ip kz la kc it lb lc kg ix ld le kk lf bi translated">内存限制阻止了扩展</h2><p id="a02f" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">你可能会问，这是什么问题？嗯，这种架构对小型ML模型很适用。为了扩展，我们只需要增加Gunicorn web workers的数量，它应该能够处理更多的并发请求，从而实现更好的吞吐量。在这个架构中，添加一个额外的Gunicorn web worker意味着将ML模型的另一个副本加载到内存中。如果我们的ML型号足够小，比如1GB，这就不是问题。</p><p id="cf3f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，对于更大的ML模型来说，这不会是真的。一个服务实例具有有限的CPU/GPU内存，因此ML模型越大，我们可以使用的web workers就越少。使用GPU实例的需要使情况变得更糟。CPU内存比GPU内存便宜，您可以轻松找到一个具有足够大的CPU内存的实例，而无需倾家荡产。但是GPU实例的价格和您可以获得的有限GPU内存对大ML模型构成了昂贵的限制。例如，AWS提供服务的典型GPU实例如<strong class="ig hi"><em class="lh">【g4dn . 12x large】</em></strong>总共有64GB GPU内存，成本为3.912美元/小时。仅用1GB的ML模型就可以启动10到20个web workers，这足以实现所需的扩展。然而，对于一个40 GB的ML模型，我们只能勉强启动2个web workers，吞吐量受到严重限制。</p><p id="8d05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了进一步说明，下面的图表从内存使用的角度来看典型的ML服务架构。如你所见，由于内存限制，1个Gunicorn web worker是我们能做的最好的。添加另一个web worker将意味着加载40GB ML模型的另一个副本，这将使实例内存溢出。对于典型的ML服务架构，服务大型ML模型限制了我们可以使用的web工作器的数量，因此限制了我们可以实现的服务吞吐量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es li"><img src="../Images/6efa38eae4c19bf66a41188cbced9a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZXCCZcFHB2TaBR14"/></div></div></figure><p id="484a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lh">图2:典型ML服务架构中的内存使用情况</em></p><h1 id="fd48" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用Tensorflow服务扩展ML服务</h1><p id="4be8" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">上述ML服务架构的问题是web workers和ML模型的耦合。从好的方面来说，它使得运行ML预测就像在Flask server中调用python函数来运行Tensorflow会话一样简单。不利的一面是，如果不将更多的ML模型拷贝加载到内存中，就不可能扩展web workers。</p><h2 id="63e0" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ip kz la kc it lb lc kg ix ld le kk lf bi translated">具有张量流服务的ML服务架构</h2><p id="ea93" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated"><a class="ae kr" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> Tensorflow服务</a>，由Tensorflow团队开发，是一个服务于机器学习模型的高性能服务系统。以下是Tensorflow Serving提供的一些重要功能:</p><ul class=""><li id="62d9" class="lj lk hh ig b ih ii il im ip ll it lm ix ln jb lo lp lq lr bi translated">在一个REST API或gRPC API中服务多个ML模型</li><li id="b39e" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">现成的模型版本管理(ML模型的A/B测试)</li><li id="2f08" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">请求批处理以进行更精细的模型推理性能调整</li><li id="5ad3" class="lj lk hh ig b ih ls il lt ip lu it lv ix lw jb lo lp lq lr bi translated">由于高效、低开销的实施，推理时间的延迟最小</li></ul><p id="ef98" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面提供的所有特性中，最好的一个也许是它在一个单独的Tensorflow服务服务器中服务ML模型，并公开一个单独的REST/gRPC API，因此成功地将ML模型从web服务器上解耦。如下图所示，用户请求首先到达web服务器，而不是直接在web服务器中运行ML推理，它将API推理请求发送到Tensorflow服务服务器，在那里运行ML推理，然后将ML预测返回到web服务器。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/71a5d34b646302e1a671404604912af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*djehIuNwiG_mNRW7"/></div></div></figure><p id="1a78" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lh">图3:使用Tensorflow服务的ML服务架构</em></p><h2 id="eae7" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ip kz la kc it lb lc kg ix ld le kk lf bi translated">Tensorflow服务消除了内存限制</h2><p id="1246" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">Tensorflow服务通过将ML模型与web服务器解耦，消除了典型ML服务架构所遭受的内存限制，并有效地允许web工作者进行扩展。在这种服务架构中，增加web服务器中web worker的数量不需要在每个web worker中加载ML模型的额外副本。相反，我们只需将ML模型的一个副本加载到内存中，Tensorflow服务服务器将ML模型作为单个API端点提供服务。由于能够使用多个web workers来接收来自客户端的请求，我们有效地提高了ML服务的吞吐量。</p><p id="e733" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以我们的用例为例，如下图所示。通过采用Tensorflow服务于我们的ML模型，我们能够将我们的web工作者从1个扩展到12个，在彻底的负载测试之后，这将我们的实例RPS从2个提高到12个。我们使用的服务实例当然会有比我们的ML模型的总大小更大的GPU内存，允许我们加载ML模型的一个副本，并且还使用多个Gunicorn workers。</p><p id="5061" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能会问，为什么不使用12个以上的Gunicorn工人来提高产量呢？与典型的web服务器相似，这种ML服务架构也需要针对特定用例进行微调和调整，以实现最佳性能并满足潜在的约束。在下一节中，我们将了解一些微调方法和约束。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lx"><img src="../Images/7c7df3f271ca85c17d938fc3c873afb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RdNyK_oMasNMYI1_"/></div></div></figure><p id="9023" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lh">图4:使用Tensorflow服务的ML服务架构中的内存使用情况</em></p><h2 id="03a6" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ip kz la kc it lb lc kg ix ld le kk lf bi translated">性能微调</h2><p id="63c4" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated"><strong class="ig hi">首先，为你的ML服务微调网络工人的数量。</strong>我们可以使用的web workers的数量仍然有限，主要是因为您可能在ML推断之前/之后运行的预处理和后处理步骤需要内存。在常见的ML服务中，您可能需要对从客户端请求接收的数据运行预处理步骤，以便为推断做准备，并且您可能需要对推断结果运行后处理步骤，以便在将响应发送回客户端之前对其进行优化或重新格式化。预处理或后处理步骤需要在web worker中处理，而不是Tensorflow服务服务器。如果预处理或后处理步骤需要在服务器启动以获得运行时性能时将任何张量、参数或小模型加载到内存中，那么每个web worker都会消耗一定量的GPU内存。由于实例内存的限制，这将限制我们可以扩展的web workers的数量。需要对ML服务进行微调，以找出在不溢出实例内存的情况下可以使用的web workers的最佳数量。除此之外，还要为运行时操作留出内存空间。</p><p id="544b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">其次，通过请求批处理</strong>微调Tensorflow服务服务器。您可能已经注意到，即使我们有多个web workers来处理客户端请求，我们的Tensorflow服务服务器仍然是单worker服务器。如果ML推理占用了您ML服务延迟的大部分，它可能会成为一个瓶颈，因为从web works到Tensorflow服务服务器的推理请求将在高流量负载时开始堆积。为了改善这一限制，Tensorflow Serving提供了开箱即用的服务器端请求批处理功能，在其官方文档<a class="ae kr" href="https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching" rel="noopener ugc nofollow" target="_blank">中有更详细的描述。</a>请求批处理使Tensorflow服务服务器能够等待一定的超时时间，直到它累积了特定数量的请求，然后对所有请求批量运行推理。这一特性可以显著提高Tensorflow服务服务器的吞吐量，并在一定程度上降低推理延迟。需要进行微调，以找到最适合您的ML服务的最佳超时和批量大小。</p><p id="39cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第三，为您的ML服务微调web worker类型。</strong>WSGI服务器工作者的类型，在我们的例子中是Gunicorn工作者，也会影响ML服务的性能。典型的Gunicorn工作器类型，如同步工作器、异步工作器(Gevent)或异步工作器(gthread)的工作方式互不相同。根据服务的特性，选择正确的web worker类型会显著影响ML服务的性能。例如，如果您的ML服务是计算绑定的，这通常是由于昂贵的运行时计算，那么sync worker是一种更好的工作类型。相反，如果您的ML服务是I/O绑定的，这通常是由于进行大量的文件系统读/写或网络请求，那么async worker或asyncIO worker会工作得更好。为了找出最好的工作方式，调整您使用的web worker类型并对您的服务进行负载测试是很重要的。</p><h1 id="ea39" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">最后的话</h1><p id="9da5" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">在本文中，通过我们的用例演示，我们了解了内存限制如何成为使用大型ML模型扩展ML服务的瓶颈，以及Tensorflow服务如何改变ML服务架构以消除内存限制并允许ML服务扩展。随着深度学习培训的采用，大尺寸ML模型将会继续存在，甚至变得更大。因此，在大尺寸ML模型的生产中扩展ML服务是一项成本高昂的业务，尤其是在业务/应用必须满足越来越苛刻的流量的情况下。如果你也在类似的情况下扩展你的ML服务，考虑尝试Tensorflow服务。</p></div><div class="ab cl ly lz go ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ha hb hc hd he"><p id="5b34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lh">最初发布于</em><a class="ae kr" href="https://stephenweixu.com/blog/improve-ml-service-throughput-using-tensorflow-serving" rel="noopener ugc nofollow" target="_blank"><em class="lh">https://stephenweixu . com/blog/improve-ml-service-throughput-using-tensor flow-serving</em></a><em class="lh">。</em></p><div class="mf mg ez fb mh mi"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hi fi z dy mn ea eb mo ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw jm mi"/></div></div></a></div></div></div>    
</body>
</html>