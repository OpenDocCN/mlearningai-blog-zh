<html>
<head>
<title>Country prediction using Word Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单词嵌入的国家预测</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/country-prediction-using-word-embedding-f5c0f930c87b?source=collection_archive---------1-----------------------#2021-05-05">https://medium.com/mlearning-ai/country-prediction-using-word-embedding-f5c0f930c87b?source=collection_archive---------1-----------------------#2021-05-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/0d5f42260e93ab70b9fa2f0b5790c5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6VkpIbc-KU8cWUqHmkMzw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source :<a class="ae it" href="https://wiki.pathmind.com/images/wiki/countries_capitals.png" rel="noopener ugc nofollow" target="_blank"> https://wiki.pathmind.com/word2vec</a></figcaption></figure><p id="c78e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自然语言处理(NLP)是人工智能的一个分支，它帮助理解和解释人类语言，弥合了人类和机器语言之间的差距。</p><p id="7cf2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">给定一个首都城市的名称，我们使用单词之间的类比概念来预测一个国家。</p><p id="f077" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">单词嵌入:</strong></p><p id="6e2a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">机器学习和深度学习算法一般处理数值型数据。因此，为了将文本转换成数字，BagofWords技术被开发来从文本中提取数字特征。它使用单词频率分布的概念来查找每个单词在文本中出现的次数，这也称为向量化。它通过在语料库内的文档中创建单词出现矩阵来将文本转换成特征。因此，每个文档被表示为语料库中词汇长度大小的字数统计向量。然而，这种模型导致矩阵稀疏，并且无法捕捉文本中有意义的关系。</p><p id="c76c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">单词嵌入是一种解决上述两个问题的技术。使用这种方法，语言中的每个单词被表示为低维空间中的实值向量，使得语义相似的向量彼此靠近放置。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es js"><img src="../Images/db961d8b8f49d5ee75199480615851c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMrbKQohkfFJlnlhmELcMw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source: <a class="ae it" href="https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg" rel="noopener ugc nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg</a></figcaption></figure><p id="4b70" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">创建这种有意义的向量空间给算法一个机会来识别模式并检测给定任务中的相似性。</p><p id="4e37" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有许多降维技术可以用来从高维空间捕捉重要信息，并将其投影到一个更小的维度空间。PCA是可用于创建单词嵌入的最常见的降维技术之一。PCA的工作方式是将单词向量包作为输入，尝试找出最相关的特征，并尝试以某种方式组合这些特征，以便提取最大信息并将其投影到更小的维度空间。使用最近邻算法将语义相似的项目放置在向量空间中彼此靠近的位置。</p><p id="e559" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Word2vec是Google基于分布假设的概念训练单词嵌入的算法，使得语义相似的单词被映射到几何上接近的嵌入向量。gensim.models提供了KeyedVectors类来直接加载使用Word2vec模型预训练的单词向量。你可以从<a class="ae it" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="fea0" class="kc kd hh jy b fi ke kf l kg kh">import nltk<br/>from gensim.models import KeyedVectors<br/>embeddings = KeyedVectors.load_word2vec_format(‘./GoogleNews-vectors-negative300.bin’, binary = True)<br/>f = open(‘capitals.txt’, ‘r’).read()<br/>set_words = set(nltk.word_tokenize(f))<br/>word_embeddings = get_word_embeddings(embeddings)<br/>print(len(word_embeddings))<br/>pickle.dump( word_embeddings, open( “word_embeddings_subset.p”, “wb” ) )</span></pre><p id="9a25" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">寻找模型中向量之间的相似性:</p><p id="9fbb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">欧几里德距离:</strong></p><figure class="jt ju jv jw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/701ce53d5ca2b395f707b6d2ea25c7a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*GlhXiQ5U-yjnv6S7RXFTcg.png"/></div></div></figure><p id="f531" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">欧几里德距离通过计算两点之间线段的长度来计算两个向量之间的相似性。单词越相似，欧几里德距离就越有可能接近0。</p><p id="ae8b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">余弦相似度:</strong></p><p id="ad8e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">当比较不同大小的文档时，欧几里德距离有时会误导理解两个文档之间的相似性。因此，在我们处理文本数据的场景中，向量的大小并不重要，因为比较长度不均匀的文档的可能性很大。欧几里德距离的另一个缺点是它在高维空间中不能很好地工作。因此，我们使用余弦相似性度量来纠正这一点。两个向量之间的夹角余弦量化了两个文档之间的相似性。如果我们把向量的方向看作它的意义，它就能更好地捕捉语义的相似性。此外，向量之间的角度更不受字数等外部因素的影响。</p><figure class="jt ju jv jw fd ii er es paragraph-image"><div class="er es kj"><img src="../Images/9310ed529cfe2ad1f7b45999d7a00f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*H8MCgatBJbPBlApI31l5-A.png"/></div></figure><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="eac8" class="kc kd hh jy b fi ke kf l kg kh">def cos_similarity(u,v):<br/>   dot = np.dot(u,v)<br/>   det = np.linalg.norm(u)*np.linalg.norm(v)<br/>   cos = dot/det</span></pre><p id="a8c7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用基本的三角函数，</p><p id="2160" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Cos (0) = 1(如果角度为0，向量在同一直线上，方向相同，因此非常相似)</p><p id="9c74" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Cos (90) = 0(如果角度为90，则矢量是正交的，因此它们不相似)</p><p id="aa4c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">cos(180)=-1(如果角度为180，则矢量完全不同)</p><p id="b436" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">所以当文档之间的角度θ在0到90°之间时(0 &lt;= Cos(θ) &lt;= 1), the documents are similar, else dissimilar</p><p id="23ab" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">求每个首都的国家:</strong></p><p id="9c5f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">给定单词嵌入词典、一个关系(国家/地区-首都)和一个首都城市，该函数返回最有可能具有相似关系的国家/地区及其相似性得分。</p><p id="e517" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们用的是方程King — man + woman = queen，这是最著名的word2vec算法之一，表示单词的隐藏代数结构。因此，为了获得首都城市的国家，我们使用一个类似的等式，country 2 = capital 1-country 1+capital 2，并使用单词嵌入和相似性函数在数学上实现它</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7618" class="kc kd hh jy b fi ke kf l kg kh">def get_country(city1, country1, city2, embeddings):<br/>    vec = country1_embedding — capital1_embedding + capital2_ embedding<br/>    similarity = -1<br/>    for word in embeddings.keys():<br/>        word_emb = embeddings[word]<br/>        cur_similarity = cosine_similarity(vec,word_emb)<br/>        if cur_similarity &gt; similarity:<br/>            similarity = cur_similarity<br/>            country = (word, similarity)<br/>return country</span><span id="339e" class="kc kd hh jy b fi kk kf l kg kh">get_country('Athens', 'Greece', 'Cairo', word_embeddings)</span></pre><p id="e9d2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的函数调用返回:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d71c" class="kc kd hh jy b fi ke kf l kg kh">('Egypt', 0.7626821)</span></pre><p id="e9c5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用该模型预测国家的准确率达到了0.92！</p><p id="aa6e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意:这个博客是基于https://www.deeplearning.ai/<a class="ae it" href="https://www.deeplearning.ai/" rel="noopener ugc nofollow" target="_blank">的新专业NLP</a></p></div></div>    
</body>
</html>