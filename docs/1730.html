<html>
<head>
<title>BERT for Individual: Tutorial+Baseline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">个人BERT:教程+基线</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/bert-for-individual-tutorial-baseline-bf5ee69066e1?source=collection_archive---------11-----------------------#2022-01-23">https://medium.com/mlearning-ai/bert-for-individual-tutorial-baseline-bf5ee69066e1?source=collection_archive---------11-----------------------#2022-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="3a7d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="5793" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">因此，如果你像我一样，在作为初学者完成了几个月的计算机视觉模型构建后，刚刚开始在NLP工作，那么这个故事肯定会为你提供一些东西。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="ab fe cl kf"><img src="../Images/8917bef9ec630c1d48b2c5d935a6b86c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-oQKmzvHrzqeSQEnM9f_kQ.png"/></div><figcaption class="ki kj et er es kk kl bd b be z dx">Reference: <a class="ae km" href="https://guillim.github.io/machine-learning/2020/09/29/BERT-architecture.html" rel="noopener ugc nofollow" target="_blank">https://guillim.github.io/machine-learning/2020/09/29/BERT-architecture.html</a></figcaption></figure><h1 id="3aa5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">伯特风景</h1><blockquote class="kn ko kp"><p id="87dc" class="jc jd kq je b jf kr jh ji jj ks jl jm kt ku jp jq kv kw jt ju kx ky jx jy jz ha bi translated"><em class="hh"> BERT是一个深度学习模型，在各种各样的自然语言处理任务上给出了最先进的结果。它代表</em> <strong class="je hi"> <em class="hh">双向编码器，表示为变压器</em> </strong> <em class="hh">。它已经在维基百科和BooksCorpus上进行了预训练，并且需要(仅)针对特定任务进行微调。</em></p></blockquote><p id="ccdc" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">它通过在各种各样的NLP任务中呈现最先进的结果，在机器学习社区中引起了轰动，包括<strong class="je hi">问答(SQuAD v1.1) </strong>、<strong class="je hi">自然语言推理(MNLI) </strong>等等。</p><p id="c128" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">毫不夸张地说，BERT极大地改变了NLP的面貌。想象一下，使用在大型未标记数据集上训练的单个模型，在11个单独的NLP任务上获得最先进的结果。所有这一切只需要一点点微调。那是伯特！这是我们设计NLP模型的一个结构性转变。</p><p id="6707" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">BERT启发了许多最近的NLP架构、训练方法和语言模型，如Google的TransformerXL、OpenAI的GPT-2、XLNet、ERNIE2.0、Roberta等。</p><h1 id="64dd" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.伯特是什么？</h1><p id="5d07" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本质上是大量的Transformer编码器堆积在一起(不是整个Transformer架构，而只是编码器)。双向的想法是BERT和它的前身OpenAI GPT之间的关键区别。BERT是双向的，因为它的自我注意层在两个方向上执行自我注意。</p><p id="4c25" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">这一节有几件事我想解释一下。</p><ul class=""><li id="a171" class="kz la hh je b jf kr jj ks jn lb jr lc jv ld jz le lf lg lh bi translated">首先，很容易理解BERT代表变压器的双向编码器表示。这里的每个单词都有它的含义，我们会一个接一个地遇到。目前，<strong class="je hi">该系列的关键要点是——BERT基于Transformer架构。</strong></li><li id="f027" class="kz la hh je b jf li jj lj jn lk jr ll jv lm jz le lf lg lh bi translated">第二，BERT是在包括整个维基百科的大型未标记文本语料库上进行预训练的(那是25亿个单词！)和图书语料库(8亿字)。这个预训练步骤对BERT的成功非常重要。这是因为当我们在大型文本语料库上训练模型时，我们的模型开始对语言如何工作有更深入和更亲密的理解。这些知识是瑞士军刀，几乎对任何NLP任务都有用。</li><li id="955e" class="kz la hh je b jf li jj lj jn lk jr ll jv lm jz le lf lg lh bi translated">第三，BERT是一个<strong class="je hi">深度双向</strong>模型。双向意味着BERT在训练阶段从令牌上下文的左右两侧学习信息。</li></ul><p id="c764" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">这种双向理解对于将NLP模型带到下一个层次是至关重要的。让我们看一个例子来理解它的真正含义。可能有两个句子有相同的单词，但它们的意思可能完全不同，这取决于前面或后面的内容，正如我们在下面看到的。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es ln"><img src="../Images/38ee15189c07d9cdb7b1835040079452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/0*K42o0s81YOCq6E4q.png"/></div></figure><p id="8956" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">如果不考虑这些上下文，机器就不可能真正理解意思，它可能会一次又一次地抛出无用的响应，这并不是一件好事。</p><p id="b276" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">但是伯特解决了这个问题。是的，确实如此。这是伯特改变游戏规则的一个方面。</p><ul class=""><li id="0fda" class="kz la hh je b jf kr jj ks jn lb jr lc jv ld jz le lf lg lh bi translated">第四，最后，BERT的最大优势是它带来了<strong class="je hi"> ImageNet运动</strong>, BERT最令人印象深刻的方面是我们可以通过添加几个额外的输出层来微调它，从而为各种NLP任务创建最先进的模型。</li></ul><p id="d862" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">希望这能帮助你增长见识。</p><p id="9df3" class="pw-post-body-paragraph jc jd hh je b jf kr jh ji jj ks jl jm jn ku jp jq jr kw jt ju jv ky jx jy jz ha bi translated">更多请跟进。</p></div></div>    
</body>
</html>