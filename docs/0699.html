<html>
<head>
<title>Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/dimensionality-reduction-c4727ad078e6?source=collection_archive---------0-----------------------#2021-06-16">https://medium.com/mlearning-ai/dimensionality-reduction-c4727ad078e6?source=collection_archive---------0-----------------------#2021-06-16</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="04ca" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">什么，为什么和如何</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/a2afdd88109115214b2f81651f70ec79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GLq9mDafsaen12-ho4hGOw.png"/></div></div></figure><p id="94eb" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在本文中，我们将踏上<strong class="jk hi"> <em class="ke">降维</em> </strong>之旅。我们旅程的第一站是镇<strong class="jk hi"> <em class="ke">为什么</em>。</strong>从那里，我们将乘渡船去<strong class="jk hi"><em class="ke"/></strong>镇，而我们将在<strong class="jk hi"><em class="ke"/></strong>镇结束我们的旅程。另外，在说再见之前，有一个小惊喜给你们，我真的希望你们会喜欢它。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="fc6c" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated"><strong class="ak">为什么</strong></h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/ed1342572f231994d280376fcc85125b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iOCBg263puRGTDVuKwKhQ.png"/></div></div></figure><p id="1f51" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">经常发生的情况是，在理解一个概念是什么之前，如果我们首先探索导致概念出现的因素，我们就会对概念本身有更好的直觉和更深入的理解，也就是说，我们将确切地做什么。</p><p id="f2bb" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们在日常生活中遇到的大多数真实世界数据集都是高维的，通常包含多达数百万个特征。当我们有一个高维数据集时，我们在处理它时会遇到大量的问题。其中一些包括:</p><ul class=""><li id="f1d2" class="le lf hh jk b jl jm jo jp jr lg jv lh jz li kd lj lk ll lm bi translated">人类最强的美德之一是想象的能力。更高维度的数据集使人类缺乏这种美德，因为人类只能将事物可视化到3维。</li><li id="e452" class="le lf hh jk b jl ln jo lo jr lp jv lq jz lr kd lj lk ll lm bi translated">在许多ML模型中，可以看到训练时间和运行时间所需的空间和时间复杂性与数据集中的要素数量成正比。简而言之，数据集的维度越高，空间和时间复杂度的顺序就越高。</li><li id="ff9a" class="le lf hh jk b jl ln jo lo jr lp jv lq jz lr kd lj lk ll lm bi translated">高维数据集增加了数据中的方差，这可能导致模型的过度拟合，即模型虽然在训练数据集上将具有良好的性能，但是在测试数据集上将表现得非常差。</li><li id="a6f2" class="le lf hh jk b jl ln jo lo jr lp jv lq jz lr kd lj lk ll lm bi translated">在许多实际应用中，我们更看重性能而不是精度。在这样的应用中，高维数据集也是一个很大的障碍。</li><li id="ee3d" class="le lf hh jk b jl ln jo lo jr lp jv lq jz lr kd lj lk ll lm bi translated">高维数据集中的要素通常是多重共线的，这在很大程度上降低了模型的预测。</li><li id="9035" class="le lf hh jk b jl ln jo lo jr lp jv lq jz lr kd lj lk ll lm bi translated">闵可夫斯基距离(可以被认为是欧几里德距离和曼哈顿距离的推广)在更高维数据集中失去了可解释性。因此，当遇到高维数据集时，所有严重依赖于基于距离的度量的模型都面临着严重的打击。</li></ul><p id="5784" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">以上要点突出了我们在处理高维数据集时不得不面对的一些主要问题，统称为<strong class="jk hi"> <em class="ke">维数灾难</em> </strong>。我们穿着闪亮盔甲的骑士出现了💂🏼降维的概念。事不宜迟，我们继续吧。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="076f" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated">什么</h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/cd7395f250ffc62b5809b944c6aefaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkBzvEYHzuJselp-WVx4Rg.png"/></div></div></figure><p id="5439" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了解决所有上述问题和许多类似的问题，我们使用降维。根据维基百科的定义，它被定义为将数据从高维空间转换到低维空间，以便低维表示保留原始数据的一些有意义的属性，理想情况下接近其内在维度。</p><p id="e9f8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">迷路了吗？🤔让我们再次定义它。它只是将我们的数据从高维空间转换到低维空间，同时尽可能多地保留信息。</p><p id="3450" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，下一个自然的问题出现了，“所有这些听起来很神奇，但它是如何做到的呢？这是某种魔法吗？”✴️，这正是我们将在下一节回答的问题。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="6043" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated">怎么做</h1><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/040ee19339678b96894dfac9b005b755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bC73ot1Z-fmdDXEd4Z_XDA.png"/></div></div></figure><p id="56a3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，最重要的问题来了，<strong class="jk hi"> <em class="ke">“如何？”</em>。</strong>有些人可能想知道为什么这是最重要的问题。原因很简单，因为这个问题没有单一的答案。</p><p id="ed95" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">机器学习社区吹嘘了大量的降维技术。事实上，如果我描述其中的每一个，这篇文章将会从一篇低维的文章变成一篇高维的论文😂。</p><p id="74a6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">因此，在本文中，我决定只探索这些技术的外部领域。但是正如我之前提到的，有一个惊喜在等着你们，所以，现在是<strong class="jk hi"> <em class="ke">惊喜时间</em> </strong> <em class="ke">。</em></p><p id="282a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我将很快就这些技术中的一些写详细的文章，对于其他的，我将包括最好的可能的资源让你们所有人详细掌握它们中的每一个。</p><p id="fe50" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">不过在开始我们的旅行之前，我想提一件非常重要的事情。下面列出的技术并不是<strong class="jk hi"> <em class="ke">穷举</em>。</strong>虽然我已经尝试了尽可能多的技术，但是仍然有更多的技术存在。现在，让我们开始马拉松。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="fd55" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">缺失值比率</p></blockquote><p id="bbe9" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">背后的想法很简单。对于我们的每个特征，我们计算<strong class="jk hi">缺失值比率</strong>，它可以简单地计算为<strong class="jk hi">(缺失值的数量/观察总数)* 100 </strong>，然后我们设置一个阈值。现在，我们简单地排除那些丢失值比率高于阈值的特征。关于缺失值比率更详细的理解和实现，请参考此<a class="ae ly" href="https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-missing-value-ratio-and-its-implementation/" rel="noopener ugc nofollow" target="_blank"> <em class="ke">篇</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="e38e" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">低方差滤波器</p></blockquote><p id="0c79" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这项技术也很简单。我们简单地计算数据集中所有特征的方差，然后丢弃所有方差低于某个阈值的特征，同样，阈值的选择完全是主观的。关于低方差滤波器更详细的理解和实现，请参考这篇<a class="ae ly" href="https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-low-variance-filter-and-its-implementation/" rel="noopener ugc nofollow" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="a92c" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">高相关滤波器</p></blockquote><p id="6472" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这种技术中，我们简单地找出所有数字特征之间的相关性。如果相关系数超过某个阈值，我们可以丢弃其中一个特征。选择我们需要放弃的特性完全是主观的。更多详情，请参考本文<a class="ae ly" href="https://solegaonkar.github.io/ConceptHighCorrelationFilter.html" rel="noopener ugc nofollow" target="_blank"> <em class="ke">篇</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="73f8" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">随机森林</p></blockquote><p id="a725" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">虽然它是一个基于树的模型，用于非线性数据的回归和分类任务，但它也可以通过其内置的<strong class="jk hi"> feature_importances_ </strong>属性用于特征选择，该属性计算每个特征的特征重要性分数。关于随机森林模型的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2" rel="noopener" target="_blank"> <em class="ke">文章</em> </a> <em class="ke">。</em></p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="036f" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">主成分分析</p></blockquote><p id="c079" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在PCA中，我们基本上从现有变量中提取新变量，也称为<strong class="jk hi">主成分，</strong>其中每个主成分都是原始特征的线性组合。此外，它们是按照它们各自解释的方差的递减顺序提取的。我们使用特征向量和特征值的数学概念来计算主成分。</p><p id="4085" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">要想对PCA有一个难忘的了解，参考这个栈交换<a class="ae ly" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank"> <em class="ke">线程</em> </a>。这是我遇到的对任何话题的最好解释之一，不仅仅是PCA。关于它的实现，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60" rel="noopener" target="_blank"> <em class="ke">文章</em> </a> <em class="ke">。</em></p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="cfd2" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">独立成分分析</p></blockquote><p id="524e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这是最广泛使用的降维技术之一，它基于信息论。PCA和ICA的主要区别在于PCA寻找不相关的因素，而ICA寻找独立的因素。关于它的实现，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/independent-component-analysis-ica-in-python-a0ef0db0955e" rel="noopener" target="_blank"> <em class="ke">文章</em> </a> <em class="ke">。</em></p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="aa57" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">核主成分分析</p></blockquote><p id="6fe6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">核PCA是使用核方法技术的PCA的扩展。核主成分分析可以很好地处理非线性数据集，而正常的主成分分析不能有效地使用。关于内核PCA的详细了解，参考这篇<a class="ae ly" rel="noopener" href="/swlh/vc-high-dimensional-pca-and-kernel-pca-415ef47e2d15"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="4e0e" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">因素分析</p></blockquote><p id="4102" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这项技术基于<strong class="jk hi">相关性</strong>的概念。特定组中的所有变量在它们之间将具有高相关性，但是与其他组的变量具有低相关性，并且我们将每个组称为一个因子。关于因素分析的详细理解，请参考这篇<a class="ae ly" rel="noopener" href="/mlearning-ai/a-deep-dive-into-factor-analysis-d64e550c358f"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="095e" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">线性判别分析(LDA)</p></blockquote><p id="8d38" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">虽然它通常用于多类分类，但它也可以用于降维。这是一种受监督的算法，也考虑了类别标签。关于线性判别分析的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="8ac5" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">对应分析</p></blockquote><p id="fd22" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">也被称为<strong class="jk hi">倒数平均</strong>，这是一种传统上应用于<strong class="jk hi">列联表</strong>的技术。虽然它在概念上类似于PCA，但是它适用于分类数据而不是连续数据。到目前为止，这就是关于CA的内容，但是很快我将会写一篇关于CA和MCA的文章，在这篇文章中，我也会对列联表进行一些阐述。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="1b52" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">多重对应分析</p></blockquote><p id="fc91" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它可以简单地定义为CA在两个以上分类特征上的扩展。它用于检测和表示数据集中的底层结构。它通过将数据表示为低维欧几里得空间中的点来实现这一点。到目前为止，这就是MCA，但很快我会写一篇关于CA和MCA的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="d562" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">多因素分析</p></blockquote><p id="6031" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它寻找存在于所有特征中的共同结构。当数据集由一组数字或分类要素组成时，使用该方法。这是因为MFA用于分析由几个组特征描述的一组观察值。它可以看作是主成分分析、多成分分析和FAMD的延伸。到目前为止，这就是MFA，但很快我会写一篇完全基于这种技术的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="1bb9" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">混合数据的因子分析(FAMD)</p></blockquote><p id="64cf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它用于降低包含定量和定性特征的数据集的维度。这意味着FAMD适用于具有分类和数字特征的数据。可以看作是PCA和MCA的混合。到目前为止，这就是FAMD，但很快我会写一篇完全基于这一技术的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="287b" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">奇异值分解</p></blockquote><p id="411c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">虽然它在数字信号处理中用于降噪和图像压缩，但它也可以用于降维。是从<strong class="jk hi">线性代数</strong>的海洋里借来的概念。关于奇异值分解的详细理解，参考这篇<a class="ae ly" href="https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="6dc3" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">截断奇异值分解</p></blockquote><p id="2236" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi">截断SVD </strong>不同于常规SVD，因为它产生一个因子分解，其中列数等于指定的<strong class="jk hi">截断</strong>。它适用于许多行值为零的稀疏数据。到目前为止，这就是截断奇异值分解，但是很快我将会写一篇基于奇异值分解和截断奇异值分解的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="834f" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">t分布随机邻域嵌入(t-SNE)</p></blockquote><p id="db4a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它也是一种非线性降维技术，就像核PCA一样，主要用于数据可视化。除此之外，它还广泛应用于图像处理和自然语言处理。关于t分布随机邻域嵌入的详细理解，可以参考这篇<a class="ae ly" href="https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="b930" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">多维标度(MDS)</p></blockquote><p id="8d01" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">就像t-SNE，MDS是另一种非线性降维技术。它试图保持实例之间的距离，同时减少非线性数据的维度。关于多维缩放的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/mds-multidimensional-scaling-smart-way-to-reduce-dimensionality-in-python-7c126984e60b" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="7529" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">一致流形近似和投影(UMAP)</p></blockquote><p id="9876" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这是一种降维技术，可以像t-SNE一样用于数据可视化。然而，它也可以用于非线性数据集的降维。到目前为止，这就是UMAP，但很快我会写一篇完全基于这一技术的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="f23d" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">等距特征映射(Isomap)</p></blockquote><p id="373a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">就像t-SNE一样，它也用于非线性数据集的降维。它可以被看作是MDS或内核PCA的扩展。关于等距特征映射的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/isomap-embedding-an-awesome-approach-to-non-linear-dimensionality-reduction-fc7efbca47a0" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="daf5" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">局部线性嵌入(LLE)</p></blockquote><p id="f8fa" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这是一种无监督的降维方法。它试图减少特征的数量，同时试图保留原始非线性特征结构的几何特征。关于局部线性嵌入的详细理解，参考这篇<a class="ae ly" href="https://towardsdatascience.com/lle-locally-linear-embedding-a-nifty-way-to-reduce-dimensionality-in-python-ab5c38336107" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="aff7" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">黑森特征映射(HLLE)</p></blockquote><p id="baef" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它将数据投影到一个较低的维度，同时保留像LLE这样的局部邻域，但使用Hessian算子来更好地实现这一结果，因此得名。到目前为止，这就是HLLE的全部内容，但是很快我将会写一篇完全基于这种技术的文章。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="c0ea" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">光谱嵌入(拉普拉斯特征映射)</p></blockquote><p id="5eac" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">它使用频谱技术，通过将附近的输入映射到附近的输出来执行降维。它保持局部性而不是局部线性。对于光谱嵌入的详细了解，可以查看我的<a class="ae ly" href="https://elemento.medium.com/demystifying-spectral-embedding-b2368bba580" rel="noopener"> <em class="ke">文章</em> </a> <em class="ke"> </em>上相同。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="32be" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">反向特征消除</p></blockquote><p id="893c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">该技术通过递归要素消除(RFE)过程从数据集中移除要素。该算法从初始特征集开始，并继续消除特征，直到它检测到性能分数中的可忽略变化。关于后向特征消除的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/backward-elimination-for-feature-selection-in-machine-learning-c6a3a8f8cef4" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="1891" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">正向特征选择</p></blockquote><p id="82f8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这种方法可以被认为是反向特征消除的相反过程。它不是递归地删除特性，而是递归地添加特性。它从单个特性开始，并不断添加特性，直到检测到性能分数的微小变化。关于向前特性选择的详细理解，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/feature-importance-and-forward-feature-selection-752638849962" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><blockquote class="ls lt lu"><p id="a1b4" class="ji jj ke jk b jl jm ii jn jo jp il jq lv js jt ju lw jw jx jy lx ka kb kc kd ha bi translated">自动编码器</p></blockquote><p id="3f5a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这是一种人工网络，旨在将它们的输入复制到它们的输出。他们将输入压缩成一个<strong class="jk hi">潜在空间表示</strong>，然后从这个表示中重建输出。要详细了解自动编码器，请参考这篇<a class="ae ly" href="https://towardsdatascience.com/autoencoders-in-practice-dimensionality-reduction-and-image-denoising-ed9b9201e7e1" rel="noopener" target="_blank"> <em class="ke">文章</em> </a>。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="29e9" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated">关闭</h1><p id="47ce" class="pw-post-body-paragraph ji jj hh jk b jl lz ii jn jo ma il jq jr mb jt ju jv mc jx jy jz md kb kc kd ha bi translated">唷！在写这篇文章之前，我做梦也没想到会有这么多降维技术。每当我遇到以上列表中没有的降维技术时，我一定会更新这篇文章。</p><p id="968c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果你们中的任何人遇到了以上列表中没有的降维技术，请在评论区告诉我，或者直接联系我。</p><p id="efbf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">另外，我还想提一件事。从上面的列表中可以看出，我已经提到我将分别就这些技术中的许多技术撰写文章，但是如果我找到了深入解释上述任何技术的资源，那么代替我自己撰写文章，我将只是包括该资源，以便我们所有人都可以受益。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="dd3b" class="km kn hh bd ko kp kq kr ks kt ku kv kw in kx io ky iq kz ir la it lb iu lc ld bi translated">关于我的一点点👋</h1><p id="cbe1" class="pw-post-body-paragraph ji jj hh jk b jl lz ii jn jo ma il jq jr mb jt ju jv mc jx jy jz md kb kc kd ha bi translated">如果你没有兴趣认识作者，或者你已经认识我，你可以安全地跳过这一节。我保证这里没有隐藏的宝藏😆。</p><p id="cf57" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我是一个机器学习和深度学习的爱好者，这是我第一篇基于相同内容的文章。如果你喜欢，请把你的手放在一起👏如果你想阅读基于机器学习和深度学习的进一步文章<em class="ke"> #StayTuned。</em></p><p id="0fcd" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">非常感谢各位，让这次旅程成为可能。这的确是一次有趣的旅行，很快我会带着另一次冒险旅行回来。</p><div class="me mf ez fb mg mh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hi fi z dy mm ea eb mn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">medium.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv jg mh"/></div></div></a></div></div></div>    
</body>
</html>