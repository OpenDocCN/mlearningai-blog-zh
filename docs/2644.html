<html>
<head>
<title>Multiclass Classification with Auto-Tuning CatBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有自动调整CatBoost的多类分类</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/multiclass-classification-with-auto-tuning-catboost-5d352e30778d?source=collection_archive---------0-----------------------#2022-05-26">https://medium.com/mlearning-ai/multiclass-classification-with-auto-tuning-catboost-5d352e30778d?source=collection_archive---------0-----------------------#2022-05-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="2b11" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">带有HyperOpt的CatBoost成为多类分类的完美工具…</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/b5c0c867d5b8b671c2b9bbccaeaa33d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oHsSUJm1lw3fzt00"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Image from <a class="ae jw" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Flightfieldstudios.net%2Fstock-photos%2Fdates-fruit.html&amp;psig=AOvVaw0wuEZzZoABqoXX0LIblb9r&amp;ust=1653654249772000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjRxqFwoTCOiQvKyU_fcCFQAAAAAdAAAAABAP" rel="noopener ugc nofollow" target="_blank">Kaggle</a></figcaption></figure></div><div class="ab cl jx jy go jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="ha hb hc hd he"><p id="6ba6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">在本文中，我们将研究一个枣果的Kaggle数据集。数据集由7种类型的34个相关特征组成。我们的任务是将枣果分为7类。你可以通过下面的链接了解更多关于数据集的信息</p><div class="kh ki ez fb kj kk"><a href="https://www.kaggle.com/datasets/muratkokludataset/date-fruit-datasets" rel="noopener  ugc nofollow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">枣果数据集</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">7类；巴尔希、德格莱特·努尔、苏凯里、罗塔卜·莫扎法蒂、鲁萨纳、萨法维、萨盖</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">www.kaggle.com</p></div></div><div class="kt l"><div class="ku l kv kw kx kt ky jq kk"/></div></div></a></div><p id="53d6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">经过以上环节，我们了解到枣果的外观对其类型有很大影响。从898幅水果图像中提取了包括形态特征、形状和颜色在内的34个特征。</p><p id="3443" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">从逻辑回归到人工神经网络的不同技术已经在这个数据集上使用过。使用这些方法获得的性能结果分别为91.0%和92.2%。堆叠这些模型可获得92.8%的最高性能。</p><p id="ec8b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">我们将使用CatBoost并用HyperOpt调整超参数，并检查我们是否可以做得更好或与之前的结果匹配。</p><p id="b8d2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">现在让我们导入数据集并进行一些分析。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="c092" class="le lf hh la b fi lg lh l li lj">import numpy as np <br/>import pandas as pd<br/>import os<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix,precision_score<br/><br/>import catboost as ctb</span><span id="d9f8" class="le lf hh la b fi lk lh l li lj">from hyperopt import hp<br/>from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials</span><span id="0472" class="le lf hh la b fi lk lh l li lj">!pip install openpyxl # to read excel file</span><span id="585c" class="le lf hh la b fi lk lh l li lj">#reading the Kaggle dataset</span><span id="5304" class="le lf hh la b fi lk lh l li lj">data_path = "../input/date-fruit-datasets/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx"<br/>data=pd.read_excel(data_path)<br/></span><span id="7837" class="le lf hh la b fi lk lh l li lj">df = data.copy()</span><span id="8988" class="le lf hh la b fi lk lh l li lj">#To understand the data distribution<br/>df.describe()</span><span id="2a6a" class="le lf hh la b fi lk lh l li lj">#To understand the data types<br/>df.dtypes</span><span id="9a7d" class="le lf hh la b fi lk lh l li lj">#To check for nulls in data<br/>df.isnull().sum()</span><span id="aa4d" class="le lf hh la b fi lk lh l li lj"><em class="ij">#checking class distribution</em><br/>df['Class'].value_counts()*100/df.shape[0]</span></pre><p id="0fcd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">通过在<a class="ae jw" href="https://www.kaggle.com/code/nandakishorejoshi/datefruit-catboost-with-hyperopt" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>上运行上面的代码，我们将能够将Date_Fruit数据导入到熊猫数据框(df)中。在这篇文章之后，我们运行几个命令来分析数据，观察结果是</p><ol class=""><li id="c644" class="ll lm hh ik b il im ip iq ke ln kf lo kg lp jf lq lr ls lt bi translated">描述完数据后，我们看到大多数特征的平均值和中间值(第50百分位)非常接近。这意味着数据是正态分布的。</li><li id="0ba3" class="ll lm hh ik b il lu ip lv ke lw kf lx kg ly jf lq lr ls lt bi translated">在检查数据类型时，我们看到除了目标变量(Class ),所有其他变量都是数字类型</li><li id="50e1" class="ll lm hh ik b il lu ip lv ke lw kf lx kg ly jf lq lr ls lt bi translated">数据集中没有缺失值</li><li id="ca21" class="ll lm hh ik b il lu ip lv ke lw kf lx kg ly jf lq lr ls lt bi translated">所有七类枣果都不是均匀分布的。各等级的分布百分比如下图所示。</li></ol><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lz"><img src="../Images/4c9816b10cdf52fa48632c30f4d4d143.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*2phlwW66uPn-wC8r_Y882g.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 1 : Class distribution (in %)</figcaption></figure><p id="d379" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">通过上述分析，并了解到之前在该数据集上使用了逻辑回归和人工神经网络(如Kaggle上的<a class="ae jw" href="https://www.kaggle.com/datasets/muratkokludataset/date-fruit-datasets" rel="noopener ugc nofollow" target="_blank">数据集描述</a>中所述)，我们可以使用另一组强大的模型。</p><blockquote class="ma"><p id="95af" class="mb mc hh bd md me mf mg mh mi mj jf dx translated"><strong class="ak">增压机</strong></p></blockquote><p id="f82c" class="pw-post-body-paragraph ih ii hh ik b il mk in io ip ml ir is ke mm iv iw kf mn iz ja kg mo jd je jf ha bi translated">这里可以使用的一些常见的提升算法有XGBoost、LightGBM和CatBoost。这些算法的比较将在下面的文章中解释</p><div class="kh ki ez fb kj kk"><a href="https://towardsdatascience.com/performance-comparison-catboost-vs-xgboost-and-catboost-vs-lightgbm-886c1c96db64" rel="noopener follow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">性能比较:CatBoost与XGBoost以及CatBoost与LightGBM</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">机器学习中的助推算法——第七部分</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">towardsdatascience.com</p></div></div><div class="kt l"><div class="mp l kv kw kx kt ky jq kk"/></div></div></a></div><p id="34fa" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">从上面的文章中我们可以得出结论，LightGBM在性能和执行速度上都超过了Catboost和XGBoost。下一个关于速度的是Catboost，然后是XGBoost。我们还可以看到，这三种算法的性能几乎相同。</p><p id="9104" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">这一次，我们将选择Catboost(该系列中最新、探索最少的)，并尝试设定新的基准。</p><p id="718c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">现在，让我们对这些类进行标签编码，并创建测试和训练数据集</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="18c2" class="le lf hh la b fi lg lh l li lj">#label encodeing the Class feature<br/>le = LabelEncoder()<br/>df['label'] = le.fit_transform(df.Class.values)</span><span id="7e1b" class="le lf hh la b fi lk lh l li lj">#Creating train and test datasets by defining dependent and independent features</span><span id="1997" class="le lf hh la b fi lk lh l li lj">X,Y=df.drop(['Class','label'],axis=1),df['label']<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y,<br/>                                                    test_size=0.2,<br/>                                                    random_state=42,<br/>                                                    shuffle=True)</span></pre><p id="d13c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">当我们使用Catboost时，让我们在下面的链接中探索可以调整的一组超参数</p><div class="kh ki ez fb kj kk"><a href="https://catboost.ai/en/docs/concepts/parameter-tuning" rel="noopener  ugc nofollow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">参数调谐</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">CatBoost为参数调整提供了一个灵活的界面，可以进行配置以适应不同的任务。本节…</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">catboost.ai</p></div></div><div class="kt l"><div class="mq l kv kw kx kt ky jq kk"/></div></div></a></div><p id="a609" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">从上面的链接我们可以看到，有相当多的超参数，取值范围很广。下面的代码中定义了一些超参数及其取值范围</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="f105" class="le lf hh la b fi lg lh l li lj"><em class="ij">#define parameter range</em><br/>learning_rate=np.linspace(0.01,0.1,10)<br/>max_depth=np.arange(2, 18, 2)<br/>colsample_bylevel=np.arange(0.3, 0.8, 0.1)<br/>iterations=np.arange(50, 1000, 50)<br/>l2_leaf_reg=np.arange(0,10)<br/>bagging_temperature=np.arange(0,100,10)</span><span id="2f8f" class="le lf hh la b fi lk lh l li lj">#define the categorical features if any in the dataset for catboost to handle<br/>categorical_features_indices = np.where(X_train.dtypes == np.object)[0]</span></pre><p id="c32b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">手动调整所有超参数并找到正确的值来构建最佳模型非常困难。出于这个原因，我们使用一个自动超参数调谐包称为Hyperopt。Hyperopt是用Python编写的分布式异步超参数优化包。下面的链接中提到了Hyperopt的细节。</p><div class="kh ki ez fb kj kk"><a href="http://hyperopt.github.io/hyperopt/" rel="noopener  ugc nofollow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">远视文件</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">从PyPI pip安装hyperopt运行您的第一个示例#定义一个目标函数def…</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">hyperopt.github.io</p></div></div></div></a></div><p id="bb4a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">远视主要有三个组成部分</p><ol class=""><li id="3158" class="ll lm hh ik b il im ip iq ke ln kf lo kg lp jf lq lr ls lt bi translated">目标函数:</li></ol><p id="72c8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">目标函数是手头的任务。它可能是求解一个简单的线性代数方程，或者只是一个简单的if语句，从而产生一个动作，或者训练一个机器学习模型来找到最佳的超参数。</p><p id="8897" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">2.搜索空间:</p><p id="11ce" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">这是目标函数可以使用的值的空间。在我们的例子中，我们称之为参数空间。这里我们定义了每个超参数的取值范围。上面代码段中定义的那个</p><p id="cfa4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">3.最小化功能:</p><p id="7502" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">这是使目标函数在参数空间或搜索空间上产生的损失最小化的函数。</p><p id="d662" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">为每个目标定义正确的损失函数是获得超参数最佳组合的关键。为Catboost调整超参数的所有上述步骤的代码如下所示</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="50c2" class="le lf hh la b fi lg lh l li lj"><em class="ij">#Define parameter space, fit conditions</em><br/>ctb_clf_params = {<br/>    'learning_rate':     hp.choice('learning_rate',    learning_rate),<br/>    'max_depth':         hp.choice('max_depth',         max_depth),<br/>    <em class="ij">#'colsample_bylevel': hp.choice('colsample_bylevel', colsample_bylevel),</em><br/>    'iterations':       hp.choice('iterations',            iterations),<br/>    <em class="ij">#'l2_leaf_reg':       hp.choice('l2_leaf_reg',            l2_leaf_reg),</em><br/>    <em class="ij">#'bagging_temperature':       hp.choice('bagging_temperature',            bagging_temperature),</em><br/>    'loss_function':       'MultiClass',<br/>    <br/>}<br/>ctb_fit_params = {<br/>    'early_stopping_rounds': 5,<br/>    'verbose': False,<br/>    'cat_features': categorical_features_indices<br/>}<br/>ctb_para = dict()<br/>ctb_para['clf_params'] = ctb_clf_params<br/>ctb_para['fit_params'] = ctb_fit_params</span></pre><p id="90bd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">在上面的代码中，我们用拟合条件定义了参数空间。(一些参数空间被注释为对这些参数进行调优会降低性能。因此，使用默认值。表示只是在实现的情况下给出一个想法)。拟合条件定义模型拟合条件。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="4056" class="le lf hh la b fi lg lh l li lj"><em class="ij">#define Hyperopt class</em><br/>class <strong class="la hi">HYPOpt</strong>(object):<br/><br/>    def __init__(self, x_train, x_test, y_train, y_test):<br/>        self.x_train = x_train<br/>        self.x_test  = x_test<br/>        self.y_train = y_train<br/>        self.y_test  = y_test<br/><br/>    def process(self, fn_name, space, trials, algo, max_evals):<br/>        fn = getattr(self, fn_name)<br/>        try:<br/>            print('entering fmin')<br/>            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)  <strong class="la hi">#----1</strong><br/>        except <strong class="la hi">Exception</strong> as e:<br/>            return {'status': STATUS_FAIL,<br/>                    'exception': str(e)}<br/>        return result<br/><br/>    def ctb_clf(self, para):     <strong class="la hi">#---- 2</strong><br/>        clf= ctb.CatBoostClassifier(**para['reg_params'])<br/>        print('ctb initialized')<br/>        return self.train_clf(reg, para)<br/><br/>    def train_clf(self, clf, para):    <strong class="la hi">#----- 3</strong><br/>        print('fitting model')<br/>        clf.fit(self.x_train, self.y_train,<br/>                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],<br/>                **para['fit_params'])<br/>        print('model fitted')<br/>        pred = clf.predict(self.x_test)<br/>       f1=sklearn.metrics.f1_score(self.y_test,pred,average='micro')<br/>        f1=f1*(-1)                     <strong class="la hi">#---- 4</strong><br/>        print(f1)<br/>        return {'loss': f1, 'status': STATUS_OK}</span></pre><p id="2faf" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">#1-定义内置远视最小化功能</p><p id="40a9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">#2-客观罚款</p><p id="3b5f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">#3-适合模型的函数</p><p id="c03e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">#4-损失函数(负F1用作损失)</p><p id="fd5d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">在上面的代码中，我们创建了一个Hyperopt类并定义了一个目标函数。这里的目标是构建一个Catboost模型。我们使用函数<em class="ij"> ctb_clf </em>来定义模型，使用<em class="ij"> train_clf </em>来训练模型。</p><p id="5b51" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">我们还定义了一个损失函数。这里我们使用F1作为衡量标准。Hyperopt总是试图用其内置的<em class="ij"> fmin </em>方法将损失降到最低。因此，我们将负F1传递给<em class="ij"> fmin </em>，从而最大化F1分数。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="8e02" class="le lf hh la b fi lg lh l li lj"><em class="ij">#define objective and find the best hyperparameters</em><br/><br/>obj = HYPOpt(X_train, X_test, y_train, y_test)<br/>ctb_opt = obj.process(fn_name='ctb_clf', space=ctb_para, trials=Trials(), algo=tpe.suggest, max_evals=5)</span></pre><p id="cad0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">在上面的代码中，我们实例化该类并调用<em class="ij">进程</em>函数来启动超参数优化。使用来自参数空间<em class="ij"> ctb_para的参数的不同组合创建catboost的五次迭代(max_evals =5)。</em>产生最小损耗的参数作为<em class="ij">过程</em>函数的输出存储到<em class="ij"> ctb_opt </em>中，如图2所示。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mr"><img src="../Images/46c6a19067864e785687f2bf48e9afd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iX4jSOihnYy4TtHEoWOdvA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 2 : Indexes of Optimal parameters</figcaption></figure><p id="8fba" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">上面的输出是超参数空间的值的索引。我们需要存储实际值。字典用于存储实际值，如下面的代码所示</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="f009" class="le lf hh la b fi lg lh l li lj"><em class="ij">#Save best parametrs in a dictionary</em><br/>best_param={}<br/>best_param['learning_rate']=learning_rate[ctb_opt['learning_rate']]<br/>best_param['iterations']=iterations[ctb_opt['iterations']]<br/>best_param['max_depth']=max_depth[ctb_opt['max_depth']]</span></pre><p id="6e00" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">Hyperopt选择的最佳参数如下所示</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ms"><img src="../Images/a52e428259e02ba1151b752e21a55be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cI7JyxkWFbbRQ-_AmCEMMQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 3 : Actual value of Optimal parameters</figcaption></figure><p id="53f4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">借助Hyperopt的最佳参数，我们使用这些参数来构建最终的Catboost模型，并在测试数据上对其进行测试。</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="3e4d" class="le lf hh la b fi lg lh l li lj"><em class="ij">#build model with the best hyperparameters</em><br/>model=ctb.CatBoostClassifier(iterations=best_param['iterations'], <br/>                             depth=best_param['max_depth'],<br/>                             learning_rate=best_param['learning_rate'],<br/>                            loss_function='MultiClass',<br/>                            random_seed=42<br/>                            )</span><span id="eeaa" class="le lf hh la b fi lk lh l li lj">model.fit(X_train,y_train,cat_features=categorical_features_indices, eval_set=None, plot=True)</span></pre><p id="4339" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">建立了最佳模型后，我们需要验证测试数据的结果。我们将检查各种度量，从混淆矩阵到训练和测试数据的准确性，以检查过度拟合。我们还将看到微观、宏观和加权精度、召回和F1分数，以了解每个类的模型性能。</p><p id="44b3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">下面的代码帮助我们检查测试和训练的准确性，并得到混淆矩阵</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="8c49" class="le lf hh la b fi lg lh l li lj">train_pred = model.predict(X_train)<br/>train_acc = accuracy_score(y_train,train_pred)<br/>print('Train Accuracy: ', train_acc)<br/> <br/>test_pred = model.predict(X_test)<br/>test_acc = accuracy_score(y_test,test_pred)<br/>print('Test Accuracy:', test_acc)<br/>y_pred=test_pred</span><span id="85a3" class="le lf hh la b fi lk lh l li lj">confusion = confusion_matrix(y_test, test_pred)<br/>print('Confusion Matrix<strong class="la hi">\n</strong>')<br/>print(confusion)</span></pre><p id="66fe" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">我们可以借助下面的代码检查微观、宏观和加权精度、召回和F1分数</p><pre class="jh ji jj jk fd kz la lb lc aw ld bi"><span id="9218" class="le lf hh la b fi lg lh l li lj">print('Micro Precision: <strong class="la hi">{:.2f}</strong>'.format(precision_score(y_test, y_pred, average='micro')))<br/>print('Micro Recall: <strong class="la hi">{:.2f}</strong>'.format(recall_score(y_test, y_pred, average='micro')))<br/>print('Micro F1-score: <strong class="la hi">{:.2f}\n</strong>'.format(f1_score(y_test, y_pred, average='micro')))<br/><br/>print('Macro Precision: <strong class="la hi">{:.2f}</strong>'.format(precision_score(y_test, y_pred, average='macro')))<br/>print('Macro Recall: <strong class="la hi">{:.2f}</strong>'.format(recall_score(y_test, y_pred, average='macro')))<br/>print('Macro F1-score: <strong class="la hi">{:.2f}\n</strong>'.format(f1_score(y_test, y_pred, average='macro')))<br/><br/>print('Weighted Precision: <strong class="la hi">{:.2f}</strong>'.format(precision_score(y_test, y_pred, average='weighted')))<br/>print('Weighted Recall: <strong class="la hi">{:.2f}</strong>'.format(recall_score(y_test, y_pred, average='weighted')))<br/>print('Weighted F1-score: <strong class="la hi">{:.2f}</strong>'.format(f1_score(y_test, y_pred, average='weighted')))</span></pre><p id="9068" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">所有代码的输出如下图所示</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mt"><img src="../Images/f665c6b21050b18fb9fb3a90111d8163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*xQDZWaJYIPRCr7DmiYlbSg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 4 : Train and Test Accuracy</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mu"><img src="../Images/988aadad96f5205dcbf509eae598f5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*J4VXssrXPJuNLwFFgwuMoQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 5 : Confusion matrix</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mv"><img src="../Images/debd6f2c1f9df5fdf4181851ddf40952.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*2vNTQwRLltwknUU1ojXhqA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Figure 6 : Micro, Macro, Weighted metrices</figcaption></figure><p id="057d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">从上面的数字中我们可以看到，测试精度与Kaggle竞赛获得的最佳结果相当。微观和加权指标超过了最佳结果。</p><p id="3d5b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated"><strong class="ik hi">总结</strong></p><p id="a355" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">在本文中，我们尝试实现Catboost来解决Kaggle数据集。我们学到的一些要点是:</p><ol class=""><li id="0fd9" class="ll lm hh ik b il im ip iq ke ln kf lo kg lp jf lq lr ls lt bi translated">Catboost是一个非常强大的模型。它主要用于独立特征包含分类变量的情况。</li><li id="43f2" class="ll lm hh ik b il lu ip lv ke lw kf lx kg ly jf lq lr ls lt bi translated">Catboost有大量参数需要优化。</li><li id="63cb" class="ll lm hh ik b il lu ip lv ke lw kf lx kg ly jf lq lr ls lt bi translated">超图是一种为所有最大似然模型寻找最佳超参数的有效方法。</li></ol><p id="ae7d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ke iu iv iw kf iy iz ja kg jc jd je jf ha bi translated">下面是带完整代码的Kaggle笔记本的链接:</p><div class="kh ki ez fb kj kk"><a href="https://www.kaggle.com/code/nandakishorejoshi/datefruit-catboost-with-hyperopt" rel="noopener  ugc nofollow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">枣果_催化剂_用_超高倍</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">使用Kaggle笔记本探索和运行机器学习代码|使用枣果数据集的数据</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">www.kaggle.com</p></div></div><div class="kt l"><div class="mw l kv kw kx kt ky jq kk"/></div></div></a></div><div class="kh ki ez fb kj kk"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hi fi z dy kp ea eb kq ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">medium.com</p></div></div><div class="kt l"><div class="mx l kv kw kx kt ky jq kk"/></div></div></a></div></div></div>    
</body>
</html>