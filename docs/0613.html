<html>
<head>
<title>Hands on NLP-Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理实践</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/hands-on-nlp-natural-language-processing-8224385663a5?source=collection_archive---------2-----------------------#2021-05-26">https://medium.com/mlearning-ai/hands-on-nlp-natural-language-processing-8224385663a5?source=collection_archive---------2-----------------------#2021-05-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b83a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">详细介绍单词包、TF_IDF、RNN、GRU和LSTM。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/24d446034e3bb491d608924e40dcfccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HIxiUEP8EvfCr7uj"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@lazargugleta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lazar Gugleta</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7ec6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">是啊！Alexa是从NLP建模设计的。</p><p id="dd62" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，NLP是如何工作的，它是如何被设计用于建模的？</p><p id="e720" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我将为NLP建模准备数据。此外，我会从理论和代码两方面详细解释一切。</p><h1 id="1f74" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">什么是自然语言处理？</h1><p id="4e81" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">自然语言处理是用人类语言构建与机器交互的模型。这是语言学、计算机科学和人工智能的子领域。</p><p id="4e96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">NLP只不过是设计通过人类命令进行交互的系统。由于命令只是文本格式，因此模型需要根据文本信息进行训练。为了做到这一点，NLP只有很少的属性来使文本信息正确排序以用于建模。</p><p id="bbe3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦文本准备好了，我们就可以开始用一些技术来训练我们的模型，以便更好地预测NLP模型。像RNN、GRU和LSTM的技巧。此外，我还展示了一些其他技术，如单词袋、TF-IDF、Word2Vec和ML算法。</p><h1 id="8922" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">建模文本准备中应遵循的步骤</h1><ol class=""><li id="d86f" class="kw kx hh ig b ih kr il ks ip ky it kz ix la jb lb lc ld le bi translated"><strong class="ig hi">标记化</strong></li></ol><p id="4087" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它被用来把文章分成句子和单词。我们可以利用标记化把文章分解成句子，句子分解成单词。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="314a" class="lk ju hh lg b fi ll lm l ln lo">import nltk<br/>from nltk.tokenize import sent_tokenize<br/>from nltk.tokenize import word_tokenize</span><span id="38f2" class="lk ju hh lg b fi lp lm l ln lo">text = 'I like to watch cricket news. But, I love to watch movies than cricket. Can we go to library by tomorrow.'</span><span id="c96e" class="lk ju hh lg b fi lp lm l ln lo">sentence = sent_tokenize(text)<br/>sentence</span><span id="9c1c" class="lk ju hh lg b fi lp lm l ln lo">Output : <br/>['I like to watch cricket news.',<br/> 'But, I love to watch movies than cricket.',<br/> 'Can we go to library by tomorrow.']</span></pre><h2 id="ac97" class="lk ju hh bd jv lq lr ls jz lt lu lv kd ip lw lx kh it ly lz kl ix ma mb kp mc bi translated">2.堵塞物</h2><p id="2ea5" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">它被用来切词。词干化只不过是缩短单词。</p><p id="8c69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">三种类型的词干:</p><ul class=""><li id="fd80" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated"><strong class="ig hi">波特斯特默</strong></li></ul><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="aa39" class="lk ju hh lg b fi ll lm l ln lo">from nltk.tokenize import word_tokenize<br/>from nltk.stem import PorterStemmer</span><span id="6d55" class="lk ju hh lg b fi lp lm l ln lo">text='NLP is good and easy to learn with no efforts'</span><span id="7419" class="lk ju hh lg b fi lp lm l ln lo">words=word_tokenize(text)<br/>words</span><span id="d044" class="lk ju hh lg b fi lp lm l ln lo">ps = PorterStemmer()</span><span id="6072" class="lk ju hh lg b fi lp lm l ln lo">for words in words:<br/>    print(ps.stem(words))<br/></span><span id="1b89" class="lk ju hh lg b fi lp lm l ln lo">Output : <br/>words- ['NLP', 'is', 'good', 'and', 'easy', 'to', 'learn', 'with', 'no', 'efforts']</span><span id="daaa" class="lk ju hh lg b fi lp lm l ln lo">nlp<br/>is<br/>good<br/>and<br/>easi<br/>to<br/>learn<br/>with<br/>no<br/>effort</span></pre><ul class=""><li id="0652" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated"><strong class="ig hi">兰卡斯特·斯坦默</strong></li></ul><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="6e57" class="lk ju hh lg b fi ll lm l ln lo">from nltk.stem import LancasterStemmer<br/>lst=LancasterStemmer()</span><span id="e9a5" class="lk ju hh lg b fi lp lm l ln lo">words_to_stem=['call','calling','caller','called']<br/>for words in words_to_stem:<br/>    print(words+":"+lst.stem(words))<br/></span><span id="5249" class="lk ju hh lg b fi lp lm l ln lo">Output :<br/>call:cal<br/>calling:cal<br/>caller:cal<br/>called:cal</span></pre><ul class=""><li id="62b2" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated"><strong class="ig hi">雪球阻止器</strong></li></ul><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="27d6" class="lk ju hh lg b fi ll lm l ln lo">from nltk.stem import SnowballStemmer<br/>sbs=SnowballStemmer(lang)</span><span id="8185" class="lk ju hh lg b fi lp lm l ln lo">words_to_stem=['call','calling','caller','called']</span><span id="deb9" class="lk ju hh lg b fi lp lm l ln lo">for words in words_to_stem:<br/>    print(words+":"+sbs.stem(words))<br/></span><span id="84c2" class="lk ju hh lg b fi lp lm l ln lo">Output :<br/>call:call<br/>calling:call<br/>caller:caller<br/>called:call</span></pre><h2 id="5f55" class="lk ju hh bd jv lq lr ls jz lt lu lv kd ip lw lx kh it ly lz kl ix ma mb kp mc bi translated">3.词汇化</h2><p id="88d8" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">它类似于词干提取，但通过词汇化获得完整单词的含义。正如我们所见，上述代码的输出没有任何意义。因此，通过使用词汇化，我们可以从文本中获得有意义的完整单词。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="5d5a" class="lk ju hh lg b fi ll lm l ln lo">from nltk.stem import WordNetLemmatizer<br/>word_len=WordNetLemmatizer()</span><span id="a22a" class="lk ju hh lg b fi lp lm l ln lo">words_to_stem=['call','calling','caller','called']</span><span id="40be" class="lk ju hh lg b fi lp lm l ln lo">for words in words_to_stem:<br/>    print(words+":"+word_len.lemmatize(words))<br/></span><span id="b42a" class="lk ju hh lg b fi lp lm l ln lo">Output :<br/>call:call<br/>calling:calling<br/>caller:caller<br/>called:called</span></pre><h2 id="6fb3" class="lk ju hh bd jv lq lr ls jz lt lu lv kd ip lw lx kh it ly lz kl ix ma mb kp mc bi translated">4.停用词</h2><p id="f00a" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">停用词是在文本中最经常出现的词，但是对于构建NLP模型来说，这些词不是必需的。通过文本中的这些停用词，模型有时会因预测而产生噪音。所以我们可以用NLP中的语法删除这些单词。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="1e94" class="lk ju hh lg b fi ll lm l ln lo">import nltk<br/>from nlkt.corpus import stopwords<br/>stopwords.words('english')     ## We can use any specific languages</span></pre><p id="afd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的代码中，我用英语来处理英语语言。所以我们可以指定任何我们想要处理的语言。</p><h2 id="9e78" class="lk ju hh bd jv lq lr ls jz lt lu lv kd ip lw lx kh it ly lz kl ix ma mb kp mc bi translated">5.位置标签</h2><p id="500d" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">对于任何带有标签分配的语言，它都可以作为文本中句子的词性。和POS标签是不需要的。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="00af" class="lk ju hh lg b fi ll lm l ln lo">sample='Have u find the person your searching for past one week'<br/>sample_tokens=word_tokenize(sample)</span><span id="c7cd" class="lk ju hh lg b fi lp lm l ln lo">for words in sample_tokens:<br/>    print(nltk.pos_tag([words]))<br/></span><span id="7299" class="lk ju hh lg b fi lp lm l ln lo">Output :<br/>[('Have', 'VB')]<br/>[('u', 'NN')]<br/>[('find', 'VB')]<br/>[('the', 'DT')]<br/>[('person', 'NN')]<br/>[('your', 'PRP$')]<br/>[('searching', 'VBG')]<br/>[('for', 'IN')]<br/>[('past', 'NN')]<br/>[('one', 'CD')]<br/>[('week', 'NN')]</span></pre><h2 id="e491" class="lk ju hh bd jv lq lr ls jz lt lu lv kd ip lw lx kh it ly lz kl ix ma mb kp mc bi translated">6.频率分布和n元图</h2><p id="414c" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">文本中单词的频率。换句话说，就是文本中重复出现的最常见的单词。n元语法只不过是文本或语音中n个项目(符号、单词和字母)的连续序列。这些n元语法也被称为带状疱疹。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="0ea9" class="lk ju hh lg b fi ll lm l ln lo">from nltk.corpus import gutenberg,webtext<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>from nltk import ngrams<br/>from nltk.probability import FreqDist</span><span id="b062" class="lk ju hh lg b fi lp lm l ln lo">stopwords=stopwords.words('english')<br/>filtered_words=[w for w in webtext.words('firefox.txt') if w not in stopwords and len(w)&gt;3]</span><span id="c253" class="lk ju hh lg b fi lp lm l ln lo">ngrams=ngrams(filtered_words,5)<br/>fdist=FreqDist(ngrams)<br/>fdist.most_common(20)<br/></span><span id="78e7" class="lk ju hh lg b fi lp lm l ln lo">Output :<br/>[(('launching', 'browser', 'window', 'binding', 'browser'), 6),<br/> (('Error', 'launching', 'browser', 'window', 'binding'), 5),<br/> (('allow', 'sites', 'removed', 'cookies', 'future'), 2),<br/> (('sites', 'removed', 'cookies', 'future', 'cookies'), 2)]</span></pre><p id="e3ea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完成文本准备。</p><p id="ebe3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">嘿伙计！机器只能理解数字。那么，如何处理字符串呢？？</p><p id="5920" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们可以在以下工具的帮助下进行文本编码:</p><ol class=""><li id="7d8e" class="kw kx hh ig b ih ii il im ip md it me ix mf jb lb lc ld le bi translated">单词包(计数矢量器)</li><li id="1ba5" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb lb lc ld le bi translated">TF-IDF (Tfidf矢量器)</li><li id="0d7d" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb lb lc ld le bi translated">Word2Vec</li></ol><p id="3cfd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">单词袋</strong>是文本数据中特征提取的方法</p><p id="96f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> TF-IDF </strong>是一种统计方法，用于判断词在文档集合中的相关出现。TF-IDF重视生僻字。</p><p id="2e1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> Word2Vec </strong>是NLP中使用的最突出的技术，它有大量的文本数据，每个单词都用一个称为向量的数字来表示。Word2Vec重视不同单词之间的语义信息&amp;关系。也表示为维度向量。</p><p id="7bb3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">注</strong>:单词包和TF-IDF方法并不存储语义信息。</p><p id="b9b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最重要的是，这些技术用于机器学习问题。当谈到神经网络时，我们必须学习更多关于文本编码的知识。</p><p id="d63b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，让我们看看代码</p><p id="4ba6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">导入包并加载数据</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="972e" class="lk ju hh lg b fi ll lm l ln lo">import pandas as pd<br/>import numpy as np<br/>from matplotlib import pyplot as plt</span><span id="3ff5" class="lk ju hh lg b fi lp lm l ln lo">df = pd.read_csv('emotion-labels-train.csv')<br/>df.head()</span><span id="a4e3" class="lk ju hh lg b fi lp lm l ln lo">label={'joy':0,'fear':1,'anger':2,'sadness':3}</span><span id="8864" class="lk ju hh lg b fi lp lm l ln lo">df.label=[label[i] for i in df.label]<br/>print(df.label)<br/></span><span id="38c5" class="lk ju hh lg b fi lp lm l ln lo">import nltk<br/>import re<br/>from nltk.corpus import stopwords<br/>from nltk.stem import PorterStemmer<br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>wl=WordNetLemmatizer()<br/>ps=PorterStemmer()<br/>corpus=[]</span><span id="1efe" class="lk ju hh lg b fi lp lm l ln lo">for i in range(0,len(tweets)):<br/>    process=re.sub('[^a-zA-Z]',' ',tweets['text'][i])<br/>    process=process.lower()<br/>    process=process.split()<br/>    <br/>    process=[wl.lemmatize(word) for word in process if word not in stopwords.words('english')]<br/>    process=' '.join(process)<br/>    corpus.append(process)</span></pre><p id="3f00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用单词袋</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="4b80" class="lk ju hh lg b fi ll lm l ln lo"># Bag of Words or Count Vectorizer</span><span id="223d" class="lk ju hh lg b fi lp lm l ln lo">from sklearn.feature_extraction.text import CountVectorizer<br/>cv=CountVectorizer(max_features=3000,ngram_range=(1,3))<br/>x=cv.fit_transform(corpus).toarray()</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mm"><img src="../Images/a308b4bbec56013f01ce2d8209857d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*yGLm6qbPO6L0bfopoXNUyg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Source: “Image by author”</figcaption></figure><p id="670e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在ML中使用决策树，我得到了80 %的准确率。</p></div><div class="ab cl mn mo go mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ha hb hc hd he"><p id="ade2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们用神经网络检查一下，试着用很少的技术建立一个模型。</p><p id="0805" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为，重要的是要提到文本准备在ML和神经网络模型中是相似的。唯一的区别是文本编码技术的用法。</p><p id="dac1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简单来说，</p><blockquote class="mu mv mw"><p id="d210" class="ie if mx ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated">对于ML，使用单词包、TF-IDF、Word2Vec等..</p><p id="b8ba" class="ie if mx ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated">对于神经网络，使用一个热编码、填充序列、字嵌入等..</p></blockquote><p id="b887" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我认为这消除了NLP建模中的所有疑问。</p><p id="e619" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">快乐的脸！！</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nb"><img src="../Images/a9c98e37aacc8926b9a93657aa3bb4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8sF0oRu_IzxYdMHR"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@benwhitephotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ben White</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="94a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是One hot编码？</strong></p><p id="0ef6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它是给文本中的单词分配不同的数字</p><p id="113a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">什么是填充序列？</strong></p><p id="a56c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使文档或文本数据中的所有句子长度相同。其中通过用零填充，句子将具有相同的长度。我们可以对零使用后置或前置填充。</p><p id="6a48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">嵌入:</strong></p><p id="4cfd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它们以高维的形式工作，使得相关的单词成为一维向量。</p><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="2c43" class="lk ju hh lg b fi ll lm l ln lo">Embedding(vocab_size, embedding_dim, input_length = max_length)</span></pre><ul class=""><li id="7fb3" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated">嵌入维数是单词编码中矢量表示的维数。</li></ul><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="914c" class="lk ju hh lg b fi ll lm l ln lo"># Building Simple RNN model</span><span id="4731" class="lk ju hh lg b fi lp lm l ln lo">from tensorflow.keras.models import Sequential<br/>embedding_vector=40</span><span id="6109" class="lk ju hh lg b fi lp lm l ln lo">model=Sequential([<br/>    Embedding(vocab_size,embedding_vector,input_length=sent_len),<br/>    tf.keras.layers.SimpleRNN(64,activation='relu'),<br/>    tf.keras.layers.Dropout(0.3),<br/>    tf.keras.layers.Dense(128,activation='relu'),<br/>    tf.keras.layers.Dropout(0.3),<br/>    tf.keras.layers.Dense(64,activation='relu'),<br/>    tf.keras.layers.Dropout(0.3),<br/>    tf.keras.layers.Dense(4,activation='softmax')<br/>])</span><span id="7c43" class="lk ju hh lg b fi lp lm l ln lo">model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nc"><img src="../Images/bea70928e9fb9efd105b19495413752a.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*JpTmtHjhCcZpdtBlYz9JIw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Source: “Image by author”</figcaption></figure><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="0dd9" class="lk ju hh lg b fi ll lm l ln lo"># let's try with LSTM's and Bidirectional RNN</span><span id="05c5" class="lk ju hh lg b fi lp lm l ln lo">model=Sequential([<br/>    Embedding(vocab_size,embedding_vector,input_length=sent_len),<br/>    LSTM(512,return_sequences=True),<br/>    LSTM(512,return_sequences=True),<br/>    Bidirectional(LSTM(256,activation='tanh')),<br/>    tf.keras.layers.Dropout(0.2),<br/>    Dense(256,activation='relu'),<br/>    tf.keras.layers.Dropout(0.3),<br/>    Dense(128,activation='relu'),<br/>    Dense(4,activation='softmax')<br/>])</span><span id="ebc6" class="lk ju hh lg b fi lp lm l ln lo">model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nd"><img src="../Images/862cf6d3f10ee0c902314c54563f5062.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*Is8DxEHeOQ5UTYaX7k5OjQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Source: “Image by author”</figcaption></figure><p id="e184" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为，我只用了10个时代得到了68%。尝试通过改变学习率和参数调整来增加周期，以获得更好的精度。</p><h1 id="34d9" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">结论</strong></h1><p id="08f2" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">这里用一个结论指出了整个建模过程。</p><p id="f317" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要记住的几点:</p><ul class=""><li id="9f3b" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated">使用标记化、词干化、词条化和停用词为建模准备文本。</li><li id="6a13" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">机器只能理解数字，所以我们使用</li><li id="2b40" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">机器学习问题:单词包，TF-IDF，Word2Vec等..</li><li id="2038" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">深度学习问题:一个热编码，填充序列，单词嵌入</li></ul><pre class="jd je jf jg fd lf lg lh li aw lj bi"><span id="f8ad" class="lk ju hh lg b fi ll lm l ln lo">from tensorflow.keras.preprocessing.text import Tokenizer</span></pre><ul class=""><li id="89f6" class="kw kx hh ig b ih ii il im ip md it me ix mf jb mg lc ld le bi translated">分词器有“num_words”来分词，通过它我们可以调用“fit_on_texts”来对句子进行分词。</li><li id="9122" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">“num_words”用于指定要分词的最大单词数，并选择最常见的“n”个单词。</li><li id="2bab" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">使用“word_index ”,我们可以看到分配给文本的索引号</li><li id="0045" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">通过“文本到序列”的方法，我们可以将文本转换成序列，这在自然语言处理中是很重要的。</li><li id="c86a" class="kw kx hh ig b ih mh il mi ip mj it mk ix ml jb mg lc ld le bi translated">通过填充序列，序列可以被填充，其中所有文本可以被分配相同的长度给输入模型。</li></ul></div><div class="ab cl mn mo go mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ha hb hc hd he"><p id="db37" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文中使用的所有代码和数据集都可以从我的<a class="ae js" href="https://github.com/Akshit9/Emotions_Classifier" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中获得。</p><p id="bd36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您的阅读和关注。</p><p id="5eb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢我的文章，点击拍手图标…</p><p id="5a10" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">保持联系:</p><div class="ne nf ez fb ng nh"><a href="https://www.linkedin.com/in/akshithkumar-05/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hi fi z dy nm ea eb nn ed ef hg bi translated">Akshith Kumar -技术作家-机器学习| LinkedIn</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">查看Akshith Kumar在世界上最大的职业社区LinkedIn上的个人资料。阿克什思有5份工作列在…</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">www.linkedin.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv jm nh"/></div></div></a></div></div></div>    
</body>
</html>