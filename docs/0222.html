<html>
<head>
<title>Batch Normalization and its Advantages</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批处理规范化及其优点</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/batch-normalization-and-its-advantages-a1dc52af83d0?source=collection_archive---------2-----------------------#2021-03-06">https://medium.com/mlearning-ai/batch-normalization-and-its-advantages-a1dc52af83d0?source=collection_archive---------2-----------------------#2021-03-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4992" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最近在看一篇关于NFNets的文章，这是Deepmind在图像分类方面最先进的算法，不需要归一化。理解深度神经网络中批量规范化的功能及其影响是理解NFNets算法的关键要素之一。只是想分享一下我对批量标准化的理解。</p><p id="4e75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">训练具有n层的深度神经网络是具有挑战性的，因为它们对少数参数敏感，如初始随机权重、学习速率等。一个可能的原因是，当反向传播时，输入到网络深层的分布可能在每个小批量之后改变。深层神经网络中各层输入分布的这种变化被称为“内部协方差偏移”。</p><h1 id="89c7" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">什么是批量正常化？</h1><p id="83b0" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">批量标准化是一种用于训练非常深的神经网络的技术，该神经网络对每个小批量的层的输入进行标准化。这具有稳定学习过程和显著减少训练深度网络所需的训练时期的效果。</p><blockquote class="kf kg kh"><p id="1347" class="ie if ki ig b ih ii ij ik il im in io kj iq ir is kk iu iv iw kl iy iz ja jb ha bi translated">算法</p></blockquote><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es km"><img src="../Images/18d5aea15364ec29772e6660822375a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*hevKa1HncdbaffoIgdh2FA.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">image from paper</figcaption></figure><p id="d82d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BN变换可以被添加到网络中以操纵任何激活。在符号y = BNγ，β(x)中，我们表示参数γ和β将被学习，但是应该注意，BN变换在每个训练示例中不独立地处理激活。相反，BNγ，β(x)取决于训练样本和小批量中的其他样本。经缩放和移位的值y被传递到其他网络层。规范化激活xb是我们转换的内部，但它们的存在是至关重要的。任何xb的值的分布具有0的期望值和1的方差，只要每个小批量的元素是从相同的分布中取样的，并且如果我们忽略ε。每个归一化激活xb (k)可以被视为由线性变换y (k) = γ (k)xb (k) + β (k)组成的子网络的输入，随后是由原始网络完成的其他处理。这些子网络输入都具有固定的均值和方差，尽管这些归一化xb (k)的联合分布可以在训练过程中改变，但是我们期望归一化输入的引入加速子网络的训练，并因此加速整个网络的训练。</p><h1 id="ec48" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">加速批处理规范化网络</h1><p id="83eb" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">简单地向网络添加批量标准化并不能充分利用我们的方法。为此，我们进行了以下修改:</p><ul class=""><li id="454c" class="ky kz hh ig b ih ii il im ip la it lb ix lc jb ld le lf lg bi translated"><strong class="ig hi"> <em class="ki">提高学习率:</em> </strong>在一个批量标准化的模型中，他们已经能够从更高的学习率实现训练加速，而没有副作用</li><li id="8f11" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated"><strong class="ig hi"> <em class="ki">去除漏失:</em> </strong>谷歌研究团队发现，从BN-Inception中去除漏失可以让网络达到更高的验证精度。我们推测批量标准化提供了与丢弃相似的正则化好处，因为对训练样本观察到的激活受同一小批量中随机选择的样本的影响。</li><li id="c551" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated"><strong class="ig hi"> <em class="ki">更彻底地混洗训练样本</em> </strong>:谷歌研究团队启用了训练数据的分片内混洗，这可以防止相同的样本总是一起出现在一个小批量中。这导致验证准确性提高了约1%,这与批量标准化作为正则化的观点相一致:当每次看到一个实例时，其影响不同时，我们方法中固有的随机化应该是最有益的。</li><li id="b64a" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated"><strong class="ig hi">降低L2权重正则化:</strong>虽然在初始阶段，模型参数上的L2损失控制过拟合，但是在修改的BN-初始阶段，该损失的权重降低了5倍。他们发现这提高了保留验证数据的准确性。</li><li id="8849" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated"><strong class="ig hi"> <em class="ki">加速学习率衰减</em> </strong>。在训练初期，学习率呈指数衰减。因为我们的网络训练速度比初始阶段快，所以我们降低学习速度的速度比初始阶段快6倍。</li><li id="e26a" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated"><strong class="ig hi"> <em class="ki">去除局部响应归一化</em> </strong>虽然Inception和其他网络(Srivastava et al .，2014)从中受益，但我们发现批量归一化是没有必要的。减少光度失真。因为批量标准化网络训练更快，并且观察每个训练示例的次数更少，所以我们通过减少扭曲让训练者专注于更多“真实”的图像。</li></ul><p id="d45f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">整篇文章是对论文中重要信息的汇编。我希望这有助于更好更快地理解批处理规范化。</p><p id="d4e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考</strong></p><p id="8ade" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae lm" href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf" rel="noopener ugc nofollow" target="_blank">http://static . Google user content . com/media/research . Google . com/en//pubs/archive/43442 . pdf</a></p></div></div>    
</body>
</html>