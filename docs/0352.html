<html>
<head>
<title>Short Python code for Backward elimination with detailed explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于反向消除的简短Python代码，并附有详细解释</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/short-python-code-for-backward-elimination-with-detailed-explanation-52894a9a7880?source=collection_archive---------0-----------------------#2021-03-28">https://medium.com/mlearning-ai/short-python-code-for-backward-elimination-with-detailed-explanation-52894a9a7880?source=collection_archive---------0-----------------------#2021-03-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bdaa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">向后消除是一种高级的特征选择技术，用于选择最佳数量的特征。有时，使用所有功能可能会导致机器学习模型变慢或出现其他性能问题。</p><h1 id="edbc" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">机器学习中的反向消去法简介</h1><p id="2a73" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">如果您的模型有几个特征，可能不是所有的特征都同样重要。一些特征实际上可以从其他特征中导出。因此，为了提高性能或准确性，您可以忽略一些特性。</p><p id="2cd2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有时，您必须亲自决定是保留衍生特征还是丢弃它们。例如，你的房子的总土地面积是从你的总土地的长度和宽度导出的字段。那么，我们能安全地从机器学习算法中移除总土地面积特征来预测房价吗？</p><p id="a2af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这么想吧。你喜欢正面比正面小但在巷子深处的房子更宽的房子吗？因此，在这种情况下，我们必须保留两个冗余特征(长度或宽度)中的至少一个，以及总陆地面积特征。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/424c3b1cf8ef97109a782106f5001259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ChdvWyHM3AttRAy8vKsb6w.jpeg"/></div></div></figure><p id="6ddd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了确保您拥有最佳数量的特征，您必须遵循一些降维技术，如lasso reduction(收缩较大的回归系数以减少过度拟合)、主成分分析(PCA)或<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/backward-elimination-code-in-python-0321/" rel="noopener ugc nofollow" target="_blank">向后消除</a>。</p><p id="7b40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要开始使用Python 中的<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/backward-elimination-code-in-python-0321/" rel="noopener ugc nofollow" target="_blank">反向消除代码，首先需要准备好数据。第一步是添加一个由1组成的</a><a class="ae kf" href="https://numpy.org/doc/stable/reference/generated/numpy.ones.html" rel="noopener ugc nofollow" target="_blank">数组</a>(该数组的所有元素都是“1”)以使该回归算法工作——由1组成的数组表示分配给独立变量X的第一维的常数，通常称为x0。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ks"><img src="../Images/e599934781fadca1c9462326db96deb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y0lYoSD2xUdSqiis5Xli1Q.png"/></div></div></figure><p id="e906" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里显示的代码可以作为<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/machine-learning-model-predict-car-mileage/" rel="noopener ugc nofollow" target="_blank">汽车里程预测文章</a>中编写的代码之后的代码步骤<strong class="ig hi">。换句话说，向后消除的python代码是汽车里程预测文章的第2部分。</strong></p><p id="5ec8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">反向消除python代码</strong>的步骤简述如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kt"><img src="../Images/06476b4bab83a493ea4fbb62a67baa15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*i3NqSJzkUad9LclM.jpg"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx"><a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/backward-elimination-code-in-python-0321/" rel="noopener ugc nofollow" target="_blank">https://fivestepguide.com/technology/machine-learning/backward-elimination-code-in-python-0321/</a></figcaption></figure><h1 id="fbd5" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">1.选择一个P值级别</h1><p id="3b17" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">通常，P值的5% <a class="ae kf" href="https://towardsdatascience.com/null-hypothesis-and-the-p-value-fdc129db6502" rel="noopener" target="_blank">显著性水平对于正常情况是完美的。所以保持P值= 0.05</a></p><h1 id="f15b" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">2.用所有特征拟合模型</h1><p id="c378" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在，让你的机器学习模型具备所有特性。如果您有50个特征，请将所有这些特征与测试数据集上的模型相匹配。</p><pre class="kh ki kj kk fd ky kz la lb aw lc bi"><span id="3a3b" class="ld jd hh kz b fi le lf l lg lh">import statsmodels.api as sm </span><span id="406e" class="ld jd hh kz b fi li lf l lg lh">X_train_opt = np.append(arr = np.ones((274,1)).astype(int), values = X_train, axis = 1) </span><span id="65a4" class="ld jd hh kz b fi li lf l lg lh">X_train_opt = X_train_opt[:,[0, 1, 2, 3, 4, 5, 6, 7]] </span><span id="f9e2" class="ld jd hh kz b fi li lf l lg lh">regressor_OLS = sm.OLS(endog = y_train, exog = X_train_opt).fit()<br/>regressor_OLS.summary()</span></pre><p id="5bc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输出是一个大的统计结果表。请注意，我们只对P值结果感兴趣(用黄色突出显示)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lj"><img src="../Images/b59162aaef235219a17d70dbb13f5c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/0*moJoNIwEVowR87_U.png"/></div></figure><h1 id="afae" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">3.哪个要素的P值最高？</h1><p id="a7ee" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">记录所有特征的P值。此后，我们将搜索具有<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/backward-elimination-code-in-python-0321/#predictor-with-highest-P-value" rel="noopener ugc nofollow" target="_blank">最高P值</a>的特征。仅当其P值大于所选的显著性水平(如0.05)时，才继续。否则，请将此视为最终的功能列表。</p><p id="0a84" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据上面的截图，x1和x2的P值大于显著性水平，x2的值较高。</p><h1 id="5f48" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">4.移除P值最高的要素</h1><p id="2d6a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">修改特征集，使其包含除上一步中确定的特征之外的所有特征。在我们的例子中，它是x2。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lk"><img src="../Images/da18af115b087a899479f4cd884ff09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wetJQzLH64un0bQeimlaaA.png"/></div></div></figure><h1 id="7d23" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">5.再次拟合模型(步骤2)，如果所有特征的p值都大于显著性水平，则停止拟合</h1><p id="6214" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在使用statsmodels.api库对python代码的倒数第二步使用OLS函数进行向后消除。</p><p id="05dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在拟合没有x2的模型。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lk"><img src="../Images/d8185f96574e3c9cefe38b5ddc53fbd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGowPcRl_oD_KalRmQ-zEQ.png"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/81d0d5df49a8579fde7947da9ce0a3a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*dZIoeTWl2Tk7Gfsp.png"/></div></figure><p id="c137" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，x1的P值大于显著性水平。</p><p id="05ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如前所述，在Python中重复反向消除代码，直到我们移除p值高于显著性级别(即0.05)的所有要素。</p><h2 id="1ad1" class="ld jd hh bd je lm ln lo ji lp lq lr jm ip ls lt jq it lu lv ju ix lw lx jy ly bi translated">6.现在，移除x1并再次拟合模型</h2><pre class="kh ki kj kk fd ky kz la lb aw lc bi"><span id="f806" class="ld jd hh kz b fi le lf l lg lh"># Note that now the model is run without 1st and 2nd features<br/>X_train_opt = np.append(arr = np.ones((274,1)).astype(int), values = X_train, axis = 1) </span><span id="6ac1" class="ld jd hh kz b fi li lf l lg lh">X_train_opt = X_train_opt[:,[0,3, 4, 5, 6, 7]] </span><span id="2630" class="ld jd hh kz b fi li lf l lg lh">regressor_OLS = sm.O#### LS(endog = y_train, exog = X_train_opt).fit() <br/>regressor_OLS.summary()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lz"><img src="../Images/f1e8d1ff29a38d113af049f9a339283e.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*DR1h4e80n0JSsqp0.png"/></div></figure><p id="930b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，您可以看到所有要素的P值都小于显著性级别。</p><h1 id="5ef9" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">用缩减的特征集测试ML模型性能</h1><p id="ed0d" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在我们知道，我们的算法所需的最佳特征集只是特征号3到7。因此，我们创建了另一个X_train和X_test，仅具有特征号3到7(在红色边框中)。</p><pre class="kh ki kj kk fd ky kz la lb aw lc bi"><span id="e85d" class="ld jd hh kz b fi le lf l lg lh">X_train2 = X_train.iloc[:,[2,3, 4, 5, 6]]</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ma"><img src="../Images/0f39cb0fa2a0da173b21da4772e20f07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WQUH2hP68xag5rpg.png"/></div></div></figure><h1 id="7f6e" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">在减少的特征上尝试随机森林机器学习</h1><p id="7d33" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在，我们尝试使用随机森林模型来检查性能。</p><pre class="kh ki kj kk fd ky kz la lb aw lc bi"><span id="1c59" class="ld jd hh kz b fi le lf l lg lh">rf = RandomForestRegressor(n_estimators = 10)<br/>rf.fit(X_train2,y_train)<br/>y_pred = rf.predict(X_test2)</span><span id="6f89" class="ld jd hh kz b fi li lf l lg lh">import matplotlib.gridspec as gridspec</span><span id="671e" class="ld jd hh kz b fi li lf l lg lh">fig = plt.figure(figsize=(12,5))<br/>grid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)</span><span id="8905" class="ld jd hh kz b fi li lf l lg lh">ax1 = fig.add_subplot(grid[0, 0])<br/>ax2 = fig.add_subplot(grid[0, 1])</span><span id="6489" class="ld jd hh kz b fi li lf l lg lh">sns.scatterplot(x = y_test['mpg'], y = y_pred, ax=ax1)<br/>sns.regplot(x = y_test['mpg'], y=y_pred, ax=ax1)</span><span id="e6df" class="ld jd hh kz b fi li lf l lg lh">ax1.set_title("Log of Predictions vs. actuals")<br/>ax1.set_xlabel('Actual MPG')<br/>ax1.set_ylabel('predicted MPG')</span><span id="099d" class="ld jd hh kz b fi li lf l lg lh">sns.scatterplot(x = np.exp(y_test['mpg']), y = np.exp(y_pred), ax=ax2,)<br/>sns.regplot(x = np.exp(y_test['mpg']), y=np.exp(y_pred), ax=ax2)</span><span id="aa0b" class="ld jd hh kz b fi li lf l lg lh">ax2.set_title("Real values of Predictions vs. actuals")<br/>ax2.set_xlabel('Actual MPG')<br/>ax2.set_ylabel('predicted MPG')</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mb"><img src="../Images/3594b959af141154848e18ff93773aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/0*leVGjkZbbv8rV78U.png"/></div></figure><pre class="kh ki kj kk fd ky kz la lb aw lc bi"><span id="1f76" class="ld jd hh kz b fi le lf l lg lh">print(‘MAE:’, metrics.mean_absolute_error(y_test, y_pred))<br/>print(‘MSE:’, metrics.mean_squared_error(y_test, y_pred))<br/>print(‘RMSE:’, np.sqrt(metrics.mean_squared_error(y_test, y_pred)))</span><span id="0687" class="ld jd hh kz b fi li lf l lg lh">Output →<br/>MAE: 0.07700061119950408<br/>MSE: 0.010134333336198278<br/>RMSE: 0.1006694260249768</span></pre><p id="6300" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于包含大量要素的大型数据集，您会发现MAE等方面存在明显差异。在特征完整和特征减少的随机森林之间。然而，这是一个具有很少特征的数据集，因此我们看不出预测的性能或准确性有多大差异。</p><p id="e2df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您阅读这篇文章。我在<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习主题</a>下写过几篇这样的文章，拥有广泛的知识，尤其是关于机器学习基础的知识。你可能会喜欢点击“<a class="ae kf" href="https://fivestepguide.com/technology/machine-learning/" rel="noopener ugc nofollow" target="_blank">数据科学</a>”类别并阅读那些文章。</p><h1 id="c0fc" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">常见问题:特征选择有哪些不同的方法？</h1><p id="08d2" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">有几种维度减少或特征选择技术:<br/>–套索减少:收缩大的回归系数以减少过度拟合<br/>–主成分分析(PCA)<br/>–丢弃相关变量以创建减少特征数据集<br/>–丢弃衍生特征。这是一个判断性的决定。<br/>–在使用随机森林<br/>后，通过绘制独立变量的图表来消除识别后的特征–使用线性回归来选择基于‘p’值的特征<br/>–向前选择，<br/>–向后选择<br/>–逐步选择</p><h1 id="9ddf" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">常见问题解答:什么是“维数灾难”？</h1><p id="122e" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">它表示基础数据集的要素比可能需要的要多。<br/>此外，如果你的特征多于观察值，你就有过度拟合的风险。观察值可能变得更加难以聚类。因为如果你有太多的维度，它会导致每个观察值看起来都很接近。<br/> PCA是最流行的降维技术。</p></div></div>    
</body>
</html>