<html>
<head>
<title>Linear Regression Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归基础</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/linear-regression-basics-aaa37daf09d6?source=collection_archive---------12-----------------------#2022-01-23">https://medium.com/mlearning-ai/linear-regression-basics-aaa37daf09d6?source=collection_archive---------12-----------------------#2022-01-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/715b2e5b14583c121cc684f6e729e566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imGlbK_rIf5qzZZ1nE1nIg.png"/></div></div></figure><h1 id="5193" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="ef1e" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在这篇文章中，我将谈论回归分析/机器学习中最关键的技术之一，称为线性回归。根据<a class="ae kl" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank">维基百科</a>，回归分析被定义为一组统计过程，用于估计因变量和自变量之间的关系强度。假设因变量和自变量之间呈线性关系，试图估计这种关系强度的过程称为线性回归。简而言之，在线性回归中，我们试图估计自变量(也称为特征)和因变量(也称为目标变量)之间的关系，假设它们之间存在线性关系。</p><h1 id="83b5" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">线性回归的假设</h1><p id="44af" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">有五个与线性回归模型相关的假设:</p><ol class=""><li id="c088" class="km kn hh jp b jq ko ju kp jy kq kc kr kg ks kk kt ku kv kw bi translated">线性:自变量和因变量的均值之间的关系是线性的。</li><li id="7384" class="km kn hh jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">同方差:残差的方差对于自变量的任何值都是相同的。</li><li id="660e" class="km kn hh jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">独立性:观测值相互独立。</li><li id="a56e" class="km kn hh jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">正态性:因变量和自变量的任何固定值都是正态分布的。</li><li id="23fa" class="km kn hh jp b jq kx ju ky jy kz kc la kg lb kk kt ku kv kw bi translated">无自相关:自变量的当前值和过去值之间不应有相关性。</li></ol><p id="bea9" class="pw-post-body-paragraph jn jo hh jp b jq ko js jt ju kp jw jx jy lc ka kb kc ld ke kf kg le ki kj kk ha bi translated">接下来，我们谈谈线性回归的基础知识。</p><h1 id="98d2" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">线性回归基础</h1><p id="046f" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在本节中，我们将讨论线性回归背后的数学原理。当我们只有一个独立变量时，它被称为简单线性回归或单变量线性回归，它由下式给出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lf"><img src="../Images/569d1166a930f0decf041aa8c0f0990e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fRbPdk1gD5oQhwbY.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx">Simple Linear Regression</figcaption></figure><p id="b7c1" class="pw-post-body-paragraph jn jo hh jp b jq ko js jt ju kp jw jx jy lc ka kb kc ld ke kf kg le ki kj kk ha bi translated">上式中，<em class="lo"> y1 </em>代表目标(因变量)，<em class="lo"> xi </em>代表自变量，𝜷 <em class="lo"> 1 </em>代表自变量的权重或系数，𝜷 <em class="lo"> 0 </em>代表偏项或简单来说，线性方程的截距。</p><p id="7d2c" class="pw-post-body-paragraph jn jo hh jp b jq ko js jt ju kp jw jx jy lc ka kb kc ld ke kf kg le ki kj kk ha bi translated">如果我们有两个以上的自变量，那么线性回归称为多元线性回归。上述方程也可以推广到多元线性回归。它是由，</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/1086e0975a4aa25ffcf58a312bf502c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*BLAO5rMOmghy0Oxr.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx">Multivariate Linear Regression</figcaption></figure><h1 id="ddee" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">结论</h1><p id="1547" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">因此，在本文中，我们讨论了线性回归的基础。如果您想深入了解线性回归，并在数据集上从头开始查看它的实现，您可以在这里<a class="ae kl" href="https://keepingupwithdatascience.wordpress.com/2022/01/23/linear-regression-in-machine-learning-from-scratch/" rel="noopener ugc nofollow" target="_blank">这样做</a>。</p><p id="cb9e" class="pw-post-body-paragraph jn jo hh jp b jq ko js jt ju kp jw jx jy lc ka kb kc ld ke kf kg le ki kj kk ha bi translated">我希望你能发现我的帖子内容丰富。我定期在我的博客上发布数据科学内容。如果你想和我联系，请随时通过<a class="ae kl" href="https://www.linkedin.com/in/chitwanmanchanda/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。感谢阅读，快乐学习！！！！</p><div class="lq lr ez fb ls lt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hi fi z dy ly ea eb lz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">medium.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh in lt"/></div></div></a></div></div></div>    
</body>
</html>