<html>
<head>
<title>Read Paper with Me: Data2Vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与我一起阅读论文:Data2Vec:语音、视觉和语言自我监督学习的一般框架</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/interpretation-of-data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-43299fef41d4?source=collection_archive---------0-----------------------#2022-01-22">https://medium.com/mlearning-ai/interpretation-of-data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-43299fef41d4?source=collection_archive---------0-----------------------#2022-01-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="9c15" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">跨模态学习模型data2vec解读</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/d58c5864c013f84754774ee91a6e51fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sLst-IqlHS01QXHvNwoh-w.png"/></div></div></figure><ul class=""><li id="d150" class="ji jj hh jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">这篇博客介绍了一篇来自Meta AI的关于自我监督学习的新论文:<a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank"> data2vec:语音、视觉和语言自我监督学习的通用框架</a></li><li id="7da4" class="ji jj hh jk b jl kb jn kc jp kd jr ke jt kf jv jw jx jy jz bi translated">如果你很难理解这个博客，建议先看看<a class="ae ka" rel="noopener" href="/@YunchaoLanceLiu/read-paper-with-me-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-e6580ce8dae5">我在BYOL </a>的博客。这部作品是以BYOL为基础的。</li></ul></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="0e7d" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">背景:</strong>传统的机器学习依靠有标签的数据进行训练。然而，数据的注释既费钱又费力。从更大的意义上说，给世界上所有的数据贴标签是不可能的。最近，s<a class="ae ka" href="https://viso.ai/deep-learning/self-supervised-learning-for-computer-vision/" rel="noopener ugc nofollow" target="_blank">elf-supervised learning(SSL)</a>作为一种解决这一问题的有前途的方法而受到关注，并用于<a class="ae ka" href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" rel="noopener ugc nofollow" target="_blank">逼近人工智能系统中的常识，</a>最终实现人工一般智能(AGI)。</p><p id="5904" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">SSL不是使用来自标记数据的监督信号，而是利用数据之间的关系。然而，不同的模态通常需要不同的SSL模型。因此，本文提出了一个统一的框架，称为data2vec(这个名字是对另一个著名的算法<a class="ae ka" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>的一种发挥)，用于三种模态的SSL:图像(在原始论文中，<em class="la">计算机视觉</em>指的是这个)、文本(在原始论文中有时称为<em class="la">语言</em>)、语音。Data2vec实现了所有三种模式的最先进(SOTA)结果。</p><p id="996c" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">在我们继续之前，注意这三种模态之间的区别是很重要的:图像是2D结构化数据。文本是离散的1D数据，而语音是连续的1D数据。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lb"><img src="../Images/55648a75c65d6bc402c82693ecf7a030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*FazEMTTCkY7TqdisLCf-BA.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Examples of three modalities: Image, speech, language (consistently called<em class="lg"> texts</em> in this blog). Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="3104" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">*此项工作不用于<a class="ae ka" href="https://towardsdatascience.com/multimodal-deep-learning-ce7d1d994f4" rel="noopener" target="_blank">多通道学习</a>中的培训(即，在培训过程中，三种模式中只有一种作为输入传递，而不是它们的混合)，但有助于多通道学习</p></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="1c0c" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">方法概述:</strong> data2vec使用一种模式，但有两种模式:教师模式和学生模式。在每个时间步骤中，data2vect的学生模式将尝试从教师模式学习并更新模型参数</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lh"><img src="../Images/9ccebd7468d0c4eedd95dc84393d92f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBU0sfo0sAfTcExxRn6yEQ.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">In each time step, data2vec in student mode learns from its teacher mode and updates its parameters. Image by the author.</figcaption></figure><p id="0ce3" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">具体来说，教师模式从给定样本(即图像、语音、文本)生成表示。同一样本的屏蔽版本被传递到学生模式。学习通过最小化学生对由教师参数构建的目标的预测之间的目标函数而发生。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es li"><img src="../Images/d02aa19f6a9a9f886344faf217b5bd96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*414n_QE9vgkgpRhGgbNRHg.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">For a certain timestep, the input of the teacher is a full sample (i.e., unchanged image, speech, text) and that of the student is a masked one. The student learns from the teacher by predicting a target constructed using the last K layers of the teacher (shown in blue). Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="abe3" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">方法:</strong></p><ol class=""><li id="464f" class="ji jj hh jk b jl jm jn jo jp jq jr js jt ju jv lj jx jy jz bi translated">模型架构</li></ol><p id="afa2" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">data2vec模型将样本转换为表示形式。完整样本的表示来自教师，屏蔽样本的表示来自学生。</p><p id="88b8" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">这些是<em class="la">语境化的表示，</em>意味着它们编码特定的时间步长以及来自样本的其他信息，这是由于在转换器中使用了自我关注。这是这部作品和以前作品的主要区别，以前的作品缺乏上下文信息。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lk"><img src="../Images/a53411e6f77bfa4827f695457464be58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oSH8HhWPxnT-FTgIsLjKbQ.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Samples turn into representations by passing through the data2vec model. First of all, samples are embedded into tokens. If in student mode, then a masking method is applied to the tokens before passing them to a <a class="ae ka" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Transformer</a>. If in teacher mode, the tokens are directly passed to the Transformer. Image by the author.</figcaption></figure><p id="cf59" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">2.目标结构</p><p id="4e53" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">使用指数移动平均线(<a class="ae ka" href="https://en.wikipedia.org/wiki/Moving_average#%7B%7Banchor%7Cexponential_moving_average%7CExponential%7D%7DExponential_moving_average" rel="noopener ugc nofollow" target="_blank"> EMA </a> [7，8】，其为更近的数据点放置更大的权重)来更新教师的参数。这里的τ是线性增加的，并且允许在训练开始时更频繁地更新教师。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ll"><img src="../Images/514202718def8a1ad2aa362dd23101d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*dJpyiOOkPrT0v2QpYBtFVw.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">θ is the student-mode model parameters. Δ the teacher-mode model parameters and is updated using the exponential moving average (<a class="ae ka" href="https://en.wikipedia.org/wiki/Moving_average#%7B%7Banchor%7Cexponential_moving_average%7CExponential%7D%7DExponential_moving_average" rel="noopener ugc nofollow" target="_blank">EMA</a> [7, 8]) of θ. τ linearly increases from τ₀ to a target value τₑ for the first τₙ updates and then stays constant for the remaining of the training. Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="f965" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">然后，通过使用变压器的顶部K个(更靠近输出)模块来构建目标。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lm"><img src="../Images/85b6eede2eefcd46a50d507eb7628fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*v3HA16lcKmWCfnHmquDbOg.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">L is the total blocks in the network. aₜˡ-circumflex (cannot type the caret symbol) is obtained by a normalization from aₜˡ. aₜˡ denotes the output of block <em class="lg">l</em> at timestep t. Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="ac00" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">3.目标函数</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ln"><img src="../Images/d395686d082f3f68a0f177dfe1ad2971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G23_truB_bBusqWa28jS8Q.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">yₜ is the target. fₜ(x) is the prediction. β controls the transition from a squared loss to an L₁ loss. When the gap is large, L₁ is used to make the loss less sensitive to outliers. Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="29ed" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">总之，教师和学生具有动态行为:学生的参数通过在步骤3中优化目标函数来更新，而教师的参数通过在步骤1中计算EMA来更新。这种动态被认为是防止模型崩溃成一个常数表示[7]。以后再写博客介绍[7]。</p></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="8aba" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">结果:</strong></p><p id="0e8d" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">该文件报告了所有三种模式的SOTA结果。</p><ol class=""><li id="46d7" class="ji jj hh jk b jl jm jn jo jp jq jr js jt ju jv lj jx jy jz bi translated">图像(公制:精确度。价值越高，性能越好):</li></ol><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lo"><img src="../Images/4bca7953f0d9370c3dd40081ea83b3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmMr9DPFulHsHSuB-tlrIQ.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="5f71" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">2.语音(度量:单词错误率。值越低，性能越好)</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es li"><img src="../Images/b179f8619e6d273853c581bbdb8458b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N68U2S6eyT-oBX3WHmbC5A.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="f9cf" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">3.文本(度量:粘合分数。价值越高，性能越好)</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es li"><img src="../Images/591637d95602d7dd68eaa345866fb689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6l1LfcHRnqqN2S3ukfHAw.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="56bd" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">消融研究:</strong></p><ol class=""><li id="cbe5" class="ji jj hh jk b jl jm jn jo jp jq jr js jt ju jv lj jx jy jz bi translated">前K个块</li></ol><p id="c7ab" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">该论文认为，在教师模式中使用前K个块的平均值比仅使用前一个更好(在论文的这一部分中，作者使用“前K个层”，这与论文的“目标”部分中的“前K个块”不一致。我假设“层”和“块”可以互换使用，都指转换器中的块)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es li"><img src="../Images/770e1d466f6a935f5b8dd057df85ed4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LA_BR4c0F9t-_aJC7q6m5A.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Using the average of top K blocks of the teacher model is better than just using the single top block. The results shown have better performance when the value is lower(speech), higher(NLP, i.e., texts) and higher(Vision, i.e., images ) respectively. The effect is more pronounced in speech and texts than in images. Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure><p id="687b" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">2.目标特征类型</p><p id="5401" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">除了使用顶部的K块，作者还尝试使用教师模式的不同部分，并发现使用FFN是最好的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lp"><img src="../Images/9fa6142b6a49a73f69b82a82cd3814a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*Uhk3rFnEMUcFa8i9z7TLZw.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx">WER stands for word error rate. Image from <a class="ae ka" href="https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language" rel="noopener ugc nofollow" target="_blank">the original paper</a>.</figcaption></figure></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="a3eb" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">结论:</strong>本文介绍了一种新的通用自监督学习框架，并实现了三种模态的SOTA性能。</p><p id="52a5" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">该框架包括一个模型和两种模式:教师和学生。教师得到完整的样本输入，而学生得到相同样本的屏蔽输入。自我监督学习是通过让学生向老师学习来实现的。</p></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="5650" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">个人备注:</strong></p><ol class=""><li id="ea7b" class="ji jj hh jk b jl jm jn jo jp jq jr js jt ju jv lj jx jy jz bi translated">更有意思的是，看看这种方法在非结构化的模态上表现如何，比如图形:P</li><li id="0fa4" class="ji jj hh jk b jl kb jn kc jp kd jr ke jt kf jv lj jx jy jz bi translated">变压器和<a class="ae ka" rel="noopener" href="/mlearning-ai/read-paper-with-me-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-e6580ce8dae5"> BYOL </a>在这种方法的成功中发挥了重要作用。Transformer是一种灵活的架构，不局限于特定的设备，因此可以应用于不同的设备。BYOL提供了这种方法的核心自我监督学习部分。</li><li id="0a30" class="ji jj hh jk b jl kb jn kc jp kd jr ke jt kf jv lj jx jy jz bi translated">这项工作是统一不同模态输入的关键步骤。由于人类很可能使用类似的学习过程来理解视觉世界，就像他们理解语言一样[9，10]，这项工作对于让我们更接近AGI具有重要意义。</li></ol></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="a75f" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">参考文献:</strong></p><p id="2684" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[1]鲍，h，董，l，和魏，F. <a class="ae ka" href="https://arxiv.org/abs/2106.08254" rel="noopener ugc nofollow" target="_blank">拜特:伯特预训练的图像变压器</a> (2021)。ArXiv</p><p id="6399" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[2] Dosovitskiy、a .、Beyer、l .、a .、Weissenborn、d .、Zhai、x .、Unterthiner、t .、Dehghani、m .、Minderer、m .、Heigold、g .、Gelly、s .、Uszkoreit、j .和Houlsby、N. <a class="ae ka" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">一幅图像相当于16x16个字:大规模图像识别的变形金刚</a> (2020)。arXiv</p><p id="0e8c" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[3] Baevski，A .，Zhou，y .，Mohamed，A .，Auli，M. <a class="ae ka" href="https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> wav2vec 2.0:语音表征的自我监督学习框架</a> (2020)。进行中。神经突</p><p id="5e17" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[4] Sennrich，r .，Haddow，b .，Birch，A. <a class="ae ka" href="https://aclanthology.org/P16-1162/" rel="noopener ugc nofollow" target="_blank">带子词单元的生僻字的神经机器翻译</a> (2016)。进行中。ACL的</p><p id="f89d" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[5] Devlin，j .，Chang，m-w .，Lee，k .，Toutanova，K. <a class="ae ka" href="https://aclanthology.org/N19-1423.pdf" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向转换器的预训练</a> (2019)。继续。纳克的</p><p id="3d8f" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[6]瓦斯瓦尼，a .，沙泽尔，n .，帕尔马，n .，乌兹科雷特，j .，琼斯，l .，戈麦斯，A. N .，凯泽，l .，波洛苏欣，I. <a class="ae ka" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)。进行中。乳头</p><p id="faff" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[7] Grill、J.-B .、Strub、f .、Altche、f .、Tallec、c .、Richemond、P. H .、Buchatskaya、e .、Doersch、c .、Pires、B. A .、Guo、Z. D .、Azar、M. G .、Piot、b .、Kavukcuoglu、k .、Munos、r .、Valko、M. <a class="ae ka" href="https://arxiv.org/abs/2006.07733" rel="noopener ugc nofollow" target="_blank">引导你自己的潜能:自我监督学习的新方法</a> (2020)。arXiv。</p><p id="777b" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[8] Caron，m .、Touvron，h .、Misra，I .、Jegou，h .、Mairal，j .、Bo- janowski，p .、Joulin，A. <a class="ae ka" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">《自我监督视觉变形金刚的新兴特性》</a> (2021)。arXiv</p><p id="9bd1" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[9] Friston，k .和Kiebel，S. <a class="ae ka" href="https://royalsocietypublishing.org/doi/10.1098/rstb.2008.0300" rel="noopener ugc nofollow" target="_blank">根据自由能原理的预测编码</a> (2009)。皇家学会哲学汇刊:生物科学</p><p id="b867" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated">[10] Friston，K. <a class="ae ka" href="https://www.nature.com/articles/nrn2787" rel="noopener ugc nofollow" target="_blank">自由能原理:统一的大脑理论？</a> (2010)，《自然评论神经科学》</p></div><div class="ab cl kg kh go ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="ha hb hc hd he"><p id="938c" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><strong class="jk hi">延伸阅读:</strong></p><p id="5dca" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><a class="ae ka" rel="noopener" href="/@YunchaoLanceLiu/read-paper-with-me-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-e6580ce8dae5">BYOL</a>(data 2 vec构建的方法)</p><p id="15bf" class="pw-post-body-paragraph kn ko hh jk b jl jm ii kp jn jo il kq jp kr ks kt jr ku kv kw jt kx ky kz jv ha bi translated"><a class="ae ka" href="https://jimmy-shen.medium.com/finally-we-have-a-more-general-modality-unspecific-representation-method-data2vec-5dcba6c853ef" rel="noopener"> Jimmy Chen在data2vec上的博客</a></p></div></div>    
</body>
</html>