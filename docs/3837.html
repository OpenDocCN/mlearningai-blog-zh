<html>
<head>
<title>Zero to Hero in hyperparameter optimization of machine learning with Code: Classification: Part -1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">代码机器学习的超参数优化:分类:第一部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/zero-to-hero-in-hyperparameter-optimization-of-machine-learning-with-code-classification-part-1-3d6e01834ca2?source=collection_archive---------2-----------------------#2022-10-29">https://medium.com/mlearning-ai/zero-to-hero-in-hyperparameter-optimization-of-machine-learning-with-code-classification-part-1-3d6e01834ca2?source=collection_archive---------2-----------------------#2022-10-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="c9e3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">机器学习算法已经广泛应用于各种应用和领域。为了使机器学习模型适合不同的问题，必须调整其超参数。为机器学习模型选择最佳超参数配置对模型的性能有直接影响。它通常需要深入了解机器学习算法和适当的超参数优化技术。虽然存在几种自动优化技术，但是当应用于不同类型的问题时，它们具有不同的优点和缺点。</p><p id="1d4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先我们将做分类的超参数优化。</p><p id="6e68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">分类算法:</strong></p><p id="e371" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将应用以下算法</p><ol class=""><li id="59d6" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">随机森林</li><li id="1d40" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">支持向量机(SVM)</li><li id="7c61" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">k近邻(KNN)</li><li id="4b4e" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">人工神经网络</li></ol><p id="9556" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">超参数优化技术:</strong></p><ol class=""><li id="5967" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">网格搜索</li><li id="50a0" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">随机搜索</li><li id="5996" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">超波段</li><li id="db41" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">高斯过程贝叶斯优化(BO-GP)</li><li id="52cd" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">基于树结构Parzen估计量的贝叶斯优化</li><li id="4f22" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">粒子群优化算法</li><li id="b362" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">遗传算法。</li></ol><figure class="jq jr js jt fd ju"><div class="bz dy l di"><div class="jv jw l"/></div></figure><h1 id="89c6" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">加载MNIST数据集</h1><p id="f03f" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">MNIST数据库(改进的国家标准和技术研究所数据库)是一个手写数字的大型数据库，通常用于训练各种图像处理系统。MNIST数据库具有60，000个样本的训练集和10，000个样本的测试集。这是从NIST可获得的更大集合的子集。数字已经过大小标准化，并在固定大小的图像中居中。</p><figure class="jq jr js jt fd ju"><div class="bz dy l di"><div class="jv jw l"/></div></figure><h1 id="8ded" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">具有默认超参数的分类器</h1><p id="32dc" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">随机森林:</p><figure class="jq jr js jt fd ju"><div class="bz dy l di"><div class="jv jw l"/></div></figure><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="7fcf" class="lf jy hh lb b fi lg lh l li lj">Accuracy:0.896553659132438</span></pre><p id="1339" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM:</p><figure class="jq jr js jt fd ju"><div class="bz dy l di"><div class="jv jw l"/></div></figure><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="2563" class="lf jy hh lb b fi lg lh l li lj">Accuracy:0.9705030401091262</span></pre><p id="6ba3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="ba4e" class="lf jy hh lb b fi lg lh l li lj">clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>clf<strong class="lb hi">.</strong>fit(X,y)<br/>scores <strong class="lb hi">=</strong> cross_val_score(clf, X, y, cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(scores<strong class="lb hi">.</strong>mean()))</span><span id="929d" class="lf jy hh lb b fi lk lh l li lj">Accuracy:0.9627317178438357</span></pre><p id="f4af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">安:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="9c3f" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> keras.models <strong class="lb hi">import</strong> Sequential, Model<br/><strong class="lb hi">from</strong> keras.layers <strong class="lb hi">import</strong> Dense, Input<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> GridSearchCV<br/><strong class="lb hi">from</strong> keras.wrappers.scikit_learn <strong class="lb hi">import</strong> KerasClassifier<br/><strong class="lb hi">from</strong> keras.callbacks <strong class="lb hi">import</strong> EarlyStopping<br/><strong class="lb hi">def</strong> ANN(optimizer <strong class="lb hi">=</strong> 'sgd',neurons<strong class="lb hi">=</strong>32,batch_size<strong class="lb hi">=</strong>32,epochs<strong class="lb hi">=</strong>20,activation<strong class="lb hi">=</strong>'relu',patience<strong class="lb hi">=</strong>3,loss<strong class="lb hi">=</strong>'categorical_crossentropy'):<br/>    model <strong class="lb hi">=</strong> Sequential()<br/>    model<strong class="lb hi">.</strong>add(Dense(neurons, input_shape<strong class="lb hi">=</strong>(X<strong class="lb hi">.</strong>shape[1],), activation<strong class="lb hi">=</strong>activation))<br/>    model<strong class="lb hi">.</strong>add(Dense(neurons, activation<strong class="lb hi">=</strong>activation))<br/>    model<strong class="lb hi">.</strong>add(Dense(10,activation<strong class="lb hi">=</strong>'softmax'))  <em class="ll"># 10 is the number of classes in the dataset, you can change it based on your dataset</em><br/>    model<strong class="lb hi">.</strong>compile(optimizer <strong class="lb hi">=</strong> optimizer, loss<strong class="lb hi">=</strong>loss)<br/>    early_stopping <strong class="lb hi">=</strong> EarlyStopping(monitor<strong class="lb hi">=</strong>"loss", patience <strong class="lb hi">=</strong> patience)<em class="ll"># early stop patience</em><br/>    history <strong class="lb hi">=</strong> model<strong class="lb hi">.</strong>fit(X, pd<strong class="lb hi">.</strong>get_dummies(y)<strong class="lb hi">.</strong>values,<br/>              batch_size<strong class="lb hi">=</strong>batch_size,<br/>              epochs<strong class="lb hi">=</strong>epochs,<br/>              callbacks <strong class="lb hi">=</strong> [early_stopping],<br/>              verbose<strong class="lb hi">=</strong>0) <em class="ll">#verbose set to 1 will show the training </em><br/>    <strong class="lb hi">return</strong> model</span><span id="7ccb" class="lf jy hh lb b fi lk lh l li lj">clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/>scores <strong class="lb hi">=</strong> cross_val_score(clf, X, y, cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(scores<strong class="lb hi">.</strong>mean()))</span><span id="0850" class="lf jy hh lb b fi lk lh l li lj">Accuracy:0.9894268224819144</span></pre><p id="349b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们将尝试超参数优化技术。</p><h1 id="9a92" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">1.网格搜索</h1><p id="c9b8" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="b49e" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">简单的实现。</li></ul><p id="4db5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="eff0" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">耗时，</li><li id="710a" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">仅对分类HPs有效。</li></ul><p id="b2ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="25ca" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> GridSearchCV<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': [10, 20, 30],<br/>    <em class="ll">#'max_features': ['sqrt',0.5],</em><br/>    'max_depth': [15,20,30,50],<br/>    <em class="ll">#'min_samples_leaf': [1,2,4,8],</em><br/>    <em class="ll">#"bootstrap":[True,False],</em><br/>    "criterion":['gini','entropy']<br/>}<br/>clf <strong class="lb hi">=</strong> RandomForestClassifier(random_state<strong class="lb hi">=</strong>0)<br/>grid <strong class="lb hi">=</strong> GridSearchCV(clf, rf_params, cv<strong class="lb hi">=</strong>3, scoring<strong class="lb hi">=</strong>'accuracy')<br/>grid<strong class="lb hi">.</strong>fit(X, y)<br/>print(grid<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(grid<strong class="lb hi">.</strong>best_score_))</span><span id="2b3a" class="lf jy hh lb b fi lk lh l li lj">{'n_estimators': 30, 'max_depth': 15, 'criterion': 'entropy'}<br/>Accuracy:0.9332220367278798</span></pre><p id="eba8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="c309" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> GridSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'C': [1,10, 100],<br/>    "kernel":['linear','poly','rbf','sigmoid']<br/>}<br/>clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale')<br/>grid <strong class="lb hi">=</strong> GridSearchCV(clf, rf_params, cv<strong class="lb hi">=</strong>3, scoring<strong class="lb hi">=</strong>'accuracy')<br/>grid<strong class="lb hi">.</strong>fit(X, y)<br/>print(grid<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(grid<strong class="lb hi">.</strong>best_score_))</span><span id="3fb1" class="lf jy hh lb b fi lk lh l li lj">{'kernel': 'rbf', 'C': 10}<br/>Accuracy:0.9744017807456873</span></pre><p id="68c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="f9ed" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> GridSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_neighbors': [2, 3, 5,10,15,20],<br/>}<br/>clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>grid <strong class="lb hi">=</strong> GridSearchCV(clf, rf_params, cv<strong class="lb hi">=</strong>3, scoring<strong class="lb hi">=</strong>'accuracy')<br/>grid<strong class="lb hi">.</strong>fit(X, y)<br/>print(grid<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(grid<strong class="lb hi">.</strong>best_score_))</span><span id="3619" class="lf jy hh lb b fi lk lh l li lj">{'n_neighbors': 3}<br/>Accuracy:0.9682804674457429</span></pre><p id="9190" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="f235" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> GridSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'optimizer': ['adam','rmsprop','sgd'],<br/>    'activation': ['relu','tanh'],<br/>    'batch_size': [16,32],<br/>    'neurons':[16,32],<br/>    'epochs':[20,50],<br/>    'patience':[2,5]<br/>}<br/>clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/>grid <strong class="lb hi">=</strong> GridSearchCV(clf, rf_params, cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>grid<strong class="lb hi">.</strong>fit(X, y)<br/>print(grid<strong class="lb hi">.</strong>best_params_)<br/>print("MSE:"<strong class="lb hi">+</strong> str(grid<strong class="lb hi">.</strong>best_score_))</span><span id="dd55" class="lf jy hh lb b fi lk lh l li lj">{'activation': 'tanh', 'epochs': 50, 'optimizer': 'adam', 'patience': 5, 'batch_size': 32, 'neurons': 32}<br/>MSE:0.9961046188091264</span></pre><h1 id="3ae9" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">2:随机搜索</h1><p id="3aa6" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">在搜索空间中随机搜索超参数组合</p><p id="2963" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="21fc" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">比GridSearch更高效。</li><li id="5c25" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">启用并行化。</li></ul><p id="be2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="d922" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">不考虑以前的结果。</li><li id="5df7" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">对条件超参数无效。</li></ul><p id="c9dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="219b" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> random <strong class="lb hi">import</strong> randrange <strong class="lb hi">as</strong> sp_randrange<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> RandomizedSearchCV<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': sp_randint(10,100),<br/>    "max_features":sp_randint(1,64),<br/>    'max_depth': sp_randint(5,50),<br/>    "min_samples_split":sp_randint(2,11),<br/>    "min_samples_leaf":sp_randint(1,11),<br/>    "criterion":['gini','entropy']<br/>}<br/>n_iter_search<strong class="lb hi">=</strong>20 <em class="ll">#number of iterations is set to 20, you can increase this number if time permits</em><br/>clf <strong class="lb hi">=</strong> RandomForestClassifier(random_state<strong class="lb hi">=</strong>0)<br/>Random <strong class="lb hi">=</strong> RandomizedSearchCV(clf, param_distributions<strong class="lb hi">=</strong>rf_params,n_iter<strong class="lb hi">=</strong>n_iter_search,cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Random<strong class="lb hi">.</strong>fit(X, y)<br/>print(Random<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Random<strong class="lb hi">.</strong>best_score_))</span><span id="d7a2" class="lf jy hh lb b fi lk lh l li lj">{'n_estimators': 36, 'max_features': 6, 'max_depth': 42, 'min_samples_split': 6, 'min_samples_leaf': 2, 'criterion': 'gini'}<br/>Accuracy:0.9309961046188091</span></pre><p id="1a70" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="aaf7" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> RandomizedSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'C': stats<strong class="lb hi">.</strong>uniform(0,50),<br/>    "kernel":['linear','poly','rbf','sigmoid']<br/>}<br/>n_iter_search<strong class="lb hi">=</strong>20<br/>clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale')<br/>Random <strong class="lb hi">=</strong> RandomizedSearchCV(clf, param_distributions<strong class="lb hi">=</strong>rf_params,n_iter<strong class="lb hi">=</strong>n_iter_search,cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Random<strong class="lb hi">.</strong>fit(X, y)<br/>print(Random<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Random<strong class="lb hi">.</strong>best_score_))</span><span id="84ee" class="lf jy hh lb b fi lk lh l li lj">{'kernel': 'rbf', 'C': 17.026713515892954}<br/>Accuracy:0.9744017807456873</span></pre><p id="ca8a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="68a9" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> RandomizedSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_neighbors': range(1,20),<br/>}<br/>n_iter_search<strong class="lb hi">=</strong>10<br/>clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>Random <strong class="lb hi">=</strong> RandomizedSearchCV(clf, param_distributions<strong class="lb hi">=</strong>rf_params,n_iter<strong class="lb hi">=</strong>n_iter_search,cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Random<strong class="lb hi">.</strong>fit(X, y)<br/>print(Random<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Random<strong class="lb hi">.</strong>best_score_))</span><span id="536a" class="lf jy hh lb b fi lk lh l li lj">{'n_neighbors': 4}<br/>Accuracy:0.9643850862548692</span></pre><p id="4f25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="8ef4" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> random <strong class="lb hi">import</strong> randrange <strong class="lb hi">as</strong> sp_randrange<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> RandomizedSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'optimizer': ['adam','rmsprop','sgd'],<br/>    'activation': ['relu','tanh'],<br/>    'batch_size': [16,32,64],<br/>    'neurons':sp_randint(10,100),<br/>    'epochs':[20,50],<br/>    <em class="ll">#'epochs':[20,50,100,200],</em><br/>    'patience':sp_randint(3,20)<br/>}<br/>n_iter_search<strong class="lb hi">=</strong>10<br/>clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/>Random <strong class="lb hi">=</strong> RandomizedSearchCV(clf, param_distributions<strong class="lb hi">=</strong>rf_params,n_iter<strong class="lb hi">=</strong>n_iter_search,cv<strong class="lb hi">=</strong>3,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Random<strong class="lb hi">.</strong>fit(X, y)<br/>print(Random<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Random<strong class="lb hi">.</strong>best_score_))</span><span id="40c9" class="lf jy hh lb b fi lk lh l li lj">{'activation': 'relu', 'epochs': 20, 'optimizer': 'adam', 'patience': 8, 'batch_size': 16, 'neurons': 89}<br/>Accuracy:1.0</span></pre><h1 id="a149" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">3:超波段</h1><p id="3f92" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">生成小型子集，并根据其性能为每个超参数组合分配预算</p><p id="f225" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="a296" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">启用并行化。</li></ul><p id="a7a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="fa50" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">对有条件的HPs无效。</li><li id="320c" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">要求预算较小的子集具有代表性。</li></ul><p id="2b97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="27cf" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> hyperband <strong class="lb hi">import</strong> HyperbandSearchCV<br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> random <strong class="lb hi">import</strong> randrange <strong class="lb hi">as</strong> sp_randrange<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': sp_randint(10,100),<br/>    "max_features":sp_randint(1,64),<br/>    'max_depth': sp_randint(5,50),<br/>    "min_samples_split":sp_randint(2,11),<br/>    "min_samples_leaf":sp_randint(1,11),<br/>    "criterion":['gini','entropy']<br/>}<br/>clf <strong class="lb hi">=</strong> RandomForestClassifier(random_state<strong class="lb hi">=</strong>0)<br/>hyper <strong class="lb hi">=</strong> HyperbandSearchCV(clf, param_distributions <strong class="lb hi">=</strong>rf_params,cv<strong class="lb hi">=</strong>3,min_iter<strong class="lb hi">=</strong>10,max_iter<strong class="lb hi">=</strong>100,scoring<strong class="lb hi">=</strong>'accuracy')<br/>hyper<strong class="lb hi">.</strong>fit(X, y)<br/>print(hyper<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(hyper<strong class="lb hi">.</strong>best_score_))</span><span id="9a2a" class="lf jy hh lb b fi lk lh l li lj">{'n_estimators': 100, 'max_features': 9, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 2, 'criterion': 'gini'}<br/>Accuracy:0.9321090706733445</span></pre><p id="e56a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="9078" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> hyperband <strong class="lb hi">import</strong> HyperbandSearchCV<br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> random <strong class="lb hi">import</strong> randrange <strong class="lb hi">as</strong> sp_randrange<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'C': stats<strong class="lb hi">.</strong>uniform(0,50),<br/>    "kernel":['linear','poly','rbf','sigmoid']<br/>}<br/>clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale')<br/>hyper <strong class="lb hi">=</strong> HyperbandSearchCV(clf, param_distributions <strong class="lb hi">=</strong>rf_params,cv<strong class="lb hi">=</strong>3,min_iter<strong class="lb hi">=</strong>1,max_iter<strong class="lb hi">=</strong>50,scoring<strong class="lb hi">=</strong>'accuracy',resource_param<strong class="lb hi">=</strong>'C')<br/>hyper<strong class="lb hi">.</strong>fit(X, y)<br/>print(hyper<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(hyper<strong class="lb hi">.</strong>best_score_))</span><span id="58d9" class="lf jy hh lb b fi lk lh l li lj">{'kernel': 'rbf', 'C': 16}<br/>Accuracy:0.9744017807456873</span></pre><p id="be7c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="cbae" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> hyperband <strong class="lb hi">import</strong> HyperbandSearchCV<br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/><strong class="lb hi">from</strong> random <strong class="lb hi">import</strong> randrange <strong class="lb hi">as</strong> sp_randrange<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_neighbors': range(1,20),<br/>}<br/>clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>hyper <strong class="lb hi">=</strong> HyperbandSearchCV(clf, param_distributions <strong class="lb hi">=</strong>rf_params,cv<strong class="lb hi">=</strong>3,min_iter<strong class="lb hi">=</strong>1,max_iter<strong class="lb hi">=</strong>20,scoring<strong class="lb hi">=</strong>'accuracy',resource_param<strong class="lb hi">=</strong>'n_neighbors')<br/>hyper<strong class="lb hi">.</strong>fit(X, y)<br/>print(hyper<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(hyper<strong class="lb hi">.</strong>best_score_))</span><span id="864c" class="lf jy hh lb b fi lk lh l li lj">{'n_neighbors': 2}<br/>Accuracy:0.9621591541457986</span></pre><p id="6937" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="fa0f" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> hyperband <strong class="lb hi">import</strong> HyperbandSearchCV<br/><strong class="lb hi">from</strong> scipy.stats <strong class="lb hi">import</strong> randint <strong class="lb hi">as</strong> sp_randint<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'optimizer': ['adam','rmsprop','sgd'],<br/>    'activation': ['relu','tanh'],<br/>    'batch_size': [16,32,64],<br/>    'neurons':sp_randint(10,100),<br/>    'epochs':[20,50],<br/>    <em class="ll">#'epochs':[20,50,100,200],</em><br/>    'patience':sp_randint(3,20)<br/>}<br/>clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, epochs<strong class="lb hi">=</strong>20, verbose<strong class="lb hi">=</strong>0)<br/>hyper <strong class="lb hi">=</strong> HyperbandSearchCV(clf, param_distributions <strong class="lb hi">=</strong>rf_params,cv<strong class="lb hi">=</strong>3,min_iter<strong class="lb hi">=</strong>1,max_iter<strong class="lb hi">=</strong>10,scoring<strong class="lb hi">=</strong>'accuracy',resource_param<strong class="lb hi">=</strong>'epochs')<br/>hyper<strong class="lb hi">.</strong>fit(X, y)<br/>print(hyper<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(hyper<strong class="lb hi">.</strong>best_score_))</span><span id="f777" class="lf jy hh lb b fi lk lh l li lj">{'activation': 'tanh', 'epochs': 10, 'batch_size': 16, 'patience': 7, 'optimizer': 'adam', 'neurons': 76}<br/>Accuracy:0.9994435169727324</span></pre><h1 id="5fff" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">4:高斯过程贝叶斯优化(BO-GP)</h1><p id="f7aa" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="8e06" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">连续HPs的快速收敛速度。</li></ul><p id="af68" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="ffce" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">并行能力差。</li><li id="3c66" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">对有条件的HPs无效。</li></ul><p id="1df1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="7a09" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> Optimizer<br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> BayesSearchCV <br/><strong class="lb hi">from</strong> skopt.space <strong class="lb hi">import</strong> Real, Categorical, Integer<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': Integer(10,100),<br/>    "max_features":Integer(1,64),<br/>    'max_depth': Integer(5,50),<br/>    "min_samples_split":Integer(2,11),<br/>    "min_samples_leaf":Integer(1,11),<br/>    "criterion":['gini','entropy']<br/>}<br/>clf <strong class="lb hi">=</strong> RandomForestClassifier(random_state<strong class="lb hi">=</strong>0)<br/>Bayes <strong class="lb hi">=</strong> BayesSearchCV(clf, rf_params,cv<strong class="lb hi">=</strong>3,n_iter<strong class="lb hi">=</strong>20, n_jobs<strong class="lb hi">=-</strong>1,scoring<strong class="lb hi">=</strong>'accuracy')<br/><em class="ll">#number of iterations is set to 20, you can increase this number if time permits</em><br/>Bayes<strong class="lb hi">.</strong>fit(X, y)<br/>print(Bayes<strong class="lb hi">.</strong>best_params_)<br/>bclf <strong class="lb hi">=</strong> Bayes<strong class="lb hi">.</strong>best_estimator_<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Bayes<strong class="lb hi">.</strong>best_score_))</span><span id="ed92" class="lf jy hh lb b fi lk lh l li lj">{'n_estimators': 100, 'max_features': 1, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'gini'}<br/>Accuracy:0.9398998330550918</span></pre><p id="562d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="b5b4" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> Optimizer<br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> BayesSearchCV <br/><strong class="lb hi">from</strong> skopt.space <strong class="lb hi">import</strong> Real, Categorical, Integer<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'C': Real(0.01,50),<br/>    "kernel":['linear','poly','rbf','sigmoid']<br/>}<br/>clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale')<br/>Bayes <strong class="lb hi">=</strong> BayesSearchCV(clf, rf_params,cv<strong class="lb hi">=</strong>3,n_iter<strong class="lb hi">=</strong>20, n_jobs<strong class="lb hi">=-</strong>1,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Bayes<strong class="lb hi">.</strong>fit(X, y)<br/>print(Bayes<strong class="lb hi">.</strong>best_params_)<br/>bclf <strong class="lb hi">=</strong> Bayes<strong class="lb hi">.</strong>best_estimator_<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Bayes<strong class="lb hi">.</strong>best_score_))</span><span id="deb0" class="lf jy hh lb b fi lk lh l li lj">{'kernel': 'rbf', 'C': 26.52140440440126}<br/>Accuracy:0.9744017807456873</span></pre><p id="bc96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="b47f" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> Optimizer<br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> BayesSearchCV <br/><strong class="lb hi">from</strong> skopt.space <strong class="lb hi">import</strong> Real, Categorical, Integer<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_neighbors': Integer(1,20),<br/>}<br/>clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>Bayes <strong class="lb hi">=</strong> BayesSearchCV(clf, rf_params,cv<strong class="lb hi">=</strong>3,n_iter<strong class="lb hi">=</strong>10, n_jobs<strong class="lb hi">=-</strong>1,scoring<strong class="lb hi">=</strong>'accuracy')<br/>Bayes<strong class="lb hi">.</strong>fit(X, y)<br/>print(Bayes<strong class="lb hi">.</strong>best_params_)<br/>bclf <strong class="lb hi">=</strong> Bayes<strong class="lb hi">.</strong>best_estimator_<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Bayes<strong class="lb hi">.</strong>best_score_))</span><span id="f3bf" class="lf jy hh lb b fi lk lh l li lj">{'n_neighbors': 3}</span></pre><p id="24e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="ff4c" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> Optimizer<br/><strong class="lb hi">from</strong> skopt <strong class="lb hi">import</strong> BayesSearchCV <br/><strong class="lb hi">from</strong> skopt.space <strong class="lb hi">import</strong> Real, Categorical, Integer<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'optimizer': ['adam','rmsprop','sgd'],<br/>    'activation': ['relu','tanh'],<br/>    'batch_size': [16,32,64],<br/>    'neurons':Integer(10,100),<br/>    'epochs':[20,50],<br/>    <em class="ll">#'epochs':[20,50,100,200],</em><br/>    'patience':Integer(3,20)<br/>}<br/>clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/>Bayes <strong class="lb hi">=</strong> BayesSearchCV(clf, rf_params,cv<strong class="lb hi">=</strong>3,n_iter<strong class="lb hi">=</strong>10, scoring<strong class="lb hi">=</strong>'accuracy')<br/>Bayes<strong class="lb hi">.</strong>fit(X, y)<br/>print(Bayes<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(Bayes<strong class="lb hi">.</strong>best_score_))</span><span id="e63f" class="lf jy hh lb b fi lk lh l li lj">{'activation': 'tanh', 'epochs': 47, 'optimizer': 'adam', 'patience': 10, 'batch_size': 16, 'neurons': 54}<br/>Accuracy:1.0</span></pre><h1 id="369c" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">5:使用树结构Parzen估计量BO-TPE的贝叶斯优化</h1><p id="1d9d" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="3c1c" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">适用于所有类型的HP。</li><li id="5b37" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">保留条件依赖。</li></ul><p id="f848" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="7c40" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">并行能力差。</li></ul><p id="5d4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="005e" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> hyperopt <strong class="lb hi">import</strong> hp, fmin, tpe, STATUS_OK, Trials<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> cross_val_score, StratifiedKFold<br/><em class="ll"># Define the objective function</em><br/><strong class="lb hi">def</strong> objective(params):<br/>    params <strong class="lb hi">=</strong> {<br/>        'n_estimators': int(params['n_estimators']), <br/>        'max_depth': int(params['max_depth']),<br/>        'max_features': int(params['max_features']),<br/>        "min_samples_split":int(params['min_samples_split']),<br/>        "min_samples_leaf":int(params['min_samples_leaf']),<br/>        "criterion":str(params['criterion'])<br/>    }<br/>    clf <strong class="lb hi">=</strong> RandomForestClassifier( <strong class="lb hi">**</strong>params)<br/>    score <strong class="lb hi">=</strong> cross_val_score(clf, X, y, scoring<strong class="lb hi">=</strong>'accuracy', cv<strong class="lb hi">=</strong>StratifiedKFold(n_splits<strong class="lb hi">=</strong>3))<strong class="lb hi">.</strong>mean()<br/>    <em class="ll">#print("ROC-AUC {:.3f} params {}".format(score, params))</em><br/><br/>    <strong class="lb hi">return</strong> {'loss':<strong class="lb hi">-</strong>score, 'status': STATUS_OK }<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>space <strong class="lb hi">=</strong> {<br/>    'n_estimators': hp<strong class="lb hi">.</strong>quniform('n_estimators', 10, 100, 1),<br/>    'max_depth': hp<strong class="lb hi">.</strong>quniform('max_depth', 5, 50, 1),<br/>    "max_features":hp<strong class="lb hi">.</strong>quniform('max_features', 1, 64, 1),<br/>    "min_samples_split":hp<strong class="lb hi">.</strong>quniform('min_samples_split',2,11,1),<br/>    "min_samples_leaf":hp<strong class="lb hi">.</strong>quniform('min_samples_leaf',1,11,1),<br/>    "criterion":hp<strong class="lb hi">.</strong>choice('criterion',['gini','entropy'])<br/>}<br/><br/>best <strong class="lb hi">=</strong> fmin(fn<strong class="lb hi">=</strong>objective,<br/>            space<strong class="lb hi">=</strong>space,<br/>            algo<strong class="lb hi">=</strong>tpe<strong class="lb hi">.</strong>suggest,<br/>            max_evals<strong class="lb hi">=</strong>20)<br/>print("Random Forest: Hyperopt estimated optimum {}"<strong class="lb hi">.</strong>format(best))</span><span id="25c2" class="lf jy hh lb b fi lk lh l li lj">100%|██████████████████████████████████████████████████| 20/20 [00:11&lt;00:00,  1.76it/s, best loss: -0.9348679045482652]<br/>Random Forest: Hyperopt estimated optimum {'n_estimators': 95.0, 'max_features': 13.0, 'max_depth': 39.0, 'min_samples_split': 3.0, 'min_samples_leaf': 2.0, 'criterion': 0}</span></pre><p id="ce5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="3097" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> hyperopt <strong class="lb hi">import</strong> hp, fmin, tpe, STATUS_OK, Trials<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> cross_val_score, StratifiedKFold<br/><strong class="lb hi">def</strong> objective(params):<br/>    params <strong class="lb hi">=</strong> {<br/>        'C': abs(float(params['C'])), <br/>        "kernel":str(params['kernel'])<br/>    }<br/>    clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale', <strong class="lb hi">**</strong>params)<br/>    score <strong class="lb hi">=</strong> cross_val_score(clf, X, y, scoring<strong class="lb hi">=</strong>'accuracy', cv<strong class="lb hi">=</strong>StratifiedKFold(n_splits<strong class="lb hi">=</strong>3))<strong class="lb hi">.</strong>mean()<br/><br/>    <strong class="lb hi">return</strong> {'loss':<strong class="lb hi">-</strong>score, 'status': STATUS_OK }<br/><br/>space <strong class="lb hi">=</strong> {<br/>    'C': hp<strong class="lb hi">.</strong>normal('C', 0, 50),<br/>    "kernel":hp<strong class="lb hi">.</strong>choice('kernel',['linear','poly','rbf','sigmoid'])<br/>}<br/><br/>best <strong class="lb hi">=</strong> fmin(fn<strong class="lb hi">=</strong>objective,<br/>            space<strong class="lb hi">=</strong>space,<br/>            algo<strong class="lb hi">=</strong>tpe<strong class="lb hi">.</strong>suggest,<br/>            max_evals<strong class="lb hi">=</strong>20)<br/>print("SVM: Hyperopt estimated optimum {}"<strong class="lb hi">.</strong>format(best))</span><span id="3d2e" class="lf jy hh lb b fi lk lh l li lj">100%|██████████████████████████████████████████████████| 20/20 [00:02&lt;00:00,  7.18it/s, best loss: -0.9749661645191837]<br/>SVM: Hyperopt estimated optimum {'kernel': 2, 'C': 5.89740125613865}</span></pre><p id="ba22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="d95c" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> hyperopt <strong class="lb hi">import</strong> hp, fmin, tpe, STATUS_OK, Trials<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> cross_val_score, StratifiedKFold<br/><strong class="lb hi">def</strong> objective(params):<br/>    params <strong class="lb hi">=</strong> {<br/>        'n_neighbors': abs(int(params['n_neighbors']))<br/>    }<br/>    clf <strong class="lb hi">=</strong> KNeighborsClassifier( <strong class="lb hi">**</strong>params)<br/>    score <strong class="lb hi">=</strong> cross_val_score(clf, X, y, scoring<strong class="lb hi">=</strong>'accuracy', cv<strong class="lb hi">=</strong>StratifiedKFold(n_splits<strong class="lb hi">=</strong>3))<strong class="lb hi">.</strong>mean()<br/><br/>    <strong class="lb hi">return</strong> {'loss':<strong class="lb hi">-</strong>score, 'status': STATUS_OK }<br/><br/>space <strong class="lb hi">=</strong> {<br/>    'n_neighbors': hp<strong class="lb hi">.</strong>quniform('n_neighbors', 1, 20, 1),<br/>}<br/><br/>best <strong class="lb hi">=</strong> fmin(fn<strong class="lb hi">=</strong>objective,<br/>            space<strong class="lb hi">=</strong>space,<br/>            algo<strong class="lb hi">=</strong>tpe<strong class="lb hi">.</strong>suggest,<br/>            max_evals<strong class="lb hi">=</strong>10)<br/>print("KNN: Hyperopt estimated optimum {}"<strong class="lb hi">.</strong>format(best))</span><span id="75c8" class="lf jy hh lb b fi lk lh l li lj">100%|███████████████████████████████████████████████████| 10/10 [00:02&lt;00:00,  4.34it/s, best loss: -0.968293886616605]<br/>KNN: Hyperopt estimated optimum {'n_neighbors': 3.0}</span></pre><p id="7f90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="630b" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> hyperopt <strong class="lb hi">import</strong> hp, fmin, tpe, STATUS_OK, Trials<br/><strong class="lb hi">from</strong> sklearn.model_selection <strong class="lb hi">import</strong> cross_val_score, StratifiedKFold<br/><strong class="lb hi">def</strong> objective(params):<br/>    params <strong class="lb hi">=</strong> {<br/>        "optimizer":str(params['optimizer']),<br/>        "activation":str(params['activation']),<br/>        'batch_size': abs(int(params['batch_size'])),<br/>        'neurons': abs(int(params['neurons'])),<br/>        'epochs': abs(int(params['epochs'])),<br/>        'patience': abs(int(params['patience']))<br/>    }<br/>    clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN,<strong class="lb hi">**</strong>params, verbose<strong class="lb hi">=</strong>0)<br/>    score <strong class="lb hi">=</strong> <strong class="lb hi">-</strong>np<strong class="lb hi">.</strong>mean(cross_val_score(clf, X, y, cv<strong class="lb hi">=</strong>3, <br/>                                    scoring<strong class="lb hi">=</strong>"accuracy"))<br/><br/>    <strong class="lb hi">return</strong> {'loss':score, 'status': STATUS_OK }<br/><br/>space <strong class="lb hi">=</strong> {<br/>    "optimizer":hp<strong class="lb hi">.</strong>choice('optimizer',['adam','rmsprop','sgd']),<br/>    "activation":hp<strong class="lb hi">.</strong>choice('activation',['relu','tanh']),<br/>    'batch_size': hp<strong class="lb hi">.</strong>quniform('batch_size', 16, 64, 16),<br/>    'neurons': hp<strong class="lb hi">.</strong>quniform('neurons', 10, 100, 10),<br/>    'epochs': hp<strong class="lb hi">.</strong>quniform('epochs', 20, 50, 10),<br/>    'patience': hp<strong class="lb hi">.</strong>quniform('patience', 3, 20, 3),<br/>}<br/><br/>best <strong class="lb hi">=</strong> fmin(fn<strong class="lb hi">=</strong>objective,<br/>            space<strong class="lb hi">=</strong>space,<br/>            algo<strong class="lb hi">=</strong>tpe<strong class="lb hi">.</strong>suggest,<br/>            max_evals<strong class="lb hi">=</strong>10)<br/>print("ANN: Hyperopt estimated optimum {}"<strong class="lb hi">.</strong>format(best))</span><span id="8ed4" class="lf jy hh lb b fi lk lh l li lj">100%|█████████████████████████████████████████████████████████████████| 10/10 [06:29&lt;00:00, 38.92s/it, best loss: -1.0]<br/>ANN: Hyperopt estimated optimum {'activation': 1, 'epochs': 30.0, 'optimizer': 0, 'patience': 9.0, 'batch_size': 48.0, 'neurons': 60.0}</span></pre><h1 id="68f0" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">6:粒子群优化算法</h1><p id="fd99" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">粒子群优化(PSO):群中的每个粒子与其他粒子通信，在每次迭代中检测和更新当前的全局最优值，直到检测到最终的最优值。</p><p id="7e76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="9da2" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">适用于所有类型的HP。</li><li id="7ee3" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">启用并行化。</li></ul><p id="cb82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="f2bb" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">需要正确的初始化。</li></ul><p id="4b7b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="bb0a" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">import</strong> optunity<br/><strong class="lb hi">import</strong> optunity.metrics<br/><br/>data<strong class="lb hi">=</strong>X<br/>labels<strong class="lb hi">=</strong>y<strong class="lb hi">.</strong>tolist()<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>search <strong class="lb hi">=</strong> {<br/>    'n_estimators': [10, 100],<br/>    'max_features': [1, 64],<br/>    'max_depth': [5,50],<br/>    "min_samples_split":[2,11],<br/>    "min_samples_leaf":[1,11],<br/>    "criterion":[0,1]<br/>         }<br/><em class="ll"># Define the objective function</em><br/>@optunity<strong class="lb hi">.</strong>cross_validated(x<strong class="lb hi">=</strong>data, y<strong class="lb hi">=</strong>labels, num_folds<strong class="lb hi">=</strong>3)<br/><strong class="lb hi">def</strong> performance(x_train, y_train, x_test, y_test,n_estimators<strong class="lb hi">=None</strong>, max_features<strong class="lb hi">=None</strong>,max_depth<strong class="lb hi">=None</strong>,min_samples_split<strong class="lb hi">=None</strong>,min_samples_leaf<strong class="lb hi">=None</strong>,criterion<strong class="lb hi">=None</strong>):<br/>    <em class="ll"># fit the model</em><br/>    <strong class="lb hi">if</strong> criterion<strong class="lb hi">&lt;</strong>0.5:<br/>        cri<strong class="lb hi">=</strong>'gini'<br/>    <strong class="lb hi">else</strong>:<br/>        cri<strong class="lb hi">=</strong>'entropy'<br/>    model <strong class="lb hi">=</strong> RandomForestClassifier(n_estimators<strong class="lb hi">=</strong>int(n_estimators),<br/>                                   max_features<strong class="lb hi">=</strong>int(max_features),<br/>                                   max_depth<strong class="lb hi">=</strong>int(max_depth),<br/>                                   min_samples_split<strong class="lb hi">=</strong>int(min_samples_split),<br/>                                   min_samples_leaf<strong class="lb hi">=</strong>int(min_samples_leaf),<br/>                                   criterion<strong class="lb hi">=</strong>cri,<br/>                                  )<br/>    <em class="ll">#predictions = model.predict(x_test)</em><br/>    scores<strong class="lb hi">=</strong>np<strong class="lb hi">.</strong>mean(cross_val_score(model, X, y, cv<strong class="lb hi">=</strong>3, n_jobs<strong class="lb hi">=-</strong>1,<br/>                                    scoring<strong class="lb hi">=</strong>"accuracy"))<br/>    <em class="ll">#return optunity.metrics.roc_auc(y_test, predictions, positive=True)</em><br/>    <strong class="lb hi">return</strong> scores<em class="ll">#optunity.metrics.accuracy(y_test, predictions)</em><br/><br/>optimal_configuration, info, _ <strong class="lb hi">=</strong> optunity<strong class="lb hi">.</strong>maximize(performance,<br/>                                                  solver_name<strong class="lb hi">=</strong>'particle swarm',<br/>                                                  num_evals<strong class="lb hi">=</strong>20,<br/>                                                   <strong class="lb hi">**</strong>search<br/>                                                  )<br/>print(optimal_configuration)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(info<strong class="lb hi">.</strong>optimum))</span><span id="2a36" class="lf jy hh lb b fi lk lh l li lj">{'n_estimators': 72.7099609375, 'max_features': 7.49072265625, 'max_depth': 28.31298828125, 'min_samples_split': 10.63525390625, 'min_samples_leaf': 5.6337890625, 'criterion': 0.22998046875}<br/>Accuracy:0.9255791726758763</span></pre><p id="49f7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="99e8" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">import</strong> optunity<br/><strong class="lb hi">import</strong> optunity.metrics<br/><br/>data<strong class="lb hi">=</strong>X<br/>labels<strong class="lb hi">=</strong>y<strong class="lb hi">.</strong>tolist()<br/><br/>search <strong class="lb hi">=</strong> {<br/>    'C': (0,50),<br/>    'kernel':[0,4]<br/>         }<br/>@optunity<strong class="lb hi">.</strong>cross_validated(x<strong class="lb hi">=</strong>data, y<strong class="lb hi">=</strong>labels, num_folds<strong class="lb hi">=</strong>3)<br/><strong class="lb hi">def</strong> performance(x_train, y_train, x_test, y_test,C<strong class="lb hi">=None</strong>,kernel<strong class="lb hi">=None</strong>):<br/>    <em class="ll"># fit the model</em><br/>    <strong class="lb hi">if</strong> kernel<strong class="lb hi">&lt;</strong>1:<br/>        ke<strong class="lb hi">=</strong>'linear'<br/>    <strong class="lb hi">elif</strong> kernel<strong class="lb hi">&lt;</strong>2:<br/>        ke<strong class="lb hi">=</strong>'poly'<br/>    <strong class="lb hi">elif</strong> kernel<strong class="lb hi">&lt;</strong>3:<br/>        ke<strong class="lb hi">=</strong>'rbf'<br/>    <strong class="lb hi">else</strong>:<br/>        ke<strong class="lb hi">=</strong>'sigmoid'<br/>    model <strong class="lb hi">=</strong> SVC(C<strong class="lb hi">=</strong>float(C),<br/>                kernel<strong class="lb hi">=</strong>ke<br/>                                  )<br/>    <em class="ll">#predictions = model.predict(x_test)</em><br/>    scores<strong class="lb hi">=</strong>np<strong class="lb hi">.</strong>mean(cross_val_score(model, X, y, cv<strong class="lb hi">=</strong>3, n_jobs<strong class="lb hi">=-</strong>1,<br/>                                    scoring<strong class="lb hi">=</strong>"accuracy"))<br/>    <em class="ll">#return optunity.metrics.roc_auc(y_test, predictions, positive=True)</em><br/>    <strong class="lb hi">return</strong> scores<em class="ll">#optunity.metrics.accuracy(y_test, predictions)</em><br/><br/>optimal_configuration, info, _ <strong class="lb hi">=</strong> optunity<strong class="lb hi">.</strong>maximize(performance,<br/>                                                  solver_name<strong class="lb hi">=</strong>'particle swarm',<br/>                                                  num_evals<strong class="lb hi">=</strong>20,<br/>                                                   <strong class="lb hi">**</strong>search<br/>                                                  )<br/>print(optimal_configuration)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(info<strong class="lb hi">.</strong>optimum))</span><span id="6d54" class="lf jy hh lb b fi lk lh l li lj">{'kernel': 1.55078125, 'C': 47.998046875}<br/>Accuracy:0.9604833211865952</span></pre><p id="5e0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="2446" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">import</strong> optunity<br/><strong class="lb hi">import</strong> optunity.metrics<br/><br/>data<strong class="lb hi">=</strong>X<br/>labels<strong class="lb hi">=</strong>y<strong class="lb hi">.</strong>tolist()<br/><br/>search <strong class="lb hi">=</strong> {<br/>    'n_neighbors': [1, 20],<br/>         }<br/>@optunity<strong class="lb hi">.</strong>cross_validated(x<strong class="lb hi">=</strong>data, y<strong class="lb hi">=</strong>labels, num_folds<strong class="lb hi">=</strong>3)<br/><strong class="lb hi">def</strong> performance(x_train, y_train, x_test, y_test,n_neighbors<strong class="lb hi">=None</strong>):<br/>    <em class="ll"># fit the model</em><br/>    model <strong class="lb hi">=</strong> KNeighborsClassifier(n_neighbors<strong class="lb hi">=</strong>int(n_neighbors),<br/>                                  )<br/>    scores<strong class="lb hi">=</strong>np<strong class="lb hi">.</strong>mean(cross_val_score(model, X, y, cv<strong class="lb hi">=</strong>3, n_jobs<strong class="lb hi">=-</strong>1,<br/>                                    scoring<strong class="lb hi">=</strong>"accuracy"))<br/>    <strong class="lb hi">return</strong> scores<br/><br/>optimal_configuration, info, _ <strong class="lb hi">=</strong> optunity<strong class="lb hi">.</strong>maximize(performance,<br/>                                                  solver_name<strong class="lb hi">=</strong>'particle swarm',<br/>                                                  num_evals<strong class="lb hi">=</strong>10,<br/>                                                   <strong class="lb hi">**</strong>search<br/>                                                  )<br/>print(optimal_configuration)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(info<strong class="lb hi">.</strong>optimum))</span><span id="33ca" class="lf jy hh lb b fi lk lh l li lj">{'n_neighbors': 3.12451171875}<br/>Accuracy:0.968293886616605</span></pre><p id="8423" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="a97c" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">import</strong> optunity<br/><strong class="lb hi">import</strong> optunity.metrics<br/><br/>data<strong class="lb hi">=</strong>X<br/>labels<strong class="lb hi">=</strong>y<strong class="lb hi">.</strong>tolist()<br/><br/>search <strong class="lb hi">=</strong> {<br/>    'optimizer':[0,3],<br/>    'activation':[0,2],<br/>    'batch_size': [0, 2],<br/>    'neurons': [10, 100],<br/>    'epochs': [20, 50],<br/>    'patience': [3, 20],<br/>         }<br/>@optunity<strong class="lb hi">.</strong>cross_validated(x<strong class="lb hi">=</strong>data, y<strong class="lb hi">=</strong>labels, num_folds<strong class="lb hi">=</strong>3)<br/><strong class="lb hi">def</strong> performance(x_train, y_train, x_test, y_test,optimizer<strong class="lb hi">=None</strong>,activation<strong class="lb hi">=None</strong>,batch_size<strong class="lb hi">=None</strong>,neurons<strong class="lb hi">=None</strong>,epochs<strong class="lb hi">=None</strong>,patience<strong class="lb hi">=None</strong>):<br/>    <em class="ll"># fit the model</em><br/>    <strong class="lb hi">if</strong> optimizer<strong class="lb hi">&lt;</strong>1:<br/>        op<strong class="lb hi">=</strong>'adam'<br/>    <strong class="lb hi">elif</strong> optimizer<strong class="lb hi">&lt;</strong>2:<br/>        op<strong class="lb hi">=</strong>'sgd'<br/>    <strong class="lb hi">else</strong>:<br/>        op<strong class="lb hi">=</strong>'rmsprop'<br/>    <strong class="lb hi">if</strong> activation<strong class="lb hi">&lt;</strong>1:<br/>        ac<strong class="lb hi">=</strong>'relu'<br/>    <strong class="lb hi">else</strong>:<br/>        ac<strong class="lb hi">=</strong>'tanh'<br/>    <strong class="lb hi">if</strong> batch_size<strong class="lb hi">&lt;</strong>1:<br/>        ba<strong class="lb hi">=</strong>16<br/>    <strong class="lb hi">else</strong>:<br/>        ba<strong class="lb hi">=</strong>32<br/>    model <strong class="lb hi">=</strong> ANN(optimizer<strong class="lb hi">=</strong>op,<br/>                activation<strong class="lb hi">=</strong>ac,<br/>                batch_size<strong class="lb hi">=</strong>ba,<br/>                neurons<strong class="lb hi">=</strong>int(neurons),<br/>                epochs<strong class="lb hi">=</strong>int(epochs),<br/>                patience<strong class="lb hi">=</strong>int(patience)<br/>                                  )<br/>    clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/>    scores<strong class="lb hi">=</strong>np<strong class="lb hi">.</strong>mean(cross_val_score(clf, X, y, cv<strong class="lb hi">=</strong>3, <br/>                                    scoring<strong class="lb hi">=</strong>"accuracy"))<br/><br/>    <strong class="lb hi">return</strong> scores<br/><br/>optimal_configuration, info, _ <strong class="lb hi">=</strong> optunity<strong class="lb hi">.</strong>maximize(performance,<br/>                                                  solver_name<strong class="lb hi">=</strong>'particle swarm',<br/>                                                  num_evals<strong class="lb hi">=</strong>20,<br/>                                                   <strong class="lb hi">**</strong>search<br/>                                                  )<br/>print(optimal_configuration)<br/>print("MSE:"<strong class="lb hi">+</strong> str(info<strong class="lb hi">.</strong>optimum))</span><span id="9358" class="lf jy hh lb b fi lk lh l li lj">{'activation': 1.962890625, 'epochs': 21.611328125, 'optimizer': 2.6572265625, 'patience': 16.0322265625, 'batch_size': 1.041015625, 'neurons': 56.318359375}<br/>MSE:0.9905397885364496</span></pre><h1 id="5b08" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">7:遗传算法</h1><p id="625d" class="pw-post-body-paragraph ie if hh ig b ih kv ij ik il kw in io ip kx ir is it ky iv iw ix kz iz ja jb ha bi translated">遗传算法在每一代中检测性能良好的超参数组合，并将它们传递给下一代，直到识别出性能最佳的组合。</p><p id="9e05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">优点:</strong></p><ul class=""><li id="2171" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">适用于所有类型的HP。</li><li id="70bf" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb lm ji jj jk bi translated">不需要良好的初始化。</li></ul><p id="6b0e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">缺点:</strong></p><ul class=""><li id="b486" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb lm ji jj jk bi translated">并行能力差</li></ul><p id="3642" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机林实现:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="c277" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#Random Forest</em><br/><strong class="lb hi">from</strong> evolutionary_search <strong class="lb hi">import</strong> EvolutionaryAlgorithmSearchCV<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': np<strong class="lb hi">.</strong>logspace(1,1.8,num <strong class="lb hi">=</strong> 10 ,base<strong class="lb hi">=</strong>20,dtype<strong class="lb hi">=</strong>'int'),<br/>    'max_depth': np<strong class="lb hi">.</strong>logspace(1,2,num <strong class="lb hi">=</strong> 10 ,base<strong class="lb hi">=</strong>10,dtype<strong class="lb hi">=</strong>'int'),<br/>    "max_features":np<strong class="lb hi">.</strong>logspace(0.2,1,num <strong class="lb hi">=</strong> 5 ,base<strong class="lb hi">=</strong>8,dtype<strong class="lb hi">=</strong>'int'),<br/>    "min_samples_split":np<strong class="lb hi">.</strong>logspace(0.4, 1, num<strong class="lb hi">=</strong>5, base<strong class="lb hi">=</strong>10, dtype<strong class="lb hi">=</strong>'int'), <em class="ll">#[2, 3, 5, 7, 10],</em><br/>    "min_samples_leaf":np<strong class="lb hi">.</strong>logspace(0.1,1,num <strong class="lb hi">=</strong> 5 ,base<strong class="lb hi">=</strong>11,dtype<strong class="lb hi">=</strong>'int'),<br/>    "criterion":['gini','entropy']<br/>}<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_estimators': range(10,100),<br/>    "max_features":range(1,64),<br/>    'max_depth': range(5,50),<br/>    "min_samples_split":range(2,11),<br/>    "min_samples_leaf":range(1,11),<br/>    <em class="ll">#Categorical(name='criterion', categories=['gini','entropy'])#</em><br/>    "criterion":['gini','entropy']<br/>}<br/>clf <strong class="lb hi">=</strong> RandomForestClassifier(random_state<strong class="lb hi">=</strong>0)<br/><em class="ll"># Set the hyperparameters of GA </em><br/>ga1 <strong class="lb hi">=</strong> EvolutionaryAlgorithmSearchCV(estimator<strong class="lb hi">=</strong>clf,<br/>                                   params<strong class="lb hi">=</strong>rf_params,<br/>                                   scoring<strong class="lb hi">=</strong>"accuracy",<br/>                                   cv<strong class="lb hi">=</strong>3,<br/>                                   verbose<strong class="lb hi">=</strong>1,<br/>                                   population_size<strong class="lb hi">=</strong>10,<br/>                                   gene_mutation_prob<strong class="lb hi">=</strong>0.10,<br/>                                   gene_crossover_prob<strong class="lb hi">=</strong>0.5,<br/>                                   tournament_size<strong class="lb hi">=</strong>3,<br/>                                   generations_number<strong class="lb hi">=</strong>5,<br/>                                   n_jobs<strong class="lb hi">=</strong>1)<br/>ga1<strong class="lb hi">.</strong>fit(X, y)<br/>print(ga1<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(ga1<strong class="lb hi">.</strong>best_score_))</span><span id="a8b1" class="lf jy hh lb b fi lk lh l li lj">Types [1, 1, 1, 1, 1, 1] and maxint [89, 62, 44, 8, 9, 1] detected<br/>--- Evolve in 45927000 possible combinations ---<br/>gen	nevals	avg     	min     	max     	std      <br/>0  	10    	0.898664	0.871452	0.920423	0.0133659<br/>1  	8     	0.90345 	0.883139	0.919866	0.00932254<br/>2  	6     	0.911408	0.902059	0.919866	0.00498231<br/>3  	6     	0.914969	0.904285	0.919866	0.00508078<br/>4  	6     	0.918976	0.913189	0.919866	0.0020401 <br/>5  	6     	0.919588	0.917084	0.919866	0.000834725<br/>Best individual is: {'n_estimators': 38, 'max_features': 3, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 3, 'criterion': 'gini'}<br/>with fitness: 0.9204229271007234<br/>{'n_estimators': 38, 'max_features': 3, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 3, 'criterion': 'gini'}<br/>Accuracy:0.9204229271007234</span></pre><p id="47f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SVM实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="1398" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#SVM</em><br/><strong class="lb hi">from</strong> evolutionary_search <strong class="lb hi">import</strong> EvolutionaryAlgorithmSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'C': np<strong class="lb hi">.</strong>random<strong class="lb hi">.</strong>uniform(0,50,1000),<br/>    "kernel":['linear','poly','rbf','sigmoid']<br/>}<br/>clf <strong class="lb hi">=</strong> SVC(gamma<strong class="lb hi">=</strong>'scale')<br/>ga1 <strong class="lb hi">=</strong> EvolutionaryAlgorithmSearchCV(estimator<strong class="lb hi">=</strong>clf,<br/>                                   params<strong class="lb hi">=</strong>rf_params,<br/>                                   scoring<strong class="lb hi">=</strong>"accuracy",<br/>                                   cv<strong class="lb hi">=</strong>3,<br/>                                   verbose<strong class="lb hi">=</strong>1,<br/>                                   population_size<strong class="lb hi">=</strong>10,<br/>                                   gene_mutation_prob<strong class="lb hi">=</strong>0.10,<br/>                                   gene_crossover_prob<strong class="lb hi">=</strong>0.5,<br/>                                   tournament_size<strong class="lb hi">=</strong>3,<br/>                                   generations_number<strong class="lb hi">=</strong>5,<br/>                                   n_jobs<strong class="lb hi">=</strong>1)<br/>ga1<strong class="lb hi">.</strong>fit(X, y)<br/>print(ga1<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(ga1<strong class="lb hi">.</strong>best_score_))</span><span id="1a9a" class="lf jy hh lb b fi lk lh l li lj">Types [1, 2] and maxint [3, 999] detected<br/>--- Evolve in 4000 possible combinations ---<br/>gen	nevals	avg     	min     	max     	std     <br/>0  	10    	0.906177	0.760156	0.974402	0.089037<br/>1  	7     	0.949527	0.753478	0.974402	0.0655796<br/>2  	7     	0.974402	0.974402	0.974402	0        <br/>3  	6     	0.952977	0.760156	0.974402	0.0642738<br/>4  	4     	0.974402	0.974402	0.974402	0        <br/>5  	3     	0.971341	0.943795	0.974402	0.00918197<br/>Best individual is: {'kernel': 'rbf', 'C': 26.705081969400556}<br/>with fitness: 0.9744017807456873<br/>{'kernel': 'rbf', 'C': 26.705081969400556}<br/>Accuracy:0.9744017807456873</span></pre><p id="1165" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">KNN实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="bf15" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#KNN</em><br/><strong class="lb hi">from</strong> evolutionary_search <strong class="lb hi">import</strong> EvolutionaryAlgorithmSearchCV<br/>rf_params <strong class="lb hi">=</strong> {<br/>    'n_neighbors': range(1,20),<br/>}<br/>clf <strong class="lb hi">=</strong> KNeighborsClassifier()<br/>ga1 <strong class="lb hi">=</strong> EvolutionaryAlgorithmSearchCV(estimator<strong class="lb hi">=</strong>clf,<br/>                                   params<strong class="lb hi">=</strong>rf_params,<br/>                                   scoring<strong class="lb hi">=</strong>"accuracy",<br/>                                   cv<strong class="lb hi">=</strong>3,<br/>                                   verbose<strong class="lb hi">=</strong>1,<br/>                                   population_size<strong class="lb hi">=</strong>10,<br/>                                   gene_mutation_prob<strong class="lb hi">=</strong>0.10,<br/>                                   gene_crossover_prob<strong class="lb hi">=</strong>0.5,<br/>                                   tournament_size<strong class="lb hi">=</strong>3,<br/>                                   generations_number<strong class="lb hi">=</strong>5,<br/>                                   n_jobs<strong class="lb hi">=</strong>1)<br/>ga1<strong class="lb hi">.</strong>fit(X, y)<br/>print(ga1<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(ga1<strong class="lb hi">.</strong>best_score_))</span><span id="1c97" class="lf jy hh lb b fi lk lh l li lj">Types [1] and maxint [18] detected<br/>--- Evolve in 19 possible combinations ---<br/>gen	nevals	avg     	min     	max    	std       <br/>0  	10    	0.955092	0.946578	0.96828	0.00705242<br/>1  	7     	0.962994	0.951029	0.96828	0.00558289<br/>2  	5     	0.967501	0.964385	0.96828	0.00155815<br/>3  	8     	0.967891	0.964385	0.96828	0.00116861<br/>4  	8     	0.966221	0.947691	0.96828	0.00617696<br/>5  	6     	0.96828 	0.96828 	0.96828	0         <br/>Best individual is: {'n_neighbors': 3}<br/>with fitness: 0.9682804674457429<br/>{'n_neighbors': 3}<br/>Accuracy:0.9682804674457429</span></pre><p id="7e4b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人工神经网络实施:</p><pre class="jq jr js jt fd la lb lc ld aw le bi"><span id="f306" class="lf jy hh lb b fi lg lh l li lj"><em class="ll">#ANN</em><br/><strong class="lb hi">from</strong> evolutionary_search <strong class="lb hi">import</strong> EvolutionaryAlgorithmSearchCV<br/><em class="ll"># Define the hyperparameter configuration space</em><br/>rf_params <strong class="lb hi">=</strong> {<br/>    'optimizer': ['adam','rmsprop','sgd'],<br/>    'activation': ['relu','tanh'],<br/>    'batch_size': [16,32,64],<br/>    'neurons':range(10,100),<br/>    'epochs':[20,50],<br/>    <em class="ll">#'epochs':[20,50,100,200],</em><br/>    'patience':range(3,20)<br/>}<br/>clf <strong class="lb hi">=</strong> KerasClassifier(build_fn<strong class="lb hi">=</strong>ANN, verbose<strong class="lb hi">=</strong>0)<br/><em class="ll"># Set the hyperparameters of GA    </em><br/>ga1 <strong class="lb hi">=</strong> EvolutionaryAlgorithmSearchCV(estimator<strong class="lb hi">=</strong>clf,<br/>                                   params<strong class="lb hi">=</strong>rf_params,<br/>                                   scoring<strong class="lb hi">=</strong>"accuracy",<br/>                                   cv<strong class="lb hi">=</strong>3,<br/>                                   verbose<strong class="lb hi">=</strong>1,<br/>                                   population_size<strong class="lb hi">=</strong>10,<br/>                                   gene_mutation_prob<strong class="lb hi">=</strong>0.10,<br/>                                   gene_crossover_prob<strong class="lb hi">=</strong>0.5,<br/>                                   tournament_size<strong class="lb hi">=</strong>3,<br/>                                   generations_number<strong class="lb hi">=</strong>5,<br/>                                   n_jobs<strong class="lb hi">=</strong>1)<br/>ga1<strong class="lb hi">.</strong>fit(X, y)<br/>print(ga1<strong class="lb hi">.</strong>best_params_)<br/>print("Accuracy:"<strong class="lb hi">+</strong> str(ga1<strong class="lb hi">.</strong>best_score_))</span><span id="91b0" class="lf jy hh lb b fi lk lh l li lj">Types [1, 1, 1, 1, 1, 1] and maxint [1, 1, 2, 16, 2, 89] detected<br/>--- Evolve in 55080 possible combinations ---<br/>gen	nevals	avg     	min    	max     	std      <br/>0  	10    	0.985031	0.93044	0.999444	0.0217448<br/>1  	6     	0.998164	0.995548	0.999444	0.00174296<br/>2  	4     	0.999444	0.999444	0.999444	1.11022e-16<br/>3  	3     	0.999444	0.999444	0.999444	1.11022e-16<br/>4  	6     	0.999444	0.999444	0.999444	1.11022e-16<br/>5  	8     	0.999444	0.999444	0.999444	1.11022e-16<br/>Best individual is: {'activation': 'relu', 'epochs': 50, 'optimizer': 'sgd', 'patience': 14, 'batch_size': 16, 'neurons': 64}<br/>with fitness: 0.9994435169727324<br/>{'activation': 'relu', 'epochs': 50, 'optimizer': 'sgd', 'patience': 14, 'batch_size': 16, 'neurons': 64}<br/>Accuracy:0.9994435169727324</span></pre><p id="a6fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">信用:</strong></p><p id="85f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">长度杨和a .，“机器学习算法的超参数优化:理论与实践”，神经计算，第415卷，第295–316页，2020年，</p><p id="1d9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">https://doi.org/10.1016/j.neucom.2020.07.061。</p><div class="lo lp ez fb lq lr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hi fi z dy lw ea eb lx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">medium.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf mg lr"/></div></div></a></div></div></div>    
</body>
</html>