<html>
<head>
<title>Web Scraping &amp; Word Cloud (NLP Techniques) | Amazon Product Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网络抓取和文字云(NLP技术)|亚马逊产品评论</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/web-scraping-word-cloud-nlp-techniques-amazon-product-review-99c1d44e58e8?source=collection_archive---------1-----------------------#2022-02-10">https://medium.com/mlearning-ai/web-scraping-word-cloud-nlp-techniques-amazon-product-review-99c1d44e58e8?source=collection_archive---------1-----------------------#2022-02-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0ff9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为一名网上购物者，我相信你通常会在购买之前阅读其他买家的评论。从本文中，您将了解自然语言处理(NLP)技术如何帮助您从网站中提取和解析数据以供研究和学习，并最终使用matplotlib创建单词云(又名标签云)。我选择了<a class="ae jc" href="https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&amp;th=1" rel="noopener ugc nofollow" target="_blank">索尼SRS-XB13超低音无线便携式紧凑型扬声器IP67防水蓝牙</a>来进行情绪分析(确定积极参与和消极参与的比例)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="ab fe cl ji"><img src="../Images/eab799599153353069d8473f0a74a56c.png" data-original-src="https://miro.medium.com/v2/0*r_gFGcTYdjTRkACI"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Source: <a class="ae jc" href="https://www.pexels.com/photo/person-holding-white-and-brown-newspaper-3957616/" rel="noopener ugc nofollow" target="_blank">Pexels@Ekrulila</a></figcaption></figure><h1 id="cdc4" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤1:提取并在本地保存网页🔏</h1><p id="92d0" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">使用<a class="ae jc" href="https://docs.python-requests.org/en/latest/" rel="noopener ugc nofollow" target="_blank">请求</a>和<a class="ae jc" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">美汤</a>库从选定的URL中捕获HTML页面的内容。</p><pre class="jd je jf jg fd ks kt ku kv aw kw bi"><span id="0b5f" class="kx jq hh kt b fi ky kz l la lb">import requests<br/>from bs4 import BeautifulSoup as bs # for web scraping</span><span id="fb9b" class="kx jq hh kt b fi lc kz l la lb"># Creating empty review list<br/>sony_speaker_reviews = []<br/>for i in range(1,30):<br/>    speaker = []<br/>    url = "<a class="ae jc" href="https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&amp;th=1" rel="noopener ugc nofollow" target="_blank">https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&amp;th=1</a>"+str(i)<br/>    header={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}<br/>    response = requests.get(url,headers = header)<br/>    # Creating soup object to iterate over the extracted content<br/>    soup = bs(response.text,"lxml")<br/>    # Extract the content under the specific tag<br/>    reviews = soup.find_all("div",{"data-hook":"review-collapsed"})<br/>    for i in range(len(reviews)):<br/>        speaker.append(reviews[i].text)<br/>    # Adding the reviews of one page to empty list which in future contains all the reviews<br/>    sony_speaker_reviews += speaker</span><span id="5de1" class="kx jq hh kt b fi lc kz l la lb"># Writing reviews in a text file<br/>with open('sony_speaker_reviews.txt','w', encoding = 'utf8') as output:<br/>    output.write(str(sony_speaker_reviews))</span></pre><h1 id="1dac" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤2:数据清理和特征Extraction🧹</h1><p id="e1f2" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">为了清理数据，我们需要将所有评论合并到一个段落中，将所有单词改为小写，并删除不需要的符号。</p><p id="0db5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，我们可以使用<strong class="ig hi">词干</strong>来精简单词，抓住单词的基本意思。单词“supporting”和“supporter”共用词根“support”或者，你可以使用<strong class="ig hi">词汇化</strong>，因为它会给你一个完整的有意义的英语单词，而不仅仅是一个单词的片段。例如“买方”而不是“不夜”。🌵</p><p id="a7fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在平时的口语和写作中，我们经常使用冠词、代词、介词和连词来造句。这些词被称为停用词🛑，它们是通用的，不那么重要；因此，我们需要在进行分析之前将它们过滤掉。如果您正在分析的内容是英语，请确保选择<code class="du ld le lf kt b">english</code>作为语言，因为该语料库也包含其他语言的停用词。</p><pre class="jd je jf jg fd ks kt ku kv aw kw bi"><span id="8d94" class="kx jq hh kt b fi ky kz l la lb">import re<br/>import nltk<br/>nltk.download('wordnet')<br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>nltk.download('omw-1.4')<br/>nltk.download("stopwords")<br/>from nltk.corpus import stopwords</span><span id="f7a9" class="kx jq hh kt b fi lc kz l la lb"># Joining all the reviews into single paragraph <br/>sn_rev_string = " ".join(sony_speaker_reviews)</span><span id="657a" class="kx jq hh kt b fi lc kz l la lb"># Change to lower case and removing unwanted symbols incase if exists<br/>sn_rev_string = re.sub("[^A-Za-z" "]+"," ",sn_rev_string).lower()<br/>sn_rev_string = re.sub("[0-9" "]+"," ",sn_rev_string)</span><span id="7c7e" class="kx jq hh kt b fi lc kz l la lb"># words that contained in sony speaker reviews<br/>sn_reviews_words = sn_rev_string.split(" ")</span><span id="4ecb" class="kx jq hh kt b fi lc kz l la lb"># Lemmatizing<br/>wordnet = WordNetLemmatizer()<br/>sn_reviews_words=[wordnet.lemmatize(word) for word in sn_reviews_words]</span><span id="698b" class="kx jq hh kt b fi lc kz l la lb"># Filtering Stop Words<br/>stop_words = set(stopwords.words("english"))<br/>stop_words.update(['amazon','product','speaker','sony'])<br/>sn_reviews_words = [w for w in sn_reviews_words if not w.casefold() in stop_words]</span></pre><p id="07c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们分析文本时，我们需要按单词或句子对文本进行标记/分离。通过单词标记可以帮助我们确定经常出现的单词。同时，通过句子<strong class="ig hi"> </strong>来标记可以帮助你看到这些单词是如何相互联系的，并看到更多的上下文。我们可以用<strong class="ig hi">标记化</strong>创建<strong class="ig hi"> N元语法</strong>。例如，从句子“我喜欢学习数据科学。”，单词将是:“我”，“爱”，“学习”，“关于”，“数据”，“科学”。一个二元模型可能是:“我爱”，“爱学习”，或者“数据科学”。然而，三元模型)就像“我喜欢学习”，或者“关于数据科学”。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lg"><img src="../Images/4d7903d6d8e7478c56fbc2b01ab6133a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6DClpG09WaUTHw86"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Source: <a class="ae jc" href="https://www.pexels.com/photo/five-yellow-stars-on-blue-and-pink-background-9821386/" rel="noopener ugc nofollow" target="_blank">Pexels@Towfiqu barbhuiya</a></figcaption></figure><p id="a23f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一方面，<strong class="ig hi">词频逆文档频率(TFIDF) </strong>强调很少出现但非常重要的n元语法。🏹</p><blockquote class="ll lm ln"><p id="e7c2" class="ie if lo ig b ih ii ij ik il im in io lp iq ir is lq iu iv iw lr iy iz ja jb ha bi translated">TFIDF = TF x IDF</p><p id="0f27" class="ie if lo ig b ih ii ij ik il im in io lp iq ir is lq iu iv iw lr iy iz ja jb ha bi translated">词频(TF):一个词在文档中出现的次数。</p><p id="4a09" class="ie if lo ig b ih ii ij ik il im in io lp iq ir is lq iu iv iw lr iy iz ja jb ha bi translated">逆文档频率(IDF):一个词对整个语料库的重要性；log(文档总数/包含该术语的文档数)</p></blockquote><p id="bccb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当n-gram在文档中具有高频率而在语料库中具有低文档频率时，TFIDF得分高。</p><p id="9797" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，在一个3000字的文档中，“编码”一词出现了45次。假设在10，000个文档的语料库中有25个文档包含术语“编码”。</p><p id="948d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TF(编码)= 45/3000 = 0.015</p><p id="79e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">IDF(编码)= log (10，000/25) = 2.60</p><p id="c62d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TFIDF(编码)= 0.015 * 2.60 = 0.039</p><p id="3e41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是创建二元模型并使用TF-IDF方法为每个二元模型分配权重的代码。</p><pre class="jd je jf jg fd ks kt ku kv aw kw bi"><span id="b417" class="kx jq hh kt b fi ky kz l la lb">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="0fee" class="kx jq hh kt b fi lc kz l la lb"># TFIDF: bigram<br/>bigrams_list = list(nltk.bigrams(sn_reviews_words))<br/>bigram = [' '.join(tup) for tup in bigrams_list]</span><span id="d0de" class="kx jq hh kt b fi lc kz l la lb">vectorizer = TfidfVectorizer(bigram,use_idf=True,ngram_range=(2,2))<br/>X = vectorizer.fit_transform(bigram)<br/>vectorizer.vocabulary_<br/>sum_words = X.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]<br/>words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)</span></pre><h1 id="5231" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤3:创建Word Cloud⛅</h1><p id="5272" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">既然我们已经创建了二元模型并使用TF-IDF方法计算了权重，现在我们可以创建一个二元模型词云。<code class="du ld le lf kt b">plt.imshow()</code>中的参数<code class="du ld le lf kt b">interpolation="bilinear"</code>是为了让图像的显示更加流畅。🏃‍♀️</p><pre class="jd je jf jg fd ks kt ku kv aw kw bi"><span id="2fba" class="kx jq hh kt b fi ky kz l la lb">import matplotlib.pyplot as plt<br/>from wordcloud import WordCloud</span><span id="fd51" class="kx jq hh kt b fi lc kz l la lb">words_dict = dict(words_freq)</span><span id="31d4" class="kx jq hh kt b fi lc kz l la lb">wordCloud = WordCloud(height=1400, width=1800)<br/>wordCloud.generate_from_frequencies(words_dict)<br/>plt.title('Most Frequently Occurring Bigrams')<br/>plt.imshow(wordCloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="3bbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了执行文本情感分析，我们需要使用NLTK包中的<a class="ae jc" href="https://www.nltk.org/api/nltk.sentiment.vader.html" rel="noopener ugc nofollow" target="_blank"> VADER(用于情感推理的价感知词典)模型</a>。该模型可以通过将每个单词所携带的情绪的极性(积极/消极)和强度(强度)相加来计算情绪得分。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lg"><img src="../Images/40bc66a75dc636f35d779e635d90d094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pABFjMgJjb9K9UUz"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Source: <a class="ae jc" href="https://www.pexels.com/photo/woman-holding-a-smiley-balloon-1236678/" rel="noopener ugc nofollow" target="_blank">Pexels@Julia Avamotive</a></figcaption></figure><pre class="jd je jf jg fd ks kt ku kv aw kw bi"><span id="539a" class="kx jq hh kt b fi ky kz l la lb">from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>nltk.download('vader_lexicon')</span><span id="c4d1" class="kx jq hh kt b fi lc kz l la lb"># initialize VADER<br/>sid = SentimentIntensityAnalyzer()<br/>pos_word_list=[]<br/>neu_word_list=[]<br/>neg_word_list=[]</span><span id="6eb3" class="kx jq hh kt b fi lc kz l la lb">for word in sn_reviews_words:<br/>    if (sid.polarity_scores(word)['compound']) &gt;= 0.25:<br/>        pos_word_list.append(word)<br/>    elif (sid.polarity_scores(word)['compound']) &lt;= -0.25:<br/>        neg_word_list.append(word)<br/>    else:<br/>        neu_word_list.append(word)                </span><span id="da90" class="kx jq hh kt b fi lc kz l la lb"># Positive word cloud<br/># Choosing the only words which are present in positive words<br/>sn_pos_in_pos = " ".join ([w for w in pos_word_list])<br/>wordcloud_pos_in_pos = WordCloud(<br/>                      background_color='black',<br/>                      width=1800,<br/>                      height=1400<br/>                     ).generate(sn_pos_in_pos)<br/>plt.title("Positive Words In The Review of Sony Speaker")<br/>plt.imshow(wordcloud_pos_in_pos, interpolation="bilinear")</span><span id="91d7" class="kx jq hh kt b fi lc kz l la lb"># negative word cloud<br/># Choosing the only words which are present in negwords<br/>sn_neg_in_neg = " ".join ([w for w in neg_word_list])<br/>wordcloud_neg_in_neg = WordCloud(<br/>                      background_color='black',<br/>                      width=1800,<br/>                      height=1400<br/>                     ).generate(sn_neg_in_neg)<br/>plt.title("Negative Words In The Review of Sony Speaker")<br/>plt.imshow(wordcloud_neg_in_neg, interpolation="bilinear")</span></pre><div class="jd je jf jg fd ab cb"><figure class="ls jh lt lu lv lw lx paragraph-image"><img src="../Images/ce77df9c47165287e558e614fa6e2703.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*g1DOAfpoBD2tS2UY3uMPFw.png"/></figure><figure class="ls jh ly lu lv lw lx paragraph-image"><img src="../Images/f93f23ce0b415b07b5551fd291203221.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*MveJp_snubXGHL1vXPjDMQ.png"/></figure><figure class="ls jh lz lu lv lw lx paragraph-image"><img src="../Images/244570cd1a77675a83616c1ed4f2e050.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*qhu4-WAcb8853TPg_iSAJA.png"/></figure></div><p id="f2b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以上只是一个网页抓取项目的例子。你可以在不同的网站上进行挑选，比如<a class="ae jc" href="https://github.com/collections" rel="noopener ugc nofollow" target="_blank">GitHub Popular Repositories</a>、<a class="ae jc" href="https://www.themoviedb.org/movie" rel="noopener ugc nofollow" target="_blank">电影数据集(TMDb) </a>等等。无论如何，希望这篇文章能帮助你在你的网页抓取之旅。快乐的网页抓取！😉</p><div class="ma mb ez fb mc md"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">medium.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jj md"/></div></div></a></div><p id="f4b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">🟠在MLearning.ai  成为<a class="ae jc" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">作家</a></p></div></div>    
</body>
</html>