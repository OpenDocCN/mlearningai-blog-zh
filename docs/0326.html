<html>
<head>
<title>Support Vector Machine Math for people who don’t care about optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为不关心优化的人提供支持向量机数学</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/support-vector-machines-svms-for-people-who-dont-care-about-optimization-77873fa49bca?source=collection_archive---------3-----------------------#2021-03-23">https://medium.com/mlearning-ai/support-vector-machines-svms-for-people-who-dont-care-about-optimization-77873fa49bca?source=collection_archive---------3-----------------------#2021-03-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f205" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我一直回避支持向量机背后的数学，因为我知道这是一个优化问题，除非我用梯度下降来最小化一些东西，否则我对优化或线性规划或任何这些都不太了解。</p><p id="58fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，我最终花了一些时间阅读了优化方面的内容，并希望让没有多少优化背景的人更容易理解SVM数学的概念。</p><p id="fc2f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们讨论一下我们要做的是什么。考虑以下情节:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/f5dd61ad2eefb790114b6f7c7541d241.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*d-1rLjOrQJVyeoZA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Image 1: Example Hyperplane Solved For Using an SVM. Figure Credit: <a class="ae jo" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf" rel="noopener ugc nofollow" target="_blank">Andrew Ng</a></figcaption></figure><p id="bc3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将X和O分开的线称为分离超平面，其中超平面只是n维空间中的一个平面，大小为<em class="jp"> n-1 </em>(在上面的例子中，n=2)。无论如何，当我们使用支持向量机进行分类时，我们希望找到使我们的数据线性可分的超平面。然而，<strong class="ig hi">与神经网络等其他生成分离超平面的方法不同，支持向量机在生成平面时不考虑远离直线的点</strong>。在上面的示例图像中，点A离平面很远，因此它对线的最佳位置没有太大影响。然而，如果我们将C点下移，我们显然需要重新调整直线，以确保数据是线性可分的。</p><p id="99f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是支持向量机的主要优点之一，它们只考虑会改变直线位置的点。<strong class="ig hi">这些点被称为支持向量，并且是公式化超平面时唯一考虑的点。</strong></p><p id="48c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们讨论支持向量机如何求解这个超平面时，我们将在二元分类问题的上下文中这样做，其中我们将一些输入<em class="jp"> X </em>映射到我们的目标空间<em class="jp">y∈{-1，1} </em>。我们的目标是构建以下分类器。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jq"><img src="../Images/536e1a50bf108b53da67874006d012ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*QucjskERyMmqQ5bMa0h19w.png"/></div></figure><p id="08ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上面的等式中:</p><blockquote class="jr js jt"><p id="09cf" class="ie if jp ig b ih ii ij ik il im in io ju iq ir is jv iu iv iw jw iy iz ja jb ha bi translated">g是我们的激活函数，如果(<em class="hh"> w^Tx + b) </em>为正，则返回1，如果为负，则返回-1。</p><p id="4dbc" class="ie if jp ig b ih ii ij ik il im in io ju iq ir is jv iu iv iw jw iy iz ja jb ha bi translated">w和b是我们希望学习的模型的权重和偏差</p><p id="e910" class="ie if jp ig b ih ii ij ik il im in io ju iq ir is jv iu iv iw jw iy iz ja jb ha bi translated">x是我们的输入向量。</p></blockquote><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jx"><img src="../Images/8495479274870cdad71d9018490a4fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/0*4Xs5I1tpXA-Igs4q.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Example SVM “margin” γ. Figure Credit: <a class="ae jo" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf" rel="noopener ugc nofollow" target="_blank">Andrew Ng</a></figcaption></figure><p id="ce8c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我读过的大多数关于支持向量机的数学教程都喜欢先讨论最大间隔分类器，然后再讨论支持向量机。边距是分离超平面和数据点之间的距离。在上图中，γ是从线到点<em class="jp"> A </em>的边距。最大间隔分类器基本上只是用于完全线性可分数据的SVM。<strong class="ig hi">最大间隔分类器的目标是拟合一个超平面，使超平面每侧最近的数据点之间的距离/间隔最大化，从而使数据线性分离</strong>，如下图所示。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jy"><img src="../Images/8eb4a665577b5ff87c9f2a59ad52b670.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*VxbqxQILPPwFchOG.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Maximum Margin Classifier. Figure Credit: <a class="ae jo" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf" rel="noopener ugc nofollow" target="_blank">Andrew Ng</a></figcaption></figure><h1 id="07b7" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">拉格朗日乘数和最优化的东西</h1><p id="b7b4" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">可怕的优化部分来了。别担心，我保证会解释清楚这是什么意思。因此，最大间隔分类器希望解决以下问题:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/c3ad7399f2f06c43012cd7145d325fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*4ezbOhciteOUnZd4JMG3bg.png"/></div></figure><p id="a7ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，这意味着什么呢？为了完整起见，我们先指出优化问题遵循这个总布局。首先，我们陈述我们想要优化(最大化/最小化)什么，然后我们陈述必须满足什么约束，通常用<em class="jp">这样的</em>或s.t .符号。</p><p id="c4d0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来看看这个优化问题。那么回想一下，直线每一边最近的数据点将会离超平面γ，对吗？因此，我们只想确保所有数据点尽可能远离直线(最大化γ)…这就是目标的意思。</p><p id="e401" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，<em class="jp"> y(i)(w^T x(i)+b)≥γ </em>部分要求我们也确保所有点被正确分类，因为γ必须是正的，并且对于任何正确分类的数据点，<em class="jp"> y(i)(w^T x(i)+b) </em>都是正的。我不打算完全解释∨w ∨= 1约束，因为我必须解释函数边界和几何边界之间的区别，而且…我不认为我们需要这样做。有关更多详细信息，请参考https://see . Stanford . edu/materials/aimlcs 229/cs 229-notes 3 . pdf的第4节。现在，我们只能说，通过使用∨w ∨= 1约束，我们获得了一些不错的属性。</p><blockquote class="jr js jt"><p id="013f" class="ie if jp ig b ih ii ij ik il im in io ju iq ir is jv iu iv iw jw iy iz ja jb ha bi translated">注意:当我说优化是“棘手”的时候，我的意思是这个函数很难优化。有一些建立方程的方法，允许更容易地使用现有的优化算法来最小化/最大化函数</p></blockquote><p id="4b09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，这实际上不是最大间隔分类器所需的优化的最终公式。我上面描述的提供了一些直觉，但实际上是一个相当棘手的优化。</p><p id="5061" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了使它变得简单一点，我们可以通过针对以下方面进行优化来获得相同的解决方案:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/1bdfd10b6fd05cfd72b7987ad53ffa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*SMe7vPBzgRmZAbK7vwc1BA.png"/></div></figure><p id="c7a3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可能会注意到，我们去掉了∨w ∨= 1的约束，在目标函数的分母上增加了∨w∨。在不深入细节的情况下，我只想说，针对最大利润(除以∨w∨时得到的结果)的标准化版本进行优化，可以获得与最大化利润本身相同的结果。不幸的是，我们仍然有一点棘手的优化，但还有一件事我们可以做。</p><p id="0b87" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有一个技巧可以将这种优化重组为更可行的东西，当我说可行时，我的意思是将其格式化，以便能够插入到一些优化软件中(例如scipy.optimize)。在<strong class="ig hi">超高等级</strong>下，我们将这样做的方式如下。</p><p id="7b03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">回想一下我们的分类器<em class="jp"> h(x)=g(wTx+b) </em>。有趣的是，我们可以任意缩放<em class="jp"> w，b </em>并得到相同的分类结果，因为我们的最终输出函数<em class="jp"> sign()，</em>只关心输出的符号。即<em class="jp">sign(g(wTx+b))= sign(g(2 wTx+2b))</em>。由于缩放约束不影响结果，我们可以强制裕度γ=1。换句话说，我们可以在不改变我们正在做的事情的情况下，重新调整所有的东西，使得裕度γ=1。由于规模的概念不影响分类，让我们将优化重写为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es le"><img src="../Images/8160abdf9762ffff0ab40b1cadcf5de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*49YC9kvP_wT4moxzObc2DQ.png"/></div></figure><p id="89db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是因为当γ=1时，优化γ/‖w‖与最小化1/∨w∨是相同的。因此，我们现在强制保证金等于1。最后，我们有一些东西可以插入一些黑盒优化软件。此外，我很快注意到，这是我们的优化问题的原始公式。稍后将详细介绍这一点…</p><p id="5086" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，棘手的部分来了，但我的意思是…这是支持向量机工作的核心。</p><p id="1c85" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设您有以下优化问题:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lf"><img src="../Images/5094fe645b3210ab374d4658c927f399.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*oQcJch36zqG6PKv-Xi8UOg.png"/></div></figure><p id="618a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以基本上只是一个函数，我们想用两个约束最小化。我们可以用拉格朗日乘数来解决这个问题(我假设你记得这些是什么)。我们将广义拉格朗日函数定义如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lg"><img src="../Images/733a3bb5d3b222e78f021befe53cf93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*jxZEBo2wEk8kSusj8-E8Jg.png"/></div></figure><p id="a69a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么我们需要拉格朗日函数？我们这样做是因为我们可以求解<em class="jp">拉格朗日乘数</em>，这允许我们解析地最小化某个函数<em class="jp"> f(w) </em>。<strong class="ig hi">你需要知道的:</strong>拉格朗日乘数是α和β，它们允许我们将<em class="jp"> L </em>的偏导数设置为0，并在给定一些<em class="jp"> w </em>的情况下最小化方程。因此，如果我们有乘数，我们可以最小化函数…这是我们的目标。</p><p id="8459" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">好了，现在我发现奇怪的部分来了。优化问题中有一些东西叫做“对偶”和“原始”公式。在某些条件下(在我们的例子中满足)，它们产生相同的解。对于支持向量机，对偶公式，我将跳过所有的数学细节，允许我们以更有用的方式重写最大间隔分类器优化。</p><p id="cce8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，请注意，如果我们将最大间隔分类器的方程代入我们的广义拉格朗日方程，我们会得到:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/1c67ac0c3c23530cc99fed432764a9f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*4nderZFgz-ttZOYCfk1nHg.png"/></div></figure><p id="9124" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在做了大量的数学运算，得到了这个拉格朗日函数的对偶形式之后，我们找到了一个新的优化函数，它给出了与之前的原始形式相同的结果。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/e147042810a1bcd78db000c26a043bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*sZc-ccelqo_YHaBJTfeMhw.png"/></div></figure><blockquote class="jr js jt"><p id="a375" class="ie if jp ig b ih ii ij ik il im in io ju iq ir is jv iu iv iw jw iy iz ja jb ha bi translated">快速回顾:我们采用拉格朗日的一般公式，插入我们的最大间隔分类器方程，然后做一些简化，使用拉格朗日乘数将问题重写为优化问题。你只需要知道这些。</p></blockquote><p id="712d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我相信你现在的问题是…“为什么我们需要这样做？”</p><p id="cea9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们去掉了对<em class="jp"> w，b </em>的依赖。我们剩下的是<em class="jp"> x(i)，x(j) </em>的点积，我们可以在执行优化之前计算这些点积。你看到这有多酷了吗？我们需要做的就是求解αs。这使得问题更加有效，并让我们在低维空间中工作。此外，如果我们想学习一个非线性的决策边界，我们可以用一些非线性的核函数来代替点积。总的来说，我们现在只依赖于α的一切！但是我们如何用αs来得到我们的超平面呢？</p><p id="09d4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于权重向量，我们可以将其表示为训练数据、训练标签和α的线性组合</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lj"><img src="../Images/582f2985d5c5261937c7fcf299d988a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*byuxMtDeMtVqNZIpEO_7BA.png"/></div></figure><p id="0f3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们想对一个测试数据点<em class="jp">x′</em>进行新的预测时，我们只需要简单地做</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lk"><img src="../Images/9cffcdf6bbf98a50683547643bad6d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*lNl3USrFYRvIWnziv_UCoA.png"/></div></figure><p id="8caa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请记住，大多数权重将为0，因为影响决策边界的唯一数据点是支持向量。这是这种新配方的另一个好处。也就是说，非零α将是我们的支持向量。因此，我们可以得到我们的最佳分离超平面，只看非零α……非常酷！</p><h1 id="f62e" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">非线性可分数据的情况</h1><p id="27cb" class="pw-post-body-paragraph ie if hh ig b ih kx ij ik il ky in io ip kz ir is it la iv iw ix lb iz ja jb ha bi translated">如果我们添加数据点，使我们的数据集不再是线性可分的，那么我们到目前为止所讨论的将会打破。这是因为，如果你还记得我对优化函数的原始形式的讨论，我们要求所有的数据点都被正确分类。考虑到这一点，我们可以允许一些“松弛”，并用L1正则化重新制定优化，这基本上允许一些错误分类。</p><p id="5596" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是正则化SVM的原始形式</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ll"><img src="../Images/4958d372ed21fdf2e6f06a08976587a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ZwZUZgDaO-ohlj8b9Q17lg.png"/></div></figure><p id="cc82" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，允许每个训练点<em class="jp"> x(i) </em>与分离超平面的距离小于1(还记得以前我们限制点至少γ=1)。对于利润率小于1的样本，我们支付的价格是<em class="jp"> C </em>。也就是说，<em class="jp"> C </em>控制着我们对犯错的在意程度。如果<em class="jp"> C </em>很大，我们允许许多错误，并使用大量的正则化。如果<em class="jp"> C </em>小，我们基本上就有了我们原来的优化问题，尽量少出错。</p><p id="912c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对偶形式，我跳过了它的推导，看起来像这样:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/86283d80f754eae7b90d0edce6d8b1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*ITAiuA1fJnfYK0Cj4_yMTA.png"/></div></figure><p id="c75f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如您所见，这与我们已经讨论过的线性可分方法非常相似。另外，请注意，这是我们要从头开始编写SVM的代码。让我们看看这个怎么样。</p><p id="5511" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们生成一些数据。请注意，我们将默认的(0，1)标签更改为(-1，1)。这是必需的，因为我们希望我们的超平面方程分别等于0和边距等于-1，1。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="4d24" class="ls ka hh lo b fi lt lu l lv lw">from sklearn.datasets import make_blobs<br/>from scipy import optimize<br/>import numpy <strong class="lo hi">as</strong> np<br/><strong class="lo hi">def</strong> <strong class="lo hi">gen_data</strong>(n_samples):<br/>    X, t <strong class="lo hi">=</strong> make_blobs(n_samples<strong class="lo hi">=</strong>n_samples, centers<strong class="lo hi">=</strong>2, cluster_std<strong class="lo hi">=</strong>2.0)<br/>    minmax <strong class="lo hi">=</strong> MinMaxScaler()<br/>    X <strong class="lo hi">=</strong> minmax.fit_transform(X)<br/>    <strong class="lo hi">for</strong> i <strong class="lo hi">in</strong> range(len(t)):<br/>        <strong class="lo hi">if</strong> t[i] <strong class="lo hi">==</strong> 0:<br/>            t[i] <strong class="lo hi">=</strong> <strong class="lo hi">-</strong>1<br/>    <strong class="lo hi">return</strong> X,t</span></pre><p id="6f9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个函数将计算我们讨论过的所有点积。做完这些，剩下要做的就是计算αs。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="6394" class="ls ka hh lo b fi lt lu l lv lw"><strong class="lo hi">def</strong> <strong class="lo hi">H</strong>(X):<br/>    N <strong class="lo hi">=</strong> len(X)<br/>    H <strong class="lo hi">=</strong> np.empty((N, N))<br/>    <strong class="lo hi">for</strong> i <strong class="lo hi">in</strong> range(N):<br/>        <strong class="lo hi">for</strong> j <strong class="lo hi">in</strong> range(N):<br/>            H[i, j] <strong class="lo hi">=</strong> np.dot(X[i], X[j])<br/>    <strong class="lo hi">return</strong> H</span></pre><p id="7ff9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是我们将传递给优化器的目标函数和约束。关于scipy.optimize的几点说明</p><p id="1bd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1) Scipy会默认最小化你的函数。因此，为了计算最大值，我们返回函数的负值</p><p id="7ddf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2)不等式约束默认情况下确保返回的值大于0</p><p id="eac2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3)等式约束确保您返回的值等于0</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="95c0" class="ls ka hh lo b fi lt lu l lv lw"><strong class="lo hi">def</strong> <strong class="lo hi">loss</strong>(a):<br/>    '''<br/>    a: vector of alpha values <br/>    t: (accessed globally) vector of labels in (-1,1)<br/>    '''<br/>    at <strong class="lo hi">=</strong> a <strong class="lo hi">*</strong> t  <br/>    <strong class="lo hi">return</strong> <strong class="lo hi">-</strong>(a.sum() <strong class="lo hi">-</strong> 0.5 <strong class="lo hi">*</strong> np.dot(at.T, np.dot(K, at)))<br/><br/><em class="jp"># This is the inequality constraint <br/># For N datapoints, this will produce a <br/># matrix with 2N rows, where the first N<br/># assure alpha &gt; 0 and the second N<br/># assure alpha &lt; C. This is just a clever<br/># formulation of this I found on github! (see sources) <br/></em><strong class="lo hi">def</strong> <strong class="lo hi">constraint1</strong>(x):<br/>    <em class="jp"># A,b accessed globally<br/></em>    <strong class="lo hi">return</strong> b <strong class="lo hi">-</strong> np.dot(A,x)<br/>    <br/><em class="jp"># This is the equality constraint sum_i alpha_i*y_i = 0<br/></em><strong class="lo hi">def</strong> <strong class="lo hi">constraint2</strong>(x):<br/>    <em class="jp"># t accessed globally<br/></em>    <strong class="lo hi">return</strong> np.dot(x,t)</span></pre><p id="fd34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是程序的主要逻辑</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="7891" class="ls ka hh lo b fi lt lu l lv lw"><em class="jp"># Get data and labels with 100 data points <br/></em>X,t <strong class="lo hi">=</strong> gen_data(100)<br/>N <strong class="lo hi">=</strong> len(X)<br/><em class="jp"># Set C value (error tolerence) <br/></em>C <strong class="lo hi">=</strong> 1<br/><em class="jp"># Generate the dot_product matrix <br/></em>K <strong class="lo hi">=</strong> H(X)<br/><br/><em class="jp"># see constraint 1 comments to understand what happens here<br/></em>A <strong class="lo hi">=</strong> np.vstack((<strong class="lo hi">-</strong>np.eye(N), np.eye(N)))<br/>b <strong class="lo hi">=</strong> np.concatenate((np.zeros(N), C <strong class="lo hi">*</strong> np.ones(N)))<br/><br/><em class="jp"># Define constraints for scipy<br/></em>constraints <strong class="lo hi">=</strong> ({'type': 'ineq', 'fun': constraint1},<br/>               {'type': 'eq', 'fun': constraint2})<br/><br/><br/><em class="jp"># initalize alpha values (doesn't matter too much how you do this)<br/></em>a0 <strong class="lo hi">=</strong> np.random.rand(N) <br/>res <strong class="lo hi">=</strong> minimize(loss, a0, constraints<strong class="lo hi">=</strong>constraints)<br/><br/>a <strong class="lo hi">=</strong> res.x  <em class="jp"># optimal Lagrange multipliers<br/></em>a[np.isclose(a, 0)] <strong class="lo hi">=</strong> 0  <em class="jp"># zero out nearly zeros<br/></em>a[np.isclose(a, C)] <strong class="lo hi">=</strong> C  <em class="jp"># round the ones that are nearly C<br/></em><br/><em class="jp"># points with a==0 do not contribute to prediction<br/></em>support_idx <strong class="lo hi">=</strong> np.where(0 <strong class="lo hi">&lt;</strong> a)[0]  <em class="jp"># index of points with a&gt;0; i.e. support vectors</em></span></pre><p id="b54a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经完成了优化，通过以下方式求解<em class="jp"> b </em>项</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lx"><img src="../Images/f4b27fefcb57fc48066de94c9d8bc7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*kBc1HEcGTFiPMUZ7q9AAXg.png"/></div></figure><p id="c1bc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中<em class="jp"> N，M </em>是支持向量的数量。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="e952" class="ls ka hh lo b fi lt lu l lv lw"><em class="jp"># Solve for b intercept term <br/></em>a_times_t <strong class="lo hi">=</strong> a <strong class="lo hi">*</strong> t<br/>all_b <strong class="lo hi">=</strong> []<br/><strong class="lo hi">for</strong> n <strong class="lo hi">in</strong> support_idx:<br/>    x_n <strong class="lo hi">=</strong> X[n]<br/>    eval_ <strong class="lo hi">=</strong> np.array([np.dot(x_m, X[n]) <strong class="lo hi">if</strong> a_m <strong class="lo hi">&gt;</strong> 0 <strong class="lo hi">else</strong> 0 <strong class="lo hi">for</strong> x_m, a_m <strong class="lo hi">in</strong> zip(X, a)])<br/>    b <strong class="lo hi">=</strong> t[n] <strong class="lo hi">-</strong> a_times_t.dot(eval_)<br/>    all_b.append(b)<br/>b <strong class="lo hi">=</strong> np.mean(all_b)</span></pre><p id="4c89" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们定义一个函数来“预测”测试数据，也就是在一组均匀分布的数字上生成超平面。</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="89b9" class="ls ka hh lo b fi lt lu l lv lw"><strong class="lo hi">def</strong> <strong class="lo hi">predict</strong>(test, X, t, k, a, b):<br/><br/>    a_times_t <strong class="lo hi">=</strong> a <strong class="lo hi">*</strong> t<br/>    y <strong class="lo hi">=</strong> np.empty(len(test))  <em class="jp"># array of predictions<br/></em>    <strong class="lo hi">for</strong> i, s <strong class="lo hi">in</strong> enumerate(test):<br/>        <em class="jp"># evaluate dot product between new data point and support vectors; 0 if not a support vector<br/></em>        eval <strong class="lo hi">=</strong> np.array([np.dot(s, x_m) <strong class="lo hi">if</strong> a_m <strong class="lo hi">&gt;</strong> 0 <strong class="lo hi">else</strong> 0 <strong class="lo hi">for</strong> x_m, a_m <strong class="lo hi">in</strong> zip(X, a)])<br/>        y[i] <strong class="lo hi">=</strong> a_times_t.dot(eval) <strong class="lo hi">+</strong> b<br/>    <strong class="lo hi">return</strong> y</span></pre><p id="c56f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们策划！</p><pre class="jd je jf jg fd ln lo lp lq aw lr bi"><span id="7721" class="ls ka hh lo b fi lt lu l lv lw">plt.scatter(X[:,0], X[:,1], c <strong class="lo hi">=</strong> t)<br/><em class="jp"># plot the decision boundary and margins in the input space<br/></em>grid <strong class="lo hi">=</strong> np.arange(X.min(), X.max(), 0.05)<br/>xx, yy <strong class="lo hi">=</strong> np.meshgrid(grid, grid)<br/>zs <strong class="lo hi">=</strong> predict(np.array(list(zip(np.ravel(xx), np.ravel(yy)))), X, t, None, a, b)<br/>zz <strong class="lo hi">=</strong> zs.reshape(xx.shape)<br/>CS <strong class="lo hi">=</strong> plt.contour(xx, yy, zz, levels<strong class="lo hi">=</strong>[<strong class="lo hi">-</strong>1, 0, 1], )</span></pre><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ly"><img src="../Images/d221b0d9917fa95ea651830bc7d9be08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*Q7iGzzrKTqJQhXmC.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Example SVM Result</figcaption></figure><p id="4300" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">来源:</p><ul class=""><li id="6d3f" class="lz ma hh ig b ih ii il im ip mb it mc ix md jb me mf mg mh bi translated">基于来自https://github.com/yiboyang/PRMLPY/blob/master/ch7/svm.py<a class="ae jo" href="https://github.com/yiboyang/PRMLPY/blob/master/ch7/svm.py" rel="noopener ugc nofollow" target="_blank">的非线性SVC的代码</a></li><li id="a507" class="lz ma hh ig b ih mi il mj ip mk it ml ix mm jb me mf mg mh bi translated">从https://see . Stanford . edu/materials/aimlcs 229/cs 229-notes 3 . pdf…这样一篇伟大的文章中学到了我现在对支持向量机的大部分了解</li><li id="21be" class="lz ma hh ig b ih mi il mj ip mk it ml ix mm jb me mf mg mh bi translated">这也很有帮助！<a class="ae jo" href="https://web.mit.edu/6.034/wwwbob/svm.pdf" rel="noopener ugc nofollow" target="_blank">https://web.mit.edu/6.034/wwwbob/svm.pdf</a></li></ul></div></div>    
</body>
</html>