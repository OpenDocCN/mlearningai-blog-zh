<html>
<head>
<title>HyperParameter Tuning: Fixing High Bias(Underfitting) in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整:修复神经网络中的高偏差(欠拟合)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/hyperparameter-tuning-fixing-high-bias-underfitting-in-neural-networks-5184ead3cbed?source=collection_archive---------2-----------------------#2021-07-18">https://medium.com/mlearning-ai/hyperparameter-tuning-fixing-high-bias-underfitting-in-neural-networks-5184ead3cbed?source=collection_archive---------2-----------------------#2021-07-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="346c" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">减少神经网络中高偏差(欠拟合)问题的快速方法。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/61de4bb042d0ebec962a7b1ceff36737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*zXo0AvlXqCs_6C5TJrOTag.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Hyper-Parameter Tuning</figcaption></figure><h1 id="8e76" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">介绍</h1><p id="7da3" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">在这篇博客中，我们将通过一些方法和技术来解决神经网络中的高偏差(欠拟合)问题。高偏差是神经网络训练过程中面临的一个常见问题。当训练集和测试集的精度都不够高时，就会出现高偏差的问题。该问题通常象征着被训练的模型没有学习到输入-输出映射，并且也不能在交叉验证或测试集上正确地概括。</p><p id="b6fc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们将在本博客中逐步检查各种因素对训练准确性的影响。</p><h1 id="9aa0" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">导入和预处理</h1><p id="37a3" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">我们将从导入<strong class="ik hi"> TensorFlow、NumPy和Matplotlib </strong>库开始，并初始化一些超参数，如历元数、学习率和优化器</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="4716" class="ld jt hh kz b fi le lf l lg lh"><strong class="kz hi">import</strong> <strong class="kz hi">tensorflow</strong> <strong class="kz hi">as</strong> <strong class="kz hi">tf</strong> <br/><strong class="kz hi">import</strong> <strong class="kz hi">numpy</strong> <strong class="kz hi">as</strong> <strong class="kz hi">np</strong><br/><strong class="kz hi">import</strong> <strong class="kz hi">matplotlib.pyplot</strong> <strong class="kz hi">as</strong> <strong class="kz hi">plt</strong><br/><br/><br/>tf.random.set_seed(1)<br/>EPOCHS = 20<br/>LR = 0.001<br/>OPT = tf.keras.optimizers.Adam(LR)<br/>plt.style.use('fivethirtyeight')<br/>plt.rcParams["figure.figsize"] = (8,5)</span></pre><p id="e661" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们将使用著名的<strong class="ik hi"> Mnist </strong>数据集进行演示。Mnist数据集包含<strong class="ik hi"> 60，000张</strong>图片，采用<strong class="ik hi"> 80:20 </strong>训练测试分割。所有的图像都是灰度的，并且具有形状<strong class="ik hi"> (28，28) </strong>。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="acea" class="ld jt hh kz b fi le lf l lg lh">(x_train , y_train) , (x_test , y_test ) = tf.keras.datasets.mnist.load_data()</span><span id="0953" class="ld jt hh kz b fi li lf l lg lh">x_train = x_train /255 <br/>x_test = x_test/255</span></pre><p id="3a24" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们可以通过TensorFlow库直接访问这个数据集。数据已经被分成训练和测试子集。下一步，我们将正常化我们的图像。</p><h1 id="5d8e" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">模型设计</strong></h1><p id="077f" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">我们将首先建立一个简单的神经网络，没有隐藏层，只有一个输入层和一个输出层。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="3c14" class="ld jt hh kz b fi le lf l lg lh">model = tf.keras.Sequential(<br/>[tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="f32a" class="ld jt hh kz b fi li lf l lg lh">model.compile(optimizer=OPT,<br/>              loss = "sparse_categorical_crossentropy",<br/>              metrics = ["accuracy"])</span></pre><p id="4101" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们将使用稀疏分类交叉熵作为损失来编译这个模型，并将度量设置为准确性。</p><h1 id="3334" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">增加</strong>数据<strong class="ak">的效果</strong></h1><p id="312f" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">我们将训练上面定义的模型两次，但是使用不同的数据分布。为了证明数据对高偏差的影响，我们将定义一个新的训练数据子集，其中仅包含总训练数据的60% 。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="7471" class="ld jt hh kz b fi le lf l lg lh">(x_train_partial , y_train_partial) =   (x_train[:30000], y_train[:30000])</span></pre><p id="e99e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">与原始数据集中的<strong class="ik hi"> 50，000 </strong>图像相比，新的(x_train_partial，y_train_partial)数据集具有<strong class="ik hi"> 30，000 </strong>图像。在训练完这两个数据集后，我们现在可以绘制一个训练精度与历元数的关系图，以检查增加数据的效果。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/d00fdb20f10d59490546b6c81261981e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*MApdtv7idg7s5CIOgfs30A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of Data</figcaption></figure><p id="e4dd" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">从上图可以清楚地看出，增加数据无助于解决高偏差问题。</p><h1 id="e02a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">增加隐藏层的效果</h1><p id="cc4a" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">现在，我们将增加网络中隐藏层的数量，并验证其对模型训练准确性的影响。我们将训练四个不同的模型，其中几个隐藏层分别设置为<strong class="ik hi"> 1、2、3和5 </strong>层<strong class="ik hi">层</strong>。所有4种模型的架构如下:</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="d36e" class="ld jt hh kz b fi le lf l lg lh">one_layer_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="7b74" class="ld jt hh kz b fi li lf l lg lh">two_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(10 , activation = "relu"),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="a973" class="ld jt hh kz b fi li lf l lg lh">three_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(40 , activation = "relu"),<br/>tf.keras.layers.Dense(20 , activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span><span id="303e" class="ld jt hh kz b fi li lf l lg lh">five_layers_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),                              tf.keras.layers.Dense(10 , activation = "relu"),                              tf.keras.layers.Dense(20 , activation = "relu"),                              tf.keras.layers.Dense(40 , activation = "relu"),                              tf.keras.layers.Dense(20 , activation = "relu"),                             tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="9ddc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">在对上述所有模型的20个时期的完整数据集进行训练后，我们得到了下面的准确性比较图:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/61de4bb042d0ebec962a7b1ceff36737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*zXo0AvlXqCs_6C5TJrOTag.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of hidden layers</figcaption></figure><p id="d4d7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">可以清楚地看到，随着我们在训练过程中进一步深入，增加隐藏层的数量直接增加了准确性。对于mnist数据集，选择3个隐藏层似乎会产生最佳结果。</p><p id="5880" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们现在将使用这个3个隐藏层的神经网络作为我们的参考，并检查在这个体系结构的不同层中增加节点的效果。</p><h1 id="e77f" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">隐藏层中单元(节点)数量的影响</h1><p id="b381" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">我们现在将增加先前训练的3层网络的不同层中的节点数量。通常的做法是按降序设置不同层中的单元数量。在这次演示中，我们将训练两种不同的模型。第一种型号的单位数量较少，而第二种型号的单位数量较多。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="9d8b" class="ld jt hh kz b fi le lf l lg lh">small_units_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(80,activation = "relu"),<br/>tf.keras.layers.Dense(40,activation = "relu"),<br/>tf.keras.layers.Dense(20,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])<br/></span><span id="9245" class="ld jt hh kz b fi li lf l lg lh">large_units_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),                              tf.keras.layers.Dense(512,activation = "relu"),                              tf.keras.layers.Dense(128,activation = "relu"),                              tf.keras.layers.Dense(64,activation = "relu"),                             tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="4446" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们将第二个模型中的单位设置为2 的<strong class="ik hi">次方。这被认为是在我们的神经网络中设置单元数量的最佳默认选择。</strong></p><p id="3b97" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">在对上述两个模型的20个时期的完整数据集进行训练后，我们得到了下图以进行精确度比较:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/28720323bc6f629ba7ea57adafbd28bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*gnNNDMXaxNRMUav9EFEZCg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of units</figcaption></figure><p id="ce21" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">单元的数量显然对训练精度有很大的影响。随着每层中单元数量的增加，精确度也会增加。在上面的例子中，随着层数的增加以及每层单元数的增加，精度从<strong class="ik hi"> 93% </strong>增加到超过<strong class="ik hi"> 99% </strong>。</p><h1 id="7f62" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">批量标准化的效果</h1><p id="568c" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">接下来，我们将检查添加批量归一化图层对修复高偏差的影响。我们将用之前的最佳模型作为验证批量归一化效果的参考。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="236d" class="ld jt hh kz b fi le lf l lg lh">bn_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(512,activation = "relu"),<br/>tf.keras.layers.BatchNormalization(),<br/>tf.keras.layers.Dense(128,activation = "relu"),<br/>tf.keras.layers.BatchNormalization(),<br/>tf.keras.layers.Dense(64,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="d2e1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们在隐藏层之间添加了一对<strong class="ik hi"> BatchNormalization </strong>层。我们现在将训练这个模型，并将其准确性与我们以前的最佳模型进行比较。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/1e9f9ede4399bad663e97e47307279c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*ZVrxUA1WdLV0-nwwrXrd8g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Effect of Batch Normalization</figcaption></figure><p id="5dd4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">显而易见，增加批量标准化肯定无助于提高训练精度，从而降低高偏差。批次规范化对减少高方差和解决过拟合问题有作用。</p><h1 id="5ecb" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">辍学的影响</strong></h1><p id="faf9" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">最后，我们将检查漏失层对解决高偏置问题的影响。我们将增加<strong class="ik hi">两个漏失层</strong>到我们之前的最佳模型。</p><pre class="jh ji jj jk fd ky kz la lb aw lc bi"><span id="575b" class="ld jt hh kz b fi le lf l lg lh">dropout_model = tf.keras.Sequential([<br/>tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),<br/>tf.keras.layers.Dense(512,activation = "relu"),<br/>tf.keras.layers.Dropout(0.3),<br/>tf.keras.layers.Dense(128,activation = "relu"),<br/>tf.keras.layers.Dropout(0.2),<br/>tf.keras.layers.Dense(64,activation = "relu"),<br/>tf.keras.layers.Dense(10,activation = "softmax")])</span></pre><p id="782c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我们在隐藏层之间添加了两个丢弃概率分别为<strong class="ik hi"> 0.3 </strong>和<strong class="ik hi"> 0.2 </strong>的丢弃层。我们现在将训练这个模型，并将其准确性与我们以前的最佳模型进行比较。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/291add507ff8be69ac1e7388b49c860d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*dZe7EDCCbAKBz0ODykrKFw.png"/></div></figure><p id="0a08" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">很明显，在我们的隐藏层之间添加漏失层无助于提高训练精度。</p><h1 id="450a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="f81a" class="pw-post-body-paragraph ih ii hh ik b il kq in io ip kr ir is ks kt iv iw ku kv iz ja kw kx jd je jf ha bi translated">在用不同的超参数对多个模型训练相同的数据后，我们可以得出结论，以下变化可以帮助我们解决高偏差问题:</p><ul class=""><li id="65f7" class="lj lk hh ik b il im ip iq ks ll ku lm kw ln jf lo lp lq lr bi translated"><strong class="ik hi">增加隐藏层数。</strong></li><li id="6c14" class="lj lk hh ik b il ls ip lt ks lu ku lv kw lw jf lo lp lq lr bi translated"><strong class="ik hi">增加隐藏单位的数量。</strong></li><li id="7d45" class="lj lk hh ik b il ls ip lt ks lu ku lv kw lw jf lo lp lq lr bi translated"><strong class="ik hi">为更多的时代而训练。</strong></li><li id="d848" class="lj lk hh ik b il ls ip lt ks lu ku lv kw lw jf lo lp lq lr bi translated"><strong class="ik hi">尝试更多神经网络。</strong></li></ul><p id="dbb4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">此外，以下变化对高偏差没有太大影响:</p><ul class=""><li id="89ba" class="lj lk hh ik b il im ip iq ks ll ku lm kw ln jf lo lp lq lr bi translated"><strong class="ik hi">增加训练数据量。</strong></li><li id="80d3" class="lj lk hh ik b il ls ip lt ks lu ku lv kw lw jf lo lp lq lr bi translated"><strong class="ik hi">添加批量归一化</strong></li><li id="5a7e" class="lj lk hh ik b il ls ip lt ks lu ku lv kw lw jf lo lp lq lr bi translated"><strong class="ik hi">添加辍学者</strong></li></ul><p id="dc4b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">虽然神经网络中的上述更新不会对修复欠拟合问题产生巨大影响，但它们肯定有助于减少高方差(或过拟合)。</p><p id="915b" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">我希望你们都喜欢这个快速的小博客！！！下周我将讨论解决高方差问题的各种超参数调优方法。</p><p id="2d4d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is ks iu iv iw ku iy iz ja kw jc jd je jf ha bi translated">本博客中所有模型和图表的代码可以在这里获得——<a class="ae lx" href="https://github.com/sanskar-hasija/Hyperparameter-Tuning" rel="noopener ugc nofollow" target="_blank">https://github.com/sanskar-hasija/Hyperparameter-Tuning</a></p></div></div>    
</body>
</html>