<html>
<head>
<title>Fundamentals of Object Detection: Faster, Mask, Cascade R-CNN 🔥</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体检测的基础:更快，屏蔽，级联R-CNN🔥</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/details-of-faster-mask-cascade-r-cnn-7b9c34326cdf?source=collection_archive---------0-----------------------#2022-06-07">https://medium.com/mlearning-ai/details-of-faster-mask-cascade-r-cnn-7b9c34326cdf?source=collection_archive---------0-----------------------#2022-06-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4559" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更快的R-CNN仍然是比较现代网络架构的对象检测性能的积极引用的基准。在本帖中，我们将讨论论文的见解和贡献。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/432afb8d1b7ec0031bcebc0562beabd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G-xU8gmUlxGOKjnM"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@davisuko?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">davisuko</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="1b3f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">引言——为什么我们会在意2022年？</h2><p id="14d0" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">更快的R-CNN是计算机视觉的一个巨大突破，因为它实现了实时对象检测，并实现了对象检测管道的端到端训练。7年后的2022年，其变体仍被用作实际应用的健壮起点和比较网络架构的基准。</p><ul class=""><li id="df89" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">该论文具有重要的研究贡献，因为它在概念上是每个两阶段对象检测方案的基础，这是一个流行和活跃的研究分支(表面上，该论文也被引用了40K+次)。</li><li id="a8dc" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">尽管不再是s.o.t.a(最先进的),管道仍然非常有效，并且是架构不可知的。更快的R-CNN始终是您的对象检测应用程序的健壮起点🙂</li><li id="f977" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">提出<strong class="ig hi">网络架构的论文常规比较“cascade-mask-rcnn”框架上对象检测的性能</strong>(He et al .，2017；蔡等，2018)，更快rcnn的流行变种。</li><li id="a36d" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">尽管它很受欢迎并且意义重大，但是这个框架<strong class="ig hi">非常复杂</strong>，并且有许多重要的细节需要针对不同的用例进行修改，这也是这篇文章的动机。训练中涉及到很多超参数和细节，有时候不是那么明显。</li></ul><h2 id="c666" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">概观</h2><p id="41a8" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">我们将从简要回顾快速R-CNN和更快的R-CNN管道开始。然后，我们将讨论流行的级联掩码rcnn框架，以及它们如何提高更快的R-CNN的性能。这篇文章主要基于论文中有趣的<em class="lh">见解，而不是超参数等的本质细节。但是，我们将在下一篇帖子中讨论实现中涉及的细节，如果您有兴趣，请继续关注！</em></p><p id="0e42" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在写这篇文章的时候，我参考了流行的公共对象检测框架的实现，但是这篇文章主要关注于<code class="du li lj lk ll b">torchvision</code>实现【R1】。</p><p id="2638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【R1】<em class="lh">torch vision . models . detection . faster _ rcnn。fasterr CNN—</em>[<a class="ae js" href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py" rel="noopener ugc nofollow" target="_blank">github</a>][<a class="ae js" href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a> ]</p><p id="fed2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【R2】<em class="lh">Facebook research/maskrcnn-benchmark</em>和<em class="lh">detectron 2—</em><a class="ae js" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">【github】</a></p><p id="04a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[R3] <em class="lh">更快-rcnn . py torch</em>——[<a class="ae js" href="https://github.com/jwyang/faster-rcnn.pytorch" rel="noopener ugc nofollow" target="_blank">github</a></p><p id="fa39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文章末尾提供了提到的论文的参考文献。</p></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><h2 id="d140" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">关键词的摘要和列表</h2><p id="e8cb" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">您可以跳过这一部分，稍后再回到这一部分:)</p><p id="eb42" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">~快速R-CNN (Girshick等人，2014；Girshick等人，2015) </strong></p><ul class=""><li id="7be9" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">两阶段对象检测:生成区域建议→计算每个感兴趣区域(RoI)的特征→将每个RoI分类为某个对象或背景，回归精确的框坐标。</li><li id="99db" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">选择性搜索:生成区域建议的经典方法。</li><li id="3ad8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">在快速R-CNN中提出RoI Pooling来接收具有各种形状的不同对象的CNN特征，并缩减为恒定的形状。</li><li id="b070" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">背景和前景感兴趣区域在采样数据时是平衡的，因为大多数感兴趣区域是背景，随机采样将导致太多简单的背景样本。快速R-CNN的<em class="lh">图像中心采样</em>拾取N幅图像，以平衡的方式从每幅图像中提取(batch_size / N)个ROI。</li><li id="4ab8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">使用额外的<em class="lh">背景</em>类和softmax &amp;交叉熵损失来训练对象类的分类。如Girshick等人在2015年提出的，用平滑的L1损失来训练包围盒回归。两个对象以组合损失被联合学习。</li></ul><p id="a31c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">更快的R-CNN(任等，2015) </strong></p><ul class=""><li id="3591" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">基于选择性搜索的区域建议是推理速度的主要瓶颈。快速R-CNN提出了一种基于学习的方法，该方法基于区域提议网络，用于精确和廉价的区域提议。</li><li id="74e1" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">区域提议网络(RPN)在CNN特征地图上滑动，以预测对象分数——对象或背景和框回归的概率——精确的框坐标细化<em class="lh"> k </em>参考<em class="lh">锚框</em>。</li><li id="560c" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">RPN与最初的快速R-CNN检测网络共享特征，并且被联合训练。作者还讨论了3种联合训练RPN和快速R-CNN的方案。</li></ul><p id="0d56" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">屏蔽R-CNN(何等，2017) </strong></p><ul class=""><li id="0998" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">为训练更快的R-CNN总结一个更好的基线，包括更好的数据扩充、超参数和模型架构。</li><li id="1650" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">提出了一个名为RoIAlign的RoIPool替代方案，它允许RoI的像素到像素对齐。</li><li id="6b67" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">提出了一个基于RPN区域建议的密集预测框架，并证明了其在关键点估计和实例分割方面的有效性。</li></ul><p id="3942" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">级联R-CNN(蔡等，2018) </strong></p><ul class=""><li id="df39" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">调查并观察IoU重叠阈值问题中有趣的挑战和现象。</li><li id="31cd" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">用小IoU阈值进行训练会产生噪声样本，而用大阈值进行训练会降低性能，因为与训练相比，在推断时会出现过拟合和低质量区域建议。</li><li id="1834" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">当根据其他IoU阈值进行评估时，使用特定IoU阈值训练的模型可能是次优的，或者给出了不同IoU阈值水平的区域建议。</li><li id="7fa9" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">建议级联专门的检测器来解决挑战，并能够显著提高性能。</li></ul></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><h1 id="368e" class="lt ju hh bd jv lu lv lw jz lx ly lz kd ma mb mc kg md me mf kj mg mh mi km mj bi translated">快速概述更快的R-CNN</h1><h2 id="e168" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">两阶段目标检测</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mk"><img src="../Images/bd60a767c92073bf314b98ffd5f5cf89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjT_TnaffUQQOpc7FbLqJg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Girshick et al., 2014, an overview of R-CNN, a popular 2-stage object detection method</figcaption></figure><p id="a505" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在两阶段对象检测中(Girshick等人，2014年)，一系列区域建议用于表示潜在对象的位置，在上图中用黄色方框表示。然后进一步评估每个区域提议，从而确定对象的类别和精确位置。</p><p id="10a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以前的方法，如R-CNN(Girshick等人，2014年)和Fast R-CNN(Girshick等人，2015年)依靠试探法，如<a class="ae js" href="https://pyimagesearch.com/2020/06/29/opencv-selective-search-for-object-detection/" rel="noopener ugc nofollow" target="_blank">选择性搜索</a>，从图像中计算预定义的区域建议。然后通过基于学习的方法处理这些初始区域提议，这些方法决定每个提议是否是实际对象(即，没有背景)，将类别分配给提议，并改进边界框。</p><h2 id="ba7d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">快速R-CNN流水线</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ml"><img src="../Images/3f55306015ba8a297c1b28772b8881ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FUTOXkVNzfkxSl4op8CgQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Girshick et al., 2015, an overview of the Fast R-CNN framework</figcaption></figure><p id="07b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在快速R-CNN框架中，图像和预先计算的感兴趣区域被提供给模型。使用CNN将图像缩小为特征图，并且基于RoI的位置裁剪每个RoI的特征。这个裁剪的特征表示该RoI的高级信息。</p><p id="15eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后应用<em class="lh"> RoIPooling </em>图层将每个裁剪后的要素处理成相同的大小，这样它就可以被MLP处理。一系列完全连接的层为每个RoI创建RoI特征向量。最后，特征向量用于1)将对象类别分配给RoI，以及2)细化RoI的初始框坐标。</p><h2 id="48e3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">快速R-CNN训练</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mm"><img src="../Images/dd1695e1e6e654ccfb35f0c3d129ccf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*1pwQBFNSrVZtxzpFsOLiRQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Image by author, example output of selective search</figcaption></figure><p id="0a0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">快速R-CNN训练的目标是从RoI中准确预测对象类别和边界框细化。选择性搜索等区域建议方法可为每幅图像生成约2000个ROI。图像中的对象通常少得多，因此与包含实际感兴趣对象的前景ROI相比，绝大多数区域提议包含背景(负)ROI。因此，大多数方法使用一批平衡的RoI来训练模型，其中正RoI和负RoI的比率被严格控制。</p><p id="9173" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练批次大小为K=128的快速R-CNN时，我们从图像数据集中读取N=2个图像，并从每个图像中选择64=K/N个ROI，其中16个为正ROI，48个为负ROI。如果值在[0.1，0.5]之间，则具有与至少0.5的基本事实边界框重叠的并集上的交集(IoU)的对象提议被认为是正的和负的。</p><p id="4548" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于每个RoI，该模型预测1)对象类别和2)边界框细化。对于类别预测，负ROI被标记为“0”类别，而正ROI被标记为相应的对象类别。使用预测和基础事实之间的交叉熵来训练该模型。边界框细化预测试图预测区域提议和相关g.t .边界框的确切位置之间的xywh偏移。这是通过最小化预测和实际情况之间的平滑MSE损失(Girshick等人，2015年)来实现的。稍后，我们将与代码一起精确定义如何计算偏移量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mn"><img src="../Images/787b276b599043966e682eae90cebbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZE3YXm4WMhd4lorETLtFxA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Girshick et al., 2015, smoothed MSE loss</figcaption></figure><p id="0632" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“快速R-CNN RoI头”的两个分支用组合损失函数来共同训练。注意，仅当对象提议是<em class="lh">前景</em>类时，才计算边界框回归损失。这在下面的等式中被描述为“[u ≥ 1]=1如果u ≥ 1否则0”。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mo"><img src="../Images/24f15e5bef1c20f055df2f596ae7839d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MU2AhZMQMHz8l5wsby_giA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Girshick et al., 2015, Joint Multi-task loss function of Fast R-CNN.</figcaption></figure><h2 id="5916" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">更快的R-CNN</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mp"><img src="../Images/57d1a3792988769a96f3924714ab4563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDr_adQD2FcqpIBpBiooqw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Ren et al., 2015, Faster R-CNN can efficiently generate proposals.</figcaption></figure><p id="ab91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上表描述了每一步推理所需的时间。描述快速R-CNN框架的第一行(SS+快速R-CNN)表明，主要的瓶颈不是CNN特征提取器，而是“提议”阶段，该阶段执行选择性搜索(SS)以找到区域提议。为了取代选择性搜索，更快的R-CNN(Ren等人，2015年)建议区域提议网络(RPN)与卷积特征提取器共享特征，以有效地生成区域提议。结果展示了丰富但几乎无成本的区域提议。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mq"><img src="../Images/39249cdcd3959dbefda87f32fc9565c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRva18DrBo52MJJg1yRWPg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Ren et al., 2015, an overview of Faster R-CNN</figcaption></figure><p id="1fd7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更快的R-CNN的整体管道与快速R-CNN非常相似。CNN网络从图像中提取特征。快速R-CNN对图像执行选择性搜索以生成区域提议，而区域提议网络(RPN)采用CNN特征来生成用于更快R-CNN的区域提议。RPN输出约2K个区域建议。区域提议和CNN特征然后被馈送到快速R-CNN头，该头预测对象分类和边界框回归。下面代码片段中的第8~10行描述了高级实现。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mr ms l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx"><a class="ae js" href="https://github.com/pytorch/vision/blob/ba4b0db5f8379e33dae038f82219531a44ff08c3/torchvision/models/detection/generalized_rcnn.py" rel="noopener ugc nofollow" target="_blank">[code]</a></figcaption></figure><h2 id="ba52" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">区域提案网络</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mt"><img src="../Images/44f3950145829e79bc9f8b452059c507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elcA0o7khL8tky9FRF7hRw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Ren et al., 2015, an overview of region proposal network(RPN)</figcaption></figure><p id="24fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了生成区域提议，区域提议网络在卷积特征图上滑动。这个小网络接收部分特征图并预测多个区域提议。在每个滑动窗口位置，最多可以提出k个区域建议。这k个提议基于k个参考锚盒，参考锚盒被定义为比例={128，256，512}和纵横比={1:1，1:2，2:1}的组合，k=9用于更快的R-CNN。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mu"><img src="../Images/d5280eee13f0adc516f2db633de199a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YwySodD2QU_CeuAz4PXBw.png"/></div></div></figure><p id="8999" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">具体地，特征图被投影到中间层中，该中间层用于进行2个预测:1)对象性分数(<em class="lh"> cls </em>)预测基于以窗口为中心的每个k锚框是对象还是不是对象的区域提议的概率，以及2)回归层(<em class="lh"> reg </em>)预测k个框的坐标。回归层预测相对于相应参考锚定框的框细化。通过组合参考锚点、来自回归层的框坐标细化，以及通过预测的客观性分数来过滤提议，该模型可以预测准确的区域提议。</p><figure class="jd je jf jg fd jh"><div class="bz dy l di"><div class="mr ms l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx"><a class="ae js" href="https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/models/detection/rpn.py#L16" rel="noopener ugc nofollow" target="_blank">[code]</a></figcaption></figure><p id="575e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的是，解析卷积特征图并生成中间层的滑动窗口可以表示为核大小为n×n且没有填充的单个卷积层。然后，可以使用1×1卷积层来预测来自中间特征的客观分数或框细化。上面的代码确实演示了这一点(注意self.conv在这个实现中可以有多个层)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mv"><img src="../Images/c3b21a822aa424cbb3fe3347d18c597f.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*QAF7XNppuNAAhR3Ss7TmhA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Ren et al., 2015, training loss of RPN in Faster R-CNN</figcaption></figure><p id="d798" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以通过向每个锚点分配二进制类别标签(是否为对象)并测量交叉熵损失来训练对象性得分。与用于快速R-CNN训练的“背景”类相似但略有不同，如果锚具有高于0.7的IoU重叠，则锚被认为是正的，对于所有地面实况框，锚被认为是负的IoU重叠低于0.3。中间有IoU重叠的锚点不用于训练客观性分数。</p><p id="a0b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，在使用RPN进行训练和推理的过程中，需要注意一些细节。在训练期间，所有的跨界锚点都被忽略，因此它们不会造成损失，但是在推断期间所有的锚点都被保留。作者建议在测试时将跨边界框裁剪到图像边界。阈值为0.7的非最大抑制(NMS)被应用于区域提议，以便移除冗余提议。在测试时，基于客观性分数的前N名排序建议用于检测。</p><h2 id="42ef" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">更快的R-CNN训练</h2><p id="957a" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">快速R-CNN是快速R-CNN和使用区域建议网络的可学习区域建议的组合。因此，培训有两个主要的培训目标:</p><ul class=""><li id="eed4" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">快速R-CNN模块—盒回归(平滑L1)，对象分类(交叉熵)</li><li id="4893" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">区域建议网络—客观性分数(交叉熵)，箱式回归</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mq"><img src="../Images/39249cdcd3959dbefda87f32fc9565c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRva18DrBo52MJJg1yRWPg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Ren et al., 2015, an overview of feature sharing between RPN and Fast R-CNN</figcaption></figure><p id="99d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">快速R-CNN和RPN都可以独立训练，但由于训练一个模块会改变另一个模块的有用权重，Ren等人(2015)讨论了通过共享如上图所述的特征来学习快速R-CNN和RPN的统一网络的几种方法。作者讨论了如下3种训练方案:</p><ul class=""><li id="cb2c" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated"><em class="lh">交替训练</em>方法首先训练RPN，然后使用来自RPN的建议训练快速R-CNN。然后用快速R-CNN调好的网络初始化RPN，这个过程是迭代的。</li><li id="15e2" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><em class="lh">近似联合训练</em>方法来预测区域建议，并使用区域建议来计算RPN损失，并使用这些区域建议来训练快速R-CNN检测器。这是有效的，因为在训练RPN和快速R-CNN时只进行一次向前/向后传递。</li><li id="0bf8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated"><em class="lh">考虑非近似联合训练</em>方法，因为在快速R-CNN的RoIPooling中使用的区域提议实际上是输入图像的函数。换句话说，来自快速R-CNN检测器的梯度也必须流入在做出区域提议中涉及的RPN和卷积特征。然而，RoIPooling是<em class="lh">近似联合训练</em>，因为梯度在它们之间被切割。</li></ul><p id="b319" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Ren等人(2015)的作者默认采用替代训练方法，但指出近似联合训练方法更快，同时根据经验证明结果接近。Mask R-CNN后期实验(何等，2017)实际上采用的是近似联合训练策略。事实上，上面的代码示例<code class="du li lj lk ll b">GeneralizedRCNN</code>演示了这种方法。然而，作者将非近似联合训练方法描述为非平凡的，并且没有提供关于它的实验。</p><p id="2836" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们将深入研究级联掩码rcnn变体，它可以提高更快的R-CNN的性能！！🔥</p></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mw"><img src="../Images/12df90e6dbf2eb5e123a9106ef81efd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8I7p9esf_oerDYRqsq9a1Q.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">He et al., 2017, Mask R-CNN results on instance segmentation</figcaption></figure><h2 id="ec3d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">改进快速R-CNN</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mx"><img src="../Images/8c3246600487283ced288d9de8792b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*1DKvHILEkX-BPRzgXPciTg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">He et al., 2017, Ablation on enhancing Faster R-CNN</figcaption></figure><p id="fcf1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Mask R-CNN(He et al .，2017)主要致力于提出一种用于实例分割的强大基线系统，但作者也提出了一种基于更快R-CNN的明显更好的对象检测基线。我们将首先讨论在何等人2017年的附录中描述的用于提高对象检测性能的超参数和算法。</p><ul class=""><li id="074a" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">更新基线:将训练步长从120k增加到180k，将NMS后处理阈值从0.3增加到0.5。</li><li id="d862" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">端到端(e2e)训练:使用近似的联合训练策略，而不是原来的交替训练策略，这实际上提高了性能。</li><li id="9307" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">ImageNet-5k:在ImageNet-5k而不是1k子集上预先训练特征提取器。</li><li id="65ca" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">训练时间增加:随机缩放图像作为数据增加，进一步增加训练预算到26万</li><li id="9e82" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">更深入:使用ResNext152模型更深入</li><li id="2ad8" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">非本地:通过使用非本地块来改进模型架构</li><li id="90f2" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">测试时间增强:在[400，1200]像素的图像尺度上使用<a class="ae js" href="https://github.com/qubvel/ttach" rel="noopener ugc nofollow" target="_blank"> TTA </a>，步长为100，并在水平翻转上使用。</li></ul><p id="f13a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有一些改编自Lin等人(2017)的修改，在本消融术中未提及:</p><ul class=""><li id="0c3c" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">调整图像大小，使其比例(短边)为800像素</li><li id="a5d5" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">15使用比例={32，64，128，256，512}和纵横比={1:1，1:2，2:1}的参考锚。</li></ul><h2 id="11b8" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">罗伊Align</h2><p id="3128" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">RoIAlign是RoIPooling的替代产品，无需量化即可实现像素到像素的对齐。作者认为这是快速/更快R-CNN 的主要缺失部分，用于密集预测任务，例如实例分割，并发现这一更新对实例分割至关重要。该层仍然为边界框级别的性能提供了细微的改进。我们先来看看RoIPooling的问题。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es my"><img src="../Images/ca6335674842ca4579ae17e0c0dbeefd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*Peb1stsxklVqkMkK_GKeUQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Original image by He et al., 2017, Illustration of the quantization of RoIPool</figcaption></figure><p id="7792" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RoIPool是一种标准操作，用于从每个RoI中提取小特征图(例如，7×7 ), RoI类似于上面的黑盒。RoIPool首先将浮点数RoI量化为特征图的离散粒度(即红框)。这个量化的RoI然后被细分成空间箱，这些空间箱本身被量化(即，蓝点)，并且最后由每个箱覆盖的特征值被聚集(通常通过最大池)。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es my"><img src="../Images/bd759208323e0c575433a4c82df0eae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*PTOIYGh9O-mxKEo1UtWxKA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">He et al., 2017, Overview of RoIAlign</figcaption></figure><p id="4588" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">RoIAlign没有量化坐标来索引特征地图的最近离散值，而是使用双线性插值来计算4个邻近特征的值并聚合它们。不执行量化。</p><h2 id="f35a" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">实例分割</h2><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mz"><img src="../Images/7291023c85efe28c6a65fd66d76288bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ks3NaHJYRBH_yapXfoGUjw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">He et al., 2017, Bounding box object detection results of Mask R-CNN</figcaption></figure><p id="7613" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实例分割的任务是检测对象的边界框，同时精确分割每个实例。虽然这个主题可能看起来不相关，但我想简单提一下，作者表明共同学习遮罩和边界框有利于检测器。在上表中，当在<em class="lh">快速R-CNN，RoIAlign </em>实验中移除屏蔽分支时，性能下降。</p><p id="0b93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，请注意，虽然我们从对象检测的角度将该论文视为更快的R-CNN的变体，但Mask R-CNN是一个令人惊叹的基线，可以使用两级检测器和RPN的方法解决许多其他涉及定位的任务。</p><blockquote class="na nb nc"><p id="3e97" class="ie if lh ig b ih ii ij ik il im in io nd iq ir is ne iu iv iw nf iy iz ja jb ha bi translated">我们展示了COCO挑战套件的所有三个方面的最佳结果，包括实例分割、包围盒对象检测和人物关键点检测。没有花里胡哨，Mask R-CNN在每项任务上都优于所有现有的单一模型条目，包括COCO 2016挑战赛的获胜者。我们独立地为每个类别预测一个二进制掩码，而没有类别之间的竞争</p></blockquote><p id="3d83" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，我们将讨论我们的最后一篇论文，名为Cascade R-CNN(蔡等人，2018年)，该论文基于对阈值的深刻观察。🔍</p><h2 id="9164" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">IoU阈值</h2><p id="6e00" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">我们已经看到，在两阶段对象检测中，IoU阈值被定义为确定区域提议是正的(对象)还是负的(背景)。如果IoU与基础事实对象的重叠大于阈值，则区域方案被视为正，否则为负。我们还讨论了快速R-CNN和后来的方法使用0.5的IoU阈值。似乎很公平，但在蔡等人2018年的文章中，作者认为门槛问题可能不是那么微不足道。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ng"><img src="../Images/4b1caf25514d9dc825b7883156909b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*Gs_cq7LE2skGjCIfi0hiKg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Cai et al., 2018, a threshold of 0.5 permits noisy detections!</figcaption></figure><p id="923b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，作者表明常用的IoU阈值0.5可能会允许许多被认为是人类误报的检测，如上图所示。好，那如果我们用更高的阈值比如0.7训练，<em class="lh">问题解决</em>？下图(d)说明了用于训练的3个不同阈值的对象检测性能，其中x轴是用于测量AP的IoU阈值。我们可以看到，在曲线的大多数部分，使用较高的阈值具有较差的对象检测性能。</p><p id="3a88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者指出了两个问题，这使得提高门槛和只允许高质量的盒子非常具有挑战性。</p><ol class=""><li id="6500" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb nh kz la lb bi translated">过度拟合，因为数据集的多样性随着IoU重叠阈值的增加而呈指数下降。</li><li id="3d4b" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb nh kz la lb bi translated">IoU之间的推理时间不匹配:虽然我们可能能够在训练期间人工采样这样好的区域提议，但是RPN并不总是能够生成具有高IoU重叠的区域提议。</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ni"><img src="../Images/486a0d16c735be848aecc374f74aef6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThEYjC-WxNAoaEWlLmIZ9g.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Cai et al., 2018, the performance of box regressor and detector based on IoU threshold used to train model</figcaption></figure><p id="572a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们能做些什么吗🙁？图(c，d)给出了我们可以研究的两个有趣的证据。首先，曲线(c)显示了快速R-CNN头部边界框回归前后测得的IoU重叠。</p><ul class=""><li id="1e14" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">注意，边界框的精度趋于提高(例如，具有0.7 IoU重叠的初始框被细化为具有约0.8 IoU重叠)，因为大多数曲线都在灰线上。</li><li id="0f3e" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">尤其是当输入的IoU阈值与训练期间使用的IoU阈值相似时。当给出0.5~0.6 IoU重叠的相似图像时，用IoU重叠&gt; 0.5的区域建议训练的边界框回归器可能做出良好的改进。</li></ul><p id="342a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，在图(d)中，对于低IoU示例，用u = 0.5训练的检测器优于u = 0.6的检测器，而在较高IoU电平时表现不佳。重要的是，我们注意到，一般来说，在单个IoU电平上优化的检波器在其它电平上不一定是最佳的。为了总结在应该使用的IoU重叠阈值方面观察到的挑战，</p><ul class=""><li id="5e0b" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb ky kz la lb bi translated">使用高IoU重叠阈值进行训练会导致1)由于样本减少而过度拟合，以及2)由于较差的区域提议而导致推断时间不匹配。</li><li id="4288" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb ky kz la lb bi translated">针对某一IoU阈值优化的检波器可能不适合其它IoU阈值。为了在另一个阈值上实现最佳性能，需要一个用不同阈值训练的优化器。</li></ul><blockquote class="na nb nc"><p id="8667" class="ie if lh ig b ih ii ij ik il im in io nd iq ir is ne iu iv iw nf iy iz ja jb ha bi translated">…我们将假设的质量定义为其与基础事实的IoU，将检测器的质量定义为用于训练它的IoU阈值u。基本思想是，单个检测机只能对单个质量水平达到最佳。</p></blockquote><h2 id="f166" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">级联R-CNN</h2><p id="382c" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">那么，我们如何优化所有IoU重叠电平呢？我们将首先讨论解决这个问题的两种简单方法，并建议将它们结合起来可能是最有效的。更快的R-CNN框架如下图(a)所示。卷积特征被给予RPN(H0 ),并且创建对象提议B0。然后，这些提议被用于解析卷积特征图，并且RoI头预测最终的边界框细化B1和对象类别C1。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nj"><img src="../Images/096be9dee55d8ce4fd04e5cf6327688c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VpAMgnbNMzhm2tA2cRtf2Q.png"/></div></div></figure><p id="1ab0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一种方法是迭代边界框细化，如图(b)所示。细化的对象建议(或边界框)C1用于再次解析卷积特征图。同样快速的R-CNN网络预测新的和可能改进的对象类C2和边界框B2。这个过程可以反复重复。然而，作者认为这种想法忽略了两个问题。</p><ol class=""><li id="5a47" class="kt ku hh ig b ih ii il im ip kv it kw ix kx jb nh kz la lb bi translated">随着改进的继续，IoU可能会增加。如前所示，在u = 0.5时训练的回归量对于较高的IoU重叠来说是次优的。</li><li id="4937" class="kt ku hh ig b ih lc il ld ip le it lf ix lg jb nh kz la lb bi translated">边界框的分布在每次迭代后会发生显著变化。虽然回归变量对于初始分布是最优的，但在此之后它可能是次优的。</li></ol><p id="4f12" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一种方法是开发分类器的集成，每个分类器用不同的IoU阈值进行优化。图(c)对此进行了描述，损耗如下式所示，其中U={0.5，0.55，，0.75}是一组递增的IoU阈值。这种方法是有缺陷的，因为正样本集随着u的增加而快速减少，这是由于RPN的较差区域提议，如下面最左边的直方图所示，该直方图示出了初始RPN区域提议的质量。这意味着具有高IoU阈值的检测器非常容易过度拟合，并且推断质量低，这使得它们毫无意义。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nk"><img src="../Images/62aad75ea89c810a0a181a33a1a66709.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*KA_EMvftJgZLQLQ1fvKr6Q.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nl"><img src="../Images/73cb9827c8376d96674b7ab6d3d25faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*ludet4t8wbLkyZao6hLrYA.png"/></div></figure><p id="13f1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，图(d)中描述了级联R-CNN框架，该框架使用级联的<em class="lh">专用</em>回归器，这些回归器通过增加IoU重叠阈值进行训练。与作为后处理应用的(b)的迭代盒细化不同，该框架用于训练和推理。使用先前的观察，即包围盒回归通常提高下一个对象提议的质量，我们可以共同增加用于训练的IoU阈值。</p><p id="34ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者建议按顺序使用IoU重叠阈值{0.5，0.6，0.7}训练的快速R-CNN头将RPN的输出细化3次。上面的直方图说明了盒子的质量在整个回归层中是如何提高的。你可能认为所提出的方法非常类似于(b)的迭代盒细化策略，但是惊讶地看到Cascade R-CNN的巨大改进。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es nm"><img src="../Images/28023d080d11d743bf3cf0c41615eef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*qRrRcIsrGad9uNde5E2YJg.png"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nn"><img src="../Images/808985593c0e5bca5c23216d1dd48b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZjjI_hVuufvkFIS5uu7KQ.png"/></div></div></figure><p id="bd8e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，消融是非常引人注目的，因为它能够显著提高不同检测框架和主干架构的性能。</p></div><div class="ab cl lm ln go lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ha hb hc hd he"><h2 id="9ca8" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">评论</h2><p id="69b5" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">cascade-mask-rcnn基准是Mask R-CNN和Cascade R-CNN的组合，其中Mask R-CNN的RoIAlign层和改进的超参数与Cascade R-CNN的级联包围盒回归模块一起使用。常见的实现可以在<a class="ae js" href="https://github.com/open-mmlab/mmdetection/tree/master/configs/cascade_rcnn" rel="noopener ugc nofollow" target="_blank"> mmDetection </a>中找到。</p><p id="9a21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章是为了减轻我的技术债务，但我真的很喜欢看计算机视觉传奇论文的见解。特别是，我很高兴读到neet RoIAlign层和参考锚框+轻微细化的概念，这是一个很好的抽象，表示以某个点为中心的边界框的无限可能性。</p><p id="b1f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我感觉到的一件事是，对于前景和背景ROI之间的类别不平衡问题，两级检测器的解决方法还有改进的空间。自R-CNN的第一个提议(Girshick等人，2014年)以来，一直使用1:3采样策略。</p><p id="dc6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另外，<em class="lh">非近似联合训练</em>的训练网络方法对我来说也很有前途。</p><p id="4920" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我没有包括更多最近的物体检测策略，但是我期待着回顾许多关于物体检测的论文，所以如果你感兴趣，请继续关注😇！</p><h1 id="aa26" class="lt ju hh bd jv lu no lw jz lx np lz kd ma nq mc kg md nr mf kj mg ns mi km mj bi translated">参考</h1><p id="24d0" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">Girshick、j . Donahue、t . Darrell和j . Malik(2014年)。丰富的特征层次，用于精确的对象检测和语义分割。在<em class="lh">IEEE计算机视觉和模式识别会议论文集</em>(第580–587页)。</p><p id="7fdb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">吉尔希克河(2015年)。快速r-cnn。IEEE计算机视觉国际会议记录(第1440-1448页)。</p><p id="d086" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">任，孙，何，吉希克(2015)。更快的r-cnn:用区域建议网络实现实时目标检测。<em class="lh">神经信息处理系统的进展</em>、<em class="lh"> 28 </em>。</p><p id="6077" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">林，T. Y .，多拉尔，p .，吉尔希克，r .，何，k .，哈里哈兰，b .，&amp;贝隆吉，S. (2017)。用于目标检测的特征金字塔网络。在<em class="lh">IEEE计算机视觉和模式识别会议论文集</em>(第2117-2125页)。</p><p id="e093" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">He，k .，Gkioxari，g .，Dollár，p .，&amp; Girshick，R. (2017年)。屏蔽r-cnn。IEEE计算机视觉国际会议论文集(第2961-2969页)。</p><p id="67e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">蔡，赵，，倪(2018)。级联r-cnn:钻研高质量的对象检测。在<em class="lh">IEEE计算机视觉和模式识别会议论文集</em>(第6154–6162页)。</p><div class="nt nu ez fb nv nw"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nx ab dw"><div class="ny ab nz cl cj oa"><h2 class="bd hi fi z dy ob ea eb oc ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="od l"><h3 class="bd b fi z dy ob ea eb oc ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="oe l"><p class="bd b fp z dy ob ea eb oc ed ef dx translated">medium.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok jm nw"/></div></div></a></div></div></div>    
</body>
</html>