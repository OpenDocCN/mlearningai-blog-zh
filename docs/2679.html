<html>
<head>
<title>Need help hitting your word count?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">需要帮助你计算字数吗？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/need-help-hitting-your-word-count-ca69c9e7cbbb?source=collection_archive---------6-----------------------#2022-05-29">https://medium.com/mlearning-ai/need-help-hitting-your-word-count-ca69c9e7cbbb?source=collection_archive---------6-----------------------#2022-05-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="6a51" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">让伯特用人工智能来充实你的文章！</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/ef7b5c957e0fe80386441c21131e75ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_yC8ExF0mzAx3yd-HJR3g.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Generated by OpenAI’s DALL-E2</figcaption></figure><p id="d85f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">你可能已经看到OpenAI的GPT如何在给出提示的情况下生成非常令人信服的文本，但是如果你可以使用AI以一种在相邻句子的上下文中有意义的方式添加到文档中，那不是很好吗？还需要500字，但你已经写了你能想到的关于这个话题的所有内容？没问题。</p><p id="124f" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">GPT是因果语言模型或自回归模型的一个例子。这意味着它模拟了一个单词出现的概率，给出了前面出现的文本。这些模型是生成语言的自然选择，但它们只能考虑前面的句子，而不能考虑后面的任何句子。</p><p id="4958" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">输入伯特。正如它的全名——来自变压器的双向编码器表示——所表明的，它从向后和向前两个方向获取上下文。但是GPT是原始<a class="ae ki" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">变压器</a>的解码器部分，而伯特只是编码器。这使得BERT适用于分类任务，但如何使用它进行语言生成并不明显。</p><p id="8949" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">碰巧的是，伯特接受训练的任务之一是猜测文本中随机隐藏的单词。在<a class="ae ki" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a> Python包中的<em class="kj"> BertForMaskedLM </em>模型可以用这种方式来填补空白。然而，单纯要求伯特填补大量连续空缺的结果是非常令人失望的。相反，一种称为MCMC(马尔可夫链蒙特卡罗)的技术可以用于从以周围令牌为条件的分布中进行采样。这个想法最早发表在论文<a class="ae ki" href="https://arxiv.org/abs/1902.04094" rel="noopener ugc nofollow" target="_blank"> BERT有嘴，而且是必须要说话:BERT作为马尔可夫随机场语言模型</a>。</p><p id="c993" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">那么它的效果如何呢？这里有一个取自《T8》封底的例子，小龙虾在那里唱歌。<strong class="jo hi">粗体</strong>中的句子是模型添加的。</p><blockquote class="kk kl km"><p id="4eec" class="jm jn kj jo b jp jq ii jr js jt il ju kn jw jx jy ko ka kb kc kp ke kf kg kh ha bi translated">多年来，“沼泽女孩”的谣言一直困扰着巴克利湾，这是北卡罗来纳州海岸的一个安静的小镇。在过去的几年里，这个城镇有着臭名昭著的犯罪、毒品和谋杀的历史。因此，1969年末，当英俊的蔡斯·安德鲁斯被发现死亡时，当地人立即怀疑所谓的沼泽女孩基亚·克拉克。<strong class="jo hi">毕竟，她是酒保，也是泥溪传奇谋杀案的嫌疑人。</strong>但是Kya不是他们说的那样。<strong class="jo hi">大约十几年前，当她在佛罗里达州被发现时，她强壮健康。敏感而聪明的她在这片她称之为家的沼泽地里独自生存了多年，在海鸥中寻找朋友，在沙滩上学习。她的自然本能与生命周期相协调，因此她很快乐。然后，她渴望被抚摸和被爱的时候到了。在她寻找自己身份的过程中，那些在危险中挺身而出帮助她的人。当两个来自镇上的年轻人被她的野性之美所吸引时，Kya开始了新的生活——直到不可思议的事情发生了。她踏上了一段旅程，进入了一个没有人能理解她想说什么的世界。</strong></p></blockquote><p id="a0a9" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">HuggingFace现在还托管了<a class="ae ki" href="https://huggingface.co/spaces" rel="noopener ugc nofollow" target="_blank"> Spaces </a>，在那里你可以用几行代码在<a class="ae ki" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>或<a class="ae ki" href="https://www.gradio.app/" rel="noopener ugc nofollow" target="_blank"> Gradio </a>应用中部署你的模型。你可以在这里试试我的<em class="kj">嵌入式</em>模型</p><div class="kq kr ez fb ks kt"><a href="https://huggingface.co/spaces/teticio/inBERTolate" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab dw"><div class="kv ab kw cl cj kx"><h2 class="bd hi fi z dy ky ea eb kz ed ef hg bi translated">teticio的拥抱脸空间</h2><div class="la l"><h3 class="bd b fi z dy ky ea eb kz ed ef dx translated">发现由社区制作的令人惊叹的ML应用程序</h3></div><div class="lb l"><p class="bd b fp z dy ky ea eb kz ed ef dx translated">huggingface.co</p></div></div><div class="lc l"><div class="ld l le lf lg lc lh jg kt"/></div></div></a></div><p id="9cdc" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">并在这里找到源代码</p><div class="kq kr ez fb ks kt"><a href="https://github.com/teticio/inBERTolate" rel="noopener  ugc nofollow" target="_blank"><div class="ku ab dw"><div class="kv ab kw cl cj kx"><h2 class="bd hi fi z dy ky ea eb kz ed ef hg bi translated">GitHub - teticio/inBERTolate:使用BERT来充实你的文章，从而达到你的字数要求！</h2><div class="la l"><h3 class="bd b fi z dy ky ea eb kz ed ef dx translated">生成与前面和后面的句子都在上下文中的句子。像GPT这样的模特不是…</h3></div><div class="lb l"><p class="bd b fp z dy ky ea eb kz ed ef dx translated">github.com</p></div></div><div class="lc l"><div class="li l le lf lg lc lh jg kt"/></div></div></a></div><p id="1465" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">如果你发现它有点太随机，那么尝试减少<em class="kj">温度</em>参数或增加<em class="kj">典型_p </em>参数。</p><h1 id="95d9" class="lj lk hh bd ll lm ln lo lp lq lr ls lt in lu io lv iq lw ir lx it ly iu lz ma bi translated">小心使用</h1><p id="f799" class="pw-post-body-paragraph jm jn hh jo b jp mb ii jr js mc il ju jv md jx jy jz me kb kc kd mf kf kg kh ha bi translated">语言模型从给定的数据中学习模式，但它们也学习偏见。由于BERT并不常用于语言生成，我很想看看它与GPT-2在这方面有何不同，并对结果感到震惊。</p><p id="56f1" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我向RoBERTA-large模型提供了两个提示，以查看这将如何影响结果:</p><blockquote class="kk kl km"><p id="e422" class="jm jn kj jo b jp jq ii jr js jt il ju kn jw jx jy ko ka kb kc kp ke kf kg kh ha bi translated">“白人曾是一名…”</p></blockquote><p id="f548" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">和</p><blockquote class="kk kl km"><p id="905d" class="jm jn kj jo b jp jq ii jr js jt il ju kn jw jx jy ko ka kb kc kp ke kf kg kh ha bi translated">"那个黑人当了一名……"</p></blockquote><p id="d8de" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">模型生成如下(不是我写的！):</p><blockquote class="kk kl km"><p id="e3a6" class="jm jn kj jo b jp jq ii jr js jt il ju kn jw jx jy ko ka kb kc kp ke kf kg kh ha bi translated">这位白人在希拉里·克林顿执政期间担任气候研究员，并被任命为该办公室的副主任。</p><p id="0395" class="jm jn kj jo b jp jq ii jr js jt il ju kn jw jx jy ko ka kb kc kp ke kf kg kh ha bi translated">“一个黑人在附近做失业的看门人。下院议员发现了他，并把他和另外两个人一起处以私刑。”。</p></blockquote><p id="d95a" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这比我预想的要极端得多。尽管人们永远不应该根据单一样本得出结论，但这说明了模型所依据的数据。如果我告诉你<a class="ae ki" href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/" rel="noopener ugc nofollow" target="_blank"> RoBERTA </a>是由Meta(脸书)建立和训练的，你可能会得出错误的结论，因为他们使用的<a class="ae ki" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">数据集</a>实际上是基于从Reddit帖子中抓取的书籍、新闻和网站。</p><div class="kq kr ez fb ks kt"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ku ab dw"><div class="kv ab kw cl cj kx"><h2 class="bd hi fi z dy ky ea eb kz ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="la l"><h3 class="bd b fi z dy ky ea eb kz ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lb l"><p class="bd b fp z dy ky ea eb kz ed ef dx translated">medium.com</p></div></div><div class="lc l"><div class="mg l le lf lg lc lh jg kt"/></div></div></a></div></div></div>    
</body>
</html>