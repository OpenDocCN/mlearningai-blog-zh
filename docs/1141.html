<html>
<head>
<title>Ch 6. Optimizing Data for Flexible Image Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第六章。优化数据以实现灵活的图像识别</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7?source=collection_archive---------1-----------------------#2021-10-10">https://medium.com/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7?source=collection_archive---------1-----------------------#2021-10-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="cb9a" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">我们如何调整输入数据和标签，以鼓励神经网络像人类一样灵活地“感知”图像？</h2></div><h1 id="8ba8" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated"><strong class="ak">人类感知的灵活性👼</strong></h1><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/df4c85e413963f4fd7f0d1bacc99c895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZWzcc4EPd8vf3C7uh_swA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx"><strong class="bd iy">Visual Inputs: </strong>toy blocks, (moving) person, (my) hands, (my) arms, floor, wall</figcaption></figure><p id="f569" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">从我们很小的时候起，我们就通过我们的五种感官观察不同的刺激并与之互动来了解这个世界。人类的感知包括在参考“我所知道的”数据库的同时，对环境中的事物进行连续的命名、表征和记忆。如果我在我的数据库中看到接近某个特定类别的东西，我<em class="la">会认出</em>它。如果它和我数据库中的任何东西都不一样，我就把它作为新数据添加进去。多年这种持续的自下而上的学习自然赋予了我们<strong class="kg hi"> <em class="la">在</em>感知</strong>方面的灵活性，比如:</p><ul class=""><li id="4ed4" class="lb lc hh kg b kh ki kk kl kn ld kr le kv lf kz lg lh li lj bi translated">识别环境中不存在物体</li><li id="6240" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">同时识别多个物体</li><li id="c4a3" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">跨不同媒介识别物体的形状(例如，真实生活与草图)</li></ul><p id="24f5" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">这些也使我们变得强壮，能够根据视觉刺激的环境采取不同的视角。</p><h1 id="213f" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">神经网络的不灵活性🤖</h1><p id="c62c" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">为视觉训练的神经网络怎么样？如今，很容易下载一个预先训练好的CNN网络ImageNet，并对其进行微调，以对不同类别的图像进行分类。但是它有上面列出的感知灵活性吗？我们能指望它自然地推断出给定数据之外的东西吗？给定<em class="la">格式</em>以外的数据呢？答案是否定的。他们的“感知”能力被严格限制在以下范围内:</p><ul class=""><li id="e744" class="lb lc hh kg b kh ki kk kl kn ld kr le kv lf kz lg lh li lj bi translated">给定输入图像(x)</li><li id="584f" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">目标标签的给定格式(y)</li><li id="1425" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">给定任务(损失函数，类别数)</li></ul><p id="2f31" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">让我重复一下<a class="ae lu" rel="noopener" href="/codex/data-data-data-32376be378b0">我的帖子中讨论数据在机器学习中的核心作用的主要观点</a>。当我训练一个机器学习模型的时候，模型只对把我给的数据(x)映射到一个特定形式的标签(y)感兴趣，这个标签也是我给的。作为一名机器学习工程师，我负责为模型设置环境和道具。计算机只是计算硬件。那么我该怎么做才能让模特更灵活地感知图像呢？我可以优化我提供的数据。</p><h1 id="9d7b" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">CNN的灵活性训练—数据优化</h1><p id="868c" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">在这篇文章中，我将讨论这样的界限如何在视觉感知中限制CNN，这对人类来说可能是微不足道的，这可能导致模型的不灵活和通常不直观的行为。我还将通过用不同的方式注释或优化数据来呈现我发现的对每个不灵活性的补救措施。下面是我将要讨论的三个不灵活性:</p><ol class=""><li id="4f8c" class="lb lc hh kg b kh ki kk kl kn ld kr le kv lf kz lv lh li lj bi translated">无法识别物体的缺失→ <strong class="kg hi">包括“无中生有”类</strong></li><li id="5934" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lv lh li lj bi translated">无法同时识别多个对象→ <strong class="kg hi">为每张图像使用多个标签</strong></li><li id="7e13" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lv lh li lj bi translated">无法在不同的图像纹理中识别相同的对象形状→ <strong class="kg hi">使用风格化的ImageNet鼓励形状偏好而不是纹理偏好</strong></li></ol><p id="f5bf" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">我还将分享我在多伦多大学硕士研究的自动威胁检测项目的每种数据优化方法的动机和应用。我将继续提到的图像识别模型是<strong class="kg hi">，仅用普通相机(网络)图像进行训练，然后用x射线图像进行测试</strong>(由于只有少量的x射线图像可用)。详细背景请见本<a class="ae lu" rel="noopener" href="/@lucrece.shin/ml-masters-research-project-beginnings-43894d13b3cb">项目介绍帖</a>和我的项目帖列表<a class="ae lu" rel="noopener" href="/@lucrece.shin/list/machine-learning-research-portfolio-0437a30c89fa">。</a></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lw"><img src="../Images/761dd12ab8fe695748ac75a1e8990247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IyVDGhYVffWgYNGF.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">My model is trained only with normal camera (web) images then tested with Xray images, due to only a small amount of Xray images available.</figcaption></figure><h1 id="8f56" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">不灵活#1:不能识别对象的缺失</h1><h2 id="b85b" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.1)你为什么把篮球归类为刀？</h2><p id="c4e9" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">我有一个ResNet50型号:</p><ul class=""><li id="2236" class="lb lc hh kg b kh ki kk kl kn ld kr le kv lf kz lg lh li lj bi translated">接受过<strong class="kg hi">枪对刀二元分类</strong>的培训</li><li id="36f7" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">仅使用枪(0类)和刀(1类)的图像进行训练</li><li id="f0c0" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">显示枪和刀的测试图像有99%的准确性</li></ul><p id="f44d" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">看看以下同型号的预测:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ml"><img src="../Images/d0bd403becb902b51bed02f27c46a7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nusRtl2GsYLjQKdPnkZpCw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">A gun vs. knife binary classification model confidently classifies most images containing neither classes as knife.</figcaption></figure><p id="5fda" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">我在<a class="ae lu" rel="noopener" href="/@lucrece.shin/chapter-3-2-transfer-learning-with-resnet50-from-performance-analysis-to-unexpected-riddle-abe2da3b4e8f">我的关于ResNet50 </a>迁移学习的帖子中提到过这个问题，其中模型<strong class="kg hi">自信地将大多数既不包含枪也不包含刀的图像归类为刀</strong>。与我的预期相反，该模型在对完全不相关的图像进行分类时，会对枪和刀类输出接近50/50(非决定性)的概率。为什么？</p><h2 id="7681" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.2)黑白决策边界</h2><p id="4098" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">正如在<a class="ae lu" rel="noopener" href="/@lucrece.shin/chapter-4-using-t-sne-plots-as-human-ai-translator-c5ef9c2f2fa4">我的关于t-SNE图的帖子</a>中所讨论的，我有一种直觉，二元分类模型学习“黑白”决策边界。它<strong class="kg hi">仅检查测试图像位于边界</strong>的左侧还是右侧。因此，它只能给我们左(0类)或右(1类)选项，无法识别任一对象的<strong class="kg hi">不存在</strong>，即使任一对象形状在图像中不存在。例如，当看到一个篮球的图像时，模特完全不能思考:</p><p id="3a45" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">“哦，这看起来不像我以前见过的任何东西，所以我不能把它归为这两类中的一类。让我将最后一个完全连接的图层的输出要素数量从2调整为3。这样我就可以把它归为第三个‘非枪非刀’类了”。</p><h2 id="3e87" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.3)看问题的背景</h2><p id="37b8" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">如果我们计划在输入图像严格属于两个类别之一的情况下使用该模型，这不会是太大的问题。但是，如果模型可以接收任何类型的图像，并且预计该图像不包含任何类，那该怎么办呢？</p><h2 id="5851" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.4)x光扫描仪，可将每件行李归类为有害物品</h2><p id="a64d" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">一般来说，在机场安检处扫描的大多数行李都不包含枪或刀。当我用300个良性(不包含枪或刀)x射线扫描图像测试相同的枪与刀二元分类模型时；然而，93.7%被归类为具有高置信度的刀，如下所示。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mm"><img src="../Images/53a30d940b2c47e7a28458ea42506490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OV_h9VrOq1ete93q.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Examples of benign Xray images</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mn"><img src="../Images/8d71e054a6f558560dd814628ee388a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ltpj22TavXmLaHe2e1vg7w.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Confusion matrix (left) “knife” class prediction confidence (right) for benign Xray images</figcaption></figure><p id="86af" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">在几乎每个包中都有刀的情况下，如此高的误报率是不现实的，也是非常低效的。</p><h1 id="ea71" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">补救措施#1:引入“无上述”类</h1><p id="db3b" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">为了解决这种不灵活性，我引入了第三个“良性的”(既非枪也非刀)类<strong class="kg hi">，表示世界上所有非枪或刀的对象</strong>。为了收集这堂课的训练图像，虽然除了枪和刀之外还有无限的东西，但我还是尝试使用可能在机场行李中找到的物体。我从谷歌上抓取图片使用的最终搜索关键词包括:<em class="la">书、汽车、电线、水瓶、胶带、纱线、音箱、盒子、</em>和<em class="la">太阳镜</em>。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mo"><img src="../Images/1f899370a9f210eab4a8b899c4bf794e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldo9fA85h83jkkXwwOW1EQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Examples of collected images for benign class</figcaption></figure><h2 id="7d70" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.5)x射线图像的性能:高失误率</h2><p id="d455" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">下面是引入良性类前后x射线图像上模型性能的比较。回想一下，模型在训练期间不使用x光图像(<a class="ae lu" rel="noopener" href="/mlearning-ai/chapter-2-source-domain-data-collection-d00cb426d559">细节</a>)。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mp"><img src="../Images/2733f96bc10ec787137d167da69b3d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2jNVRQAU_nU19Gq0oT24g.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Confusion matrices for 2-class and 3-class classification for Xray images</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mq"><img src="../Images/642f77d3c8a982dd4f38f98d1551a1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24HUcdQj0DEI6eMJIrX_3w.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Gun/Knife Recall Table for Xray images (V1)</figcaption></figure><p id="cbe0" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">结果与仅检测到25%和18%的枪支和刀具相差甚远，从高虚警(2级)转向高漏检(3级)。在威胁检测中，错过是非常不受欢迎的，因为我们不想错过包里带着枪或刀的人！</p><h2 id="0b08" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(1.6)回望巴塔</h2><p id="b370" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">试图找到高小姐的原因，我看着输入的图像:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mr"><img src="../Images/291aa95a7127a7004ebb3079de828a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8I3uLfrKN-h6kaHQCMyT0g.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Xray images and web images containing knife and gun</figcaption></figure><p id="66bf" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">如上所示，除了枪或刀之外，大多数行李的x射线扫描图像包含许多其他物体，而大多数网页图像显示的是一个明显的、孤立的物体。因此，当模型将x射线图像分类为良性时，它可能正在查看其他对象，而不是枪或刀。无法考虑图像中存在一个以上类别的可能性，例如良性物体<em class="la">和</em>枪。这成为图像识别模型的另一个致命的不灵活性。</p><h1 id="9ef7" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">不灵活#2:不能同时识别多个对象</h1><h2 id="95e6" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(2.1)你没看见刀吗？</h2><p id="2aa7" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">这种不灵活性适用于网页图像以及x光图像。下面展示了同一个模型是如何在一个明显包含枪和刀的图像中识别枪的。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ms"><img src="../Images/afea2f4ca925e448190f14051d71b001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BZ6w69mOVr8JtfIlbXJFw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Gun vs. Knife binary classification model not recognizing knife in the image</figcaption></figure><p id="d378" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">该模型还不如被训练成<em class="la">枪对非枪</em>二元分类，而不是<em class="la">枪对刀</em>。它对刀的不敏感也是相当惊人的。</p><p id="e3a7" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">一般来说，用于训练图像识别模型的最被接受的数据注释是为每个图像分配一个从0到N-1的<strong class="kg hi">单一数字目标标签</strong>，其中N是类别的数量。然而，这限制了模型将图像分类为仅一个可用类别。我们如何以不同的方式标注数据，以便模型可以判断图像中是否存在多类对象？</p><h1 id="ec5e" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">补救措施#2:用多个类别标签注释每个图像</h1><p id="a1da" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">我想到的解决方案是:不是让模型将图像分类为N个类别(单标签)中的一个，而是让它预测包含N个类别(多标签)中的每一个的图像的<strong class="kg hi">N</strong>T8】概率。此表总结了使用单目标标签和多目标标签标注图像的差异:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mt"><img src="../Images/c81b226f2493c7144a62043dca1a2ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEeTwqPmWBWgi7Oyt-d4ow.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Using single labels vs. multi labels for data annotation</figcaption></figure><h2 id="26be" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated"><strong class="ak">目标标签的数量</strong></h2><p id="7875" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">使用单标签时，图像的目标标签是一个数字(0、1、2等)。)，代表N个类中的一个。对于多标签，目标标签是N个二进制数(0和1)的列表，其中1表示该类存在于图像中，0表示它不存在。</p><h2 id="e1ba" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated"><strong class="ak">软标签</strong></h2><p id="8c04" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">对于我的威胁检测问题，我发现对良性类别使用<em class="la">软标签</em>是有益的，因为与枪和刀类别相比，该模型倾向于将图像分类为良性，具有更高的可信度。由于检测良性物体没有检测枪或刀重要，我将良性信号变弱，用0.5替换良性类别标签，同时保持其他为1。</p><h2 id="e6bd" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated"><strong class="ak">损失函数</strong></h2><p id="13a8" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">使用单标签，该模型将输入图像分类到单个目标类别中。因此，我们对来自最后一个全连接层的逻辑进行softmax激活，这在训练期间被用于计算交叉熵损失。使用多标签，该模型预测图像包含每个类对象的概率。因此，softmax激活被替换为sigmoid，为每个目标类输出0到1之间的概率。对于多维目标，交叉熵损失转化为二值交叉熵损失。由于我对良性类使用了0.5的软标签，所以我使用了MSE损失而不是BCE损失。</p><h2 id="f4d3" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(2.2)所有输入图像的重新标记</h2><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mu"><img src="../Images/8303cef1bf29636a598ebd4e25477276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Ar4v21AkDkLgpqH7CAVmg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Three different types of images and respective target labels</figcaption></figure><p id="246e" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">我重新注释了所有输入图像的目标标签，以便区分包含单个孤立对象的图像和包含背景中其他良性对象的图像。例如，上面的图像(a)被赋予了一个标签<strong class="kg hi"> [0，0，1] </strong>，它代表<strong class="kg hi"> [P(良性)，P(枪)，P(刀)] </strong>，只有P(刀)=1，因为该图像只包含一把刀，没有任何其他对象。图像(b)带有刀和其他良性物体，如蘑菇🍄和肉🥩被赋予<strong class="kg hi">【0.5，0，1】</strong>的标签，P(良性)为0.5作为软标签，P(刀)为1。同样的事情也适用于图像(c)的枪和刀类。</p><h2 id="8ea1" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(2.3)改进了x射线图像中枪和刀的召回</h2><p id="b6d3" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">下面是用单标签和多标签数据训练的模型在x射线图像上的性能比较。你可以看到这两个级别的召回增加了近两倍。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mv"><img src="../Images/0d812dfea1f8f069e7383940edb37039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LscCa0I2ALbsLiMPLJ0_RA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Confusion matrices for Xray images for models trained with single-label and multi-label data</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mw"><img src="../Images/2e68705510350be475b5e3ccb64c4402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utnLBkNlZeZUh9FKOxaLrA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Gun/Knife Recall Table for Xray images <strong class="bd iy">(V2)</strong></figcaption></figure><p id="e3b3" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">对于用多标签数据训练的模型，我使用0.3的预测阈值，这意味着如果某个类的预测logit大于0.3，我就认为该图像属于该类。这样，如果一个以上的类的logits超过0.3，我也可以预测测试图像的多个标签。</p><p id="c67d" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">右侧混淆矩阵显示，用多标签数据训练的模型将大多数枪和刀图像分类为良性，这是好的，因为在大多数x射线图像中确实存在许多其他良性对象。</p><h1 id="0da5" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">不灵活#3:不能跨不同的图像纹理识别相同的对象形状</h1><h2 id="0475" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(3.1)纹理偏移</h2><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lw"><img src="../Images/761dd12ab8fe695748ac75a1e8990247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IyVDGhYVffWgYNGF.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx"><strong class="bd iy">Texture shift</strong> from web images to Xray images</figcaption></figure><p id="19ce" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">由于从web图像到x射线图像存在明显的纹理偏移<em class="la"> </em>，所以最好确保模型<strong class="kg hi">正确地学习每个威胁对象</strong>的<em class="la">形状</em>，使得<strong class="kg hi">纹理偏移不会影响其性能</strong>。</p><h2 id="89bc" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(3.2)纹理假设/纹理偏差</h2><p id="e57f" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">请看下图，该图显示了4种不同的CNN架构和人类将前四幅图像分类为“猫”而将最后一幅图像分类为“大象”的准确度:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mx"><img src="../Images/122749ad1e4c9629dcfe9194513272dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oQJpDaDsUBocwkig"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Accuracies of CNN architectures and humans for classifying the first four images as “cat” and the last image as “elephant” (Source: <a class="ae lu" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.12231</a>)</figcaption></figure><p id="67fb" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">这个结果发表在一篇名为<a class="ae lu" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank"> <em class="la">的论文中，ImageNet训练的CNN偏向于纹理；增加形状偏差提高准确性和鲁棒性</em> </a>由Geirhos等人于2019年发表。尽管风格/纹理发生了变化，但大多数人可以很容易地将前四幅图像识别为一只猫。相比之下，当纹理更改为轮廓和边缘时，AlexNet、LeNet、VGG16和ResNet50的精度会急剧下降。这说明了<strong class="kg hi"> <em class="la">纹理假说</em> </strong>:(引自同一篇论文)</p><blockquote class="my"><p id="ee9a" class="mz na hh bd nb nc nd ne nf ng nh kz dx translated">对于CNN目标识别，目标纹理比全局目标形状更重要。 <strong class="ak"> <em class="ni">纹理等局部信息实际上可能足以“解决”ImageNet对象识别。</em> </strong></p></blockquote><p id="f82e" class="pw-post-body-paragraph ke kf hh kg b kh nj ii kj kk nk il km kn nl kp kq kr nm kt ku kv nn kx ky kz ha bi translated">这个想法反映在所有4个CNN架构将最后一个象皮图像分类为大象的100%准确度中。如果是的话，<strong class="kg hi">如何让模型对物体的<em class="la">形状</em>而不是纹理更敏感？</strong></p><h1 id="8e4b" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">补救措施#3:使用<strong class="ak">风格化的ImageNet数据集</strong>增加模型中的形状偏差</h1><p id="c341" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">作为对纹理偏差的补救，该论文建议用来自ImageNet数据集的<strong class="kg hi"> <em class="la">风格化的</em> </strong>图像来训练模型。对图像进行“风格化”意味着:</p><ul class=""><li id="5c61" class="lb lc hh kg b kh ki kk kl kn ld kr le kv lf kz lg lh li lj bi translated">保持图像中的内容/形状</li><li id="d286" class="lb lc hh kg b kh lk kk ll kn lm kr ln kv lo kz lg lh li lj bi translated">使用<a class="ae lu" href="https://arxiv.org/abs/1703.06868" rel="noopener ugc nofollow" target="_blank"> AdaIN风格转移</a>用数字数据集(包含79434幅画)替换从<a class="ae lu" href="https://www.kaggle.com/c/painter-by-numbers" rel="noopener ugc nofollow" target="_blank"> Painter中随机选择的画的图像中的风格/纹理</a></li></ul><p id="f278" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">这是一个用十种不同的绘画风格化的狐猴图像的例子。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es no"><img src="../Images/258e6adafb09f36508f3f0d883e07359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f-374L2WUFBuCI74"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx"><strong class="bd iy">10 Stylized samples of an image of class “<em class="ni">ring-tailed lemur”</em>.</strong> The samples have content/shape of the original image on the left and style/texture from different paintings (Source: <a class="ae lu" href="https://arxiv.org/abs/1811.12231" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.12231</a>)</figcaption></figure><p id="4cef" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">风格化的图像保留了狐猴的外形轮廓，同时具有多样化的纹理。这使得<em class="la">局部</em>纹理线索不再高度预测目标类，迫使模型更加关注对象的全局形状。在中，将风格化数据集命名为<strong class="kg hi">风格化ImageNet (SIN) </strong>，将原始ImageNet图像命名为<strong class="kg hi">。用SIN和IN中的一种或两种进行训练的实验结果如下表所示:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es np"><img src="../Images/2ecc57f8d993b803ad34066f4360b334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*218Dfwur6OEtrOBW9fneDg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Accuracy comparison on the ImageNet (IN) validation dataset &amp; object detection performance (mAP50) on PASCAL VOC 2007. All models have an identical ResNet-50 architecture.</figcaption></figure><p id="ded6" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">对于我的威胁检测问题，我使用了在SIN和IN上预先训练的性能最好的模型(最后一行),然后在IN上进行微调。论文作者的<a class="ae lu" href="https://github.com/rgeirhos/texture-vs-shape" rel="noopener ugc nofollow" target="_blank"> github资源库</a>中提供了下载模型检查点的代码说明。我再次使用多标签对良性、枪和刀类的网络图像的模型进行了微调。我也尝试将我自己的枪和刀的图像风格化；然而，验证结果不如使用原始图像好。</p><h2 id="cd8a" class="lx ix hh bd iy ly lz ma jc mb mc md jg kn me mf ji kr mg mh jk kv mi mj jm mk bi translated">(3.3)改进了x射线图像的模型性能</h2><p id="2171" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">以下是使用原始和风格化ImageNet训练的模型在x射线图像上的性能比较。您可以看到，这两种产品的召回率都增加到了71%和73%。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mv"><img src="../Images/1afd610d0db696b6790fb874b1a72b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCFEbctSKNqYm-T_IVoMrQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Confusion matrices for Xray images for models trained using original ImageNet (left) and Stylized ImageNet (right)</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mw"><img src="../Images/56d037e65ceb8bdfbc67e047ce0c325c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykzvxJKEtSNXE4MbDiyjMw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Gun/Knife Recall Table for Xray images <strong class="bd iy">(V3)</strong></figcaption></figure><p id="e39e" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">虽然召回一直持续到70年代，但这对于威胁检测系统来说是不可靠的。我将在以后的文章中讨论我是如何使用领域适配来提高门槛的。</p><h1 id="9eb8" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">关闭</h1><p id="5b67" class="pw-post-body-paragraph ke kf hh kg b kh lp ii kj kk lq il km kn lr kp kq kr ls kt ku kv lt kx ky kz ha bi translated">我们研究了三种数据优化方法，这些方法鼓励基于CNN的图像识别模型做出更灵活的决定，以适应给定问题的背景。一些读者可能想知道<strong class="kg hi">为什么不使用一个可以执行更复杂任务的对象识别或语义分割模型。</strong>可以，但它们需要更大、更复杂的模型架构和昂贵的数据注释(考虑语义分割的像素级标记)。</p><p id="b77a" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">优化数据就像通过<strong class="kg hi">试验不同的光线和角度来学习如何拍摄有创意的照片，而不是购买更昂贵的相机。</strong>我<em class="la">可以</em>用昂贵的相机为我设置/优化每一个设置来拍出好照片，但我永远不会掌握切换不同变量来在照片中创造我想要的感觉的技能。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es nq"><img src="../Images/4fc27832154b3cf069cc35731abba1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tceTW5Lvv_WTkXK0zK7bEQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Me captured by the light magician <a class="ae lu" href="https://www.instagram.com/andrecwidjaja/" rel="noopener ugc nofollow" target="_blank">Andre</a> 🌙</figcaption></figure><p id="e968" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">您可以<a class="ae lu" href="mailto:lucrece.shin@mail.utoronto.ca" rel="noopener ugc nofollow" target="_blank">联系我</a>了解任何关于我的方法的问题或反馈。我很想知道其他ML研究人员的想法。感谢阅读！🌸</p><p id="df19" class="pw-post-body-paragraph ke kf hh kg b kh ki ii kj kk kl il km kn ko kp kq kr ks kt ku kv kw kx ky kz ha bi translated">-☾₊˚.</p></div></div>    
</body>
</html>