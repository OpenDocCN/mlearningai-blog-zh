<html>
<head>
<title>BERT: Key Takeaways</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特:关键要点</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/bert-key-takeaways-172d9af46402?source=collection_archive---------1-----------------------#2022-03-13">https://medium.com/mlearning-ai/bert-key-takeaways-172d9af46402?source=collection_archive---------1-----------------------#2022-03-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="091b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基于<a class="ae jc" href="https://aclanthology.org/N19-1423.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>Delvin等人，2019</p><h1 id="816d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">概观</h1><p id="197e" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated"><strong class="ig hi">问题:</strong>我们如何改进当前的NLP模型？</p><p id="c20a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">他们做了什么来回答这个问题:</strong>作者使用预训练和微调框架开发了BERT。该网络使用“掩蔽语言模型”(MLM)和“下一句预测”任务，从“通过联合调节所有层中的左右上下文的未标记文本”预训练深度双向表示。作者使用预先训练的参数初始化BERT模型，并对其进行微调，以便它可以使用每个任务的标记数据为特定的模型任务工作(微调仅使用一个附加的输出层)。</p><p id="169a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">动机/原理:</strong>以前使用微调的模型是单向的。如果我们使用双向模型，我们将能够在任务的微调过程中使用最少的特定于任务的参数，因为令牌的上下文很重要。</p><p id="2180" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">发现:</strong> BERT推进了11个NLP任务的技术水平。</p><p id="33b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">解释:</strong>该模型表明，语言表示的双向预训练非常重要，预训练的表示减少了对许多高度工程化的特定任务架构的需求。</p><h1 id="3b1f" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">关键概念</h1><h2 id="5531" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">预培训和微调</h2><p id="c778" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">为文本开发一个通用的神经网络编码器，当给定足够的训练数据时，它可以解决任何新的语言理解任务(以便它可以定义可能的输出)。</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="kz la l"/></div></figure><h2 id="6c9c" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">语言建模</h2><p id="25dc" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">给定一定量的上下文，预测文本中的下一个单词。这本质上是文本分类，但有成千上万个类别。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lb"><img src="../Images/f626859b2a1e428ed5108b92deca5001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*vDYDfl5AX--xvvg-.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">From <a class="ae jc" rel="noopener" href="/@florijan.stamenkovic_99541/rnn-language-modelling-with-pytorch-packed-batching-and-tied-weights-9d8952db35a9">https://medium.com/@florijan.stamenkovic_99541/rnn-language-modelling-with-pytorch-packed-batching-and-tied-weights-9d8952db35a9</a></figcaption></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/22d3ccc4887bf98287dc83bcee261c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wz-FUYePNsOSAllZ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">From <a class="ae jc" rel="noopener" href="/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27">https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27</a></figcaption></figure><h2 id="855f" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">伯特模型</h2><ul class=""><li id="ef3a" class="ln lo hh ig b ih kb il kc ip lp it lq ix lr jb ls lt lu lv bi translated">层数(即变压器块):L</li><li id="df70" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">隐藏尺寸:H，</li><li id="7c57" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">num自我关注头数:A</li></ul><p id="2a36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BERTBASE (L=12，H=768，A=12，总参数=110M)</p><p id="b367" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">伯特拉奇(L=24，H=1024，A=16，总参数= 340米)</p><p id="c0da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了便于比较，选择了与GPT开放航空公司相同的模型尺寸。</p><h2 id="c1b7" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">掩蔽LM和掩蔽预训练程序</h2><p id="80a0" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">文献中的完形填空(Taylor，1953)。</p><ul class=""><li id="f084" class="ln lo hh ig b ih ii il im ip mb it mc ix md jb ls lt lu lv bi translated">80%的情况下:将单词替换为[MASK]标记，例如，我的狗是多毛的→我的狗是[MASK]</li><li id="02d1" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">10%的情况下:用一个随机的词替换这个词，例如，我的狗是多毛的→我的狗是苹果</li><li id="5223" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">10%的时候:保持单词不变，例如:我的狗有毛→我的狗有毛。</li></ul><p id="13d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">为什么会有这些百分比？</strong>作者想要使表示偏向实际观察到的单词。由于transformer编码器无法知道它将被要求预测哪些单词，或者哪些单词已被随机单词替换，因此它必须保留每个输入标记的分布式上下文表示。</p><p id="4a71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机替换不会伤害模型吗？</strong>因为它只发生在1.5%的代币上，所以不会发生。</p><p id="aef2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">但是，如果我们只对每批中的15%的令牌进行预测，我们是否需要更多的预训练步骤来使模型收敛？</strong>“MLM模型的收敛速度确实比LTR模型稍慢。然而，就绝对准确性而言，MLM模型几乎立即开始胜过LTR模型”(Delvin等人)</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="kz la l"/></div></figure><h2 id="44d9" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">下一句预测预训练</h2><p id="8c60" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">问题回答(QA)和自然语言推理(NLI)是基于理解两个句子之间的关系，这不是语言建模直接捕获的。</p><p id="a3a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了训练BERT理解句子关系，作者对二值化的下一句预测任务进行了预训练:</p><ul class=""><li id="389b" class="ln lo hh ig b ih ii il im ip mb it mc ix md jb ls lt lu lv bi translated">50%的时间B是跟在A后面的下一个句子(标记为IsNext)</li><li id="84b0" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">50%的情况下，它是来自语料库的随机句子(标记为NotNext)。</li></ul><h2 id="86ce" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">微调</h2><p id="76b6" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">通过交换适当的输入和输出来完成。对于每项任务，作者将特定于任务的输入和输出插入到BERT中，并端到端地微调所有参数。</p><p id="b10a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在输入端，来自预训练的句子A和句子B类似于</p><p id="c816" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(1)释义中的句子对</p><p id="3e2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(2)蕴涵中的假设前提对</p><p id="f2d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">(3)问答中的问题-段落对</p><h2 id="9d49" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">比较伯特、埃尔莫和奥佩利BERT</h2><p id="d363" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">OpenAI GPT是以前最好的模型，它在大型文本语料库上训练从左到右的转换器LM。除了双向和两个预训练任务之外，BERT的设计使be尽可能类似于GPT，以便两者可以很容易地进行比较。</p><h2 id="de92" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">模型概述</h2><p id="c872" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated"><strong class="ig hi">埃尔莫</strong>:前向和后向深度LSTM网络，你用附加的最终任务模型层冻结它</p><p id="8dbf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> OpenAI的GPT </strong>:与ELMo的想法相同，但使用变压器编码器网络而不是LSTM，并且在预训练后不冻结步骤(整个网络针对每个任务进行微调)。此外，预训练是针对更长的文本跨度，而不仅仅是孤立的句子。GPT只是从左到右。</p><p id="db40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">伯特:与GPT想法相似。掩蔽语言建模。在损失函数中包含两项的两组不同的自我监督学习任务的混合上进行预训练。这两项任务都不是真正的语言建模。这两项任务都不需要预测未来，所以我们使用编码器变压器。编码器转换器不能用于标准语言建模，并且对于任何涉及文本生成的任务都是低效的。我们必须一次预测所有的单词，但是注意力部分意味着我们不再隐藏句子的某些部分(可以作弊)。</p><p id="5808" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">以前的无监督的基于特征的方法:</strong>微调从左到右表示的学习嵌入。固定特征是从预先训练好的模型中提取出来的，具有一定的优势。埃尔莫。</p><p id="826d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">以前的无监督微调方法:</strong>仅来自未标记文本的预训练单词嵌入。通用终端</p><p id="2074" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">从监督数据中转移学习:</strong> BERT</p><h2 id="1522" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">输入/输出表示</h2><ul class=""><li id="7ac7" class="ln lo hh ig b ih kb il kc ip lp it lq ix lr jb ls lt lu lv bi translated">能够在一个标记序列中明确地表示一个单句和一对句子(例如<question answer="">)。</question></li><li id="b16a" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">作者使用术语“句子”来指代任何连续文本的跨度(不一定是我们通常认为的句子。</li><li id="4d16" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">作者使用具有30，000个标记词汇的单词块嵌入。</li><li id="209d" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">每个序列的第一个标记总是分类标记[CLS]。</li><li id="2384" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">“句子”用标记[SEP]分隔。</li></ul><h1 id="5032" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">临时演员</h1><h2 id="e761" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">下游任务概述</h2><ul class=""><li id="f57d" class="ln lo hh ig b ih kb il kc ip lp it lq ix lr jb ls lt lu lv bi translated">小队:问答</li><li id="6b1d" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">SNLI:句子蕴涵(一个句子对另一个句子是否正确)</li><li id="c003" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">SRL:同一个句子不同部分之间的关系</li><li id="4e4b" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">Coref:找出像(她，他的，等等)这样模棱两可的词。)参考</li><li id="ab31" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">NER:识别专有名词</li><li id="00a2" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">SST-5:情绪分析</li><li id="86c6" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">可乐:决定一个句子是否合乎语法</li><li id="d923" class="ln lo hh ig b ih lw il lx ip ly it lz ix ma jb ls lt lu lv bi translated">MNLI:句子蕴涵(蕴涵，矛盾，中性)</li></ul><h2 id="3458" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated"><strong class="ak">数据扩充</strong></h2><p id="b119" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">通过添加已有数据的略微修改的副本或从现有数据新创建的合成数据来增加数据量。</p><h2 id="5b38" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">精彩的回顾</h2><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="kz la l"/></div></figure><div class="me mf ez fb mg mh"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hi fi z dy mm ea eb mn ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">medium.com</p></div></div><div class="mq l"><div class="mr l ms mt mu mq mv lc mh"/></div></div></a></div></div></div>    
</body>
</html>