<html>
<head>
<title>Playing MOBA game using Deep Reinforcement Learning — part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度强化学习玩MOBA游戏——第三部分</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/playing-moba-game-using-deep-reinforcement-learning-part-3-f5b19b2f984f?source=collection_archive---------4-----------------------#2021-12-18">https://medium.com/mlearning-ai/playing-moba-game-using-deep-reinforcement-learning-part-3-f5b19b2f984f?source=collection_archive---------4-----------------------#2021-12-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8fb8ebd35ab3d92d541994e155684e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJK3gouIRxu039q3JJOuHw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image from <a class="ae it" href="https://wallpapercave.com/dota-2-wallpapers" rel="noopener ugc nofollow" target="_blank">wallpapercave</a></figcaption></figure><p id="9b5f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在前两篇文章中，我们准备了使用深度强化学习来训练大规模MOBA游戏所需的基本组件。在本帖中，我们将看到如何结合Dota2和Seed RL的代码。</p><p id="63f0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你可以在https://github.com/kimbring2/MOBA_RL/tree/main/dota2找到这篇文章的代码。</p><h1 id="482f" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">基于学习的Dota2代码</h1><p id="99df" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">Derk和Dota2网络的主要区别在于动作网络的数量。在前一种情况下，只使用一个网络，因为游戏简单。然而，后者需要使用六个网络进行行动选择。因此，在Seed RL代码中创建代理网络时，需要进行以下更改，以反映不同的。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Dota2 Agent Network</figcaption></figure><p id="305a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">像Derk代理的情况一样，Dota2代理将来自参与者的观察发送给学习者以选择动作。区别在于动作由多个参数组成。因此，我们必须将它们组合起来，以生成最终的操作，并将其发送到环境步骤。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Dota2 Action Selection from RL</figcaption></figure><p id="05c1" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">演员兼评论家DRL在训练时将从所选动作的对数概率值计算的损失用于策略分布。因此，您还需要计算添加动作的损失。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Dota2 Loss Calculation</figcaption></figure><h1 id="3791" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">Dota2的基于规则的代码</h1><p id="568b" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">在Dota2中，与Derk不同，agent需要在游戏过程中获取物品和能力。我把物品和能力的名称保存在列表中，当英雄的黄金和等级满足一定条件时按顺序使用，因为这部分用DRL实现有点困难。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lb"><img src="../Images/4fc6efeedae6c81562cf00ce4698e61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7r4S3zaiAOYF7O8sEa4FAw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Item buying route of each hero</figcaption></figure><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/3517c024095535aad4eafa3032e1e522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yg5_tI7slcyNNSW_xUO1Jg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Ability learning route of each hero</figcaption></figure><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Dota2 Action Selection from Rule</figcaption></figure><h1 id="0d95" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">奖励设置</h1><p id="bb8d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">DRL的一个重要的事情是奖励设置。根据我们的奖励，代理将学习一种完全不同的行为。不像Derk那样从环境内部自动计算奖励，我们需要从原始观察中计算奖励。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ld"><img src="../Images/146e45f213c16aea1c4ca025ea3bb2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2E4RSWdahsuopjJbSb8mmg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Reward Table</figcaption></figure><p id="295d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">强化学习的奖励基本上是为了获得经验。另外，对敌方单位造成伤害和对己方单位造成恢复的代理对最后一击有不同的权重。最后，根据游戏结果给予巨额奖励，以设定整个游戏过程中的长期策略。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="kz la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Reward example for ShadowFiend</figcaption></figure><p id="8301" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以使用的最基本的奖励成分是经验值、生命值和最后一击，也就是英雄在最后一次攻击中杀死怪物并获得金币的次数。</p><h1 id="9331" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">Dota2的训练结果</h1><p id="361d" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">如果你用我们在2天内设置的基本奖励来训练代理，你可以看到两个代理的奖励增加到某一点，如下图所示。</p><figure class="kv kw kx ky fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es le"><img src="../Images/8cac5e67dba90fcd2db3ff1ffef20ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9m5hXu7S4WADcsBAZdQOOw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Reward Graph of Basic Reward Set</figcaption></figure><p id="4593" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">仅使用图表无法完全确认培训是否成功完成。因此，我们需要启动游戏客户端来直观地查看代理的行为，如下图所示。</p><figure class="kv kw kx ky fd ii"><div class="bz dy l di"><div class="lf la l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Evaluation Result of Agent</figcaption></figure><p id="89b3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们可以看到英雄移动到中间通道，那里的小兵只是来战斗以获得经验值，即使我们没有使用任何基于规则的代码来移动和攻击，</p><h1 id="0fa7" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="3c44" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">在这篇文章中，我们看一下我们需要修改和添加什么，以便像Derk一样使用DRL来训练Dota2。如您所见，我们需要在Dota2中使用基于规则的小方法进行项目和能力管理。对于英雄的移动和攻击，我们可以使用DRL。此外，我们检查，与Derk相比，训练时间相当长。</p><div class="lg lh ez fb li lj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hi fi z dy lo ea eb lp ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">如何成为移动人工智能的作者</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">medium.com</p></div></div><div class="ls l"><div class="lt l lu lv lw ls lx in lj"/></div></div></a></div></div></div>    
</body>
</html>