<html>
<head>
<title>Basic Operations of Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流的基本运算</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/basic-operations-of-tensorflow-bf206868f491?source=collection_archive---------3-----------------------#2022-02-09">https://medium.com/mlearning-ai/basic-operations-of-tensorflow-bf206868f491?source=collection_archive---------3-----------------------#2022-02-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="7046" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">DNN张量流基本原理</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/aad58ff8b33ad59711d5ef92a6b9c1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A5Y2hwJqwIdMwJxC"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@mourimoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mourizal Zativa</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d67b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">Tensorflow在过去几年中因深度学习模型而获得了很多关注。它为神经网络的创建提供了简单的API，可以在不同的平台上有效地扩展和托管。张量是一个数学术语，定义如下:</p><blockquote class="kj kk kl"><p id="9366" class="jn jo km jp b jq jr ii js jt ju il jv kn jx jy jz ko kb kc kd kp kf kg kh ki ha bi translated"><em class="hh">在数学中，一个</em> <strong class="jp hi"> <em class="hh">张量</em> </strong> <em class="hh">是一个代数对象，它描述了与一个向量空间相关的代数对象集合之间的多线性关系。</em></p><p id="3c88" class="jn jo km jp b jq jr ii js jt ju il jv kn jx jy jz ko kb kc kd kp kf kg kh ki ha bi translated"><em class="hh">维基百科</em></p></blockquote><p id="91bb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一般来说，张量是一个多维数组。然而，Tensorflow是一个python框架，它有一套全面的工具和库来创建机器学习项目。这里，张量与张量流的关系是指大部分输入、中间和输出数据都是张量形式。在这篇博客中，我们将学习TensorFlow的基本操作，并构建一个简单的线性回归神经网络。</p><h1 id="d82a" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">张量流常数和变量</h1><p id="cb1b" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">Tensorflow使用自己的常量类型，变量也有不同的方法来声明、定义和初始化它们。主要原因是在神经网络的多种运算中使用了Python向量化和广播，这对于标准的常量和变量是不可能的。常量和变量可以使用 TensorFlow包中的<strong class="jp hi">常量和变量函数来声明。见上面代码快照TensorFlow导入为tf并声明了常量和变量。</strong></p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="5ce9" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">零和一</h1><p id="992c" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">当在神经网络的工作中执行各种操作时，我们必须将所有元素都初始化为0或1的矩阵。Tensorflow具有0和1函数，可用于在一行代码中创建它们。参见下面的代码快照，tf.ones用于创建一个3乘2的矩阵，所有1的数据类型为int32。类似地，也可以为零点创建矩阵。假设我们希望创建一个1和0的矩阵，并且它的形状应该类似于一个已经存在的张量，我们可以使用张量包中提供的函数。在下面的代码中，A1和A23是两个常量张量，B1和B23是使用A1和A23的形状创建的。使用<strong class="jp hi"> <em class="km"> zeros_like可以执行类似的操作。</em>T15】</strong></p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="3501" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为神经网络中的前向传播创建权重和偏差张量时，1和0函数非常有用。除了1和0，我们还可以使用<strong class="jp hi"> <em class="km"> tf.fill </em> </strong>方法初始化任意形状的张量。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="7016" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">操作</h1><p id="83b4" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">使用张量流实现神经网络需要在常数和变量之间执行各种操作。在本节中，将通过代码简要讨论一些常见的操作。</p><h1 id="b128" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">增加</h1><p id="4e32" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">张量的添加是程序员可能需要执行的简单而重复的过程。TensorFlow中的加法是按元素执行的，可以使用以下代码完成。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="f1a2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">a3将两个张量的每个元素相加，并存储在合成张量中。请参见下面打印命令的结果。对于a3张量的每个元素，七加三的结果是十。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lp"><img src="../Images/5db12fe184c2ceac15a2f9f39056a3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/0*4ci3IoV18OZxZ_60"/></div></figure><h1 id="303e" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">乘法和矩阵乘法</h1><p id="6ac2" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">张量的乘法有两种方法，第一种是元素乘法，就像加法一样，第二种是矩阵乘法。两者都可以使用Tensorflow轻松实现，参见下面的代码。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="940d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在进行乘法运算时，需要记住张量的形状。在逐元素乘法中，两个张量的形状应该完全相同。但是，对于矩阵乘法，第一个张量的列数应该与第二个张量的行数相同。</p><h1 id="40de" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">减少功能</h1><p id="154f" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">在神经网络的实现中，我们可能需要将一个张量简化成一个单一的值。因此，这个单一值将反映神经网络的下一层或输出的完整张量。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="f0da" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在上面的代码中，我举了一个reduce_sum的例子，但是，有许多函数与reduce方法相关，如下表所示。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><h1 id="a321" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">梯度</h1><p id="b6fa" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">当更新神经网络中张量的权重时，我们需要找到损失或使用的其他函数的最小值、最大值或最佳值。梯度可以帮助我们识别，因为梯度等于零是最佳值，梯度变化大于零，我们可以进一步最小化该值，最后，梯度小于零，我们可以最大化该值。用简单的语言来说，梯度提供了一个变量的变化率取决于另一个变量的变化。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="dc33" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在上面的代码中，我们使用了上下文管理器和渐变磁带函数来观察变量的渐变。随着<strong class="jp hi"> <em class="km"> x </em> </strong>的任何变化，它监控<strong class="jp hi"> <em class="km"> y </em> </strong>的变化。更多详情请访问<a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank"> GradientTape </a>页面。</p><h1 id="8b52" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">使再成形</h1><p id="a5d0" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">神经网络广泛用于图像处理，然而，图像以矩阵格式以范围在0到255之间的数值的形式存储。但是，神经网络只接受一维输入，因此我们需要将输入图像整形为一维张量，以便它可以被馈送到神经网络。整形功能对此很有用。在下面的代码中，我们使用随机数创建了一个简单的灰度图像，然后将这个图像转换成一个线性张量。代码下方的图像显示了2*2图像的图形表示。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lq"><img src="../Images/267f3b749fd200b44c1bb69c2088e401.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/0*Ez0Y_t1pqQK_T4RI"/></div></figure><h1 id="505e" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">随机数生成</h1><p id="6309" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">初始化权重或偏差张量的最佳方法是使用随机过程分布。TensorFlow提供了各种随机数生成器，可直接用于使用随机分布的值生成张量。在reshape函数的代码中，我们使用均匀随机分布创建了一个图像，其中最大值为255。同样，我们可以使用<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/categorical" rel="noopener ugc nofollow" target="_blank">categorical</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/fixed_unigram_candidate_sampler" rel="noopener ugc nofollow" target="_blank">fixed_unigram_candidate_sampler</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/gamma" rel="noopener ugc nofollow" target="_blank">gamma</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/poisson" rel="noopener ugc nofollow" target="_blank">poisson</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/normal" rel="noopener ugc nofollow" target="_blank">normal</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random/stateless_binomial" rel="noopener ugc nofollow" target="_blank">stateless_binomial</a></code>等来生成随机数。请访问<a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/random" rel="noopener ugc nofollow" target="_blank">该</a>页面了解所有随机数发生器功能。</p><h1 id="1177" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">神经网络的损耗</h1><p id="5969" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">这种损失实际上有助于理解神经网络对于任何预测任务的准确性。它还显示了训练过程训练重量的路径。所有权重更新都基于从目标输出的预测输出生成的损失值。损失基本上分为分类和回归两类问题。分类问题多使用<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy" rel="noopener ugc nofollow" target="_blank">BinaryCrossentropy</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">CategoricalCrossentropy</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity" rel="noopener ugc nofollow" target="_blank">CosineSimilarity</a></code>。另一方面，回归问题使用<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError" rel="noopener ugc nofollow" target="_blank">MeanAbsoluteError</a></code>、<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError" rel="noopener ugc nofollow" target="_blank">MeanSquaredError</a></code>和<code class="du lr ls lt lu b"><a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber" rel="noopener ugc nofollow" target="_blank">Huber</a></code>。要了解更多详情，您可以访问TensorFlow的文档页面，了解损失<a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="25bf" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">优化者</h1><p id="d628" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">顾名思义，这些方法优化了神经网络的权重和偏置张量，以最小化我们在前面的块中学习到的损失。更详细的比较和优化器的工作可以从<a class="ae jm" href="https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5" rel="noopener" target="_blank">这篇</a>关于媒体的文章中读到。基本的优化器是梯度下降，它使用下面的等式基于损失值的梯度更新权重和偏差。</p><p id="9314" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">W = W—αδW</p><p id="c656" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然而，最常用的优化器是ADAM 优化器。Tensorflow文档在<a class="ae jm" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers" rel="noopener ugc nofollow" target="_blank">该</a>页面上提供了关于每个优化器的详细信息。</p><h1 id="5a0c" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">使用张量流的线性回归</h1><p id="0404" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">现在让我们使用上述知识，创建一个简单的模型来训练线性回归的截距和斜率变量，见下面的代码。auto-mpg数据集从Kaggle下载，从数据集中提取两列作为NumPy数组(行:2，3)。线性回归的截距和斜率被创建为张量流变量，初始值均为0.2(线:6，7)。定义了一个linear_regression函数，它根据线性回归方程返回值(第10，11行)。还定义了一个损耗函数，它使用mse计算损耗并返回它(第14 -18行)。使用Adam优化器，它试图通过在1000次迭代中操纵截距和斜率的值来最小化损失。截距和斜率的损失和最终值被打印出来，见代码下面的输出图像。我们可以看到损失在逐渐减少。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="ln lo l"/></div></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lv"><img src="../Images/d3ee7668e942bedc233b69937bb974e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/0*A_Aj_M45XEzEI2KF"/></div></figure><h1 id="28ee" class="kq kr hh bd ks kt ku kv kw kx ky kz la in lb io lc iq ld ir le it lf iu lg lh bi translated">结论</h1><p id="d6c7" class="pw-post-body-paragraph jn jo hh jp b jq li ii js jt lj il jv jw lk jy jz ka ll kc kd ke lm kg kh ki ha bi translated">在这篇博客中，我们讨论了构建神经网络模型所需的张量流的一些基础知识。最后，我们创建了一个简单的线性回归模型来寻找截距和斜率的最佳值。这篇博客的目的是讨论在使用高级API开发复杂应用的神经网络之前，我们需要学习的一些基础知识。</p></div><div class="ab cl lw lx go ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ha hb hc hd he"><p id="5c6e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><em class="km">原载于2022年2月9日</em><a class="ae jm" href="https://rajindersandhu.com/2022/02/09/basics-of-tensorflow/" rel="noopener ugc nofollow" target="_blank"><em class="km">【http://rajindersandhu.com】</em></a><em class="km">。</em></p><div class="md me ez fb mf mg"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hi fi z dy ml ea eb mm ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu jg mg"/></div></div></a></div><p id="2471" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" rel="noopener" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb">成为ML写手</a></p></div></div>    
</body>
</html>