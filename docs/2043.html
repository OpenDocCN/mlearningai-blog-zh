<html>
<head>
<title>Transfer learning with Transformers trainer and pipeline for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨Transformers trainerå’Œpipelineè¿›è¡ŒNLPçš„è¿ç§»å­¦ä¹ </h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/mlearning-ai/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d?source=collection_archive---------2-----------------------#2022-02-26">https://medium.com/mlearning-ai/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d?source=collection_archive---------2-----------------------#2022-02-26</a></blockquote><div><div class="es gk gl gm gn go"/><div class="gp gq gr gs gt"><div class=""/><p id="786b" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">æ·±åº¦å­¦ä¹ å¦‚ä»Šæœ‰å¾ˆå¤šæœ‰è¶£çš„åº”ç”¨ï¼Œ<a class="ae ir" href="https://www.businessinsider.com/samsung-lg-unveil-artificial-intelligence-equipped-smart-fridges-2020-1" rel="noopener ugc nofollow" target="_blank">æ™ºèƒ½å†°ç®±</a>ï¼Œ<a class="ae ir" href="https://digital.hbs.edu/platform-digit/submission/thyssenkrup-steelmaker-transforms-elevators-using-ai/" rel="noopener ugc nofollow" target="_blank">æ™ºèƒ½ç”µæ¢¯</a>ï¼Œ<a class="ae ir" href="https://towardsdatascience.com/deep-brew-transforming-starbucks-into-an-ai-data-driven-company-8eb2e370af7b" rel="noopener" target="_blank">æ·±é…¿</a>ï¼Œ<a class="ae ir" href="https://en.wikipedia.org/wiki/Self-driving_car" rel="noopener ugc nofollow" target="_blank">è‡ªåŠ¨é©¾é©¶</a>ï¼Œä»…ä¸¾å‡ ä¾‹ã€‚</p><div class="is it eg ei iu iv"><a href="https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">7æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨-æœºå™¨å­¦ä¹ æŒæ¡</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ­£ä»ç»Ÿè®¡æ–¹æ³•è½¬å‘ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚æœ‰â€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">machinelearningmastery.com</p></div></div><div class="jj l"><div class="jk l jl jm jn jj jo jp iv"/></div></div></a></div><p id="9244" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">ç„¶è€Œï¼Œé€šå¸¸ä½ éœ€è¦å¤§é‡æ•°æ®æ¥è®­ç»ƒ<a class="ae ir" href="https://pub.towardsai.net/state-of-the-art-models-in-every-machine-learning-field-2021-c7cf074da8b2" rel="noopener ugc nofollow" target="_blank"> SOTA </a>(æœ€å…ˆè¿›çš„)æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¹Ÿè®¸ä½ éœ€è¦è®¡ç®—èƒ½åŠ›ï¼Œä½†æ˜¯åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®ã€‚å¦‚æœæ²¡æœ‰é‚£ä¹ˆå¤šæ•°æ®ï¼Œä½ èƒ½åšä»€ä¹ˆï¼Ÿè¿›å…¥<a class="ae ir" href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" rel="noopener" target="_blank">è¿ç§»å­¦ä¹ </a>çš„ä¸–ç•Œã€‚</p><p id="c99f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œè¿ç§»å­¦ä¹ æ˜¯é‡ç”¨é€šè¿‡è§£å†³å…¶ä»–é—®é¢˜è·å¾—çš„â€œçŸ¥è¯†â€(è¿™äº›çŸ¥è¯†æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œå¹¶è¾¾åˆ°SOTAæ°´å¹³)ï¼Œä»¥æœ‰æ•ˆåœ°è§£å†³ä¸€ä¸ªä¸åŒä½†ç›¸å…³çš„é—®é¢˜ï¼Œè€Œä¸éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ã€‚</p><div class="is it eg ei iu iv"><a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">æ·±åº¦å­¦ä¹ è¿ç§»å­¦ä¹ çš„æ¸©å’Œä»‹ç»â€”â€”æœºå™¨å­¦ä¹ æŒæ¡</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸ºä¸€é¡¹ä»»åŠ¡å¼€å‘çš„æ¨¡å‹è¢«é‡æ–°ç”¨ä½œä¸€é¡¹ä»»åŠ¡çš„èµ·ç‚¹</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">machinelearningmastery.com</p></div></div><div class="jj l"><div class="jq l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://www.seldon.io/transfer-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">æœºå™¨å­¦ä¹ çš„è¿ç§»å­¦ä¹ </h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">æœºå™¨å­¦ä¹ çš„è¿ç§»å­¦ä¹ æ˜¯æŒ‡é¢„è®­ç»ƒæ¨¡å‹çš„å…ƒç´ åœ¨æ–°çš„æœºå™¨å­¦ä¹ ä¸­è¢«é‡ç”¨â€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.seldon.io</p></div></div><div class="jj l"><div class="jr l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://research.aimultiple.com/transfer-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">2022å¹´çš„è¿ç§»å­¦ä¹ :å®ƒæ˜¯ä»€ä¹ˆ&amp;å¦‚ä½•å·¥ä½œ</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹å¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚è®­ç»ƒç®—æ³•å¯èƒ½ä¸ä¼šåƒâ€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">research.aimultiple.com</p></div></div><div class="jj l"><div class="js l jl jm jn jj jo jp iv"/></div></div></a></div><p id="3dff" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">åœ¨ä¸€ç³»åˆ—æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³ä¸ºå¼€å‘äººå‘˜æä¾›åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ä¸­ä½¿ç”¨è¿ç§»å­¦ä¹ çš„å¿«é€Ÿå…¥é—¨ã€‚</p><p id="a5f0" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œ<a class="ae ir" href="https://towardsdatascience.com/transformers-89034557de14" rel="noopener" target="_blank"> transformersæ¶æ„</a>æ˜¯è§£å†³ä¸åŒé—®é¢˜çš„é¦–é€‰æ¨¡å‹ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡ã€æ–‡æœ¬ç”Ÿæˆã€é—®é¢˜å›ç­”ç­‰ã€‚å¤šäºäº†<a class="ae ir" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">æ‹¥æŠ±è„¸</a>å’Œå®ƒçš„ç”Ÿæ€ç³»ç»Ÿï¼Œ<a class="ae ir" href="https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5" rel="noopener" target="_blank">ç”¨å˜å½¢é‡‘åˆš</a>è½¬ç§»å­¦ä¹ å·²ç»å˜å¾—éå¸¸å®¹æ˜“å¼€å§‹äº†ã€‚</p><p id="ff4a" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">è®©æˆ‘ä»¬æ¥å…³æ³¨ä¸€ä¸‹å˜å½¢é‡‘åˆšçš„è¿ç§»å­¦ä¹ ï¼Œä¸»è¦æ˜¯å¦‚ä½•ä»å˜å½¢é‡‘åˆšåº“ä¸­å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å˜å½¢é‡‘åˆšåº“æä¾›äº†<a class="ae ir" href="https://huggingface.co/docs/transformers/training" rel="noopener ugc nofollow" target="_blank">è®­ç»ƒå™¨</a>å’Œ<a class="ae ir" href="https://huggingface.co/docs/transformers/quicktour" rel="noopener ugc nofollow" target="_blank">æµæ°´çº¿</a>ï¼Œè®©è®­ç»ƒå’Œé¢„æµ‹å˜å¾—çœŸæ­£å®¹æ˜“ã€‚</p><h1 id="3e64" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">æ–‡æœ¬åˆ†ç±»</h1><h2 id="634d" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åŠ è½½æ•°æ®é›†</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="8783" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="4ca6" class="kr ju gw lk b ev ls lp l lq lr">raw_datasets = load_dataset("imdb")</span></pre><h2 id="9560" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®</h2><p id="35d2" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">ç›®çš„æ˜¯å°†æ–‡æœ¬æ ‡è®°ä¸ºæ¨¡å‹ç¨åå¯è¯»çš„æ ¼å¼ã€‚é‡è¦çš„æ˜¯è¦åŠ è½½ä¸è®­ç»ƒæ¨¡å‹æ—¶ç›¸åŒçš„åˆ†è¯å™¨ï¼Œä»¥ç¡®ä¿æ­£ç¡®åœ°å¯¹å•è¯è¿›è¡Œåˆ†è¯ã€‚</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="de0b" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoTokenizer</span><span id="70ae" class="kr ju gw lk b ev ls lp l lq lr">tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")</span><span id="1d74" class="kr ju gw lk b ev ls lp l lq lr">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True)<br/><br/><br/>tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)</span></pre><p id="913f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">ç†è§£æ•°æ®çš„å½¢çŠ¶å¯¹æˆ‘ä»¬æ¥è¯´ä¹Ÿå¾ˆé‡è¦ã€‚å¦‚æœä½ çœ‹çœ‹tokenized_datasetså†…éƒ¨ï¼Œå®ƒæ˜¯</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="caad" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],<br/>        num_rows: 25000<br/>    })</span></pre><p id="05db" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">æ–‡æœ¬å’Œæ ‡ç­¾åœ¨åŸå§‹åŸå§‹æ•°æ®é›†ä¸­ã€‚è¦åšåˆ†ç±»ï¼Œä½ éœ€è¦â€œæ ‡ç­¾â€æ¥å‘Šè¯‰åœ°é¢çœŸç›¸ã€‚â€œinput _ idsâ€æ˜¯è®°å·ç´¢å¼•ï¼Œè®°å·çš„æ•°å­—è¡¨ç¤ºæ„æˆå°†è¢«æ¨¡å‹ç”¨ä½œè¾“å…¥çš„åºåˆ—ã€‚â€œattention_maskâ€æŒ‡ç¤ºä»¤ç‰Œç´¢å¼•çš„ä½ç½®ï¼Œä½¿å¾—æ¨¡å‹ä¸å…³æ³¨å®ƒä»¬(é€šå¸¸1æŒ‡ç¤ºåº”è¯¥å…³æ³¨çš„å€¼)ã€‚â€œtoken_type_idsâ€è¢«ä¸€äº›æ¨¡å‹ç”¨æ¥è¯†åˆ«åºåˆ—çš„ç±»å‹(æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ<a class="ae ir" href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="noopener ugc nofollow" target="_blank"> Token Type IDs </a>)ã€‚äº†è§£æ›´å¤šå…³äº<a class="ae ir" href="https://huggingface.co/docs/transformers/preprocessing" rel="noopener ugc nofollow" target="_blank">é¢„å¤„ç†</a>çš„ä¿¡æ¯ã€‚</p><h2 id="8d5a" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åˆ†å‰²è®­ç»ƒï¼Œæµ‹è¯•æ•°æ®é›†</h2><p id="fdf4" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">raw_datasetså¯¹è±¡æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæœ‰ä¸‰ä¸ªé”®:â€œtrainâ€ã€â€œtestâ€å’Œâ€œunsupervisedâ€(å¯¹åº”äºæ•°æ®é›†çš„ä¸‰ä¸ªæ‹†åˆ†)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨â€œè®­ç»ƒâ€åˆ†å‰²è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨â€œæµ‹è¯•â€åˆ†å‰²è¿›è¡ŒéªŒè¯ã€‚</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="f0e4" class="kr ju gw lk b ev lo lp l lq lr">small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))<br/>small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))<br/>full_train_dataset = tokenized_datasets["train"]<br/>full_eval_dataset = tokenized_datasets["test"]</span></pre><h2 id="23a8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">å®šä¹‰é¢„è®­ç»ƒæ¨¡å¼</h2><p id="7488" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">è¯·æ³¨æ„ï¼Œæ¨¡å‹åç§°ä¸tokenizerç›¸åŒ</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="d9bf" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForSequenceClassification</span><span id="1dec" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")</span></pre><h2 id="933b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">è¿åŠ¨é‹</h2><p id="1ca6" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">ç°åœ¨æœ€ç²¾å½©çš„éƒ¨åˆ†æ¥äº†ã€‚å®šä¹‰æ‚¨çš„è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†åˆ›å»ºè®­ç»ƒå™¨</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b404" class="kr ju gw lk b ev lo lp l lq lr">from transformers import TrainingArguments</span><span id="454a" class="kr ju gw lk b ev ls lp l lq lr">training_args = TrainingArguments("test_trainer")</span><span id="d879" class="kr ju gw lk b ev ls lp l lq lr">from transformers import Trainer</span><span id="0267" class="kr ju gw lk b ev ls lp l lq lr">trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)</span></pre><h2 id="7ad9" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">å¼€å§‹è®­ç»ƒ</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4230" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><p id="4d1e" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">è¿™ä¸ªAPIéå¸¸ç®€å•(å½“ç„¶ä½ å¯ä»¥åšå¾ˆå¤šå®šåˆ¶ï¼Œæ¯”å¦‚è®­ç»ƒæ—¶é—´ï¼Œå­¦ä¹ é€Ÿåº¦)ã€‚å¦åˆ™ï¼Œåœ¨æœ¬æœºPytorchä¸­å°†ä¼šå‡ºç°å¦‚ä¸‹å†…å®¹:</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b955" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AdamW</span><span id="7018" class="kr ju gw lk b ev ls lp l lq lr">optimizer = AdamW(model.parameters(), lr=5e-5)</span><span id="8353" class="kr ju gw lk b ev ls lp l lq lr">from transformers import get_scheduler</span><span id="7b17" class="kr ju gw lk b ev ls lp l lq lr">num_epochs = 3<br/>num_training_steps = num_epochs * len(train_dataloader)<br/>lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)</span><span id="1255" class="kr ju gw lk b ev ls lp l lq lr">import torch</span><span id="a907" class="kr ju gw lk b ev ls lp l lq lr">device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")<br/>model.to(device)</span><span id="5e49" class="kr ju gw lk b ev ls lp l lq lr">from tqdm.auto import tqdm</span><span id="fade" class="kr ju gw lk b ev ls lp l lq lr">progress_bar = tqdm(range(num_training_steps))</span><span id="73be" class="kr ju gw lk b ev ls lp l lq lr">model.train()<br/>for epoch in range(num_epochs):<br/>    for batch in train_dataloader:<br/>        batch = {k: v.to(device) for k, v in batch.items()}<br/>        outputs = model(**batch)<br/>        loss = outputs.loss<br/>        loss.backward()</span><span id="fed1" class="kr ju gw lk b ev ls lp l lq lr">optimizer.step()<br/>        lr_scheduler.step()<br/>        optimizer.zero_grad()<br/>        progress_bar.update(1)</span></pre><p id="af6d" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">å°†è¿™äº›æ ·æ¿ä»£ç æ”¾å…¥ä¸€ä¸ªæ›´å¹²å‡€çš„ç•Œé¢æ˜¯å¾ˆå¥½çš„ã€‚</p><h2 id="0f1b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç®¡é“</h2><p id="fc43" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">ç°åœ¨æˆ‘ä»¬ç”¨å¾®è°ƒåçš„æ¨¡å‹æ¥åšé¢„æµ‹ã€‚é¦–å…ˆï¼Œä¿å­˜æ¨¡å‹</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="3030" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><p id="e730" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">å¹¶é¢„æµ‹ã€‚å˜å½¢é‡‘åˆšåº“æä¾›äº†åŸºäºä»»åŠ¡çš„ç®¡é“æ¥ç®€åŒ–æˆ‘ä»¬çš„ä»»åŠ¡ã€‚</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="19de" class="kr ju gw lk b ev lo lp l lq lr">from transformers import pipeline</span><span id="cd1a" class="kr ju gw lk b ev ls lp l lq lr"># load from previously saved model<br/>pipe = pipeline("text-classification", model="test_trainer", tokenizer="bert-base-cased")<br/>pipe("This restaurant is awesome")</span></pre><p id="3b3d" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">è¾“å‡º</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="1fd6" class="kr ju gw lk b ev lo lp l lq lr">[{'label': 'LABEL_1', 'score': 0.9312610626220703}]</span></pre><h1 id="58ac" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">é—®ç­”<br/>åŠ è½½æ•°æ®é›†</h1><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="86a1" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="9770" class="kr ju gw lk b ev ls lp l lq lr">squad = load_dataset("squad")</span></pre><h2 id="6630" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b3ad" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoTokenizer</span><span id="3f00" class="kr ju gw lk b ev ls lp l lq lr">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")</span><span id="4b8b" class="kr ju gw lk b ev ls lp l lq lr">def preprocess_function(examples):<br/>    questions = [q.strip() for q in examples["question"]]<br/>    inputs = tokenizer(<br/>        questions,<br/>        examples["context"],<br/>        max_length=384,<br/>        truncation="only_second",<br/>        return_offsets_mapping=True,<br/>        padding="max_length",<br/>    )</span><span id="d1dd" class="kr ju gw lk b ev ls lp l lq lr"># offset_mapping gives corresponding start and end character in the original text for each token in our input IDs</span><span id="1c62" class="kr ju gw lk b ev ls lp l lq lr">offset_mapping = inputs.pop("offset_mapping")<br/>    answers = examples["answers"]<br/>    start_positions = []<br/>    end_positions = []</span><span id="fc8d" class="kr ju gw lk b ev ls lp l lq lr">for i, offset in enumerate(offset_mapping):<br/>        answer = answers[i]<br/>        start_char = answer["answer_start"][0]<br/>        end_char = answer["answer_start"][0] + len(answer["text"][0])</span><span id="2677" class="kr ju gw lk b ev ls lp l lq lr"># sequence_ids helps distinguish which parts of the offsets correspond to the question and which part correspond to the context<br/>        sequence_ids = inputs.sequence_ids(i)</span><span id="1de9" class="kr ju gw lk b ev ls lp l lq lr"># Find the start and end of the context<br/>        idx = 0<br/>        while sequence_ids[idx] != 1:<br/>            idx += 1<br/>        context_start = idx<br/>        while sequence_ids[idx] == 1:<br/>            idx += 1<br/>        context_end = idx - 1</span><span id="503e" class="kr ju gw lk b ev ls lp l lq lr"># If the answer is not fully inside the context, label it (0, 0)<br/>        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:<br/>            start_positions.append(0)<br/>            end_positions.append(0)<br/>        else:<br/>            # Otherwise it's the start and end token positions<br/>            idx = context_start<br/>            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:<br/>                idx += 1<br/>            start_positions.append(idx - 1)</span><span id="cdee" class="kr ju gw lk b ev ls lp l lq lr">idx = context_end<br/>            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:<br/>                idx -= 1<br/>            end_positions.append(idx + 1)</span><span id="4606" class="kr ju gw lk b ev ls lp l lq lr">inputs["start_positions"] = start_positions<br/>    inputs["end_positions"] = end_positions<br/>    return inputs</span><span id="37ca" class="kr ju gw lk b ev ls lp l lq lr">tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)</span></pre><p id="cb29" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">è®°å·åŒ–_å°é˜Ÿçš„å½¢çŠ¶</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="181a" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],<br/>        num_rows: 87599<br/>    })</span></pre><p id="0d82" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">åŸå§‹çš„å°é˜Ÿæ•°æ®é›†æœ‰ä¸Šä¸‹æ–‡ã€é—®é¢˜ã€ç­”æ¡ˆï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦åœ¨æ ‡è®°ä¸­æ‰¾åˆ°è¿™äº›ç­”æ¡ˆçš„ç¡®åˆ‡å¼€å§‹å’Œç»“æŸä½ç½®ã€‚è¿™å°±æ˜¯â€œèµ·å§‹ä½ç½®â€ã€â€œç»“æŸä½ç½®â€å‘Šè¯‰æˆ‘ä»¬çš„ã€‚</p><h2 id="2acc" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">è´Ÿè½½æ¨¡å‹</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="74d6" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer</span><span id="15d3" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")</span></pre><h2 id="d115" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç›¸åŒçš„åŸ¹è®­å¸ˆæ¨¡å¼</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="3fcb" class="kr ju gw lk b ev lo lp l lq lr">training_args = TrainingArguments(<br/>    output_dir="./results",<br/>    evaluation_strategy="epoch",<br/>    learning_rate=2e-5,<br/>    per_device_train_batch_size=16,<br/>    per_device_eval_batch_size=16,<br/>    num_train_epochs=3,<br/>    weight_decay=0.01<br/>)</span><span id="f9dd" class="kr ju gw lk b ev ls lp l lq lr">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=tokenized_squad["train"],<br/>    eval_dataset=tokenized_squad["validation"],<br/>    data_collator=data_collator,<br/>    tokenizer=tokenizer<br/>)</span></pre><h2 id="57e6" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç«è½¦</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4d38" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><h2 id="57ea" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç®¡é“é¢„æµ‹</h2><p id="3e59" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">é¦–å…ˆä¿å­˜æ¨¡å‹</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="a481" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><p id="a061" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">ç°åœ¨æˆ‘ä»¬ä½¿ç”¨â€œé—®ç­”â€ç®¡é“æ¥å®Œæˆé—®ç­”ä»»åŠ¡</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="188d" class="kr ju gw lk b ev lo lp l lq lr">context  = r"""<br/>The Mars Orbiter Mission (MOM), also called Mangalyaan ("Mars-craft", from Mangala, "Mars" and yÄna, "craft, vehicle")<br/>is a space probe orbiting Mars since 24 September 2014? It was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO). It is India's first interplanetary mission<br/>and it made India the fourth country to achieve Mars orbit, after Roscosmos, NASA, and the European Space Company. and it made India the first country to achieve this in the first attempt.<br/>The Mars Orbiter took off from the First Launch Pad at Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08 UTC on 5 November 2013.<br/>The launch window was approximately 20 days long and started on 28 October 2013.<br/>The MOM probe spent about 36 days in  Earth orbit, where it made a series of seven apogee-raising orbital maneuvers before trans-Mars injection<br/>on 30 November 2013 (UTC).[23] After a 298-day long journey to the Mars Orbit, it was put into Mars orbit on 24 September 2014."""<br/>nlp = pipeline("question-answering")<br/>result = nlp(question="When did Mars Mission Launched?", context=context)<br/>print(result['answer'])</span></pre><h1 id="d618" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">æ‘˜è¦</h1><h2 id="95e8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åŠ è½½æ•°æ®é›†</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4501" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="ad00" class="kr ju gw lk b ev ls lp l lq lr">raw_datasets = load_dataset("xsum")</span></pre><h2 id="e2d8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="7d7d" class="kr ju gw lk b ev lo lp l lq lr">model_checkpoint = "t5-small"<br/>from transformers import AutoTokenizer<br/>    <br/>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><span id="fc4b" class="kr ju gw lk b ev ls lp l lq lr">if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:<br/>    prefix = "summarize: "<br/>else:<br/>    prefix = ""</span><span id="8f17" class="kr ju gw lk b ev ls lp l lq lr">max_input_length = 1024<br/>max_target_length = 128</span><span id="36e4" class="kr ju gw lk b ev ls lp l lq lr">def preprocess_function(examples):<br/>    inputs = [prefix + doc for doc in examples["document"]]<br/>    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)</span><span id="7794" class="kr ju gw lk b ev ls lp l lq lr"># Setup the tokenizer for targets<br/>    with tokenizer.as_target_tokenizer():<br/>        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True)</span><span id="7f32" class="kr ju gw lk b ev ls lp l lq lr">model_inputs["labels"] = labels["input_ids"]<br/>    return model_inputs</span><span id="9f92" class="kr ju gw lk b ev ls lp l lq lr">tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)</span></pre><p id="d49f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">ç¬¦å·åŒ–æ•°æ®é›†çš„å½¢çŠ¶</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4e40" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],<br/>        num_rows: 204045<br/>    })</span></pre><p id="cc3b" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">â€œæ–‡æ¡£â€ã€â€œæ‘˜è¦â€ã€â€œidâ€æ¥è‡ªåŸå§‹æ•°æ®é›†ï¼Œè¡¨ç¤ºåŸå§‹æ–‡æ¡£ã€æ–‡æ¡£æ‘˜è¦ã€æ–‡æ¡£ç´¢å¼•å·ã€‚â€œæ ‡ç­¾â€æ˜¯æ ‡è®°åŒ–æ‘˜è¦çš„â€œè¾“å…¥æ ‡è¯†â€ã€‚</p><h2 id="a109" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">è´Ÿè½½æ¨¡å‹</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="557e" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer</span><span id="1194" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</span></pre><h2 id="736f" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">åˆ›å»ºåŸ¹è®­å¸ˆ</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="475d" class="kr ju gw lk b ev lo lp l lq lr">batch_size = 16<br/>model_name = model_checkpoint.split("/")[-1]<br/>args = Seq2SeqTrainingArguments(<br/>    f"{model_name}-finetuned-xsum",<br/>    evaluation_strategy = "epoch",<br/>    learning_rate=2e-5,<br/>    per_device_train_batch_size=batch_size,<br/>    per_device_eval_batch_size=batch_size,<br/>    weight_decay=0.01,<br/>    save_total_limit=3,<br/>    num_train_epochs=1,<br/>    predict_with_generate=True,<br/>    fp16=True<br/>)</span><span id="42a7" class="kr ju gw lk b ev ls lp l lq lr">data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)</span><span id="320e" class="kr ju gw lk b ev ls lp l lq lr">trainer = Seq2SeqTrainer(<br/>    model,<br/>    args,<br/>    train_dataset=tokenized_datasets["train"],<br/>    eval_dataset=tokenized_datasets["validation"],<br/>    data_collator=data_collator,<br/>    tokenizer=tokenizer<br/>)</span></pre><h2 id="455b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç«è½¦</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="8d18" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><h2 id="4fb3" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç®¡é“é¢„æµ‹</h2><p id="3528" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">é¦–å…ˆä¿å­˜æ¨¡å‹</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="6e3a" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><h2 id="dc42" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ä½¿ç”¨ç®¡é“é¢„æµ‹</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="bc20" class="kr ju gw lk b ev lo lp l lq lr"># use bart in pytorch<br/>from transformers import pipeline<br/>summarizer = pipeline("summarization", model=f"{model_name}-finetuned-xsum",tokenizer=model_checkpoint)<br/>summarizer("Designing a proper application architecture is the difference between a project that flourishes and one that gets pigeonholed into a set of inextensible features. When it comes to an open source project like Ostia, it becomes especially important, as the â€˜activation energyâ€™ for a developer to get up to speed on a project is a huge barrier.", min_length=5, max_length=20)<br/>#[{'summary_text': 'Ostia is an open source project that flourishes and gets pigeonhole'}]</span></pre><h1 id="d549" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">å·¥ä½œæµæ¨¡å¼</h1><p id="186d" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">å¯¹äºè¿™äº›ä»»åŠ¡ï¼Œæ¨¡å¼æ˜¯</p><ol class=""><li id="443c" class="ly lz gw hv b hw hx ia ib ie ma ii mb im mc iq md me mf mg bi translated">åŠ è½½åŸå§‹æ•°æ®é›†</li><li id="c1e6" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">åŠ è½½æ ‡è®°å™¨å¹¶æ ‡è®°æ•°æ®</li><li id="7b8f" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">å¯é€‰:åˆ†ç±»ä»»åŠ¡æ—¶æ‹†åˆ†æ•°æ®</li><li id="3970" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">å®šä¹‰åŸ¹è®­å‚æ•°å’ŒåŸ¹è®­å¸ˆ</li><li id="c01d" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">å¼€å§‹è®­ç»ƒ/å¾®è°ƒ</li><li id="b8f3" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">ä¿å­˜æ¨¡å‹ï¼Œç„¶ååŠ è½½å¸¦æœ‰ç®¡é“å’Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹</li><li id="2108" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">ä½¿ç”¨ç®¡é“é¢„æµ‹</li></ol><h1 id="976d" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">é™„å½•</h1><h2 id="de99" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">è¿ç§»å­¦ä¹ </h2><div class="is it eg ei iu iv"><a href="https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751" rel="noopener follow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">ä»é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ä¸­è½¬ç§»å­¦ä¹ </h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">å¦‚ä½•å¿«é€Ÿç®€å•åœ°è§£å†³ä»»ä½•å›¾åƒåˆ†ç±»é—®é¢˜</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">towardsdatascience.com</p></div></div><div class="jj l"><div class="mm l jl jm jn jj jo jp iv"/></div></div></a></div><h2 id="66f5" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">ç®¡é“</h2><div class="is it eg ei iu iv"><a href="https://www.analyticsvidhya.com/blog/2021/12/all-nlp-tasks-using-transformers-package/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">ä½¿ç”¨Transformers Pipeline-Analytics vid hyaçš„æ‰€æœ‰NLPä»»åŠ¡</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">1.ç†è§£å˜å‹å™¨åŒ…2ã€‚æ–‡æœ¬åˆ†ç±»3ã€‚é—®é¢˜å›ç­”4ã€‚æ–‡æœ¬æ‘˜è¦5ã€‚è¯­è¨€â€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.analyticsvidhya.com</p></div></div><div class="jj l"><div class="mn l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">æ‹¥æŠ±è„¸å˜å‹å™¨ç®¡é“åŠŸèƒ½|é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">è¿™ç¯‡æ–‡ç« ä½œä¸ºæ•°æ®ç§‘å­¦åšå®¢çš„ä¸€éƒ¨åˆ†å‘è¡¨ã€‚ç›®çš„è¿™ç¯‡åšå®¢æ–‡ç« å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨â€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.analyticsvidhya.com</p></div></div><div class="jj l"><div class="mo l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a rel="noopener follow" target="_blank" href="/geekculture/pipeline-object-in-transformers-using-hugging-face-6577f57a4c18"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">å˜å½¢é‡‘åˆšä¸­çš„ç®¡é“å¯¹è±¡ğŸ¤—</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">ä¸Šå‘¨æˆ‘å®Œæˆäº†ä¸€ä¸ªå…è´¹çš„æ‹¥æŠ±è„¸è¯¾ç¨‹ï¼Œåœ¨é‚£é‡Œæˆ‘å­¦åˆ°äº†å¾ˆå¤šæ–°ä¸œè¥¿ï¼Œæ¯”å¦‚å˜å½¢é‡‘åˆšâ€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">medium.com</p></div></div><div class="jj l"><div class="mp l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://huggingface.co/docs/transformers/glossary" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">è¯æ±‡è¡¨</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">è‡ªåŠ¨ç¼–ç æ¨¡å‹:è§MLMè‡ªå›å½’æ¨¡å‹:è§CLM CLM:å› æœè¯­è¨€å»ºæ¨¡ï¼Œä¸€ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œå…¶ä¸­â€¦</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">huggingface.co</p></div></div><div class="jj l"><div class="mq l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">Mlearning.aiæäº¤å»ºè®®</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">å¦‚ä½•æˆä¸ºMlearning.aiä¸Šçš„ä½œå®¶</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">medium.com</p></div></div><div class="jj l"><div class="mr l jl jm jn jj jo jp iv"/></div></div></a></div></div></div>    
</body>
</html>