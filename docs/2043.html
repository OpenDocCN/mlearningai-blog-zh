<html>
<head>
<title>Transfer learning with Transformers trainer and pipeline for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Transformers trainer和pipeline进行NLP的迁移学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d?source=collection_archive---------2-----------------------#2022-02-26">https://medium.com/mlearning-ai/transfer-learning-with-transformers-trainer-and-pipeline-for-nlp-8b1d2c1a8c3d?source=collection_archive---------2-----------------------#2022-02-26</a></blockquote><div><div class="es gk gl gm gn go"/><div class="gp gq gr gs gt"><div class=""/><p id="786b" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">深度学习如今有很多有趣的应用，<a class="ae ir" href="https://www.businessinsider.com/samsung-lg-unveil-artificial-intelligence-equipped-smart-fridges-2020-1" rel="noopener ugc nofollow" target="_blank">智能冰箱</a>，<a class="ae ir" href="https://digital.hbs.edu/platform-digit/submission/thyssenkrup-steelmaker-transforms-elevators-using-ai/" rel="noopener ugc nofollow" target="_blank">智能电梯</a>，<a class="ae ir" href="https://towardsdatascience.com/deep-brew-transforming-starbucks-into-an-ai-data-driven-company-8eb2e370af7b" rel="noopener" target="_blank">深酿</a>，<a class="ae ir" href="https://en.wikipedia.org/wiki/Self-driving_car" rel="noopener ugc nofollow" target="_blank">自动驾驶</a>，仅举几例。</p><div class="is it eg ei iu iv"><a href="https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">7深度学习在自然语言处理中的应用-机器学习掌握</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">自然语言处理领域正从统计方法转向神经网络方法。有…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">machinelearningmastery.com</p></div></div><div class="jj l"><div class="jk l jl jm jn jj jo jp iv"/></div></div></a></div><p id="9244" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">然而，通常你需要大量数据来训练<a class="ae ir" href="https://pub.towardsai.net/state-of-the-art-models-in-every-machine-learning-field-2021-c7cf074da8b2" rel="noopener ugc nofollow" target="_blank"> SOTA </a>(最先进的)深度学习模型。也许你需要计算能力，但是到目前为止，你需要大量高质量的数据。如果没有那么多数据，你能做什么？进入<a class="ae ir" href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" rel="noopener" target="_blank">迁移学习</a>的世界。</p><p id="c99f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">在高层次上，迁移学习是重用通过解决其他问题获得的“知识”(这些知识是在大量数据上训练的，并达到SOTA水平)，以有效地解决一个不同但相关的问题，而不需要大量的标记数据。</p><div class="is it eg ei iu iv"><a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">深度学习迁移学习的温和介绍——机器学习掌握</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">迁移学习是一种机器学习方法，在这种方法中，为一项任务开发的模型被重新用作一项任务的起点</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">machinelearningmastery.com</p></div></div><div class="jj l"><div class="jq l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://www.seldon.io/transfer-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">机器学习的迁移学习</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">机器学习的迁移学习是指预训练模型的元素在新的机器学习中被重用…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.seldon.io</p></div></div><div class="jj l"><div class="jr l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://research.aimultiple.com/transfer-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">2022年的迁移学习:它是什么&amp;如何工作</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">训练机器学习模型可能是一项具有挑战性的数据科学任务。训练算法可能不会像…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">research.aimultiple.com</p></div></div><div class="jj l"><div class="js l jl jm jn jj jo jp iv"/></div></div></a></div><p id="3dff" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">在一系列文章中，我想为开发人员提供在自然语言处理、计算机视觉中使用迁移学习的快速入门。</p><p id="a5f0" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">对于自然语言处理，<a class="ae ir" href="https://towardsdatascience.com/transformers-89034557de14" rel="noopener" target="_blank"> transformers架构</a>是解决不同问题的首选模型，例如文本分类、机器翻译、语言建模、文本生成、问题回答等。多亏了<a class="ae ir" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>和它的生态系统，<a class="ae ir" href="https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5" rel="noopener" target="_blank">用变形金刚</a>转移学习已经变得非常容易开始了。</p><p id="ff4a" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">让我们来关注一下变形金刚的迁移学习，主要是如何从变形金刚库中微调一个预训练的模型。变形金刚库提供了<a class="ae ir" href="https://huggingface.co/docs/transformers/training" rel="noopener ugc nofollow" target="_blank">训练器</a>和<a class="ae ir" href="https://huggingface.co/docs/transformers/quicktour" rel="noopener ugc nofollow" target="_blank">流水线</a>，让训练和预测变得真正容易。</p><h1 id="3e64" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">文本分类</h1><h2 id="634d" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">加载数据集</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="8783" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="4ca6" class="kr ju gw lk b ev ls lp l lq lr">raw_datasets = load_dataset("imdb")</span></pre><h2 id="9560" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">加载标记器并标记数据</h2><p id="35d2" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">目的是将文本标记为模型稍后可读的格式。重要的是要加载与训练模型时相同的分词器，以确保正确地对单词进行分词。</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="de0b" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoTokenizer</span><span id="70ae" class="kr ju gw lk b ev ls lp l lq lr">tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")</span><span id="1d74" class="kr ju gw lk b ev ls lp l lq lr">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True)<br/><br/><br/>tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)</span></pre><p id="913f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">理解数据的形状对我们来说也很重要。如果你看看tokenized_datasets内部，它是</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="caad" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],<br/>        num_rows: 25000<br/>    })</span></pre><p id="05db" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">文本和标签在原始原始数据集中。要做分类，你需要“标签”来告诉地面真相。“input _ ids”是记号索引，记号的数字表示构成将被模型用作输入的序列。“attention_mask”指示令牌索引的位置，使得模型不关注它们(通常1指示应该关注的值)。“token_type_ids”被一些模型用来识别序列的类型(更多信息请参考<a class="ae ir" href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="noopener ugc nofollow" target="_blank"> Token Type IDs </a>)。了解更多关于<a class="ae ir" href="https://huggingface.co/docs/transformers/preprocessing" rel="noopener ugc nofollow" target="_blank">预处理</a>的信息。</p><h2 id="8d5a" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">分割训练，测试数据集</h2><p id="fdf4" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">raw_datasets对象是一个字典，有三个键:“train”、“test”和“unsupervised”(对应于数据集的三个拆分)。我们将使用“训练”分割进行训练，使用“测试”分割进行验证。</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="f0e4" class="kr ju gw lk b ev lo lp l lq lr">small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))<br/>small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))<br/>full_train_dataset = tokenized_datasets["train"]<br/>full_eval_dataset = tokenized_datasets["test"]</span></pre><h2 id="23a8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">定义预训练模式</h2><p id="7488" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">请注意，模型名称与tokenizer相同</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="d9bf" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForSequenceClassification</span><span id="1dec" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")</span></pre><h2 id="933b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">运动鞋</h2><p id="1ca6" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">现在最精彩的部分来了。定义您的训练参数，使用您的模型、训练参数、训练数据集和评估数据集创建训练器</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b404" class="kr ju gw lk b ev lo lp l lq lr">from transformers import TrainingArguments</span><span id="454a" class="kr ju gw lk b ev ls lp l lq lr">training_args = TrainingArguments("test_trainer")</span><span id="d879" class="kr ju gw lk b ev ls lp l lq lr">from transformers import Trainer</span><span id="0267" class="kr ju gw lk b ev ls lp l lq lr">trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)</span></pre><h2 id="7ad9" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">开始训练</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4230" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><p id="4d1e" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">这个API非常简单(当然你可以做很多定制，比如训练时间，学习速度)。否则，在本机Pytorch中将会出现如下内容:</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b955" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AdamW</span><span id="7018" class="kr ju gw lk b ev ls lp l lq lr">optimizer = AdamW(model.parameters(), lr=5e-5)</span><span id="8353" class="kr ju gw lk b ev ls lp l lq lr">from transformers import get_scheduler</span><span id="7b17" class="kr ju gw lk b ev ls lp l lq lr">num_epochs = 3<br/>num_training_steps = num_epochs * len(train_dataloader)<br/>lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)</span><span id="1255" class="kr ju gw lk b ev ls lp l lq lr">import torch</span><span id="a907" class="kr ju gw lk b ev ls lp l lq lr">device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")<br/>model.to(device)</span><span id="5e49" class="kr ju gw lk b ev ls lp l lq lr">from tqdm.auto import tqdm</span><span id="fade" class="kr ju gw lk b ev ls lp l lq lr">progress_bar = tqdm(range(num_training_steps))</span><span id="73be" class="kr ju gw lk b ev ls lp l lq lr">model.train()<br/>for epoch in range(num_epochs):<br/>    for batch in train_dataloader:<br/>        batch = {k: v.to(device) for k, v in batch.items()}<br/>        outputs = model(**batch)<br/>        loss = outputs.loss<br/>        loss.backward()</span><span id="fed1" class="kr ju gw lk b ev ls lp l lq lr">optimizer.step()<br/>        lr_scheduler.step()<br/>        optimizer.zero_grad()<br/>        progress_bar.update(1)</span></pre><p id="af6d" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">将这些样板代码放入一个更干净的界面是很好的。</p><h2 id="0f1b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">管道</h2><p id="fc43" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">现在我们用微调后的模型来做预测。首先，保存模型</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="3030" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><p id="e730" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">并预测。变形金刚库提供了基于任务的管道来简化我们的任务。</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="19de" class="kr ju gw lk b ev lo lp l lq lr">from transformers import pipeline</span><span id="cd1a" class="kr ju gw lk b ev ls lp l lq lr"># load from previously saved model<br/>pipe = pipeline("text-classification", model="test_trainer", tokenizer="bert-base-cased")<br/>pipe("This restaurant is awesome")</span></pre><p id="3b3d" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">输出</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="1fd6" class="kr ju gw lk b ev lo lp l lq lr">[{'label': 'LABEL_1', 'score': 0.9312610626220703}]</span></pre><h1 id="58ac" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">问答<br/>加载数据集</h1><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="86a1" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="9770" class="kr ju gw lk b ev ls lp l lq lr">squad = load_dataset("squad")</span></pre><h2 id="6630" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">加载标记器并标记数据</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="b3ad" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoTokenizer</span><span id="3f00" class="kr ju gw lk b ev ls lp l lq lr">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")</span><span id="4b8b" class="kr ju gw lk b ev ls lp l lq lr">def preprocess_function(examples):<br/>    questions = [q.strip() for q in examples["question"]]<br/>    inputs = tokenizer(<br/>        questions,<br/>        examples["context"],<br/>        max_length=384,<br/>        truncation="only_second",<br/>        return_offsets_mapping=True,<br/>        padding="max_length",<br/>    )</span><span id="d1dd" class="kr ju gw lk b ev ls lp l lq lr"># offset_mapping gives corresponding start and end character in the original text for each token in our input IDs</span><span id="1c62" class="kr ju gw lk b ev ls lp l lq lr">offset_mapping = inputs.pop("offset_mapping")<br/>    answers = examples["answers"]<br/>    start_positions = []<br/>    end_positions = []</span><span id="fc8d" class="kr ju gw lk b ev ls lp l lq lr">for i, offset in enumerate(offset_mapping):<br/>        answer = answers[i]<br/>        start_char = answer["answer_start"][0]<br/>        end_char = answer["answer_start"][0] + len(answer["text"][0])</span><span id="2677" class="kr ju gw lk b ev ls lp l lq lr"># sequence_ids helps distinguish which parts of the offsets correspond to the question and which part correspond to the context<br/>        sequence_ids = inputs.sequence_ids(i)</span><span id="1de9" class="kr ju gw lk b ev ls lp l lq lr"># Find the start and end of the context<br/>        idx = 0<br/>        while sequence_ids[idx] != 1:<br/>            idx += 1<br/>        context_start = idx<br/>        while sequence_ids[idx] == 1:<br/>            idx += 1<br/>        context_end = idx - 1</span><span id="503e" class="kr ju gw lk b ev ls lp l lq lr"># If the answer is not fully inside the context, label it (0, 0)<br/>        if offset[context_start][0] &gt; end_char or offset[context_end][1] &lt; start_char:<br/>            start_positions.append(0)<br/>            end_positions.append(0)<br/>        else:<br/>            # Otherwise it's the start and end token positions<br/>            idx = context_start<br/>            while idx &lt;= context_end and offset[idx][0] &lt;= start_char:<br/>                idx += 1<br/>            start_positions.append(idx - 1)</span><span id="cdee" class="kr ju gw lk b ev ls lp l lq lr">idx = context_end<br/>            while idx &gt;= context_start and offset[idx][1] &gt;= end_char:<br/>                idx -= 1<br/>            end_positions.append(idx + 1)</span><span id="4606" class="kr ju gw lk b ev ls lp l lq lr">inputs["start_positions"] = start_positions<br/>    inputs["end_positions"] = end_positions<br/>    return inputs</span><span id="37ca" class="kr ju gw lk b ev ls lp l lq lr">tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)</span></pre><p id="cb29" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">记号化_小队的形状</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="181a" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],<br/>        num_rows: 87599<br/>    })</span></pre><p id="0d82" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">原始的小队数据集有上下文、问题、答案，但是我们需要在标记中找到这些答案的确切开始和结束位置。这就是“起始位置”、“结束位置”告诉我们的。</p><h2 id="2acc" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">负载模型</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="74d6" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer</span><span id="15d3" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")</span></pre><h2 id="d115" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">相同的培训师模式</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="3fcb" class="kr ju gw lk b ev lo lp l lq lr">training_args = TrainingArguments(<br/>    output_dir="./results",<br/>    evaluation_strategy="epoch",<br/>    learning_rate=2e-5,<br/>    per_device_train_batch_size=16,<br/>    per_device_eval_batch_size=16,<br/>    num_train_epochs=3,<br/>    weight_decay=0.01<br/>)</span><span id="f9dd" class="kr ju gw lk b ev ls lp l lq lr">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=tokenized_squad["train"],<br/>    eval_dataset=tokenized_squad["validation"],<br/>    data_collator=data_collator,<br/>    tokenizer=tokenizer<br/>)</span></pre><h2 id="57e6" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">火车</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4d38" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><h2 id="57ea" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">管道预测</h2><p id="3e59" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">首先保存模型</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="a481" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><p id="a061" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">现在我们使用“问答”管道来完成问答任务</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="188d" class="kr ju gw lk b ev lo lp l lq lr">context  = r"""<br/>The Mars Orbiter Mission (MOM), also called Mangalyaan ("Mars-craft", from Mangala, "Mars" and yāna, "craft, vehicle")<br/>is a space probe orbiting Mars since 24 September 2014? It was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO). It is India's first interplanetary mission<br/>and it made India the fourth country to achieve Mars orbit, after Roscosmos, NASA, and the European Space Company. and it made India the first country to achieve this in the first attempt.<br/>The Mars Orbiter took off from the First Launch Pad at Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08 UTC on 5 November 2013.<br/>The launch window was approximately 20 days long and started on 28 October 2013.<br/>The MOM probe spent about 36 days in  Earth orbit, where it made a series of seven apogee-raising orbital maneuvers before trans-Mars injection<br/>on 30 November 2013 (UTC).[23] After a 298-day long journey to the Mars Orbit, it was put into Mars orbit on 24 September 2014."""<br/>nlp = pipeline("question-answering")<br/>result = nlp(question="When did Mars Mission Launched?", context=context)<br/>print(result['answer'])</span></pre><h1 id="d618" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">摘要</h1><h2 id="95e8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">加载数据集</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4501" class="kr ju gw lk b ev lo lp l lq lr">from datasets import load_dataset</span><span id="ad00" class="kr ju gw lk b ev ls lp l lq lr">raw_datasets = load_dataset("xsum")</span></pre><h2 id="e2d8" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">加载标记器并标记数据</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="7d7d" class="kr ju gw lk b ev lo lp l lq lr">model_checkpoint = "t5-small"<br/>from transformers import AutoTokenizer<br/>    <br/>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><span id="fc4b" class="kr ju gw lk b ev ls lp l lq lr">if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:<br/>    prefix = "summarize: "<br/>else:<br/>    prefix = ""</span><span id="8f17" class="kr ju gw lk b ev ls lp l lq lr">max_input_length = 1024<br/>max_target_length = 128</span><span id="36e4" class="kr ju gw lk b ev ls lp l lq lr">def preprocess_function(examples):<br/>    inputs = [prefix + doc for doc in examples["document"]]<br/>    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)</span><span id="7794" class="kr ju gw lk b ev ls lp l lq lr"># Setup the tokenizer for targets<br/>    with tokenizer.as_target_tokenizer():<br/>        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True)</span><span id="7f32" class="kr ju gw lk b ev ls lp l lq lr">model_inputs["labels"] = labels["input_ids"]<br/>    return model_inputs</span><span id="9f92" class="kr ju gw lk b ev ls lp l lq lr">tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)</span></pre><p id="d49f" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">符号化数据集的形状</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="4e40" class="kr ju gw lk b ev lo lp l lq lr">DatasetDict({<br/>    train: Dataset({<br/>        features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],<br/>        num_rows: 204045<br/>    })</span></pre><p id="cc3b" class="pw-post-body-paragraph ht hu gw hv b hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq gp bi translated">“文档”、“摘要”、“id”来自原始数据集，表示原始文档、文档摘要、文档索引号。“标签”是标记化摘要的“输入标识”。</p><h2 id="a109" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">负载模型</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="557e" class="kr ju gw lk b ev lo lp l lq lr">from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer</span><span id="1194" class="kr ju gw lk b ev ls lp l lq lr">model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</span></pre><h2 id="736f" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">创建培训师</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="475d" class="kr ju gw lk b ev lo lp l lq lr">batch_size = 16<br/>model_name = model_checkpoint.split("/")[-1]<br/>args = Seq2SeqTrainingArguments(<br/>    f"{model_name}-finetuned-xsum",<br/>    evaluation_strategy = "epoch",<br/>    learning_rate=2e-5,<br/>    per_device_train_batch_size=batch_size,<br/>    per_device_eval_batch_size=batch_size,<br/>    weight_decay=0.01,<br/>    save_total_limit=3,<br/>    num_train_epochs=1,<br/>    predict_with_generate=True,<br/>    fp16=True<br/>)</span><span id="42a7" class="kr ju gw lk b ev ls lp l lq lr">data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)</span><span id="320e" class="kr ju gw lk b ev ls lp l lq lr">trainer = Seq2SeqTrainer(<br/>    model,<br/>    args,<br/>    train_dataset=tokenized_datasets["train"],<br/>    eval_dataset=tokenized_datasets["validation"],<br/>    data_collator=data_collator,<br/>    tokenizer=tokenizer<br/>)</span></pre><h2 id="455b" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">火车</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="8d18" class="kr ju gw lk b ev lo lp l lq lr">trainer.train()</span></pre><h2 id="4fb3" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">管道预测</h2><p id="3528" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">首先保存模型</p><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="6e3a" class="kr ju gw lk b ev lo lp l lq lr">trainer.save_model()</span></pre><h2 id="dc42" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">使用管道预测</h2><pre class="lf lg lh li ek lj lk ll lm aw ln bi"><span id="bc20" class="kr ju gw lk b ev lo lp l lq lr"># use bart in pytorch<br/>from transformers import pipeline<br/>summarizer = pipeline("summarization", model=f"{model_name}-finetuned-xsum",tokenizer=model_checkpoint)<br/>summarizer("Designing a proper application architecture is the difference between a project that flourishes and one that gets pigeonholed into a set of inextensible features. When it comes to an open source project like Ostia, it becomes especially important, as the ‘activation energy’ for a developer to get up to speed on a project is a huge barrier.", min_length=5, max_length=20)<br/>#[{'summary_text': 'Ostia is an open source project that flourishes and gets pigeonhole'}]</span></pre><h1 id="d549" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">工作流模式</h1><p id="186d" class="pw-post-body-paragraph ht hu gw hv b hw lt hy hz ia lu ic id ie lv ig ih ii lw ik il im lx io ip iq gp bi translated">对于这些任务，模式是</p><ol class=""><li id="443c" class="ly lz gw hv b hw hx ia ib ie ma ii mb im mc iq md me mf mg bi translated">加载原始数据集</li><li id="c1e6" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">加载标记器并标记数据</li><li id="7b8f" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">可选:分类任务时拆分数据</li><li id="3970" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">定义培训参数和培训师</li><li id="c01d" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">开始训练/微调</li><li id="b8f3" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">保存模型，然后加载带有管道和特定任务的模型</li><li id="2108" class="ly lz gw hv b hw mh ia mi ie mj ii mk im ml iq md me mf mg bi translated">使用管道预测</li></ol><h1 id="976d" class="jt ju gw bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">附录</h1><h2 id="de99" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">迁移学习</h2><div class="is it eg ei iu iv"><a href="https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751" rel="noopener follow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">从预先训练的模型中转移学习</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">如何快速简单地解决任何图像分类问题</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">towardsdatascience.com</p></div></div><div class="jj l"><div class="mm l jl jm jn jj jo jp iv"/></div></div></a></div><h2 id="66f5" class="kr ju gw bd jv ks kt ku jz kv kw kx kd ie ky kz kh ii la lb kl im lc ld kp le bi translated">管道</h2><div class="is it eg ei iu iv"><a href="https://www.analyticsvidhya.com/blog/2021/12/all-nlp-tasks-using-transformers-package/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">使用Transformers Pipeline-Analytics vid hya的所有NLP任务</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">1.理解变压器包2。文本分类3。问题回答4。文本摘要5。语言…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.analyticsvidhya.com</p></div></div><div class="jj l"><div class="mn l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">拥抱脸变压器管道功能|高级自然语言处理</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">这篇文章作为数据科学博客的一部分发表。目的这篇博客文章将学习如何使用…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">www.analyticsvidhya.com</p></div></div><div class="jj l"><div class="mo l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a rel="noopener follow" target="_blank" href="/geekculture/pipeline-object-in-transformers-using-hugging-face-6577f57a4c18"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">变形金刚中的管道对象🤗</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">上周我完成了一个免费的拥抱脸课程，在那里我学到了很多新东西，比如变形金刚…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">medium.com</p></div></div><div class="jj l"><div class="mp l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a href="https://huggingface.co/docs/transformers/glossary" rel="noopener  ugc nofollow" target="_blank"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">词汇表</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">自动编码模型:见MLM自回归模型:见CLM CLM:因果语言建模，一个预训练任务，其中…</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">huggingface.co</p></div></div><div class="jj l"><div class="mq l jl jm jn jj jo jp iv"/></div></div></a></div><div class="is it eg ei iu iv"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="iw ab fy"><div class="ix ab iy cl cj iz"><h2 class="bd gx ev z ja jb jc jd je jf jg gv bi translated">Mlearning.ai提交建议</h2><div class="jh l"><h3 class="bd b ev z ja jb jc jd je jf jg ft translated">如何成为Mlearning.ai上的作家</h3></div><div class="ji l"><p class="bd b fc z ja jb jc jd je jf jg ft translated">medium.com</p></div></div><div class="jj l"><div class="mr l jl jm jn jj jo jp iv"/></div></div></a></div></div></div>    
</body>
</html>