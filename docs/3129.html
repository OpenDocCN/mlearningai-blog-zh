<html>
<head>
<title>Variational Auto Encoder (VAE) for the Numerai Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数字数据集的变分自动编码器(VAE)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/variational-auto-encoders-vae-for-the-numerai-dataset-2709dcc7e449?source=collection_archive---------1-----------------------#2022-07-24">https://medium.com/mlearning-ai/variational-auto-encoders-vae-for-the-numerai-dataset-2709dcc7e449?source=collection_archive---------1-----------------------#2022-07-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e972" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数字数据集包含全球股票市场几十年的历史数据。在数据集上训练的机器学习模型学习预测股票回报，并根据数字锦标赛中的表现赚取加密货币(NMR)。这篇博文首先解释了“为什么”variable auto encoder是数字模型开发人员堆栈中的一个合适的工具。然后，我们讨论“什么”是一个变分自动编码器，并显示“如何”你可以训练一个。</p><h1 id="3bad" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">为什么要改变自动编码器？</h1><p id="ea58" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">我们可以使用VAEs进行异常检测、去噪和生成合成数据。</p><h2 id="e92f" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">异常检测</h2><p id="838f" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">异常检测是关于识别明显偏离大多数数据并且不符合正常行为的明确定义的样本。在数字数据集中，可能存在财务异常时期，检测这些时期可以提供信息。</p><h2 id="0a4c" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">去噪</h2><p id="8c29" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">降噪是从信号中去除噪声的过程。我们可以应用VAE去噪偏离大多数的特征。去噪转换噪声特征，而异常检测标记噪声样本。</p><h2 id="22c9" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">合成数据生成</h2><p id="0805" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">结合了合成数据和真实数据的训练模型已经显示出<a class="ae kt" href="https://www.forbes.com/sites/robtoews/2022/06/12/synthetic-data-is-about-to-transform-artificial-intelligence/?sh=118013c75238" rel="noopener ugc nofollow" target="_blank">有希望的结果</a>。使用VAE，我们可以从正态分布中采样，并将其传递给解码器以获得新的样本。</p><h1 id="6c17" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">什么是变分自动编码器？</h1><p id="64b1" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">自动编码器由两个主要部分组成:1)将输入映射为代码的编码器，以及2)使用代码重构输入的解码器。在文献中，代码也被称为表示变量或潜在变量。是什么让它变得不同？将潜在表示的分布强制为诸如高斯的已知分布。典型的AE无法控制潜在空间的分布。变分自动编码器(VAE)提供了一种<em class="ku">概率</em>方式来描述潜在空间中的观察。因此，我们不会构建一个输出单个值来描述每个潜在状态属性的编码器，而是将我们的编码器公式化来描述每个潜在属性的概率分布。在本教程中，我们使用在下面的论文中介绍的原始VAE，我们称之为香草VAE:</p><div class="kv kw ez fb kx ky"><a href="https://arxiv.org/abs/1312.6114" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">自动编码变分贝叶斯</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">我们如何在有向概率模型中进行有效的推理和学习，在连续的…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">arxiv.org</p></div></div><div class="lh l"><div class="li l lj lk ll lh lm ln ky"/></div></div></a></div><p id="fc40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用https://github.com/AntixK/PyTorch-VAE的<a class="ae kt" href="https://github.com/AntixK/PyTorch-VAE" rel="noopener ugc nofollow" target="_blank">作为我们的代码库。这个代码库包括各种VAE架构，但是我们将重点放在它的普通VAE上。</a></p><h2 id="7b98" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">体系结构</h2><p id="e91e" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">编码器由一个或多个完全连接的层组成，其中最后一层输出正态分布的平均值和方差。平均值和方差值用于从相应的正态分布中采样，作为解码器的输入。解码器由一个或多个完全连接的层组成，并输出编码器输入的重建版本。下图展示了VAE的建筑:</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/31d49c3a85b8b48e772ee1c00ac87686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2DZwupQZTnpBEH1s.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">The architecture of the VAE [<a class="ae kt" href="https://en.wikipedia.org/wiki/Variational_autoencoder#/media/File:Reparameterized_Variational_Autoencoder.png" rel="noopener ugc nofollow" target="_blank">source</a>]</figcaption></figure><p id="6d49" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">VAE的编码器模型将输出表征潜在空间中每个维度的分布的参数，而不是像在标准自动编码器中那样立即报告潜在状态的值。我们将输出两个向量，反映潜在状态分布的均值和方差，因为我们假设我们的先验具有正态分布。然后，我们的解码器模型将通过从这些定义的分布中采样来构建潜在向量，之后，它将重建原始输入。</p><h2 id="58f0" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">培养</h2><p id="58de" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">在普通VAE的损失函数中有两项:1)重建误差和2) KL散度:</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es md"><img src="../Images/abfb938fcc633b5d67c8f12f8cd628c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gOyPC6yuQkBIBc4_BP6mQQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">The loss function for training a vanilla variational autoencoder</figcaption></figure><p id="3cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">传统VAE中使用的重建误差是均方误差(MSE)。MSE损失试图使重构信号类似于输入信号。KL散度损失试图使代码的分布接近正态分布。<code class="du me mf mg mh b">q(z|x)</code>是代码给定输入信号的分布，<code class="du me mf mg mh b">p(z)</code>是正态分布。PyTorch代码如下所示:</p><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="126c" class="kf jd hh mh b fi mm mn l mo mp">recons_loss = F.mse_loss(recons, input)</span><span id="39e9" class="kf jd hh mh b fi mq mn l mo mp">kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)</span></pre><p id="ba6e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我从主<a class="ae kt" href="https://github.com/AntixK/PyTorch-VAE" rel="noopener ugc nofollow" target="_blank"> Pytorch-VAE的</a>分支为数字数据集创建了一个分支:</p><div class="kv kw ez fb kx ky"><a href="https://github.com/erfaneshrati/PyTorch-VAE" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">GitHub-erfaneshrati/py torch-VAE:py torch中可变自动编码器(VAE)的集合。</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">更新04/14/2022:这个分叉是为支持Numerai的数据集而写的，它是一个可变自动编码器的集合…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">github.com</p></div></div><div class="lh l"><div class="mr l lj lk ll lh lm ln ky"/></div></div></a></div><p id="d74c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">普通的VAE配置文件如下所示:</p><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="9b7b" class="kf jd hh mh b fi mm mn l mo mp">model_params:<br/>  name: 'NumeraiHistogram of KL divergence (left) and mean-squared reconstruction lossVAE'<br/>  in_channels: 1191<br/>  latent_dim: 32<br/><br/>data_params:  <br/>  data_path: "/train.parquet"<br/>  train_batch_size: 4096  <br/>  val_batch_size:  4096  <br/>  num_workers: 8<br/><br/>exp_params:  <br/>  LR: 0.005  <br/>  weight_decay: 0.0  <br/>  scheduler_gamma: 0.95  <br/>  kld_weight: 0.00025  <br/>  manual_seed: 1265</span><span id="e59a" class="kf jd hh mh b fi mq mn l mo mp">trainer_params:  <br/>  gpus: [1]  <br/>  max_epochs: 300</span><span id="58ff" class="kf jd hh mh b fi mq mn l mo mp">logging_params:  <br/>  save_dir: "logs/"  <br/>  name: "NumeraiVAE"</span></pre><p id="ddad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">配置中的关键参数是<code class="du me mf mg mh b">in_channels</code>:输入特征的数量，<code class="du me mf mg mh b">latent_dim</code>:VAE的潜在尺寸。编码器/解码器包括线性层，随后是批量标准化和泄漏ReLU激活。</p><p id="da88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编码器的型号定义:</p><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="ba96" class="kf jd hh mh b fi mm mn l mo mp"># Build Encoder<br/>modules = []        <br/>modules.append(            <br/>  nn.Sequential(                 <br/>    nn.Linear(in_channels, latent_dim),<br/>    nn.BatchNorm1d(latent_dim),<br/>    nn.LeakyReLU(),        <br/>  ))<br/>        <br/>self.encoder = nn.Sequential(*modules)        <br/>self.fc_mu = nn.Linear(latent_dim, latent_dim)        <br/>self.fc_var = nn.Linear(latent_dim, latent_dim)</span></pre><p id="a9be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解码器的模型定义:</p><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="c51f" class="kf jd hh mh b fi mm mn l mo mp"># Build Decoder        <br/>modules = []<br/>        <br/>self.decoder_input = nn.Linear(latent_dim, latent_dim)<br/>        <br/>modules.append(            <br/>  nn.Sequential(                 <br/>    nn.Linear(latent_dim, in_channels),<br/>    nn.BatchNorm1d(in_channels),<br/>    nn.LeakyReLU()        <br/>))<br/>        <br/>self.decoder = nn.Sequential(*modules)</span></pre><h2 id="9e3e" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">培训怎么跑？</h2><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="f1ec" class="kf jd hh mh b fi mm mn l mo mp">python3 run.py --config configs/numerai_vae.yaml</span></pre><p id="0819" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它应该打印以下日志:</p><pre class="lp lq lr ls fd mi mh mj mk aw ml bi"><span id="b1fb" class="kf jd hh mh b fi mm mn l mo mp">GPU available: True, used: True<br/>TPU available: False, using: 0 TPU cores<br/>IPU available: False, using: 0 IPUs<br/>======= Training NumeraiVAE =======<br/>Global seed set to 1265<br/>initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1<br/>----------------------------------------------------------------------------------------------------<br/>distributed_backend=nccl<br/>All distributed processes registered. Starting with 1 processes<br/>----------------------------------------------------------------------------------------------------<br/>LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]</span><span id="b85c" class="kf jd hh mh b fi mq mn l mo mp">| Name  | Type       | Params<br/>-------------------------------------<br/>0 | model | NumeraiVAE | 83.1 K<br/>-------------------------------------<br/>83.1 K    Trainable params<br/>0         Non-trainable params<br/>83.1 K    Total params<br/>0.332     Total estimated model params size (MB)<br/>Global seed set to 1265                                                                                                                          <br/>Epoch 19: 100%|██████████████████████████████████████████████████████████████████████████| 592/592 [00:20&lt;00:00, 28.49it/s, loss=0.0818, v_num=3]</span></pre><h2 id="e92c" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">如何使用VAE进行异常检测？</h2><p id="bdb8" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">异常是具有高损耗值的样本。损失值可以是重建损失、KLD损失或它们的组合。</p><div class="lp lq lr ls fd ab cb"><figure class="ms lt mt mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><img src="../Images/8bf2c06a4b3edb089c5433706e51093c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xTCYDDj6bcgKbFZwz7RvzA.png"/></div></figure><figure class="ms lt my mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><img src="../Images/7614783ca8d9bb5ad3fa426f4e4d0d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*bJfCzlJhjofqREs9zxvhlw.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx mz di na nb">Histogram of KL divergence (left) and mean-squared reconstruction loss (right) on the Numerai’s training dataset.</figcaption></figure></div><div class="ab cb"><figure class="ms lt nc mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><img src="../Images/73294fd7e689a0caea4bf67220368e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fzapcWBii7HvYK7igJK0Mw.png"/></div></figure><figure class="ms lt nc mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><img src="../Images/e8c7609a154a31ff1aeeffbd76e3cf77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6RvdJvXjVbNB8i7QAMLMMQ.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx mz di na nb">Visualization of the KL-divergence and mean-squared error of the Numerai’s training dataset. The latent dimension of the trained VAE of this figure is two so that we can visualize it.</figcaption></figure></div><h2 id="b05b" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">如何用VAE去噪？</h2><p id="4b96" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">VAE被训练来重建其输入。有噪声的输入首先被传递到编码器以获得代码。然后，代码被传递到解码器，以获得去噪声的输入。</p><h2 id="d12a" class="kf jd hh bd je kg kh ki ji kj kk kl jm ip km kn jq it ko kp ju ix kq kr jy ks bi translated">如何用VAE生成合成数据？</h2><p id="1560" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">由于解码器的输入遵循已知的分布(即高斯分布)，我们可以从高斯分布进行采样，并将值传递给解码器，以获得新的合成数据。</p><h1 id="84d4" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">进一步阅读</h1><p id="250f" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">[1] <a class="ae kt" href="https://www.jeremyjordan.me/variational-autoencoders/" rel="noopener ugc nofollow" target="_blank">变型自动编码器</a></p><p id="1abc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢你阅读这篇博文。欢迎分享您对在数字锦标赛中使用VAEs的意见！</p><div class="kv kw ez fb kx ky"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">medium.com</p></div></div><div class="lh l"><div class="nd l lj lk ll lh lm ln ky"/></div></div></a></div></div></div>    
</body>
</html>