<html>
<head>
<title>Improving Linear Regression — The Why and the How</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进线性回归——原因和方法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/improving-linear-regression-the-why-and-the-how-c4c9ea189384?source=collection_archive---------6-----------------------#2022-11-29">https://medium.com/mlearning-ai/improving-linear-regression-the-why-and-the-how-c4c9ea189384?source=collection_archive---------6-----------------------#2022-11-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0d75" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">线性回归是用于回归问题的线性模型。在精度上早就被其他车型超越了。然而，这是非常有用的基线，给你快速的结果，并且在做的时候是可以解释的。此外，它是一个完美的“第一个模型”,让人们成功地向门外汉描述机器学习的要点。因此，本文从使用python中的scikit-learn API实现简单的线性回归模型开始，并进一步介绍使我们的实现更加精彩的技术。这些技术可以应用于广泛的模型，它们背后的思想是机器学习的核心。</p><h2 id="6f26" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">但是什么是线性模型呢？</h2><p id="f87c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">简单地说，线性模型试图在你的数据中找到<em class="kc">线性关系</em>。如果目标值是这些特征的线性组合，这是最理想的。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/87ac14c407ef839d9883944b7e76ad57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*F8lGp1L4Q__yQCEHaMnM7w.png"/></div></figure><h1 id="83ca" class="kl jd hh bd je km kn ko ji kp kq kr jm ks kt ku jp kv kw kx js ky kz la jv lb bi translated">从sklearn导入线性模型</h1><h2 id="734d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">线性_模型。线性回归</h2><p id="dbd9" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">当我们的数据的目标值是连续的，即实值时，使用回归。对于线性回归，我们想要建立模型f(w，b，x ),作为示例x的特征的线性组合，如下:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lc"><img src="../Images/9eff7961a90df3416cd392e853eae810.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/1*ktySU1AwwpMqQc7pqx-liQ.gif"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Linear Regression</figcaption></figure><p id="ff5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的目标是找到W和b，以最小化目标函数，在这种情况下，是均方误差。因此，该算法的目标是最小化:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lh"><img src="../Images/c1bcd11af63dcf1bb37b921fc83a594a.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/1*2RB5fjMlkrZz0X-7vdCzBA.gif"/></div><figcaption class="ld le et er es lf lg bd b be z dx">The objective function for Linear Regression</figcaption></figure><p id="56c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中N是数据样本的数量。</p><h2 id="6b50" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">数据集</h2><p id="8480" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们使用糖尿病数据，这是sklearn API本身的一部分。就我们的意图而言，将它视为玩具或模拟数据，我们使用它只是因为这样做简单方便。</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="9d5b" class="ln jd hh lj b be lo lp l lq lr">import matplotlib.pyplot as plt<br/>import numpy as np<br/>from sklearn import datasets<br/><br/>#importing the datasetfor our use<br/><br/>diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)<br/>plt.scatter(diabetes_X[:, np.newaxis, 2], diabetes_y)<br/>plt.xticks(())<br/>plt.yticks(())<br/><br/>plt.show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ls"><img src="../Images/e09fea5897507ab93112a30751a767da.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*-C4ZzqQadfg8MdEy_0yzmA.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Image of the diabetes data-points</figcaption></figure><p id="0079" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们看看如何对这些数据进行线性回归。使用sklearn，我们可以用三行简单的代码来完成。</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="640a" class="ln jd hh lj b be lo lp l lq lr">from sklearn import linear_model<br/><br/>regr = linear_model.LinearRegression() #Create a Linear Regression model<br/>diabetes_X = diabetes_X[:, np.newaxis, 2] #Reformat the data for the model<br/><br/>regr.fit(diabetes_X, diabetes_y) #train the model on the data</span></pre><p id="e9f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们现在可以使用我们的模型对新数据进行预测，并看看它能做得有多好。我们也可以挖一点来找到系数(W)和截距(b)</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="6269" class="ln jd hh lj b be lo lp l lq lr">W = regr.coef_ #Returns the weight W<br/>b = regr.intercept_ #Return the intercept b<br/>X = diabetes_X<br/>print('Weight Value: ',W)<br/>print('Intercept Value: ',b)<br/>plt.scatter(X[:50], diabetes_y[:50], color="black")<br/>plt.plot(X, W * X + b)</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lt"><img src="../Images/4b393a52e0e08a6900c9d4bcf2508c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*yDS6kFYGBhfUpQCG9lydkA.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">The plot of our model against a sample of the data we used</figcaption></figure><p id="8b0c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是线性回归。存在许多基于谦逊回归模型的变体。我们会看看其中的一些，但首先，</p><h2 id="4fec" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">正规化</h2><p id="ae27" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">正则化方法是迫使学习算法创建不太复杂的模型的方法，换句话说，它们防止过拟合。许多模型只是将正则化添加到LinearRegression()模型的<em class="kc">目标函数</em>中，因此理解这个概念是有用的。</p><p id="0c0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有三种模型将正则化添加到线性回归的目标函数中。</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="e7a5" class="ln jd hh lj b be lo lp l lq lr"><br/>+------------------+----------------------------------+-----------------------+<br/>| Regularization   |              Command             | Type of regularization|<br/>+------------------+----------------------------------+-----------------------+<br/>| Ridge Regression | sklearn.linear_model.Ridge()     | l2                    |<br/>| Lasso Regression | sklearn.linear_model.Lasso()     | l1                    |<br/>| ElasticNet       | sklearn.linear_model.ElasticNet()| l1+l2                 |<br/>+------------------+----------------------------------+-----------------------+</span></pre><p id="5372" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正规化有两种类型。回想一下，线性回归的目标函数是:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lh"><img src="../Images/c1bcd11af63dcf1bb37b921fc83a594a.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/1*2RB5fjMlkrZz0X-7vdCzBA.gif"/></div></figure><p id="e586" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们取一个超参数‘C’，将其乘以权重矩阵W，并将其添加到目标函数中，以将等式转换为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lu"><img src="../Images/a003a45ba67f3f9d7d6c19677547cb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/1*xfwG1BGcXJv7f9IH2lNf9w.gif"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Objective function of l1 Linear Regression</figcaption></figure><p id="d50b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这被称为l1正则化。另一方面，如果我们将权重矩阵的平方乘以“C ”,则为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lv"><img src="../Images/abc31a4cb58d9a4172812d5031d8122b.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/1*01bDzYxgu1_6d9HZBJQxVQ.gif"/></div><figcaption class="ld le et er es lf lg bd b be z dx">Objective function of l2 Linear Regression</figcaption></figure><p id="35e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是l2正则化。它们之间的主要区别是l2正则化的目标函数仅<em class="kc">趋向于</em>0，而使用l1正则化我们实际上可以达到0。在实践中，如果您怀疑模型过度拟合，可以从使用岭l2正则化开始。</p><h2 id="e055" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">交叉验证和超参数调整</h2><p id="d0a9" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">我们在上一节中将“C”称为超参数。这意味着，它是一个数值，我们作为程序员输入到模型中。(不像算法为我们学习的W和b)。每个模型都有自己的超参数集。程序员<strong class="ig hi">实验性地调整</strong>超参数。通常，这意味着大量的反复试验。幸运的是，sklearn给了我们一些功能来自动完成这个过程。请注意，手动调整超参数一段时间可能有助于数据科学家更好地理解数据，这通常被认为是最佳实践。</p><p id="cd41" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设您希望使用以下C值来尝试岭回归:[0.001，0.01，0.1，1，10，100]，并希望以最高精度保存模型。您可以通过使用以下方法来实现:</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="cd64" class="ln jd hh lj b be lo lp l lq lr">regr = linear_model.RidgeCV(alphas = [0.001,0.01,0.1,1,10,100],cv = 3)<br/>regr.fit(diabetes_X, diabetes_y)</span></pre><p id="e0d8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Scikit-learn使用变量alpha代替我们一直使用的C。但是“cv = 3”是什么意思呢？这意味着数据集分为三部分。两部分用于训练(称为训练集)，一部分用于测试超参数的效率(称为验证集)</p><p id="10bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">更常见的情况是，您不会只有一个超参数需要优化。这就是网格搜索的用武之地。例如，elasticNet回归函数采用两个超参数，l1_score和alpha。如果我们想要测试l1分数的[0.1，0.2，0.3]和l2分数的[1，2，3]，也就是我们的alpha值，我们可以使用linear_model。ElasticNetCV()，它将为您创建超参数的组合。总共9个模型(3*3)将以这种方式训练、评估，最好的返回。</p><p id="7968" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们在实践中展示这一点，使用与之前相同的数据集:</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="51dc" class="ln jd hh lj b be lo lp l lq lr">diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)<br/>diabetes_X = diabetes_X[:, np.newaxis, 2]<br/>#We import the dataset as usual<br/><br/>diabetes_X_test = diabetes_X[-20:] #We will test how good our model is<br/>diabetes_y_test = diabetes_y[-20:]<br/><br/>diabetes_X = diabetes_X[:-20]<br/>diabetes_y = diabetes_y[:-20]<br/><br/>regr = linear_model.ElasticNetCV(<br/>    l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], <br/>    #range of values for l1<br/>    fit_intercept=[True,False], <br/>    #do we want the intercept or not?<br/>    alphas=[ 0.0125, 0.025, 0.05, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0],<br/>    #range of values for l2<br/>    cv = 3<br/>                  )<br/>regr.fit(diabetes_X,diabetes_y)</span></pre><p id="326b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们编写一个简单的助手函数来测试我们的模型，并根据数据绘制它。</p><pre class="ke kf kg kh fd li lj lk bn ll lm bi"><span id="ab23" class="ln jd hh lj b be lo lp l lq lr">from sklearn.metrics import mean_squared_error, r2_score<br/><br/>def plot_regr(model,X,Y):<br/>  W = model.coef_<br/>  b = model.intercept_<br/>  Y_pred = model.predict(diabetes_X_test)<br/>  print(f"Mean squared error: {mean_squared_error(Y, Y_pred)}")<br/>  print(f"R2 error: {r2_score(Y, Y_pred)}")<br/>  plt.scatter(X, Y, color="black")<br/>  plt.plot(X, W * X + b)<br/><br/>plot_regr(regr,diabetes_X_test,diabetes_y_test)</span></pre><pre class="lw li lj lk bn ll lm bi"><span id="9ba4" class="ln jd hh lj b be lo lp l lq lr">Mean squared error: 2554.4111933368745<br/>R2 error: 0.47126338325849815</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lt"><img src="../Images/d2022c8a3464681794dd67ddf9c9f141.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*QyYZuBHb5KSLwnnkinWCrg.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx">ElasticNet implimentation</figcaption></figure><p id="2c80" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自然，这种模式是…可怕的。请记住，正则化有助于降低模型的复杂性，而线性回归(ElasticNet的数学基础)已经非常简单了。话说回来，线性回归本身并没有那么好。然而，到目前为止，我们知道它是快速的，并且作为基线模型是完美的。</p><p id="fa9a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="kc">希望你觉得这有用。一定要关注我，跟上我的内容！感谢反馈！</em> </strong></p><div class="lx ly ez fb lz ma"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hi fi z dy mf ea eb mg ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo kj ma"/></div></div></a></div></div></div>    
</body>
</html>