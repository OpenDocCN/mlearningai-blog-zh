<html>
<head>
<title>OpenAi Request For Research XOR Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAi请求研究异或问题</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/openai-request-for-research-67d8dc58e75a?source=collection_archive---------6-----------------------#2021-12-26">https://medium.com/mlearning-ai/openai-request-for-research-67d8dc58e75a?source=collection_archive---------6-----------------------#2021-12-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a07f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">OpenAi发布了一些他们想要解决的研究问题，作为一个机器学习的狂热学习者，我决定解决他们的热身问题。你可以在这里查看他们的其他请求<a class="ae jc" href="https://openai.com/blog/requests-for-research-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/requests-for-research-2/</a></p><p id="c04f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">问题是训练一个LSTM来解决<code class="du jd je jf jg b">XOR</code>问题:也就是说，给定一个比特序列，确定它的奇偶性。<a class="ae jc" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> LSTM </a>应该消耗序列，一次一位，然后在序列结束时输出正确的答案。测试以下两种方法:</p><ul class=""><li id="5c2c" class="jh ji hh ig b ih ii il im ip jj it jk ix jl jb jm jn jo jp bi translated">生成长度为50的随机100，000个二进制字符串的数据集。训练LSTM；你得到了什么表现？</li><li id="68bf" class="jh ji hh ig b ih jq il jr ip js it jt ix ju jb jm jn jo jp bi translated">生成随机100，000个二进制字符串的数据集，其中每个字符串的长度在1到50之间独立随机选择。训练LSTM。它成功了吗？如何解释这种差异？</li></ul><p id="7ef4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">异或问题:</p><p id="a7e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">什么是异或问题？简单地说，这是一个二进制分类问题，其中输入数据由二进制数的向量组成，目标由0或1组成。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/faf55d8026a9aca853980bb5af311e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JnwlESZow5mQ1FmKrITwDw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx">Image adapted from <a class="ae jc" href="http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf" rel="noopener ugc nofollow" target="_blank">Kevin Swingler</a>.</figcaption></figure><h1 id="d5b1" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">问题1: </strong></h1><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="1b78" class="ln km hh jg b fi lo lp l lq lr">import numpy as np</span><span id="f3d8" class="ln km hh jg b fi ls lp l lq lr">def gen_len50(size=100000):</span><span id="a23d" class="ln km hh jg b fi ls lp l lq lr">X_data_1 = []</span><span id="5d95" class="ln km hh jg b fi ls lp l lq lr">Y_data_1 = []</span><span id="9972" class="ln km hh jg b fi ls lp l lq lr">for i in range(size):</span><span id="d33a" class="ln km hh jg b fi ls lp l lq lr">length = random.randint(1, 50)</span><span id="cc7d" class="ln km hh jg b fi ls lp l lq lr">data = np.random.randint(2, size=(1, length)).astype("float32")</span><span id="33fb" class="ln km hh jg b fi ls lp l lq lr">data = pad_sequences(data, maxlen=50, dtype='float32',       padding='pre')</span><span id="2c87" class="ln km hh jg b fi ls lp l lq lr">X_data_1.append(data)</span><span id="a9e5" class="ln km hh jg b fi ls lp l lq lr">labels = [0 if np.sum(X_data_1[i])%2 == 0 else 1]</span><span id="851d" class="ln km hh jg b fi ls lp l lq lr">Y_data_1.append(labels)</span><span id="db6c" class="ln km hh jg b fi ls lp l lq lr">return X_data_1, Y_data_1</span></pre><p id="d2ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了生成数据集，我们导入numpy，遍历for循环100000次，生成一个长度为100000的列表，每个列表由一个长度为50的二进制字符串组成。每个数据元素都有一个相应的标签，或者是0，或者是1。</p><p id="ff40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们使用Keras定义了一个非常简单的LSTM网络，Keras是一个非常流行且易于使用的机器学习库。</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="a5f2" class="ln km hh jg b fi lo lp l lq lr">model = Sequential([</span><span id="921e" class="ln km hh jg b fi ls lp l lq lr">LSTM(32, return_sequences=True, activation='sigmoid', input_shape=(1, 50)),</span><span id="2e86" class="ln km hh jg b fi ls lp l lq lr">LSTM(64, return_sequences=True, activation='sigmoid'),</span><span id="5f70" class="ln km hh jg b fi ls lp l lq lr">Dense(1, activation='sigmoid')</span><span id="373e" class="ln km hh jg b fi ls lp l lq lr">])</span></pre><p id="7aae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将数据集分为训练和测试数据集，以20%的分割大小在训练后评估模型</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="9e11" class="ln km hh jg b fi lo lp l lq lr">split_size = 100000 * 0.20</span><span id="1446" class="ln km hh jg b fi ls lp l lq lr">X_train = X_data1[split_size:]</span><span id="ece2" class="ln km hh jg b fi ls lp l lq lr">X_test = X_data1[:split_size]</span><span id="c1fe" class="ln km hh jg b fi ls lp l lq lr">Y_train = Y_data1[split_size:]</span><span id="4b46" class="ln km hh jg b fi ls lp l lq lr">Y_test = Y_data1[:split_size]</span></pre><p id="29fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用Adam作为优化器，使用二进制交叉熵作为由于问题是二进制分类问题引起的损失，并跟踪准确性，来编译模型。</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="3492" class="ln km hh jg b fi lo lp l lq lr">model.compile('adam', loss='binary_crossentropy', metrics=['acc'])</span></pre><p id="385a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将列表转换成numpy数组，并对数据进行规范化以优化训练。</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="f350" class="ln km hh jg b fi lo lp l lq lr">X_train = np.asarray(X_train)</span><span id="d30f" class="ln km hh jg b fi ls lp l lq lr">norm = np.linalg.norm(X_train)<br/>normal_array = X_train/norm<br/>print(normal_array)</span></pre><p id="5d97" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们对所有的培训和测试列表都这样做</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="3157" class="ln km hh jg b fi lo lp l lq lr">history = model.fit(X_train, Y_train, epochs=50, batch_size=32, shuffle=True)</span></pre><p id="00af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后，我们根据50个时期的标准化X和y值训练该模型。远不足以优化LSTM模型，该模型因训练时间过长而臭名昭著，但足以在问题1和2之间得出结论。</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="cb9f" class="ln km hh jg b fi lo lp l lq lr">def plot_model(history):</span><span id="5b39" class="ln km hh jg b fi ls lp l lq lr">''' Plot model accuracy and loss</span><span id="1bd8" class="ln km hh jg b fi ls lp l lq lr">Args:</span><span id="a840" class="ln km hh jg b fi ls lp l lq lr">history: Keras dictionary contatining training/validation loss/acc</span><span id="1358" class="ln km hh jg b fi ls lp l lq lr">Returns:</span><span id="63ab" class="ln km hh jg b fi ls lp l lq lr">Plots model's training/validation loss and accuracy history</span><span id="3b2f" class="ln km hh jg b fi ls lp l lq lr">'''</span><span id="a085" class="ln km hh jg b fi ls lp l lq lr">loss = history.history['loss']</span><span id="a39d" class="ln km hh jg b fi ls lp l lq lr">epochs = range(1, len(loss) + 1)<br/></span><span id="1ca5" class="ln km hh jg b fi ls lp l lq lr">plt.figure()</span><span id="bae5" class="ln km hh jg b fi ls lp l lq lr">plt.plot(epochs, loss, 'b', label='Training loss')</span><span id="6230" class="ln km hh jg b fi ls lp l lq lr">plt.title('Training  loss')</span><span id="9ef5" class="ln km hh jg b fi ls lp l lq lr">plt.xlabel('Epochs')</span><span id="696d" class="ln km hh jg b fi ls lp l lq lr">plt.ylabel('Loss')</span><span id="ffd0" class="ln km hh jg b fi ls lp l lq lr">plt.legend()</span><span id="e498" class="ln km hh jg b fi ls lp l lq lr">plt.figure()</span><span id="7549" class="ln km hh jg b fi ls lp l lq lr">acc = history.history['acc']</span><span id="edfc" class="ln km hh jg b fi ls lp l lq lr">plt.plot(epochs, acc, 'bo', label='Training acc')</span><span id="5577" class="ln km hh jg b fi ls lp l lq lr">plt.title('Training accuracy')</span><span id="3c35" class="ln km hh jg b fi ls lp l lq lr">plt.xlabel('Epochs')</span><span id="a482" class="ln km hh jg b fi ls lp l lq lr">plt.ylabel('Loss')</span><span id="86d3" class="ln km hh jg b fi ls lp l lq lr">plt.legend()</span><span id="f674" class="ln km hh jg b fi ls lp l lq lr">plt.show()</span><span id="ccbb" class="ln km hh jg b fi ls lp l lq lr">return</span></pre><p id="de99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们可以画出准确度和损失，并从中得出结论</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es lt"><img src="../Images/c0bc8be7159f9b4fccff1d53e05d485d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHnyGXydrgdbuboUvOYJBg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx">Author’s Image of Loss and Accuracy for 1st model</figcaption></figure><p id="4229" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">准确度显示出持续上升的趋势，但是到最后是缓慢的曲线，导致准确度停滞不前。用测试数据评估该模型导致51%的准确度。不算很好，但也不算太差，因为最高准确率是56%。</p><h1 id="f1ee" class="kl km hh bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">问题二:</strong></h1><p id="40ba" class="pw-post-body-paragraph ie if hh ig b ih lu ij ik il lv in io ip lw ir is it lx iv iw ix ly iz ja jb ha bi translated">问题2说使用相同的技术，但是将二进制字符串的长度改为从1到50的随机长度。我们通过简单地添加额外的一行来生成一个从1到50的随机整数，并使用它作为长度而不是常数50来促进这种变化。我们还在前面添加填充来将所有长度转换为50</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="c37f" class="ln km hh jg b fi lo lp l lq lr">def gen_data(size=100000):</span><span id="97ad" class="ln km hh jg b fi ls lp l lq lr">X_data_1 = []</span><span id="40c8" class="ln km hh jg b fi ls lp l lq lr">Y_data_1 = []</span><span id="02ac" class="ln km hh jg b fi ls lp l lq lr">for i in range(size):</span><span id="c4ad" class="ln km hh jg b fi ls lp l lq lr">length = random.randint(1, 50)</span><span id="dbd8" class="ln km hh jg b fi ls lp l lq lr">data = np.random.randint(2, size=(1, length)).astype("float32")</span><span id="4249" class="ln km hh jg b fi ls lp l lq lr">data = pad_sequences(data, maxlen=50, dtype='float32', padding='pre')</span><span id="cbb0" class="ln km hh jg b fi ls lp l lq lr">X_data_1.append(data)</span><span id="7b38" class="ln km hh jg b fi ls lp l lq lr">labels = [0 if np.sum(X_data_1[i])%2 == 0 else 1]</span><span id="5a18" class="ln km hh jg b fi ls lp l lq lr">Y_data_1.append(labels)</span><span id="8423" class="ln km hh jg b fi ls lp l lq lr">return X_data_1, Y_data_1</span></pre><p id="ddc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了便于比较，我们使用了之前使用的相同的LSTM模型。</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="0afe" class="ln km hh jg b fi lo lp l lq lr">model = Sequential([</span><span id="eb84" class="ln km hh jg b fi ls lp l lq lr">LSTM(32, return_sequences=True, activation='sigmoid', input_shape=(1, 50)),</span><span id="2d41" class="ln km hh jg b fi ls lp l lq lr">LSTM(64, return_sequences=True, activation='sigmoid'),</span><span id="989f" class="ln km hh jg b fi ls lp l lq lr">Dense(1, activation='sigmoid')</span><span id="c59e" class="ln km hh jg b fi ls lp l lq lr">])</span></pre><p id="b577" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们用同样的技术分割数据</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="9786" class="ln km hh jg b fi lo lp l lq lr">split_size = 100000 * 0.20</span><span id="40c6" class="ln km hh jg b fi ls lp l lq lr">X_train = X_data1[split_size:]</span><span id="d4a6" class="ln km hh jg b fi ls lp l lq lr">X_test = X_data1[:split_size]</span><span id="d681" class="ln km hh jg b fi ls lp l lq lr">Y_train = Y_data1[split_size:]</span><span id="926e" class="ln km hh jg b fi ls lp l lq lr">Y_test = Y_data1[:split_size]</span></pre><p id="3746" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并编译模型</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="6dc1" class="ln km hh jg b fi lo lp l lq lr">model.compile('adam', loss='binary_crossentropy', metrics=['acc'])</span></pre><p id="d24e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并对数据进行规范化，将其转换为numpy数组</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="3fb8" class="ln km hh jg b fi lo lp l lq lr">X_train = np.asarray(X_train)</span><span id="4d32" class="ln km hh jg b fi ls lp l lq lr">norm = np.linalg.norm(X_train)<br/>normal_array = X_train/norm<br/>print(normal_array)</span></pre><p id="ce6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然后我们训练这个模型</p><pre class="jw jx jy jz fd lj jg lk ll aw lm bi"><span id="b531" class="ln km hh jg b fi lo lp l lq lr">history = model.fit(X_train, Y_train, epochs=50, batch_size=32, shuffle=True)</span></pre><p id="9c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们再次绘制和分析模型的性能</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es lz"><img src="../Images/5a2b21d09503bbab96146064414437fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9h8S0LbkA2Tqd_Z1njKLaA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx">Author’s Image For Loss And Accuracy of 2nd model</figcaption></figure><p id="afad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这一次，该模型达到了近60%的准确率，并且在评估时表现出了57%的准确率，这太棒了！</p><p id="d2cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，这是一种非常不切实际的解决问题的方法，使用密集神经网络甚至简单的递归神经网络，而不是更复杂的LSTM，就可以很容易地解决这个问题，我这样解决的唯一原因是因为这是问题中特别要求的。</p><p id="1477" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">项目代码:<a class="ae jc" href="https://github.com/arnavdantuluri/XOROpenAi.git" rel="noopener ugc nofollow" target="_blank">https://github.com/arnavdantuluri/XOROpenAi.git</a></p><div class="ma mb ez fb mc md"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hi fi z dy mi ea eb mj ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">medium.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr kf md"/></div></div></a></div></div></div>    
</body>
</html>