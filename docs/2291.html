<html>
<head>
<title>Classification Algorithms-3: Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类算法-3:决策树</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/classification-algorithms-3-decision-tree-801d59011780?source=collection_archive---------9-----------------------#2022-04-08">https://medium.com/mlearning-ai/classification-algorithms-3-decision-tree-801d59011780?source=collection_archive---------9-----------------------#2022-04-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="64b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">萨提亚·克里希南·苏雷什</p><p id="b67f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树分类器是目前最流行的分类算法之一。它功能多样、直观且强大。决策树分类器最大的一个优点就是容易理解。也就是说，让我们来看看本文中决策树算法的工作原理(随文章附带的笔记本可以在这里找到<a class="ae jc" href="https://github.com/SathyaKrishnan1211/Low-key-ML/blob/master/Notebooks/Decision_Tree.ipynb" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="12ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树的工作基于它从输入数据中开发或识别的决策。让我们通过一个例子来理解这一点，在这个例子中，我们使用DT对虹膜数据集进行分类(仅花瓣宽度和花瓣长度)。看一下下图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/a95bea165d6be62759bed62b7200bb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Ah8H0GAocxWm9KOL_87acA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">DT with max_depth=2</figcaption></figure><p id="84f3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上图中，DT首先根据花瓣长度应小于等于2.45的条件对样本进行分类。如果输入向量满足该条件，则它可能属于“setosa”类。我们可以通过查看下面的数据集来确认这一点，我们可以清楚地看到花瓣长度小于2.45的样本属于0类(setosa)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/82f05a69eb079cb41276437575704b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*vZZrqwtJgZ094_nqyeBK5g.png"/></div></figure><p id="d0d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦它将setosa完全分类，它就会继续对其他两个类进行分类，但这次是基于“花瓣宽度”特征。如果没有给定max_depth值，则2 DT将寻找更多的决策边界，直到它可以将单独的组放入单独的叶子中，或者用DT术语来说，直到gini变为0。</p><p id="78ae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么什么是基尼呢？</p><p id="122e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">基尼是节点/叶的杂质的度量。它是决策树形成决策的基础。DT首先从一个特征中随机选取一个值，这个值也是随机选取的。拾取的值将用于将数据集分成两片，一片叶子包含位于所选值下方的数据点，另一片叶子包含位于所选值上方的数据点。然后使用下面的公式计算叶子的基尼杂质</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/1ed9ab95b1be1e12e3ee45413f1dd167.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*UdbLhUAYkwJVRObjyUyLMg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Gini impurity</figcaption></figure><p id="9fe4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">上面公式中的p’是属于该叶的数据点的概率。一旦为叶子计算了基尼值，基尼杂质的加权平均值就成为节点的基尼值。对其他随机选择的值重复上述过程，并选择基尼值最小的值。如果没有提到“max_depth”参数，则重复这个过程，直到所有的叶子只包含一个类，或者如果不能找到新的决策点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jr"><img src="../Images/6897b201fce6a6e96ab6b0ec4381efdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wQBH0WC94ht9bYgejKRnw.png"/></div></div></figure><p id="f314" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DT的最大问题之一是，它所形成的判定边界总是与其中一个轴正交，并且它不会形成与该轴成90度以外的角度的判定边界。对于可以用简单的有角度的线完成的分割，DT采用垂直于轴的三或四条线。</p><p id="7638" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">DT的另一个问题是，如果不加检查，它很容易使数据过拟合。DT有许多正则化参数，我们将研究其中的3个。</p><p id="2541" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">正规化:</strong> <br/> 1。最大深度<br/> 2。最小_样本_分割<br/> 3。最小样本叶</p><ol class=""><li id="7eea" class="jw jx hh ig b ih ii il im ip jy it jz ix ka jb kb kc kd ke bi translated">max_depth: <br/>该参数指定DT的深度。在scikit-learn中，它的缺省值是None。如果你训练一个具有max_depth的DT为None，那么模型将建立一个树，直到叶子包含属于一个特定类的样本，因此，导致过度拟合。要解决这个问题，您可以将max_depth参数调整为一个较低的值，以便模型不会过度拟合数据。下面的图比较了具有不同max_depth值的两个模型，您可以清楚地看到左侧的模型过拟合，它将无法很好地进行概化，而右侧的模型允许一两个错误分类，它将会很好地进行概化。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kf"><img src="../Images/5cc32957f94ca83d9d53c4f890675a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypIRB4O8bCR5DyFbt9M9Rg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">max_depth comparison</figcaption></figure><p id="c173" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.min_samples_split: <br/>有时候，如果一个节点包含10个样本，这些样本属于一百万个样本中的两个不同的类，如果没有达到max_depth值，DT仍然会尝试分割那个节点。分割该节点是没有意义的，因为它只是整个训练数据集的一小部分。为了防止这些情况，可以使用min_samples_split参数，该参数指定如果要进一步分割节点，节点必须具有的最小样本数。这与max_depth结合将大大降低过度拟合的效果。再次制作一个情节来支持上述陈述。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kg"><img src="../Images/be38bd47caba095f6680c4fee722252d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kf6PIMMUBeGT-lOzg1tt2w.png"/></div></div></figure><p id="c66f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.min_samples_leaf: <br/>这个超参数来自上面讨论的min_samples_split。min_samples_leaf指定了一个叶子必须具有的最小样本数，从而在条件不成立时防止节点分裂。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kh"><img src="../Images/5dd3d7acdfc966a0af3eccb4b26075c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKoAZw9C8KIItvNfIJPBEQ.png"/></div></div></figure><p id="049b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所有这些正则化方法都被称为“修剪决策树”,因为您正在删除导致过度拟合的节点。</p><p id="8cb6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:</strong> <br/>本文讨论了很多概念。看看这篇文章附带的笔记本。我希望你在写这篇文章的时候和我一样开心。鼓掌，订阅，留言评论。</p><div class="ki kj ez fb kk kl"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="km ab dw"><div class="kn ab ko cl cj kp"><h2 class="bd hi fi z dy kq ea eb kr ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ks l"><h3 class="bd b fi z dy kq ea eb kr ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kt l"><p class="bd b fp z dy kq ea eb kr ed ef dx translated">medium.com</p></div></div><div class="ku l"><div class="kv l kw kx ky ku kz jj kl"/></div></div></a></div></div></div>    
</body>
</html>