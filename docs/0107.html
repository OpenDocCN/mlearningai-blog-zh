<html>
<head>
<title>K-Nearest-Neighbor (KNN) explained, with examples!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k近邻(KNN)解释，有例子！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/k-nearest-neighbor-knn-explained-with-examples-c32825fc9c43?source=collection_archive---------0-----------------------#2021-02-07">https://medium.com/mlearning-ai/k-nearest-neighbor-knn-explained-with-examples-c32825fc9c43?source=collection_archive---------0-----------------------#2021-02-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f5c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你好，欢迎来到我的第一个关于灵媒的故事！</p><p id="72bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天，我将解释K-最近邻算法是如何工作的，以及它如何用于分类。我们将触及理论、偏差/方差权衡以及实施和一个非常简单的例子。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/0bbba9a8d4f99ebecc23ac909f968a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGzXg4OiKkiQFFf4Pafs3Q.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">The strong relation of neighbourhood reveals your true identity.</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h2 id="c54d" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">K近邻理论(KNN)</h2><p id="b710" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">k-最近邻是一种非参数算法，这意味着该算法不需要或假设关于分布的先验信息。这意味着KNN只依赖数据，更准确地说，是训练数据。下图中可以看到一个例子:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kz"><img src="../Images/9602c3580b248790c7069b80363d883e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDfjb4oc-qnQeX4zp1vRaA.png"/></div></div></figure><p id="9747" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一般来说，算法非常简单。当模型遇到未标记的数据点时，它测量到K个最近邻居的距离，从而测量名称，然后未标记的数据点将被分类为属于K个最近邻居中具有最多训练实例的类。请参见下图中的插图:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kz"><img src="../Images/b5c5df4631f282b8cfc963635a615ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DS0deLag_WOE-SE9W6d3Ig.png"/></div></div></figure><p id="fcab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里黄色的X是一个未标记的数据点，K = 3。最显著的类是类1，因此数据点被分类为类1。</p><p id="5b36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可能已经猜到，K是算法的超参数，它指定了未标记数据点将与之比较的邻居(训练实例)的数量。设置K=1会将算法减少到最近邻(NN)，从而选择最近数据点的类别作为未标记数据点的标签。</p><p id="a13c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除了邻居的数量，K控制模型的偏差/方差权衡。一般来说，你对低偏差和低方差感兴趣，但这几乎是不可能的。因此，要么选择一个具有高偏差的模型，然后拟合该模型以提供低方差，要么反之亦然。高偏差倾向于丢失数据集中的重要模式，而高方差倾向于过度拟合训练数据。</p><p id="1147" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">换句话说，设置K = 1，往往会使训练集的复杂结构过拟合，数据中的噪声会产生很大影响，但设置K太高，会使模型无法在数据中找到相关模式。K的最佳值可以通过在一组预定义的K值上进行简单的网格搜索来找到。</p><h2 id="c249" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">结果的概率计算</h2><p id="17f7" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">通常情况下，KNN算法不用于概率估计，但是，有可能估计一个给定分类的密度和后验概率。</p><p id="5ac2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们假设，我们在未标记的数据点创建一个球体，并且我们扩展该球体直到它正好包含K个邻居(训练实例)。请参见下图:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es kz"><img src="../Images/a4925fa828668e8e0350dc5c4a2a7349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PP5-blzmzEEG3zcATHqXYQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">In this case K = 3.</figcaption></figure><p id="0214" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设V是球体的体积，K_k是属于类别K的点的数量，N_k是训练数据中类别K的点的总数，N是训练点的总数。那么可能性可以计算为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es la"><img src="../Images/299cf332573cd6327f160168ff79af60.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/1*ksvMz0LI_-d1Zh5wN1UIVg.gif"/></div></figure><p id="8015" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">证据或边缘化可以计算为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lb"><img src="../Images/9553fdbfb5f45fbe3058f353e816782d.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/1*oZkBf9aKtNS4ksA-Hs8WBw.gif"/></div></figure><p id="cb05" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">并且所有类别(在这种情况下，类别1和类别2)的先验概率可以计算如下:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lc"><img src="../Images/1b266bb623cedeae5ea171e77a30ae8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/1*chQFYU1f1wqGHB5fFmeYZg.gif"/></div></figure><p id="6cab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然计算了先验概率、似然性和证据，我们可以通过组合上述等式来计算数据点属于某一类的后验概率:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ld"><img src="../Images/524de35b66ed3de6057a68b1cdd56e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/1*dext-3_d5KYd2a8bpDNuAw.gif"/></div></figure><p id="4707" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，后验概率可以通过属于给定类别的点与先前确定的K值的比率来导出。分类时，通常选择后验概率最高的类作为感兴趣的类。</p><h2 id="5cd9" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">使用Scikit-Learn实现KNN</h2><p id="039c" class="pw-post-body-paragraph ie if hh ig b ih ku ij ik il kv in io ip kw ir is it kx iv iw ix ky iz ja jb ha bi translated">使用Python的Scikit-Learn库实现KNN非常简单。</p><pre class="jd je jf jg fd le lf lg lh aw li bi"><span id="e9d4" class="jz ka hh lf b fi lj lk l ll lm"><strong class="lf hi">from</strong> <strong class="lf hi">sklearn.neighbors</strong> <strong class="lf hi">import</strong> KNeighborsClassifier</span><span id="2034" class="jz ka hh lf b fi ln lk l ll lm">X_train = ...    # Training data in format (n_instances, n_features)<br/>y_train = ...    # Training labels (n_instances, )<br/>k = 3<br/>model = KNeighborsClassifier(n_neighbors=k)<br/>model.fit(X_train, y_train)</span></pre><div class="lo lp ez fb lq lr"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hi fi z dy lw ea eb lx ed ef hg bi translated">sk learn . neighbors . kneighborsclassifier-scikit-learn 0 . 24 . 1文档</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">实现k-最近邻投票的分类器。了解更多信息。参数n_neighborsint，默认值=5…</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">scikit-learn.org</p></div></div><div class="ma l"><div class="mb l mc md me ma mf jm lr"/></div></div></a></div><p id="c1ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是现在的全部内容，我会很快带着更多关于机器学习的文章回来。我希望这篇文章有助于理解作为一种算法的KNN，并且你准备在你的ML管道中使用这种算法。</p><p id="3cda" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如有任何意见或建议，请随时与我联系。</p><p id="23d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢您的阅读…</p><h2 id="afec" class="jz ka hh bd kb kc kd ke kf kg kh ki kj ip kk kl km it kn ko kp ix kq kr ks kt bi translated">来源:</h2><ul class=""><li id="3f70" class="mg mh hh ig b ih ku il kv ip mi it mj ix mk jb ml mm mn mo bi translated">模式识别和机器学习，2006，Christopher M. Bishop</li><li id="2af5" class="mg mh hh ig b ih mp il mq ip mr it ms ix mt jb ml mm mn mo bi translated">sci kit-学习</li></ul></div></div>    
</body>
</html>