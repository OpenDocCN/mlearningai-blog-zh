<html>
<head>
<title>A Detailed Catalog of Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的详细目录</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/a-detailed-catalog-of-dimensionality-reduction-ca33d6f2744?source=collection_archive---------1-----------------------#2022-05-06">https://medium.com/mlearning-ai/a-detailed-catalog-of-dimensionality-reduction-ca33d6f2744?source=collection_archive---------1-----------------------#2022-05-06</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="ae78" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">用Python语言解释的多种降维方法</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/3a5732c32fc83430cd77b8a9cfa4f274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7yK-CeIVqdyQh17yJ1lgA.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@evieshaffer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Evie S.</a> on <a class="ae jm" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="535d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">维数是数据集的要素输入数量。在降维过程中，我们旨在通过将高维空间中的数据降维到低维空间(一种投影)来使用这些数据。这里的目的是原样保留高维数据中有意义的数据(即通过归约丢弃无意义的信息)或以最少的损失完成大小归约过程。</p><p id="d605" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们想象一下2D空间中的一些点。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es kj"><img src="../Images/3b86462b1768ceed747b5ddf296d6e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ed7cIZrxQeDrpNq8GOlJRA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Points in 2D space. Chart by author.</figcaption></figure><p id="ac52" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它们有一个位于某处的平均值，它们有特征向量(大的特征向量v1和小的特征向量v2)。我们可以只用v1坐标来表示这些点。想象所有这些点都在蓝线上，它们离均值有多远？另一个v2维度并不重要。所以，我们降低了维度(从2D到1D)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es kk"><img src="../Images/c446a926456ad337cd4ba5808f2f2ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_g00YKgX-gJ1mRmZ2BPoZg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">1 Dimension reduction. Image by author.</figcaption></figure><p id="d28e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们应用这种归约操作的原因是为了避免维数灾难。</p><h2 id="a372" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">维度的诅咒</h2><p id="c290" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">数据越大，我们要处理的麻烦就越多。随着维度的增加，数据空间的体积也在增加，并且增加的速度使得数据在这个巨大的空间中变得越来越稀疏。随着数据变得越来越稀疏，该数据有意义所需的数据数量增加，也就是说所需的数据相对于维度呈指数增长。</p><p id="fb3d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">所以我们还是简单的考虑一下吧。在表格数据集中，每一列有一个维度，在一个有n列的数据集中总共有n个维度。行是这个n维空间内的点。把几行、多列的情况想象成星系中的一个星座。正如Kuss [2002] 指出的那样，模型在处理稀疏数据时表现不佳。他们不能泛化模型，过度拟合。</p><p id="c098" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">由于在执行降维操作时，我们获得了一个较小的数据集，因此当然会损失一些信息。然而，我们不应该认为这是有害的。相反，它总共为我们贡献了很多。</p><p id="c2fb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">学习发生在<strong class="jp hi">的时间更少。需要更少的计算资源</strong>。</p><p id="197a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">减少维度数量也意味着降低复杂性。由于复杂性也会导致过度拟合，我们<strong class="jp hi">用这个过程来减少过度拟合</strong>。</p><p id="ff93" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">考虑到当人类的大脑在理解4个维度上有困难时，会有你需要理解更多的情况。因此，更少的维度更容易理解和可视化。我们可以<strong class="jp hi">将</strong>输入可视化，可以减少到二维或三维。</p><p id="5761" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果您的输入包含线性相关的要素(<strong class="jp hi">多重共线性</strong>)，您可以使用降维来消除它们。</p><p id="593d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">自然，一些<strong class="jp hi">不重要的特征</strong>也会在降维过程中被剔除。因此，您将删除对模型没有贡献的特征，这些特征的行为类似于噪声。</p><p id="38bc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一些方法可以将<strong class="jp hi">非线性数据转换成线性</strong>分解形式。</p><p id="ed26" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">简而言之，我们可以说，拥有太多的特征对一个机器学习算法是不利的。因此，降维操作被用于消除维数灾难。</p><p id="3704" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">根据所讨论的数据集，降维可以分为两个子类别，可以应用特征选择或特征提取方法。我们可以用一个线性代数的例子来演示这两个子范畴。</p><p id="b75c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们有一个等式:a + b + c + d + e = f。然后我们从两个给定的特征中创建一个新的特征，ab = a + b。那么新的等式将是；</p><p id="7139" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">ab + c + d + e = f(这是特征提取)</p><p id="d96e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">然后，我们知道c和d是零或者非常接近零。因此，我们删除它们，因为它们的影响可以消除。c，d ~= 0。最后，我们得到；</p><p id="b5ed" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">ab + e = f(这是特征选择)</p><p id="5e48" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在特征选择方法中，手边的数据没有变换。尝试从可用数据集中提取重要特征。输出矩阵是输入矩阵的一部分。您可以在我关于这个主题的文章中找到特性选择方法的细节。</p><div class="ll lm ez fb ln lo"><a href="https://towardsdev.com/the-most-used-feature-selection-methods-c117273759f8" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">最常用的特征选择方法</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">机器学习中常用特征选择方法的解释。</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">towardsdev.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc jg lo"/></div></div></a></div><p id="3876" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在特征提取中，输出数据不是输入的一部分，而是输入的转换形式。一个由许多维度组成的空间被转换或投影到一个更少维度的空间，因此，我们发现了新的特征。线性和非线性方法都可用。</p><h1 id="48ec" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">线性的</h1><p id="4e1a" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">这些方法最适用于线性数据。它们在非线性数据情况下可能表现不佳。</p><h2 id="2adf" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">PCA:主成分分析</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mo"><img src="../Images/4291666a6c8d5e2b86f399860913f4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MGXnqtt_fL5VhdjH4l26TQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="928e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它是最著名的降维方法。它是一种正交的线性变换，将数据变换到一个新的坐标系中，使得数据的某个投影的最大方差位于第一个主分量上，第二个最大方差位于第二个主分量上，依此类推。<strong class="jp hi">方差</strong>是PCA中的关键词。为什么？因为在这种理解中，我们说:如果一个变量在每次观察中取几乎相同的值，就模型而言，它没有增加多少信息，这个变量被认为是不重要的。</p><p id="87d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">PCA试图像回归线一样通过空间中的点画一条直线。这些线是主要成分。维数决定了主成分的个数。PCA的任务是将这些成分按重要性排序。</p><p id="6445" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">由于PCA方法，数据被压缩，所使用的存储空间减少。计算完成得更快。它消除了不必要的功能。</p><p id="f1a3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">反过来，可能会丢失信息。因为它是一种线性方法，所以它总是寻找特征之间的线性关系。如果数据的均值和协方差不足以解释数据，那么PCA也是不足的。在使用PCA的情况下，可解释性不幸丧失。</p><p id="067a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它是一种无监督的机器学习算法，通过数学方法将一个变量矩阵转换为一个具有不相关变量的更小矩阵，每个变量都称为主分量。为了更好地理解它，让我们使用虹膜数据集。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mp"><img src="../Images/e587f3f2be8064710e128f3efa60689c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVwgAvfNUt5JsgFq861D3g.png"/></div></div></figure><p id="1140" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">作为输入，我们有一个具有4个特征和150个观察值的矩阵(150，4)。</p><p id="0fa7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">首先我们必须缩放X矩阵。如果我们不缩放，具有大数字的特征将解释所有的方差，这会误导我们。</p><p id="0cb3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我会应用sklearn的StandardScaler。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ca"><img src="../Images/0e41307a82d6cc97002e9fdcd3c57d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NELSFHs_xuknGfO64X4FXQ.png"/></div></div></figure><p id="4fd7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">接下来，我们需要计算x的协方差矩阵。</p><p id="5b51" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">协方差矩阵</strong></p><p id="4ed0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">协方差矩阵给出了每个元素的协方差对。协方差是两个变量如何相互依赖的度量。这是对这两个变量如何一起变化的度量。如果两个变量(x和y)的协方差足够高，它们在变化的情况下共同作用。</p><p id="78ca" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">方差计算如下:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mq"><img src="../Images/320329bb8ae0645077f7d366d81ab8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*182G4eIQO8rur85l0pSqDQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Variation equation</figcaption></figure><p id="0b24" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">两个变量的协方差计算如下:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/f6e5f3f1d4c52463497acfe7355d8690.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*63OisBwWfBPkV8N2IoyhWA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Covariance equation</figcaption></figure><p id="dca5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">协方差矩阵是:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ms"><img src="../Images/4ecc549747f1899999f16643acb4d96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*DXuG5U9I3VUo9bqVEphjpA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Covariance matrix</figcaption></figure><p id="7af5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在让我们计算x的协方差矩阵。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mt"><img src="../Images/dec61268e21460731c10f8125ebde4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aoHTprekO_at5YVACjSCnw.png"/></div></div></figure><p id="a60e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">下一步是特征分解。我们需要计算这个协方差矩阵的特征值和特征向量。这个本征事物的概念需要详细考虑。我写了另一篇关于这个话题的文章，现在让我们简单解释一下。</p><div class="ll lm ez fb ln lo"><a rel="noopener follow" target="_blank" href="/@okanyenigun/eigenvalues-eigenvectors-8ae946539ba9"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">特征值和特征向量</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">降维的一个基本课题</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">medium.com</p></div></div><div class="lx l"><div class="mu l lz ma mb lx mc jg lo"/></div></div></a></div><p id="d1b8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">特征向量是在相关的线性变换下不改变方向的向量。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mv"><img src="../Images/21fe786c37755899393212bf41f47483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yANunkw1ZLIgL5-Lf9Cjog.png"/></div></div></figure><p id="29d6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我将计算特征值的百分比。每个百分比都指出了它们所解释的可变性。具有最高特征值的向量成为最重要的分量。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mw"><img src="../Images/b44cebd40f82ddf77783b97b7892bc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gozJ4cuf90k7YsqbopGiA.png"/></div></div></figure><p id="55bf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我将选择2个维度，因为前2个特征值提供了95.8 %的可变性。</p><p id="2cfe" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">最后，X将通过特征向量进行缩放。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/1a94951fde8983a529334ac63ff98de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoIXE52K5N1XGs6tA5Gscg.png"/></div></div></figure><p id="c50c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">所有代码:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="560a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Sklearn实现</strong></p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="3b4f" class="kl km hh nb b fi nf ng l nh ni">from sklearn.decomposition import PCA</span><span id="b8ce" class="kl km hh nb b fi nj ng l nh ni"><em class="nk">class </em>sklearn.decomposition.PCA(<em class="nk">n_components=None</em>, <em class="nk">*</em>, <em class="nk">copy=True</em>, <em class="nk">whiten=False</em>, <em class="nk">svd_solver='auto'</em>, <em class="nk">tol=0.0</em>, <em class="nk">iterated_power='auto'</em>, <em class="nk">random_state=None</em>)<a class="ae jm" href="https://github.com/scikit-learn/scikit-learn/blob/baf828ca1/sklearn/decomposition/_pca.py#L116" rel="noopener ugc nofollow" target="_blank">[source]</a><a class="ae jm" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" rel="noopener ugc nofollow" target="_blank">¶</a></span></pre><p id="0684" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">参数</strong>；</p><p id="884f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> n_components </strong> : <em class="nk"> int，float或‘mle’，默认=无</em></p><p id="3982" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这定义了我们想要保留的组件数量。如果是None(默认情况)，那么；</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="44c6" class="kl km hh nb b fi nf ng l nh ni">n_components = min(n_samples, n_features)</span></pre><p id="afd0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果它在0和1之间，这意味着我们希望保留n个分量，这解释了给定的方差百分比。</p><p id="9283" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果设置为'<em class="nk"> mle </em>，那么算法使用<a class="ae jm" href="https://vismod.media.mit.edu/tech-reports/TR-514.pdf" rel="noopener ugc nofollow" target="_blank"> Minka的mle方法</a>进行配置。这是对尺寸的自动猜测。</p><p id="b379" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">复制</strong> : <em class="nk"> bool，default=True </em></p><p id="8b18" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果为真，则覆盖给定x上的转换值。</p><p id="0852" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">变白</strong> : <em class="nk"> bool，default=False </em></p><p id="751a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它对变换后的向量应用数学运算。它们乘以样本数的平方根，然后除以奇异值，以获得具有单位分量方差的不相关值。这种方法将方差归一化。可以提高准确度。</p><p id="5db5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> svd_solver </strong> : <em class="nk"> { '自动'，'完整'，' arpack '，'随机化' }，默认= '自动'</em></p><p id="02bf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">自动:自动选择。如果随机化不适用，则继续使用全SVD。</p><p id="d8fc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">完整:运行<em class="nk"> scipy.linalg.svd </em></p><p id="493d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">arpack: n_components必须低于min(X.shape)。运行<em class="nk">scipy . sparse . Lina LG . svds</em></p><p id="e141" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">随机化:如果输入大于500x500，并且要提取的<em class="nk"> n_components </em>低于最小维度的80%,那么<em class="nk">随机化</em>可以被应用和建议。<a class="ae jm" href="https://arxiv.org/pdf/0909.4061.pdf" rel="noopener ugc nofollow" target="_blank">哈尔科的奇异值分解法</a>。</p><p id="8d8c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> tol </strong> : <em class="nk"> float，默认=0.0 </em></p><p id="7184" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果解算器是'<em class="nk"> arpack 【T23 ' '，那么公差值可以设置为奇异值。</em></p><p id="d8e9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">iterated _ power</strong>:<em class="nk">int或' auto '，默认='auto' </em></p><p id="a6ed" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果求解器是'<em class="nk">随机化的</em>'，那么可以定义幂法的迭代次数。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/497c92c2bf232f9962eaa3c1a34ba937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykydqY5axAuwl7cQaSpuCw.png"/></div></div></figure><p id="b96a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们可以得到解释的方差百分比；</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="3fe1" class="kl km hh nb b fi nf ng l nh ni">pca.explained_variance_ratio_<br/>#[0.72962445 0.22850762]</span></pre><p id="fa79" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，让我们在乳腺癌数据集上使用主成分分析。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="9132" class="kl km hh nb b fi nf ng l nh ni">pca.explained_variance_ratio_<br/>#array([0.44272026, 0.18971182])</span></pre><p id="fdb0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">最后，我们将一个30个特征的矩阵减少到2个维度，2个维度解释了63%的方差。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nl"><img src="../Images/b257d8d7cc20d4fba4fcde2afea792c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*K_3azUH3zL2MtTAf_Oac3Q.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><h2 id="11ef" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">FA:因子分析</h2><p id="ccb4" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">这是另一种约简方法，它试图通过变量之间的关系来发现数据集中的潜在变量。我们称这些潜在变量为因素。</p><p id="e1f2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在这里，我们看看每个变量与其余变量的相关性。根据这些相关性，这些变量被放在不同的组中。也就是说，这些组中的变量彼此之间的相关性很高，但与其他变量的相关性很低。</p><p id="1655" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们举个简单的例子。例如，我们有两个变量:CPU功率和耗电量。这两个变量之间将有很高的相关性，因为随着CPU能力的增加，它消耗的电量也将增加。然后我们把这两个变量放在同一个组。我们称这个组为因子。当我们浏览数据集中的要素时，我们会减小数据量，因为组(因子)的数量会比要素少。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nm"><img src="../Images/30e1f9a3c6f20a469915693cf7bd6b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgVKlf45SXxUvGOhRuPQDw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source:<a class="ae jm" href="https://www.researchgate.net/figure/Conceptual-overview-of-Exploratory-Factor-Analysis_fig1_209835856" rel="noopener ugc nofollow" target="_blank"> Jason W Osborne, Researchgate</a></figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/116afbffa9329a8265f04d017e45b3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*742x4SMUNzqzMMHtxqMStA.png"/></div></div></figure><p id="082c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了演示FA，我将使用BFI数据集。可以从<a class="ae jm" href="https://vincentarelbundock.github.io/Rdatasets/datasets.html" rel="noopener ugc nofollow" target="_blank">这里</a>下载。这个数据集是一个人格评估项目。清洗完数据集后，看起来是这样的；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nn"><img src="../Images/3a66f6c9acd502f4e94a742f560cf70b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jh1eiyKRmwv7qp5_nB430w.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Dataset.</figcaption></figure><p id="d321" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">原则上，在应用因子分析之前，我们需要测试数据集是否适合这种分析。这被称为“抽样充分性测试”。为此，我们可以应用两种不同的测试方法。</p><p id="abda" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> Bartlett的测试</strong>检查这些特征是否彼此相关。在此过程中，它将相关矩阵与单位矩阵进行比较。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es no"><img src="../Images/a51e764a8bc823236a146608c5359abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SmGikez3NuqOtLjnq2yuCA.png"/></div></div></figure><p id="44cc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">p值为0，表明相关矩阵不是单位矩阵。它通过了测试。</p><p id="5834" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在<strong class="jp hi">凯泽-迈耶-奥尔金测试</strong>中，考虑了特征之间的差异。KMO控制方差比。测试结果在0到1之间。因子分析的结果越高越好(实际的好结果限制为0.6)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es np"><img src="../Images/6ce7644c2030f4f440bd78916dcc9e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NA4NGa1bgGd4QF7uFuOV6A.png"/></div></div></figure><p id="4bd7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">0.84是个不错的成绩。</p><p id="db87" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了确定因子的数量，我们将使用特征值。实际上，我们可以将值的极限设置为1。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/a9b89b8a538638478edc5704b883f3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYueCq3KYwkEXiH2SSyxaQ.png"/></div></div></figure><p id="2974" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们画出这些值；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nq"><img src="../Images/328d212d5db9d7a264222939caa16cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50y-YNrOiObRW4OZyfQ4ow.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Eigenvalues. Image by author.</figcaption></figure><p id="3cf0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们可以选择6个因子，因为只有6个特征值大于1。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nr"><img src="../Images/72679e49013be5406be408dad364c32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ryIDZQFtAbA5UoCdK2Qaw.png"/></div></div></figure><p id="8ca2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">旋转参数是为了更容易解释分析而给出的参数。</p><p id="86ab" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因子加载显示了每个原始特征与因子的关系。越高越好。正如你所看到的，第六因子中的所有值都很低。事实上，第6个因素是不需要的。最好从5个因素重新开始。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ns"><img src="../Images/235007a23781cd668ef31592682b7b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAFBAy4EbNsq5xUwVhygqA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Loadings.</figcaption></figure><p id="bfcc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">总方差的45%是由这些因素解释的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nt"><img src="../Images/145ecb85c7174054f08a4b2a0952c594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89E-IMQ4QzmFXadmLZMdPg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Variance Table</figcaption></figure><p id="05b1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">本演示的全部代码；</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><h2 id="78d4" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">线性鉴别分析</h2><p id="3e43" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">LDA可以用作多分类模型，也可以用于降维。这是一种受监督的方法(与PCA相反)，因此它使用类别标签。</p><p id="845b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了使LDA正常工作，数据必须呈正态分布。不需要缩放。类必须存在。最大分量可以比目标特征中的类的总数少一个。</p><p id="499d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，我们有两个类，如下所示。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nu"><img src="../Images/dc3d5238c9e6fa6961b9cdbc2cdb6116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*_L5G0_4rlhCpcjYAivAMVA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Two classes. Image by author.</figcaption></figure><p id="ee4a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">假设我们将这些点投影到x轴上。正如你所注意到的，它不是这样工作的。大量信息丢失。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nv"><img src="../Images/975a1a08d166828bce5bb932c74a4e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*GPwZGjvPZYBq4wp0d6AyNA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="8eff" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">取而代之的是，让我们根据x和y在点之间放一条线性线(类似回归)，然后，让我们将这些点投影到这条线上。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nw"><img src="../Images/06604c117b50451f1e85262defaa8821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*tRBIDjZh9wIB5r4DOwXqFQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="b2a7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在好多了！解释了很多。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es nx"><img src="../Images/3767441296ab2890b6df679fe839fd90.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*yR3774uSQNp9OlPebEvKtg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="19ef" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">因此，这些线性组合(判别函数)由n个输入特征产生。特征被投射到这些线性判别式上。请记住，这些线性判别式假设目标要素的类别具有相同方差的多元正态分布。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ny"><img src="../Images/d8d93669154914e3b31dcc3237f167f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snPJRuvcYO-EUION8X5PqQ.png"/></div></div></figure><p id="fb62" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们创建虚拟分类数据来演示LDA。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="cfca" class="kl km hh nb b fi nf ng l nh ni">from sklearn.datasets import make_classification</span><span id="23b1" class="kl km hh nb b fi nj ng l nh ni">X, y = make_classification(n_samples=1000, n_features=10,   n_informative=5, n_redundant=2, random_state=1, n_classes=4)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es no"><img src="../Images/8308be0fff8aeed5b68b4b937532f8de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6PBxk-GChaj1KY83qVbNQQ.png"/></div></div></figure><p id="3c08" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">超过85%的差异由两个部分解释。您可以在下面的图表中看到包含两个组件的类。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nz"><img src="../Images/76b151806dbd008d71fb86016f453c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RziR7PQ6w3xsGeZOlnIZ6A.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Classes. Image by author.</figcaption></figure><h2 id="763a" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">独立分量分析</h2><p id="cefc" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">ICA是一种基于信息论的方法。与PCA不同，它寻找的是相互独立的因素，而不是不相关的因素。让我解释一下这种差异:如果我们说两个变量彼此不相关，我们表明它们之间没有线性关系。如果我们说这两个变量是独立的，我们是说它们在任何意义上都不相互依赖。</p><p id="9f02" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">ICA认为原始变量中存在一些潜在的混合变量，并且这些潜在变量必须是相互独立的。它通过分离信息来最大化ICA分量的非高斯性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oa"><img src="../Images/0866b6cefdc5b212fe8e12ad6ae28109.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*rlxhmCmQnPC3-dvzSYkaoA.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">a) PCA b) ICA <a class="ae jm" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/be4ddd9b3d281144e2858c52eb593eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCkHTTjlNIYeEl_PLn-LSA.png"/></div></div></figure><p id="8dd9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们可以再次使用乳腺癌数据集。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ob"><img src="../Images/d5ca264bde180dc70d2a152c5bf32087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLco32gee7HZyHDkHtpPtw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><h1 id="445b" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">矩阵分解</h1><p id="26f8" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">我们可以用线性代数知识来降维。我们可以分解(矩阵分解或矩阵因式分解)复杂的矩阵，使其更具可计算性。</p><p id="4762" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">分解矩阵由复矩阵的组成部分组成。我举个例子简化一下。我们可以说2 x 5是10的因数。就像取10的因子一样，我们对矩阵进行类似的处理。</p><h2 id="b124" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">LU矩阵分解</h2><p id="9d03" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">该方法将矩阵分解成两个分量；A是我们要分解的方阵，L和U是三角矩阵。</p><p id="dc51" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">A = L . U</p><p id="aca8" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">LU分解的更稳定版本是LUP分解，其中在方程中引入了新的矩阵P。父矩阵被重新排序以提供一个更健壮的解决方案，这就是我们需要P矩阵的地方。</p><p id="4ba3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">A =公共事业单位</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="34df" class="kl km hh nb b fi nf ng l nh ni">import numpy as np<br/>from scipy.linalg import lu</span><span id="3433" class="kl km hh nb b fi nj ng l nh ni">#let's define a square matrix 3x3<br/>arr = np.array([[1,1,1],[2,2,2],[3,3,3]])</span><span id="d0bc" class="kl km hh nb b fi nj ng l nh ni">#we will use lu method to get decomposed matrixes<br/>P, L, U = lu(arr)</span><span id="a3a7" class="kl km hh nb b fi nj ng l nh ni">#P<br/>[[0. 0. 1.]<br/> [0. 1. 0.]<br/> [1. 0. 0.]]</span><span id="7f24" class="kl km hh nb b fi nj ng l nh ni">#L -&gt; lower triangle<br/>[[1.         0.         0.        ]<br/> [0.66666667 1.         0.        ]<br/> [0.33333333 0.         1.        ]]</span><span id="7677" class="kl km hh nb b fi nj ng l nh ni">#U -&gt; upper triangle<br/>[[3. 3. 3.]<br/> [0. 0. 0.]<br/> [0. 0. 0.]]</span></pre><h2 id="fc1d" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">QR矩阵分解</h2><p id="0f74" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">如果矩阵不是正方形，可以使用QR矩阵分解。它应用于m×n矩阵。</p><p id="11c1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">A = Q . R</p><p id="bc48" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">q是一个m×m的方阵，R是一个形状为m×n的上三角矩阵。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="7dbd" class="kl km hh nb b fi nf ng l nh ni">from numpy.linalg import qr</span><span id="816a" class="kl km hh nb b fi nj ng l nh ni">Q, R = qr(arr, "complete")</span><span id="3e06" class="kl km hh nb b fi nj ng l nh ni">#Q<br/>[[-0.26726124  0.95618289  0.11952286]<br/> [-0.53452248 -0.04390192 -0.84401323]<br/> [-0.80178373 -0.28945968  0.52283453]]</span><span id="1b4a" class="kl km hh nb b fi nj ng l nh ni">#R <br/>[[-3.74165739e+00 -3.74165739e+00 -3.74165739e+00]<br/> [ 0.00000000e+00 -4.96506831e-16 -4.96506831e-16]<br/> [ 0.00000000e+00  0.00000000e+00  4.93038066e-32]]</span></pre><h2 id="f80f" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">乔莱斯基分解</h2><p id="6c9d" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">我们可以将这种方法应用于平方对称矩阵，并且所有特征值都应该是正的。我们把矩阵A分解成一个下三角矩阵和l的转置矩阵。我们也可以用一个上三角来分解它。</p><p id="c383" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">L^T或U^T</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="e03e" class="kl km hh nb b fi nf ng l nh ni">from numpy.linalg import cholesky</span><span id="4339" class="kl km hh nb b fi nj ng l nh ni">#a new matrix that is positive<br/>arr = np.array([[3,1,1],[2,4,2],[3,3,6]])<br/>L = cholesky(arr)</span><span id="0026" class="kl km hh nb b fi nj ng l nh ni">#L<br/>[[1.73205081 0.         0.        ]<br/> [1.15470054 1.63299316 0.        ]<br/> [1.73205081 0.61237244 1.62018517]]</span></pre><h2 id="a134" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">奇异值分解</h2><p id="4a66" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">如果数据集非常稀疏(大部分数据为零)，可以使用它。使用领域包括推荐系统、评级、图像压缩、去噪等。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oc"><img src="../Images/8f3d2007222b941e56114eb9c6729dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*AYFoHtW8QCd7_HfPIoIOUg.png"/></div></figure><p id="2b53" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">矩阵A被分解成U-正交矩阵、S-对角矩阵和V-另一个对角矩阵。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es od"><img src="../Images/b4326e8d1fc4a68cd776f22d413e7d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ly30AAmhP5kd679vDWe8eg.png"/></div></div></figure><h2 id="56b9" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">NMF:非负矩阵分解</h2><p id="4e4e" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">这是一种无监督的学习算法。这里，输入矩阵必须是非负矩阵。这个输入矩阵分解成两个非负的W和H矩阵。</p><p id="4731" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">A =宽高</p><p id="f7c6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">此方法提供数据压缩。它对噪声具有鲁棒性，并且易于解释。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="94f7" class="kl km hh nb b fi nf ng l nh ni"><strong class="nb hi">from</strong> sklearn.decomposition <strong class="nb hi">import</strong> NMF <br/>model = NMF(n_components=200, init='nndsvd', random_state=0) <br/>W = model.fit_transform(X) </span></pre><h1 id="a2f1" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">非线性(流形学习)</h1><p id="5d6e" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">如果我们的数据是非线性类型的，线性降维方法将不会表现得足够好。在这种情况下，最好使用非线性方法。</p><h2 id="f2fd" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">多方面的</h2><p id="d043" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">让我们用扁平的地球来简单地描述流形。这些朋友环顾四周，得出结论:既然他们看到的那块地球是平的，那么地球也一定是平的。然而，当我们考虑到他们所看到的世界的规模时，这只是很小的一部分。这小小的见证是这个世界的一个缩影。如果我们把所有这些流形放在一起，那么我们将拥有真实数据的世界，我们将理解世界的球形结构。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oe"><img src="../Images/6df9c1c8502a71af30c3eb8661c8441f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Qe1ybr6F1Ku4HL4uUVRC7A.jpeg"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">The surface of the Earth requires (at least) two charts to include every point. Here the <a class="ae jm" href="https://en.wikipedia.org/wiki/Globe" rel="noopener ugc nofollow" target="_blank">globe</a> is decomposed into charts around the <a class="ae jm" href="https://en.wikipedia.org/wiki/North_pole" rel="noopener ugc nofollow" target="_blank">North</a> and <a class="ae jm" href="https://en.wikipedia.org/wiki/South_Pole" rel="noopener ugc nofollow" target="_blank">South Poles</a>. Source: <a class="ae jm" href="https://en.wikipedia.org/wiki/Manifold" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="d9c4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">同样，让我们考虑一条n维曲线。当我们把这条曲线上的所有流形放在一起，还是得到原来的n维曲线。</p><p id="6206" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有不同的方法可以找到流形上两点之间的最短距离。曲面在测地线距离处是弯曲的，在欧几里德距离处是平坦的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es of"><img src="../Images/eb4e4bcf7228fa004464fbd08ec417c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*3qda3GzTyXaUDxIn0Tqihg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Geodesic And Euclidean Distances. <a class="ae jm" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="5f84" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">核主成分分析</h2><p id="e263" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">正如该方法的名称所示，该方法类似于用于非线性情况的PCA方法。它使用了“内核技巧”。</p><p id="7cc9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">数据首先在内核的帮助下被投影到一个更高维的空间。这样，类变得更容易分离。然后，将经典的PCA应用于这个新的高维数据，并将数据降回到低维空间。</p><p id="0a8d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">核函数计算数据非线性映射的点积。这里的意思基本上是这样的:原始特征的非线性组合被创建并从原始维度放大到更高维度的空间。例如，我们有三个特征x1、x2和x3:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es og"><img src="../Images/6b48efe286ab2f5eff5e095fb7a675ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*sHiRfWdpoE5aWyPN7Z0wOw.png"/></div></figure><p id="3cf9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们将内核应用于这些特征，然后我们得到更高维度的东西。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es oh"><img src="../Images/9c990161a155be8b1aa6e43aa5e3ba29.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*eG1WXkf5KTjgo7j0RekYHg.png"/></div></figure><p id="89fd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了形象化这一点，让我们首先创建虚拟聚类数据。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="b654" class="kl km hh nb b fi nf ng l nh ni">from sklearn.datasets import make_circles<br/>X, y = make_circles(n_samples=1000, factor=.2, noise=.1)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oi"><img src="../Images/75c11f9dc8705748ea430d4a0294a387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKbKXKMQkaHAHGwI1F83Qg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="ef35" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">正如您所看到的，在这里似乎不可能线性地分离这两个类。现在让我们应用KPCA和增加维度。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="8b3c" class="kl km hh nb b fi nf ng l nh ni">kpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10, )<br/>X_kpca = kpca.fit_transform(X)</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oj"><img src="../Images/ebad96ba8ff1d4fb528618b33e427dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CdnHc2K4MFaIzkS5nuWDsA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Image by author.</figcaption></figure><p id="a800" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">不错！现在，我们可以用一条直线把它们分开。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="8312" class="kl km hh nb b fi nf ng l nh ni"><strong class="nb hi">from</strong> <strong class="nb hi">sklearn.decomposition</strong> <strong class="nb hi">import</strong> KernelPCA</span><span id="9c13" class="kl km hh nb b fi nj ng l nh ni"><em class="nk">class </em>sklearn.decomposition.KernelPCA(<em class="nk">n_components=None</em>, <em class="nk">*</em>, <em class="nk">kernel='linear'</em>, <em class="nk">gamma=None</em>, <em class="nk">degree=3</em>, <em class="nk">coef0=1</em>, <em class="nk">kernel_params=None</em>, <em class="nk">alpha=1.0</em>, <em class="nk">fit_inverse_transform=False</em>, <em class="nk">eigen_solver='auto'</em>, <em class="nk">tol=0</em>, <em class="nk">max_iter=None</em>, <em class="nk">iterated_power='auto'</em>, <em class="nk">remove_zero_eig=False</em>, <em class="nk">random_state=None</em>, <em class="nk">copy_X=True</em>, <em class="nk">n_jobs=None</em>)<a class="ae jm" href="https://github.com/scikit-learn/scikit-learn/blob/baf828ca1/sklearn/decomposition/_kernel_pca.py#L21" rel="noopener ugc nofollow" target="_blank">[source]</a></span></pre><p id="fffc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">参数；</strong></p><p id="a8fc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">n _组件</strong> : <em class="nk"> int，默认=无</em></p><p id="a15c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果选择“无”，则保留所有非零分量。</p><p id="aff4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">内核</strong> : <em class="nk"> { '线性'，'多边形'，' rbf '，' sigmoid '，'余弦'，'预计算' }，默认= '线性'</em></p><p id="d1c5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它是PCA的内核类型。<em class="nk"> rbf </em>是用的最多的一个。</p><p id="4b33" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">伽玛</strong> : <em class="nk">浮点，默认=无</em></p><p id="d6d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它是核系数(如果核是rbf、poly或sigmoid之一)。</p><p id="281e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">度数</strong> : <em class="nk"> int，默认=3 </em></p><p id="d190" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果选择内核为<em class="nk"> poly </em>，我们可以给出一个度数值。</p><p id="43b1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> coef0 </strong> : <em class="nk">浮动，默认=1 </em></p><p id="97a4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果内核是sigmoid或者poly，我们可以传递一个独立的term值。</p><p id="2a0e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">内核参数</strong> : <em class="nk">字典，默认=无</em></p><p id="f41c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们可以像字典一样传递参数。</p><p id="d640" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> alpha </strong> : <em class="nk"> float，默认=1.0 </em></p><p id="f806" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果<em class="nk"> fit_inverse_transform </em>为真，则可以设置岭回归的超参数值。</p><p id="75a3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> eigen_solver </strong> : <em class="nk"> { '自动'，'密集'，' arpack '，'随机化' }，默认= '自动'</em></p><p id="aa80" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">特征值求解器。如果组件数量少于训练样本，建议使用<em class="nk">随机化</em>或<em class="nk"> arpack </em>。</p><p id="3ff4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> tol </strong> : <em class="nk"> float，默认=0 </em></p><p id="f2c0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果本征解算器是<em class="nk"> arpack </em>时的收敛公差。</p><p id="ab88" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> max_iter </strong> : <em class="nk"> int，default=None </em></p><p id="5eea" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这是<em class="nk"> arpack </em>特征值求解器的最大迭代次数。</p><p id="f26e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">iterated _ power</strong>:<em class="nk">int≥0，或' auto '，默认='auto' </em></p><p id="839d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果svd解算器被选择为<em class="nk">随机化</em>，则为迭代次数。</p><p id="9852" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">remove _ zero _ EIG</strong>:<em class="nk">bool，default=False </em></p><p id="87c6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果设置为真，则删除所有零特征值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ok"><img src="../Images/1dedd95cdbb83e16b010e6070cae953b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_BXjDAMb8M_zltRXawXqA.png"/></div></div></figure><h2 id="7850" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">t-SNE:t-分布随机邻居嵌入</h2><p id="78e0" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">它是一种有监督的方法，用于自然语言处理和图像处理领域，此外，它还有助于数据可视化。建议在应用t-SNE之前使用PCA或TSVD。</p><p id="0328" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">记住，主成分分析试图最大化方差，与此不同，SNE霸王龙试图最小化两个分布之间的差异。</p><p id="d453" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi">发散</strong></p><p id="9dcd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">虽然题目不难，但有时可能很难理解，因为数学是一个抽象的概念，而这些概念也是非常不寻常的概念。上过向量微积分课程也会让你更容易理解这门学科。</p><p id="de72" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果我们复习一个具体的概念，让我们想想一条河。在河流的每一点，水都有一定的流速值，相应的，单位时间内也有一定的水流量。如果要用向量来表示，就需要把这条河表示成由箭头组成的面或者物体。这就是所谓的向量场。为了说明这一点，需要一个给出河流中期望点的水流量的方程。使用各种测量和最合适的方程模型(阶数等)来创建该方程。).</p><p id="7980" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">生成的方程是ai+bj+ck形式的向量函数。该功能提供了关于有多少水沿哪个方向通过的信息。我们现在可以使用这个函数，即向量场，来达到我们想要的目的。例如，我们可以学习我们想要的点的流量。</p><p id="719d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">或者说，我们想知道是否存在点流变化。正如我们使用导数来寻找函数的单位变化，我们也根据x，y和z分别对I，j和k求导，并将该区域的坐标写在适当的位置。我们得到的三个值给出了该点在每个方向上矢量的增加或减少。即单位时间通过的水量如果有增加，则为正，如果有减少，则为负，否则为零。当我们把这三个值相加，我们就明白了该点向量的值是否总共增加了。</p><p id="20fa" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果结果是肯定的:有所增加。这意味着要么是狭窄，要么是补充了水分。如果结果是否定的:有减少。正常情况下，每分钟通过10kg的水，就变成了3kg。要么是河流变宽了，要么是水从那部分被抽走了。</p><p id="b4e6" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">总而言之，散度用于检测向量场在某一点的值是否增加。</p><p id="825b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><strong class="jp hi"> KL —散度</strong></p><p id="c9ed" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在机器学习领域的很多情况下，我们可能需要比较两个概率分布。例如，我们有一个随机变量，这个变量有两种不同的概率分布。一个是我们估计的近似值，一个是实际分布。在这种情况下，我们需要测量这两个分布之间的统计距离。我们用散度来衡量这一点。在这里，散度是这两个概率分布之间的差异的度量。</p><p id="fab4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">KL散度是一种计算这个散度分数的数学方法。</p><p id="dbba" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">KL(P | | Q)=—X P(X)中的X总和* log(Q(x) / P(x))</p><p id="ae49" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在，让我们回到SNE霸王龙的话题。该方法首先计算高维数据的概率分布。然后，它在较低的维度上创建这些分布的相似副本。最后，它最小化了这两种分布之间的差异。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mx"><img src="../Images/5628f20152248263f1633a06adc9ca32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9Hj6ZlfgPXQNZ7vAoqd9w.png"/></div></div></figure><p id="a2bf" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了演示SNE霸王龙，我们可以使用MNIST的数据。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="my mz l"/></div></figure><p id="96db" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">看起来不错！聚集得很好。点的邻居数量决定了它在数据中的密度。我们可以用困惑值来控制密度。该值越高，考虑的全局结构就越多。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ol"><img src="../Images/67f1f883b659ec7e5b8a68168a76dbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHGk3yxNJeIeDp9_sSgrNQ.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">MNIST Clusters after t-SNE applied. Image by author.</figcaption></figure><p id="e074" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">显然，在非线性情况下，使用t-SNE比PCA更好。但是如果说的是大数据量，计算成本会很高。</p><h2 id="d52d" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">UMAP:一致流形近似和投影</h2><p id="dd42" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">t-SNE不错，但也有缺点。很多信息可能会在这个过程中丢失，计算起来太费时间了。UMAP提供了更快的过程和更好的可视化能力。</p><p id="e89d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">首先，测量高维空间中的点之间的距离。然后它们被投射到更低维度的空间中，并且在更低维度的空间中再次测量距离。通过使用SGD(随机梯度下降),这些距离之间的差异试图被最小化。</p><pre class="ix iy iz ja fd na nb nc nd aw ne bi"><span id="a690" class="kl km hh nb b fi nf ng l nh ni">import umap<br/>X_umap = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3).fit_transform(X)</span></pre><h2 id="d2ae" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">LLE:局部线性嵌入</h2><p id="9abd" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">另一种无监督的非线性方法。它使用数据的拓扑结构，并将其保存在一个低维空间中。这是一种快速的方法，但如果数据中有噪声，效果就不好。</p><p id="5697" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它查找每个数据点的最近邻，并创建一个充满权重的矩阵。这些权重被计算为其k个邻居的线性组合。然后最小化它们之间的平方距离。最后，通过特征向量优化将权重映射到低维空间。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es om"><img src="../Images/9e8f0f70fe4905abc0a9547e1f8ef088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IANszF8JP3axw-i3rPbVSA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source: <em class="on">Dimensionality reduction technique: LLE | Source: S. T. Roweis and L. K. Saul, Nonlinear dimensionality reduction by locally linear embedding</em></figcaption></figure><h2 id="8b72" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">MDS:多维标度</h2><p id="7ab2" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">我们可以应用度量和非度量多维标度方法。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oo"><img src="../Images/2b8001564e83b604f43a672538113920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XL9Rfjvvb67n6tkoybSDIQ.png"/></div></div></figure><h2 id="36d2" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">光谱嵌入</h2><p id="9147" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">非线性，无人监管。这种方法在低维空间的表示中搜索不同组中的聚类。</p><p id="a739" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">计算数据的拉普拉斯矩阵。本征物是从分解中获得的。该方法使用第二小特征值及其向量。</p><p id="3491" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这种方法用于图像分割项目。</p><h2 id="80bd" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">Isomap:等距映射</h2><p id="f415" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">Isomap方法假设流形是平坦的。因此，测地线距离和欧几里德距离是相等的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es op"><img src="../Images/bf22a6d9a43b74dd0576e8c176b65385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SrCweATWyGILpeyEn_NGQw.png"/></div></div></figure><h2 id="a77a" class="kl km hh bd kn ko kp kq kr ks kt ku kv jw kw kx ky ka kz la lb ke lc ld le lf bi translated">自动编码器</h2><p id="b383" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">自动编码器是一种人工神经网络。这是另一篇详细文章的主题。但是，我们应该知道，自动编码器也用于降维操作。</p><p id="7f21" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在自动编码器中，输入和输出都有<em class="nk"> n </em>个单位。中间至少有一个隐藏层。第一部分是编码器。编码器接受n维输入，并将其压缩到m维。第二部分是解码器。它从编码器获取m维数据，并将其扩展回n维。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es oq"><img src="../Images/1f931e9e4a79f05bd4a13826f20a7c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*V1M3p1tsjoWLKeLs5D2tlA.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">AutoEncoder. Source: <a class="ae jm" href="https://commons.wikimedia.org/wiki/File:Autoencoder_structure.png" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><h1 id="3fbd" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">结论</h1><p id="f3e4" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated">这是一篇很长的文章。还有很多其他的事情可以做，很多方法可以应用。在本文中，我主要从分类和聚类问题的角度处理了降维操作。然而，它通常用于声音处理、图像处理以及机器学习或人工智能应用中。</p><p id="e00b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">降维操作与微积分和线性代数的主题密切相关。事实上，它可能是机器学习项目中数据预处理阶段最数学化的部分。保持我们的线性代数和本征值学科的知识是有用的。</p><p id="0a59" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">希望这个目录对你有用。</p><h1 id="753a" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">阅读更多内容…</h1><div class="ll lm ez fb ln lo"><a rel="noopener follow" target="_blank" href="/@okanyenigun/eigenvalues-eigenvectors-8ae946539ba9"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">特征值和特征向量</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">降维的一个基本课题</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">medium.com</p></div></div><div class="lx l"><div class="mu l lz ma mb lx mc jg lo"/></div></div></a></div><div class="ll lm ez fb ln lo"><a href="https://python.plainenglish.io/what-are-the-feature-transformation-techniques-ba594b523ec4" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">特征变换技术有哪些？</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">特征转换技术的浏览</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">python .平原英语. io</p></div></div><div class="lx l"><div class="or l lz ma mb lx mc jg lo"/></div></div></a></div><div class="ll lm ez fb ln lo"><a href="https://python.plainenglish.io/outliers-and-anomalies-210eefc51aa" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">什么是异常值和异常值？</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">对异常世界的探索</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">python .平原英语. io</p></div></div><div class="lx l"><div class="os l lz ma mb lx mc jg lo"/></div></div></a></div><div class="ll lm ez fb ln lo"><a href="https://python.plainenglish.io/how-to-handle-missing-values-ef4abd02673f" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">如何处理缺失值？</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">处理丢失数据所需的一切</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">python .平原英语. io</p></div></div><div class="lx l"><div class="ot l lz ma mb lx mc jg lo"/></div></div></a></div><div class="ll lm ez fb ln lo"><a href="https://towardsdev.com/the-most-used-feature-selection-methods-c117273759f8" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">最常用的特征选择方法</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">机器学习中常用特征选择方法的解释。</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">towardsdev.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc jg lo"/></div></div></a></div><h1 id="bd4d" class="md km hh bd kn me mf mg kr mh mi mj kv in mk io ky iq ml ir lb it mm iu le mn bi translated">参考</h1><p id="0ea6" class="pw-post-body-paragraph jn jo hh jp b jq lg ii js jt lh il jv jw li jy jz ka lj kc kd ke lk kg kh ki ha bi translated"><a class="ae jm" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Dimensionality_reduction</a></p><p id="e2e3" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/dimensionally-reduction-for-machine-learning/</a></p><p id="d9b2" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/introduction-to-matrix-decompositions-for-machine-learning/</a></p><p id="be92" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" rel="noopener" href="/data-science-365/statistical-and-mathematical-concepts-behind-pca-a2cb25940cd4">https://medium . com/data-science-365/statistical-and-mathematical-concepts-behind-PCA-a2cb 25940 CD 4</a></p><p id="92a4" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></p><p id="f0b5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://towardsdatascience.com/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0" rel="noopener" target="_blank">https://towards data science . com/principal-component-analysis-PCA-with-scikit-learn-1e 84 a 0 c 731 b 0</a></p><p id="06ed" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://towardsdatascience.com/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0" rel="noopener" target="_blank">https://towards data science . com/factor-analysis-on-women-track-records-data-with-r-and-python-6731 a73c D2 e 0</a></p><p id="d73a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b" rel="noopener" target="_blank">https://towards data science . com/11-dimensionally-reduction-techniques-you-should-know-in-2021-dcb 9500d 388 b</a></p><p id="9c58" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=3uxOyk-SczU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=3uxOyk-SczU</a></p><p id="7412" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=AU_hBML2H1c" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=AU_hBML2H1c</a></p><p id="76f0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.youtube.com/watch?v=jPmV3j1dAv4" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=jPmV3j1dAv4</a></p><p id="2f85" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/divergence-between-probability-distributions/</a></p><p id="8737" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://eksisozluk.com/entry/73389025" rel="noopener ugc nofollow" target="_blank">https://eksisozluk.com/entry/73389025</a></p><p id="11fc" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://neptune.ai/blog/dimensionality-reduction" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/dimensionality-reduction</a></p><p id="e232" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.geeksforgeeks.org/dimensionality-reduction/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/dimensionality-reduction/</a></p><p id="2b33" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.javatpoint.com/dimensionality-reduction-technique" rel="noopener ugc nofollow" target="_blank">https://www . Java point . com/dimensionally-reduction-technique</a></p><p id="ec35" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://thenewstack . io/3-机器学习中的数据降维新技术/ </a></p><p id="d6d5" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/08/dimensionally-reduction-techniques-python/</a></p><p id="b521" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://scikit-learn.org/stable/modules/unsupervised_reduction.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/unsupervised _ reduction . html</a></p><p id="905a" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.tmwr.org/dimensionality.html" rel="noopener ugc nofollow" target="_blank">https://www.tmwr.org/dimensionality.html</a></p><p id="b7e0" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated"><a class="ae jm" href="https://www.datacamp.com/tutorial/introduction-factor-analysis" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/tutorial/introduction-factor-analysis</a></p><div class="ll lm ez fb ln lo"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hi fi z dy lt ea eb lu ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">medium.com</p></div></div><div class="lx l"><div class="ou l lz ma mb lx mc jg lo"/></div></div></a></div></div></div>    
</body>
</html>