<html>
<head>
<title>Let's make Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们做决策树</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/lets-make-decision-trees-9aa874dc9731?source=collection_archive---------9-----------------------#2021-06-05">https://medium.com/mlearning-ai/lets-make-decision-trees-9aa874dc9731?source=collection_archive---------9-----------------------#2021-06-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="bade" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">决策树采用分而治之的解决问题方法</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/077ecee2e6d62d03868b58d35c0fa32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wSnaRZJlfCIT35ACPhfDqg.jpeg"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@jordanfmcqueen?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jordan McQueen</a> on <a class="ae jm" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="34f2" class="jn jo hh bd jp jq jr js jt ju jv jw jx in jy io jz iq ka ir kb it kc iu kd ke bi translated">介绍</h1><p id="322f" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">决策树是一种监督学习算法，用于使用流程图的分类，其中每个内部节点表示一个条件(测试)，每个终端节点持有一个类别标签(1或0，是或否，默认或非默认)。</p><p id="06c7" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">让我们用一个例子来理解决策树:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lg"><img src="../Images/b68ac7a35bfd7bd83690512c93a49658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngIQeF61YCEbmU6D64_P6A.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="8375" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">假设一家大公司想要收购一家在市场上提供类似产品的小型创业公司。现在，高管和管理层必须决定收购这家公司。实时会有多种因素需要考虑，但现在，让我们保持简单。首先，公司将查看市场份额，如果市场份额小于5 %，则向左移动，如果在5%到20%之间(举例来说，不考虑20%以上)，则向右移动，如图所示。如果创业公司的市场份额低于5%,但创业公司的员工经过培训，公司将收购创业公司，如果员工没有经过培训，公司将不会收购。现在，如果初创公司的市场份额在5%至20%之间，并且长期负债较低，那么公司将收购初创公司，但如果初创公司负债累累，那么公司不会收购。这就是决策树的工作原理。</p><p id="f6d3" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">想象一下，我们不是只有三个特征，而是有许多特征，不是一个创业公司，而是有一个成千上万个创业公司的列表，我们想从这些公司中只收购最好的。这就是决策树在机器学习中的作用。一个巨大的数据集被输入到DT模型中，它会根据基尼系数和熵等分割标准(将在后面的部分讨论)在分割后自动构建树。在这里，DT考虑了所有可能导致最终决策的路径，其中DT遵循最佳路径。</p></div><div class="ab cl lh li go lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ha hb hc hd he"><h1 id="974a" class="jn jo hh bd jp jq lo js jt ju lp jw jx in lq io jz iq lr ir kb it ls iu kd ke bi translated">分类和回归树</h1><p id="0e8e" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">是的，我们可以使用决策树进行分类和回归。如果因变量是离散的，我们使用分类，如果是连续的，我们使用回归树。分类树使用基尼指数或熵来分割节点，而回归树使用误差平方和(SSE)来分割。回归树不在本文的讨论范围之内。</p><p id="0b59" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">注意:CART是一个二叉树，其中每个节点只分为两个分支。CHAID可以从一个节点有两个以上的分裂点击这里了解更多。</p><h2 id="eb3c" class="lt jo hh bd jp lu lv lw jt lx ly lz jx ko ma mb jz ks mc md kb kw me mf kd mg bi translated">基尼指数计算</h2><p id="0b84" class="pw-post-body-paragraph kf kg hh kh b ki kj ii kk kl km il kn ko kp kq kr ks kt ku kv kw kx ky kz la ha bi translated">基尼系数是一种基于杂质的标准，用于衡量目标属性值的概率分布之间的差异。基尼系数决定了一个特定的特征是否增加了模型的可预测性。基尼系数越高，变量的预测能力越低。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mh"><img src="../Images/2d37f6cffd1c62370942cef59af84f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*14GB_PHhFvMs-F9AoPl3Qg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="cc2a" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">接近0的基尼系数表示分类的纯粹性，较高的基尼系数表示类别的随机分布。这里<strong class="kh hi"> <em class="mi"> m </em> </strong> <em class="mi"> </em>是类的数量，而<strong class="kh hi"> <em class="mi"> Pi </em> </strong>是记录被分类到特定类的概率。</p><p id="d7d3" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">现在让我们用下面的数据来计算基尼指数</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mj"><img src="../Images/1ba1aaaf47409cab2fc871a83b33d324.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*Fm5l6qbNm3UWmWBhd3oHqg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="8169" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">我们的目的是找到哪个特征更适合于分裂<strong class="kh hi"> <em class="mi">性别</em> </strong>或<strong class="kh hi"> <em class="mi">教育</em> </strong>以分类该人是否从事任何运动。我们将分别计算这两列的基尼系数。让我们首先考虑性别列，拆分后我们得到以下流程图:</p><p id="8ab7" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">让我们计算数据集的基尼系数；</p><p id="712c" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">我们有2个类(0和1)；</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mk"><img src="../Images/f6cddea80a388dcbeeec0502fafdb216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*A-RuuSTZNesDOixs3BHtIw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="6944" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">下一步是计算2个特征(性别和教育)的基尼系数，并决定哪个特征将被考虑分割。</p><p id="013f" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">对于性别:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ml"><img src="../Images/a84b3282e4dd4384f0fede56a7e14fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*Zjk-Ly1xYbkX8QtCb0hQqw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mm"><img src="../Images/5b41fcd3673a428f2b7be3e73e5ac138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SzoOfrlt0bI5Eecpn-FDg.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="a1e0" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">我们将通过加权两个分支的杂质和该分支中的元素数量来计算性别的基尼系数。<strong class="kh hi">杂质测量值为0.45，因为我们有45%的时间会错误地标注数据。</strong></p><p id="7999" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">对于教育:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mn"><img src="../Images/2d1f288cfd59cf0d39ad49887bbf457e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*iQHAPh4ER1U7gi5_yWc-ig.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mo"><img src="../Images/0a72d6865629451fd6186ad1362ec7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xiShd5DVDeL7IxaGH6-iw.png"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">image by author</figcaption></figure><p id="d9a2" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">我们将通过对两个分支的不纯度和该分支中的元素数量进行加权来计算教育的基尼不纯度。<strong class="kh hi">杂质测量值为0.16，因为我们大约有16%的时间会错误地标注数据。</strong></p><p id="2d70" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">最后一步是计算两个特征的基尼系数:</p><p id="6625" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><em class="mi">基尼系数(性别)= 0.48–0.45 = 0.03</em></p><p id="e0d4" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><em class="mi">基尼系数(教育)= 0.48–0.16 = 0.32</em></p><p id="871a" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><strong class="kh hi"> <em class="mi">【基尼增益(教育)</em></strong><em class="mi">&gt;</em><strong class="kh hi"><em class="mi">【基尼增益(性别)</em> </strong></p><p id="ca3a" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">我们将考虑<em class="mi">教育</em>而不是<em class="mi">性别</em>来分割根节点的数据，因为教育领域的基尼收益大于性别领域。</p><p id="6332" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><em class="mi">最大化基尼增益</em>的特征被考虑用于分裂，决策树在计算基尼增益后递归分裂特征。</p><p id="1e61" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><strong class="kh hi">限制</strong></p><ul class=""><li id="2bd5" class="mp mq hh kh b ki lb kl lc ko mr ks ms kw mt la mu mv mw mx bi translated">DTs容易过度拟合:我们执行<em class="mi">修剪</em>来消除过度拟合。</li><li id="5d2b" class="mp mq hh kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">DT是一种贪婪算法:当使用最佳自变量进行拆分时，模型不会考虑未来状态，如果选择不同的自变量而不是最佳自变量，模型会具有更高的准确性。为了克服这一点，我们执行<em class="mi">交叉验证。</em></li></ul><p id="3c95" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">爱情！活下去！笑！干杯</p><p id="9bac" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">参考:</p><p id="c5fc" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated"><a class="ae jm" href="https://www.researchgate.net/publication/225237661_Decision_Trees" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/225237661 _ Decision _ Trees</a></p><p id="3bd5" class="pw-post-body-paragraph kf kg hh kh b ki lb ii kk kl lc il kn ko ld kq kr ks le ku kv kw lf ky kz la ha bi translated">更多文章:</p><div class="nd ne ez fb nf ng"><a rel="noopener follow" target="_blank" href="/geekculture/clustering-made-easy-1fe01dd8048f"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">集群变得简单</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">使聚集</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">让Clusteringmedium.com变得容易</p></div></div><div class="np l"><div class="nq l nr ns nt np nu jg ng"/></div></div></a></div><div class="nd ne ez fb nf ng"><a rel="noopener follow" target="_blank" href="/geekculture/principal-component-analysis-3d2b3a0bb93e"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">主成分分析</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">一种选择最重要特征的方法</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">medium.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu jg ng"/></div></div></a></div><div class="nd ne ez fb nf ng"><a rel="noopener follow" target="_blank" href="/geekculture/things-to-remember-in-deep-learning-eca746ed29c8"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hi fi z dy nl ea eb nm ed ef hg bi translated">深度学习中需要记住的事情</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">深度学习之旅</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">medium.com</p></div></div><div class="np l"><div class="nw l nr ns nt np nu jg ng"/></div></div></a></div></div></div>    
</body>
</html>