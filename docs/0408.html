<html>
<head>
<title>FIZZOG(Comprehensive guide): End to end Emotion detection Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FIZZOG(综合指南):端到端情感检测项目</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/fizzog-comprehensive-guide-end-to-end-emotion-detection-project-e4400626cc7c?source=collection_archive---------0-----------------------#2021-04-07">https://medium.com/mlearning-ai/fizzog-comprehensive-guide-end-to-end-emotion-detection-project-e4400626cc7c?source=collection_archive---------0-----------------------#2021-04-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="ab22" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">人工情绪智能是人工智能的一个分支，它允许计算机理解人类的非语言线索，如T2的肢体语言和面部表情。</p></blockquote><p id="417e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><em class="ij">这个项目的目的是通过建立、训练和部署一个系统来根据人们的<br/>面部图像对人们的情绪进行分类，该系统可以自动监控超过20000张面部图像及其相关的面部表情标签以及大约2000张图像及其</em> <a class="ae jj" href="https://drive.google.com/drive/folders/1SClbp3CHBAADPgsnnbleWoSh2CUoGLdW?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <em class="ij">数据集中的面部关键点注释。</em> </a></p><p id="bbb2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">完整代码:<a class="ae jj" href="https://github.com/astha77-bot/Fizzog.git" rel="noopener ugc nofollow" target="_blank">https://github.com/astha77-bot/Fizzog.git</a></p><p id="dbc7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第1部分:</strong>我们将基于卷积<br/>神经网络和残差块来创建深度学习模型，以预测面部关键点，其中数据集由15个面部关键点的x和y坐标以及96×96像素的图像组成。</p><p id="35f2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第二部分:</strong>第二个模型会对人的情绪进行分类。数据包含属于5个类别的图像:<br/> 0 =生气<br/> 1 =厌恶<br/> 2 =悲伤<br/> 3 =高兴<br/> 4 =惊讶</p><p id="628d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第三部分:</strong>结合人脸关键点检测和人脸表情模型。</p><p id="36e2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第4部分</strong>:部署两个训练好的模型。(<em class="ij">项目部署部分可从后续文章</em>中阅读。)</p><h1 id="de6b" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak"> <em class="ki">第一部分:面部关键点检测</em> </strong></h1><p id="fee2" class="pw-post-body-paragraph ih ii hh ik b il kj in io ip kk ir is jg kl iv iw jh km iz ja ji kn jd je jf ha bi translated">我们将基于卷积<br/>神经网络和残差块来创建深度学习模型，以预测面部关键点，其中数据集由15个面部关键点的x和y坐标组成，图像为96×96像素。</p><p id="9509" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤1 </strong> : <strong class="ik hi">导入数据集和库</strong></p><p id="75f3" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">导入数据集(data.csv)和必要的库，并根据需要对其进行更改，以便进一步推进项目。</p><p id="2b06" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里，由于图像以空格分隔的字符串形式出现在最后一列，我们必须使用分隔符分隔值，然后使用np.fromstring将其转换为numpy数组，并从1D数组转换为shape (96，96)的2D数组。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="6be6" class="kx jl hh kt b fi ky kz l la lb">keyfacial_df[‘Image’] = keyfacial_df[‘Image’].apply(lambda x: np.fromstring(x, dtype = int, sep = ‘ ‘).reshape(96, 96))</span></pre><p id="294a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第二步:图像可视化</strong></p><p id="e6d2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">我们需要数据可视化，因为可视化的信息汇总比查看电子表格中的数千行更容易识别模式和趋势。因此，我们从数据集中随机绘制了一组图像以及面部关键点。</p><p id="5b91" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">图像数据从df['Image']获得，并使用plt.imshow绘制，其中图像中相应的关键面部点有15个x和y坐标。因为x坐标在偶数列中，如0，2，4，..y坐标在奇数列中，如1，3，5，范围增加到31。我们使用。loc命令，该命令根据它所引用的列获取图像的坐标值，其中rx表示红色的叉号。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="1adb" class="kx jl hh kt b fi ky kz l la lb">import random<br/>fig = plt.figure(figsize=(10, 10))</span><span id="f603" class="kx jl hh kt b fi lc kz l la lb">for i in range(64):<br/> k=random.randint(1,len(keyfacial_df))<br/> ax = fig.add_subplot(8,8,i+1) <br/> image = plt.imshow(keyfacial_df[‘Image’][k],cmap = ‘gray’)<br/> for j in range(1,31,2):<br/> plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], ‘rx’)</span></pre><figure class="ko kp kq kr fd le er es paragraph-image"><div class="er es ld"><img src="../Images/624475c7a204ce2c3c8012eb8ce56a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*q7x-MeuaKQpj-cZWIXbUqA.png"/></div></figure><p id="39d2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤3:图像增强</strong></p><p id="be4f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">图像增强是构建卷积神经网络的一种有用技术，它可以在不获取新图像的情况下增加训练集的大小。这个想法很简单；复制带有某种变化的图像，以便模型可以从更多的示例中学习。理想情况下，我们可以以某种方式增强图像，保留对做出预测至关重要的特征，但重新排列像素足以增加一些噪声。</p><p id="3f03" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">对于那些使用Keras的人来说,“ImageDataGenerator”中有一组方便的参数，可以很容易地放大图像。使用Keras函数的缺点是用户不能准确指定要扩充什么类，而且扩充选项有限。</p><p id="4cf4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">在这里，我们将通过翻转图像以及降低亮度来扩充数据，然后连接到我们的数据集。</p><p id="95f9" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> i)水平翻转图像</strong></p><p id="9ad4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里我们将水平翻转我们的图像，因此Y坐标将是相同的，只有x坐标值会改变，我们所要做的就是从图像的宽度中减去我们的初始x坐标值(这里96是图像宽度的尺寸)。)</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e050" class="kx jl hh kt b fi ky kz l la lb">keyfacial_df_copy[‘Image’] = keyfacial_df_copy[‘Image’].apply(lambda x: np.flip(x, axis = 1))</span><span id="babc" class="kx jl hh kt b fi lc kz l la lb">for i in range(len(columns)):<br/> if i%2 == 0:<br/> keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. — float(x) )</span></pre><figure class="ko kp kq kr fd le er es paragraph-image"><div class="er es lh"><img src="../Images/2f614afc6c16cd9ca73a9220860193c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*S61iflTfCNcHt8im3as23A.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx">Horizontally Flipped image</figcaption></figure><p id="a01a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">将原始数据帧与扩充数据帧连接起来。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="ae75" class="kx jl hh kt b fi ky kz l la lb">augmented_df = np.concatenate((keyfacial_df, keyfacial_df_copy))</span></pre><p id="eb0e" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi"> ii)增加图像的亮度</strong></p><p id="c2cc" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">这里，我们将像素值乘以1.5到2之间的随机值，以增加图像的亮度，我们截取0到255之间的值，即应该在0到255之间，因为如果x=200，然后随机选择2，则2*200&gt;255是不可接受的。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="69e0" class="kx jl hh kt b fi ky kz l la lb">import random</span><span id="ca97" class="kx jl hh kt b fi lc kz l la lb">keyfacial_df_copy = copy.copy(keyfacial_df)<br/>keyfacial_df_copy[‘Image’] = keyfacial_df_copy[‘Image’].apply(lambda x:np.clip(random.uniform(1.5, 2)* x, 0.0, 255.0))<br/>augmented_df = np.concatenate((augmented_df, keyfacial_df_copy))<br/>augmented_df.shape</span><span id="c480" class="kx jl hh kt b fi lc kz l la lb">plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')<br/>for j in range(1, 31, 2):<br/>        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')</span></pre><figure class="ko kp kq kr fd le er es paragraph-image"><div class="er es lh"><img src="../Images/f0889a17c8db1a2dd664729d53370908.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*j8l_DyftcMF3fD93GwYLWg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx">Image with increased brightness</figcaption></figure><p id="b5b1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第四步:数据归一化和训练数据准备</strong></p><p id="dd80" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">标准化的目的是以无量纲和/或具有相似分布的方式转换数据。这种标准化过程被称为其他名称，如标准化、特征缩放等。在任何机器学习应用和模型拟合中，归一化都是数据预处理的必要步骤。</p><p id="12af" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">假设我们有一个包含两个变量的数据集:行驶时间和行驶距离。时间以小时为单位(例如5、10、25小时)，速度以英里为单位(例如80、120、150公里/小时)。你看出问题了吗？</p><p id="d712" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">当然，一个明显的问题是，这两个变量是用两种不同的单位来衡量的——一个是小时，另一个是英里。另一个问题是数据的分布，这个问题并不明显，但如果你仔细观察就会发现，这两个变量(变量内和变量间)的数据分布非常不同。</p><p id="489a" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">因此，标准化赋予每个变量同等的权重/重要性，这样就不会有单个变量仅仅因为它们是较大的数字就将模型性能导向一个方向。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="6ee3" class="kx jl hh kt b fi ky kz l la lb"># Obtain the value of images which is present in the 31st column (since index start from 0, we refer to 31st column by 30)<br/>img = augmented_df[:,30]</span><span id="2c69" class="kx jl hh kt b fi lc kz l la lb"># Normalize the images<br/>img = img/255.</span><span id="b5f3" class="kx jl hh kt b fi lc kz l la lb"># Create an empty array of shape (x, 96, 96, 1) to feed the model<br/>X = np.empty((len(img), 96, 96, 1))</span><span id="c1d9" class="kx jl hh kt b fi lc kz l la lb"># Iterate through the img list and add image values to the empty array after expanding it’s dimension from (96, 96) to (96, 96, 1)<br/>for i in range(len(img)):<br/> X[i,] = np.expand_dims(img[i], axis = 2)</span><span id="7656" class="kx jl hh kt b fi lc kz l la lb"># Convert the array type to float32<br/>X = np.asarray(X).astype(np.float32)<br/></span><span id="1876" class="kx jl hh kt b fi lc kz l la lb">X.shape</span></pre><p id="14d8" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi">(6420, 96, 96, 1)</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="dc2a" class="kx jl hh kt b fi ky kz l la lb"># Obtain the value of x &amp; y coordinates which are to used as target.<br/>y = augmented_df[:,:30]<br/>y = np.asarray(y).astype(np.float32)<br/>y.shape</span></pre><p id="bbd7" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi">(6420, 30)</p><p id="cb10" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">然后将数据拆分为训练和测试数据。</p><p id="e7ec" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated">步骤5:理解神经网络、梯度下降和resnets。</p><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/@pierre_guillou/understand-how-works-resnet-without-talking-about-residual-64698f157e0c"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">理解Resnet的工作原理…而不谈论残差</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">我对我找到的所有关于Resnet的博客帖子都99%满意(例如伟大的博客帖子“解码ResNet…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md lf lp"/></div></div></a></div><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">剩余网络的理解和实现</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">剩余学习框架简化了比以前使用的网络更深入的网络训练。</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="me l ma mb mc ly md lf lp"/></div></div></a></div><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/swlh/resnet-a-simple-understanding-of-the-residual-networks-bfd8a1b4a447"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">ResNet:对剩余网络的简单理解</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">一个全面的指南，以了解ResNets(剩余网络)的开始，以及他们如何帮助解决一个…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="mf l ma mb mc ly md lf lp"/></div></div></a></div><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/yottabytes/everything-you-need-to-know-about-gradient-descent-applied-to-neural-networks-d70f85e0cc14"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">你需要知道的关于梯度下降应用于神经网络的一切</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">解释//版本//算法步骤//优化技术</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="mg l ma mb mc ly md lf lp"/></div></div></a></div><p id="34a4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第六步:建立深度残差神经网络关键人脸检测模型</strong></p><figure class="ko kp kq kr fd le er es paragraph-image"><div class="er es mh"><img src="../Images/24a6f50fcf5fd81cd539aa1eceee9393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*xLAV2UdgCbLj5x4uvmWVzQ.jpeg"/></div></figure><figure class="ko kp kq kr fd le er es paragraph-image"><div class="er es mi"><img src="../Images/70e25738c2f461cd054f5eb15f75374c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*Pq6F7ghh3pyL_SnErCTDHQ.jpeg"/></div></figure><div class="ko kp kq kr fd ab cb"><figure class="mj le mk ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><img src="../Images/25bb41666a59349f47a136ea8f0eca38.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*m3aQ0pgt3bohMtBCSpqppQ.jpeg"/></div></figure><figure class="mj le mt ml mm mn mo paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><img src="../Images/22ea246202ed6d7e64a9389e47e27b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*LZntFItCR_ss6x15J6CkXA.jpeg"/></div></figure></div><p id="540d" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第七步:编译训练面部关键点检测深度学习模型</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="14e5" class="kx jl hh kt b fi ky kz l la lb">adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)</span><span id="6138" class="kx jl hh kt b fi lc kz l la lb">model_1_facialKeyPoints.compile(loss = “mean_squared_error”, optimizer = adam , metrics = [‘accuracy’])</span><span id="2231" class="kx jl hh kt b fi lc kz l la lb"># Check this out for more information on Adam optimizer: <a class="ae jj" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam</a></span></pre><p id="db46" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤8:评估经过训练的关键面部点检测模型的性能</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e4d9" class="kx jl hh kt b fi ky kz l la lb">with open(‘detection.json’, ‘r’) as json_file:</span><span id="ea73" class="kx jl hh kt b fi lc kz l la lb">json_savedModel= json_file.read()</span><span id="5a09" class="kx jl hh kt b fi lc kz l la lb"># load the model architecture</span><span id="49ad" class="kx jl hh kt b fi lc kz l la lb">model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)</span><span id="9a8c" class="kx jl hh kt b fi lc kz l la lb">model_1_facialKeyPoints.load_weights(‘weights_keypoint.hdf5’)</span><span id="d045" class="kx jl hh kt b fi lc kz l la lb">adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)</span><span id="c818" class="kx jl hh kt b fi lc kz l la lb">model_1_facialKeyPoints.compile(loss=”mean_squared_error”, optimizer= adam , metrics = [‘accuracy’])</span></pre><p id="bcc2" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">评估模型</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="81f9" class="kx jl hh kt b fi ky kz l la lb">result = model_1_facialKeyPoints.evaluate(X_test, y_test)</span><span id="5b78" class="kx jl hh kt b fi lc kz l la lb">print(“Accuracy : {}”.format(result[1]))</span></pre><h1 id="b1e9" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">第二部分:</strong>面部表情模型</h1><p id="a870" class="pw-post-body-paragraph ih ii hh ik b il kj in io ip kk ir is jg kl iv iw jh km iz ja ji kn jd je jf ha bi translated">第二个模型将对人们的情绪进行分类。数据包含属于5个类别的图像:<br/> 0 =生气<br/> 1 =厌恶<br/> 2 =悲伤<br/> 3 =高兴<br/> 4 =惊讶</p><p id="c1f4" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤1 </strong>:导入并浏览面部表情检测数据集。</p><p id="538f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤2 </strong>:可视化图像和绘图标签。</p><p id="1517" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第三步</strong>:进行数据准备和数据扩充。</p><p id="5c51" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第四步</strong>:建立和训练面部表情分类的深度学习模型</p><p id="7869" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤5 </strong>:了解如何评估分类器模型(混淆矩阵、准确度、精确度和召回率)</p><div class="lm ln ez fb lo lp"><a href="https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">如何计算非平衡分类的精度、召回率和F值-机器学习…</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">分类准确度是正确预测的总数除以为…做出的预测的总数</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ly l"><div class="mu l ma mb mc ly md lf lp"/></div></div></a></div><div class="lm ln ez fb lo lp"><a href="https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hi fi z dy lu ea eb lv ed ef hg bi translated">你应该知道的关于机器学习混淆矩阵的一切</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">混乱矩阵-不那么混乱！您是否遇到过这样的情况，您期望您的机器学习模型能够…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="ly l"><div class="mv l ma mb mc ly md lf lp"/></div></div></a></div><p id="abd5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">步骤6 </strong>:评估训练好的面部表情分类模型的性能。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="7d83" class="kx jl hh kt b fi ky kz l la lb">with open(‘emotion.json’, ‘r’) as json_file:</span><span id="431b" class="kx jl hh kt b fi lc kz l la lb">json_savedModel= json_file.read()</span><span id="59c2" class="kx jl hh kt b fi lc kz l la lb"># load the model architecture</span><span id="abb5" class="kx jl hh kt b fi lc kz l la lb">model_2_emotion = tf.keras.models.model_from_json(json_savedModel)</span><span id="4d4f" class="kx jl hh kt b fi lc kz l la lb">model_2_emotion.load_weights(‘weights_emotions.hdf5’)</span><span id="0eef" class="kx jl hh kt b fi lc kz l la lb">model_2_emotion.compile(optimizer = “Adam”, loss = “categorical_crossentropy”, metrics = [“accuracy”])</span><span id="1611" class="kx jl hh kt b fi lc kz l la lb">score = model_2_emotion.evaluate(X_Test, y_Test)</span><span id="26a9" class="kx jl hh kt b fi lc kz l la lb">print(‘Test Accuracy: {}’.format(score[1]))</span></pre><p id="7d60" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is jg iu iv iw jh iy iz ja ji jc jd je jf ha bi translated"><strong class="ik hi">第三部分:</strong>结合人脸关键点检测和人脸表情模型。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="04b8" class="kx jl hh kt b fi ky kz l la lb">def predict(X_test):</span><span id="9fbc" class="kx jl hh kt b fi lc kz l la lb"># Making prediction from the keypoint model</span><span id="5a1a" class="kx jl hh kt b fi lc kz l la lb">df_predict = model_1_facialKeyPoints.predict(X_test)</span><span id="3fe5" class="kx jl hh kt b fi lc kz l la lb"># Making prediction from the emotion model</span><span id="64da" class="kx jl hh kt b fi lc kz l la lb">df_emotion = np.argmax(model_2_emotion.predict(X_test), axis=-1)</span><span id="734e" class="kx jl hh kt b fi lc kz l la lb"># Reshaping array from (856,) to (856,1)</span><span id="6509" class="kx jl hh kt b fi lc kz l la lb">df_emotion = np.expand_dims(df_emotion, axis = 1)</span><span id="941f" class="kx jl hh kt b fi lc kz l la lb"># Converting the predictions into a dataframe</span><span id="9207" class="kx jl hh kt b fi lc kz l la lb">df_predict = pd.DataFrame(df_predict, columns= columns)</span><span id="eb1f" class="kx jl hh kt b fi lc kz l la lb"># Adding emotion into the predicted dataframe</span><span id="49b3" class="kx jl hh kt b fi lc kz l la lb">df_predict[‘emotion’] = df_emotion</span><span id="6014" class="kx jl hh kt b fi lc kz l la lb">return df_predict</span></pre><h1 id="c599" class="jk jl hh bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">第四部分</strong>:部署两个训练好的模型。</h1><p id="7022" class="pw-post-body-paragraph ih ii hh ik b il kj in io ip kk ir is jg kl iv iw jh km iz ja ji kn jd je jf ha bi translated"><em class="ij">(项目部署部分可从后续文章中阅读。那里见！)</em></p></div></div>    
</body>
</html>