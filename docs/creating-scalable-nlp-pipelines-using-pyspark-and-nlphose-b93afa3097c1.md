# ä½¿ç”¨ PySpark å’Œ Nlphose åˆ›å»ºå¯ä¼¸ç¼©çš„ NLP ç®¡é“

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/creating-scalable-nlp-pipelines-using-pyspark-and-nlphose-b93afa3097c1?source=collection_archive---------6----------------------->

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä½¿ç”¨[NLP hoste](https://github.com/code2k13/nlphose)å’Œ Pyspark æ¥æ‰§è¡Œ NLP ç®¡é“ï¼Œå¹¶æ”¶é›†å…³äºå„’å‹’Â·å‡¡å°”çº³çš„è‘—ä½œã€Š80 å¤©ç¯æ¸¸ä¸–ç•Œã€‹ä¸­çš„è‘—åæ—…ç¨‹çš„ä¿¡æ¯ã€‚è¿™é‡Œæ˜¯æœ¬æ–‡ä¸­ä½¿ç”¨çš„â¬‡ï¸ [Pyspark ç¬”è®°æœ¬çš„é“¾æ¥ã€‚](https://github.com/code2k13/nlphose/blob/main/Nlphose_Pyspark.ipynb)

![](img/47370a9c399940216f6062427501617f.png)

æ ¹æ®æˆ‘çš„ä¸ªäººç»éªŒï¼Œæˆ‘å‘ç°ä»éç»“æ„åŒ–æ•°æ®ä¸­æŒ–æ˜æ•°æ®éœ€è¦ä½¿ç”¨å¤šç§æŠ€æœ¯ã€‚æ²¡æœ‰ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹æˆ–åº“å¯ä»¥æä¾›æ‚¨éœ€è¦çš„ä¸€åˆ‡ã€‚é€šå¸¸ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ç”¨ä¸åŒç¼–ç¨‹è¯­è¨€/æ¡†æ¶ç¼–å†™çš„ç»„ä»¶ã€‚è¿™å°±æ˜¯æˆ‘çš„å¼€æºé¡¹ç›® [Nlphose](https://github.com/code2k13/nlphose) å‡ºç°çš„åœ°æ–¹ã€‚Nlphose æ”¯æŒä½¿ç”¨ä¸€ç»„ç®€å•çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œåœ¨å‡ ç§’é’Ÿå†…åˆ›å»ºå¤æ‚çš„ NLP ç®¡é“ï¼Œç”¨äºå¤„ç†é™æ€æ–‡ä»¶æˆ–æµæ–‡æœ¬ã€‚æ‚¨å¯ä»¥åœ¨ç»ˆç«¯ä¸­æ‰§è¡Œå•ä¸ªå‘½ä»¤ï¼Œå¯¹æ–‡æœ¬æ‰§è¡Œå¤šç§æ“ä½œï¼Œå¦‚ NERã€æƒ…æ„Ÿåˆ†æã€åˆ†å—ã€è¯­è¨€è¯†åˆ«ã€Q & Aã€0-shot åˆ†ç±»ç­‰ã€‚Spark æ˜¯å¹¿æ³›ä½¿ç”¨çš„å¤§æ•°æ®å¤„ç†å·¥å…·ï¼Œå¯ç”¨äºå¹¶è¡ŒåŒ–å·¥ä½œè´Ÿè½½ã€‚

![](img/8d37ff8f4695e8585d2effabc7938664.png)

[Nlphose](https://github.com/code2k13/nlphose) åŸºäºâ€œ [Unix å·¥å…·å“²å­¦](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/c1089.htm)â€ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯åˆ›é€ ç®€å•çš„å·¥å…·ï¼Œå®ƒä»¬å¯ä»¥ä¸€èµ·å·¥ä½œæ¥å®Œæˆå¤šé¡¹ä»»åŠ¡ã€‚Nlphose è„šæœ¬ä¾é æ ‡å‡†çš„â€œæ–‡ä»¶æµâ€æ¥è¯»å†™æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç®¡é“è¿æ¥åœ¨ä¸€èµ·ï¼Œä»¥åˆ›å»ºå¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†ç®¡é“ã€‚æ¯ä¸ªè„šæœ¬ä»â€œæ ‡å‡†è¾“å…¥â€è¯»å– JSONï¼Œå¹¶å†™å…¥â€œæ ‡å‡†è¾“å‡ºâ€ã€‚æ‰€æœ‰çš„è„šæœ¬éƒ½æœŸæœ› JSON ä»¥ nlphose å…¼å®¹æ ¼å¼ç¼–ç ï¼Œå¹¶ä¸”è¾“å‡ºä¹Ÿéœ€è¦æ˜¯ nlphose å…¼å®¹çš„ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»å…³äº NLP hose[æ¶æ„çš„æ›´å¤šç»†èŠ‚ã€‚](https://github.com/code2k13/nlphose/wiki/Architecture-of-nlphose)

# Nlphose æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

Nlphose åœ¨è®¾è®¡ä¸Šä¸åƒ SparkML é‚£æ ·æ”¯æŒ Spark/Pysparkã€‚Pyspark ä¸­çš„ Nlphose ä¾èµ–äºå®‰è£…åœ¨ spark é›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„â€œDockerâ€ã€‚å½“åœ¨ [Google Dataproc](https://cloud.google.com/dataproc/) ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„é›†ç¾¤æ—¶ï¼Œè¿™æ˜¯å¾ˆå®¹æ˜“åšåˆ°çš„(å¯¹äºä»»ä½•å…¶ä»– Spark åˆ†å¸ƒä¹Ÿåº”è¯¥æ˜¯ä¸€æ ·çš„)ã€‚é™¤äº† Docker ä¹‹å¤–ï¼ŒNlphose ä¸éœ€è¦åœ¨ Spark é›†ç¾¤çš„å·¥ä½œèŠ‚ç‚¹ä¸Šå®‰è£…ä»»ä½•å…¶ä»–ä¾èµ–é¡¹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢æè¿°çš„â€œPipeLineExecutorâ€ç±»ä» Pyspark ä¸­æ‰§è¡Œ Nlphose ç®¡é“ã€‚

åœ¨å¹•åï¼Œè¿™ä¸ªç±»ä½¿ç”¨â€œ[å­è¿›ç¨‹](https://docs.python.org/3/library/subprocess.html)æ¨¡å—ä¸ºä¸€ä¸ª Spark ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæ–°çš„ docker å®¹å™¨ï¼Œå¹¶æ‰§è¡Œä¸€ä¸ª Nlphose ç®¡é“ã€‚ä½¿ç”¨ stdout å’Œ stdin æ‰§è¡Œ I/Oã€‚è¿™å¬èµ·æ¥å¾ˆç®€å•ï¼Œä½†è¿™å°±æ˜¯æˆ‘æ„å»º Nlphose çš„æ–¹å¼ã€‚å®ƒä»æœªè¢«è®¾æƒ³ä¸ºæ”¯æŒä»»ä½•ç‰¹å®šçš„è®¡ç®—æ¡†æ¶æˆ–åº“ã€‚æ‚¨å¯ä»¥ç”¨è®¸å¤šä¸åŒçš„æ–¹å¼è¿è¡Œ Nlphoseï¼Œè¿™é‡Œå°†ä»‹ç»å…¶ä¸­ä¸€ç§æ–¹å¼ã€‚

# æˆ‘ä»¬å¼€å§‹å§

é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£…ä¸€ä¸ªåŒ…ï¼Œç¨åæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥æ„å»ºä¸€ä¸ªå¯è§†åŒ–ã€‚

ï¼pip å®‰è£…å­—æ•°

ä¸‹é¢çš„å‘½ä»¤ä» gutenber.org çš„[ä¸‹è½½ç”µå­ä¹¦ã€Š80 å¤©ç¯æ¸¸ä¸–ç•Œã€‹ï¼Œå¹¶ä½¿ç”¨ Nlphose æä¾›çš„å·¥å…·å°†å•ä¸ªæ–‡æœ¬æ–‡ä»¶åˆ†å‰²æˆè¡Œåˆ†éš”çš„ jsonã€‚æ¯ä¸ª json å¯¹è±¡éƒ½æœ‰ä¸€ä¸ªâ€œidâ€å’Œâ€œtextâ€å±æ€§ã€‚](https://ashishware.com/2022/01/23/PysparkNlphose/gutenber.org)

```
!docker run code2k13/nlphose:latest \
/bin/bash -c â€œwget [https://www.gutenberg.org/files/103/103-0.txt](https://www.gutenberg.org/files/103/103-0.txt) && ./file2json.py 103â€“0.txt -n 2â€ > ebook.json
```

æˆ‘ä»¬åˆ é™¤æ‰€æœ‰ä¸å†éœ€è¦çš„ docker å®¹å™¨ã€‚

```
!docker system prune -f
```

è®©æˆ‘ä»¬å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ã€‚è¯»å–æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„ json æ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º pandas æ•°æ®æ¡†ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªâ€œgroup_idâ€åˆ—æ·»åŠ åˆ°è¡Œä¸­ï¼Œè¯¥åˆ—ä» 0 åˆ° 3 ä¹‹é—´éšæœºåˆ†é…ä¸€ä¸ª groupIdã€‚å®Œæˆåï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ PySpark æ•°æ®æ¡†æ¶å¹¶æ˜¾ç¤ºä¸€äº›ç»“æœã€‚

```
import pandas as pd
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql.functions import desc
from pyspark.sql.functions import udfdf_pd = pd.read_json(â€œebook.jsonâ€,lines=True) 
df_pd[â€˜group_idâ€™] = [i for i in range(0,3)]*347
df= spark.createDataFrame(df_pd)
df= spark.createDataFrame(df_pd)+ â€” â€” â€” â€” -+ â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” +
|file_name| id| text|group_id|
+ â€” â€” â€” â€” -+ â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” +
|103â€“0.txt|2bbbfe64â€“7c1e-11eâ€¦| The Project Guteâ€¦| 0|
|103â€“0.txt|2bbea7ea-7c1e-11eâ€¦| Title: Around thâ€¦| 1|
|103â€“0.txt|2bbf2eb8â€“7c1e-11eâ€¦|IN WHICH PHILEAS â€¦| 2|
|103â€“0.txt|2bbfdbd8â€“7c1e-11eâ€¦| Certainly an Engâ€¦| 0|
|103â€“0.txt|2bbff29e-7c1e-11eâ€¦| Phileas Fogg wasâ€¦| 1|
|103â€“0.txt|2bc00734â€“7c1e-11eâ€¦| The way in whichâ€¦| 2|
|103â€“0.txt|2bc02570â€“7c1e-11eâ€¦| He was recommendâ€¦| 0|
|103â€“0.txt|2bc095f0â€“7c1e-11eâ€¦| Was Phileas Foggâ€¦| 1|
|103â€“0.txt|2bc0ed20â€“7c1e-11eâ€¦| Had he travelledâ€¦| 2|
|103â€“0.txt|2bc159d6â€“7c1e-11eâ€¦| It was at least â€¦| 0|
|103â€“0.txt|2bc1a3be-7c1e-11eâ€¦| Phileas Fogg wasâ€¦| 1|
|103â€“0.txt|2bc2a2aa-7c1e-11eâ€¦|He breakfasted anâ€¦| 2|
|103â€“0.txt|2bc2c280â€“7c1e-11eâ€¦| If to live in thâ€¦| 0|
|103â€“0.txt|2bc30b3c-7c1e-11eâ€¦| The mansion in Sâ€¦| 1|
|103â€“0.txt|2bc34dd6â€“7c1e-11eâ€¦| Phileas Fogg wasâ€¦| 2|
|103â€“0.txt|2bc35f88â€“7c1e-11eâ€¦|Fogg would, accorâ€¦| 0|
|103â€“0.txt|2bc3772a-7c1e-11eâ€¦| A rap at this moâ€¦| 1|
|103â€“0.txt|2bc3818e-7c1e-11eâ€¦| â€œThe new servantâ€¦| 2|
|103â€“0.txt|2bc38e0e-7c1e-11eâ€¦| A young man of tâ€¦| 0|
|103â€“0.txt|2bc45c6c-7c1e-11eâ€¦| â€œYou are a Frencâ€¦| 1|
+ â€” â€” â€” â€” -+ â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” + â€” â€” â€” â€” +
```

# ä½¿ç”¨ Pyspark è¿è¡Œ nlphose ç®¡é“

å¦‚å‰æ‰€è¿°ï¼ŒNlphose æ²¡æœ‰ä¸ Pyspark/Spark çš„æœ¬åœ°é›†æˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºâ€œPipeLineExecutorâ€çš„ç±»ï¼Œå®ƒå¯åŠ¨ä¸€ä¸ª docker å®¹å™¨å¹¶æ‰§è¡Œä¸€ä¸ª Nlphose å‘½ä»¤ã€‚è¿™ä¸ªç±»ä½¿ç”¨â€œæ ‡å‡†è¾“å…¥â€å’Œâ€œæ ‡å‡†è¾“å‡ºâ€ä¸ docker å®¹å™¨é€šä¿¡ã€‚æœ€åï¼Œå½“ docker å®¹å™¨å®Œæˆæ‰§è¡Œæ—¶ï¼Œæˆ‘ä»¬æ‰§è¡Œâ€˜docker system prune-fâ€™æ¥æ¸…é™¤ä»»ä½•æœªä½¿ç”¨çš„å®¹å™¨ã€‚â€œexecute_pipelineâ€æ–¹æ³•å°†æ•°æ®ä» dataframe å†™å…¥ stdin(é€è¡Œ)ï¼Œä» stdout è¯»å–è¾“å‡ºï¼Œå¹¶è¿”å›ä»è¾“å‡ºåˆ›å»ºçš„ dataframeã€‚

```
import subprocess
import pandas as pd
import json

class PipeLineExecutor:
  def __init__(self, nlphose_command,data,id_column='id',text_column='text'):
    self.nlphose_command = nlphose_command
    self.id_column = id_column
    self.text_column = text_column
    self.data = data

  def execute_pipeline(self):
    try:
     prune_proc = subprocess.Popen(["docker system prune -f"],shell=True)
     prune_proc.communicate()

     proc = subprocess.Popen([self.nlphose_command],shell=True,stdout=subprocess.PIPE, stdin=subprocess.PIPE,stderr=subprocess.PIPE)
     for idx,row in self.data.iterrows():       
        proc.stdin.write(bytes(json.dumps({"id":row[self.id_column],"text":row[self.text_column]}),"utf8"))
        proc.stdin.write(b"\n")
        proc.stdin.flush()

     output,error = proc.communicate()
     output_str = str(output,'utf-8')
     output_str = output_str
     data = output_str.split("\n")    
     data = [d for d in data if len(d) > 2]
    finally:
        prune_proc = subprocess.Popen(["docker system prune -f"],shell=True)
        prune_proc.communicate()
    return pd.DataFrame(data)
```

# Nlphose å‘½ä»¤

ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œå¤šé¡¹ä»»åŠ¡:

*   å®ƒä½¿ç”¨ dockerhub ä¸­çš„ [code2k13/nlphose:latest](https://hub.docker.com/r/code2k13/nlphose#!) å›¾åƒå¯åŠ¨ docker å®¹å™¨ã€‚
*   å®ƒå°†ä¸»æœºçš„ stdinã€stdout å’Œ stderr é‡å®šå‘åˆ° docker å®¹å™¨ä¸­ã€‚
*   ç„¶åï¼Œå®ƒåœ¨ docker å®¹å™¨ä¸­è¿è¡Œ nlphose å‘½ä»¤ï¼Œè¯¥å‘½ä»¤å¯¹æ¥è‡ªâ€œstdinâ€çš„ json æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Œå¹¶å°†è¾“å‡ºå†™å…¥â€œstdoutâ€:
*   [å®ä½“è¯†åˆ«](https://github.com/code2k13/nlphose/wiki/Name-Entity-Recognition)
*   å¯»æ‰¾â€œä»–ä»¬æºå¸¦äº†ä»€ä¹ˆâ€è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆ[ä½¿ç”¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹](https://github.com/code2k13/nlphose/wiki/Question-Answering)ã€‚

```
command = '''
docker run -a stdin -a stdout -a stderr -i code2k13/nlphose:latest /bin/bash -c â€œ./entity.py |\
./xformer.py â€” pipeline question-answering â€” param â€˜what did they carry?â€™ 
â€œ 
'''
```

ä¸‹é¢çš„å‡½æ•°æ ¼å¼åŒ– PipelineExecutor ä»»åŠ¡è¿”å›çš„æ•°æ®ã€‚â€œpipeline executor . execute _ pipelineâ€è¿”å›çš„ dataframe æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—ï¼Œå…¶ä¸­åŒ…å« Nlphose å‘½ä»¤çš„è¾“å‡ºã€‚æ•°æ®å¸§ä¸­çš„æ¯ä¸€è¡Œä»£è¡¨ Nlphose å‘½ä»¤çš„ä¸€è¡Œ/ä¸€ä¸ªæ–‡æ¡£è¾“å‡ºã€‚

```
def get_answer(row):
  try:
    x =  json.loads(row[0],strict=False)
    row['json_obj'] = json.dumps(x)
    if x['xfrmr_question_answering']['score'] > 0.80:
        row['id'] =  str(x['id'])
        row['answer'] = x['xfrmr_question_answering']['answer']        
    else:
        row['id'] = str(x['id'])
        row['answer'] = None

  except Exception as e:
    row['id'] = None
    row['answer'] = "ERROR " + str(e) #.message
    row['json_obj'] = None

  return row
```

ä¸‹é¢çš„å‡½æ•°åˆ›å»ºä¸€ä¸ªâ€œPipeLineExecutorâ€å¯¹è±¡ï¼Œå°†æ•°æ®ä¼ é€’ç»™å®ƒï¼Œç„¶åå¯¹è¯¥å¯¹è±¡è°ƒç”¨â€œexecute_pipelineâ€æ–¹æ³•ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨â€œget_answerâ€æ–¹æ³•æ¥æ ¼å¼åŒ–â€œexecute_pipelineâ€æ–¹æ³•çš„è¾“å‡ºã€‚

```
def run_pipeline(data):
  nlphose_executor = PipeLineExecutor(command,data,"id","text")
  result = nlphose_executor.execute_pipeline()
  result =  result.apply(get_answer,axis=1)     
  return  result[["id","answer","json_obj"]]
```

# ä½¿ç”¨ PySpark æ‰©å±•ç®¡é“

æˆ‘ä»¬ä½¿ç”¨ PySpark çš„â€œ [applyInPandas](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html) â€æ¥å¹¶è¡ŒåŒ–å’Œå¤§è§„æ¨¡å¤„ç†æ–‡æœ¬ã€‚PySpark è‡ªåŠ¨å¤„ç† Spark é›†ç¾¤ä¸Š Nlphose ç®¡é“çš„ç¼©æ”¾ã€‚å¯¹è¾“å…¥æ•°æ®çš„æ¯ä¸ªâ€œç»„â€è°ƒç”¨â€œrun_pipelineâ€æ–¹æ³•ã€‚é‡è¦çš„æ˜¯æ ¹æ®èŠ‚ç‚¹æ•°é‡è®¾ç½®é€‚å½“æ•°é‡çš„ç»„ï¼Œä»¥ä¾¿åœ¨ Spark é›†ç¾¤ä¸Šæœ‰æ•ˆåœ°å¤„ç†æ•°æ®ã€‚

```
output = df.groupby(â€œgroup_idâ€).applyInPandas(run_pipeline, schema=â€id string,answer string,json_obj stringâ€)
output.cache()
```

# å¯è§†åŒ–æˆ‘ä»¬çš„å‘ç°

ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº† nlphose ç®¡é“çš„æ‰§è¡Œï¼Œæˆ‘ä»¬å°±å¼€å§‹å¯è§†åŒ–æˆ‘ä»¬çš„å‘ç°ã€‚æˆ‘å·²ç»åˆ›å»ºäº†ä¸¤ä¸ªå¯è§†åŒ–:

*   æ˜¾ç¤ºä¹¦ä¸­æåˆ°çš„åœ°æ–¹çš„åœ°å›¾ã€‚
*   äººç‰©æ—…é€”ä¸­æºå¸¦çš„æ‰€æœ‰é‡è¦ç‰©å“çš„å•è¯äº‘ã€‚

# åœ¨ä¸–ç•Œåœ°å›¾ä¸Šæ ‡å‡ºä¹¦ä¸­æœ€å¸¸è§çš„ä½ç½®

ä¸‹é¢çš„ä»£ç ä» Nlphose ç®¡é“ä¸­æå–çº¬åº¦å’Œç»åº¦ä¿¡æ¯ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæœ€å¸¸è§ä½ç½®çš„åˆ—è¡¨ã€‚

> *ğŸ’¡æ³¨æ„:Nlphose å®ä½“æå–å°†ä½¿ç”¨åŸºäºå­—å…¸çš„æ–¹æ³•è‡ªåŠ¨çŒœæµ‹å·²çŸ¥ä½ç½®çš„åæ ‡*

```
def get_latlon2(data):
        json_obj = json.loads(data)
        if 'entities' in json_obj.keys():
            for e in json_obj['entities']:
                if e['label'] == 'GPE' and 'cords' in e.keys():
                    return json.dumps({'data':[e['entity'],e['cords']['lat'],e['cords']['lon']]})
        return None

get_latlon_udf2 = udf(get_latlon2)  
df_locations = output.withColumn("locations",get_latlon_udf2(output["json_obj"]))
top_locations = df_locations.filter("`locations` != 'null'").groupby("locations").count().sort(desc("count")).filter("`count` >= 1")
top_locations.cache() 
top_locations.show()
```

ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨â€œgeopandasâ€åŒ…åœ¨ä¸–ç•Œåœ°å›¾ä¸Šç»˜åˆ¶è¿™äº›ä½ç½®ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ•°æ®å¸§è½¬æ¢æˆâ€œgeopandasâ€èƒ½å¤Ÿç†è§£çš„æ ¼å¼ã€‚è¿™æ˜¯é€šè¿‡åº”ç”¨å‡½æ•°â€˜add _ lat _ longâ€™æ¥å®Œæˆçš„

```
def add_lat_long(row):
     obj =  json.loads(row[0])["data"]
     row["lat"] = obj[1]
     row["lon"] = obj[2]
     return rowimport geopandas

df_locations = top_locations.toPandas()
df_locations = df_locations.apply(add_lat_long,axis=1)

gdf = geopandas.GeoDataFrame(df_locations, geometry=geopandas.points_from_xy(df_locations.lon, df_locations.lat))
world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))
ax = world.plot(color=(25/255,211/255,243/255) ,edgecolor=(25/255,211/255,243/255),
                    linewidth=0.4,edgecolors='none',figsize=(15, 15))
ax.axis('off')   
gdf.plot(ax=ax,alpha=0.5,marker=".",markersize=df_locations['count']*100,color='seagreen')
```

å¦‚æœä½ ç†Ÿæ‚‰è¿™æœ¬ä¹¦ï¼Œä½ ä¼šæ„è¯†åˆ°æˆ‘ä»¬å‡ ä¹æç»˜äº†ç¦æ ¼åœ¨ä»–è‘—åçš„æ—…ç¨‹ä¸­æ‰€èµ°çš„å®é™…è·¯çº¿ã€‚

![](img/c7e8f8614232ff12f293995b2685a293.png)

ä½œä¸ºå‚è€ƒï¼Œä¸‹é¢æ˜¯ä»–ä»ç»´åŸºç™¾ç§‘ä¸Šæˆªå–çš„å®é™…è·¯çº¿çš„å›¾ç‰‡

![](img/53209d7243962798057c59962208ac88.png)

å…«åå¤©ç¯æ¸¸ä¸–ç•Œåœ°å›¾

[Roke](https://commons.wikimedia.org/wiki/File:Around_the_World_in_Eighty_Days_map.png) ï¼Œ [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) ï¼Œé€šè¿‡ç»´åŸºå…±äº«

# åˆ›å»ºä¸€ä¸ªç¦æ ¼æ—…é€”ä¸­æºå¸¦ç‰©å“çš„å•è¯äº‘

ä¸‹é¢çš„ä»£ç ä½¿ç”¨â€œæå–é—®é¢˜å›ç­”â€æ‰¾åˆ°æ—…è¡Œè€…æºå¸¦çš„æœ€å¸¸è§çš„ç‰©å“ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå•è¯äº‘ã€‚

```
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from matplotlib.pyplot import figurefigure(figsize=(12, 6), dpi=120)
wordcloud = WordCloud(background_color=â€™whiteâ€™,width=1024,height=500).generate(â€˜ â€˜.join(output.filter(â€œ`answer` != â€˜nullâ€™â€).toPandas()[â€˜answerâ€™].tolist()))
plt.imshow(wordcloud)
plt.axis(â€œoffâ€)
plt.show()
```

![](img/c30c84e0ea50f33524d0023aff05a4c3.png)

# ç»“è®º

PySpark æ˜¯æ•°æ®ç§‘å­¦å®¶å’Œ ML å®è·µè€…æœ€å–œæ¬¢çš„å·¥å…·çš„åŸå› ä¹‹ä¸€æ˜¯å› ä¸ºä½¿ç”¨æ•°æ®æ¡†æ¶éå¸¸æ–¹ä¾¿ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ PySpark åœ¨ Spark é›†ç¾¤ä¸Šè¿è¡Œ Nlphoseã€‚ä½¿ç”¨æœ¬æ–‡æè¿°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥éå¸¸å®¹æ˜“åœ°å°† Nlphose ç®¡é“ä½œä¸ºæ•°æ®å¤„ç†ç®¡é“çš„ä¸€éƒ¨åˆ†åµŒå…¥ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚éšæ—¶æ¬¢è¿åé¦ˆå’Œè¯„è®ºï¼Œè°¢è°¢ï¼

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)