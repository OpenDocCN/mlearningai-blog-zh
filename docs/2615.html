<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/classification-using-decision-trees-6b01939e01a5?source=collection_archive---------2-----------------------#2022-05-23">https://medium.com/mlearning-ai/classification-using-decision-trees-6b01939e01a5?source=collection_archive---------2-----------------------#2022-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="02b0" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">使用决策树的分类</h2><p id="a04c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">我们尝试使用逻辑回归来解决分类问题。你可以阅读下面的文章:</p><div class="iz ja ez fb jb jc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/classification-using-logistic-regression-6f2927b82c63"><div class="jd ab dw"><div class="je ab jf cl cj jg"><h2 class="bd jh fi z dy ji ea eb jj ed ef jk bi translated">使用逻辑回归分类</h2><div class="jl l"><p class="bd b fp z dy ji ea eb jj ed ef dx translated">medium.com</p></div></div><div class="jm l"><div class="jn l jo jp jq jm jr js jc"/></div></div></a></div><p id="4062" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">一种更受欢迎的解决方案是决策树。决策树是一种分而治之的算法类别，它从整个数据开始，并试图将数据分成更多同类组，这些同类组随后用于预测正确的输出变量。</p><p id="9123" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">分类和回归树(CART)可用于执行分类(如果预测变量是二元的)或回归(如果因变量是连续的)。</p><p id="b4cb" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">用于分类时，CART算法使用Gini之类的杂质指数，这是衡量节点不纯(非同质)程度的指标。CART创建了一个倒排树，其中整个数据集都在节点上。</p><h2 id="d38a" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">决策树算法的步骤</h2><ol class=""><li id="2f48" class="jy jz hh ig b ih ii il im hr ka hv kb hz kc iy kd ke kf kg bi translated">从节点处的完整数据集开始</li><li id="ba97" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">确定杂质的度量(基尼系数或熵)，寻找导致最小杂质的预测变量分裂基。这实质上意味着当使用该变量并将节点一分为二时，将导致杂质的最大减少。</li><li id="5de7" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">重复上述步骤，直到出现以下任何情况:</li></ol><ul class=""><li id="5359" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">所有因变量都用尽了</li><li id="2afb" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">满足停止标准:<br/> -达到树的级别(这是模型创建中的超参数)<br/> -节点中的观察值达到最小决定值(比如5%的观察值)<br/> -或者杂质指数没有进一步减少</li></ul><p id="fa6a" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">4.最后，确定可以使用的节点划分规则</p><p id="63b1" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">让我们以重复为代价，从任何机器学习数据的通用步骤开始:</p><ol class=""><li id="2b9d" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated">读取/加载数据</li><li id="b136" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">分离因变量和自变量</li><li id="0685" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">将数据分为测试集和训练集</li><li id="3e9f" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">运行模型</li></ol><p id="3785" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">此处使用的数据是UCI德国信贷数据。该数据集在<a class="ae kq" href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" rel="noopener ugc nofollow" target="_blank">此处</a> <a class="ae kq" href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/." rel="noopener ugc nofollow" target="_blank">公开发布。</a></p><p id="b4c2" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">但是，为了运行模型，我们将对数据集进行一些更改。<br/> <em class="kr">使用数据的步骤</em>:</p><ol class=""><li id="008c" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated">从上面的链接下载数据</li><li id="b3d0" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">打开excel表格中的数据和数据选项卡-&gt;文本到列-&gt;使用分隔符拆分-&gt;选择空间-&gt;在所需位置将数据保存为CSV文件</li><li id="db1f" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">使用<code class="du ks kt ku kv b">pd.read_csv()</code>加载python中的数据</li><li id="51d9" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">按照<code class="du ks kt ku kv b">base_data_2.columns = [...]</code>中给出的方式创建列名</li><li id="e82e" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">原始数据集具有1，2具有bad_loan和non_bad_loan状态，我们将它们转换为0和1。这只是为了方便使用。</li><li id="2579" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">我们使用选择的列来运行一个截断的模型</li></ol><p id="d767" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated"><em class="kr">运行截断模型的原因:因为一些额外的变量可能存在共线性，所以我已经删除了这些变量，如foreign_worker和job正在提供类似的信息。因此，为了使文章的长度可行，我已经消除了感知共线性。您可以尝试在完整数据集上运行模型，并比较您的结果</em></p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="81f4" class="hf hg hh kv b fi le lf l lg lh"># Read the data<br/>import pandas as pd<br/>import numpy as np<br/>base_data = pd.read_csv("Datasets/german.data.csv",header = None)<br/>#print(base_data_2.head())<br/>#base_data_2 = pd.read_csv("~/Downloads/german.data.csv",header = None)<br/>base_data.columns = ['checkin_acc','duration','credit_history','purpose','amount',<br/>                      'savings_acc','present_emp_since','inst_rate',<br/>                      'personal_status','other_debters','residing_since','property','age','inst_plans','housing',<br/>                      'num_credits','job','dependent_count','telephone','foreign_worker','status']<br/>base_data['status'] = np.where(base_data['status']==1,0,1)<br/>base_data.head()<br/><br/>selected_columns = ['checkin_acc', 'duration', 'credit_history', 'amount', 'savings_acc',<br/>       'present_emp_since', 'inst_rate', 'personal_status', 'residing_since',<br/>       'age', 'inst_plans', 'num_credits', 'job', 'status']<br/>base_data = base_data[selected_columns]<br/><br/>base_data.head()</span></pre><figure class="kw kx ky kz fd li er es paragraph-image"><div class="ab fe cl lj"><img src="../Images/f5d68798fa157a2558a6666084a8c24a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*BoUFfF20brU6lZ_V8dxf8g.png"/></div></figure><p id="cdb7" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">现在我们已经下载/提取了截断的数据集，让我们试着理解这些列的含义。<br/>你可以在这里找到原始数据集<a class="ae kq" href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" rel="noopener ugc nofollow" target="_blank">的描述。<br/>在名为<code class="du ks kt ku kv b">german.doc</code>的文件中。当我们试图优化我们的模型时，最后会有一个表格。</a></p><p id="4eb4" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">让我们首先尝试描述数据，以了解数据中有什么，因为数据中有许多代码，如<code class="du ks kt ku kv b">checckin_acc</code>列中的A11、A12。</p><p id="c4c8" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated"><strong class="ig jh"> <em class="kr">数据集的描述</em> </strong></p><figure class="kw kx ky kz fd li er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ll"><img src="../Images/f8e44e3557cfa5dd15483ca9ec303c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zABTntH5n8DLrriWbWqlww.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx">Data Description of German Credit Data</figcaption></figure><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="fec9" class="hf hg hh kv b fi le lf l lg lh">base_data.info()</span><span id="9d8d" class="hf hg hh kv b fi lu lf l lg lh">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 1000 entries, 0 to 999<br/>Data columns (total 14 columns):<br/> #   Column             Non-Null Count  Dtype <br/>---  ------             --------------  ----- <br/> 0   checkin_acc        1000 non-null   object<br/> 1   duration           1000 non-null   int64 <br/> 2   credit_history     1000 non-null   object<br/> 3   amount             1000 non-null   int64 <br/> 4   savings_acc        1000 non-null   object<br/> 5   present_emp_since  1000 non-null   object<br/> 6   inst_rate          1000 non-null   int64 <br/> 7   personal_status    1000 non-null   object<br/> 8   residing_since     1000 non-null   int64 <br/> 9   age                1000 non-null   int64 <br/> 10  inst_plans         1000 non-null   object<br/> 11  num_credits        1000 non-null   int64 <br/> 12  job                1000 non-null   object<br/> 13  status             1000 non-null   int64 <br/>dtypes: int64(7), object(7)<br/>memory usage: 109.5+ KB</span></pre><p id="8f8c" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">如我们所见，数据集中有7个分类变量和7个数值变量。数据集中总共有1000个数据点。我们的输出变量是状态，它的值是</p><ul class=""><li id="2767" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated"><strong class="ig jh"> 1:信用不良(可能违约)</strong></li><li id="455e" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated"><strong class="ig jh"> 0:信用良好(无违约责任)</strong></li></ul><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="20a0" class="hf hg hh kv b fi le lf l lg lh">#Splitting data into Decision and Independent variables and then encoding the data<br/><br/>X_features = list(base_data.columns)<br/>X_features.remove('status')<br/>X_features</span><span id="ab6c" class="hf hg hh kv b fi lu lf l lg lh">['checkin_acc',<br/> 'duration',<br/> 'credit_history',<br/> 'amount',<br/> 'savings_acc',<br/> 'present_emp_since',<br/> 'inst_rate',<br/> 'personal_status',<br/> 'residing_since',<br/> 'age',<br/> 'inst_plans',<br/> 'num_credits',<br/> 'job']</span><span id="93d1" class="hf hg hh kv b fi lu lf l lg lh">#Conversion of data into dummy variables.<br/>Y_variable = base_data.status<br/>X_variable = pd.get_dummies(base_data[X_features],drop_first = True)<br/>X_variable.head()</span></pre><figure class="kw kx ky kz fd li er es paragraph-image"><div class="ab fe cl lj"><img src="../Images/b24d86b4c3b2b899d3440cfb5c1646c6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jN6Mxq8UNXN6G6D1oDiTFQ.png"/></div></figure><p id="0859" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">正如我们所知，熊猫的<code class="du ks kt ku kv b">get_dummies</code>方法只会对<code class="du ks kt ku kv b">object</code>类型的数据进行编码，而<code class="du ks kt ku kv b">drop_first</code>会从K级分类数据中创建K-1个类别。例如，从上面的数据描述表可以明显看出，我们的<code class="du ks kt ku kv b">checkin_acc</code>有4个级别。然而，在这种情况下<code class="du ks kt ku kv b">get_dummies</code>方法会删除第一个级别。</p><h2 id="25c4" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">构建模型</h2><p id="5ab0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">一旦我们预处理了数据，我们就可以开始建立模型了。我们将把数据分成训练集和测试集，然后用它们来运行决策树分类器。</p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="7c1d" class="hf hg hh kv b fi le lf l lg lh">from sklearn.model_selection import train_test_split<br/>train_X, test_X, train_Y, test_Y = train_test_split(X_variable,Y_variable, test_size = 0.3, random_state = 1)</span></pre><h2 id="801b" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">决策树算法的超参数:</h2><p id="e2e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">与每个模型一样，都有与<code class="du ks kt ku kv b">DecisionTreeClassifier</code>相关的超参数，我们将讨论其中的一些:</p><ol class=""><li id="9c25" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated"><strong class="ig jh">判据(字符串)</strong>:采用杂质判据可用选项有:<code class="du ks kt ku kv b">Gini and entropy</code></li><li id="f4cf" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated"><strong class="ig jh"> max_depth (Integer) </strong>:用于定义树可以取的最大深度。如果什么都没有给出，那么其他超参数如<code class="du ks kt ku kv b">min_samples</code>将覆盖。</li><li id="0c72" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated"><strong class="ig jh">min _ samples(Integer/Float)</strong>:最小样本数，超过该值时不会发生分割。默认值为2(意味着如果一个节点还有2个样本，该节点将不会被进一步分割)。</li></ol><ul class=""><li id="6f0b" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">整数值:表示样本的绝对计数，超过该值，分割停止</li><li id="3748" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">浮点值:表示分裂停止时剩余的总数据集的%</li></ul><ol class=""><li id="da81" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated"><strong class="ig jh"> in_sample_leaf(整数/浮点)</strong>:一个叶节点所需的最小样本数。这意味着如果叶节点(最终节点)有&lt; <code class="du ks kt ku kv b">in_sample_leaf</code>参数，那么叶节点将不会被创建。</li></ol><p id="79ec" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我们现在将只使用2个超参数:<code class="du ks kt ku kv b">criterion</code>和<code class="du ks kt ku kv b">max_depth</code></p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="848b" class="hf hg hh kv b fi le lf l lg lh">from sklearn.tree import DecisionTreeClassifier<br/>clf_tree = DecisionTreeClassifier(criterion = 'gini',max_depth = 3)</span><span id="4626" class="hf hg hh kv b fi lu lf l lg lh">clf_tree.fit(train_X,train_Y)</span><span id="daab" class="hf hg hh kv b fi lu lf l lg lh">DecisionTreeClassifier(max_depth=3)</span></pre><p id="dbf9" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated"><strong class="ig jh">在Jupyter环境中，请重新运行此单元格以显示HTML表示或信任笔记本。在GitHub上，HTML表示无法呈现，请尝试用nbviewer.org加载此页面。</strong></p><p id="786c" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">决策树分类器</p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="3335" class="hf hg hh kv b fi le lf l lg lh">DecisionTreeClassifier(max_depth=3)</span><span id="bdd0" class="hf hg hh kv b fi lu lf l lg lh">from sklearn import metrics<br/>tree_predict = clf_tree.predict(test_X)<br/>metrics.roc_auc_score(test_Y,tree_predict)</span><span id="6e50" class="hf hg hh kv b fi lu lf l lg lh">0.6299174092588569</span></pre><p id="ace9" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我们得到的AUC_SCORE为0.63，比我们使用逻辑回归创建的第一个模型差。点击此处了解更多相关信息:</p><p id="a450" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">让我们看看我们的决策树模型，然后开始评估准确性和其他相关参数。</p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="01f3" class="hf hg hh kv b fi le lf l lg lh">import matplotlib.pyplot as plt</span><span id="4018" class="hf hg hh kv b fi lu lf l lg lh">from sklearn import tree<br/>fig = plt.figure(figsize = (30,20))<br/>tree.plot_tree(clf_tree,feature_names = X_variable.columns,<br/>               class_names = [str(x) for x in np.unique(train_Y)],filled = True,fontsize = 14)<br/>plt.plot()</span><span id="6595" class="hf hg hh kv b fi lu lf l lg lh">[]</span></pre><figure class="kw kx ky kz fd li er es paragraph-image"><div class="ab fe cl lj"><img src="../Images/46404987cde606e0b6c32cb4b5122d05.png" data-original-src="https://miro.medium.com/v2/format:webp/1*xgtLa0ChacnWkBLG55cceg.png"/></div></figure><h2 id="69bd" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">阅读决策树</h2><p id="d9db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">虽然决策树的这种表示可能看起来非常直观，但是有一些隐藏在树中的技术细节，我们也需要理解。这里有几个关键点需要讨论:让我用一张图来解释一下:</p><p id="a869" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">在上面的决策树中可以看到的一些规则有:</p><ol class=""><li id="ad52" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated">如果某人没有支票账户类型A14(记住这是一个分类变量，值为&lt;0.5 means 0 which implies checking account type is NOT A14)</li></ol><ul class=""><li id="beee" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">Then doesn’t have installment plan as A143 (again a categorical variable)</li><li id="27bc" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">The Credit History type is not A34</li><li id="8bde" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">Has a high probability of having a Good Credit Score as this leaf has 92 Good Credits and 1 Bad Credits</li></ul><p id="e4ec" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">Let us look at these parameters from Business Perspective:</p><ul class=""><li id="a85b" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated"><strong class="ig jh"> A14 —没有支票账户</strong>:(在上面的路径中这是假的，表示客户有一个支票账户):意味着客户拥有一个有余额的支票账户。</li><li id="cdf6" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated"><strong class="ig jh"> A143 —无分期付款计划</strong>:(这也是错误的):客户有一些或其他分期付款计划，意味着客户在银行或商店有分期付款计划(意味着可能有可信的历史记录—此变量也可能有负面含义)</li><li id="ef42" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated"><strong class="ig jh"> A34 —关键信用历史</strong>:(这也是假的):这意味着客户没有关键信用历史。当与A143联合使用时，此变量现在有意义(因为客户没有重要的信用历史，因此能够与商店/银行保持平衡，因此在此特定上下文中，上述变量没有负面含义)。</li></ul><p id="df0c" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">如果你看上面的变量，我们看的是一个客户——他有一定的支票账户余额+在银行/商店已有信用+在过去没有重大信用事件。</p><p id="f181" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">如果给出上述客户，您也可以很容易地得出结论，该客户应该是一个信用良好的客户。然而，在大多数情况下，这种分类可能不那么直观，也不容易导出。</p><h2 id="6c5a" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">了解基尼杂质指数:</h2><p id="ec76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">假设你在一个特定的节点中随机选择一个观察值，并随机分配标签给该特定的观察值，该节点上可用的每种标签的数量，即该测量值中的绝对杂质。我们以一种消极的方式计算基尼指数(许多机器学习算法最终会进行这种目标函数的反转——你将某些东西转化为最大化的对立面，以便达到最小化问题)。</p><figure class="kw kx ky kz fd li er es paragraph-image"><div class="er es lv"><img src="../Images/e7d5674e2bd370cd453bcaf7e32207f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*dntMH5hyXOuw5d1s15w5rw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx">Created using LaTeX</figcaption></figure><p id="c2de" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">其中P1和p2等。分别是将观察结果分类为第一类和第二类的概率。</p><p id="def8" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">如果你仔细看上面的例子，这非常类似于方差公式(平方和所有…)</p><p id="e2b8" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">如果我们必须通过一个例子来看:</p><ul class=""><li id="ce6c" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">以第一个节点为例，在700次观察中有486个好信用和214个坏信用</li><li id="04d6" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">信用良好的概率为486/700，信用不良的概率为214/700</li><li id="656f" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">基尼指数由1-(486/700)-(214/700)给出</li></ul><p id="939e" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">在我们的决策树中是0.4245四舍五入到0.425。如果我们在任一节点上有单一类型的观察，那么我们将得到基尼指数= 0，因为数据中没有杂质。</p><p id="1e33" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">理论上，当有许多分类类型并且所有的观察值都是平均分布时，基尼系数的最大值可以是1，然后每个概率开始接近0。<br/>然而，在二进制分类的情况下，基尼系数的最大值在每个分类平均分布的任何节点上可以是0.5。</p><p id="88cc" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">这在叶节点非常明显，只要我们的基尼系数接近0.5，观察值就更加公平地分布。</p><h2 id="56a1" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">那么，为什么这个模型止步于如此糟糕的基尼指数:</h2><p id="98dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">嗯，那是因为我们把节点数定为3，如果我们不限制节点数，基尼系数有望提高。此外，请注意，因为基尼指数是平方函数，所以一个小的改善实际上是由特定节点的良好和不良信贷分布的大变化产生的。</p><p id="075d" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我用<code class="du ks kt ku kv b">max_depth = 5</code>运行了相同的决策树，屏幕截图显示基尼指数有很多更好的值:</p><p id="c36a" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我们看到基尼指数的值远低于0.25(除了两个节点，这也将随着树深度的增加而进一步改善)。</p><p id="6426" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">除了基尼指数，还有其他标准来衡量树的准确性……<strong class="ig jh">熵</strong></p><h2 id="ac0d" class="hf hg hh bd hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id bi translated">熵作为杂质的量度</h2><p id="6f27" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io hr ip iq ir hv is it iu hz iv iw ix iy ha bi translated">除基尼指数外，另一种杂质测量方法是熵，其计算公式如下:</p><p id="42a6" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">熵(k)由下式给出:</p><figure class="kw kx ky kz fd li er es paragraph-image"><div class="er es lw"><img src="../Images/64c5e5885ce9e3a42edd71874848aeee.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*tDjeR_eQwseNW05wcUaUPg.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx">Created using LaTeX</figcaption></figure><p id="6eb4" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">正如我们已经讨论过的，为了用熵标准创建树，我们可以在树创建中使用超参数<code class="du ks kt ku kv b">criterion</code>。让我们跑一跑，看看结果。</p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="4ae9" class="hf hg hh kv b fi le lf l lg lh">clf_tree_entropy = DecisionTreeClassifier(criterion = 'entropy',max_depth = 3)<br/>clf_tree_entropy.fit(train_X,train_Y)</span><span id="03f8" class="hf hg hh kv b fi lu lf l lg lh">DecisionTreeClassifier(criterion='entropy', max_depth=3)</span></pre><p id="dc7c" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated"><strong class="ig jh">在Jupyter环境中，请重新运行此单元格以显示HTML表示或信任笔记本。<br/>在GitHub上，HTML表示无法呈现，请尝试用nbviewer.org加载此页面。</strong></p><p id="ff72" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">决策树分类器</p><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="8d70" class="hf hg hh kv b fi le lf l lg lh">DecisionTreeClassifier(criterion='entropy', max_depth=3)</span><span id="b1eb" class="hf hg hh kv b fi lu lf l lg lh">##Plotting the graph<br/>fig = plt.figure(figsize = (30,20))<br/>tree.plot_tree(clf_tree_entropy,feature_names = X_variable.columns,<br/>               class_names = [str(x) for x in np.unique(train_Y)],filled = True,fontsize = 14)<br/>plt.plot()</span><span id="3037" class="hf hg hh kv b fi lu lf l lg lh">[]</span></pre><figure class="kw kx ky kz fd li er es paragraph-image"><div class="ab fe cl lj"><img src="../Images/0ad68f6a0bfc7981350cbd8f7762c453.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QqW-xw98on0T1zoepsxr4Q.png"/></div></figure><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="f397" class="hf hg hh kv b fi le lf l lg lh">tree_entropy_predict = clf_tree_entropy.predict(test_X)<br/>metrics.roc_auc_score(test_Y,tree_entropy_predict)</span><span id="b7d5" class="hf hg hh kv b fi lu lf l lg lh">0.633340578135188</span></pre><p id="f6de" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">ROC评分从0.62到0.63有小幅提高。然而，到目前为止，我们已经根据自己的判断使用了像<code class="du ks kt ku kv b">criterion and max_depth</code>这样的参数。我们可以使用<code class="du ks kt ku kv b">sklearn</code>中的<code class="du ks kt ku kv b">GridSearchCV</code>为我们的模型选择最佳超参数。这也被称为<strong class="ig jh">超参数优化</strong>。</p><p id="b11c" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">Gridsearch将以下参数作为输入:</p><ul class=""><li id="362d" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">您的型号名称</li><li id="3f8f" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">参数列表(基本上是您希望模型运行的所有选项)</li><li id="2a15" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">交叉验证(CV):这是每个参数组合的交叉验证折叠数。意味着我们的数据集被分成多少个样本，这可能会改变建议模型的max_depth。</li><li id="f923" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">评分—是定义我们的模型准确性指标的参数。这里我们用<code class="du ks kt ku kv b">roc_auc</code></li></ul><p id="6c31" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我们可以使用<code class="du ks kt ku kv b">best_score_</code>属性提取最佳分数，使用<code class="du ks kt ku kv b">best_params_</code>属性提取最佳参数。</p><p id="c6e1" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">请注意，我们获得了0.70的ROC值，相关参数如下:</p><ul class=""><li id="e6e2" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kp ke kf kg bi translated">标准——熵</li><li id="8a37" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kp ke kf kg bi translated">最大深度— 6</li></ul><pre class="kw kx ky kz fd la kv lb lc aw ld bi"><span id="d393" class="hf hg hh kv b fi le lf l lg lh">from sklearn.model_selection import GridSearchCV<br/>parameter_list = [{'max_depth': range(2,15),<br/>                  'criterion':['gini','entropy']}]<br/><br/>classification_tree = DecisionTreeClassifier()<br/>classification = GridSearchCV(classification_tree,<br/>                             parameter_list,<br/>                             cv = 10,<br/>                             scoring = 'roc_auc')<br/>classification.fit(train_X,train_Y)</span><span id="a15f" class="hf hg hh kv b fi lu lf l lg lh">GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),</span><span id="d449" class="hf hg hh kv b fi lu lf l lg lh">classification.best_score_</span><span id="785c" class="hf hg hh kv b fi lu lf l lg lh"><strong class="kv jh">0.6991386164855553</strong></span><span id="f990" class="hf hg hh kv b fi lu lf l lg lh">classification.best_params_</span><span id="0f30" class="hf hg hh kv b fi lu lf l lg lh"><strong class="kv jh">{'criterion': 'entropy', 'max_depth': 6}</strong></span></pre><p id="5094" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">我们关于使用决策树解决分类问题的讨论到此结束，决策树有很多好处:</p><ol class=""><li id="d035" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated">您可以实际查看决策边界，因此非常直观</li><li id="963d" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">决策树因为不需要对数据进行规范化，并且适用于数值和分类数据集</li></ol><p id="1256" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">然而，决策树也有一些缺陷:</p><ol class=""><li id="5f8c" class="jy jz hh ig b ih jt il ju hr km hv kn hz ko iy kd ke kf kg bi translated">它们不如其他一些算法精确</li><li id="494e" class="jy jz hh ig b ih kh il ki hr kj hv kk hz kl iy kd ke kf kg bi translated">数据集的微小变化会导致非常不同的树；因此，你不能使用一棵树，然后永远运行它(在这方面，一些定量方法更好)</li></ol><p id="b616" class="pw-post-body-paragraph ie if hh ig b ih jt ij ik il ju in io hr jv iq ir hv jw it iu hz jx iw ix iy ha bi translated">有一些技术可以提高决策树的性能，比如装袋、随机森林和boosting。我们改天再谈。谢谢你的耐心。</p><div class="iz ja ez fb jb jc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="jd ab dw"><div class="je ab jf cl cj jg"><h2 class="bd jh fi z dy ji ea eb jj ed ef jk bi translated">Mlearning.ai提交建议</h2><div class="lx l"><h3 class="bd b fi z dy ji ea eb jj ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="jl l"><p class="bd b fp z dy ji ea eb jj ed ef dx translated">medium.com</p></div></div><div class="jm l"><div class="ly l jo jp jq jm jr js jc"/></div></div></a></div></div></div>    
</body>
</html>