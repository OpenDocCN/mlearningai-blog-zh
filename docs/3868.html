<html>
<head>
<title>PCA: Maximising the Covariance — What Does It Mean?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:最大化协方差——这意味着什么？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/pca-maximising-the-covariance-what-does-it-mean-fe196442cb5c?source=collection_archive---------5-----------------------#2022-11-01">https://medium.com/mlearning-ai/pca-maximising-the-covariance-what-does-it-mean-fe196442cb5c?source=collection_archive---------5-----------------------#2022-11-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f90a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们进入标题中的问题之前，让我们先做一些铺垫，这样每个人都知道PCA是什么，并在同一页上。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/0d6bad6116c3fd57dce42017fabad189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0nXQIf8TfKOs-1NL"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Photo by <a class="ae jz" href="https://unsplash.com/@chrisliverani?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chris Liverani</a> on <a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="be1c" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是PCA？</h1><p id="5806" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">主成分分析，或称PCA，在这里可以得到最好的解释:"<a class="ae jz" href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" rel="noopener ugc nofollow" target="_blank">https://builtin . com/data-science/step-step-explaining-principal-component-analysis</a>"——我只是在就我计划讨论的主题上台之前，采用了与定义相同的to。</p><p id="8e2a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA是一种<a class="ae jz" href="https://builtin.com/data-science/dimensionality-reduction-python" rel="noopener ugc nofollow" target="_blank">降维</a>方法，通常用于通过将一大组变量转换成仍然包含大组中大多数信息的较小变量来降低大型<a class="ae jz" href="https://builtin.com/data-science" rel="noopener ugc nofollow" target="_blank">数据集</a>的维度。</p><p id="2f44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">减少数据集的变量数量自然会以牺牲准确性为代价，但降维的诀窍是用一点准确性换取简单性。因为更小的数据集更容易探索和可视化，并使分析数据对于<a class="ae jz" href="https://builtin.com/data-science/tour-top-10-algorithms-machine-learning-newbies" rel="noopener ugc nofollow" target="_blank">机器学习算法</a>来说更容易和更快，而无需处理额外的变量。</p><p id="2ac9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，总而言之，主成分分析的思想很简单——减少数据集的变量数量，同时尽可能多地保留信息。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="bf6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA的过程已经在许多文章和视频中进行了广泛的解释——我将在评论部分添加一些参考资料——但这不是本文的重点。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ld"><img src="../Images/e620c7c5035d218254a3cb662046fcba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n-NymZB8HKe6e_gF"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Photo by <a class="ae jz" href="https://unsplash.com/es/@pawelskor?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Paul Skorupskas</a> on <a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5672" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将讨论短语“最大化协方差”,这是许多参考资料和来源未能解释/没有充分讨论的，并给出了正确的见解。这自然会把我们带到下一个问题。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h1 id="cbd5" class="ka kb hh bd kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt li kv kw kx bi translated">什么是协方差？</h1><p id="3cf4" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">协方差衡量两个随机变量之间的关系以及它们一起变化的程度。这是当变量被线性变换时函数/数据集保持其形式的属性。</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lj"><img src="../Images/2d6eb89053da836c7c0187f7ec37720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*HJbvLhBib-jazB8Ey3tjDQ.png"/></div></figure><p id="2ee9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以，它通常倾向于这样描述:<br/> *如果协方差高—数据不稳定或分散。<br/> *同样，如果协方差较低—数据相对稳定或不太分散。</p><p id="6d67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也就是说，为什么主成分分析试图“最大化”这一部分，以适应描述数据集的更好的线，而不是最小化它？要回答这个问题，我们需要稍微了解一下进行PCA的过程。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h1 id="fbbe" class="ka kb hh bd kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt li kv kw kx bi translated">PCA实际上是做什么的？</h1><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lk"><img src="../Images/19e9679364284d2edf83677c2895f41b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A3eAY12x1IbBcPZv.gif"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Source — <a class="ae jz" href="https://i.stack.imgur.com/lNHqt.gif" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/lNHqt.gif</a></figcaption></figure><p id="c302" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们从一条随机线开始，并通过使用梯度下降来尝试更好地拟合我们的数据集，我们的意思是，我们尝试“减少这些点投影到我们线上的垂直距离的方差”。</p><p id="f139" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是首先！我们找到数据集的平均值，并将轴(组件)的中心移动到平均值，移动中心不会改变数据集，因为它们的“相对”位置不会改变。</p><p id="7077" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，我们所说的投射是什么意思呢？让我们再回顾一下向量。</p><p id="29d3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">投影的意思是“阴影在/沿着”因此，按照数学语言(尤其是向量)的指导方针，我们执行点积来找到这个“投影”</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es ll"><img src="../Images/96f966b66f5f9de5c2a4a29e03273e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/0*a3m57xMXspEaCH7N"/></div><figcaption class="jv jw et er es jx jy bd b be z dx">projection of A onto B</figcaption></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es lm"><img src="../Images/b56d3257ca7fe81fa1860436e7e4a565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*FdW2XqvERYSh_RIorZ1Bsw.png"/></div></figure><p id="5428" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">设向量A和向量B的交点为O——在这种情况下，O将是我们分量的平均值/新中心。因此，当我们尝试最大化投影时，即，使A更接近B(使用梯度下降调整线-找到更好的拟合线)，它们之间的角度不断减小，因此投影增加-这也意味着||OA||值增加！！—这意味着您的协方差/方差增加！！(唷~~)。</p><p id="1c35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我希望这篇文章能够帮助那些困惑的人，为什么我们要“最大化”协方差而不是“最小化”它——现在我已经给出了这种情况的完整描述(大多数资料都没有做到这一点)。</p><p id="bce1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下次见！再见！</p><div class="ln lo ez fb lp lq"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hi fi z dy lv ea eb lw ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">medium.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me jt lq"/></div></div></a></div></div></div>    
</body>
</html>