<html>
<head>
<title>Textual Sentiment Analysis with Support Vector Machines — Part 2: Data Pre-Processing and Vectorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于支持向量机的文本情感分析第二部分:数据预处理和矢量化</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/textual-sentiment-analysis-with-support-vector-machines-part-2-data-pre-processing-and-ee45f18083e4?source=collection_archive---------7-----------------------#2022-10-23">https://medium.com/mlearning-ai/textual-sentiment-analysis-with-support-vector-machines-part-2-data-pre-processing-and-ee45f18083e4?source=collection_archive---------7-----------------------#2022-10-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/c95f2cd24b68f42fe89a843db983f5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*SY-ddfeEmWXnjpDZIW-LKQ.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Image generated by <a class="ae hq" href="https://stability.ai/blog/stable-diffusion-public-release" rel="noopener ugc nofollow" target="_blank">Stable Diffusion</a></figcaption></figure><div class=""/><p id="60f4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">欢迎回到本系列的第二部分，我们试图建立一个能够提取推文情感的机器学习模型。在前一部分，我们解释了什么是情感分析，它通常是如何做的，以及什么时候不使用神经网络是明智的。在本文中，我们将深入研究需要对数据进行的预处理，以便能够正确地使用它们。</p><p id="48d6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这是一个由三部分组成的系列，您目前正在阅读第一部分。您可以在下面找到课程:</p><ol class=""><li id="68df" class="jo jp ht is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">第1部分 —向读者介绍情感分析和支持向量机的概念</li><li id="e52e" class="jo jp ht is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">第2部分—解释在训练分类器之前所需的数据预处理管道。</li><li id="7ab4" class="jo jp ht is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><a class="ae hq" rel="noopener" href="/mlearning-ai/textual-sentiment-analysis-with-support-vector-machines-part-3-implementation-ab10b4a7847d">第3部分</a> —描述实施步骤，并展示实际结果。</li></ol></div><div class="ab cl kc kd go ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="ha hb hc hd he"><h1 id="50b5" class="kj kk ht bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">预处理</h1><p id="4220" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">预处理是数据科学流程中最基本的部分之一。收集完数据后，我们必须清理它们。清理数据集可能是整个流程中最耗时的任务，因为它需要细致的操作，将数据从原始格式转换为我们可以正确利用的格式。如果这一步做得很差，意味着数据没有得到适当的预处理，或者没有考虑到极端情况和异常值，那么没有模型可以拯救我们。如果我们的基础，即我们的数据是垃圾，我们的最终结果也将是垃圾。正如人们常说的，垃圾进，垃圾出。</p><h2 id="f751" class="lm kk ht bd kl ln lo lp kp lq lr ls kt jb lt lu kx jf lv lw lb jj lx ly lf lz bi translated">文本</h2><p id="5360" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">预处理步骤并不简单，它与数据收集的质量和数据格式密切相关。例如，与清理文本数据相比，清理数字数据是完全不同的事情。每个机器学习模型都基于某种数学运算，用来产生最终结果。对于文本数据，我们没有数字，只有文本。因此，第一步是将这些数据转换成可以输入模型的实际数字。这个动作被称为“向量化”，其中单词被转换成数字。</p><h2 id="1943" class="lm kk ht bd kl ln lo lp kp lq lr ls kt jb lt lu kx jf lv lw lb jj lx ly lf lz bi translated">一袋单词</h2><p id="b87b" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">但是，这是怎么做到的呢？这样做有许多不同的方法。其中最基本的一种叫做“词汇袋”。在这个场景中，我们创建了一个字典，用来存储数据集中出现的唯一单词。这些单词中的每一个都被映射到一维向量中的特定索引，该向量的长度等于字典中的单词数。这用一个简单的例子就很容易理解了。考虑我们的数据集由以下两个句子组成:</p><blockquote class="ma mb mc"><p id="d7a9" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi translated">我喜欢陀螺</p><p id="8a55" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi translated">我真的喜欢藜麦</p></blockquote><p id="a9af" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果我们计算唯一的单词，我们可以构建下一个列表:</p><blockquote class="ma mb mc"><p id="6dcb" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi translated">【“我”、“爱”、“真的”、“喜欢”、“陀螺”、“藜麦”】</p></blockquote><p id="4925" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">所以“I”被映射到第一个元素(索引0)，单词“love”对应于第二个元素(索引1)，等等。在此基础上，我们可以将句子转换成数字列表，方法是将1放在索引中，表示句子中有这些特定的单词，而将0放在索引中，表示正好相反的单词。考虑到这一点，这些句子可以转换成:</p><blockquote class="ma mb mc"><p id="7452" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi">[1, 1, 0, 0, 1, 0]</p><p id="40d3" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi">[1, 0, 1, 1, 0, 1]</p></blockquote><p id="7a3a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">甚至这种从文本到数字的简单方法在许多NLP任务中也非常有用。然而，还有更健壮、更强大的技术。</p><h2 id="f6ae" class="lm kk ht bd kl ln lo lp kp lq lr ls kt jb lt lu kx jf lv lw lb jj lx ly lf lz bi translated">术语频率—逆文档频率</h2><p id="fbd9" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">术语频率—逆文档频率，简称TF-IDF，是将文本编码为数字的另一种方法。它依赖于两个独立指标的计算。第一个是术语频率，它衡量一个术语在单个文档的上下文中的频率。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mh"><img src="../Images/420b0b22d1729fc994d4db838a581b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/0*Lg-fjh95rxRHTLDe.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Term frequency formula</figcaption></figure><p id="39f1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">例如，让我们考虑下面的句子:</p><blockquote class="ma mb mc"><p id="a792" class="iq ir md is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn ha bi translated">"我去了商店，看到一些新鲜水果，就买了下来."</p></blockquote><p id="ad17" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果我们为单词“store”计算“TF ”,我们有1/15，因为句子中的总项数等于15。单词“I”的TF为3/15，意味着它的出现频率是单词“store”的三倍。更大的TF意味着更有价值的信息。然而，对于“我”这个字来说，是这样吗？</p><p id="d037" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">第二个指标是逆文档频率。为了更好地理解这一点，让我们快速看一下什么是文档频率。</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mm"><img src="../Images/b0eebdcb11228aff0caebcce8cd4627f.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*PQ-sq3K27PmISQqG.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Document frequency formula</figcaption></figure><p id="3623" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">本质上，它表示包含给定单词的文档的比例。所以，反过来就是:</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mn"><img src="../Images/f775212ba4811971c0079a75a2fb3dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*R8-W_oFVqoHy22dN.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Inverse document frequency formula</figcaption></figure><p id="e1cd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">通过反过来，我们试图强调如果一个单词出现在许多文档中，那么它可能并不那么重要。例如，仅举几个例子，像“我”、“and”和“介词”这样的词在文本数据集中出现得非常频繁，但在情感方面没有提供太多信息。这些术语对我们的模型来说价值不大。在某种程度上，“以色列国防军”试图弥补“TF”一词的错误。最后，TF-IDF公式为:</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mo"><img src="../Images/5e2443e166875ab49fc41ebe9e3c2fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/0*sgt4saT-VsCP_FCI.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">TF-IDF formula</figcaption></figure><p id="aa89" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这个过程可以被认为是一种特征提取机制，我们试图提取关于每个术语的有用见解。TF-IDF还采用了一些额外的配置，这些配置应该进行适当的调整以获得最大的效能。其中一个参数称为“最大特征”，它只考虑整个数据集中按术语频率排序的前“n”个特征。我们一会儿会看看这个。</p><h2 id="1fbf" class="lm kk ht bd kl ln lo lp kp lq lr ls kt jb lt lu kx jf lv lw lb jj lx ly lf lz bi translated">管道</h2><p id="db01" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">我们现在知道了一些编码文本的方法。应该清楚的是，TF-IDF工作于“术语”,即“文字”。在我们应用它之前，我们需要拆分我们的句子，并获得那些“单词”。在NLP领域，我们称这些为“记号”,这个过程被称为“记号化”。记号化本身就是一门艺术，所以除了它只是把句子分解成记号之外，我们不会对它进行更多的讨论。此外，我们通常将文本转换成小写格式，以避免重复。基本上，这些是我们将使用的预处理管道的前2个步骤。对于术语“管道”,我们描述了包含离散步骤的预定义流程，其中每个步骤都接收前一个步骤的输出作为输入。</p><p id="e805" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们的管道首先将文本转换成小写，然后将输出提供给记号赋予器。完成标记化后，下一步是删除“停用词”。停用词是没有意义的词，因此被过滤掉以降低问题的复杂性。最流行的是像“一个”、“这个”、“是”和“是”这样的词。</p><p id="623e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">第四步旨在通过只保留每个单词的“词条”来进一步精简我们的词典。单词“eating”是基本单词“eat”的变形形式。“吃”和“吃”也是一样，都换成了“吃”字。最终，我们的管道看起来像这样:</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div class="er es mp"><img src="../Images/ac9b3ecc0d45aa107eeec61bb9f1e281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*w0kwd4b7n4c1dH6nJP6yyA.png"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Pipeline illustration</figcaption></figure><p id="4c5e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">TF-IDF使用最终输出来创建矢量化数据。如果我们在矢量化之前仔细查看数据集的输出，并绘制每个单词的频率，我们会得到如下结果:</p><figure class="mi mj mk ml fd hj er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es mq"><img src="../Images/1684c4d62a4edc37a3387577f642c4e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4YOJbs72IFsPjzP3"/></div></div><figcaption class="hm hn et er es ho hp bd b be z dx">Word frequency</figcaption></figure><p id="9070" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们的字典有40813个单词，这意味着如果我们使用TF-IDF，每个单词将有40813个特征。就记忆而言，这非常难以处理。绕过这一点的一种方法是使用前面提到的“max_features”参数来限制功能的数量。在我们的例子中，我们可以发现只有3034个单词出现了至少10次。通过将“max_features”设置为3，034，我们可以在内存和计算方面获得更可行的编码。</p><p id="acb5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">对于停用词、标记化和词条化，我们使用了<a class="ae hq" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> nltk </a>库，而对于TF-IDF，我们使用了<a class="ae hq" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>。Python中的管道展示如下:</p><figure class="mi mj mk ml fd hj"><div class="bz dy l di"><div class="mv mw l"/></div><figcaption class="hm hn et er es ho hp bd b be z dx">Pipeline in Python</figcaption></figure><h1 id="2c44" class="kj kk ht bd kl km mx ko kp kq my ks kt ku mz kw kx ky na la lb lc nb le lf lg bi translated">摘要</h1><p id="9dd8" class="pw-post-body-paragraph iq ir ht is b it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj ll jl jm jn ha bi translated">这就结束了在进入模型构建之前需要完成的预处理。管道的输出是我们数据集的矢量化版本，可以由支持向量机模型使用。但这正是我们将在本系列的下一篇也是最后一篇文章中讨论的内容。在那之前，敬请期待！</p><div class="hg hh ez fb hi nc"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hu fi z dy nh ea eb ni ed ef hs bi translated">Mlearning.ai提交建议</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">medium.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq hk nc"/></div></div></a></div></div></div>    
</body>
</html>