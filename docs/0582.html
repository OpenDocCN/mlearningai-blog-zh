<html>
<head>
<title>Vanishing Gradient Descent Problem In-Depth</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度消失梯度下降问题</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/vanishing-gradient-descent-problem-in-depth-a6181404bd2c?source=collection_archive---------3-----------------------#2021-05-19">https://medium.com/mlearning-ai/vanishing-gradient-descent-problem-in-depth-a6181404bd2c?source=collection_archive---------3-----------------------#2021-05-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="33a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练多层神经网络模型时，消失梯度下降是一个大问题。这是因为，向网络中添加更多的层会导致权重值的消失，从而导致用于计算的几乎恒定的值。我们说这些值几乎是恒定的，因为变化并不显著。</p><p id="1ffd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们通过深入研究这个问题来理解这一点。</p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><h2 id="66aa" class="jj jk hh bd jl jm jn jo jp jq jr js jt ip ju jv jw it jx jy jz ix ka kb kc kd bi translated"><strong class="ak">什么是消失梯度下降</strong></h2><blockquote class="ke kf kg"><p id="c037" class="ie if kh ig b ih ii ij ik il im in io ki iq ir is kj iu iv iw kk iy iz ja jb ha bi translated">反向传播的误差信号随着与最终层(通常是输出层)的距离的增加而减少(通常是指数的),这是指向<strong class="ig hi">消失梯度下降</strong>的问题。</p></blockquote><p id="0964" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但这意味着什么呢？让我们通过创建自己的小网络来理解这一点。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kl"><img src="../Images/e1a94ef0b1a3f53afdd0497f0082b53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*pFfdlp3tnba8vpPPNJY60w.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Simple Neural Network (Made using <a class="ae kx" href="http://draw.io/" rel="noopener ugc nofollow" target="_blank">http://draw.io/</a>)</figcaption></figure><p id="9c7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个简单的神经网络，有一个输入层、一个隐藏层和输出层。权重写在图中节点的旁边。</p><p id="aeb4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型训练过程以下列方式进行:</p><ol class=""><li id="f3ef" class="ky kz hh ig b ih ii il im ip la it lb ix lc jb ld le lf lg bi translated">输入与各自的权重相乘，加上偏差，到达第一个隐藏层。</li><li id="fafb" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated">这个隐藏层再次根据它们的特定权重执行计算，并且信号被传播到下一层(在这种情况下是输出层)。</li><li id="9ec9" class="ky kz hh ig b ih lh il li ip lj it lk ix ll jb ld le lf lg bi translated">输出(O21)被发送到<strong class="ig hi"> <em class="kh">损失函数</em> </strong>，优化器由此创建。优化器负责调整权重值，以减少反向传播过程的损失值。</li></ol><p id="e862" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用于更新权重的公式为:</p><pre class="km kn ko kp fd lm ln lo lp aw lq bi"><span id="e8f0" class="jj jk hh ln b fi lr ls l lt lu">Wa(new) = Wa(old) - α * (𝕕L/𝕕Wa)</span></pre><p id="ff53" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，α是学习率，L是损失函数，Wa是特定节点的权重。</p><p id="a315" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们尝试在第一步之后计算W1的新值。</p><pre class="km kn ko kp fd lm ln lo lp aw lq bi"><span id="5690" class="jj jk hh ln b fi lr ls l lt lu">W1(new) = W1(old) - α * (𝕕L/𝕕W1)</span></pre><p id="5dd9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">我们如何计算(𝕕L/𝕕W1)？</strong></p><p id="7b13" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，W6的值取决于W4和W5的值。</p><p id="1ffc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">W4(和W5)的值又取决于W1、W2和W3的值。</p><p id="14b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，为了计算W1引起的损耗变化，我们需要考虑W6和W4引起的损耗变化(因为它们将W1连接到输出端)。</p><p id="e027" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此</p><pre class="km kn ko kp fd lm ln lo lp aw lq bi"><span id="6971" class="jj jk hh ln b fi lr ls l lt lu"><strong class="ln hi">(𝕕L/𝕕W1) = (𝕕W6/𝕕W4) * (𝕕W4/𝕕W1)</strong></span></pre><p id="c193" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是所谓的<strong class="ig hi"> <em class="kh">链式分化法则。</em>T13】</strong></p><p id="6f1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着我们越往下，导数越小，所以</p><p id="6476" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> (𝕕W6/𝕕W4) </strong>大于<strong class="ig hi"> (𝕕W4/𝕕W1).</strong></p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="bdf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当激活函数为s形时，通常会出现消失梯度下降的问题。但是为什么呢？</p><p id="fb24" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是因为sigmoid函数的导数值总是在0和0.25之间。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lv"><img src="../Images/02f4bef96da4eb4a0e78bdaac1decd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*uNJZm0XDJGcgXiHFmwhcig.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Sigmoid and it's derivative (created using matplotlib)</figcaption></figure><p id="f302" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以通过使用<strong class="ig hi"> matplotlib </strong>和这段<a class="ae kx" href="https://gist.github.com/Priyansh-Kedia/07f501da18bfd64543c513e137bb7e1a" rel="noopener ugc nofollow" target="_blank">代码</a>来可视化sigmoid函数及其导数。</p><p id="3925" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以我们可以清楚的看到，如果<strong class="ig hi"> (𝕕W6/𝕕W4) </strong>的值是0.1，<strong class="ig hi"> (𝕕W4/𝕕W1) </strong>的值是0.001，那么<strong class="ig hi"> (𝕕W/𝕕W1) </strong>的值就会是0.0001。</p><p id="68dd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">假设W1(old) = 2.5，α = 1，则</p><pre class="km kn ko kp fd lm ln lo lp aw lq bi"><span id="4c7b" class="jj jk hh ln b fi lr ls l lt lu">W1(new) = 2.5 - 0.0001 = 2.4999</span></pre><p id="bbf7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以清楚地看到，随着我们增加网络的层数，<strong class="ig hi"> (𝕕W/𝕕W1) </strong>的值会进一步减小，因此，权重的值没有明显的变化。</p><p id="7923" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就是所谓的<strong class="ig hi">消失梯度问题。</strong></p></div><div class="ab cl jc jd go je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="ha hb hc hd he"><p id="e4e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博文中，我们学习了神经网络中的消失梯度问题及其原因。</p><p id="e5eb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">参考</strong></p><div class="lw lx ez fb ly lz"><a href="https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab dw"><div class="mb ab mc cl cj md"><h2 class="bd hi fi z dy me ea eb mf ed ef hg bi translated">如何控制具有节点和层的神经网络模型容量-机器学习掌握</h2><div class="mg l"><h3 class="bd b fi z dy me ea eb mf ed ef dx translated">深度学习神经网络模型的能力控制其映射函数类型的范围</h3></div><div class="mh l"><p class="bd b fp z dy me ea eb mf ed ef dx translated">machinelearningmastery.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn kr lz"/></div></div></a></div><div class="lw lx ez fb ly lz"><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab dw"><div class="mb ab mc cl cj md"><h2 class="bd hi fi z dy me ea eb mf ed ef hg bi translated">消失梯度问题-维基百科</h2><div class="mg l"><h3 class="bd b fi z dy me ea eb mf ed ef dx translated">在机器学习中，用神经网络训练神经网络时会遇到消失梯度问题</h3></div><div class="mh l"><p class="bd b fp z dy me ea eb mf ed ef dx translated">en.wikipedia.org</p></div></div><div class="mi l"><div class="mo l mk ml mm mi mn kr lz"/></div></div></a></div></div></div>    
</body>
</html>