<html>
<head>
<title>Why over-parameterised deep nets don’t overfit?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么过参数化的深网不会过拟合？</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/intuitive-explanations-of-why-over-parameterised-deep-nets-dont-overfit-8a323b223ba6?source=collection_archive---------1-----------------------#2022-02-27">https://medium.com/mlearning-ai/intuitive-explanations-of-why-over-parameterised-deep-nets-dont-overfit-8a323b223ba6?source=collection_archive---------1-----------------------#2022-02-27</a></blockquote><div><div class="dt ha hb hc hd he"/><div class="hf hg hh hi hj"><div class=""/><div class=""><h2 id="a739" class="pw-subtitle-paragraph ij hl hm bd b ik il im in io ip iq ir is it iu iv iw ix iy iz ja dy translated">训练过参数化深度神经网络的惊人特性</h2></div><p id="ac90" class="pw-post-body-paragraph jb jc hm jd b je jf in jg jh ji iq jj jk jl jm jn jo jp jq jr js jt ju jv jw hf bi translated"><strong class="jd hn">过参数化</strong>:模型参数数量大于训练样本数量的状态。</p><p id="2e2c" class="pw-post-body-paragraph jb jc hm jd b je jf in jg jh ji iq jj jk jl jm jn jo jp jq jr js jt ju jv jw hf bi translated"><strong class="jd hn">概括:</strong>一个经过训练的模型在看不见的测试数据上表现如何？</p><p id="7298" class="pw-post-body-paragraph jb jc hm jd b je jf in jg jh ji iq jj jk jl jm jn jo jp jq jr js jt ju jv jw hf bi translated">传统的统计理论认为，在过度参数化的状态下，过度拟合是非常可能的。我们讨论偏差-方差权衡以及如何增加模型的复杂性超过一定…</p></div></div>    
</body>
</html>