<html>
<head>
<title>Retrieval-Enhanced Transformers: DeepMind paper summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检索增强型变形金刚:DeepMind论文摘要</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/retrieval-enhanced-transformers-deepmind-paper-summary-efc2653e64b4?source=collection_archive---------4-----------------------#2022-01-06">https://medium.com/mlearning-ai/retrieval-enhanced-transformers-deepmind-paper-summary-efc2653e64b4?source=collection_archive---------4-----------------------#2022-01-06</a></blockquote><div><div class="ds gz ha hb hc hd"/><div class="he hf hg hh hi"><div class=""/><p id="e854" class="pw-post-body-paragraph ii ij hl ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf he bi translated">上个月(2021年12月8日)<a class="ae jg" href="https://arxiv.org/abs/2112.04426" rel="noopener ugc nofollow" target="_blank"> DeepMind发表了一篇论文</a>，其中他们展示了一个聪明的技巧，让transformer模型在语言建模任务中表现更好，而不必使用最近模型使用的那么多参数。这是非常有趣的，因为这个领域的许多进展都来自于把模型做得更大和投入更多的计算能力。这很好，但这意味着没有<a class="ae jg" href="https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/" rel="noopener ugc nofollow" target="_blank">数百万美元用于培训模型的团体</a>更难与拥有资源的大型行业研究实验室竞争…</p></div></div>    
</body>
</html>