<html>
<head>
<title>NLP-Day 21: Understanding Transformer Models And Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP-第21天:理解变压器模型和架构</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/nlp-day-21-understanding-transformer-models-and-architecture-b185bcfe57eb?source=collection_archive---------0-----------------------#2022-04-27">https://medium.com/mlearning-ai/nlp-day-21-understanding-transformer-models-and-architecture-b185bcfe57eb?source=collection_archive---------0-----------------------#2022-04-27</a></blockquote><div><div class="dt ha hb hc hd he"/><div class="hf hg hh hi hj"><h2 id="81d1" class="hk hl hm bd b fq hn ho hp hq hr hs dy ht translated" aria-label="kicker paragraph"># 30日</h2><div class=""/><div class=""><h2 id="35d8" class="pw-subtitle-paragraph is hv hm bd b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj dy translated">剖析变压器模型和架构，以获得更深入的理解</h2></div><figure class="jl jm jn jo fe jp es et paragraph-image"><div class="es et jk"><img src="../Images/0144820360820b3c86f14e876949a2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*YyF8wXDRKyetLVcWW8hfkA.png"/></div><figcaption class="js jt eu es et ju jv bd b be z dy">Transformer-based architecture #30DaysOfNLP [Image by Author]</figcaption></figure><p id="a579" class="pw-post-body-paragraph jw jx hm jy b jz ka iw kb kc kd iz ke kf kg kh ki kj kk kl km kn ko kp kq kr hf bi translated"><a class="ae ks" rel="noopener" href="/mlearning-ai/nlp-day-20-you-better-pay-attention-to-transformers-part-2-c6889a0a301b"> <strong class="jy hw">在上一集</strong> </a>中，我们巩固了我们的基础，涵盖了一般意义上的注意力机制以及基于变压器的架构。</p></div></div>    
</body>
</html>