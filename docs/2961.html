<html>
<head>
<title>Machine Learning explained on Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归解释的机器学习</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/basic-machine-learning-recipe-2872c1ff1a38?source=collection_archive---------7-----------------------#2022-07-02">https://medium.com/mlearning-ai/basic-machine-learning-recipe-2872c1ff1a38?source=collection_archive---------7-----------------------#2022-07-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="188b" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">机器学习算法的基本配方</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/0f5b1f2840f7e90d3b6fb3320f6289d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lkUwlylc8RFJo4nX"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Photo by <a class="ae jm" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Winkler</a> on <a class="ae jm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="f354" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h2><p id="581a" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">线性回归是在许多科学和经济领域中使用的流行算法。在这篇文章中，我们将把线性回归放在机器学习的背景下。我们将以线性回归为例，讨论理解机器学习概念所需的基本方法。简单的线性回归可能是机器学习算法最简单的例子，这正是它如此适合这项任务的原因。</p><h2 id="333d" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">机器学习背景下的线性回归</h2><p id="62e0" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">一个典型的工作流程是，当我们执行线性回归时，我们有一些数据<em class="le"> x </em>和<em class="le"> y </em>，我们假设它们有线性关系<em class="le">。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lf"><img src="../Images/dfb6c2f9e8aff6b45e2f697fced47ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*RDxFQwEuO_pi9xiID0rYog.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Linear Regression data example</figcaption></figure><p id="8143" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">在机器学习语言中，我们会说我们有一个带有输入数据<em class="le"> x </em>和目标数据<em class="le">y</em>的<em class="le">数据集(x，y) </em>，下一步是对数据拟合一条线性线。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lf"><img src="../Images/f4980c05ee08b2b350834461ee5873e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8Ngl6V9DzaUgMG2NJZaylw.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Linear Regression: Data and Model</figcaption></figure><p id="449a" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">换句话说，我们有一个<em class="le">模型</em>，它通过关系𝑦̂ <em class="le"> = wx + b. </em>来近似我们的数据，也就是说，为了用线性回归来近似我们的数据，我们需要找到机器学习中的参数<em class="le"> w </em>和<em class="le"> b. </em>，这些是所谓的<em class="le">可训练参数。</em></p><p id="db9f" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">我们不希望找到任何模型来近似我们的数据，但我们希望找到“最佳”模型，即与我们的真实(目标)数据相比误差最小的“最佳”模型。线性回归中的误差通常用<a class="ae jm" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> <em class="le">均方误差</em> </a> (MSE)来衡量。通过计算真实值(<em class="le"> y </em>)和预测值(𝑦̂)之间的平方差来定义MSE，然后对其求和，最后除以数据集的长度(<em class="le"> N </em>)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ll"><img src="../Images/a73f2ef16428faa74d6c998b1649e1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*xcc5ci2N912FrxKD83dQgg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Mean squared error</figcaption></figure><p id="3619" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">我们可以根据(可训练)参数<em class="le"> w </em>和<em class="le"> b </em>将MSE视为一个函数，我们将这个函数称为<em class="le"> </em> <a class="ae jm" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank"> <em class="le">损失函数</em> </a> <em class="le"> (J)。</em>有时也被称为<em class="le">目标函数</em>、<em class="le">成本函数、</em>或<em class="le">误差函数</em>。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lm"><img src="../Images/f2a99a6f75f8ce2b087b2d1eba9e65db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*_WJmQkIzlR9DyB2yJ8eimQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Mean squared error as loss function depending on parameters w and b</figcaption></figure><p id="7621" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">如果我们想找到误差最小的“最佳”模型，我们需要最小化关于<em class="le"> w </em>和<em class="le"> b </em>的损失函数。最小化损失函数的最终参数𝑤̂和𝑏̂然后被用于最终模型。寻找‘最佳’参数的过程就是机器学习中我们所说的<em class="le">训练</em>。</p><h2 id="7d0e" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">通过梯度下降训练模型</h2><p id="06eb" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">现在让我们考虑寻找最佳参数𝑤̂和𝑏̂ <em class="le">、</em>的过程，即<em class="le">训练</em>。对于简单的线性回归，我们可以解析地解决这个最小化问题。(即计算导数并将其设置为零)。然而，在实践中——当涉及到编码时——使用迭代优化技术。最常见的一种叫做<a class="ae jm" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"> <em class="le">渐变下降</em> </a>。</p><p id="5b0e" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">为了理解梯度下降的概念，让我们画出损失函数(<em class="le"> J </em> ) —在我们的例子中是均方误差。为简单起见，此处仅示出一维，即<em class="le"> J </em>仅被认为取决于<em class="le"> w，</em>和<em class="le"> b </em>保持不变。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lf"><img src="../Images/bd1fd698f115db20312a8c9509cb9628.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Hq7ixkuwt_P_zMe3NNqz-g.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Gradient descent illustrated for mean squared error as loss function</figcaption></figure><p id="5557" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">梯度下降的想法是从某个值开始，然后向最小值迈出一小步——“向最小值”意味着向最陡下降的方向移动，这是该位置的负梯度方向。上图说明了这一点。也就是说，我们有一个迭代算法，在每个迭代步骤中，我们通过在负梯度方向上前进一步来更新我们的参数<em class="le"> w </em>。步长的大小叫做<a class="ae jm" href="https://en.wikipedia.org/wiki/Learning_rate" rel="noopener ugc nofollow" target="_blank"> <em class="le">学习率</em> </a> (𝛼)，是我们在训练过程中要优化的一个所谓的超参数。α的大小影响着算法的收敛性。如果它很小，收敛到最小值需要很长时间。另一方面，如果它太大，我们可能会错过最小值。这可以表述为</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es ln"><img src="../Images/e8233c3035a2099073a0a2745934696d.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*HXYJFXSWAp5IdrwWksDaRQ.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Gradient descent: calculate iteratively the next value for the weight</figcaption></figure><p id="dff1" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">总而言之，我们从一个任意值w1开始，当我们用这个w1来近似我们的模型的数据时，我们可能会有一个相当大的误差。然后，我们向负梯度方向迈出𝛼大小的一步，得到一个新的w1。使用w1作为参数的模型与真实数据相比应该比第一个模型具有更小的误差。我们继续这个算法，直到我们找到一个足够接近我们的数据的模型。这个过程的草图是这样的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lf"><img src="../Images/00275d4e7ff5bcfc56f128d89c6c0041.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*YM6BYW0ObmVsORMUOPgDQg.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Sketch of the approximation to the data during training</figcaption></figure><p id="56bd" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">这是梯度下降的基本算法。实际上有不同的变体:</p><ul class=""><li id="16bc" class="lo lp hh kn b ko lg kr lh jy lq kc lr kg ls ld lt lu lv lw bi translated">批量梯度下降:整个数据集用于更新参数(如上所述)。</li><li id="5754" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld lt lu lv lw bi translated">小批量梯度下降:数据集的子集(小批量)用于更新参数。</li><li id="0345" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld lt lu lv lw bi translated">随机梯度下降:在每个数据点之后更新权重。</li></ul><p id="dcc1" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">通常，使用小批量梯度下降，这是计算时间和收敛性之间的折衷。子集(小批量)的大小是我们必须优化的另一个超参数。在小批量梯度下降中，误差不一定在每次迭代后减少，因为我们不查看整个数据集。关于梯度下降、其变体及其利弊的详细解释可以参见<a class="ae jm" href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3" rel="noopener" target="_blank"> I. Dabburas文章【1】</a>和<a class="ae jm" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank"> ML词汇表【2】</a>中的实现示例。</p><h2 id="fb85" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">机器学习的基本方法</h2><p id="a736" class="pw-post-body-paragraph kl km hh kn b ko kp ii kq kr ks il kt jy ku kv kw kc kx ky kz kg la lb lc ld ha bi translated">总而言之，执行线性回归的主要步骤是:</p><ol class=""><li id="451d" class="lo lp hh kn b ko lg kr lh jy lq kc lr kg ls ld mc lu lv lw bi translated">数据集(<em class="le">输入</em>，<em class="le">目标</em> ) = ( <em class="le"> x </em>，<em class="le"> y </em>)</li><li id="5c7c" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld mc lu lv lw bi translated">建立一个模型(这里是:线性回归)</li><li id="1cf4" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld mc lu lv lw bi translated">定义一个损失函数(这里是:均方误差)</li><li id="0027" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld mc lu lv lw bi translated">通过最小化损失函数来训练模型(这里:使用梯度下降)</li></ol><p id="6138" class="pw-post-body-paragraph kl km hh kn b ko lg ii kq kr lh il kt jy li kv kw kc lj ky kz kg lk lb lc ld ha bi translated">正是这些步骤也是大多数机器学习算法的基本配方。也就是说，知道如何执行线性回归意味着我们已经有了更高级的机器学习算法的基础，我们只需要调整模型。</p><h2 id="d34f" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">进一步阅读</h2><ul class=""><li id="6350" class="lo lp hh kn b ko kp kr ks jy md kc me kg mf ld lt lu lv lw bi translated">[1] <a class="ae jm" rel="noopener" href="/@ImadPhd?source=post_page-----10f652806a3--------------------------------">一、达布拉</a>，梯度下降算法及其变体(2017)，towardsdatascience，<a class="ae jm" href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3" rel="noopener" target="_blank">https://towards data science . com/Gradient-Descent-Algorithm-and-Its-Variants-10f 652806 a3</a></li><li id="6162" class="lo lp hh kn b ko lx kr ly jy lz kc ma kg mb ld lt lu lv lw bi translated">[2] ML词汇表(2017)，<a class="ae jm" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">https://ML-cheat sheet . readthedocs . io/en/latest/gradient _ descent . html</a></li></ul><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/@frauke.albrecht/subscribe"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">每当弗劳克·阿尔布雷特出版时，就收到一封电子邮件。</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">每当弗劳克·阿尔布雷特出版时，就收到一封电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx jg mj"/></div></div></a></div><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/@frauke.albrecht/membership"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">通过我的推荐链接加入媒体</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">阅读Frauke Albrecht的每一个故事(以及媒体上成千上万的其他作家)。您的会员费直接支持…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="my l mu mv mw ms mx jg mj"/></div></div></a></div><div class="mg mh ez fb mi mj"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">medium.com</p></div></div><div class="ms l"><div class="mz l mu mv mw ms mx jg mj"/></div></div></a></div></div></div>    
</body>
</html>