<html>
<head>
<title>3 Actor-critic algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">3个演员-评论家算法</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/3-actor-critic-algorithms-779f14465b74?source=collection_archive---------4-----------------------#2022-05-25">https://medium.com/mlearning-ai/3-actor-critic-algorithms-779f14465b74?source=collection_archive---------4-----------------------#2022-05-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="9603" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我总结Sergey Levine教授举办的CS285讲座的系列文章的第三篇，所有的荣誉都归于他。所有图片均取自他的讲座。<a class="ae jc" href="https://samuelebolotta.medium.com/1-an-introduction-to-deep-reinforcement-learning-c5ab792af013" rel="noopener">我写的这篇文章</a>是对深度强化学习的介绍。行动者-批评家算法建立在我们在<a class="ae jc" href="https://samuelebolotta.medium.com/2-deep-reinforcement-learning-policy-gradients-5a416a99700a" rel="noopener">这篇文章</a>中讨论的政策梯度框架之上。最重要的是，它们还增加了学习价值函数和Q函数。</p><h2 id="5497" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">完善政策梯度</strong></h2><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es jy"><img src="../Images/5efdf2aac2d0781c73e948a1226f63a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*7uxaqdxQgcz4UUevG9Y8eQ.png"/></div></figure><p id="b613" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们在状态s采取行动a，将要得到的回报是对预期回报的估计，我们能得到这个量的更好的估计吗？</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kg"><img src="../Images/30e9fea925b490bdf89fa39e6ebb4334.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*yJX2yt8uexNjyRqOtTvmbg.png"/></div></figure><p id="923e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们想象一下，这条曲线代表你采样的轨迹之一，你的奖励是在特定的时间点计算的。这个绿圈代表样本I在时间步长t的状态，在这个时间点，我们将计算出我们的收益估计值，然后乘以我们行动的概率。</p><p id="383e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们计算这一估计值的方法是将我们沿着这条轨迹实际获得的回报相加:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kh"><img src="../Images/15cec3fb7f8fe6ef04f9f8a3341755ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*4hVdf8fiZoMb1fkgQvbH5g.png"/></div></figure><p id="7dd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但该轨迹只是许多可能性中的一种，因此，如果我们以某种方式意外地再次进入相同的确切状态，然后像我们在此次展示中所做的那样运行我们的策略，我们可能会得到不同的结果，因为策略和MDP在其中具有一些随机性。现在，我们使用单步估计来估计将要发生的奖励，但实际上接下来可能发生的事情有很多可能性，所以如果我们可以实际计算所有这些不同可能性的完整预期，我们会对将要发生的奖励有更好的估计。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ki"><img src="../Images/cb2fa3e047cac4980536e99d6dda2f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*5U-JRPlx-hJHuru7B1PnBA.png"/></div></figure><p id="1734" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有许多可能性的原因很简单，因为系统中存在随机性:我们的政策具有随机性，我们的MDP也具有随机性，但这种随机性可能非常显著，这意味着我们通过汇总在该轨迹中实际获得的回报而得到的单个样本估计值可能与实际期望值相差很远。</p><p id="8677" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个问题与政策梯度的高方差直接相关。联系在于，计算未来回报的政策梯度方法是对一个非常复杂的预期的单一样本估计。用于估计期望值的样本越少，估计值的方差就越大。如果我们能以某种方式产生一百万个样本，那么我们的方差就会低得多；如果我们能以某种方式精确地计算出这个期望值，我们就会有更低的方差。因此，如果我们可以获得真实的预期收益，定义为我们从时间步t的状态I和时间步t的行动I开始获得的收益总和的真实预期值，那么我们的政策梯度的方差将会低得多。然后，如果我们有这个Q函数，我们可以简单地把它代入奖励去得到一个较低的方差政策梯度。</p><p id="7072" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上一篇文章中，我们还看到了基线如何进一步降低政策梯度的方差。即使我们有了真正的Q函数，我们还能应用基线吗？答案是肯定的。我们可以减去一些量b:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kj"><img src="../Images/12a1ea1d07e9c95431e01934620ed791.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*pdSpzK-71dizI6eMp56bvQ.png"/></div></div></figure><p id="f014" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中平均报酬对b来说是个好选择，虽然不是最优选择。那么，我们的平均值是多少？我们可以对Q值进行平均，然后我们将得到这个吸引人的属性，即政策梯度将增加比平均水平更好的行动的概率，并降低比平均水平更差的行动的概率。但事实证明，我们可以进一步降低方差，因为基线实际上取决于状态。它不能依赖于动作，因为那会导致偏差，但是你可以让它依赖于状态。所以，如果你让基线依赖于状态，更好的做法是计算从该状态开始的所有可能性的平均回报。不仅仅是那个时间段所有可能性的平均回报，而是在那个特定的状态下。如果你对一个特定状态下的所有行为求Q值的平均值，这就是价值函数的定义。基线的一个非常好的选择是价值函数。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ko"><img src="../Images/2f25f27c9112e3de9907b6cebd896faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*L8gR2mZ_X5V6W88ri23M_w.png"/></div></figure><p id="9106" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q值和value函数之间的差异表示您对行动a平均优于您在时间步长t从样本I在状态s采取的平均行动的估计。这个Q-V项如此重要，以至于我们为它取了一个特殊的名称:我们称之为advantage函数，因为它表示在时间步长t从样本I采取的行动a与您期望在时间步长t从样本I在状态s获得的平均性能相比有多有利。</p><h2 id="6188" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">状态和状态-动作值函数</strong></h2><p id="0267" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">Q函数，或状态-行动值函数，表示如果你从状态s开始，采取行动a，然后遵循你的政策，你期望得到的总预期报酬:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ku"><img src="../Images/ce8d1520c839646ce639608cb2b618bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*vW5fPCOLX0AgLkVEe6DRfw.png"/></div></figure><p id="e0ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值函数是在当前策略下状态s中所有动作的期望值:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kv"><img src="../Images/ae3dde0831631cde52a00732a9e57d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*Q-u7qqFQXBVBAR5_DmUeIQ.png"/></div></figure><p id="9ce7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">优势函数是这两个量之间的差，优势函数表示行动a与状态s中策略的平均性能相比有多好。因此，如果我们简单地将策略的对数的梯度乘以优势值，就可以得到策略梯度的很好的估计值。</p><p id="ba52" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，在现实中我们不会有优势的正确值，我们将不得不估计它，例如使用一些函数近似值。所以，我们对优势的估计越好，我们的方差就越低。同样值得一提的是，我们将在今天的文章中讨论的演员-评论家方法不一定会产生优势函数的无偏估计。因此，虽然我们到目前为止讨论的政策梯度是无偏的，但如果你的优势函数是不正确的，那么你的整个政策梯度也可能是有偏的。通常，我们可以接受这一点，因为方差的巨大减少通常值得我们通过使用近似Q值和值函数获得的偏差的轻微增加。</p><p id="a9b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">总结一下，传统的政策梯度使用了一种蒙特卡罗方法来估计优势，这种方法是利用当前轨迹剩余部分中的一个样本，通过合计轨迹中的回报并减去基线来计算的。这是一个无偏但高方差的单样本估计，我们可以用一个近似优势函数来代替它，该函数本身通常由近似Q函数或近似值函数计算得出，从而得到一个低得多的方差估计。</p><h2 id="eee8" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">值函数拟合</strong></h2><p id="177a" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">拟合值函数:我们有三个可能的量，即Q、V和A。最终，我们想要A，但我们可能会问的问题是:我们应该拟合这三个量中的哪一个，我们应该将它拟合到什么？</p><p id="3626" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Q函数是当我们从状态s开始，采取行动a，然后遵循我们的政策时，我们将得到的期望回报值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kw"><img src="../Images/63dbdba0e1cdfa441f1b54e3aab1b75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*KV0YBQ8k_Y5VQEWowp6aTg.png"/></div></figure><p id="1614" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这样做的一个非常方便的特性是，因为s和a实际上不是随机变量，我们可以将Q函数改写为简单的当前奖励加上未来奖励的期望值——因为当前奖励取决于状态和行动，并且它们不是随机的。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kx"><img src="../Images/c9ccb1c65b75b13c7d9a55cf289a440f.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*KI3I_iIcbGQOMLKkGRCwJg.png"/></div></figure><p id="0625" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们增加的这个量就是在状态s (t+1)下价值函数的期望值，当我们在状态s ^ t下采取行动a时，我们会得到这个值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ky"><img src="../Images/bc2bb83910588e996af84185a19523fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*iIVDHx0l5wULAF4pn-x75A.png"/></div></figure><p id="255b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以类似地根据价值函数将Q函数写成当前回报加上下一时间步的价值函数的预期回报值。这里的期望当然是针对跃迁动力学的。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kz"><img src="../Images/1cb00a34181625d5b0279dcebce66833.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*aOCH0QwHU8lmXHD85tjJJg.png"/></div></figure><p id="3469" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以做一个小的近似，我们可以说，我们在当前轨迹中看到的实际状态s (t+1)是我们将得到的平均s (t+1)的某种代表:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es la"><img src="../Images/da69e9ccedfefbabf5ea609319cedecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*NQT-f7gGEQ5WGmM1pkUQnQ.png"/></div></figure><p id="c5d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这一点上，我们已经做了一个近似。这不是一个精确的等式，我们本质上是用单样本估计量来近似下一个时间步的状态分布。但现在，它只是一个时间步的单样本估计量，之后的一切仍然由价值函数表示。</p><p id="4e15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们想这么做的原因是，如果我们将这个Q值的近似等式代入优势等式，我们会得到这个非常吸引人的表达式，其中优势现在大约等于当前奖励加上下一个值减去当前值:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lb"><img src="../Images/55baf39b4e51882a0127fee24da37a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*7pNP2Hv-GcjyMvNMBUBVAw.png"/></div></figure><p id="7592" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这仍然是一个近似值，因为确切地说，下一个值需要在s (t+1)的所有可能值的预期中，而我们只是替换了我们看到的实际s(t+1)。但这个等式非常吸引人的地方在于，现在它完全取决于V，而V比Q或A更容易学习——因为Q和A都取决于状态和动作，而V只取决于状态。当你的函数逼近器依赖的东西越少，学习起来就越容易，因为你不需要那么多样本。所以也许我们应该做的只是拟合当前值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lc"><img src="../Images/79db932c6669fcdcfc79e118d09ac9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*WSIA6ZStsUhs6UAEMytsBA.png"/></div></figure><p id="d0a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们拟合当前值时，我们会有某种模型，例如将状态映射到近似值的神经网络。先说拟合现值的过程；这个过程有时被称为策略评估，因为当前值代表策略在每个状态下的值，所以计算值就是评估。</p><h2 id="e1bd" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">政策评估</strong></h2><p id="b34e" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们可以做的一件事是我们可以使用蒙特卡罗政策评估；从某种意义上说，这就是政策梯度的作用。我们多次运行我们的策略，然后将沿着策略生成的轨迹获得的回报相加，并将其用作策略总回报的无偏但高方差的估计。因此，我们可以说，状态s(t)的值大约是我们沿着访问状态s(t)的轨迹访问状态t后看到的所有奖励的总和。理想情况下，我们希望能够做的是，总结所有可能的轨迹，当你从那个状态开始，因为有不止一种可能性。不幸的是，在无模型设置中，这通常是不可能的，因为这要求我们能够重置回状态s(t ),并从该状态开始运行多个试验。一般来说，我们不认为我们能够做到这一点；我们只假设我们能够从初始状态开始进行多次试验。</p><h2 id="a2d9" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">采用函数逼近的蒙特卡洛评估</strong></h2><p id="99ea" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">如果我们对这种蒙特卡罗评估方案的价值函数使用神经网络函数逼近器，会发生什么情况？在我们访问的每个州，我们将剩余的奖励加在一起，这将产生我们的目标值，但然后不是将“奖励去”直接插入我们的政策梯度，我们将实际上为这些值拟合一个神经网络，这将实际上减少我们的方差。这是因为即使我们不能两次访问同一个状态，我们的函数逼近器或神经网络实际上会意识到我们在不同轨迹中访问的不同状态彼此相似。这本质上是一种概括，意味着您的函数近似器理解邻近的州应该采用相似的值。虽然比不上从同一个州造多条路，但还是蛮不错的。</p><p id="2076" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们这样做的方式是，我们将通过获取我们所有的展示来生成训练数据，并且对于沿着每个展示的每个状态，我们创建一个元组，该元组由来自时间步长t的样本I的状态s和对应于我们从s(i，t)开始看到的奖励总和的标签组成。我们将这些标签称为y(i，t)，它们是目标。然后我们将解决一个监督回归问题；我们将训练我们的神经网络价值函数，使其参数最小化价值函数的预测和该状态下价值的单样本蒙特卡罗估计之间的所有样本的平方误差之和。当然，如果我们的函数approximator大量过拟合，并在每个单个状态下精确地产生训练标签，那么与在我们的策略梯度中直接使用y(i，t)值相比，我们不会获得太多，但是如果我们得到泛化，我们实际上会得到更低的方差，因为这个函数现在将在相似的状态下平均出不相似的标签。</p><h2 id="8945" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">我们能做得更好吗？</h2><p id="0a89" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">当训练我们的价值函数时，我们想要的理想目标是从状态s(i，t)开始的回报的真实期望值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ld"><img src="../Images/7411a7197132cc0c5463f05d8eac8ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Xw7b_5a7NMWgAYIZy-J-iQ.png"/></div></figure><p id="3b22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们之前使用的蒙特卡洛目标使用该数量的单一样本估计值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es le"><img src="../Images/4fee2088fd4e83b1db1636a61ccd1573.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*hMH8VrMpFpXaT_MNMbiKqA.png"/></div></figure><p id="f03c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但我们也可以使用我们之前写出的关系，我们看到Q函数简单地等于当前时间步的回报加上从下一个时间步开始的预期回报。如果我们写出这个量，那么我们可以像以前一样进行同样的替换，实际上用我们对价值函数的估计来替换求和中的第二项。这是一个比我们的单样本估计更好的更低方差的估计。这表示:让我们使用我们看到的当前时间步的实际奖励加上我们看到的下一个时间步的实际状态的值。当然，我们不知道这个值，所以我们要用之前的函数近似器来近似这个值。我们假设我们之前的值是正确的，我们将把它插入到当前值的位置。这就是所谓的自助估计量。这里我们将直接使用之前的拟合值函数来估计这个量。</p><p id="b4b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，现在我们的训练数据将由我们看到的状态元组和标签组成，这些标签对应于我们在该时间步实际获得的奖励，加上实际下一个状态的价值函数估计值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lf"><img src="../Images/301c1a99e8d1ee00a47f70302dcf025f.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*neXjZGNQdiGPJj8NJsDjAg.png"/></div></figure><p id="46ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这一估计可能不正确，但随着我们重复这一过程，希望这些值会越来越接近正确的值，因为这些值是所有可能的未来回报的平均值，所以我们预计它们的方差会更低。现在我们的目标值是y(i，t):</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lg"><img src="../Images/cc78993ebab3d1fff956abcd2bd085c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*uAPVCe5dYwieVyID-pwuTw.png"/></div></figure><p id="8842" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的训练过程就像以前一样，将在监督下回归到这些目标上。这有时被称为自举估计，自举估计具有较低的方差，因为它不使用单样本估计量，但它也有较高的偏差，因为它可能是不正确的。</p><h2 id="e727" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">从评价到演员评论家</strong></h2><p id="a285" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">一个基本的批量演员-评论家算法可以看起来像这样:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lh"><img src="../Images/07a44fb594d4bedb886f4493eb56c447.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*exuo4Tb2eJNDzevDpBeGzg.png"/></div></figure><p id="d719" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第一步是通过我们的策略运行推广来生成样本。第二步是将我们的近似值函数拟合到那些抽样奖励中，而不是简单地将所有奖励相加。第三步，对于我们采样的每个状态-动作元组，我们将近似优势评估为奖励加上下一个状态的近似值减去当前状态的值。第四步，我们使用这些优势值通过在每个时间步取策略的对数的梯度并乘以近似优势来构造策略梯度估计器。最后，第五步是采取梯度下降步骤。</p><p id="e266" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在讨论政策评估时谈到的部分主要是第二步:我们如何拟合价值函数？我们讨论了如何做出不同的选择。我们可以用单样本蒙特卡罗估计来拟合，这意味着我们实际上总结了我们沿着这条轨迹得到的回报，这给了我们目标值。我们还讨论了如何使用bootstrap估计，其中我们使用实际观察到的奖励加上下一个状态的估计值，通过使用我们之前的价值函数估计器，这为我们提供了一些不同的选项来拟合critical。</p><p id="2216" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一点小题外话:让我们讨论一下，当我们在无限地平线设置中用这个自举规则拟合价值函数时会发生什么。我们可能遇到的麻烦是，如果一集的长度是无限的，那么每次我们应用这个bootstrap规则，我们的价值函数就会增加。我们如何修改这个规则，以确保我们可以总是有有限的值，并且我们可以处理无限的地平线设置？一个非常简单的技巧是假设我们想要更大的回报。我们要做的是在数值前面引入一个小乘数γ:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es li"><img src="../Images/3d3bc59d4a075fbaaa4f62b3fe006dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*J5zoCiM4pmScVK5NYIfJmQ.png"/></div></div></figure><p id="2d99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">0.99真的很好用。这个数字改变了你的MDP。想象一下有四种状态。你可以在这四种状态之间转换，这些转换由某种概率分布决定。当我们加上gamma时，我们可以这样想，我们增加了第五个状态，一个死亡状态，我们有一个负gamma的概率，在每个时间步转换到死亡状态:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lj"><img src="../Images/b0f744ae8a9c50d26b5a848e1a54853b.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*5vwLuaARyagjZVPA8aP2PA.png"/></div></figure><p id="09d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦我们进入死亡状态，我们就永远不会离开，所以在这个MDP中没有复活，我们的回报总是零——所以这意味着下一个时间步的期望值将总是表示为伽马乘以它在原始MDP中的期望值加1减去伽马乘以零。</p><h2 id="249d" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">折扣系数</strong></h2><p id="8477" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">首先，我们能否将贴现因子引入常规蒙特卡罗政策梯度？是的，我们可以只计算单个样本的奖励，然后将gamma值代入其中:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lk"><img src="../Images/634bf111cfb02ff8487f74de6cb50c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*iYk0BX3QMTtGZ5j-UZFg5g.png"/></div></figure><p id="0bd3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我这里的等式正是我们之前计算的奖励，只是现在我在奖励前面加上了这个gamma的t次方减去t。这意味着第一个奖励的乘数是1，第二个奖励的乘数是γ，第三个奖励的乘数是γ的平方，以此类推。所以，我们更容易受到及时发生在我们身边的奖励的影响。这种类型的估计量本质上是一个单样本版本，如果您将价值函数与贴现自助法一起使用，您将会得到这个版本</p><p id="473b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有一种方法可以将折扣引入蒙特卡洛政策梯度，这种方法非常相似，但有一个微妙而重要的区别:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ll"><img src="../Images/1448519dfe3748bd6d3fd034f94c0bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*X-CqWTHf6ieldbBubBHvOg.png"/></div></figure><p id="cc1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个选项是说，因为你有这个折扣，你不仅不太关心未来的回报，也不太关心未来的决策。所以，如果你从第一步开始，未来的回报不那么重要，你的决定也不那么重要——因为你未来的决定只会影响未来的回报。因此，结果是你实际上把每个时间步的梯度用gamma折现到t-1，本质上这意味着在第一个时间步做出正确的决定比在第二个时间步做出正确的决定更重要，因为第二个时间步不会影响第一个时间步的回报。</p><p id="ab91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">事实上，如果你真的想解决打折的问题，这是正确的做法。但实际上，我们通常使用的版本是选项一。考虑一个循环连续RL任务，其目标是使代理尽可能远地运行。虽然我们可以将此建模为一个有折扣奖励的任务，但在现实中，我们真的希望这个人尽可能跑得远，理想情况下是无限远——所以我们真的不想要折扣问题。我们想做的是，用折扣来帮助我们得到有限的值，这样我们就可以实际上做RL，但我们真正想做的是得到一个解决方案，可以运行任意长的时间。</p><p id="4557" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们将折扣引入演员-评论家算法时会发生什么？唯一改变的是第三步:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lm"><img src="../Images/5ec36bbf79835afdd20280cbf420f57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*kwjjbde6fh4uJQ08nA9y6Q.png"/></div></figure><p id="8762" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第三步中，您可以看到我们在下一步的值前面添加了一个gamma。一旦我们将演员-评论家算法带入无限地平线设置，我们可以做的事情之一是，我们实际上可以导出一个完全在线的演员-评论家方法。到目前为止，当我们谈到政策梯度时，我们总是在一种间歇的批处理模式设置中使用政策梯度，我们收集一批轨迹，每个轨迹一直运行到结束，然后我们使用该批来评估我们的梯度，并最终更新我们的政策。但是，当我们使用演员-评论家时，我们也可以有一个在线版本，在每一个时间点，我们也更新我们的政策。这是一个在线演员-评论家算法的样子:我们将采取一个行动，从概率分布中取样，并得到一个转换。在第二步中，我们通过使用奖励加上下一个状态的值作为我们的目标来更新我们的值函数。因为我们使用的是引导更新，所以我们不需要知道在接下来的时间点我们会得到什么状态；我们只需要下一步的质数。在第三步中，我们将优势评估为奖励加上下一个状态的价值函数减去当前状态的价值函数。然后，利用这一点，我们可以通过简单地将策略的对数梯度乘以我们刚刚计算的优势，来估计策略梯度，然后我们可以用策略梯度更新策略参数。然后重复这个过程，我们在每个时间点都这样做。</p><h2 id="3117" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">演员兼评论家的设计决策</strong></h2><p id="2fc3" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">当我们尝试做DRL时，这个食谱有一些问题。为了将这些算法实例化为DRL算法，我们必须选择如何表示价值函数和策略。我们可以做几个选择。</p><p id="1956" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个非常合理的开始选择是拥有两个完全独立的网络——一个网络将状态映射到值，另一个独立的网络将相同的状态映射到动作的分布。这些网络没有任何共同点。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ln"><img src="../Images/9a249ce0add0324218d801e69b6c006a.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*a-_fmFrjyfWN7rJSl242Fw.png"/></div></figure><p id="9a1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个方便的选择，因为它实现起来相对简单，训练起来也相当稳定。不利的一面是，它可能会被认为有些低效，因为演员和评论家之间没有共享特征；这可能是一个更重要的问题，例如，如果你直接从图像中学习，而这两个网络都是卷积神经网络，那么你可能真的希望它们共享它们的内部表示，这样，例如，如果价值函数首先找到了好的表示，那么策略就可以从中受益。在这种情况下，您可能会选择共享网络设计，其中一个中继线可能代表卷积层，然后您有单独的头部，一个用于值，一个用于策略操作分布。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lo"><img src="../Images/ec836fddaf6b7dc5baa4b2774eb3ef74.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*d3YWElARRYmFVEOuQNLSQQ.png"/></div></figure><p id="0db1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这种共享网络设计有点难以训练，可能会更不稳定，因为这些共享层受到非常不同梯度的影响。来自价值回归的梯度和来自政策梯度的梯度将处于不同的尺度上，它们将具有不同的统计数据，因此可能需要更多的超参数调整来稳定这种方法。但是原则上它会更有效率，因为你有这些共享的表示。</p><p id="b303" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我们得到一个实际可行的深度强化学习actor critic方法之前，还有另一个重要的问题需要讨论，那就是批量大小的问题。正如这里所描述的，这种算法是完全在线的，这意味着它一次学习一个样本；因此，它采取一个行动，获得一个转变，更新该转变的价值函数，然后更新该转变的策略。两个更新都只使用一个样本。现在，我们从深度学习的基础知识中知道，仅使用一个样本用随机梯度下降更新深度神经网络不会很好地工作，所以那些更新会有一点太多的差异。如果我们有一个批处理，这些更新都将工作得最好，我们可以得到一个批处理的方法之一是使用并行工作器</p><p id="e19c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">想法是这样的。这是最基本的并行化行动者-批评家。你可以运行多个模拟器，而不是只有一个数据收集线程，每个模拟器会在第一步中选择一个动作并生成一个转换，但是它们会使用不同的随机种子，所以它们会做一些稍微不同的事情。然后，在第二步和第四步中，您将使用来自所有线程的数据进行更新，因此更新是同步的，这意味着您在第一步中对每个线程执行一个步骤，然后将所有数据收集到您的批处理中，并使用它来更新值函数，然后使用它来同步更新策略。然后你重复这个过程。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lp"><img src="../Images/5dc2452e53987546371e921fb06b86c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*rcHPWT0AprrV9miTE0cWyA.png"/></div></figure><p id="4554" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，这将使您的批处理大小等于工作线程的数量。这可能有点贵，因为如果你想要一个32的批处理大小，那么你需要32个工作线程；但是它确实工作得很好。如果我们把它变成异步并行的actor-critical，它会变得更快，这意味着我们基本上放弃了同步点。现在，我们有了这些不同的线程，它们都以各自的速度运行，到了更新的时候，我们将引入最新的参数，并对该线程进行更新，但我们实际上不会同步所有线程。因此，一旦我们从所有工人那里积累了一些转换，我们就会进行更新。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lq"><img src="../Images/7c446774a178403e9f5404a3624b7c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*WYTioGf4hgwg1mFh0PrICw.png"/></div></figure><p id="7a0f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当然，这种方法的问题是，实际的转换可能不是由完全相同的参数收集的，因此，如果其中一个线程落后，可能它的转换是由旧的参与者生成的，那么基本上不会进行实际更新，直到您从更快的线程获得转换，而那些线程将使用新的参与者。所以总的来说，你在这里放入批处理的所有过渡可能是由稍微不同的参与者生成的。它们不会有太大的不同，因为这些线程不会以如此惊人的不同速率运行，但它们会有点滞后。事实上，异步更新在数学上并不等同于标准的同步更新，因为您有少量的延迟，这与您使用异步SGD得到的延迟类似；但是在实践中，通常结果是使方法异步带来的收益和性能超过了使用稍微老一点的参与者所带来的偏见。这里最关键的是“稍微”老一点，因为演员们不会太老——如果他们太老了，那么这当然行不通。</p><p id="19ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这可能会让我们思考另一个问题:在异步actor critic算法中，关键是我们可以使用由稍老的actor生成的转换。如果我们可以不使用由更老的演员生成的过渡，那么也许我们甚至不需要多线程，也许我们可以使用来自同一个演员的更老的过渡。基本上，我们可以使用一个历史记录，并从该历史记录中加载转换，甚至不用担心多线程。这就是非政策演员评论家背后的原则。非策略演员-评论家的设计是，现在你将有一个线程，你将使用那个线程更新，但是当你更新时，你将使用你已经看到的所有转换的重放缓冲区，并且你将实际上从重放缓冲区加载你的批处理。因此，您不必使用最新的转换，而是收集存储在重放缓冲区中的转换，然后从重放缓冲区中对整个批次进行采样。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lr"><img src="../Images/77ce52fdee2e42a77e2f2a35fbb75eec.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*wuO75paMZtHNXf_6Pz0mLA.png"/></div></figure><p id="6930" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这一点上，我们必须修改算法，因为天真地这样做是行不通的。我们从重放缓冲区加载的这一批肯定来自更老的策略，所以它不像异步actor critic，其中转换来自稍老的actor，我们可以忽略它；现在它来自更老的演员，我们不能忽视，我们必须实际改变我们的算法。当然，我们将使用之前看到的转换为这些更新中的每一个形成一个批处理。</p><p id="dd00" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在一个脱离策略的行动者-批评家算法中，我们将像往常一样从我们的最新策略中采取一个动作，获得相应的转换，然后不是使用该转换来学习，而是实际上将其存储在我们的重放缓冲区中；然后，我们将从重放缓冲区中抽取一批样本。然后，我们将使用批处理中每个转换的目标来更新我们的价值函数。然后，我们将评估我们批次中每个样本的优势，然后我们将使用该批次更新我们的策略梯度。现在，策略梯度也是n个样本的平均值，然后我们将像以前一样应用策略梯度。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ls"><img src="../Images/5ad2f5ab6e004e5402ac83fb89cee915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VQzbVixXVi9Q9RRrz7AwA.png"/></div></div></figure><p id="d7e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">但是，这个算法是行不通的。第一个问题是，当您从重放缓冲区加载这些过渡时，请记住这些过渡中的动作是由较老的参与者执行的，因此当您使用那些较老的参与者来获取动作并计算目标值时，这不会给您正确的目标值；它会给你其他演员的价值，而不是你最新的演员。第二个问题是，出于同样原因，你不能这样计算政策梯度；在计算策略梯度时，非常重要的是，我们实际上从我们的策略中获得了抽样操作，因为这需要是策略下的预期值。如果不是这样，我们就需要进行某种修正，比如重要性抽样。</p><p id="7454" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先说一下固定价值函数。如果价值函数告诉你，如果你从状态s开始，然后遵循政策，你将得到的预期回报，那么Q函数告诉你，如果你从状态s开始，然后采取行动a，然后遵循政策，你将得到的回报。注意，这里没有假设动作实际上来自您的策略，所以Q函数对任何动作都是有效的函数；只是在后续的所有步骤中你都要遵循政策。所以我们不会学习V，而是学习Q；这个神经网络将接收一个状态和一个动作，并输出一个Q值。否则，更新背后的原理是相同的:我们将计算目标值，然后我们将回归到这些目标值上。只是现在我们将这个动作作为Q函数的输入。另一种思考方式是，我们不能再假设我们的动作来自我们最新的策略，因此我们将学习一个对任何动作都有效的状态-动作值函数，这样我们甚至可以使用不是来自策略的动作来训练它，然后使用来自策略的动作来查询它。</p><p id="a799" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里的问题是，在我学习v之前，我在我的目标中使用v，但是现在我在学习Q，但是我仍然需要v作为我的目标值。那么我从哪里得到它？请记住，值函数也可以表示为Q函数的期望值，其中期望值是在您的策略下获得的，因此我们可以用在下一个操作a中计算的Q来替换目标值中的v，只是这个操作不是来自我们的重放缓冲区，它实际上是您的当前策略在发现自己处于该状态时会采取的操作。我们正在利用这样一个事实，即我们可以对我们的策略进行功能访问，这样我们就可以询问我们的策略，如果它发现自己处于这种旧状态，它会做什么，即使它实际上从未发生过。</p><p id="5f7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在我们已经解决了价值函数的问题，但是我们必须处理政策梯度。我们要做的就是使用相同的技巧，但这次我们要在下一个动作中使用，我们要在当前动作中使用。为了评估策略梯度，我们只需要从该州的最新策略中找出一个动作样本。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lt"><img src="../Images/f96a081a3f358925f70d00acb906ffdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*t8rAW90-W25v0nhc5BRbjg.png"/></div></figure><h2 id="5e84" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated">还剩下什么？</h2><p id="2f5e" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">还有一点问题，因为我们实际使用的状态不是来自最新政策的状态边缘，而是来自旧政策的状态边缘。不幸的是，我们在这里基本上什么也做不了，所以这将是这个过程中的偏差来源，我们只能接受它。直觉告诉我们为什么它不是那么糟糕，因为最终我们想要pθ(s)上的最优策略，但是我们在更广的分布上得到最优策略，所以我们的重放缓冲区将包含来自最新策略的样本以及来自其他旧策略的许多样本，以便分布比我们想要的更广。出于这个原因，我们不会错过我们最新政策中的州，我们也必须对一些我们可能永远不会访问的其他州很好，所以我们做了一些额外的工作，但我们不会错过重要的东西，这就是为什么这基本上可行的直觉。</p><h2 id="30ae" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">一些实施细节</strong></h2><p id="7e35" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">通常我们可以为第四步做很多更有趣的事情，例如我们可以使用一个叫做重新参数化的技巧，我将在另一篇文章中讨论它。还有许多更好的方法来拟合Q函数，我们将在接下来的两篇文章中讨论Q学习。</p><h2 id="18b1" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">作为基准线的批评家</strong></h2><p id="bc38" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">现在，我将讨论另一种使用critic的方法，即将critic作为政策梯度的基线，与我们目前讨论的标准参与者critic算法相比，这将有一些有趣的权衡。</p><p id="1770" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我们到目前为止讨论过的演员评论家的等式:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lu"><img src="../Images/93722575a6a283eed37a80ee1be0b383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*1EDIu9IpMcVFhex1o5zwxQ.png"/></div></figure><p id="4945" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">演员评论包括等级对数项乘以奖励加上gamma乘以下一个值减去当前值。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lv"><img src="../Images/851d199bea72ecc074c9b60f1b832384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*214H1qj3HVRs6ZAdVp5DcA.png"/></div></figure><p id="fe6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">政策梯度由梯度对数项乘以要获得的奖励总和减去基线组成。</p><p id="a503" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">actor critic policy gradient estimator的优势在于它极大地降低了方差，因为critic中的函数逼近器集成了所有可能发生的不同可能性，而不是依赖于单个样本；不幸的是，演员评论家梯度估计也有缺点，它不再是无偏的。原因是，如果你的价值函数有一点点不正确，这可能是因为它是一个在有限数量的样本上训练的函数逼近器，那么你就不能再表明在预期中这个梯度实际上会收敛。另一方面，最初的政策梯度是无偏的，所以即使它可能有很高的方差，它也会得出正确的值。</p><p id="382a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，我们能不能得到一个政策梯度估计量，它仍然是无偏的，但以其他方式使用批评家来降低方差？答案是，我们可以实际构建一个政策梯度估计器，它的方差比行动者批评者版本略高，但没有像政策梯度版本那样的偏差。我们能做到这一点的方法是使用所谓的状态依赖基线。结果是，我们可以证明，不仅当你减去任何常数b时，政策梯度保持无偏，如果你减去任何只依赖于状态而不依赖于行动的函数，它实际上仍然保持无偏。基线的一个非常好的选择是价值函数，因为你会期望这个单样本估计量等于价值函数，所以如果你使用价值函数作为基线，那么乘以梯度对数的数字在期望中应该更小，这意味着它们的方差更小，这意味着你的整个政策梯度的方差更小。这并没有像完全的演员-评论家算法那样降低方差，因为你仍然有未来奖励的总和，但它比恒定的基线低得多，而且它仍然是无偏的。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lw"><img src="../Images/0b029c529964a875fb41760df68a10ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*JFc4tu7-1sp-MFmhpQ6Y3w.png"/></div></figure><p id="d680" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们让基线依赖于更多的东西，比如状态和动作，会怎么样呢？这样我们会得到更低的方差吗？答案是肯定的，但在这一点上事情变得更加复杂，所以这就是我们现在要谈论的。在文献中，使用状态和动作相关基线的方法有时被称为控制变量。</p><h2 id="77ba" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">控制变量:依赖于行动的基线</strong></h2><p id="bfe5" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">真正的优势是当我们有一个依赖于状态的基线时，我们在策略梯度中使用的近似优势。它们是所有未来奖励的总和减去当前价值:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lx"><img src="../Images/934eaf8f753e566f98785409f41db727.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*QZGfkSo8YnncGZxzMt7DnA.png"/></div></figure><p id="f22d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这很好，因为它具有较低的方差，但如果减去Q值，我们可以使方差更低。如果我们这样做，它有一个很好的性质，如果你的批评是正确的，它实际上会变为零。因此，如果你的批评是正确的，你未来的行为不是太随机，那么你会期望这些量最终会收敛到零。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ly"><img src="../Images/b6f3081087cf5538e0eeb5882afc5ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*YgON82iD18lNOqSfxlxNYw.png"/></div></figure><p id="4687" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不幸的是，如果你把这个优势估计值插入你的政策梯度，政策梯度就不再正确，因为有一个你必须补偿的误差项。不像标准基线在期望值上积分为零，依赖于动作的基线不再积分为零，它积分为一个误差项，你必须考虑这个误差项。因此，如果您纳入一个依赖于状态和动作的基线，并考虑误差项，则您会得到以下等式:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lz"><img src="../Images/f0e8298a9ac77b5a1a0d94e298d5537a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvxncbmzTaO1HfUC6fpl0g.png"/></div></div></figure><p id="0ece" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个等式是政策梯度的有效估计，即使你的基线不依赖于行动，但在这种情况下，第二项基本上消失了；但是如果你的基线依赖于行动，第二项就不再是零了。所以第一项是你的政策梯度和你的基线，第二项是你的基线政策下的期望值的梯度。第二个任期看起来很像最初的政策梯度，那么这真的更好吗？事实证明，这实际上在某些情况下要好得多。例如，在许多情况下，第二项实际上可以非常非常准确地计算。如果你有离散的行动，你可以总结所有可能的行动；如果你有连续的行动，那么你可以对大量的行动进行采样，因为评估我们行动的预期不需要对新的状态进行采样，所以它不需要实际与世界互动，这意味着你可以从相同的状态生成更多的样本，这是我们以前必须实际进行整个部署时无法做到的。此外，在许多连续作用的情况下，如果仔细选择Q函数类上的分布类，这个积分也有解析解。例如，高斯分布下的二次函数的期望值有一个解析解。因此，在许多情况下，第二项可以用这样的方式计算，即它的方差为零或非常接近于零，而第一项是低方差。这种技巧可以用来提供一个非常低的方差策略梯度，特别是如果你能得到一个好的Q函数估计量。</p><p id="7dd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我们已经讨论了使用批评者的方法，并得到无偏的政策梯度估计量，但是我们是否也可以使用批评者并得到仍然有偏的政策梯度，但只是有一点点偏？</p><h2 id="3f9d" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">资格痕迹&amp; n步返回</strong></h2><p id="6f6d" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">首先，让我们看看演员-评论家算法中的优势估计器:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ma"><img src="../Images/aa05690c97fbfb43c47c74a78958b04e.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*jBrDj8oL9XXXjtT0WBN53A.png"/></div></figure><p id="5380" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这比政策梯度有更低的方差，但如果值是错误的，它总是至少有一点点错误，就会有更高的偏差。我们还可以看看我们在策略梯度算法中使用的蒙特卡罗优势估计器:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ma"><img src="../Images/403526f7c75af7962ada5054007ebad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*cspwJe0rqvIYGySCQdQPtg.png"/></div></figure><p id="b85c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这没有偏倚，但由于是单样本估计，它有更高的方差。所以在这一点上，你可能会想，我们能不能在两者之间找点什么呢？例如，我们是否可以将接下来五个时间步的奖励相加，而不是无限个时间步，然后将值放在最后？事实证明，我们可以利用这一点来很好地控制偏置方差权衡。有几个因素使得这很有趣。首先，当你使用折扣时，通常你的回报会随着时间的推移而减少，因为你对它进行了贴现，所以这意味着如果你不把价值函数放在下一个时间点，而是放在回报更小的未来，你从价值函数中得到的偏差就不是问题了。另一方面，从单样本估计量中得到的方差也是未来更大的问题。总的来说，我们对明天会发生什么比十年后会发生什么更有把握。通常，如果你从当前状态有这么多可能的轨迹，它们会向未来延伸得更远，在现在它们会更紧密地聚集在一起，这意味着你在未来会有更高的方差，而在现在会有更低的方差。这意味着，如果你要对这个轨迹的一大块使用单样本估计量，你宁愿在接近现在的时候使用它，然后在方差变得太大之前切断它，然后用方差低得多的价值函数替换它。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mb"><img src="../Images/92175280e2c9ec868eebcc68c344edd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*qxP4TCtA-vVoUuUTNIC34Q.png"/></div></figure><p id="a882" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">价值函数将考虑所有这些不同的可能性，但可能会有一些偏差。实现这一点的方法是构建一个所谓的n步收益估计器。在一个n步回报估计中，你把回报加起来，直到某个时间步n，然后你把它切掉，用价值函数代替。下面是n步回报估计量的公式:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mc"><img src="../Images/adc581d1ccb5500591d4bcff26a51bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*tANWIcVJiVTg0FrxKP8wUQ.png"/></div></figure><p id="310b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你把你的奖励加起来，但是现在你不把它们加起来直到无穷大，你把它们从t加起来直到t + n。你仍然减去你的基线，但是你有剩下的那块，从n + 1到无穷大的所有东西，你用你的价值函数替换。因此，您在时间步长t + n的状态下评估您的奖励函数，然后将γ乘以n。这是一个n步回报估计器，通常选择n大于1比n = 1更有效。一般来说，n越大，偏差越低，方差越高。最有效点通常不在n = 1处，也不在n =无穷大处，您可能希望使用一个中间值。我要讨论的最后一个技巧是，实际上推广n步回报估计量，并实际构建一个使用许多不同n步回报的混合估计量。</p><h2 id="4409" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">广义优势估计</strong></h2><p id="977c" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们必须只选择一个n吗？我们必须在某个特定的时间点做这个硬切片吗？如果我们真的想构造所有可能的n步收益估计量，并把它们平均在一起呢？下面是n步回报估计量的公式:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es md"><img src="../Images/6407406ad69f4daa6af0286006e11307.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*2n1aYQKeq67GxHUxTCvTKA.png"/></div></figure><p id="b0b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以构建一种融合估计量，我们称之为广义优势估计的GAE，它由所有可能的n步收益估计量的加权平均值组成，不同的n具有不同的权重。选择权重的方法是利用这样的认识，即如果使用小n，您会有更多的偏差，如果使用大n，则有更多的方差。因此，您通常更喜欢早些切割，因为这样您的方差较小，但您希望保留一些将来会更远的轨迹。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es me"><img src="../Images/19551b77c6c81dbc56f254e847bcb495.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*5VkW2TrQXOCBQroMc3x_LA.png"/></div></figure><p id="855b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">产生特别简单算法的一个相当不错的选择是使用指数衰减。如果你使用指数衰减，在每一个时间步，你有一个负λ乘以该时间步的价值函数，再加上GAE估计量的λ。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mf"><img src="../Images/1b7e604013f7ab9a7532e9a5770ae9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVxGoiLGfoSUGHA4nk853w.png"/></div></div></figure><p id="3915" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这就给出了所有可能的步进回报估计量的加权和，但事实证明，我们可以通过将GAE估计量表示为以下形式来收集这些项并得到一个更简单的形式:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mg"><img src="../Images/2ebcc52555b15a8deee659af56af8b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*pltBo7K5p5R_yt3GbIyWjg.png"/></div></figure><p id="0e23" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你只是在每个时间步长上构造这些一步优势估计量的加权和，用γ乘以λ到t的质数减去t，你就可以精确地恢复所有可能的n步收益的和。因此，这将允许您通过选择这个λ来权衡偏差和方差，其作用非常类似于折扣。较大的lambda看得更远，而较小的lambda使用更接近现在的价值函数。这与折扣有着非常相似的效果。这表明，即使我们没有λ，伽马的作用也是一种偏差方差权衡，事实上就是这样。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mh"><img src="../Images/7398546a45abfcb21aa828960f353858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*1yEW_npi7egkbk9O0FVHjA.png"/></div></figure><p id="e445" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">所以折扣本身也可以被解释为一种方差的减少，因为折扣越小，你的蒙特卡罗单样本估计量就会降低对未来回报的权重，而这些回报恰恰是你预期会有高方差的回报</p><h2 id="dce9" class="jd je hh bd jf jg jh ji jj jk jl jm jn ip jo jp jq it jr js jt ix ju jv jw jx bi translated"><strong class="ak">总结</strong></h2><p id="305c" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们讨论了actor-critic算法如何由几个部分组成:actor，即策略，和critic，即值函数。actor-critic算法可以被视为具有显著降低的方差的策略梯度版本，并且像策略梯度和所有其他RL算法一样，它由三个部分组成:一个部分是我们生成样本，一个部分是我们估计我们的回报，这现在对应于拟合价值函数，一个部分是我们使用梯度上升来更新我们的策略。就像在政策梯度中一样，政策评估是指拟合价值函数的过程，折扣因子是我们可以用来使无限视野的政策评估可行的东西。这有几种不同的解释；你可以把它解释为对死亡的恐惧，意思是你希望在死前早点收到奖励，但你也可以把它解释为一种方差减少的技巧。我们讨论了演员-评论家算法的设计；你可以有一个双头网络或两个独立的网络，你可以有批量远程或在线演员-评论家算法。此外，您可以使用并行性来获得大于1的小批量。我们还讨论了状态依赖基线，甚至动作依赖控制变量，作为使用critic的另一种方式，同时保持不偏不倚。我们讨论了如何将这些与n步收益或广义优势估计结合起来，广义优势估计将许多不同的n步收益估计平均在一起。</p><p id="ea6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mi">随时给我留言或者:</em></p><ol class=""><li id="b511" class="mj mk hh ig b ih ii il im ip ml it mm ix mn jb mo mp mq mr bi translated">通过<strong class="ig hi"> </strong> <a class="ae jc" href="https://www.linkedin.com/in/samuele-bolotta-841b16160/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae jc" href="https://twitter.com/SamBolotta" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我</li><li id="f286" class="mj mk hh ig b ih ms il mt ip mu it mv ix mw jb mo mp mq mr bi translated">跟随我在<a class="ae jc" rel="noopener" href="/@samuelebolotta">媒体</a></li></ol><div class="mx my ez fb mz na"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nb ab dw"><div class="nc ab nd cl cj ne"><h2 class="bd hi fi z dy nf ea eb ng ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nh l"><h3 class="bd b fi z dy nf ea eb ng ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ni l"><p class="bd b fp z dy nf ea eb ng ed ef dx translated">medium.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ke na"/></div></div></a></div></div></div>    
</body>
</html>