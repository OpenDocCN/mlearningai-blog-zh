# ä½¿ç”¨ LSTM çš„å‡æ–°é—»åˆ†ç±»å™¨â€”â€”å¸¦æºä»£ç 

> åŸæ–‡ï¼š<https://medium.com/mlearning-ai/fake-news-classifier-using-lstm-with-source-code-fun-project-4f0845e73c6a?source=collection_archive---------6----------------------->

æ‰€ä»¥åœ¨è¿™ä¸ªåšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LSTM å®ç°ä¸€ä¸ªå‡æ–°é—»åˆ†ç±»å™¨ã€‚æ‰€ä»¥æ²¡æœ‰ä»»ä½•è¿›ä¸€æ­¥çš„åŸå› ã€‚

**åœ¨è¿™é‡Œé˜…è¯»å¸¦æºä»£ç çš„å…¨æ–‡â€”**[https://machine learning projects . net/fake-news-classifier-using-lstm/](https://machinelearningprojects.net/fake-news-classifier-using-lstm/)

ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹è§†é¢‘â€”ã€https://youtu.be/XcHtSSKE6PI 

# è®©æˆ‘ä»¬å¼€å§‹å§â€¦

## æ­¥éª¤ 1-å¯¼å…¥å‡æ–°é—»åˆ†ç±»å™¨æ‰€éœ€çš„åº“ã€‚

```
import re
import nltk
import numpy as np
import pandas as pd
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import one_hot
from sklearn.metrics import confusion_matrix,accuracy_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout

nltk.download('stopwords')
```

## æ­¥éª¤ 2 â€”è¯»å–è¾“å…¥æ•°æ®ã€‚

```
df = pd.read_csv('train.csv')
df.dropna(inplace=True)
df.reset_index(inplace=True)
df.head(10)
```

![](img/a4bbb20083513265173fdc14d5ca6839.png)

## æ­¥éª¤ 3-åˆ›å»º X å’Œ y æ•°æ®ã€‚

```
X = df['title']
y = df['label']
```

*   å¯¹äº Xï¼Œæˆ‘ä»¬åªå–æ ‡é¢˜åˆ—ã€‚
*   å¯¹äº yï¼Œæˆ‘ä»¬åªå–æ ‡ç­¾åˆ—ã€‚

## æ­¥éª¤ 4-æ¸…ç†è¾“å…¥æ•°æ®ã€‚

```
ps = PorterStemmer()
corpus = []

for i in range(len(X)):
    text = X[i]
    text = re.sub('[^a-zA-Z]',' ',text)
    text = text.lower()
    text = text.split()
    text = [ps.stem(t) for t in text if t not in stopwords.words('english')]
    corpus.append(' '.join(text))
```

*   åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éå† Xï¼Œç„¶åç®€å•åœ°ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æ¸…ç†æˆ‘ä»¬çš„æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨è¯­æ–™åº“åˆ—è¡¨ä¸­ã€‚
*   é¦–å…ˆï¼Œæˆ‘ä»¬åªæ˜¯æŠŠæ‰€æœ‰ä¸æ˜¯å­—æ¯è¡¨çš„ä¸œè¥¿éƒ½æ¢æˆäº†ç©ºæ ¼ã€‚
*   ç„¶åæˆ‘ä»¬æŠŠå®ƒå°å†™å¹¶æ‹†åˆ†ã€‚
*   ç„¶åæˆ‘ä»¬æ£€æŸ¥å•è¯æ˜¯å¦ä¸åœ¨åœç”¨è¯ä¸­ï¼Œç„¶åé˜»æ­¢å®ƒã€‚
*   åªéœ€å°†è¿™äº›ç»“æœè¿æ¥èµ·æ¥ï¼Œç”¨å®ƒä»¬é€ ä¸€ä¸ªå¥å­ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°è¯­æ–™åº“åˆ—è¡¨ä¸­ã€‚

## æ­¥éª¤ 5 â€”ç¼–ç è¾“å…¥æ•°æ®ã€‚

```
vocab_size = 5000
sent_len = 20

one_hot_encoded = [one_hot(x,vocab_size) for x in corpus]
one_hot_encoded = pad_sequences(one_hot_encoded,maxlen=sent_len)
one_hot_encoded[0]
```

*   è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ [one_hot](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/one_hot) å°†æ–‡æœ¬æ•°æ®ç¼–ç æˆæ•°å­—æ•°æ®ã€‚
*   è®°ä½è¿™ä¸ªçƒ­ç‚¹ä¸æ˜¯ 0 å’Œ 1ã€‚åœ¨è¿™ç§ä¸€æ¬¡æ€§ç¼–ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•£åˆ—æ³•ç»™å•è¯åˆ†é…ä¸€ä¸ªéšæœºæ•°ã€‚éšæœºå­—ä» 0-vocab_size çš„èŒƒå›´ä¸­é€‰æ‹©ã€‚
*   ç„¶åæˆ‘ä»¬ç”¨ 0 å¡«å……åºåˆ—ï¼Œä½¿æ¯ä¸€è¡Œé•¿åº¦ç›¸åŒã€‚
*   ç„¶åæˆ‘ä»¬ç®€å•åœ°æ£€æŸ¥è¿™ä¸¤ä¸ªæ“ä½œä¹‹åï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå¥å­æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚

![](img/74857e3d1cc2b7ed9bc17a8fb2b95e27.png)

## æ­¥éª¤ 6 â€”å¤„ç† X å’Œ y æ•°æ®ã€‚

```
X = np.array(one_hot_encoded)
y = np.array(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
```

*   å°† X å’Œ y è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå¹¶ä½¿ç”¨ [traintestsplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) ç®€å•æ‹†åˆ†æ•°æ®ã€‚

## æ­¥éª¤ 7 â€”åˆ›å»ºæ¨¡å‹ã€‚

```
no_of_output_features = 40

model = Sequential()

model.add(Embedding(vocab_size,no_of_output_features,input_length=sent_len))
model.add(Dropout(0.5))
model.add(LSTM(100))
model.add(Dropout(0.5))
model.add(Dense(1))

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model.summary()
```

*   åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ­£åœ¨åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚
*   æˆ‘ä»¬çš„æ¨¡å‹åªæœ‰ 4 å±‚ã€‚
*   ç¬¬ä¸€å±‚æ˜¯åµŒå…¥å±‚ï¼Œå®ƒå°†æŠŠæˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„æ•°å­—æ•°ç»„è½¬æ¢æˆä¸€ä¸ª 40 ç»´çš„å‘é‡ï¼Œåé¢æ˜¯ä¸€ä¸ª [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) å±‚ã€‚
*   ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ‰ 100 ä¸ªèŠ‚ç‚¹çš„ [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) å±‚ï¼Œåé¢æ˜¯ä¸€ä¸ªä¸¢å¼ƒå±‚ã€‚
*   è„±è½å±‚ç”¨äºé˜²æ­¢è¿‡åº¦è´´åˆã€‚

![](img/5f8ed029c45458444f8c22bcc8e18591.png)

## æ­¥éª¤ 8-è®­ç»ƒå‡æ–°é—»åˆ†ç±»å™¨æ¨¡å‹ã€‚

```
model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=64,epochs=40)
```

![](img/9a1c645ca7e797ab21d9d08191d7a28c.png)

## æ­¥éª¤ 9â€”â€”æ£€æŸ¥æ¨¡å‹çš„æŒ‡æ ‡ã€‚

```
pred = model.predict_classes(X_test)
confusion_matrix(y_test,pred)
```

*   åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬åªæ˜¯æ‰“å°[æ··æ·†çŸ©é˜µ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)æ¥æ£€æŸ¥æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ã€‚

![](img/20874c4ba64f0d476d32084d34598bf5.png)

## æ­¥éª¤ 10-æ£€æŸ¥å‡æ–°é—»åˆ†ç±»å™¨æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

```
accuracy_score(y_test,pred)
```

![](img/6854777167ace86cc1e2754ca418606b.png)

å¦‚æœå¯¹è¿™ä¸ªè¯é¢˜æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·é€šè¿‡ç”µå­é‚®ä»¶æˆ– LinkedIn è”ç³»æˆ‘ã€‚æˆ‘å·²ç»å°½åŠ›è§£é‡Šè¿™ä¸ªä»£ç äº†ã€‚

***æ¢ç´¢æ›´å¤šæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€NLPã€Flask é¡¹ç›®è®¿é—®æˆ‘çš„åšå®¢â€”*** [***æœºå™¨å­¦ä¹ é¡¹ç›®***](https://machinelearningprojects.net/)

**å¦‚éœ€è¿›ä¸€æ­¥çš„ä»£ç è§£é‡Šå’Œæºä»£ç ï¼Œè¯·è®¿é—®æ­¤å¤„**â€”[https://machine learning projects . net/fake-news-classifier-using-lstm/](https://machinelearningprojects.net/fake-news-classifier-using-lstm/)

*æ‰€ä»¥è¿™å°±æ˜¯æˆ‘å†™ç»™è¿™ä¸ªåšå®¢çš„æ‰€æœ‰å†…å®¹ï¼Œæ„Ÿè°¢ä½ é˜…è¯»å®ƒï¼Œæˆ‘å¸Œæœ›ä½ åœ¨é˜…è¯»å®Œè¿™ç¯‡æ–‡ç« åä¼šæœ‰æ‰€æ”¶è·ï¼Œç›´åˆ°ä¸‹ä¸€æ¬¡ğŸ‘‹â€¦*

***çœ‹æˆ‘ä»¥å‰çš„å¸–å­:*** [***å¥‡å¼‚å€¼åˆ†è§£***](https://machinelearningprojects.net/singular-value-decomposition/)

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)