<html>
<head>
<title>Sentiment Analysis using LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTM的情感分析</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/sentiment-analysis-using-lstm-21767a130857?source=collection_archive---------0-----------------------#2021-05-04">https://medium.com/mlearning-ai/sentiment-analysis-using-lstm-21767a130857?source=collection_archive---------0-----------------------#2021-05-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><ul class=""><li id="77bb" class="ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv bi translated">技术栈:Python，Scikit-Learn，Tensorflow，Keras</li><li id="41fa" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated"><a class="ae jb" href="https://github.com/PSSABISHEK/sentiment_analysis_LSTM/blob/master/yeet_boi.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本网址</a></li><li id="4b32" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated"><a class="ae jb" href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" rel="noopener ugc nofollow" target="_blank">数据集URL </a></li><li id="f356" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated"><a class="ae jb" href="https://github.com/PSSABISHEK/sentiment_analysis_LSTM/raw/master/proposal.docx" rel="noopener ugc nofollow" target="_blank">项目建议书</a></li><li id="3369" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated"><a class="ae jb" href="https://youtu.be/zbX7iU6bWQ0" rel="noopener ugc nofollow" target="_blank">演示网址</a></li><li id="b9ec" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated"><a class="ae jb" href="https://github.com/PSSABISHEK/sentiment_analysis_LSTM/tree/master/feature_screenshots" rel="noopener ugc nofollow" target="_blank">特征草图</a></li></ul><p id="8d73" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">在这个数据丰富的时代，企业开始利用这一机会实现指数级增长。今天的营销人员理所当然地痴迷于指标。但不要忘记，客户不仅仅是一个数据点。很容易忽略客户的感受和情绪，这可能很难量化。然而，在技术的帮助下，我们公司可以做到这一点。情绪本质上与感觉有关:态度、情绪和观点。情感分析是指应用自然语言处理和文本分析技术从一段文本中识别和提取主观信息的实践。借助我开发的这个情感分析工具，企业可以更好地了解他们的客户和目标受众，并在开发将在市场上大规模销售的产品时做出明智的决定。在了解到任何可持续发展企业的主要支柱是营销团队后，我觉得这个工具为他们增加了价值。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="4e06" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">目标</strong></h1><p id="4c8a" class="pw-post-body-paragraph jc jd hh ig b ih kw je jf ij kx jg jh il ky jj jk in kz jm jn ip la jp jq ir ha bi translated">目标是在数据集上训练的长短期记忆(LSTM)模型的帮助下，预测用户对给定评论的情感。评论的情绪是二元的，这意味着IMDB评级&lt; 5 results in a sentiment score of 0, and rating &gt; =7的情绪得分为1。</p><h1 id="a3f8" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">数据区</h1><ul class=""><li id="785a" class="ie if hh ig b ih kw ij kx il lg in lh ip li ir is it iu iv bi translated">id —每个审阅的唯一id</li><li id="153a" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">情绪——评论的情绪；1代表正面评价，0代表负面评价</li><li id="70bb" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">审阅—审阅的文本</li></ul></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="75b5" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated"><em class="lj">现在让我们了解一下构建这个应用程序所使用的技术。</em></p><h1 id="2d50" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">长短期记忆(LSTM):</h1><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lk"><img src="../Images/eac979ab6d764ddd05510c63d1c92d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4u3C6HEADE3GooBh.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">LSTM Network</figcaption></figure><p id="e7b8" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">LSTM的核心思想是细胞状态，它是各种各样的门。细胞状态就像一条传输高速公路，沿着序列链一路传输相关信息。你可以把它想象成网络的“记忆”。理论上，细胞状态可以在整个序列处理过程中携带相关信息。因此，即使是来自较早时间步骤的信息也可以传递到较晚的时间步骤，从而减少短期记忆的影响。当细胞状态继续它的旅程时，信息通过门被添加到细胞状态或从细胞状态移除。这些门是不同的神经网络，决定细胞状态允许哪些信息。盖茨夫妇可以在训练中了解哪些信息是应该保留或忘记的。</p><p id="6047" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">从技术上讲，LSTM输入只能理解实数。将符号转换为数字的一种方法是根据出现的频率为每个符号分配一个唯一的整数。例如，在上面的文本中有112个独特的符号。清单2中的函数用以下条目构建了一个字典["，":0 ] [ "the" : 1 ]，…，[ "council" : 37 ]，…，[ "spoke" : 111 ]。还生成反向字典，因为它将用于解码LSTM的输出。</p><p id="b5f7" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">LSTM主要用于NLP和时间序列预测。LSTM真的很强大，用RNN取得的成果也可以用LSTM取得。</p><h1 id="4284" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">符号化:</h1><blockquote class="ma mb mc"><p id="01e0" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated">标记化实质上是将一个短语、句子、段落或整个文本文档分割成更小的单元，如单个单词或术语。这些更小的单元中的每一个都被称为令牌。</p></blockquote><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mg"><img src="../Images/8ff5b87c83874a022eee7e7c42122176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RYR0xye8KVkQD-9m.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Tokenization</figcaption></figure><p id="95f4" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">在python中有多种方法来标记句子，下面是几个例子</p><ul class=""><li id="f877" class="ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv bi translated">拆分()</li><li id="f173" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">正则表达式</li><li id="526e" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">NLTK</li><li id="4903" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">宽大的</li><li id="c8ed" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir is it iu iv bi translated">克拉斯</li></ul><h1 id="ad4f" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">提前停止:</h1><p id="0b49" class="pw-post-body-paragraph jc jd hh ig b ih kw je jf ij kx jg jh il ky jj jk in kz jm jn ip la jp jq ir ha bi translated">训练神经网络的一个问题是选择要使用的训练时期的数量。</p><p id="962f" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">太多的时期会导致训练数据集的过度拟合，而太少的时期会导致模型的欠拟合。早期停止是一种方法，允许您指定任意大量的训练时期，并在模型性能在等待验证数据集上停止改善时停止训练。</p><p id="3ade" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">在多次试验后，早期停止的最佳准确度被设置为95%。因此，每次在模型训练期间，当训练精度达到设定值时，它将保存权重并停止训练。</p><p id="e6b0" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">Keras支持各种指标:<em class="lj"> val_loss、val_acc、train_loss、train_acc </em>以监控提前停车</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mh"><img src="../Images/80fe3abde6b2312f55c943761d9e0ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*imEf0LfEw9mbZYMy.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Early stopping with Loss as metric</figcaption></figure><h1 id="25f0" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">保存模型检查点:</h1><p id="bb7e" class="pw-post-body-paragraph jc jd hh ig b ih kw je jf ij kx jg jh il ky jj jk in kz jm jn ip la jp jq ir ha bi translated">经常会出现模型训练需要几个小时甚至几天的情况，在这种情况下，我们更愿意保存模型检查点。训练完神经网络后，您可能希望保存它以供将来使用并部署到生产中。那么，什么是保存的神经网络模型呢？它主要包含网络设计或图形以及我们训练过的网络参数值。Tensorflow提供了两种保存和恢复进度的方法。</p><p id="fda4" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">检查点捕获所有参数的精确值(tf。可变对象)由模型使用。<strong class="ig hi">检查点不包含由模型</strong>定义的计算的任何描述，因此通常只有当使用保存的参数值的源代码可用时才有用。</p><p id="02e2" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">另一方面，SavedModel格式<strong class="ig hi">除了参数值(检查点)之外，还包括由模型</strong>定义的计算的序列化描述。这种格式的模型是<em class="lj">独立于创建模型的源代码的</em>。因此，它们适合通过TensorFlow Serving、TensorFlow Lite、TensorFlow.js或其他编程语言(C、C++、Java、Go、Rust、C#等)的程序进行部署。).</p><h1 id="96f3" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">准确性是验证模型的更好的指标吗？</h1><p id="24e7" class="pw-post-body-paragraph jc jd hh ig b ih kw je jf ij kx jg jh il ky jj jk in kz jm jn ip la jp jq ir ha bi translated">答案是否定的。就拿这个例子来说吧。</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mi"><img src="../Images/2e9086f6884b63392dfb1d93af62c193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*fR_kwbK3-6bEF5qw.jpeg"/></div></figure><p id="5e17" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">很容易，你会注意到这个模型的准确率非常非常高，达到99.9%！！哇！</p><p id="c733" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">但是…(你知道这是迟早的事，对吗？)如果我提到这里的阳性实际上是一个生病的人，他携带着一种可以快速传播的病毒，会怎么样？还是这里的阳性代表诈骗案？或者这里的阳性代表恐怖分子，模型说它不是恐怖分子？你明白了。在我提出的这三种情况下，错误分类的实际阳性(或假阴性)的成本非常高。</p><p id="05c2" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">好了，现在您意识到在选择最佳模型时，准确性并不是最重要的模型指标……接下来呢？</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mj"><img src="../Images/98586c7e50b7a1a9bb1ec5a1cc1fc39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NzM1eXD_GbP-wZSm.jpeg"/></div></div></figure><p id="2861" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">F1分数可以更好地显示模型的性能。</p><p id="67b2" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">我用公式计算出来的</p><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mk"><img src="../Images/7a4ad1e2ce0ceca8ac3ca7daed92b294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*HmuHnRdL2z_mzCkn7omD7w.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">F1 Score</figcaption></figure></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><p id="f5e5" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">让我们从如何构建这个分类器开始。</p><blockquote class="ma mb mc"><p id="9486" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 1。导入必要的库</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="8cc8" class="mq jz hh mm b fi mr ms l mt mu">import numpy as np<br/>import pandas as pd<br/>import re<br/>import string<br/>import os</span><span id="1f30" class="mq jz hh mm b fi mv ms l mt mu">import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>from wordcloud import WordCloud, STOPWORDS<br/>stopwords = set(STOPWORDS)</span><span id="c470" class="mq jz hh mm b fi mv ms l mt mu">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>color = sns.color_palette()</span><span id="f9ab" class="mq jz hh mm b fi mv ms l mt mu">from sklearn.metrics import confusion_matrix</span><span id="ab1a" class="mq jz hh mm b fi mv ms l mt mu">import warnings<br/>warnings.filterwarnings('ignore')</span></pre><blockquote class="ma mb mc"><p id="d69f" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 2。使用pandas </strong>读取训练和测试文件</p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="805b" class="mq jz hh mm b fi mr ms l mt mu">df_train = pd.read_csv("./labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)<br/>df_test=pd.read_csv("./testData.tsv", header=0, delimiter="\t", quoting=3)</span></pre><blockquote class="ma mb mc"><p id="0a5f" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 3。数据预处理:</strong></p></blockquote><p id="9557" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">对于任何NLP问题，在将原始数据输入模型之前，必须对其进行清理，并以所需的格式进行处理。在这里，我将数据转换成小写字母，并使用字符串库删除标点符号，这比我在参考资料中看到的更快。此外，使用NLTK库删除了停用词(如“the”、“a”、“an”、“in”)，因为这些词在索引时无关紧要。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="540b" class="mq jz hh mm b fi mr ms l mt mu">def data_cleaning(raw_data):<br/>    raw_data = raw_data.translate(str.maketrans('', '', string.punctuation + string.digits))<br/>    words = raw_data.lower().split()<br/>    stops = set(stopwords.words("english"))<br/>    useful_words = [w for w in words if not w in stops]<br/>    return( " ".join(useful_words))</span><span id="70c7" class="mq jz hh mm b fi mv ms l mt mu">df_train['review']=df_train['review'].apply(data_cleaning)<br/>df_test["review"]=df_test["review"].apply(data_cleaning)</span></pre><blockquote class="ma mb mc"><p id="7b5c" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 4。现在，让我们使用wordcloud库来可视化数据集中的主要单词。</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="99c2" class="mq jz hh mm b fi mr ms l mt mu">def generate_wordcloud(data, title = None):<br/>    wordcloud = WordCloud(<br/>        stopwords=stopwords,<br/>        max_words=100,<br/>        max_font_size=40, <br/>        scale=4).generate(str(data))</span><span id="20af" class="mq jz hh mm b fi mv ms l mt mu">fig = plt.figure(1, figsize=(15, 15))<br/>    plt.axis('off')<br/>    plt.imshow(wordcloud)<br/>    plt.show()</span><span id="493f" class="mq jz hh mm b fi mv ms l mt mu">generate_wordcloud(df_train["review"])</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es mw"><img src="../Images/69939aca499b33ea0da0c68124780d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQYXFHEz_fUO1BiAHMTcJQ.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Wordcloud</figcaption></figure><blockquote class="ma mb mc"><p id="001d" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 5。从Tensorflow导入构建LSTM模型所需的API</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="37e4" class="mq jz hh mm b fi mr ms l mt mu">import tensorflow as tf<br/># from tensorflow import keras</span><span id="2ce8" class="mq jz hh mm b fi mv ms l mt mu">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, Flatten<br/>from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D<br/>from tensorflow.keras.models import Model, Sequential<br/>from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers</span></pre><blockquote class="ma mb mc"><p id="7f0a" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 6。分词</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="70c4" class="mq jz hh mm b fi mr ms l mt mu">y = df_train["sentiment"].values<br/>train_reviews = df_train["review"]<br/>test_reviews = df_test["review"]</span><span id="e730" class="mq jz hh mm b fi mv ms l mt mu">max_features = 6000<br/>tokenizer = Tokenizer(num_words=max_features)<br/>tokenizer.fit_on_texts(list(train_reviews))<br/>list_tokenized_train = tokenizer.texts_to_sequences(train_reviews)<br/>list_tokenized_test = tokenizer.texts_to_sequences(test_reviews)</span></pre><blockquote class="ma mb mc"><p id="e215" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 7。现在让我们定义回调，这是执行早期停止和保存模型检查点所必需的。</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="14e0" class="mq jz hh mm b fi mr ms l mt mu">class myCallback(tf.keras.callbacks.Callback):<br/>    def on_epoch_end(self, epochs, logs={}):<br/>        if logs.get('accuracy') &gt; 0.95:<br/>            print('\n Stopped Training!\n')<br/>            self.model.stop_training = True</span><span id="7941" class="mq jz hh mm b fi mv ms l mt mu">def train_model(model, model_name, n_epochs, batch_size, X_data, y_data, validation_split):    <br/>    checkpoint_path = model_name+"_cp-{epoch:04d}.ckpt"<br/>    checkpoint_dir = os.path.dirname(checkpoint_path)<br/>    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,  save_weights_only=True, verbose=1)<br/>    callbacks = myCallback()<br/>    history = model.fit(<br/>        X_data,<br/>        y_data,<br/>        steps_per_epoch=batch_size,<br/>        epochs=n_epochs,<br/>        validation_split=validation_split,<br/>        verbose=1,<br/>        callbacks=[cp_callback]<br/>    )<br/>    return history</span></pre><blockquote class="ma mb mc"><p id="b84a" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated">8。定义生成图形功能</p></blockquote><p id="07e7" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">为了保持代码的整洁，让我们创建一个函数来绘制图形，因为我们需要在每次实验后查看模型性能的变化。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="3940" class="mq jz hh mm b fi mr ms l mt mu">def generate_graph(history):<br/>    plt.plot(history.history['accuracy'], 'b')<br/>    plt.plot(history.history['val_accuracy'], 'r')<br/>    plt.title('Model Accuracy'),<br/>    plt.xlabel('Epochs')<br/>    plt.ylabel('Accuracy')<br/>    plt.legend(['Train', 'Validation'], loc='upper left')<br/>    plt.show()</span></pre><blockquote class="ma mb mc"><p id="c7c1" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 9。定义预测函数</strong></p></blockquote><p id="4203" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">此函数处理F1分数的计算。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="4ec6" class="mq jz hh mm b fi mr ms l mt mu">def predict_func(model):<br/>  prediction = model.predict(X_test)<br/>  y_pred = (prediction &gt; 0.5)</span><span id="acf7" class="mq jz hh mm b fi mv ms l mt mu">  df_test["sentiment"] = df_test["id"].map(lambda x: 1 if int(x.strip('"').split("_")[1]) &gt;= 5 else 0)<br/>  y_test = df_test["sentiment"]</span><span id="adc0" class="mq jz hh mm b fi mv ms l mt mu">  cf_matrix = confusion_matrix(y_pred, y_test)<br/>  f1_score_calc = cf_matrix[0][0] / (cf_matrix[0][0] + 0.5 * (cf_matrix[0][1] + cf_matrix[1][0]))<br/>  print('F1-score: %.3f' % f1_score_calc)<br/>  print("Confusion Matrix : ", cf_matrix)</span><span id="27a5" class="mq jz hh mm b fi mv ms l mt mu">  return f1_score_calc</span></pre><blockquote class="ma mb mc"><p id="7ad6" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 10。实验</strong></p></blockquote><p id="5af7" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">10.1模型A:这将是基础模型，它将被用来作为将来构建模型的基准。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="6286" class="mq jz hh mm b fi mr ms l mt mu">class Model_A():<br/>    def __new__(self):<br/>        inp = Input(shape=(max_length, ))<br/>        embed_size = 128<br/>        x = Embedding(max_features, embed_size)(inp)<br/>        x = LSTM(60, return_sequences=True, name='lstm_layer')(x)<br/>        x = GlobalMaxPool1D()(x)<br/>        x = Dropout(0.1)(x)<br/>        x = Dense(50, activation="relu")(x)<br/>        x = Dropout(0.1)(x)<br/>        x = Dense(1, activation="sigmoid")(x)<br/>        model = Model(inputs=inp, outputs=x)<br/>        model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])<br/>        <br/>        return model</span><span id="96ba" class="mq jz hh mm b fi mv ms l mt mu">model_a = Model_A()<br/>history_a = train_model(model_a, "model_a", 10, 64, X_train, y, 0.2)<br/>model_a_score = predict_func(model_a)</span><span id="10a6" class="mq jz hh mm b fi mv ms l mt mu">--Output--<br/>F1-score: 0.663 <br/>Confusion Matrix :  [[12106 11916]  [  394   584]]</span></pre><p id="82da" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">模型摘要</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="5dd1" class="mq jz hh mm b fi mr ms l mt mu">Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 370)]             0         <br/>_________________________________________________________________<br/>embedding (Embedding)        (None, 370, 128)          768000    <br/>_________________________________________________________________<br/>lstm_layer (LSTM)            (None, 370, 60)           45360     <br/>_________________________________________________________________<br/>global_max_pooling1d (Global (None, 60)                0         <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 60)                0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 50)                3050      <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 50)                0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 51        <br/>=================================================================<br/>Total params: 816,461<br/>Trainable params: 816,461<br/>Non-trainable params: 0<br/>_________________________________________________________________<br/>None</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mx"><img src="../Images/9689c6579ff29b7aae3b8ed3b911c7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*gmPe1rcvGxm-JYauWl9VYQ.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model A</figcaption></figure><p id="5b0f" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">10.2模型B:该模型的变化是我使用了adam optimizer和SpatialDropout1D图层，这两个图层的功能与Dropout相同，但是，它会删除整个1D要素地图，而不是单个元素。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="5c15" class="mq jz hh mm b fi mr ms l mt mu">class Model_B():<br/>    def __new__(self):<br/>        inp = Input(shape=(max_length, ))<br/>        x = Embedding(max_features, 128)(inp)<br/>        x = SpatialDropout1D(0.25)(x)<br/>        x = LSTM(100, dropout=0.5)(x)<br/>        x = Dropout(0.5)(x)<br/>        x = Dense(1, activation='sigmoid')(x)<br/>        model = Model(inputs=inp, outputs=x)<br/>        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])<br/>        return model<br/>    <br/>model_b = Model_B()<br/>history_b = train_model(model_b, "model_b", 10, 64, X_train, y, 0.2)<br/>model_b_score = predict_func(model_b)</span><span id="baec" class="mq jz hh mm b fi mv ms l mt mu">--Output--<br/>F1-score: 0.861 <br/>Confusion Matrix :  [[10974  2012]  [ 1526 10488]]</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es my"><img src="../Images/f29ed50849b4be2a876f102a99a80331.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*hTmh_bjmyDRUrQ15SuBdZg.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model B</figcaption></figure><p id="dd2b" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">10.3模型C: <strong class="ig hi"> </strong>我在训练时使用了较小的批量，并增加了训练时期的数量，以检查模型长期运行的稳定性，假设早期停止将防止模型过度拟合。此外，我还使用了双向LSTM，这有助于模型更好地运行，因为LSTM通常是单向的。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="ecb2" class="mq jz hh mm b fi mr ms l mt mu">class Model_C():<br/>  def __new__(self):<br/>    embed_size = 128<br/>    model = Sequential()<br/>    model.add(Embedding(max_features, embed_size))<br/>    model.add(Bidirectional(LSTM(75, return_sequences = True)))<br/>    model.add(GlobalMaxPool1D())<br/>    model.add(Dense(16, activation="relu"))<br/>    model.add(Dropout(0.03))<br/>    model.add(Dense(8, activation="relu"))<br/>    model.add(Dropout(0.1))<br/>    model.add(Dense(1, activation="sigmoid"))<br/>    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>    <br/>    return model<br/>    <br/>model_c = Model_C()<br/>history_c = train_model(model_c, "model_c", 10, 128, X_train, y, 0.2)<br/>model_c_score = predict_func(model_c)</span><span id="3322" class="mq jz hh mm b fi mv ms l mt mu">--Output--<br/>F1-score: 0.863 <br/>Confusion Matrix :  [[10993  1993]  [ 1507 10507]]</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mx"><img src="../Images/a330d580f351c0b6dc4e3c7c6372ceaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*bVeZo9zGay3ctHTVdr_aBw.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model C</figcaption></figure><p id="97dd" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">10.4模型D: <strong class="ig hi"> </strong>定义一个不太复杂的模型以达到相同的精度。这在生产中非常方便，因为我们需要用更少的计算成本获得更高的模型吞吐量。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="0aaf" class="mq jz hh mm b fi mr ms l mt mu">class Model_D():<br/>  def __new__(self):<br/>    embed_size = 64<br/>    model = Sequential()<br/>    model.add(Embedding(max_features, embed_size))<br/>    model.add(LSTM(50, return_sequences = True))<br/>    model.add(GlobalMaxPool1D())<br/>    model.add(Dense(16, activation="relu"))<br/>    model.add(Dropout(0.8))<br/>    model.add(Dense(1, activation="sigmoid"))<br/>    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="9b41" class="mq jz hh mm b fi mv ms l mt mu">return model</span><span id="0242" class="mq jz hh mm b fi mv ms l mt mu">model_d = Model_D()<br/>history_d = train_model(model_d, "model_d", 20, 16, X_train, y, 0.2)</span><span id="0255" class="mq jz hh mm b fi mv ms l mt mu">--Output--<br/>F1-score: 0.837 <br/>Confusion Matrix :  [[10683  2335]  [ 1817 10165]]</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es my"><img src="../Images/e3ce1fac0d5190ba28a6493a6746a58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Y0od_D_yS3GRkaYJYxwaIg.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">Model D</figcaption></figure><p id="7bc0" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">10.5随机森林分类器:我想了解在对如此庞大的数据集进行情感分析时，传统的机器学习模型落后了多远。结果它比我们的基本模型差多了。</p><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="cb6d" class="mq jz hh mm b fi mr ms l mt mu">from sklearn.ensemble import RandomForestClassifier</span><span id="b62c" class="mq jz hh mm b fi mv ms l mt mu">model_random_forest = RandomForestClassifier(n_estimators = 150, random_state=45, bootstrap = "False", criterion="gini", min_samples_split = 10, min_samples_leaf = 1)<br/>model_random_forest.fit(X_train, y)<br/>random_forest_score = predict_func(model_random_forest)</span><span id="a84d" class="mq jz hh mm b fi mv ms l mt mu">--Output--<br/>F1-score: 0.547 <br/>Confusion Matrix :  [[6995 6074]  [5505 6426]]</span></pre><blockquote class="ma mb mc"><p id="36bf" class="jc jd lj ig b ih ii je jf ij ik jg jh md ji jj jk me jl jm jn mf jo jp jq ir ha bi translated"><strong class="ig hi"> 11。模型比较—可视化</strong></p></blockquote><pre class="ll lm ln lo fd ml mm mn mo aw mp bi"><span id="776f" class="mq jz hh mm b fi mr ms l mt mu">results_fine_tuned = {"Model_A " : model_a_score,<br/>          "Model_B" : model_b_score,<br/>          "Model_C" : model_c_score,<br/>          "Model_D": model_d_score,<br/>          "Random Forest": random_forest_score}<br/>          <br/>plt.figure(figsize=(7, 7))<br/>plt.title('Comparison of models')<br/>plt.xlabel('Models')<br/>plt.ylabel('Model Score')<br/>plt.xticks(rotation=60)<br/>plt.yticks(rotation=60)<br/>plots = sns.barplot([i for i in results_fine_tuned], [results_fine_tuned[i] for i in results_fine_tuned])<br/>for p in plots.patches:<br/>    plots.annotate(format(p.get_height(), '.3f'), <br/>                   (p.get_x() + p.get_width() / 2., p.get_height()), <br/>                   ha = 'center', va = 'center', <br/>                   xytext = (0, 9), <br/>                   textcoords = 'offset points')<br/>plt.show()</span></pre><figure class="ll lm ln lo fd lp er es paragraph-image"><div class="er es mz"><img src="../Images/5de519a5038141e667b416c4607a4a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*rGl2zbETQ4oDpITlNMtIow.png"/></div></figure><p id="6c74" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated"><strong class="ig hi">最佳车型——C型86.3% </strong></p><h1 id="32b5" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">贡献</h1><ol class=""><li id="46ec" class="ie if hh ig b ih kw ij kx il lg in lh ip li ir na it iu iv bi translated">借助我去年从coursera的<a class="ae jb" href="https://www.coursera.org/professional-certificates/tensorflow-in-practice" rel="noopener ugc nofollow" target="_blank"> Tensorflow in Practice </a>认证中获得的知识，我能够使用LSTM实现并改进模型性能。</li><li id="6b96" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated">自己通过生成混淆矩阵来计算F1分数的方法</li><li id="5347" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated">实现了模型提前停止技术，防止模型过拟合。</li><li id="f2cd" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated">在每个时期后保存模型检查点，允许我们稍后通过加载权重来重新训练模型。这使得能够在云上托管模型，以便在与API集成时进行预测。</li></ol><h1 id="1d80" class="jy jz hh bd ka kb lb kd ke kf lc kh ki kj ld kl km kn le kp kq kr lf kt ku kv bi translated">结论和面临的挑战</h1><p id="8ca7" class="pw-post-body-paragraph jc jd hh ig b ih kw je jf ij kx jg jh il ky jj jk in kz jm jn ip la jp jq ir ha bi translated">在开始这个项目之前，我坐下来看看可以做些什么来建立一个良好的执行模型来预测给定文本的情感分析。回顾我以前的项目和任务，我明白传统的机器学习模型不能很好地处理如此大的数据集。因此我决定选择LSTM模特。</p><p id="b748" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">由于这一次我处理的是更大的数据集，所以训练时间更长，我会在训练时遇到错误。在模型检查点的帮助下，我能够从以前的检查点加载模型并恢复训练。</p><p id="48e6" class="pw-post-body-paragraph jc jd hh ig b ih ii je jf ij ik jg jh il ji jj jk in jl jm jn ip jo jp jq ir ha bi translated">调整批量大小是改进模型的关键因素。保持模型简单和增加批量有助于获得更高的F1分数。这样模型也不会过拟合。</p></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><div class="ll lm ln lo fd nb"><a href="https://github.com/PSSABISHEK/sentiment_analysis_LSTM/blob/master/yeet_boi.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hi fi z dy ng ea eb nh ed ef hg bi translated">PSSABISHEK/情绪分析_LSTM</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">通过在GitHub上创建一个帐户，为PSSABISHEK/sensation _ analysis _ LSTM的发展做出贡献。</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np lu nb"/></div></div></a></div></div><div class="ab cl jr js go jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="ha hb hc hd he"><h1 id="7cf1" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">参考</h1><ol class=""><li id="9b7f" class="ie if hh ig b ih kw ij kx il lg in lh ip li ir na it iu iv bi translated"><a class="ae jb" href="https://www.kaggle.com/bhrt97/steam-review-nlp-lstm-81-accuracy" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/bhrt 97/steam-review-NLP-lstm-81-accuracy</a></li><li id="2859" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://medium.datadriveninvestor.com/deep-learning-lstm-for-sentiment-analysis-in-tensorflow-with-keras-api-92e62cde7626" rel="noopener ugc nofollow" target="_blank">https://medium . datadriveninvestor . com/deep-learning-lstm-for-sensation-analysis-in-tensor flow-with-keras-API-92e 62 CDE 7626</a></li><li id="6515" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated">【https://www.tensorflow.org/tutorials/keras/save_and_load T4】</li><li id="52f7" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://www.geeksforgeeks.org/how-to-annotate-bars-in-barplot-with-matplotlib-in-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/how-to-annotate-bars-in-bar plot-with-matplotlib-in-python/</a></li><li id="80b1" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://www.kaggle.com/noi031/sentiment-analysis-with-self-attention" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/noi 031/带自我关注的情绪分析</a></li><li id="d4ad" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" rel="noopener ugc nofollow" target="_blank">https://cv-tricks . com/tensor flow-tutorial/save-restore-tensor flow-models-quick-complete-tutorial/</a></li><li id="3556" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras</a></li><li id="a10c" class="ie if hh ig b ih iw ij ix il iy in iz ip ja ir na it iu iv bi translated"><a class="ae jb" href="https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/</a></li></ol></div></div>    
</body>
</html>