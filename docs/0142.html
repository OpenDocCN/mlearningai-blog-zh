<html>
<head>
<title>Word Embeddings, WordPiece and Language-Agnostic BERT (LaBSE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入、单词块和语言无关的BERT (LaBSE)</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7?source=collection_archive---------0-----------------------#2021-02-20">https://medium.com/mlearning-ai/word-embeddings-wordpiece-and-language-agnostic-bert-labse-98c7626878c7?source=collection_archive---------0-----------------------#2021-02-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d3c67f3b236f9033acfa39dbbc849644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEmWDt4eztOcm5pr2QbxfA.png"/></div></div></figure><p id="453e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">单词嵌入是用数字格式表示单词，可以被计算机理解。最简单的例子是(是，否)表示为(1，0)。但是当我们处理大型文本和语料库时，这可能不是表示单词和句子的有效方式。对于大型语料库，词的共现及其概率起着重要作用。</p><p id="1199" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们探索一些单词表示的技巧…</p><h1 id="f3f4" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">一键编码</h1><p id="d039" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">在一键编码中，句子中的每个单词都由一个向量表示。</p><p id="1fd7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，考虑句子“我喜欢狗。”它有3个单词，每个单词都表示为</p><p id="fe6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我— (1，0，0)</p><p id="b691" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">爱— (0，1，0)</p><p id="8e87" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">狗— (0，0，1)</p><p id="1b1c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">向量中单词出现的位置被赋予1。</p><p id="27a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，当我们想要进行情感分析或问答时，这种方法不是一种认知方法。</p><p id="73be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，对于大型语料库来说，这些向量变得巨大而无意义。</p><h1 id="971f" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">CBOW和连续跳格模型</h1><p id="209e" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">重要的是用数字格式表示单词，而且这些单词要有意义，简单地说，它们应该有上下文意义来执行几个NLP任务。</p><p id="3901" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在论文“向量空间中单词表示的有效估计”中，Google提出了两种用于从非常大的数据集中计算单词的连续向量表示的架构，称为连续单词包(CBOW)和连续Skip-Gram模型。</p><p id="4f6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">CBOW模型的目标是预测上下文单词，给定4个未来单词和4个历史单词作为输入。这些未来和历史单词被转换成它们在句子中同现的概率，并作为输入提供给模型。这是使用一个带有一个隐藏层的前馈神经网络在来自谷歌新闻的6B单词语料库上训练的。隐藏层使用激活函数ReLU，输出层使用softmax，如下所示:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kq"><img src="../Images/522cb7817e4c316fc19014fd7f81d39c.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*NORZZ2qP0ADc0N5CtrHoZg.png"/></div></figure><p id="89da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中wc =上下文单词，wt =给定单词，s是评分函数。</p><p id="1dd0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于这些模型不依赖于语料库中词的顺序，因此被称为词袋。</p><p id="9150" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Skip Gram模型使用对数线性分类器预测给定输入单词前后一定范围内的单词。这也以与CBOW相似的方式训练。</p><p id="2425" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这两个模型能够产生更有效的词与词之间的语义和句法关系。这些模型的准确率为60%，通过词语相似度和词语类比来衡量。</p><p id="986c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Genism包中的Word2Vec库可以用来生成CBOW和skip grams。请参见下面的示例。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/ce131f770ce0fa02a0525477b98de424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3NRqCYaCaZ2iN1dA-2Mp-A.png"/></div></figure><h1 id="c157" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">手套</h1><p id="2be4" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">斯坦福大学提出了一个新的单词嵌入模型，称为GloVe，即单词表示的全局向量。GloVe在词语类比数据集上取得了75%的准确率，在词语相似性任务上也优于其他模型。</p><p id="79ad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Glove的工作方式类似于word2vec，使用同现概率的比率作为输入，而不是仅使用上下文单词的概率。下面给出了目标单词ice和steam与所选上下文单词(k)的共现概率。与原始概率相比，比率(用红色标记)能够更好地区分相关词(固体和气体)和不相关词(水和时尚)，也能够更好地区分两个相关词。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/ece7e8ed6750b0e3514ded72a5dabe0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*GSDUWihSOdiP-67vYS7eUQ.png"/></div></figure><p id="dffc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">GloVe在CBOW和Skip grams中使用对数双线性模型，而不是对数线性模型。Word2vec genism包可以通过使用手套预训练模型作为其输入用于手套，如下所示。它执行与word2vec相同的任务。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/df078fe7f6791dd299b5528a0454d68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*FN3hFv_jgUHgRa6g-WBKWA.png"/></div></figure><h1 id="a9c9" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">快速文本</h1><p id="aca2" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">来自脸书的FastText，基于skip gram模型的相同逻辑工作，重点是单词的词法。这意味着，该模型考虑单词的变形、派生和组成。每个单词都被表示为一包字符n-grams或称为子单词。向量表示与每个字符n元语法相关联，并且单词被表示为这些表示的总和。从而获得如下的评分函数</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kx"><img src="../Images/1fc94664effdcea228d3ee3866c94045.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*VqihAov1HlUMslgOuEkWHQ.png"/></div></div></figure><p id="0732" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">w是给定的单词，</p><p id="ca20" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Gw ⊂ {1，。。。，G}，—出现在w中的n元文法的集合，</p><p id="0cb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">zg —每个n元语法g的向量表示。</p><p id="aec3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该模型也称为子词模型，允许跨词共享表示，从而允许学习罕见词的可靠表示。该评分函数然后被用于前馈神经网络的输出层的激活函数。</p><p id="5d9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与使用softmax作为输出层中的激活函数来预测上下文单词的skip gram模型相反，FastText将其视为一组独立的二进制分类任务。那么目标是独立地预测上下文单词的存在(或不存在)。</p><p id="2756" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于选定的上下文位置c，使用二元逻辑损失，负对数似然被给出为</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/c1691648bc1890ae9432318af79c11b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*ABCUGyLrpSBwGda3ZSWJow.png"/></div></figure><p id="1069" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Genism库中提供了FastText模块，我们可以使用已经训练好的语料库，也可以在全新的语料库上训练模型。下面是一个来自已经训练好的数据的例子。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/a819abf8117315d12b1b2f6e9dfbcede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*8_TC3_roMYG4VyDT5Ljn2w.png"/></div></figure><h1 id="26af" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">BERT中的词块嵌入</h1><p id="8238" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">单词块嵌入是为亚洲语言如朝鲜语和日语的google语音识别系统开发的。这些语言有大量的字符、同音异义词，单词之间没有或只有很少的空格。没有或更少的空格意味着文本需要分段。然而，切分会在模型中产生大量的未收录词(OOV)。因此，单词块表示被创建来从大量数据中自动学习单词单元，并且不会产生任何OOV氏现象。这种处理OOV氏症的技巧被用在了伯特身上。OOV在word2vec和GloVe中被忽略，但是在FastText字符中，单词的n-gram表示补偿了OOV。</p><p id="7e63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">BERT使用30，000个单词的单词块嵌入。每个序列的第一个标记总是一个特殊的分类标记([CLS])。对应于该令牌的最终隐藏状态被用作分类任务的聚集序列表示。成对的句子被打包成一个序列。使用特殊的标记([SEP])并通过向每个标记添加学习嵌入来区分句子，该学习嵌入指示句子是属于句子A还是句子b。对于给定的标记，其输入表示通过对相应的标记、片段和位置嵌入求和来构建。这些输入用于预先训练用于掩蔽语言建模和下一句预测任务的BERT。BERT已经革新了许多NLP应用，但是BERT的构造使得它不适合于语义相似性搜索以及无监督的任务，例如通过语义搜索的聚类和信息检索。</p><p id="824d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">句子-BERT (SBERT)是使用连体和三元组网络结构的预训练BERT网络的修改，被提出来导出语义上有意义的句子嵌入，这些句子嵌入可以使用余弦相似度进行比较。</p><p id="cf93" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">访问<a class="ae kz" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">https://www.sbert.net/docs/pretrained_models.html</a>了解英语句子嵌入。</p><h1 id="e3fc" class="jn jo hh bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">LaBSE</h1><p id="64fe" class="pw-post-body-paragraph ip iq hh ir b is kl iu iv iw km iy iz ja kn jc jd je ko jg jh ji kp jk jl jm ha bi translated">语言不可知的BERT句子嵌入是对BERT的改编，用于为109种语言产生语言不可知的句子嵌入。SBERT可以产生英语句子嵌入，但是这不能用于多语言情况。LaBSE模型使用双向双编码器将掩蔽语言模型(MLM)和翻译语言模型(TLM)预训练与翻译排序任务相结合。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es la"><img src="../Images/9f931a12edc858c09253809e3c6dae69.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*U0F8DOVTbziyyjiw8VWEOw.png"/></div></figure><p id="f8c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，我们可以看到有一个双编码器，其中包含成对的编码器，提供点积评分功能。使用基于共享BERT的编码器对源句子和目标句子分别进行编码。最终的层[CLS]表示被作为每个输入的句子嵌入。源句子和目标句子之间的相似性使用余弦对由BERT编码器产生的句子嵌入进行评分。</p><p id="9673" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">双向双编码器使用带批内负采样的附加裕量softmax损耗进行训练，如下所示:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/0c3150e1f61e310b6739c433b088f5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*xpJu7xpUWDK8ZIi1-BMh_w.png"/></div></figure><p id="ac54" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">x和y的嵌入空间相似度由φ(x，y)给出其中，φ(x，y) =余弦(x，y)。</p><p id="8dc7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Github链接，包括以下LaBSE代码:</p><p id="a219" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae kz" href="https://github.com/bijular/datascience/blob/master/Word_Embedding.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/bijular/data science/blob/master/Word _ embedding . ipyn</a>b</p></div></div>    
</body>
</html>