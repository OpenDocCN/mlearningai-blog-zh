<html>
<head>
<title>Principle Component Analysis (PCA) is easy!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA)很简单！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/principle-component-analysis-pca-is-easy-f3ce0d64507f?source=collection_archive---------4-----------------------#2022-06-30">https://medium.com/mlearning-ai/principle-component-analysis-pca-is-easy-f3ce0d64507f?source=collection_archive---------4-----------------------#2022-06-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="d329" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">介绍</h1><p id="5b1d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这篇文章回顾了主成分分析(<strong class="je hi"> PCA </strong>)概念。PCA是一种<strong class="je hi">特征</strong>或<strong class="je hi">维度</strong>缩减技术。特征是用于训练的每个数据样本的值。考虑下面的例子，我们希望有一个机器学习(<strong class="je hi"> ML </strong>)模型，用于根据房屋的其他特征预测其价格。这些特征被称为<strong class="je hi">特征</strong>。特征也被称为尺寸。每个特征也可以被认为是一个轴，所以称它们为维度是看待正在发生的事情的另一种方式。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ka"><img src="../Images/61533ca1832934348fa6fa1aa2336615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Neu9HBcwjdUHbK6h.png"/></div></div></figure><p id="fd8e" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">PCA检测所有这些特征中最重要的特征。PCA机制依赖于方差概念。在底线，主成分分析显示了M个主成分(最基本的特征)是否用于模型中，以及原始数据的多少方差将被保留。但是在这个介绍之后，每个人都会问我们为什么使用PCA？</p><blockquote class="kr ks kt"><p id="9218" class="jc jd ku je b jf km jh ji jj kn jl jm kv ko jp jq kw kp jt ju kx kq jx jy jz ha bi translated">PCA是一种特征减少技术，因此我们将有一个较小的模型，因为我们检测到这些特征中的一些不影响我们的预测。知道这些之后，我们就可以收集和搜集更少的数据。从计算的角度来看，它可以提高效率和速度。简而言之，PCA有助于在保持预测质量的同时减少所需的计算。</p></blockquote><p id="46ad" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">至此，我们知道PCA为什么好了。这是因为我们要为我们的模型做更少的计算。这种特征减少机制检测并显示保持数据方差的基本特征。下一步是看看它是如何工作的，以及当PCA总是伴随着方差时意味着什么。因此，在讨论PCA如何工作之前，需要回顾方差、协方差和协方差矩阵的概念。此外，很好地理解特征值和特征向量是必不可少的。</p><h1 id="f81d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">背景</h1><p id="8feb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">本节回顾了一个人完全理解PCA所需的所有基本概念。</p><h2 id="5ecd" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">方差和标准差</h2><p id="815d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">方差是离差(分散)的度量，它度量一组数字从它们的平均值分散开多远。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lm"><img src="../Images/79e0f6e9f3d2596cc669f6dacc7778e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Lv8dMZeC0YtoYyDqdER6Ow.png"/></div></figure><p id="5690" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">以下示例显示了红色和蓝色数据集如何偏离其平均值。标准差是方差的平方根。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ln"><img src="../Images/8a3ff41f668df5657f4b662df2875401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SZLperMy77DReF6Q.png"/></div></div></figure><h2 id="7b2c" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">协方差</h2><p id="d1e1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">协方差是co+方差。当“<strong class="je hi"> co </strong>”前缀与一个单词一起出现时，就给这道菜增加了一个集合的意思。协方差是两个变量的<strong class="je hi">联合可变性</strong>的度量。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lo"><img src="../Images/873e9600346e4fa7a1f2aa18be8db5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*OjgPwmSs1O7mm2TY7j57ag.png"/></div></figure><p id="ad23" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">不同的协方差值说明了独立变量的数据是如何分散的。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lp"><img src="../Images/545ca958579e3a17e094c47ce2cf52c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n0nJ2GFxnT7wM9bCdKy-OQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx"><a class="ae lu" href="https://www.researchgate.net/figure/52-Examples-of-covariance-for-three-different-data-sets_fig6_308007227" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/figure/52-Examples-of-covariance-for-three-different-data-sets_fig6_308007227</a></figcaption></figure><h2 id="363a" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">相互关系</h2><p id="c708" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">相关性是指一对变量线性相关的程度。它是两个变量的协方差除以它们的标准差的乘积。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lv"><img src="../Images/1b1c71f82a1c0f1e35a339eab7629098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hGb9E2z3sOphFZa1.jpg"/></div></div></figure><p id="a89c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">下图显示了相关性可以通过其值揭示什么。说到相关性，我们应该考虑<strong class="je hi">线性度</strong>。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lw"><img src="../Images/940f2f8f9a6ed02a1655c7f52a26f7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7-ESw60zw3IarhI4.png"/></div></div></figure><h2 id="1ef4" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">协方差矩阵</h2><p id="6ae4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">协方差矩阵是一个方阵，给出了一组给定变量(或向量/变量列表-特征)的每对元素之间的协方差。任何协方差矩阵都是对称的，其主对角线包含方差。请注意，每个变量与其自身的协方差就是该变量的方差。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lx"><img src="../Images/3c004679aa67ff42bfcadab6e6bf1c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*XuItjQ92OvYj-qSe.png"/></div></figure><h2 id="e135" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">特征值和特征向量</h2><p id="7acb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">变换(矩阵)的特征向量和特征向量显示了在该变换下这些向量缩放的向量和缩放因子。</p><p id="0b48" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">将所有特征向量保存在一个矩阵中，以每列是一个特征向量的方式，我们建立一个变换矩阵，它的轴是特征向量。</p><h1 id="4794" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">PCA是如何工作的？</h1><p id="e8a0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">至此，我们已经回顾了掌握PCA工作原理所需的所有基本概念。</p><h2 id="5bc0" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">步骤1:计算平均向量</h2><p id="92a1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">第一步计算每个特征的所有样本的平均值。这是需要的，因为稍后我们会有一个协方差矩阵的特征向量系统。因此，在转换中，我们将需要数据集的平均值来移动新的坐标系。如果我们的数据集有N个特征，我们将有一个N元素平均向量。</p><h2 id="f262" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">步骤2:数据集的协方差矩阵</h2><p id="718e" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在此步骤中，计算数据集的协方差矩阵。由于在这个矩阵中考虑了离差，所以因为这部分，我们提到PCA把数据的方差作为关键。如果我们有N个特征，我们将有一个<strong class="je hi"> N*N </strong>协方差矩阵。</p><h2 id="f978" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">步骤3:计算数据集的特征向量和特征值</h2><p id="bb1a" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">接下来，计算协方差矩阵的特征向量和特征值。我们将有一个<strong class="je hi"> N*N </strong>矩阵保持特征向量。然后，基于特征向量的特征值的大小对特征向量进行排序。这很重要，因为特征值显示了每个特征向量上的方差，我们称之为主分量(<strong class="je hi"> PC </strong>)。例如，下面的公式显示了每个特征向量关于其特征值的方差。它被称为<strong class="je hi">变化部分</strong>。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ly"><img src="../Images/32351b4715dbfa3bfbd60daa5581cdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*2pa34HT8axJiiEl_g-Z35Q.png"/></div></figure><p id="fb44" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">下面的公式显示了保留前k个主成分可以保留多少方差。它被称为<strong class="je hi">累积方差</strong>。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lz"><img src="../Images/de798c4580d3348735ed82683dccd78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*LKRWIh4H4PJ9KktqeRkY-A.png"/></div></div></figure><p id="fb65" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">将保留特定数量的主成分的效果可视化后，该图如下所示:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ma"><img src="../Images/3160fbc52432dfa5f8707c21f7a03f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfHZMAzvxdodhBhSM6HFuQ.png"/></div></div></figure><h2 id="a3d1" class="ky if hh bd ig kz la lb ik lc ld le io jn lf lg is jr lh li iw jv lj lk ja ll bi translated">第四步:转换到潜在空间</h2><p id="bca4" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在决定了用于保持原始数据的期望方差的主成分数之后，下面的公式显示了原始数据是如何变换的。使用均值向量的原因是协方差矩阵的特征向量需要代表数据集。因此，数据集被移动到新坐标系的中间。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mb"><img src="../Images/0a6559a48a569b8c38bff8118175f9d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*TgO3rBaP9L7yPuDVmWlJ3w.png"/></div></div></figure><p id="6cfe" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">对于转移回原始空间:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mc"><img src="../Images/23e4f2ccceca9b9647ea409b55e85d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*tfO_E_E1tpsF513QFT9bgw.png"/></div></figure><p id="eacf" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">在下面的例子中，您可以发现如何使用python和NumPy库进行PCA。</p><pre class="kb kc kd ke fd md me mf mg aw mh bi"><span id="a43b" class="ky if hh me b fi mi mj l mk ml"><strong class="me hi">def</strong> mean_vec(dataset):<br/>    mean_vector = []<br/>    <br/>    <strong class="me hi">for</strong> i <strong class="me hi">in</strong> range(dataset.shape[0]):<br/>        tmp_mean_vector = np.mean(dataset[i, :])<br/>        mean_vector.append(tmp_mean_vector)<br/>        <br/>    mean_vector = np.array(mean_vector)</span><span id="cca7" class="ky if hh me b fi mm mj l mk ml">    <strong class="me hi">return</strong> mean_vector<br/><br/><strong class="me hi">def</strong> pca(dataset, k):<br/>    <em class="ku">''''</em><br/><em class="ku">    step 1: mean of features of all samples (it is assumed that features are on rows)</em><br/><em class="ku">    step 2: calculating covariance matrix</em><br/><em class="ku">    step 3: calculating eigenvalues and eigenvectors and sorting based on the eigenvalues</em><br/><em class="ku">    step 4: choosing k vector to form the transformation matrix</em><br/><em class="ku">    step 5: transforming the dataset with the transformation matrix</em><br/><em class="ku">    '''</em><br/>    <strong class="me hi"><em class="ku"># step 1</em></strong><br/>    mean_vector = mean_vec(dataset)<br/>    <br/>    <strong class="me hi"><em class="ku"># step 2</em></strong><br/>    covariance_matrix = np.cov([dataset[i,:] <strong class="me hi">for</strong> i <strong class="me hi">in</strong> range(dataset.shape[0])])<br/>    <br/>    <strong class="me hi"><em class="ku"># step 3</em></strong><br/>    eig_val_cm, eig_vec_cm = np.linalg.eig(covariance_matrix)<br/>    <br/>    <strong class="me hi"><em class="ku"># Make a list of (eigenvalue, eigenvector) tuples</em></strong><br/>    eig_pairs = [(np.abs(eig_val_cm[i]), eig_vec_cm[:,i]) <strong class="me hi">for</strong> i <strong class="me hi">in</strong> range(len(eig_val_cm))]<br/><br/>    <strong class="me hi"><em class="ku"># Sort the (eigenvalue, eigenvector) tuples from high to low</em></strong><br/>    eig_pairs.sort(key=<strong class="me hi">lambda</strong> x: x[0], reverse=<strong class="me hi">True</strong>)<br/>    <br/>    <strong class="me hi"><em class="ku"># step 4</em></strong><br/>    matrix_w = np.hstack([eig_pairs[i][1].reshape(dataset.shape[0], 1) <strong class="me hi">for</strong> i <strong class="me hi">in</strong> range(k)])<br/>    <br/>    <strong class="me hi">return</strong> matrix_w</span><span id="25a1" class="ky if hh me b fi mm mj l mk ml"># consider you have a spreedsheet of data as <strong class="me hi">dataset</strong></span><span id="1885" class="ky if hh me b fi mm mj l mk ml">matrix_w = pca(dataset, 10)</span><span id="e71c" class="ky if hh me b fi mm mj l mk ml">transformed_dataset = matrix_w.T.dot((dataset - <strong class="me hi">mean_vector</strong>).T)</span></pre><p id="59bb" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">为了理解它，读它几遍，并尝试测试一些东西。最后，作为总结，请看下图，该图显示了使用PCA如何通过使用红色和蓝色作为新系统的基向量来将维度降低到2。绿色的方差可以忽略不计，所以它是被搁置的候选者。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mn"><img src="../Images/2dc59c4faf09b7d958b92f8ac1ddcc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*cY3bIOPK00OkdhAJ.png"/></div></figure><h1 id="dc1e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><div class="mo mp ez fb mq mr"><a href="https://en.wikipedia.org/wiki/Variance#:~:text=Variance%20is%20a%20measure%20of,fit%2C%20and%20Monte%20Carlo%20sampling" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">方差-维基百科</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">在概率论和统计学中，方差是随机变量的方差的平方的期望值。</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">en.wikipedia.org</p></div></div><div class="na l"><div class="nb l nc nd ne na nf kk mr"/></div></div></a></div><div class="mo mp ez fb mq mr"><a href="https://en.wikipedia.org/wiki/Correlation" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">相关性-维基百科</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">在统计学中，相关性或依赖性是两个随机变量之间的任何统计关系，无论是否是因果关系</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">en.wikipedia.org</p></div></div><div class="na l"><div class="ng l nc nd ne na nf kk mr"/></div></div></a></div><div class="mo mp ez fb mq mr"><a href="https://en.wikipedia.org/wiki/Covariance_matrix" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">协方差矩阵-维基百科</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">在概率论和统计学中，协方差矩阵(也称为自协方差矩阵、离差矩阵…</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">en.wikipedia.org</p></div></div><div class="na l"><div class="nh l nc nd ne na nf kk mr"/></div></div></a></div><div class="mo mp ez fb mq mr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hi fi z dy mw ea eb mx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">medium.com</p></div></div><div class="na l"><div class="ni l nc nd ne na nf kk mr"/></div></div></a></div></div></div>    
</body>
</html>