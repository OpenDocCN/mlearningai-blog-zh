<html>
<head>
<title>Classification.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类。</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/classification-ebbec6cf2cf7?source=collection_archive---------7-----------------------#2021-06-04">https://medium.com/mlearning-ai/classification-ebbec6cf2cf7?source=collection_archive---------7-----------------------#2021-06-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f8f2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">众所周知，线性回归适用于定性情景，当回答明确时，我们就进行分类。顾名思义，该过程将观察结果分类成组，以便预测定性观察结果。通常，该过程首先预测每个类别进行分类的概率，从而表现得像回归算法。</p><p id="8d35" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，这种划分并不总是正确的。最小二乘法；线性回归可用于定量反应，另一方面，我们发现逻辑回归用于2类定性反应。因为有一个步骤涉及确定分类概率以便进行分类，所以它被称为回归。类似地，K-NN和Boosting可用于定性和定量数据。我们倾向于根据反应是定性的还是定量的来进行分析和模型选择，从而相应地选择线性回归或逻辑回归，然而预测因子的类型不太重要。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/14618a96266e48c19e22807b80694be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*J78-2BCXPouzN6t_BL5GkA.png"/></div></figure><p id="633e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以通过许多方法进行分类和预测。根据数据、最终目标和所涉及的场景，我们可以选择一个。无论我们选择哪种模型，重点都是准确预测。虽然我们仍在设计我们的模型，但确定预测准确性的唯一方法是找出训练数据集上的错误，并在测试数据集上重新验证。量化评估准确性的最常见方法是<strong class="ig hi">训练错误率</strong>，即当我们将评估应用于训练观察时所犯错误的比例。而一个模型的成功是当我们得到<strong class="ig hi">最小测试误差的时候。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jk"><img src="../Images/9105e9335050dc441a4108ea0e817ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*vqnPLcfdQNWvl8XO3G4Evw.png"/></div></figure><p id="2a6a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一个这样的分类器是<strong class="ig hi">朴素贝叶斯分类器</strong>。朴素贝叶斯分类器是一类简单的概率分类器，基于贝叶斯定理，特征之间具有很强的独立性。它之所以被称为天真，是因为它假设某个特征的出现独立于另一个特征的出现。当数据集大小适中或较大，并且具有多个属性时，可以使用它。主要标准是实例应该是有条件独立的。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jl"><img src="../Images/08318c633f8d0ed65ceaa182ebe66848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*yC-SAdFPD_QcQ6TDuUuWHA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">A typical R code to implement Naïve Bayes Classification</figcaption></figure><p id="9331" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，执行朴素贝叶斯的步骤仍然是一样的</p><p id="2a31" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将数据分成列车-测试</p><p id="b20b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果需要，进行特征缩放</p><p id="5ea4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使朴素贝叶斯适合训练集</p><p id="715d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">预测测试集</p><p id="fa1d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">执行模型评估。</p><p id="0dbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管<strong class="ig hi">易于实现并且给出了好的结果</strong>，但是由于变量之间的条件独立性和实际依赖性，它存在<strong class="ig hi">有时不太准确的问题。</strong></p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jq"><img src="../Images/8c344cceb94ee3e7e74eb97e41f1e9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*dMiPb1x7fEIF_E7luGnTCw.png"/></div></figure><p id="6804" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">综上所述，我们可以在以下业务场景中使用朴素贝叶斯分类</p><p id="bfc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">情绪分析</p><p id="cd94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">文件分类</p><p id="558b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">垃圾邮件过滤</p><p id="ac46" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将新闻文章分类成小组。</p><p id="0b88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在可能不总是这样，我们知道给定X的Y的条件概率，因此不能应用贝叶斯分类。在这种情况下，我们的方法可以是在给定X的情况下，尝试估计Y的条件分布，然后将给定的观察值分类到具有最高估计概率的类别。<strong class="ig hi"> KNN或K-最近邻</strong>正是这样做的。对于给定的整数值K，KNN分类器识别训练数据中最接近x0的K个点。然后我们估计该类的条件概率。因此，我们应用贝叶斯规则，将测试观察分类到具有最高概率的类中。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jr"><img src="../Images/669f37dba9d23bf3cc33bb4909097bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*x5HT-DwBp75xwMYRfaRWuQ.png"/></div></figure><p id="78ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们要实施KNN模式，我们会</p><ol class=""><li id="31c1" class="js jt hh ig b ih ii il im ip ju it jv ix jw jb jx jy jz ka bi translated">选择K个邻居的数量</li><li id="e9de" class="js jt hh ig b ih kb il kc ip kd it ke ix kf jb jx jy jz ka bi translated">根据距离，最好是欧几里德距离，取未知数据点的K个最近邻</li></ol><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kg"><img src="../Images/63987ee6edbdf96026219fb6a010c80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*fZ4Fu446QuXQmWhNjH-GTQ.png"/></div></figure><p id="a8fd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.在所有这些K个邻居中，计算每个类别中的数据点的数量。</p><p id="91f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.将新数据点分配到我们可以计算最多邻居的类别中。</p><p id="7d2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">显然，关键是选择K的<strong class="ig hi">正确值。</strong>在选择K的最佳值时，我们需要记住——如果K太小，邻域将对噪声点敏感；如果K太大，邻域将包含来自其他类的点。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jq"><img src="../Images/3cac62ce2adfaf08a962bb4ac778c284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*blcv0weU_lg2ybCQDnNvSg.png"/></div></figure><p id="b2b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> R </strong>是一种强大的语言，其中K的最佳值可以通过几行代码来确定。对于这个特定的例子，我们看到预测的准确性随着K值的增加而增加，但是在K = 73之后，准确性再次下降。因此，对于这个特定的例子，K= 73给出了最好的结果。然而，需要有一点尝试和错误来确定最佳的k。</p><p id="154e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">需要记住的另一点是<strong class="ig hi">缩放。</strong>可能必须对属性进行缩放，以防止距离测量受任何属性支配。为了避免这种偏差，我们应该将特征变量标准化。KNN的整个原理基于点与点之间的距离，因此缩放在该模型中至关重要。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kh"><img src="../Images/0014a24fd3503729814902d2b501cf59.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Whnr0CPl05mcu9HL-ss9uA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Logistic Regression</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ki"><img src="../Images/b2d46659f9d329f37e9259bb15bc23f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*-Y2nZcrf1-lZScniegqlsg.png"/></div></figure><p id="ca7a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">逻辑回归</strong>是一种非常常见的分类方法，其中我们不是对响应Y进行建模，而是尝试对Y属于特定类别的概率进行建模。不管x的值是多少，逻辑函数总是会产生S形的S形曲线。对于任何方程，我们都需要估计系数。回归系数是未知的，可以根据可用的训练数据进行估计，最小二乘法是一种常用的方法。然而<strong class="ig hi"> <em class="kj">最大似然</em> </strong>是首选方法。</p><p id="5a3b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，如果类被很好地分开，那么逻辑回归的参数e估计结果会有点不稳定。此外，当有两个以上的响应类别时，逻辑回归并不表现最佳；如当n很小时的情况，并且预测值的分布在每个类别中近似为正态分布。作为另一种选择<strong class="ig hi"> LDA或线性判别分析</strong>得到了重视。在LDA中，我们分别对每个响应类别中预测值的分布进行建模，然后使用贝叶斯定理将这些预测值转换为估计值。LDA分类器产生于这样的假设，即每一类中的观察值来自正态分布，并且具有特定于类的平均值和公共方差。</p><p id="0fbb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像LDA一样，<strong class="ig hi">【QDA】或二次判别分析</strong>也假设高斯分布，但与LDA不同，它假设每个类别都有自己的协方差矩阵。如果有K个类和p个预测器，那么是有效的；LDA假设所有K个类共享基于p(p+1)/2个参数的公共协方差矩阵，而QDA为总共K*p(p+1)/2个参数的每个类估计单独的协方差矩阵。所以LDA通过假设一个公共协方差矩阵变成线性的。因此，LDA是比QDA灵活得多的分类器，并且具有较低的方差。这可能会更好地提高预测性能。如果所有K个类共享一个公共协方差矩阵的假设严重错误，那么LDA将遭受高偏差，这必然有不利的一面。这可能有助于我们假设，如果有较少的培训保守，LDA可能是一个比QDA更好的选择。也就是说，更大的训练规模更适合QDA。</p><p id="0201" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值得一提的是，像逻辑回归、SVM这样的方法是适合线性模型的测试；而对于非线性模型，我们更喜欢朴素贝叶斯、KNN、决策树、人工神经网络和随机森林等。</p><p id="c1ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有点上<strong class="ig hi">车型评测</strong>；准确性并不总是模型评估的最佳衡量标准。理想情况下，我们会检查一些标准进行评估</p><ul class=""><li id="4cac" class="js jt hh ig b ih ii il im ip ju it jv ix jw jb kk jy jz ka bi translated">AIC —碱性信息标准</li><li id="0a15" class="js jt hh ig b ih kb il kc ip kd it ke ix kf jb kk jy jz ka bi translated">零偏差和剩余偏差</li><li id="5a23" class="js jt hh ig b ih kb il kc ip kd it ke ix kf jb kk jy jz ka bi translated">混淆矩阵——准确性、敏感性、特异性</li><li id="b693" class="js jt hh ig b ih kb il kc ip kd it ke ix kf jb kk jy jz ka bi translated">ROC——接收器工作特性</li><li id="95c4" class="js jt hh ig b ih kb il kc ip kd it ke ix kf jb kk jy jz ka bi translated">AUC —曲线下面积</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jr"><img src="../Images/47b912ff36ef8bbad0173d0484a59e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*vA9voTzxIoqPwRl7xUqdPw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">TP = True Positive, TN = True Negative, FP = False Positive, FN = False Negative</figcaption></figure><p id="1500" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">准确性本身不应该成为模型被接受的评判标准。敏感性和特异性也很重要。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/48b7748ba3b4835a93039d5cf2d1297a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBwHXWnazHgbxEp1s7N-mg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">simple R code to determine AUC</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kq"><img src="../Images/66193e3176eaca921ce91e34c15a268c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RpWxoafJYvNYbjKoR_k0sQ.png"/></div></div></figure><p id="0fc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ROC曲线是一种流行的图形，用于同时显示所有可能阈值的两种类型的误差。ROC可能不总是给出正确的解释，因此建议查看AUC。AUC给出了在所有可能的阈值上总结的分类器的总体性能。AUC越大，分类器越好。ROC曲线对于比较不同的分类器也是有用的，因为它们考虑了所有可能的阈值。</p><p id="b291" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们<strong class="ig hi">比较模型</strong></p><p id="20fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">逻辑回归和LDA的不同之处仅在于它们的拟合过程，因此看起来这两种模型可能会给出相同的输出。LDA假设高斯分布，每个类别中有一个公共协方差矩阵。如果不满足高斯假设，逻辑回归优于LDA。另一方面，KNN是一种非参数方法，对决策边界的形状没有任何假设。因此，KNN优于LDA和逻辑回归的一个明显情况是当决策边界复杂和高度非线性时。另一方面，KNN没有告诉我们重要的预测因素。通过假设二次决策边界和有限数量的训练观察，QDA充当非参数KNN和线性LDA方法以及逻辑回归之间的中间地带。还必须记住，在每个实际场景中，没有一种方法能胜过另一种方法。当真正的决策边界是线性的时，那么LDA和逻辑回归方法往往表现良好。当边界是中度非线性时，QDA将是一个更好的选择。对于更复杂的决策边界，非参数方法，如KNN将是最好的。</p></div></div>    
</body>
</html>