<html>
<head>
<title>Forget Complex Traditional Approaches to handle NLP Datasets, HuggingFace Dataset Library is your saviour!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">忘记复杂的传统方法来处理NLP数据集，HuggingFace数据集库是您的救星！</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f?source=collection_archive---------0-----------------------#2022-02-02">https://medium.com/mlearning-ai/forget-complex-traditional-approaches-to-handle-nlp-datasets-huggingface-dataset-library-is-your-1f975ce5689f?source=collection_archive---------0-----------------------#2022-02-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="c584" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">作者</h1><p id="8d52" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">纳巴伦·巴鲁阿</p><p id="62e2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/nabarunbaruaAIML" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/nabarun-barua-aiml-engineer/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@nabarun.barua">towards data science</a></p><p id="1d06" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">阿尔琼·库姆巴卡拉</p><p id="885c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><a class="ae kf" href="https://github.com/arjunKumbakkara" rel="noopener ugc nofollow" target="_blank">Git</a>/<a class="ae kf" href="https://www.linkedin.com/in/arjunkumbakkara/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>/<a class="ae kf" rel="noopener" href="/@arjunkumbakkara">towards data science</a></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/e5ddefe082a6b28d383e39b71cd411e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ruXh6n5COF88eGDC.jpg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx">Credit: HuggingFace.co</figcaption></figure><blockquote class="kw kx ky"><p id="4dbb" class="jc jd kz je b jf ka jh ji jj kb jl jm la kc jp jq lb kd jt ju lc ke jx jy jz ha bi translated">简介:这是为了展示和阐明使用Hugginfaces数据集库处理NLP数据集比使用传统的复杂方法(如归约、lambdas映射以及最重要的从csv到Dicts到Dfs到Json等等)有多容易！</p></blockquote><p id="04d2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi ld translated"><span class="l le lf lg bm lh li lj lk ll di">在</span>数据科学和人工智能的世界中，数据扮演着重要的角色，要么用于训练，要么用于获得洞察力。任何使用Tensorflow或Pytorch的人都知道Tensorflow/Pytorch数据集库或了解Pandas的见解。</p><p id="816d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在重要的问题是，我们到底为什么需要HuggingFace数据集库？</p><p id="acad" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个问题的答案分为四个部分。</p><ol class=""><li id="9919" class="lm ln hh je b jf ka jj kb jn lo jr lp jv lq jz lr ls lt lu bi translated">HuggingFace数据集库运行在<a class="ae kf" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>内存格式和<a class="ae kf" href="https://arrow.apache.org/docs/python/index.html" rel="noopener ugc nofollow" target="_blank"> pyarrow库</a>上，因此数据加载&amp;处理速度非常快。数据集库将每个数据集视为内存映射文件，这有助于RAM &amp;文件存储之间的映射，从而允许库访问和处理数据集，而无需将数据集完全加载到内存中。有关内存映射文件的更多信息，请阅读此<a class="ae kf" href="https://en.wikipedia.org/wiki/Memory-mapped_file" rel="noopener ugc nofollow" target="_blank">链接</a>。这也意味着处理数据集所需的内存量更少。</li><li id="45df" class="lm ln hh je b jf lv jj lw jn lx jr ly jv lz jz lr ls lt lu bi translated">HuggingFace数据集库也支持将不同类型的数据格式加载到内存中。示例CSV、TSV、文本文件、JSON和Pickled数据帧。</li><li id="7dc8" class="lm ln hh je b jf lv jj lw jn lx jr ly jv lz jz lr ls lt lu bi translated">如果需要在数据帧中工作，那么数据集中的简单属性改变使其作为数据帧工作，并且数据帧的所有功能在这里工作。当DataFrame的工作完成后，我们可以简单地将属性重置回数据集。在引擎盖下，内存映射正在工作，这使得整体内存消耗更加有效。</li><li id="77c2" class="lm ln hh je b jf lv jj lw jn lx jr ly jv lz jz lr ls lt lu bi translated">它还支持流数据，或者如果数据集非常大，那么在这种情况下，我们可以以流数据集的形式加载数据，这与普通数据集(即可迭代数据集)没有什么不同。</li></ol><p id="c07a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">既然我们知道了为什么要使用HuggingFace数据集库。我们会详细研究图书馆。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ma"><img src="../Images/799f4005ea4dfebc7b53571e68e64045.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/0*How3GkX1UnsaIA6o.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx">Credit: HuggingFace.co</figcaption></figure><h1 id="9c0f" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">正在加载数据集</h1><p id="7b26" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">第一步从加载数据集开始。我们可以通过多种方式加载数据集，如下所示:</p><h2 id="f6b3" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">离线数据集文件:</h2><p id="de73" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们可以简单地使用名为load_dataset的函数来加载数据集。如上所述，我们可以加载不同的格式。</p><p id="6234" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">CSV示例</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="0161" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="ce3c" class="mb if hh mq b fi my mv l mw mx">dataset1 =  load_dataset('csv', data_files= 'location/file1.csv')</span></pre><p id="316b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">JSON示例</p><p id="0b1e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">简单JSON</p><p id="54a9" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">{"a": 1，" b": 2.0，" c ":foo "，" d": false}</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="e8ca" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="8364" class="mb if hh mq b fi my mv l mw mx">dataset2 =  load_dataset('json', data_files= 'location/file2.json')</span></pre><p id="44e2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果是嵌套的JSON</p><p id="03a1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">{"version": "0.1.0 "，" data": [{"a": 1，" b": 2.0，" c": "foo "，" d": false}，{"a": 4，" b": -5.5，" c": null，" d": true}] }</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="2def" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="dfe2" class="mb if hh mq b fi my mv l mw mx">dataset3 =  load_dataset('json', data_files= 'location/file3.json', field= 'data')</span></pre><p id="ea4a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">示例文本文件</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="3210" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="89b4" class="mb if hh mq b fi my mv l mw mx">dataset4 =  load_dataset('text', data_files= 'location/file4.txt')</span></pre><p id="4d24" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">示例拼花地板</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="dca4" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="197d" class="mb if hh mq b fi my mv l mw mx">dataset5 =  load_dataset('parquet', data_files= 'location/file5.parquet')</span></pre><p id="da0b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">注意:拼花只是另一种文件存储格式。一种开源文件格式，用于处理平面列存储数据格式。</p><p id="128c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为JSON或任何其他格式加载Zip/TAR或公共压缩文件的示例</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="0e03" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="a9b3" class="mb if hh mq b fi my mv l mw mx">dataset6 =  load_dataset('json', data_files= 'location/file6.json.gz', field= 'data')</span></pre><p id="5dc3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，如果我们在上面的例子中没有给出分割，那么数据将默认加载到列车分割中。为了在加载数据集时进行训练测试分割，我们可以进行以下更改:</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="fd41" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="13cc" class="mb if hh mq b fi my mv l mw mx">DataFile = {'train':['location/file7.csv','location/file8.csv','location/file9.csv'],'test':'location/file10.csv'}</span><span id="755b" class="mb if hh mq b fi my mv l mw mx">dataset7 =  load_dataset('csv', data_files= DataFile )</span></pre><p id="1e35" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似地，对于其他文件格式，我们可以使用上述技术。现在，如果我们偶然没有培训和测试的数据集文件，我们仍然可以通过以下方式分割数据集</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="0efc" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset</span><span id="fdb7" class="mb if hh mq b fi my mv l mw mx">dataset8 =  load_dataset('csv', data_files= 'location/file8.csv')<br/>dataset8 = dataset8['train']<br/>dataset8 = dataset8.train_test_split(test_size=0.1)</span></pre><h2 id="7f44" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">远程数据集或从URL加载数据集</h2><p id="44cb" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在HuggingFace数据集库中，我们还可以加载存储在服务器中的远程数据集作为本地数据集。作为现实世界中的数据科学家，大多数时候我们会从远程服务器加载数据。我们只需给出URL而不是本地路径。</p><p id="1914" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例子</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="1b3a" class="mb if hh mq b fi mu mv l mw mx">url = "https://https://github.com/crux82/squad-it/raw/master/"</span><span id="d3c8" class="mb if hh mq b fi my mv l mw mx">data_files = {<br/>    "train": url + "SQuAD_it-train.json.gz",<br/>    "test": url + "SQuAD_it-test.json.gz",<br/>}</span><span id="24ed" class="mb if hh mq b fi my mv l mw mx">dataset = load_dataset("json", data_files=data_files, field="data")</span></pre><p id="68ae" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">对于其他数据类型，可以采用类似方法。</p><h2 id="23b2" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">加载流数据</h2><p id="3fe2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在，一天的数据以GB和TB为单位，因为不可能在系统中加载完整的数据集。对于这种情况，我们以流格式加载数据集，这种格式稍有不同。通常load_dataset返回dataset的数据类型，但在流格式中，它返回IterableDataset的数据类型。稍后我们将详细了解更多关于数据流的内容。</p><p id="70bc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们可以通过简单地传递属性streaming=True来加载任何流格式的数据集</p><p id="cc37" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例子</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="e744" class="mb if hh mq b fi mu mv l mw mx">url = "https://https://github.com/crux82/squad-it/raw/master/"</span><span id="dbca" class="mb if hh mq b fi my mv l mw mx">data_files = url + "SQuAD_it-train.json.gz"</span><span id="c069" class="mb if hh mq b fi my mv l mw mx">dataset = load_dataset("json", data_files=data_files, field="data",split="train",streaming=True)</span></pre><h2 id="ac48" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">内存中的数据</h2><p id="5ad1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">HuggingFace数据集库也允许从字典和DataFrame创建数据集。</p><p id="160f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">字典中的数据集示例</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="79de" class="mb if hh mq b fi mu mv l mw mx">from datasets import Dataset</span><span id="9079" class="mb if hh mq b fi my mv l mw mx">my_dict = {'title':['Macbeth','Tempest','Julius Caesar'],'character':['King Duncan of Scotland','Prospero','Brutus']}</span><span id="88d8" class="mb if hh mq b fi my mv l mw mx">dataset = Dataset.from_dict(my_dict)</span></pre><p id="c3ac" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">数据框中的数据集示例</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="78ee" class="mb if hh mq b fi mu mv l mw mx">from datasets import Dataset<br/>import pandas as pd</span><span id="cd08" class="mb if hh mq b fi my mv l mw mx">my_dict = {'title':['Macbeth','Tempest','Julius Caesar'],'character':['King Duncan of Scotland','Prospero','Brutus']}</span><span id="0aeb" class="mb if hh mq b fi my mv l mw mx">df = pd.DataFrame(my_dict)</span><span id="ad02" class="mb if hh mq b fi my mv l mw mx">dataset = Dataset.from_pandas(df)</span></pre><h1 id="1d92" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">预处理数据集</h1><p id="b1b8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在我们知道了如何加载数据集，是时候学习如何处理原始数据集了。一般来说，我们可能得不到一个干净的数据集，我们必须通过清理来处理数据集，并使其达到可以发送到模型的级别。</p><h2 id="7a74" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">选择数据集中的记录</h2><p id="a079" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在下面的例子中，我们从数据集中取前1000个条目，返回的数据类型是Dataset</p><p id="f9aa" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们可以通过下面的例子来选择记录</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="e62a" class="mb if hh mq b fi mu mv l mw mx">sample = dataset['train'].select(range(1000))</span></pre><p id="f75b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">同样，如果我们想看到实际的数据，那么我们可以通过简单的改变来返回数据类型字典。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="d606" class="mb if hh mq b fi mu mv l mw mx">sample = dataset['train'][0:1000]</span></pre><h2 id="382c" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">洗牌数据集</h2><p id="6c3f" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">现在，假设我们想要选择数据集，但是想要在选择数据集之前打乱数据点，或者想要打乱整个数据集。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="112f" class="mb if hh mq b fi mu mv l mw mx">sample = dataset['train'].shuffle(seed=34).select(range(1000))</span></pre><h2 id="7d42" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">从数据集列中获取唯一条目</h2><p id="6022" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当数据科学家想知道数据集的列中有哪些唯一的条目时，可能会产生需求。</p><p id="7306" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在下面的例子中，我们将得到一个唯一条目的列表</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="8609" class="mb if hh mq b fi mu mv l mw mx">list = dataset['train'].unique("Title")</span></pre><h2 id="d5bc" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">排序数据集</h2><p id="95f0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">此处列必须有数值，即列必须与Numpy兼容，因为列值将根据它们的数值排序。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="03c6" class="mb if hh mq b fi mu mv l mw mx"># It will sort the column in descending Order<br/>sorted_dataset = dataset.sort('label',reverse=True)</span></pre><h2 id="e03d" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">过滤数据集</h2><p id="956c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">当数据科学家想要从数据集中获取特定类型的数据时，可能会出现这种情况。因此HuggingFace数据集库有一个名为Filter的函数，它提取符合其过滤条件的数据点。</p><p id="3b39" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在下面的示例中，如果数据集中列“Character_Name”的数据点的值为none，则该数据点将被跳过。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="6941" class="mb if hh mq b fi mu mv l mw mx">dataset = dataset.filter(lambda x: x['Character_Name'] is not None)</span></pre><h2 id="30c7" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">重命名列</h2><p id="b1c8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">HuggingFace数据集库允许您重命名数据集的列。我们可以通过下面的例子来理解，这里传递的是实际的列名即‘Title’和要重命名的列名即‘Novel’。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="906a" class="mb if hh mq b fi mu mv l mw mx">dataset = dataset.rename_column("Title", "Novel")</span></pre><h2 id="714c" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">移除列</h2><p id="9605" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">与Rename类似，在这种情况下，数据科学家需要从数据集中删除列。我们可以通过下面的例子来理解，这里我们传递了我们想要从数据集中删除的列的列表。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="e165" class="mb if hh mq b fi mu mv l mw mx">dataset = dataset.remove_columns(['ID', 'Texts'])</span></pre><h2 id="8f90" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">为列转换数据类型</h2><p id="b2f8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在这种情况下，数据科学家可能必须更改列的特征类型，在这种情况下，我们可以转换列。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="3f18" class="mb if hh mq b fi mu mv l mw mx">dataset.features</span></pre><p id="451c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="c74c" class="mb if hh mq b fi mu mv l mw mx">{'sentence1': Value(dtype='string', id=None),<br/>'sentence2': Value(dtype='string', id=None),<br/>'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),<br/>'idx': Value(dtype='int32', id=None)}</span></pre><p id="9090" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">正如所见，我们将列标签的数据类型转换为文本二进制，如“正”和“负”</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="442c" class="mb if hh mq b fi mu mv l mw mx">from datasets import ClassLabel, Value<br/>new_features = dataset.features.copy()<br/>new_features["label"] = ClassLabel(names=['negative', 'positive'])<br/>new_features["idx"] = Value('int64')<br/>dataset = dataset.cast(new_features)<br/>dataset.features</span></pre><p id="69a3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="1fe7" class="mb if hh mq b fi mu mv l mw mx">{'sentence1': Value(dtype='string', id=None),<br/>'sentence2': Value(dtype='string', id=None),<br/>'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),<br/>'idx': Value(dtype='int64', id=None)}</span></pre><h2 id="0ac1" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">变平</h2><p id="9a60" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">有时一个列可以是几种类型的嵌套结构，需要将子字段提取到它们自己单独的列中，这可以用Flatten函数来完成。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="60bd" class="mb if hh mq b fi mu mv l mw mx">from datasets import load_dataset<br/>dataset = load_dataset('squad', split='train')<br/>dataset.features</span></pre><p id="9a5c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="e197" class="mb if hh mq b fi mu mv l mw mx">{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),<br/>'context': Value(dtype='string', id=None),<br/>'id': Value(dtype='string', id=None),<br/>'question': Value(dtype='string', id=None),<br/>'title': Value(dtype='string', id=None)}</span></pre><p id="7c15" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">回答字段包含两个子字段:文本和answer_start。用数据集把它们展平。Dataset.flatten():</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="f51e" class="mb if hh mq b fi mu mv l mw mx">flat_dataset = dataset.flatten()<br/>flat_dataset</span></pre><p id="3fde" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">输出</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="19af" class="mb if hh mq b fi mu mv l mw mx">Dataset({<br/>    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],<br/> num_rows: 87599<br/>})</span></pre><h2 id="153e" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">Map()方法</h2><p id="3827" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这是一种特殊的方法，允许在数据集中更新一列或多列，或者创建新的一列或多列。</p><p id="769b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">更新列的示例。下面我们降低标题，更新专栏。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="2432" class="mb if hh mq b fi mu mv l mw mx">dataset = dataset.map(lambda x: x['Title'].lower())</span></pre><p id="ddd0" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">创建新列的示例。下面我们在计算栏标题中的字数，并保存在新栏中。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="a15a" class="mb if hh mq b fi mu mv l mw mx">def compute_length(example):<br/>    return {"length": len(example["Title"].split())}<br/>dataset = dataset.map(compute_length)</span></pre><p id="1f9b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在上面的例子中，我们一次处理一个值，没有充分利用库的潜力。在这里，我们可以通过简单地添加属性batched=True(批处理大小是可配置的，但默认为1，000)来批量做同样的事情。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="badb" class="mb if hh mq b fi mu mv l mw mx">def compute_length(example):<br/>    return {"length": [len(i.split()) for i in example["Title"]]}<br/>dataset = dataset.map(compute_length,batched=True)</span></pre><p id="33c1" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当与Fast Tokenizer一起使用时，该函数的使用效果最佳。</p><p id="96a5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">下面的例子取自<a class="ae kf" href="http://Huggingface.co" rel="noopener ugc nofollow" target="_blank"><em class="kz"/></a>，它显示了在数据集上带批处理&amp;的快速&amp;慢速记号化器与不带批处理的相对比较。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="18ee" class="mb if hh mq b fi mu mv l mw mx">Options	            Fast tokenizer	Slow tokenizer<br/>batched=True	    10.8s	        4min41s<br/>batched=False	    59.2s	        5min3s</span></pre><p id="bd32" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">通过上面的例子我们可以看到，如果batched为真，那么它在执行函数时会快30倍。</p><p id="126c" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">该功能还支持并行性，即属性num_proc。但是请记住，使用num_proc来加速您的处理通常是一个好主意，只要您正在使用的函数本身还没有进行某种多重处理。在下面的例子中，我们使用了快速令牌化器的多处理，但这是不可取的，因为快速令牌化器已经在内部并行工作。为了更好的理解，请浏览一下<a class="ae kf" href="http://Huggingface.co" rel="noopener ugc nofollow" target="_blank"><em class="kz"/></a>。下面的例子取自HuggingFace，它显示了在数据集上带有num_proc，batch &amp;而没有batch，num_proc的快速&amp;慢速标记器的相对比较。</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="5092" class="mb if hh mq b fi mu mv l mw mx">Options	                      Fast tokenizer	 Slow tokenizer<br/>batched=True	              10.8s	         4min41s<br/>batched=False	              59.2s	         5min3s<br/>batched=True, num_proc=8      6.52s	         41.3s<br/>batched=False, num_proc=8     9.49s	         45.2s</span></pre><p id="107d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">以上是日常工作中最常用的主要功能。</p><h1 id="e066" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">数据集到数据框架</h1><p id="0a06" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">可能有一段时间，数据科学家希望看到数据帧中的数据，并对其进行一些EDA。如果数据科学家正在使用HuggingFace数据集库，那么他/她只需将此函数set_format设置为Pandas即可。例子</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="56a0" class="mb if hh mq b fi mu mv l mw mx">dataset.set_format('pandas')</span></pre><p id="655f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个函数只改变数据集的输出格式，所以您可以很容易地切换到另一种格式，而不会影响底层的数据格式，这就是Apache Arrow。格式化是就地完成的。</p><p id="e0b7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">现在，当我们访问数据集的元素时，我们得到一只熊猫。用数据框代替字典:</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="967c" class="mb if hh mq b fi mu mv l mw mx">DF = dataset['train'][:3]</span></pre><p id="a26e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在这里，熊猫的所有功能都将按预期工作。当数据集上的工作完成时，我们可以将其重置为箭头格式。例子</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="1b07" class="mb if hh mq b fi mu mv l mw mx">dataset.reset_format()</span></pre><h1 id="39b8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">流式数据集</h1><p id="b2e0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">上面我们已经谈到了流式数据集。由于流数据集是可迭代的数据集，因此这里的一些处理函数与普通数据集不同，这将在后面解释。</p><h2 id="4a91" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">Map()方法</h2><p id="2163" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">第一个变化是映射函数的工作方式，如果它在没有batched = True的情况下使用，那么在流数据集中输出将逐个返回，而普通数据集将返回完整的数据集。因此，为了优化流式数据集的使用，请使用Batched=True，这将从流式数据集获取批量数据。默认情况下，批量大小为1000，但可以配置。</p><h2 id="1cde" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">Shuffle()方法</h2><p id="089d" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">与普通数据集中混洗整个数据集不同，这里的混洗只混洗预定义buffer_size的批处理元素。</p><h2 id="2f8c" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">Take() &amp; Skip()方法</h2><p id="0d43" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在流数据集中，select()方法不起作用，取而代之的是两个方法take() &amp; skip()方法，它类似于select()方法。</p><p id="1f64" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">假设缓冲区大小为1000，我们从数据集访问一个值，然后缓冲区被填充或替换为1001个位置值。</p><p id="6797" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">例如，如果我们想获取前10条记录:</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="39ed" class="mb if hh mq b fi mu mv l mw mx">dataset = streamed_dataset.take(10)</span></pre><p id="9be3" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">类似地，如果我们想要从流数据集中创建训练和验证分割</p><pre class="kh ki kj kk fd mp mq mr ms aw mt bi"><span id="c8e0" class="mb if hh mq b fi mu mv l mw mx"># Skip the first 1,000 examples and include the rest in the training set<br/>train_dataset = streamed_dataset.skip(1000)<br/># Take the first 1,000 examples for the validation set<br/>validation_dataset = streamed_dataset.take(1000)</span></pre><p id="eee4" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这个图书馆有很多东西要讲。因此，我们决定根据使用水平(如中级和高级)将此作为一个系列。请警惕同回购的高级版本。您可以点击“观察”并通过通知了解最新情况。</p><h1 id="9251" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">高级级别上的高级概念</h1><ol class=""><li id="ba8d" class="lm ln hh je b jf jg jj jk jn mz jr na jv nb jz lr ls lt lu bi translated">关于并行文件系统的更多信息</li><li id="2eb3" class="lm ln hh je b jf lv jj lw jn lx jr ly jv lz jz lr ls lt lu bi translated">基于云的存储、检索和所有相关操作。</li><li id="2c75" class="lm ln hh je b jf lv jj lw jn lx jr ly jv lz jz lr ls lt lu bi translated">数据整理:将大块数据整理成可处理的单元</li></ol><p id="9893" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果你喜欢这个博客，请表达你的爱，给我们一个大拇指，给我们加星，如果不喜欢，请在评论区给我们反馈。希望你能在图书馆玩得开心！</p><p id="9a5f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了合作，帮助和一起学习-</p><h2 id="6333" class="mb if hh bd ig mc md me ik mf mg mh io jn mi mj is jr mk ml iw jv mm mn ja mo bi translated">加入我们的不和谐服务器:<a class="ae kf" href="https://discord.gg/Z7Kx96CYGJ" rel="noopener ugc nofollow" target="_blank">https://discord.gg/Z7Kx96CYGJ</a></h2><p id="1af1" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一路平安！</p><div class="nc nd ez fb ne nf"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ng ab dw"><div class="nh ab ni cl cj nj"><h2 class="bd hi fi z dy nk ea eb nl ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nm l"><h3 class="bd b fi z dy nk ea eb nl ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="nn l"><p class="bd b fp z dy nk ea eb nl ed ef dx translated">medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt kq nf"/></div></div></a></div></div></div>    
</body>
</html>