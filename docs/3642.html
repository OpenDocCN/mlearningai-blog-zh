<html>
<head>
<title>Building a Neural Network Zoo From Scratch: The Perceptron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建神经网络动物园:感知器</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/building-a-neural-network-zoo-from-scratch-the-perceptron-335759f48089?source=collection_archive---------3-----------------------#2022-10-02">https://medium.com/mlearning-ai/building-a-neural-network-zoo-from-scratch-the-perceptron-335759f48089?source=collection_archive---------3-----------------------#2022-10-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/2f78a3f06b0dbfcf0de48a17e3f7e8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*lx1IpwFZ6XY7w1BWMJJqrg.png"/></div><figcaption class="il im et er es in io bd b be z dx">Visualization of the Perceptron from the <a class="ae ip" href="https://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">Asimov Institute</a>.</figcaption></figure><p id="59e2" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">被广泛认为是第一个神经网络的感知机最初是由康奈尔大学的心理学家弗兰克·罗森布拉特在1958年展示的。最初被认为是早期人工智能的救星，人们很快发现，感知机只能识别最基本的模式，像<a class="ae ip" href="https://en.wikipedia.org/wiki/AND_gate" rel="noopener ugc nofollow" target="_blank">和盖茨</a>。直到很多年以后，多层感知器才被发明出来，永远改变了机器学习和AI的历程。</p><h1 id="5388" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">那么，什么是感知机呢？</h1><p id="2a0c" class="pw-post-body-paragraph iq ir hh is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn ha bi translated">简单来说，感知器是一种功能，我们可以随着时间的推移进行微调，以给出我们想要的结果。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es kr"><img src="../Images/846b1d1e989323a06da14446c5b8b9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/1*gIQT0JXeQiclFBd2vPWGmA.gif"/></div></figure><p id="7919" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果你回想一下你高中的数学课，这个函数应该看起来很熟悉——它是一条线的方程。这就是为什么感知器被认为是线性二进制分类器:它是图上可以区分两组的直线。</p><div class="ks kt ku kv fd ab cb"><figure class="kw ii kx ky kz la lb paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/b21347724f3d90a46cd09f5e2277652d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*RI-9PP_0fl3evryEoAKg4Q.png"/></div></figure><figure class="kw ii lg ky kz la lb paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/bb7aa8a14b310085ffc52f27d028ab2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TmjkubogNKMeHYL3KSpAsw.png"/></div><figcaption class="il im et er es in io bd b be z dx lh di li lj">AND function (left) seperated by a potential Perceptron line &amp; XOR function (right) which cannot be seperated by Perceptron line.</figcaption></figure></div><p id="acc5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这就是感知机的问题所在:它只能区分可以被一条<em class="lk">直线</em>分开的事物。AND函数(显示在左边)可以很容易地分开；另一方面，XOR函数不能。不可能画一条一边是绿色的X，另一边是红色的直线。</p><h1 id="c0a6" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">感知器是如何工作的？</h1><p id="0308" class="pw-post-body-paragraph iq ir hh is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn ha bi translated">感知器有两个步骤:向前传递和向后传递。在训练阶段，感知器“函数”被调用任意次数(前向传递)，每次调用时，我们将得到的输出与我们想要的输出进行比较，稍微改变函数，以获得更接近我们正在寻找的答案(后向传递)。</p><p id="a0d6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这种反向传递被称为<a class="ae ip" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>，通常是每个新的人工智能爱好者的祸根。反向传播是一堆高维矩阵和多元微积分，但理解它最简单的方法是用<a class="ae ip" href="https://www.youtube.com/watch?v=d14TUNcbn1k" rel="noopener ugc nofollow" target="_blank">计算图</a>。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ll"><img src="../Images/8a19c413bff4c79742b03b35bd1fc388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_KofL5H8Vw3_8oJz2uXLg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Step 1 of the computational graph for the Perceptron.</figcaption></figure><p id="68fa" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">计算图形非常容易制作:从左到右阅读，每个圆代表一个函数(在这种情况下是乘法和加法)，每个进入圆的变量都是前面提到的函数的输入，任何来自圆的线都是函数的输出。这个特殊的图形代表了我们的感知器，因为它首先将<em class="lk"> x </em>和<em class="lk"> W </em>相乘，然后将<em class="lk"> b </em>加到结果上。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/d967f0a5dbc902a961263bafd0b5c448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9M4bkkTsdFyLv9YWTTodw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Step 2 of the computational graph of the Perceptron.</figcaption></figure><p id="43ea" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">接下来，我们将把当前函数放在绿线上方的每个连接处，以便于理解。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/41808ce28c3659d5a27a0f2a09b2c984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*2BQZwgM9VO5l6Qd9mv6RJA.png"/></div><figcaption class="il im et er es in io bd b be z dx">Step 3 for the computational graph of the Perceptron.</figcaption></figure><p id="63ba" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">最后一步是最难的。从最右边的<em class="lk">连接(箭头来自加法功能)开始，<em class="lk">错误</em>将写在下面。任何一步的误差都可以计算如下:</em></p><ol class=""><li id="3016" class="lo lp hh is b it iu ix iy jb lq jf lr jj ls jn lt lu lv lw bi translated">最后一个连接将是代表错误的<em class="lk"> e </em>。</li><li id="6b93" class="lo lp hh is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">任何其他连接的误差将是上游误差(直接在右侧的误差)乘以上游函数(直接在右侧的函数)相对于您正在计算的连接的导数。</li></ol><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/35476dd56360a984ca22ac66b3bc683d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/1*VrMbBp_U2w5127n2IschGg.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of <em class="md">b</em>.</figcaption></figure><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es me"><img src="../Images/65d72caa879aaca239b36ca651b37f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/1*MsXuA_3jv50yIkuEoKQJlA.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of Wx.</figcaption></figure><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/d1d707cbbe1040c6a75c1dc576c33b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/1*tG-Kcw_OzrqFxx2Ru0xHCg.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of W.</figcaption></figure><figure class="ks kt ku kv fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/b6931b8d2627c053cca332e016619e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/1*VgDLZYYVqwdsSw2iV-ytPg.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Error of x.</figcaption></figure><p id="131a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">如果这些你都不理解，我建议你看一下这个视频，它会更详细地介绍构建和理解计算图的过程。你需要熟悉这个过程来理解神经网络。</p><h1 id="880b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数学够了…给我看看代码！</h1><p id="1639" class="pw-post-body-paragraph iq ir hh is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn ha bi translated">首先，导入NumPy。这将是我们在本教程中使用的唯一一个库，这样你可以深入了解感知器幕后发生的事情。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Import NumPy.</figcaption></figure><p id="9e39" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们将使用一个类来表示我们的感知器，以保持一切有组织，但你可以随意实现它。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Perceptron class.</figcaption></figure><p id="82d5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们的类会取三个输入:<code class="du mj mk ml mm b">input_size</code>，就是我们的输入会有多大；<code class="du mj mk ml mm b">num_epochs</code>，这是我们希望更新函数的次数(一般来说，越多的历元意味着越准确！);和<code class="du mj mk ml mm b">learning_rate</code>，这是一个额外的常数，它将改变我们的感知机如何快速学习。</p><p id="a0a5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">我们还需要初始化我们的网络。<code class="du mj mk ml mm b">self.weights</code>是我们的<em class="lk"> W，</em>它将被设置为一个与我们的输入大小相同的零矩阵。<code class="du mj mk ml mm b">self.bias</code>是我们的<em class="lk"> b </em>它也会被设置为零来启动。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Forward propagation function.</figcaption></figure><p id="a116" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">接下来是我们的<code class="du mj mk ml mm b">forward()</code>函数，它会调用我们的感知器函数<em class="lk"> f(x) </em>。它接受一个名为<code class="du mj mk ml mm b">input</code>的输入，将它与我们的<code class="du mj mk ml mm b">self.weights</code>变量(<em class="lk"> W </em>)相乘，并将<code class="du mj mk ml mm b">self.bias</code> ( <em class="lk"> b </em>)加到它上面。对于不熟悉NumPy的人来说，下一行可能看起来有点复杂，但我向您保证它很简单。因为我们试图对一个二元函数进行分类，我们希望我们的输出要么是1，要么是0，所以如果<code class="du mj mk ml mm b">layer_output</code>大于0，那么<code class="du mj mk ml mm b">np.where(layer_output &gt; 0, 1, 0)</code>将返回1，否则将返回0。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Backpropagation function.</figcaption></figure><p id="2a8c" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">接下来是大家最喜欢的反向传播函数。使用我们的计算图，我们知道<code class="du mj mk ml mm b">self.weights</code> ( <em class="lk"> W </em>)的误差将会是<em class="lk"> ex </em>，或者我们的误差乘以我们的输入，<code class="du mj mk ml mm b">self.bias</code>的误差简单来说就是<em class="lk"> e </em>。如前所述，你还应该使用一个学习率来控制你的网络学习的速度。这将通过将我们的误差乘以学习率来计算。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Train &amp; test functions.</figcaption></figure><p id="be95" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">最后是<code class="du mj mk ml mm b">train()</code>和<code class="du mj mk ml mm b">test()</code>功能。<code class="du mj mk ml mm b">train()</code>功能将接受两个输入:<code class="du mj mk ml mm b">inputs</code>和<code class="du mj mk ml mm b">labels</code>。这些是我们感知器功能的输入列表和它们相应的期望输出。使用<code class="du mj mk ml mm b">zip()</code>函数，我们遍历每一对，用我们的输入调用我们的<code class="du mj mk ml mm b">forward()</code>函数，用我们的误差调用<code class="du mj mk ml mm b">backward()</code>，误差是我们想要的输出和预测的输出之间的差。如果还不清楚为什么这就是我们计算误差的方式，这里有一个表格可以更清楚地显示出来。</p><figure class="ks kt ku kv fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mn"><img src="../Images/3059eb18b385ba37ea45e892ca2a5483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rxFbxEMiLGKBXoyzx-q0Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Table of our output, desired output, error and the output + the error.</figcaption></figure><p id="783a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">现在你看到了:当<code class="du mj mk ml mm b">error</code>和<code class="du mj mk ml mm b">prediction</code>相加时，我们得到了我们想要的输出。这就是为什么我们<em class="lk">在反向传播函数中把误差</em>加到权重和偏差上:以得到更接近我们想要的输出。</p><figure class="ks kt ku kv fd ii"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="il im et er es in io bd b be z dx">Perceptron initialization &amp; utilization.</figcaption></figure><p id="2515" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">最后初始化我们的感知机，训练它，并测试它。在这个例子中，我使用了1000个历元(或训练步数)和0.01的学习率，但是我强烈建议您亲自试验这些超参数，感受一下它们为什么在那里。</p></div><div class="ab cl mo mp go mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ha hb hc hd he"><p id="5da1" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">本系列的第一篇文章到此结束。最终，我希望写一篇文章来解释阿西莫夫研究所的神经网络动物园中的每一个网络。这篇文章的完整代码可以在<a class="ae ip" href="https://github.com/CallMeTwitch/Neural-Network-Zoo/blob/main/Perceptron.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。下一个关于前馈神经网络的教程可以在这里找到<a class="ae ip" rel="noopener" href="/@CallMeTwitch/building-a-neural-network-zoo-from-scratch-feed-forward-neural-networks-f754cc88eca2">。</a></p><p id="a6ef" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">感谢<a class="mv mw ge" href="https://medium.com/u/5732f22c71f9?source=post_page-----335759f48089--------------------------------" rel="noopener" target="_blank">艾米莉·赫尔</a>的剪辑！</p><div class="mx my ez fb mz na"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="nb ab dw"><div class="nc ab nd cl cj ne"><h2 class="bd hi fi z dy nf ea eb ng ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="nh l"><h3 class="bd b fi z dy nf ea eb ng ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="ni l"><p class="bd b fp z dy nf ea eb ng ed ef dx translated">medium.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ij na"/></div></div></a></div></div></div>    
</body>
</html>