<html>
<head>
<title>Classification Algorithms 4: Ensemble Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类算法4:集成模型</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/classification-algorithms-4-ensemble-models-ceecd5020f19?source=collection_archive---------2-----------------------#2022-07-10">https://medium.com/mlearning-ai/classification-algorithms-4-ensemble-models-ceecd5020f19?source=collection_archive---------2-----------------------#2022-07-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="0701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">萨提亚·克里希南·苏雷什，顺穆加普里亚</p><p id="072a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我以前的文章中，展示的模型是独立的模型。但通常情况下，模型的组合会比单独的模型表现得更好。这是因为一种叫做<em class="jd">的群体智慧理论。</em>它说，从多个预测器获得的预测的集合比单个预测器的预测更准确。组合多个预测值并聚合其预测值的过程称为集成，在此过程中建立的模型称为集成模型。大多数Kaggle比赛都是由合奏模特赢得的。在这篇文章中，我们将讨论投票，打包粘贴和随机森林。这是我们将要使用的数据集。这篇文章的代码可以在<a class="ae jc" href="https://github.com/SathyaKrishnan1211/Low-key-ML/blob/master/Notebooks/Ensemble_Models.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/332117331f096da55eef60812a9ca5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*D9OqP0OAz74Kxl7ufKzfsw.png"/></div></figure><p id="b25c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">投票:<br/> </strong>当多个预测器对相同实例进行预测时，投票是选择最常出现的预测或类别的简单过程。所做的预测也可以称为由单个预测器所做的预测的模式。投票有两种类型——硬投票和软投票。当单个模型做出的预测是离散的时，这被称为硬投票，而当做出的预测是概率时，这被称为软投票。软投票通常表现更好，因为它给予概率较高的类更多的重要性，而在硬投票中，重要性仅给予决策阈值。下图显示了如何在scikit-learn中实现硬投票和软投票。如你所见，两种投票分类器的性能都优于逻辑回归和k-neighbors模型。如前所述，软投票比硬投票表现更好。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/5244c21de4c865b47327eb3b8e3627d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UORrszS_1-jFG8JRqQk5RA.png"/></div></div></figure><p id="0570" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">打包粘贴:<br/> </strong>在投票法中，所有的模型都是用相同的数据训练的，因此如果模型相似，它们在某些情况下必然会犯相同的错误。如果每个模型是在不同于其他模型被训练的实例的数据的随机子集上被训练的，那么由所有模型产生的共同错误是无效的。这导致了更多样化的模式。单个模型将具有高偏差但较小的方差，并且当它们被组合以形成集合模型时，得到的模型将具有相似的偏差但较小的方差。单个模型会有很大的偏差，因为它们不是在整个数据上训练的，而是在数据的子集上训练的。</p><p id="b3aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当随机抽样通过替换(引导)完成时，也就是说，一个子集的数据实例可能会重复，这就叫做装袋。如果随机抽样是在没有替换的情况下进行的，则称为粘贴。装袋模型比粘贴模型具有更高的偏差，因为它们不止一次地看到一个实例。打包和粘贴使用单个预测器作为基础模型，并在相同类型的多个模型上训练随机子集。下图显示了如何在scikit-learn中实现打包和粘贴。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jr"><img src="../Images/e88d21c17ceaa9336cc21dbbea056dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8JC9SDHKmb22rw392dyYeA.png"/></div></div></figure><p id="09b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">随机森林:<br/> </strong>随机森林建立在一个叫做随机子空间的概念上。随机子空间是采样特征而不是采样实例的过程。为了理解为什么对特征进行采样，你必须首先理解<a class="ae jc" rel="noopener" href="/mlearning-ai/classification-algorithms-3-decision-tree-801d59011780">决策树</a>是如何工作的。</p><p id="6fb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">决策树是使用贪婪算法构建的。贪婪算法的缺点之一是它找到的解是局部最优的，但不是全局最优的。这意味着，它找到了一个接近最优解的解，而不是最优解。当建立决策树时，它首先找到信息增益最高或基尼指数最低的特征和相应的特征阈值。然后对子树做同样的事情。由于决策是在本地做出的，因此不会产生最佳结果。</p><p id="c4ec" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随机森林通过创建决策树的集合来克服这一缺点，其中每个决策树都是根据随机采样的特征而不是所有特征来训练的。由于这些树是在特征的子集上训练的，所以训练过程很快，并且训练过程也可以并行化。随机森林的实现如下图所示。正如你所看到的，它优于装袋分类器。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es js"><img src="../Images/79d55dd8228828fc34b64dd1c309b9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rG3tvqTVQ59TUy_ldsM5jA.png"/></div></div></figure><p id="820c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当涉及到装袋分类器和随机森林分类器时，不必创建单独的验证集。这是因为当使用替换完成采样时，一些实例可能没有用于训练单个模型。这些实例称为开箱(oob)实例。您可以将这些实例用作验证集，并查看模型在这些实例上的表现。请注意，每个单独的模型的oob实例都是不同的。下图显示了如何在scikit-learn中获取oob_score。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jt"><img src="../Images/1b0b6557c1562b53215ba6b6655a79ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8htkkAMpCAYca8gKl4RGA.png"/></div></div></figure><p id="8aff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">结论:<br/> </strong>在本文中，已经讨论了投票、装袋粘贴和随机森林。在下一篇文章中，我们将讨论AdaBoost和GradientBoost。</p><div class="ju jv ez fb jw jx"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="jy ab dw"><div class="jz ab ka cl cj kb"><h2 class="bd hi fi z dy kc ea eb kd ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ke l"><h3 class="bd b fi z dy kc ea eb kd ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="kf l"><p class="bd b fp z dy kc ea eb kd ed ef dx translated">medium.com</p></div></div><div class="kg l"><div class="kh l ki kj kk kg kl jk jx"/></div></div></a></div></div></div>    
</body>
</html>