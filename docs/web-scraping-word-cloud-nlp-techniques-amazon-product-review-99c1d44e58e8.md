# ç½‘ç»œæŠ“å–å’Œæ–‡å­—äº‘(NLP æŠ€æœ¯)|äºšé©¬é€Šäº§å“è¯„è®º

> åŽŸæ–‡ï¼š<https://medium.com/mlearning-ai/web-scraping-word-cloud-nlp-techniques-amazon-product-review-99c1d44e58e8?source=collection_archive---------1----------------------->

ä½œä¸ºä¸€åç½‘ä¸Šè´­ç‰©è€…ï¼Œæˆ‘ç›¸ä¿¡ä½ é€šå¸¸ä¼šåœ¨è´­ä¹°ä¹‹å‰é˜…è¯»å…¶ä»–ä¹°å®¶çš„è¯„è®ºã€‚ä»Žæœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯å¦‚ä½•å¸®åŠ©æ‚¨ä»Žç½‘ç«™ä¸­æå–å’Œè§£æžæ•°æ®ä»¥ä¾›ç ”ç©¶å’Œå­¦ä¹ ï¼Œå¹¶æœ€ç»ˆä½¿ç”¨ matplotlib åˆ›å»ºå•è¯äº‘(åˆåæ ‡ç­¾äº‘)ã€‚æˆ‘é€‰æ‹©äº†[ç´¢å°¼ SRS-XB13 è¶…ä½ŽéŸ³æ— çº¿ä¾¿æºå¼ç´§å‡‘åž‹æ‰¬å£°å™¨ IP67 é˜²æ°´è“ç‰™](https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1)æ¥è¿›è¡Œæƒ…ç»ªåˆ†æž(ç¡®å®šç§¯æžå‚ä¸Žå’Œæ¶ˆæžå‚ä¸Žçš„æ¯”ä¾‹)ã€‚

![](img/eab799599153353069d8473f0a74a56c.png)

Source: [Pexels@Ekrulila](https://www.pexels.com/photo/person-holding-white-and-brown-newspaper-3957616/)

# æ­¥éª¤ 1:æå–å¹¶åœ¨æœ¬åœ°ä¿å­˜ç½‘é¡µðŸ”

ä½¿ç”¨[è¯·æ±‚](https://docs.python-requests.org/en/latest/)å’Œ[ç¾Žæ±¤](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)åº“ä»Žé€‰å®šçš„ URL ä¸­æ•èŽ· HTML é¡µé¢çš„å†…å®¹ã€‚

```
import requests
from bs4 import BeautifulSoup as bs # for web scraping# Creating empty review list
sony_speaker_reviews = []
for i in range(1,30):
    speaker = []
    url = "[https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1](https://www.amazon.com/Sony-SRS-XB13-Waterproof-Bluetooth-SRSXB13/dp/B08ZJ6DQNY/ref=cm_cr_arp_d_product_top?ie=UTF8&th=1)"+str(i)
    header={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'}
    response = requests.get(url,headers = header)
    # Creating soup object to iterate over the extracted content
    soup = bs(response.text,"lxml")
    # Extract the content under the specific tag
    reviews = soup.find_all("div",{"data-hook":"review-collapsed"})
    for i in range(len(reviews)):
        speaker.append(reviews[i].text)
    # Adding the reviews of one page to empty list which in future contains all the reviews
    sony_speaker_reviews += speaker# Writing reviews in a text file
with open('sony_speaker_reviews.txt','w', encoding = 'utf8') as output:
    output.write(str(sony_speaker_reviews))
```

# æ­¥éª¤ 2:æ•°æ®æ¸…ç†å’Œç‰¹å¾ ExtractionðŸ§¹

ä¸ºäº†æ¸…ç†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰è¯„è®ºåˆå¹¶åˆ°ä¸€ä¸ªæ®µè½ä¸­ï¼Œå°†æ‰€æœ‰å•è¯æ”¹ä¸ºå°å†™ï¼Œå¹¶åˆ é™¤ä¸éœ€è¦çš„ç¬¦å·ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**è¯å¹²**æ¥ç²¾ç®€å•è¯ï¼ŒæŠ“ä½å•è¯çš„åŸºæœ¬æ„æ€ã€‚å•è¯â€œsupportingâ€å’Œâ€œsupporterâ€å…±ç”¨è¯æ ¹â€œsupportâ€æˆ–è€…ï¼Œä½ å¯ä»¥ä½¿ç”¨**è¯æ±‡åŒ–**ï¼Œå› ä¸ºå®ƒä¼šç»™ä½ ä¸€ä¸ªå®Œæ•´çš„æœ‰æ„ä¹‰çš„è‹±è¯­å•è¯ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•è¯çš„ç‰‡æ®µã€‚ä¾‹å¦‚â€œä¹°æ–¹â€è€Œä¸æ˜¯â€œä¸å¤œâ€ã€‚ðŸŒµ

åœ¨å¹³æ—¶çš„å£è¯­å’Œå†™ä½œä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨å† è¯ã€ä»£è¯ã€ä»‹è¯å’Œè¿žè¯æ¥é€ å¥ã€‚è¿™äº›è¯è¢«ç§°ä¸ºåœç”¨è¯ðŸ›‘ï¼Œå®ƒä»¬æ˜¯é€šç”¨çš„ï¼Œä¸é‚£ä¹ˆé‡è¦ï¼›å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿›è¡Œåˆ†æžä¹‹å‰å°†å®ƒä»¬è¿‡æ»¤æŽ‰ã€‚å¦‚æžœæ‚¨æ­£åœ¨åˆ†æžçš„å†…å®¹æ˜¯è‹±è¯­ï¼Œè¯·ç¡®ä¿é€‰æ‹©`english`ä½œä¸ºè¯­è¨€ï¼Œå› ä¸ºè¯¥è¯­æ–™åº“ä¹ŸåŒ…å«å…¶ä»–è¯­è¨€çš„åœç”¨è¯ã€‚

```
import re
import nltk
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('omw-1.4')
nltk.download("stopwords")
from nltk.corpus import stopwords# Joining all the reviews into single paragraph 
sn_rev_string = " ".join(sony_speaker_reviews)# Change to lower case and removing unwanted symbols incase if exists
sn_rev_string = re.sub("[^A-Za-z" "]+"," ",sn_rev_string).lower()
sn_rev_string = re.sub("[0-9" "]+"," ",sn_rev_string)# words that contained in sony speaker reviews
sn_reviews_words = sn_rev_string.split(" ")# Lemmatizing
wordnet = WordNetLemmatizer()
sn_reviews_words=[wordnet.lemmatize(word) for word in sn_reviews_words]# Filtering Stop Words
stop_words = set(stopwords.words("english"))
stop_words.update(['amazon','product','speaker','sony'])
sn_reviews_words = [w for w in sn_reviews_words if not w.casefold() in stop_words]
```

å½“æˆ‘ä»¬åˆ†æžæ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æŒ‰å•è¯æˆ–å¥å­å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°/åˆ†ç¦»ã€‚é€šè¿‡å•è¯æ ‡è®°å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç¡®å®šç»å¸¸å‡ºçŽ°çš„å•è¯ã€‚åŒæ—¶ï¼Œé€šè¿‡å¥å­æ¥æ ‡è®°å¯ä»¥å¸®åŠ©ä½ çœ‹åˆ°è¿™äº›å•è¯æ˜¯å¦‚ä½•ç›¸äº’è”ç³»çš„ï¼Œå¹¶çœ‹åˆ°æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å¯ä»¥ç”¨**æ ‡è®°åŒ–**åˆ›å»º **N å…ƒè¯­æ³•**ã€‚ä¾‹å¦‚ï¼Œä»Žå¥å­â€œæˆ‘å–œæ¬¢å­¦ä¹ æ•°æ®ç§‘å­¦ã€‚â€ï¼Œå•è¯å°†æ˜¯:â€œæˆ‘â€ï¼Œâ€œçˆ±â€ï¼Œâ€œå­¦ä¹ â€ï¼Œâ€œå…³äºŽâ€ï¼Œâ€œæ•°æ®â€ï¼Œâ€œç§‘å­¦â€ã€‚ä¸€ä¸ªäºŒå…ƒæ¨¡åž‹å¯èƒ½æ˜¯:â€œæˆ‘çˆ±â€ï¼Œâ€œçˆ±å­¦ä¹ â€ï¼Œæˆ–è€…â€œæ•°æ®ç§‘å­¦â€ã€‚ç„¶è€Œï¼Œä¸‰å…ƒæ¨¡åž‹)å°±åƒâ€œæˆ‘å–œæ¬¢å­¦ä¹ â€ï¼Œæˆ–è€…â€œå…³äºŽæ•°æ®ç§‘å­¦â€ã€‚

![](img/4d7903d6d8e7478c56fbc2b01ab6133a.png)

Source: [Pexels@Towfiqu barbhuiya](https://www.pexels.com/photo/five-yellow-stars-on-blue-and-pink-background-9821386/)

å¦ä¸€æ–¹é¢ï¼Œ**è¯é¢‘é€†æ–‡æ¡£é¢‘çŽ‡(TFIDF)** å¼ºè°ƒå¾ˆå°‘å‡ºçŽ°ä½†éžå¸¸é‡è¦çš„ n å…ƒè¯­æ³•ã€‚ðŸ¹

> TFIDF = TF x IDF
> 
> è¯é¢‘(TF):ä¸€ä¸ªè¯åœ¨æ–‡æ¡£ä¸­å‡ºçŽ°çš„æ¬¡æ•°ã€‚
> 
> é€†æ–‡æ¡£é¢‘çŽ‡(IDF):ä¸€ä¸ªè¯å¯¹æ•´ä¸ªè¯­æ–™åº“çš„é‡è¦æ€§ï¼›log(æ–‡æ¡£æ€»æ•°/åŒ…å«è¯¥æœ¯è¯­çš„æ–‡æ¡£æ•°)

å½“ n-gram åœ¨æ–‡æ¡£ä¸­å…·æœ‰é«˜é¢‘çŽ‡è€Œåœ¨è¯­æ–™åº“ä¸­å…·æœ‰ä½Žæ–‡æ¡£é¢‘çŽ‡æ—¶ï¼ŒTFIDF å¾—åˆ†é«˜ã€‚

ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ª 3000 å­—çš„æ–‡æ¡£ä¸­ï¼Œâ€œç¼–ç â€ä¸€è¯å‡ºçŽ°äº† 45 æ¬¡ã€‚å‡è®¾åœ¨ 10ï¼Œ000 ä¸ªæ–‡æ¡£çš„è¯­æ–™åº“ä¸­æœ‰ 25 ä¸ªæ–‡æ¡£åŒ…å«æœ¯è¯­â€œç¼–ç â€ã€‚

TF(ç¼–ç )= 45/3000 = 0.015

IDF(ç¼–ç )= log (10ï¼Œ000/25) = 2.60

TFIDF(ç¼–ç )= 0.015 * 2.60 = 0.039

ä¸‹é¢æ˜¯åˆ›å»ºäºŒå…ƒæ¨¡åž‹å¹¶ä½¿ç”¨ TF-IDF æ–¹æ³•ä¸ºæ¯ä¸ªäºŒå…ƒæ¨¡åž‹åˆ†é…æƒé‡çš„ä»£ç ã€‚

```
from sklearn.feature_extraction.text import TfidfVectorizer# TFIDF: bigram
bigrams_list = list(nltk.bigrams(sn_reviews_words))
bigram = [' '.join(tup) for tup in bigrams_list]vectorizer = TfidfVectorizer(bigram,use_idf=True,ngram_range=(2,2))
X = vectorizer.fit_transform(bigram)
vectorizer.vocabulary_
sum_words = X.sum(axis=0) 
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
```

# æ­¥éª¤ 3:åˆ›å»º Word Cloudâ›…

æ—¢ç„¶æˆ‘ä»¬å·²ç»åˆ›å»ºäº†äºŒå…ƒæ¨¡åž‹å¹¶ä½¿ç”¨ TF-IDF æ–¹æ³•è®¡ç®—äº†æƒé‡ï¼ŒçŽ°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªäºŒå…ƒæ¨¡åž‹è¯äº‘ã€‚`plt.imshow()`ä¸­çš„å‚æ•°`interpolation="bilinear"`æ˜¯ä¸ºäº†è®©å›¾åƒçš„æ˜¾ç¤ºæ›´åŠ æµç•…ã€‚ðŸƒâ€â™€ï¸

```
import matplotlib.pyplot as plt
from wordcloud import WordCloudwords_dict = dict(words_freq)wordCloud = WordCloud(height=1400, width=1800)
wordCloud.generate_from_frequencies(words_dict)
plt.title('Most Frequently Occurring Bigrams')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show()
```

ä¸ºäº†æ‰§è¡Œæ–‡æœ¬æƒ…æ„Ÿåˆ†æžï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ NLTK åŒ…ä¸­çš„ [VADER(ç”¨äºŽæƒ…æ„ŸæŽ¨ç†çš„ä»·æ„ŸçŸ¥è¯å…¸)æ¨¡åž‹](https://www.nltk.org/api/nltk.sentiment.vader.html)ã€‚è¯¥æ¨¡åž‹å¯ä»¥é€šè¿‡å°†æ¯ä¸ªå•è¯æ‰€æºå¸¦çš„æƒ…ç»ªçš„æžæ€§(ç§¯æž/æ¶ˆæž)å’Œå¼ºåº¦(å¼ºåº¦)ç›¸åŠ æ¥è®¡ç®—æƒ…ç»ªå¾—åˆ†ã€‚

![](img/40bc66a75dc636f35d779e635d90d094.png)

Source: [Pexels@Julia Avamotive](https://www.pexels.com/photo/woman-holding-a-smiley-balloon-1236678/)

```
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')# initialize VADER
sid = SentimentIntensityAnalyzer()
pos_word_list=[]
neu_word_list=[]
neg_word_list=[]for word in sn_reviews_words:
    if (sid.polarity_scores(word)['compound']) >= 0.25:
        pos_word_list.append(word)
    elif (sid.polarity_scores(word)['compound']) <= -0.25:
        neg_word_list.append(word)
    else:
        neu_word_list.append(word) # Positive word cloud
# Choosing the only words which are present in positive words
sn_pos_in_pos = " ".join ([w for w in pos_word_list])
wordcloud_pos_in_pos = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(sn_pos_in_pos)
plt.title("Positive Words In The Review of Sony Speaker")
plt.imshow(wordcloud_pos_in_pos, interpolation="bilinear")# negative word cloud
# Choosing the only words which are present in negwords
sn_neg_in_neg = " ".join ([w for w in neg_word_list])
wordcloud_neg_in_neg = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(sn_neg_in_neg)
plt.title("Negative Words In The Review of Sony Speaker")
plt.imshow(wordcloud_neg_in_neg, interpolation="bilinear")
```

![](img/ce77df9c47165287e558e614fa6e2703.png)![](img/f93f23ce0b415b07b5551fd291203221.png)![](img/244570cd1a77675a83616c1ed4f2e050.png)

ä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç½‘é¡µæŠ“å–é¡¹ç›®çš„ä¾‹å­ã€‚ä½ å¯ä»¥åœ¨ä¸åŒçš„ç½‘ç«™ä¸Šè¿›è¡ŒæŒ‘é€‰ï¼Œæ¯”å¦‚[GitHub Popular Repositories](https://github.com/collections)ã€[ç”µå½±æ•°æ®é›†(TMDb)](https://www.themoviedb.org/movie) ç­‰ç­‰ã€‚æ— è®ºå¦‚ä½•ï¼Œå¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ åœ¨ä½ çš„ç½‘é¡µæŠ“å–ä¹‹æ—…ã€‚å¿«ä¹çš„ç½‘é¡µæŠ“å–ï¼ðŸ˜‰

[](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) [## Mlearning.ai æäº¤å»ºè®®

### å¦‚ä½•æˆä¸º Mlearning.ai ä¸Šçš„ä½œå®¶

medium.com](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb) 

ðŸŸ åœ¨ MLearning.ai æˆä¸º[ä½œå®¶](/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb)