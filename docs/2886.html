<html>
<head>
<title>Additive Margin Softmax Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">附加保证金软最大损失</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/additive-margin-softmax-loss-3c78e37b08ed?source=collection_archive---------5-----------------------#2022-06-22">https://medium.com/mlearning-ai/additive-margin-softmax-loss-3c78e37b08ed?source=collection_archive---------5-----------------------#2022-06-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="3bbe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个故事是为了记录我最近对加性保证金Softmax (AM-softmax)损失的认识。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es jc"><img src="../Images/187875c3103efe5fec1a15cf0c19d078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*Lffcml9LSCudts7OoeGR4Q.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">source: <a class="ae jo" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.05599</a></figcaption></figure><p id="7a9c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在说AM-Softmax之前，我们需要先回顾一下什么是Angular Softmax (A-Softmax)。</p><h1 id="ea1b" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">原始Softmax </strong></h1><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es kn"><img src="../Images/2d9b4be9b57bcf22ad95db2cb042b333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*YGe676Sih_KuhWC2qh3Yig.png"/></div></figure><p id="a41b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里<em class="ko"> f </em>是最后一个全连接层的输入。<em class="ko"> W </em>是最后一层的参数。</p><h1 id="6604" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">角度最大值</strong></h1><p id="b70f" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated"><a class="ae jo" href="https://arxiv.org/pdf/1704.08063.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1704.08063.pdf</a></p><p id="9eea" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与最初的softmax相比，作者做了以下更改:</p><ol class=""><li id="97b0" class="ku kv hh ig b ih ii il im ip kw it kx ix ky jb kz la lb lc bi translated">归一化权重‖ <em class="ko"> W </em> ‖ = 1</li><li id="20bb" class="ku kv hh ig b ih ld il le ip lf it lg ix lh jb kz la lb lc bi translated">更新目标逻辑计算</li></ol><p id="26be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，A-Softmax损失变为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/ce66a5d862aab41d9a54bf79291d151f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*utk6_nFe9Hb3oATIvQooIQ.png"/></div></figure><p id="62a7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在两者之间，<em class="ko"> ψ(θ) </em>是一个分段函数，其定义为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lj"><img src="../Images/62b0e08f82e29e32e61959c1b8361796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*ervEmvfVvWMvRMEkd0talA.png"/></div></figure><p id="b91f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正常情况下，m是大于1的整数，λ是超参数，用于控制推动分类边界的难度。在训练阶段，λ从1000退火到一个小值，使每一类的角空间变得更大更紧凑。</p><h1 id="7fa1" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">附加余量最大值</strong></h1><p id="5287" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated"><a class="ae jo" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.05599</a></p><p id="626f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者提出了一个特定的函数，该函数在softmax损失函数中引入了一个附加余量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lk"><img src="../Images/ba192ffba0ea92f255897e46502450d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*o4LalztfeMMe5mssCeM_4w.png"/></div></figure><p id="691b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与L-Softmax和A-Softmax相比，这个定义更简单但更有用。在特征和权重都归一化之后，<em class="ko"> cos(θ) </em>可以进一步定义为:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ll"><img src="../Images/ed058f1572660d73b879208bf68b91ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Im2zPXO3tz0ptNFLs1Kh9A.png"/></div></figure><p id="d3ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，整个损失函数是:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lm"><img src="../Images/baf7a05681c0a8b10a476843dd62cdd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*gS5R1qSIIqj5hrwP2_EU8Q.png"/></div></figure><p id="3e90" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一个超参数。最初，作者试图让它变得可以学习。但是，整个网络收敛速度非常慢。因此，他们决定将<em class="ko"> s </em>固定为一个大值(30)，这样可以使优化更加稳定。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es ln"><img src="../Images/ff47831df6f7356249531bd2bad8b5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*b7elRPyyVSix1bHgmowoGA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Decision boundary comparion between convenitional Softmax and AM-Softmax (<a class="ae jo" href="https://arxiv.org/abs/1801.05599" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="e1b0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个<em class="ko"> -m </em>试图将嵌入推离其他类中心。超级直观。</p><p id="636b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个AM-Softmax也可以用于句子相似性任务。</p><h1 id="42f7" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">问题</strong></h1><p id="4e40" class="pw-post-body-paragraph ie if hh ig b ih kp ij ik il kq in io ip kr ir is it ks iv iw ix kt iz ja jb ha bi translated">我们是否也可以在对比学习中使用AM-Softmax？这个损失相当接近InfoNCE。不确定AM-Softmax在对比学习中能不能很好的发挥作用。</p><div class="lo lp ez fb lq lr"><a rel="noopener follow" target="_blank" href="/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hi fi z dy lw ea eb lx ed ef hg bi translated">Mlearning.ai提交建议</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">如何成为Mlearning.ai上的作家</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">medium.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf ji lr"/></div></div></a></div></div></div>    
</body>
</html>