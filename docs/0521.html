<html>
<head>
<title>Enhancing Information Retrieval via Semantic and Relevance Matching</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过语义和相关性匹配增强信息检索</h1>
<blockquote>原文：<a href="https://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818?source=collection_archive---------3-----------------------#2021-05-08">https://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818?source=collection_archive---------3-----------------------#2021-05-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/285ae97a331928c5f09452b2fccb2660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M9qM390cNgDK2Qqo.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Information retrieval</figcaption></figure><h1 id="5296" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">介绍</h1><p id="c124" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">数据是21世纪的黄金。我们每天都在创造万亿字节的数据。信息是经过处理和提炼的具有逻辑意义的数据形式。无论我们在搜索引擎上搜索，在电子商务网站上搜索产品，还是在其他任何地方搜索文章、产品、人物等，信息检索无处不在，已经成为我们日常生活不可或缺的一部分。</p><h1 id="8d2a" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">理解信息检索</h1><p id="9d61" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">在计算机科学领域中，信息检索被定义为从大量文档中获取满足信息需求的相关文档的过程。在大多数情况下，所需的信息以查询字符串的形式表示(例如:google search)，信息也是一个字符串(例如:google上的搜索结果)。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/deeb9bb9e12ce50f1afff37a0d676f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pGJvrlm0uJMKjDC0jTnUQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Sample IR system</figcaption></figure><p id="8d4a" class="pw-post-body-paragraph jr js hh jt b ju ku jw jx jy kv ka kb kc kw ke kf kg kx ki kj kk ky km kn ko ha bi translated">信息检索的核心任务是首先为查询找到匹配的文档(<strong class="jt hi">检索阶段</strong>)，然后对匹配的文档进行排序(<strong class="jt hi">排序阶段</strong>)。匹配发生在查询和集合中的每个文档之间，因为集合非常大(以十亿计)，匹配逻辑必须高效。接下来，基于文档对于查询的内容相关性、文档的性能度量和用户上下文来进行排名。</p><p id="c448" class="pw-post-body-paragraph jr js hh jt b ju ku jw jx jy kv ka kb kc kw ke kf kg kx ki kj kk ky km kn ko ha bi translated">在本文中，我们将讨论旨在根据文本内容和语义找到与查询最相关的文档的算法。我们将讨论将文档与查询匹配的发展，以及如何在排序中使用匹配信号来增加结果的相关性。</p><h1 id="5587" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">匹配算法的分类</h1><p id="6083" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">下图展示了红外匹配的重要算法。在高层次上，我们有以下方法。</p><ul class=""><li id="be55" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">单词包模型</strong>(将查询和文档作为单词包进行集合交集操作进行文本匹配)</li><li id="933c" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated"><strong class="jt hi">基于单词嵌入的</strong>(匹配发生在将文本翻译到嵌入空间并在该空间中进行代数匹配之后，一些流行的嵌入技术是word2vec和fasttext。这些嵌入是以这样的方式学习的，即语义相似的查询在嵌入空间中是邻近的，而不相关的查询在远处)</li></ul><p id="8ed6" class="pw-post-body-paragraph jr js hh jt b ju ku jw jx jy kv ka kb kc kw ke kf kg kx ki kj kk ky km kn ko ha bi translated">下一级分类基于算法是基于神经网络还是基于非神经网络。基于神经网络的进一步分类为基于<strong class="jt hi">表示的</strong>(查询和文档首先被单独翻译到嵌入空间，而彼此不知道，然后进行匹配，存在丢失精确匹配信号的风险)和基于<strong class="jt hi">交互的</strong>(显式地模拟查询词和文档术语之间的交互，精确匹配信号不会丢失)。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ln"><img src="../Images/a7e34b68d92c90adb933c1038722f51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeSl_AkN6boYc13pz1R83g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Classification of Matching algorithms in IR</figcaption></figure><p id="3eaa" class="pw-post-body-paragraph jr js hh jt b ju ku jw jx jy kv ka kb kc kw ke kf kg kx ki kj kk ky km kn ko ha bi translated">现在，我们将从关键点的角度来讨论每一个问题。我们试图涵盖广度并省略实现细节，并在本文中作为新颖的想法导出关键要点。</p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="b5b4" class="it iu hh bd iv iw lv iy iz ja lw jc jd je lx jg jh ji ly jk jl jm lz jo jp jq bi translated">基于单词的包</h1><h2 id="d509" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">基本词汇模型</h2><ul class=""><li id="9a70" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">在过去的50-60年里，这一直是最先进的技术，而且效果很好</li><li id="2c19" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">将查询和文档视为一个单词包。对于语料库中的每个术语，创建一个术语到包含该术语的文档列表的离线散列表(倒排索引的简单描述)。在在线匹配过程中，使用这个hashmap获取所有查询术语的公共文档。</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mr"><img src="../Images/16eef5e5500361e9384d092b4a9ebc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CLCEy-zUmbu1T4Nt-dyGug.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Term-Document matrix to enable efficient retrieval</figcaption></figure><ul class=""><li id="3a89" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated">有效(因为精确匹配是相关性的重要信号)和高效(因为倒排索引，我们不需要迭代所有文档)</li><li id="a61b" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">理解人类语言的肤浅方式，不考虑意义和上下文。例如<em class="ms">非中文电话</em>文档将匹配查询<em class="ms">中文电话</em>。</li><li id="99ea" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">词汇不匹配。<em class="ms">人</em>不会配<em class="ms">人</em></li><li id="7d35" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>基本单词包模型对于正常用例是有效的</li></ul><h2 id="7932" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">伪相关反馈</h2><ul class=""><li id="80ca" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://nlp.stanford.edu/IR-book/html/htmledition/pseudo-relevance-feedback-1.html" rel="noopener ugc nofollow" target="_blank">曼宁书</a>，<a class="ae mt" href="http://times.cs.uiuc.edu/czhai/pub/sigir06-mix.pdf" rel="noopener ugc nofollow" target="_blank">乐百氏PSR </a> ( 2006)</li><li id="0918" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">旨在当查询只返回少量文档时提高召回率</li><li id="41b2" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">假设顶部文档与查询相关。通过使用查询+热门文档返回结果来提高召回率</li><li id="3e7f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">顶部文档可能有一些不相关的内容，这些内容可能会将结果引向完全不同的方向。为了避免这种情况，我们以迭代的方式对每个反馈文档中潜在的不同数量的相关信息进行建模(如下图)</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/6a2b5d94669470364b0e8ce563e4b8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSNtwL3xfvnV9SgKrP1NMA.png"/></div></div></figure><ul class=""><li id="9f2d" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>使用已知的相关文档进行查询，提高召回率</li></ul><h2 id="4326" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">顺序依赖模型</h2><ul class=""><li id="dfd0" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="http://web.cs.ucla.edu/~yzsun/classes/2014Spring_CS7280/Papers/Probabilistic_Models/A%20Markov%20Random%20Field%20Model%20for%20Term%20Dependencies.pdf" rel="noopener ugc nofollow" target="_blank"> SDM </a> (2014)</li><li id="4358" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">不仅通过术语，而且通过在一定距离内共现的术语对来表示文本</li><li id="0805" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">如果在查询中接近的术语在文档中也接近，那么有强有力的证据支持相关性。</li><li id="0c8f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用马尔可夫模型，它对查询术语qi描述文档的可能性以及术语接近度的重要性进行建模</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mv"><img src="../Images/7d97cf8c3bdd25c2cd4747be6d1438fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HL513Q_PtuUQoGuG4SqLag.png"/></div></div></figure><ul class=""><li id="0337" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>单词n-grams的匹配是匹配中的一个重要信号。与一组不连续的查询术语相比，在查询中连续出现的术语提供了关于信息需求的不同(更强)的证据</li></ul><h2 id="1b1f" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">复合匹配自动完成(逗号)</h2><ul class=""><li id="8c6a" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考文献:<a class="ae mt" href="https://www.researchgate.net/publication/262220118_Composite_match_autocompletion_COMMA_A_semantic_result-oriented_autocompletion_technique_for_e-marketplaces" rel="noopener ugc nofollow" target="_blank">逗号</a> (2014)</li><li id="d3d5" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">该算法试图通过在多个领域如类别和方面上进行匹配来进行语义匹配</li><li id="4af2" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">我们对每个查询应用3个过滤器<br/> 1)标题与查询匹配的文档(语法匹配)<br/> 2)类别与查询匹配的文档(语义匹配)<br/> 3)方面与查询匹配的文档(语义匹配)</li><li id="2b01" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">最后一组是所有3个元素的结合。类别和方面匹配是仅带有前缀术语的语法匹配(给出伪语义匹配)。</li><li id="9d2c" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">对句法相关性和语义相关性(类别匹配、方面匹配的数量、方面显著性)进行排序</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mw"><img src="../Images/3c8b1b2a10a47941fd53312a5c18b1ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cm_wvgjk_RlmdL0pd9D9Lw.png"/></div></div></figure><ul class=""><li id="bd18" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>类别匹配和方面匹配信号有助于在顶部显示更多相关文档。即使当查询的所有术语都与文档标题不匹配时，类别匹配和最大方面匹配也会存在，这将有助于提高召回率</li></ul></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h2 id="6f73" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">潜在语义分析</h2><ul class=""><li id="c8bd" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考文献:<a class="ae mt" href="https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python" rel="noopener ugc nofollow" target="_blank">博客</a>，<a class="ae mt" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AC2BD6B1A006FD5867BD022C0E4D754D?doi=10.1.1.108.8490&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">大规模集成电路</a> (1990)</li><li id="fc25" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">如果我们说(雪，冬天)这个词对比(狗，冬天)这个词对更频繁地出现在一起，那就意味着它比(狗，冬天)这个词对承载着更高的语义。这是算法的基本环境。</li><li id="1bfc" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">每个文档都与主题相关联，每个术语都与主题相关联，匹配发生在主题空间中</li><li id="4577" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">然后对术语-文档矩阵进行奇异值分解(SVD)以获得主题</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mx"><img src="../Images/8b75fb225b35328a4e75ecbc01e994ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*peFq6B4iOtewo9-8aXiCvQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">SVD factorisation to learn topics</figcaption></figure><ul class=""><li id="add2" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong> LSI是主题的无监督学习，因此可能存在精度问题。然而，它可以用来扩充单词模型的基本包，其中包含查询术语匹配以及主题空间中的匹配的文档将被认为更相关。</li></ul><h2 id="3a5e" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">潜在狄利克雷分配</h2><ul class=""><li id="cd11" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/" rel="noopener ugc nofollow" target="_blank">博客</a>，<a class="ae mt" href="https://ai.stanford.edu/~ang/papers/jair03-lda.pdf" rel="noopener ugc nofollow" target="_blank"> LDA </a> (2003)</li><li id="287c" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">以不同于LSI的方式学习文档和单词的主题</li><li id="e4e3" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">初始化每个单词的随机主题。在每次迭代中向最终赋值收敛</li><li id="e3c5" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">LDA比LSI具有更好的准确性</li></ul><h2 id="cef1" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">会话查询的语义相似性</h2><ul class=""><li id="2aa1" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考文献:<a class="ae mt" href="https://iopscience.iop.org/article/10.1088/1742-6596/1004/1/012018/pdf" rel="noopener ugc nofollow" target="_blank">与会话内查询的语义相似度</a> (2018)</li><li id="41ca" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">在word2vec方法的基础上，考虑候选文档和在同一会话中点击的先前文档之间的语义相似性(在语料库中共享共同上下文的单词在空间中彼此靠近)</li><li id="a498" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>在会话中，上下文可以用来更好地理解用户的意图并匹配文档</li></ul><h2 id="91e6" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">基于参与度的内容相关性模型</h2><ul class=""><li id="1661" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://dl.acm.org/doi/10.1145/3340531.3412743" rel="noopener ugc nofollow" target="_blank">在Twitter上的相关性排名</a> ( 2020)</li><li id="1f24" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">对用户反馈进行建模是提高相关性的好方法</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es my"><img src="../Images/d8b148e67b53c97353e619c3c1ae50fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LLsI-HCp5NHeOsBAmwmA8w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Twitter Content Relevance</figcaption></figure></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h2 id="49b9" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">MERCURE系统</h2><ul class=""><li id="bebd" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://dl.acm.org/doi/pdf/10.5555/2856823.2856867" rel="noopener ugc nofollow" target="_blank"> MERCURE </a> (1994)</li><li id="7a6e" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">3层网络体系结构，也捕获了术语间的依赖性。国际关系领域中的早期神经网络</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/1695f5d01db75bfa1fb64f4ec238de36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCobyji2SUt821mDN7R0cQ.png"/></div></div></figure><h2 id="e0d3" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">深层结构化语义模型(DSSM)</h2><ul class=""><li id="47c8" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考文献:<a class="ae mt" href="https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf" rel="noopener ugc nofollow" target="_blank"> DSSM </a> (2013年)<a class="ae mt" href="https://www.iro.umontreal.ca/~lisa/pointeurs/WWW2014.pdf" rel="noopener ugc nofollow" target="_blank"> CDSSM </a> (2014年)</li><li id="1b52" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">基于神经网络的查询和文档语义匹配</li><li id="2ecb" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">神经网络的输入:高维术语向量(一种热编码来表示所包含的术语)。为了降低术语向量的维数(从500 K到30K)，我们对字符三元模型使用一键编码。</li><li id="27d8" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">输出:低维语义特征空间中的概念向量</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es na"><img src="../Images/f76dfb791463a56fecd89ee962a85e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yjh4O_HCFh5aQcdUSNAovg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Deep semantic similarity model</figcaption></figure><ul class=""><li id="51cc" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated">特征和模型都是自动学习的</li><li id="1817" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">查询-使用过去的文档点击数据进行训练</li><li id="ab78" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用卷积神经网络具有额外的优势</li><li id="d4fa" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong> DSSM帮助自动学习语义匹配特征。精确匹配信号(对于相关性匹配非常有用)在匹配之前丢失。在基于交互的神经模型(例如DRMM)中克服了这个缺点，该神经模型也对查询术语和文档术语之间的交互进行建模，以保留精确匹配信号</li></ul></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h2 id="7ee4" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">深度相关匹配模型</h2><ul class=""><li id="1d54" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://arxiv.org/pdf/1711.08611.pdf" rel="noopener ugc nofollow" target="_blank"> DRMM </a> (2017)</li><li id="3712" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">捕获相关性匹配的神经网络</li><li id="be8e" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated"><strong class="jt hi">语义匹配</strong>(含义应该相同，对查询和文档一视同仁)与<strong class="jt hi">相关性匹配</strong>(精确匹配信号，查询术语重要性，考虑查询通常小于文档)</li><li id="52c5" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">与此匹配的文档的得分顺序一般是这样的:精确匹配&gt;软匹配&gt;弱软匹配</li><li id="cdac" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">对于查询中的每个术语和文档中的所有术语，它计算嵌入空间中的余弦相似性。例如:给定一个查询术语“汽车”和一个带有术语(汽车、出租、卡车、颠簸、禁令、跑道)的文档，基于余弦相似性的相应局部交互是(1，0.2，0.7，0.3，0.1，0.1)</li><li id="5df3" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">此外，它通过将每个余弦相似度分类到这些二进制值之一{[1，0.5]，[0，0.5]，[0.5，1]，[1，1]}来创建直方图。Bin [1，1]捕捉精确匹配信号。对于前面的例子，我们将获得一个匹配直方图[0，1，3，1，1]</li><li id="894e" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">该直方图池旨在将精确匹配与软匹配分开，并将强软匹配与弱软匹配分开。</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nb"><img src="../Images/4e668307610279164c7e953dd78fa2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qe5MU61BeekHRIhB2WE9FQ.png"/></div></div></figure><ul class=""><li id="6371" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>这捕获了文档与查询的相关性。这里的限制是，我们试图将查询的一个术语与文档的一个术语匹配，这可能无法捕获所有的词汇匹配场景(例如:美国对美国，LHS的一个术语与RHS的三个术语匹配)</li></ul><h2 id="d9c4" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">Doc2Query</h2><ul class=""><li id="910f" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://arxiv.org/pdf/1904.08375.pdf" rel="noopener ugc nofollow" target="_blank"> Doc2Query </a> (2019)</li><li id="475f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用神经机器翻译从文档中生成潜在查询，并将这些查询索引为文档扩展术语</li><li id="bb73" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">对于长段落和FAQ类型的搜索很有用</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/89f0e1c01de2842e112393a2ec1a66b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*unwZsIYFB_a-Lsg9rCmWZA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Doc2Query</figcaption></figure><h2 id="978f" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">K-NRM(基于核的神经排序模型)</h2><ul class=""><li id="4275" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://arxiv.org/pdf/1706.06613.pdf" rel="noopener ugc nofollow" target="_blank"> K-NRM </a> (2017)、<a class="ae mt" href="https://arxiv.org/pdf/1809.10522.pdf" rel="noopener ugc nofollow" target="_blank"> K-NRM1 </a> (2018)、<a class="ae mt" href="http://www.cs.cmu.edu/~./callan/Papers/wsdm18-zhuyun-dai.pdf" rel="noopener ugc nofollow" target="_blank"> Conv-K-NRM </a> (2019)</li><li id="2567" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">首先，我们为查询和文档生成单词嵌入</li><li id="0d9d" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">然后交叉匹配不同长度的查询n元文法和文档n元文法(处理DRMM的缺点)</li><li id="fc5f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用内核池区分有用的软匹配和有干扰的软匹配</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nd"><img src="../Images/1d734ecb5f8ba93f2d0395595122d646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFSYPots2gKUf5UT66OOdQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">K-NRM model architecture</figcaption></figure><h2 id="3809" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">语义产品搜索</h2><ul class=""><li id="cb2b" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考:<a class="ae mt" href="https://arxiv.org/pdf/1907.00937.pdf" rel="noopener ugc nofollow" target="_blank">产品搜索</a> ( 2019)</li><li id="56ff" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">为查询和文档生成Unigrams+Bigrams+Char三元模型的嵌入，然后进行匹配</li><li id="311f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用一致散列法处理OOV词</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/0168ccc1d82bd08e6b4aa500b0c48541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2MjsYPPYJJwwIqt4y7f0FA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Generating embedding for document and query</figcaption></figure><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/06130cb48a827c92fb2796b660489fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RijYprm8UTXAtTeSLQ6Uw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Neural network for semantic product search</figcaption></figure><h2 id="13e1" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">多伯特</h2><ul class=""><li id="84ed" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考文献:<a class="ae mt" href="https://arxiv.org/pdf/1905.09217.pdf" rel="noopener ugc nofollow" target="_blank">多伯特</a> (2019)</li><li id="9f4d" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">使用单词的语境化表示</li><li id="f19d" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">令牌被嵌入到嵌入中。为了进一步将查询从文档中分离出来，段嵌入‘Q’(对于查询令牌)和‘D’(对于文档令牌)被添加到令牌嵌入中。为了捕捉词序，添加了位置嵌入。代币要经过几层变压器</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/777ba7045ee0c71533a0f04bfdcb2ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1IUp9kL3HfZ959jPOavvOA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Bert Contextualised embedding</figcaption></figure><ul class=""><li id="3956" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>一个术语的所有出现不会被同等对待。这取决于该术语是否出现在文档或查询中，还取决于该术语出现的位置。这些有助于更好地模拟术语的上下文。</li></ul><h2 id="8f22" class="ma iu hh bd iv mb mc md iz me mf mg jd kc mh mi jh kg mj mk jl kk ml mm jp mn bi translated">深度上下文化术语加权框架(DeepCT)</h2><ul class=""><li id="ee13" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko le lf lg lh bi translated">参考资料:<a class="ae mt" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401204" rel="noopener ugc nofollow" target="_blank">DeepCT</a>(2020)<a class="ae mt" href="https://dl.acm.org/doi/pdf/10.1145/3366423.3380258" rel="noopener ugc nofollow" target="_blank">HDCT</a>(2020)</li><li id="fd1b" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">识别长文本中的重要术语，有助于段落检索</li><li id="8dd7" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">根据意思而不是词频来查找文本中的核心词</li></ul><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nh"><img src="../Images/294ce0a2912fd69ec81f922e2a95c609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVchjSMgVOKGIWFwxXHpUg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">DeepCT in action</figcaption></figure><ul class=""><li id="49be" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated"><strong class="jt hi">要点:</strong>它有助于找到一段话中最核心的词，这样它就可以匹配那些核心词出现在查询中的文档。在上面的例子中，第二个文档离题了，即使它包含更多的DNA术语。</li></ul></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="1a35" class="it iu hh bd iv iw lv iy iz ja lw jc jd je lx jg jh ji ly jk jl jm lz jo jp jq bi translated">摘要</h1><ol class=""><li id="5063" class="kz la hh jt b ju jv jy jz kc mo kg mp kk mq ko ni lf lg lh bi translated">单词袋模型非常<strong class="jt hi">有效</strong>(因为精确的术语匹配是非常重要的相关性信号)和<strong class="jt hi">高效</strong>(由于倒排索引，获得具有匹配术语的文档非常快，并且不需要迭代文档)</li><li id="1d8a" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">单词模型包有一定的局限性，它<strong class="jt hi">不理解人类语言</strong> e(非中文电话的文档将匹配查询中文电话)，不理解<strong class="jt hi">词汇不匹配</strong>(例如小狗的文档将不匹配狗的文档)。术语的重要性是由术语的频率决定的，而不是由语义理解决定的。</li><li id="d08a" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">我们可以通过混合使用热门文档和用户输入的查询作为反馈来返回更多的文档，从而提高召回率</li><li id="dd17" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">对文档中的短语重要性和文档中的术语重要性进行建模(<strong class="jt hi"> P(文档/短语)，P(文档/术语)</strong>)可以增加相关性</li><li id="bb74" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">如果在查询中接近的术语在文档中也接近，那么有强有力的证据支持相关性。</li><li id="8659" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated"><strong class="jt hi">将</strong>查询不仅与文档标题匹配，还与<strong class="jt hi">类别和方面</strong>匹配，并为这些领域中的匹配导出排名信号，这有助于增加相关性</li><li id="b4ee" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">语义匹配和关联匹配的区别。<strong class="jt hi">语义匹配</strong>(含义相同，对查询和文档一视同仁)与<strong class="jt hi">相关性匹配</strong>(精确匹配信号、查询术语重要性、查询通常小于文档)</li><li id="ab93" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">使用用户对查询文档的参与度作为相关性的度量。</li><li id="4098" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko ni lf lg lh bi translated">所讨论的各种神经检索模型(DSSM、DRMM、K-NRM、语义搜索)试图捕捉以下内容:</li></ol><ul class=""><li id="53d5" class="kz la hh jt b ju ku jy kv kc lb kg lc kk ld ko le lf lg lh bi translated">查询文档术语之间的交互</li><li id="9856" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">查询-文档字符三元组之间的交互</li><li id="b1d7" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">查询文档的不同长度单词n元文法之间的交互</li><li id="12cc" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">考虑精确匹配和软匹配的信号</li><li id="685f" class="kz la hh jt b ju li jy lj kc lk kg ll kk lm ko le lf lg lh bi translated">考虑哪些信号是查询和文档中最重要的术语</li></ul><h1 id="a836" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">反馈</h1><p id="ba38" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ha bi translated">有问题吗？评论？联系方式:<a class="ae mt" href="https://www.linkedin.com/in/suryakant-pandey/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae mt" href="https://www.instagram.com/pd.skant/" rel="noopener ugc nofollow" target="_blank"> Instagram </a></p></div></div>    
</body>
</html>